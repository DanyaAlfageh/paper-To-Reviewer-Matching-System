<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<reference confidence="0.2433538">
b&apos;Proceedings of NAACL HLT 2007, pages 6572,
Rochester, NY, April 2007. c
2007 Association for Computational Linguistics
Structured Local Training and Biased Potential Functions for Conditional
Random Fields with Application to Coreference Resolution
</reference>
<author confidence="0.934486">
Yejin Choi and Claire Cardie
</author>
<affiliation confidence="0.9940675">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.962714">
Ithaca, NY 14853
</address>
<email confidence="0.998683">
{ychoi,cardie}@cs.cornell.edu
</email>
<sectionHeader confidence="0.990433" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99515595">
Conditional Random Fields (CRFs) have shown
great success for problems involving structured out-
put variables. However, for many real-world NLP
applications, exact maximum-likelihood training is
intractable because computing the global normal-
ization factor even approximately can be extremely
hard. In addition, optimizing likelihood often does
not correlate with maximizing task-specific evalu-
ation measures. In this paper, we present a novel
training procedure, structured local training, that
maximizes likelihood while exploiting the benefits
of global inference during training: hidden vari-
ables are used to capture interactions between lo-
cal inference and global inference. Furthermore,
we introduce biased potential functions that empir-
ically drive CRFs towards performance improve-
ments w.r.t. the preferred evaluation measure for
the learning task. We report promising experimen-
tal results on two coreference data sets using two
task-specific evaluation measures.
</bodyText>
<sectionHeader confidence="0.997413" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995910916666667">
Undirected graphical models such as Conditional
Random Fields (CRFs) (Lafferty et al., 2001) have
shown great success for problems involving struc-
tured output variables (e.g. Wellner et al. (2004),
Finkel et al. (2005)). For many real-world NLP ap-
plications, however, the required graph structure can
be very complex, and computing the global normal-
ization factor even approximately can be extremely
hard. Previous approaches for training CRFs have
either (1) opted for a training method that no longer
maximizes the likelihood, (e.g. McCallum and Well-
ner (2004), Roth and Yih (2005))1, or (2) opted for a
</bodyText>
<page confidence="0.916599">
1
</page>
<bodyText confidence="0.978409487179487">
Both McCallum and Wellner (2004) and Roth and Yih
(2005) used the voted perceptron algorithm (Collins, 2002) to
train intractable CRFs.
simplified graph structure to avoid intractable global
normalization (e.g. Roth and Yih (2005), Wellner et
al. (2004)).
Solutions of the first type replace the computation
of the global normalization factor
\x01
y p(y|x) with
argmaxy p(y|x) during training, since finding an
argmax of a probability distribution is often an eas-
ier problem than finding the entire probability distri-
bution. Training via the voted perceptron algorithm
(Collins, 2002) or using a max-margin criterion also
correspond to the first option (e.g. McCallum and
Wellner (2004), Finley and Joachims (2005)). But
without the global normalization, the maximum-
likelihood criterion motivated by the maximum en-
tropy principle (Berger et al., 1996) is no longer a
feasible option as an optimization criterion.
The second solution simplifies the graph struc-
ture for training, and applies complex global infer-
ence only for testing. In spite of the discrepancy
between the training model and the testing model,
it has been empirically shown that (1) performing
global inference only during testing can improve
performance (e.g. Finkel et al. (2005), Roth and Yih
(2005)), and (2) full-blown global training can of-
ten perform worse due to insufficient training data
(e.g. Punyakanok et al. (2005)). Importantly, how-
ever, attempts to reduce the discrepancy between the
training and test models by judiciously adding the
effect of global inference to the training have pro-
duced substantial performance improvements over
locally trained models (e.g. Cohen and Carvalho
(2005), Sutton and McCallum (2005a)).
In this paper, we present structured local training,
a novel training procedure for maximum-likelihood
</bodyText>
<page confidence="0.997771">
65
</page>
<bodyText confidence="0.997781212121212">
\x0ctraining of undirected graphical models, such as
CRFs. The procedure maximizes likelihood while
exploiting the benefits of global inference during
training by capturing the interactions between local
inference and global inference via hidden variables.
Furthermore, we introduce biased potential func-
tions that redefine the likelihood for CRFs so that
the performance of CRFs trained under the max-
imum likelihood criterion correlates better empiri-
cally with the preferred evaluation measures such as
F-score and MUC-score.
We focus on the problem of coreference resolu-
tion; however, our approaches are general and can
be extended to other NLP applications with struc-
tured output. Our approaches also extend to non-
conditional graphical models such as Markov Ran-
dom Fields. In experiments on two coreference data
sets, structured local training reduces the error rate
significantly (3.5%) for one coreference data set and
minimally ( 1%) for the other. Experiments using
biased potential functions increase recall uniformly
and significantly for both data sets and both task-
specific evaluation measures. Results for the com-
bination of the two techniques are promising, but
mixed: pairwise F1 increases by 0.8-5.5% for both
data sets; MUC F1 increases by 3.5% for one data
set, but slightly hurts performance for the second
data set.
In 2, we describe structured local training, and
follow with experimental results in 3. In 4, we
describe biased potential functions and follow with
experimental results in 5. We discuss related work
in 6.
</bodyText>
<sectionHeader confidence="0.834818" genericHeader="method">
2 Structured Local Training
</sectionHeader>
<subsectionHeader confidence="0.850537">
2.1 Definitions
</subsectionHeader>
<bodyText confidence="0.998321375">
For clarity, we define the following terms that we
will use throughout the paper.
local inference: 2 Inference factored into smaller
independent pieces, without considering the
structure of the output space.
global inference: Inference applied on the entire
set of output variables, considering the structure
of the output space.
</bodyText>
<page confidence="0.980962">
2
</page>
<bodyText confidence="0.9909025">
In this paper, inference refers to the operation of finding the
argmax in particular.
local training: Training that does not invoke
global inference at each iteration.
global training: Training that does invoke global
inference at each iteration.
</bodyText>
<subsectionHeader confidence="0.9719535">
2.2 A Motivating Example for Coreference
Resolution
</subsectionHeader>
<bodyText confidence="0.97190946875">
In this section, we present an example of the coref-
erence resolution problem to motivate our approach.
It has been shown that global inference-based train-
ing for coreference resolution outperforms training
with local inference only (e.g. Finley and Joachims
(2005), McCallum and Wellner (2004)). In particu-
lar, the output of coreference resolution must obey
equivalence relations, and exploiting such structural
constraints on the output space during training can
improve performance. Consider the coreference res-
olution task for the following text.
It was after the passage of this act, that Mary(1)
s attitude
towards Elizabeth(1)
became overtly hostile. The deliber-
ations surrounding the act seem to have revived all Marys
memories of the humiliations she had suffered at the
hands of Anne Boleyn. At the same time, Elizabeth(2)
s
continuing prevarications over religion confirmed that she
was indeed her mothers daughter.
In the above text, the she in the last sen-
tence is coreferent with both mentions of
Elizabeth. However, when we consider
she and Elizabeth(1) in isolation from the
remaining coreference chain, it can be difficult for
a machine learning method to determine whether
the pair is coreferent or not. Indeed, such a
pair may not look very different from the pair
she and Mary(1) in terms of feature vectors.
It is much easier, however, to determine that
she and Elizabeth(2) are coreferent, or that
</bodyText>
<equation confidence="0.848757">
Elizabeth(1) and Elizabeth(2) are coreferent.
</equation>
<bodyText confidence="0.999276272727273">
Only by taking the transitive closure of these pair-
wise coreference relations does it become clear that
she and Elizabeth(1) are coreferent. In other
words, global training might handle potentially
confusing coreference cases better because it allows
parameter learning (for each pairwise coreference
decision) to be informed by global inference.
We argue that, with appropriate modification to
the learning instances, local training is adequate for
the coreference resolution task. Specifically, we pro-
pose that confusing pairs in the training data such
</bodyText>
<page confidence="0.987273">
66
</page>
<bodyText confidence="0.995311428571429">
\x0cas she and Elizabeth(1) be learned as not-
coreferent, so long as the global inference step can
fix this error by exploiting the structure of the out-
put space, i.e. by exploiting the equivalence rela-
tions. This is the key idea of structured local train-
ing, which we elaborate formally in the following
section.
</bodyText>
<subsectionHeader confidence="0.994588">
2.3 A Hidden-Variable Model
</subsectionHeader>
<bodyText confidence="0.96855475">
In this section, we present a general description of
structured local training. Let y be a vector of out-
put variables for structured output, and let x be a
vector of input variables. In order to capture the in-
teractions between global inference and local infer-
ence, we introduce hidden variables h, |h |= |y|,
so that the global inference for p(y, h|x) can be fac-
tored into two components using the product rule, as
follows:
p(y, h|x) = p(y|h, x) p(h|x)
= p(y|h) p(h|x)
The second component p(h|x) on the right hand side
corresponds to the local model, for which the infer-
ence factorizes into smaller independent pieces, e.g.
argmaxhp(h|x) = {argmaxhi
(hi, x)}. And the
first component p(y|h, x) on the right hand side cor-
responds to the global model, whose inference may
not factorize nicely. Further, we assume that y is in-
dependent of x given h, so that p(y|h, x) = p(y|h).
That is to say, h captures sufficient information from
x, so that given h, global inference of y only de-
pends on h. The quantity of p(y|x) then is given by
marginalizing out h as follows:
</bodyText>
<equation confidence="0.994245333333333">
p(y|x) =
\x02
h
</equation>
<bodyText confidence="0.940011">
p(y, h|x)
Intuitively, the hidden variables h represent the lo-
cal decisions that can lead to a good y after global
inference is applied. In the case of coreference reso-
lution, one natural factorization would be that global
inference is a clustering algorithm, and local infer-
ence is a classification decision on each pair of noun
phrases (or mentions).3 In this paper, we assume
</bodyText>
<page confidence="0.989713">
3
</page>
<bodyText confidence="0.99123405882353">
Formally, we define each yi y to be the coreference de-
cision for the ith pair of mentions, and xi x be the input
regarding the ith pair of mentions. Then hi corresponds to the
local coreference decision that can lead to a good coreference
decision yi after the clustering algorithm has been applied.
that we only parameterize the local model p(h|x),
although it would be possible to extend the parame-
terization to the global model as well, depending on
the particular application under consideration. The
similarity between a pair of mentions is parameter-
ized via log-linear models. However, once we have
the similarity scores extracted via local inference,
the clustering algorithm does not require further pa-
rameterization.
For training, we apply the standard Expectation-
Maximization (EM) algorithm (Dempster et al.,
1977) as follows:
</bodyText>
<equation confidence="0.9577836">
E Step: Compute a distribution
\x03
P(t)
= P(h|y, x, (t1)
)
M Step: Set (t) to that maximizes
E\x03
P (t) [logP(y, h|x, )]
By repeatedly applying the above two steps for
t = 1, 2, ..., the value of converges to the local
</equation>
<bodyText confidence="0.9071475">
maxima of the conditional log likelihood L() =
logP(y|x, ).
</bodyText>
<subsectionHeader confidence="0.990927">
2.4 Application to Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.998384857142857">
For yi y (and hi h) in the coreference resolution
task, yi = 1 (and hi = 1) corresponds to ith pair of
mentions being coreferent, and yi = 0 (and hi = 0)
corresponds to ith pair being not coreferent.
[Local Model P(h|x)] For the local model, we de-
fine cliques as individual nodes,4 and parameterize
each clique potential as
</bodyText>
<equation confidence="0.995968454545454">
(hi, x) = (hi, xi) = exp
\x02
k
kfk(hi, xi)
Let (h|x)
\x04
i (hi, xi). Then,
P(h|x) =
(h, x)
\x01
h (h, x)
</equation>
<bodyText confidence="0.9817445">
Notice that in this model, finding argmaxhP(h|x)
corresponds to simply finding argmaxhi
(hi, xi) in-
dependently for each hi h.
</bodyText>
<page confidence="0.986117">
4
</page>
<bodyText confidence="0.998919">
Each node in the graphical representation of CRFs corre-
sponds to the coreferent decision for each pair of mentions. This
corresponds to the Model 3 of McCallum and Wellner (2004).
</bodyText>
<page confidence="0.992004">
67
</page>
<bodyText confidence="0.8910928">
\x0cALGORITHM-1
INPUT: x, true labeling y
, current local model P(h|x)
GOAL: Find the highest confidence labeling y\x02
such that y
</bodyText>
<equation confidence="0.981907068181818">
= single-link-clustering(y\x02
)
h
argmaxhP(h|x)
h\x02
single-link-clustering(h
)
construct a graph G = (V, E), where
E = {h\x02
i : h\x02
i h\x02
s.t. y
i = 1}
V = {v : v is a NP referred by a h\x02
i E}
with edge cost costh\x01
i
= (h\x02
i, xi) if h\x02
i \x03= y
i
with edge cost costh\x01
i
= 0 if h\x02
i = y
i
find a minimum spanning tree(or forest) M of G
for each h\x02
i h\x02
if h\x02
i = y
i
y\x02
i h
i
else if h\x02
i M
y\x02
i 1
else
y\x02
i 0
end for
return y\x02
</equation>
<figureCaption confidence="0.996635">
Figure 1: Algorithm to find the highest confidence labeling y\x02
</figureCaption>
<bodyText confidence="0.956617307692308">
that can be clustered to the true labeling y
[Global Model P(y|h)] For the global model, we
assume a deterministic clustering algorithm is given.
In particular, we focus on single-link clustering, as it
has been shown to be effective for coreference reso-
lution (e.g. Ng and Cardie (2002)). With single-link
clustering, P(y|h) = 1 if h can be clustered to y,
and P(y|h) = 0 if h cannot be clustered to y.5
[Computation of the E-step] The E-step requires
computation of the distribution of P(h|y, x, (t1)),
which we will simply denote as P(h|y, x), since all
our distributions are implicitly conditioned on the
model parameters .
</bodyText>
<equation confidence="0.999547">
P(h|y, x) =
P(h, y|x)
P(y|x)
P(y|h) P(h|x)
</equation>
<bodyText confidence="0.980137833333333">
Notice that when computing P(h|y, x), the denomi-
nator P(y|x) stays as a constant for different values
of h. The E-step requires enumeration of all possible
values of h, but it is intractable with our formulation,
because inference for the global model P(y|h) does
not factor out nicely. Therefore, we must resort to an
</bodyText>
<page confidence="0.936944">
5
</page>
<bodyText confidence="0.922928666666667">
Single-link clustering simply takes the transitive closure,
and does not consider the distance metric. In a pilot study, we
also tried a variant of a stochastic clustering algorithm that takes
into account the distance metric (set as the probabilities from
the local model) for the global model, but the performance was
worse.
ALGORITHM-2
INPUT: x, true labeling y
, current local model P(h|x)
GOAL: Find a high confidence labeling y\x02
that is
close to the true labeling y
</bodyText>
<equation confidence="0.994525789473684">
h
argmaxhP(h|x)
h\x02
single-link-clustering(h
)
for each h\x02
i h\x02
if h\x02
i = y
i
y\x02
i h
i
else
y\x02
i y
i
end for
return y\x02
</equation>
<figureCaption confidence="0.507001666666667">
Figure 2: Algorithm to find a high confidence labeling y\x02
that
is close to the true labeling y
approximation method. Neal and Hinton (1998) an-
alyze and motivate various approximate EM training
methods. One popular choice in practice is called
Viterbi training, a variant of the EM algorithm,
which has been shown effective in many NLP ap-
plications. Viterbi training approximates the distri-
bution by assigning all probability mass to a single
best assignment. The algorithm for this is shown in
Figure 1.
</figureCaption>
<bodyText confidence="0.938272944444444">
We propose another approximation option for the
E-step that is given by Figure 2. Intuitively, when
the current local model misses positive coreference
decisions, the first algorithm constructs a y\x01 that is
closest to h\x01
for single-link clustering to recover the
true labeling y, while the second algorithm con-
structs a y\x01 that is closer to y by preserving all of
the missing positive coreference decisions. 6
[Computation of M-step] Because P(y|h) is not
parameterized, finding argmax P(y, h|x) reduces
to finding argmax P(h|x), which is standard CRF
training. In order to speed up the training, we start
convex optimization for CRFs using the parame-
ter values (t1) from the previous M-step. For
the very first iteration of EM, we start by setting
P(y|x) = 1 for E-step, so that the first M-step will
finds argmax P(y|x).
</bodyText>
<page confidence="0.916374">
6
</page>
<bodyText confidence="0.96590975">
In a pilot study, we found that ALGORITHM-2 per-
forms slightly better than ALGORITHM-1. We also tried two
other approximation options, but none performed as well as
ALGORITHM-2. One of them removes the confusing sub-
instances and has the effect of setting a uniform distribution on
those sub-instances. The other computes the actual distribution
on a subset of sub-instances. For brevity, we only present ex-
perimental results using ALGORITHM-2 in this paper.
</bodyText>
<page confidence="0.995975">
68
</page>
<bodyText confidence="0.98210825">
\x0c[Inference on the test data] It is intractable to
marginalize out h from P(y, h|x). Therefore, sim-
ilar to the Viterbi-training in the E-step, we approx-
imate the distribution of h by argmaxhP(h|X).
</bodyText>
<sectionHeader confidence="0.998687" genericHeader="method">
3 ExperimentsI
</sectionHeader>
<bodyText confidence="0.9978124375">
Data set: We evaluate our approach with two
coreference data sets: MUC6 (MUC-6, 1995) and
MPQA7(Wiebe et al., 2005). For the MUC6 data set,
we extract noun phrases (mentions) automatically,
but for MPQA, we assume mentions for corefer-
ence resolution are given as in Stoyanov and Cardie
(2006). For MUC6, we use the standard training/test
data split. For MPQA, we use 150 documents for
training, and 50 documents for testing.
Configuration: We follow Ng and Cardie (2002)
for feature vector construction for each pair of men-
tions,8 and Finley and Joachims (2005) for con-
structing a training/testing instance for each docu-
ment: a training/testing instance consists of all pairs
of mentions in a document. Then, a single pair of
mentions is a sub-instance. We use the Mallet9 im-
plementation of CRFs, and set a Gaussian prior of
1.0 for all experiments. At each M-step, we train
CRFs starting from the parameters from the previous
M-step. We train CRFs up to 200 iterations, but be-
cause we start training CRFs from the previous pa-
rameters, the convergence from the second M-step
becomes much faster. We apply up to 5 EM itera-
tions, and choose best performing (t), 2 t 5
based on the performance on the training data.10
Hypothesis: For the baseline (BASE) we employ
the locally trained model for pairwise decisions
without global inference. Clustering is applied only
at test time, in order to make the assignment on the
output variables coherent. We hypothesize that for
the baseline, maximizing the likelihood for training
will correlate more with the pairwise accuracy of the
</bodyText>
<page confidence="0.986658">
7
</page>
<footnote confidence="0.71058">
Available at http://nrrc.mitre.org/NRRC/publications.htm.
</footnote>
<page confidence="0.99173">
8
</page>
<bodyText confidence="0.962394">
In particular, our feature set corresponds to All Features
in Ng and Cardie (2002), and we discretized numeric values.
</bodyText>
<page confidence="0.829128">
9
</page>
<footnote confidence="0.62099">
Available at http://mallet.cs.umass.edu.
</footnote>
<page confidence="0.872395">
10
</page>
<subsectionHeader confidence="0.454434">
Selecting (t)
</subsectionHeader>
<bodyText confidence="0.9728538">
on a separate tuning data would be better, but
the data for MUC6 in particular is very limited. Notice that we
dont pick 1
when reporting the performance of SLT, because
it is identical to the baseline.
</bodyText>
<table confidence="0.9299984">
MUC6
after clustering before clustering
e % R % P % F % e % R % P % F %
BASE 1.50 59.2 56.2 57.7 1.18 38.0 85.6 52.6
SLT 1.28 49.8 67.3 57.2 1.35 26.4 84.3 40.2
MPQA
after clustering before clustering
e % R % P % F % e % R % P % F %
BASE 9.83 75.8 57.0 65.1 7.05 52.1 83.4 64.1
SLT 6.39 62.1 80.6 70.2 7.39 43.7 90.1 58.9
</table>
<tableCaption confidence="0.999456">
Table 1: Performance of Structured Local Training: SLT re-
</tableCaption>
<bodyText confidence="0.997696708333334">
duces error rate (e %) after applying single-link clustering.
incoherent decisions before clustering than the pair-
wise accuracy of the coherent decisions after cluster-
ing. We also hypothesize that by performing struc-
tured local training (SLT), maximizing the likeli-
hood will correlate more with the pairwise accuracy
after clustering.
Results: Experimental results are shown in Ta-
ble 1. We report error rate (error rate = 100
accuracy) on the pairwise decisions (e %), and F1-
score (F %) on the coreferent pairs.11 For compar-
ison, we show numbers from both after and before
single-link clustering is applied. As hypothesized,
the error rate of BASE increases after clustering,
while the error rate of SLT decreases after cluster-
ing. Moreover, the error rate of SLT is considerably
lower than that of BASE after clustering. However,
the F1-score does not correlate with the error rate.
That is, a lower error rate does not always lead to a
higher F1-score, which motivates the Biased Poten-
tial Functions that we introduce in the next section.
Notice that when we compare the precision/recall
breakdown after clustering, SLT has higher precision
and lower recall than BASE.
</bodyText>
<sectionHeader confidence="0.970715" genericHeader="method">
4 Biased Potential Functions
</sectionHeader>
<bodyText confidence="0.99717325">
We introduce biased potential functions for train-
ing CRFs to empirically favor preferred evaluation
measures for the learning task, such as F-score and
MUC-score that have been considered hard for tradi-
</bodyText>
<page confidence="0.998704">
11
</page>
<bodyText confidence="0.9995885">
Error rate and F1-score on the coreferent pairs are not ideal
measures for the quality of clustering, however, we show them
here in order to contrast the effect of SLT. We present MUC-
scores for the same experimental settings in Table 3.
</bodyText>
<page confidence="0.995414">
69
</page>
<bodyText confidence="0.996626">
\x0ctional likelihood-based methods to optimize for. In-
tuitively, biased potential functions emphasize those
sub-components of an instance that can be of greater
importance than the rest of an instance.
</bodyText>
<subsectionHeader confidence="0.998962">
4.1 Definitions
</subsectionHeader>
<bodyText confidence="0.994008">
The conditional probability of P(y|x)12 for CRFs is
given by (Lafferty et al., 2001)
</bodyText>
<equation confidence="0.998827428571429">
P(y|x) =
\x04
i (Ci, x)
\x01
y
\x04
i (Ci, x)
</equation>
<bodyText confidence="0.997137333333333">
where (Ci, x) is a potential function defined over
each clique Ci. Potential functions are typically pa-
rameterized in an exponential form as follows.
</bodyText>
<equation confidence="0.950771">
(Ci, x) = exp
\x02
k
kfk(Ci, x)
</equation>
<bodyText confidence="0.995053833333333">
where k are the parameters and fk() are fea-
ture indicator functions. Because the Hammersley-
Clifford theorem (1971) for undirected graphical
models holds for any non-negative potential func-
tions, we propose alternative potential functions as
follows.
</bodyText>
<equation confidence="0.6756935">
(Ci, x) =
\x05
</equation>
<bodyText confidence="0.97504425">
(Ci, x) if (Ci, x) = true
(Ci, x) otherwise
where is a non-negative bias factor, and (Ci, x)
is a predicate (or an indicator function) to check cer-
tain properties on (Ci, x).13 Examples of possible
() would be whether the true assignment for Ci
in the training data contains certain class values, or
whether the current observation indexed by Ci has
particular characteristics. More specific details will
be given in 4.2.
Training and testing with biased potential func-
tions is mostly identical to the traditional log-linear
formulations by () as defined above, except for
small and straightforward modifications to the com-
putation of the likelihood and the derivative of the
likelihood.
</bodyText>
<page confidence="0.994162">
12
</page>
<bodyText confidence="0.999800666666667">
For the local model described in Section 2, y should be
replaced with h. We use y in this section however, as it is a
more conventional notation in general.
</bodyText>
<page confidence="0.998213">
13
</page>
<bodyText confidence="0.9983238">
In our problem formulation, cliques are individual nodes,
and potential functions are defined over the observations in-
dexed by the current i only: i.e. (Ci, x) = (yi, xi),
(Ci, x) = (yi, xi) and (Ci, x) = (yi, xi).
The key idea for biased potential functions is
nothing new, as it is conceptually similar to in-
stance weighting for problems with non-structured
output (e.g. Aha and Goldstone (1992), Cardie et al.
(1997)). However, biased potential functions differ
technically in that they emphasize desired subcom-
ponents without altering the i.i.d. assumption, and
still weight each instance alike. Despite the con-
ceptual simplicity, we are not aware of any previ-
ous work that explored biased potential functions for
problems with structured output.
</bodyText>
<subsectionHeader confidence="0.999165">
4.2 Applications to Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.9985685">
[Bias on Coreferent Pairs] For coreference res-
olution, pairs that are coreferent are in a minority
class14, and biased potential functions can mitigate
this skewed data problem, by amplifying the clique
potentials that correspond to coreferent pairs. We
define (yi, xi) to be true if and only if the true as-
signment for yi in the training data is coreferent.
Notice that () does not depend on what particu-
lar value yi might take, but only depends on the true
value of yi in the training data. For testing, (yi, xi)
will be always false.15
[Bias on Closer Coreferent Pairs] For corefer-
ence resolution, we hypothesize that coreferent pairs
for closer mentions have more significance, because
they tend to have clearer linguistic clues to deter-
mine coreference. We further hypothesize that by
emphasizing only close coreferent pairs, we can
have our model favor the MUC score. For this, we
define (yi, xi) to be true if and only if xi is for a
pair of mentions that are the closest coreferent pair.
</bodyText>
<sectionHeader confidence="0.998917" genericHeader="method">
5 ExperimentsII
</sectionHeader>
<bodyText confidence="0.998998666666667">
Data sets and configurations for experiments are
identical to those used in 3.
Hypothesis: We hypothesize that using biased po-
tential functions, maximizing the likelihood for
training can correlate better with F1-score or MUC-
score than the pairwise accuracy. In particular,
</bodyText>
<page confidence="0.997343">
14
</page>
<bodyText confidence="0.9977945">
Only 1.72% of the pairs are coreferent in the MUC6 data,
and about 12% are coreferent in the MPQA data.
</bodyText>
<page confidence="0.979078">
15
</page>
<bodyText confidence="0.825681333333333">
Notice that (yi, xi) changes the surface of the likelihood
for training, but does not affect the inference of finding the
argmax in our local model. That is, argmaxyi
(yi, xi) =
argmaxyi
(yi, xi) (with yi replaced with hi).
</bodyText>
<page confidence="0.816618">
70
</page>
<table confidence="0.973542958333333">
\x0cMUC6
pairwise MUC
e % R % P % F % R % P % F %
BASE 1.18 38.0 85.6 52.6 59.0 75.8 66.4
BASIC-P11.5
1.20 38.9 82.1 52.8 64.2 71.8 67.8
BASIC-P13.0
1.32 46.9 71.3 56.6 68.9 64.3 66.5
BASIC-Pa1.5
1.15 44.2 79.9 56.9 62.1 68.7 65.2
BASIC-Pa3.0
1.44 52.5 62.9 57.2 70.9 60.5 65.3
MPQA
pairwise MUC
e % R % P % F % R % P % F %
BASE 7.05 52.1 83.4 64.1 75.6 81.5 78.4
BASIC-P11.5
7.18 54.6 79.6 64.8 77.7 76.5 77.1
BASIC-P13.0
7.22 59.9 75.4 66.8 83.3 71.7 77.1
BASIC-Pa1.5
7.65 59.7 72.2 65.4 79.8 73.2 76.4
BASIC-Pa3.0
8.22 69.2 65.1 67.1 85.8 67.8 75.7
</table>
<tableCaption confidence="0.999223">
Table 2: Performance of Biased Potential Functions: pairwise
</tableCaption>
<bodyText confidence="0.983868571428571">
scores are taken before single-link-clustering is applied.
we hypothesize that biasing on every coreferent
pair will correlate more with F1-score, and bias-
ing on close coreferent pairs will correlate more
with MUC-score. In general, we expect that bias-
ing on coreferent pairs will boost recall, potentially
decreasing precision.
Results [BPF]: Experimental results for biased
potential functions, without structured local train-
ing, are shown in Table 2. BASIC-P1 denotes local
training with biased potential on the closest corefer-
ent pairs with bias factor , and BASIC-Pa denotes
local training with biased potential on the all coref-
erent pairs with bias factor , where = 1.5 or 3.0.
For brevity, we only show pairwise numbers before
applying single-link-clustering.16 As hypothesized,
biased potential functions in general boost recall at
the cost of precision. Also, for a fixed value of
, BASIC-P1 gives better MUC-F1 than BASIC-
Pa, and BASIC-Pa gives better pairwise-F1 than
BASIC-P1 for both data sets.
</bodyText>
<subsectionHeader confidence="0.774383">
Results [SLT+BPF]: Experimental results that
</subsectionHeader>
<bodyText confidence="0.983743666666667">
combine SLT and BPF are shown in Table 3. Sim-
ilarly as before, SLT-Px denotes SLT with biased
potential scheme Px, with bias factor . For brevity,
</bodyText>
<page confidence="0.984907">
16
</page>
<bodyText confidence="0.98951225">
This is because we showed in 3 that basic local training
does not correlate well with pairwise scores after clustering, and
in order to see the direct effect of biased potential functions, we
examine pairwise numbers before clustering.
</bodyText>
<table confidence="0.9813945">
MUC6
pairwise MUC
e % R % P % F % R % P % F %
BASE 1.50 59.2 56.2 57.7 59.0 75.8 66.4
SLT 1.28 49.8 67.3 57.2 56.3 77.8 65.3
SLT-P11.5
1.19 52.8 70.6 60.4 59.3 74.6 66.1
SLT-P13.0
1.42 63.5 57.9 60.6 67.5 70.7 69.1
SLT-Pa1.5
1.43 58.6 58.5 58.5* 64.0 73.6 68.5
SLT-Pa3.0
1.71 65.2 50.3 56.8 70.5 69.3 69.9*
MPQA
pairwise MUC
e % R % P % F % R % P % F %
BASE 9.83 75.8 57.0 65.1 75.6 81.5 78.4
SLT 6.39 62.1 80.6 70.2 69.1 88.2 77.5
SLT-P11.5
6.54 64.9 77.4 70.6* 72.2 84.5 77.9*
SLT-P13.0
9.09 77.2 59.6 67.3 78.4 79.5 78.9
SLT-Pa1.5
6.74 65.2 75.7 70.1 72.4 87.2 79.1
SLT-Pa3.0
14.71 78.2 43.9 56.2 80.5 73.8 77.0
</table>
<tableCaption confidence="0.998842">
Table 3: Performance of Biased Potential Functions with
</tableCaption>
<bodyText confidence="0.950275814814815">
Structured Local Training: All numbers are taken after single-
link clustering.
we only show numbers after applying single-link-
clustering. Unlike the results shown in Table 2,
for a fixed value of , SLT-P1 correlates better
with pairwise-F1, and SLT-Pa correlates better with
MUC-F1. This indicates that when biased poten-
tial functions are used in conjunction with SLT, the
effect of biased potential functions can be different
from the case without SLT. Comparing F1-scores in
Table 2 and Table 3, we see that the combination of
biased potential functions with SLT improves per-
formance in general. In particular, SLT-P13.0 and
SLT-Pa1.5 consistently improve performance over
BASE on both data sets, for both pairwise-F1 and
MUC-F1. We present performance scores for all
variations of configurations for reference, but we
also mark the particular configuration SLT-Px (by
* on F1-scores) that is chosen when selecting the
configuration based on the performance on the train-
ing data for each performance measure. To con-
clude, structured local training with biased poten-
tial functions bring a substantial improvement for
MUC-F1 score, from 66.4% to 69.9% for MUC6
data set. For pairwise-F1, the performance increase
from 57.7% to 58.5% for MUC6, and from 65.1% to
70.6% for MPQA.17
</bodyText>
<page confidence="0.992594">
17
</page>
<bodyText confidence="0.991124">
Performance on the MPQA data for MUC-F1 is slightly
decreased from 78.4% to 77.9%. Note the MUC scores for the
</bodyText>
<page confidence="0.99472">
71
</page>
<sectionHeader confidence="0.97973" genericHeader="method">
\x0c6 Related Work
</sectionHeader>
<bodyText confidence="0.99973321875">
Structured local training is motivated by recent re-
search that has shown that reducing the discrep-
ancy between the training model and testing model
can improve the performance without incurring the
heavy computational overhead of full-blown global
inference-based training. 18 (e.g. Cohen and Car-
valho (2005), Sutton and McCallum (2005a), Sutton
and McCallum (2005b)). Our work differs in that
(1) we use hidden variables to capture the interac-
tions between local inference and global inference,
(2) we present an application to coreference resolu-
tion, while previous work has shown applications for
variants of sequence tagging. McCallum and Well-
ner (2004) showed a global training approach with
CRFs for coreference resolution, but they used the
voted perceptron algorithm for training, which no
longer maximizes the likelihood. In addition, they
assume that all and only those noun phrases involved
in coreference resolution are given.
The performance of our system on MUC6 data
set is comparable to previously reported systems.
Using the same feature set, Ng and Cardie (2002)
reports 64.5% of MUC-score, while our system
achieved 69.9%. Ng and Cardie (2002) reports
70.4% of MUC-score using hand-selected features.
With an additional feature selection or feature induc-
tion step, the performance of our system might fur-
ther improve. McCallum and Wellner (2004) reports
73.42% of MUC-score on MUC6 data set, but their
experiments assumed perfect identification of all and
only those noun phrases involved in a coreference
relation, thus substantially simplifying the task.
</bodyText>
<sectionHeader confidence="0.998166" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998709666666667">
We present a novel training procedure, structured
local training, that maximizes likelihood while
exploiting the benefits of global inference during
training. This is achieved by incorporating hidden
variables to capture the interactions between local
MPQA baseline are already quite high to begin with.
</bodyText>
<page confidence="0.991887">
18
</page>
<bodyText confidence="0.911192">
The computational cost for SLT in our experiments were
about twice of the cost for the local training of the baseline. This
is the case because M-step converges very fast from the second
EM iteration, by initializing CRFs using parameters from the
previous M-step. Biased potential functions hardly adds extra
computational cost. In practice, BPFs reduce training time sub-
stantially: we observed that the higher the bias is, the quicker
CRFs converge.
inference and global inference. In addition, we
introduce biased potential functions that allow
CRFs to empirically favor performance measures
such as F1-score or MUC-score. We focused on the
application of coreference resolution in this paper,
but the key ideas of our approaches can be extended
to other applications, and other machine learning
techniques motivated by Markov networks.
Acknowledgments We thank the reviewers as well
as Eric Breck and Ves Stoyanov for their many helpful com-
ments. This work was supported by the Advanced Research and
Development Activity (ARDA), by NSF Grants BCS-0624277,
IIS-0535099, and IIS-0208028, and by gifts from Google and
the Xerox Foundation.
</bodyText>
<sectionHeader confidence="0.797893" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999590130434783">
D.W. Aha and R.L. Goldstone. 1992. Concept learning and flexible weighting. In
Proc. of the Fourteenth Annual Conference of the Cognitive Science Society.
A. Berger, S.D. Pietra, V.D. Pietra 1996. A Maximum Entropy Approach to
Natural Language Processing. In Computational Linguistics,22.
C. Cardie and N. Howe. 1997. Improving Minority Class Prediction Using Case-
Specific Feature Weights. In ICML.
W.W. Cohen and V. Carvalho. 2005. Stacked Sequential Learning. In IJCAI.
M. Collins. 2002. Discriminative Training Methods for Hidden Markov Models:
Theory and Experiments with Perceptron Algorithms. In EMNLP.
A.P. Dempster, N. M. Laird and D. B. Rubin. 1977. Maximum Likelihood from
Incomplete Data via the EM Algorithm. In Journal of the Loyal Statistical
Society, B.39.
J. Finkel, T. Grenager and C. D. Manning. 2005. Incorporating Non-local Infor-
mation Into Information Extraction Systems By Gibbs Sampling. In ACL.
T. Finley and T. Joachims. 2005. Supervised Clustering with Support Vector
Machines. In ICML.
J. Hammersley and P. Clifford. 1971. Markov fields on finite graphs and lattices.
Unpublished manuscript.
J. Lafferty, A. McCallum and F. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML.
A. McCallum and B. Wellner. 2004. Conditional Models of Identity Uncertainty
with Application to Noun Coreference. In NIPS.
MUC-6 1995. In Proc. of the Sixth Message Understanding Conference (MUC-6)
Morgan Kaufmann.
R. M. Neal and G. E. Hinton. 1998. A view of the EM algorithm that justies
incremental, sparse, and other variants. In Learning in Graphical Models,
Kluwer.
V. Ng and C. Cardie. 2002. Improving Machine Learning Approaches to Coref-
erence Resolution. In ACL.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak 2005. Learning and Inference
over Constrained Output. In IJCAI.
D. Roth and W. Yih. 2005. Integer Linear Programming Inference for Conditional
Random Fields. In ICML.
V. Stoyanov and C. Cardie. 2006. Partially Supervised Coreference Resolution
for Opinion Summarization through Structured Rule Learning. In EMNLP.
C. Sutton and A. McCallum. 2005. Fast, Piecewise Training for Discriminative
Finite-state and Parsing Models. In Technical Report IR-403, University of
Massachusetts.
C. Sutton and A. McCallum. 2005. Piecewise Training for Undirected Models.
In UAI.
B. Wellner, A. McCallum, F. Peng and M. Hay. 2004. An Integrated, Conditional
Model of Information Extraction and Coreference with Application to Citation
Matching. In UAI.
J. Wiebe and T. Wilson and C. Cardie 2005. Annotating Expressions of Opinions
and Emotions in Language. In Language Resources and Evaluation, volume
39, issue 2-3.
</reference>
<page confidence="0.936282">
72
</page>
<figure confidence="0.280795">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.732114">
<note confidence="0.922984666666667">b&apos;Proceedings of NAACL HLT 2007, pages 6572, Rochester, NY, April 2007. c 2007 Association for Computational Linguistics</note>
<title confidence="0.944532">Structured Local Training and Biased Potential Functions for Conditional Random Fields with Application to Coreference Resolution</title>
<author confidence="0.997775">Yejin Choi</author>
<author confidence="0.997775">Claire Cardie</author>
<affiliation confidence="0.9997355">Department of Computer Science Cornell University</affiliation>
<address confidence="0.997758">Ithaca, NY 14853</address>
<email confidence="0.999563">ychoi@cs.cornell.edu</email>
<email confidence="0.999563">cardie@cs.cornell.edu</email>
<abstract confidence="0.999730142857143">Conditional Random Fields (CRFs) have shown great success for problems involving structured output variables. However, for many real-world NLP applications, exact maximum-likelihood training is intractable because computing the global normalization factor even approximately can be extremely hard. In addition, optimizing likelihood often does not correlate with maximizing task-specific evaluation measures. In this paper, we present a novel training procedure, structured local training, that maximizes likelihood while exploiting the benefits of global inference during training: hidden variables are used to capture interactions between local inference and global inference. Furthermore, we introduce biased potential functions that empirically drive CRFs towards performance improvements w.r.t. the preferred evaluation measure for the learning task. We report promising experimental results on two coreference data sets using two task-specific evaluation measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>R L Goldstone</author>
</authors>
<title>b&apos;Proceedings of NAACL HLT</title>
<date>2007</date>
<journal>Aha</journal>
<booktitle>In Proc. of the Fourteenth Annual Conference of the Cognitive Science Society.</booktitle>
<pages>6572</pages>
<location>Rochester, NY,</location>
<marker>Goldstone, 2007</marker>
<rawString>b&apos;Proceedings of NAACL HLT 2007, pages 6572, Rochester, NY, April 2007. c 2007 Association for Computational Linguistics Structured Local Training and Biased Potential Functions for Conditional Random Fields with Application to Coreference Resolution D.W. Aha and R.L. Goldstone. 1992. Concept learning and flexible weighting. In Proc. of the Fourteenth Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S D Pietra</author>
<author>V D Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<booktitle>In Computational Linguistics,22.</booktitle>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S.D. Pietra, V.D. Pietra 1996. A Maximum Entropy Approach to Natural Language Processing. In Computational Linguistics,22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>N Howe</author>
</authors>
<title>Improving Minority Class Prediction Using CaseSpecific Feature Weights.</title>
<date>1997</date>
<booktitle>In ICML.</booktitle>
<marker>Cardie, Howe, 1997</marker>
<rawString>C. Cardie and N. Howe. 1997. Improving Minority Class Prediction Using CaseSpecific Feature Weights. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
<author>V Carvalho</author>
</authors>
<title>Stacked Sequential Learning.</title>
<date>2005</date>
<booktitle>In IJCAI.</booktitle>
<marker>Cohen, Carvalho, 2005</marker>
<rawString>W.W. Cohen and V. Carvalho. 2005. Stacked Sequential Learning. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood from Incomplete Data via the EM Algorithm.</title>
<date>1977</date>
<journal>In Journal of the Loyal Statistical Society, B.39.</journal>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A.P. Dempster, N. M. Laird and D. B. Rubin. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. In Journal of the Loyal Statistical Society, B.39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>T Grenager</author>
<author>C D Manning</author>
</authors>
<title>Incorporating Non-local Information Into Information Extraction Systems By Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. Finkel, T. Grenager and C. D. Manning. 2005. Incorporating Non-local Information Into Information Extraction Systems By Gibbs Sampling. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Finley</author>
<author>T Joachims</author>
</authors>
<title>Supervised Clustering with Support Vector Machines.</title>
<date>2005</date>
<booktitle>In ICML.</booktitle>
<marker>Finley, Joachims, 2005</marker>
<rawString>T. Finley and T. Joachims. 2005. Supervised Clustering with Support Vector Machines. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hammersley</author>
<author>P Clifford</author>
</authors>
<title>Markov fields on finite graphs and lattices.</title>
<date>1971</date>
<note>Unpublished manuscript.</note>
<marker>Hammersley, Clifford, 1971</marker>
<rawString>J. Hammersley and P. Clifford. 1971. Markov fields on finite graphs and lattices. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Conditional Models of Identity Uncertainty with Application to Noun Coreference.</title>
<date>2004</date>
<booktitle>In NIPS.</booktitle>
<marker>McCallum, Wellner, 2004</marker>
<rawString>A. McCallum and B. Wellner. 2004. Conditional Models of Identity Uncertainty with Application to Noun Coreference. In NIPS.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>In Proc. of the Sixth Message Understanding Conference (MUC-6)</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<marker>1995</marker>
<rawString>MUC-6 1995. In Proc. of the Sixth Message Understanding Conference (MUC-6) Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Neal</author>
<author>G E Hinton</author>
</authors>
<title>A view of the EM algorithm that justies incremental, sparse, and other variants.</title>
<date>1998</date>
<booktitle>In Learning in Graphical Models,</booktitle>
<publisher>Kluwer.</publisher>
<marker>Neal, Hinton, 1998</marker>
<rawString>R. M. Neal and G. E. Hinton. 1998. A view of the EM algorithm that justies incremental, sparse, and other variants. In Learning in Graphical Models, Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving Machine Learning Approaches to Coreference Resolution.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving Machine Learning Approaches to Coreference Resolution. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
<author>D Zimak</author>
</authors>
<title>Learning and Inference over Constrained Output. In IJCAI.</title>
<date>2005</date>
<marker>Punyakanok, Roth, Yih, Zimak, 2005</marker>
<rawString>V. Punyakanok, D. Roth, W. Yih, and D. Zimak 2005. Learning and Inference over Constrained Output. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Integer Linear Programming Inference for Conditional Random Fields.</title>
<date>2005</date>
<booktitle>In ICML.</booktitle>
<marker>Roth, Yih, 2005</marker>
<rawString>D. Roth and W. Yih. 2005. Integer Linear Programming Inference for Conditional Random Fields. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>C Cardie</author>
</authors>
<title>Partially Supervised Coreference Resolution for Opinion Summarization through Structured Rule Learning.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<marker>Stoyanov, Cardie, 2006</marker>
<rawString>V. Stoyanov and C. Cardie. 2006. Partially Supervised Coreference Resolution for Opinion Summarization through Structured Rule Learning. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
</authors>
<title>Fast, Piecewise Training for Discriminative Finite-state and Parsing Models. In</title>
<date>2005</date>
<tech>Technical Report IR-403,</tech>
<institution>University of Massachusetts.</institution>
<marker>Sutton, McCallum, 2005</marker>
<rawString>C. Sutton and A. McCallum. 2005. Fast, Piecewise Training for Discriminative Finite-state and Parsing Models. In Technical Report IR-403, University of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
</authors>
<title>Piecewise Training for Undirected Models. In UAI.</title>
<date>2005</date>
<marker>Sutton, McCallum, 2005</marker>
<rawString>C. Sutton and A. McCallum. 2005. Piecewise Training for Undirected Models. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wellner</author>
<author>A McCallum</author>
<author>F Peng</author>
<author>M Hay</author>
</authors>
<title>An Integrated, Conditional Model of Information Extraction and Coreference with Application to Citation Matching.</title>
<date>2004</date>
<booktitle>In UAI.</booktitle>
<marker>Wellner, McCallum, Peng, Hay, 2004</marker>
<rawString>B. Wellner, A. McCallum, F. Peng and M. Hay. 2004. An Integrated, Conditional Model of Information Extraction and Coreference with Application to Citation Matching. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<date>2005</date>
<booktitle>Annotating Expressions of Opinions and Emotions in Language. In Language Resources and Evaluation,</booktitle>
<volume>39</volume>
<pages>2--3</pages>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>J. Wiebe and T. Wilson and C. Cardie 2005. Annotating Expressions of Opinions and Emotions in Language. In Language Resources and Evaluation, volume 39, issue 2-3.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>