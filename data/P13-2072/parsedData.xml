<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<note confidence="0.314349">
b&amp;apos;Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 406411,
</note>
<address confidence="0.499042">
Sofia, Bulgaria, August 4-9 2013. c
</address>
<title confidence="0.619502333333333">
2013 Association for Computational Linguistics
Learning Non-linear Features for Machine Translation Using Gradient
Boosting Machines
</title>
<author confidence="0.731953">
Kristina Toutanova
</author>
<affiliation confidence="0.822179">
Microsoft Research
</affiliation>
<address confidence="0.975762">
Redmond, WA 98502
</address>
<email confidence="0.988842">
kristout@microsoft.com
</email>
<author confidence="0.941192">
Byung-Gyu Ahn
</author>
<affiliation confidence="0.987126">
Johns Hopkins University
</affiliation>
<address confidence="0.987762">
Baltimore, MD 21218
</address>
<email confidence="0.997292">
bahn@cs.jhu.edu
</email>
<sectionHeader confidence="0.988713" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994959833333333">
In this paper we show how to auto-
matically induce non-linear features for
machine translation. The new features
are selected to approximately maximize
a BLEU-related objective and decompose
on the level of local phrases, which guar-
antees that the asymptotic complexity of
machine translation decoding does not in-
crease. We achieve this by applying gra-
dient boosting machines (Friedman, 2000)
to learn new weak learners (features) in the
form of regression trees, using a differen-
tiable loss function related to BLEU. Our
results indicate that small gains in perfor-
mance can be achieved using this method
but we do not see the dramatic gains ob-
served using feature induction for other
important machine learning tasks.
</bodyText>
<sectionHeader confidence="0.998326" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999206150943396">
The linear model for machine translation (Och and
Ney, 2002) has become the de-facto standard in
the field. Recently, researchers have proposed a
large number of additional features (TaroWatan-
abe et al., 2007; Chiang et al., 2009) and param-
eter tuning methods (Chiang et al., 2008b; Hop-
kins and May, 2011; Cherry and Foster, 2012)
which are better able to scale to the larger pa-
rameter space. However, a significant feature en-
gineering effort is still required from practition-
ers. When a linear model does not fit well, re-
searchers are careful to manually add important
feature conjunctions, as for example, (Daume III
and Jagarlamudi, 2011; Clark et al., 2012). In the
related field of web search ranking, automatically
learned non-linear features have brought dramatic
improvements in quality (Burges et al., 2005; Wu
This research was conducted during the authors intern-
ship at Microsoft Research
et al., 2010). Here we adapt the main insights of
such work to the machine translation setting and
share results on two language pairs.
Some recent works have attempted to relax the
linearity assumption on MT features (Nguyen et
al., 2007), by defining non-parametric models on
complete translation hypotheses, for use in an n-
best re-ranking setting. In this paper we develop
a framework for inducing non-linear features in
the form of regression decision trees, which de-
compose locally and can be integrated efficiently
in decoding. The regression trees encode non-
linear feature combinations of the original fea-
tures. We build on the work by Friedman (2000)
which shows how to induce features to minimize
any differentiable loss function. In our applica-
tion the features are regression decision trees, and
the loss function is the pairwise ranking log-loss
from the PRO method for parameter tuning (Hop-
kins and May, 2011). Additionally, we show how
to design the learning process such that the in-
duced features are local on phrase-pairs and their
language model and reordering context, and thus
can be incorporated in decoding efficiently.
Our results using re-ranking on two language
pairs show that the feature induction approach can
bring small gains in performance. Overall, even
though the method shows some promise, we do
not see the dramatic gains that have been seen for
the web search ranking task (Wu et al., 2010). Fur-
ther improvements in the original feature set and
the induction algorithm, as well as full integration
in decoding are needed to potentially result in sub-
stantial performance improvements.
</bodyText>
<sectionHeader confidence="0.959781" genericHeader="method">
2 Feature learning using gradient
</sectionHeader>
<bodyText confidence="0.96936">
boosting machines
In the linear model for machine translation, the
scores of translation hypotheses are weighted
sums of a set of input features over the hypotheses.
</bodyText>
<page confidence="0.999818">
406
</page>
<figureCaption confidence="0.885358">
\x0cFigure 1: A Bulgarian source sentence (meaning the
</figureCaption>
<bodyText confidence="0.976415731707317">
conference in Bulgaria, together with a candidate transla-
tion. Local and global features for the translation hypoth-
esis are shown. f0=smoothed relative frequency estimate
of log p(s|t); f1=lexical weighting estimate of log p(s|t);
f2=joint count of the phrase-pair; f3=sum of language model
log-probabilities of target phrase words given context.
For a set of features f1(h), . . . , fL(h) and weights
for these features 1, . . . , L, the hypothesis
scores are defined as: F(h) =
l=1...L lfl(h).
In current state-of-the-art models, the features
fl(h) decompose locally on phrase-pairs (with
language model and reordering context) inside the
hypotheses. This enables hypothesis recombina-
tion during machine translation decoding, leading
to faster and more accurate search. As an exam-
ple, Figure 1 shows a Bulgarian source sentence
(spelled phonetically in Latin script) and a can-
didate translation. Two phrase-pairs are used to
compose the translation, and each phrase-pair has
a set of local feature function values. A mini-
mal set of four features is shown, for simplicity.
We can see that the hypothesis-level (global) fea-
ture values are sums of phrase-level (local) feature
values. The score of a translation given feature
weights can be computed either by scoring the
phrase-pairs and adding the scores, or by scoring
the complete hypothesis by computing its global
feature values. The local feature values do look at
some limited context outside of a phrase-pair, to
compute language model scores and re-ordering
scores; therefore we say that the features are de-
fined on phrase-pairs in context.
We start with such a state-of-the-art linear
model with decomposable features and show how
we can automatically induce additional features.
The new features are also locally decomposable,
so that the scores of hypotheses can be computed
as sums of phrase-level scores. The new local
phrase-level features are non-linear combinations
of the original phrase-level features.
</bodyText>
<figureCaption confidence="0.70485">
Figure 2: Example of two decision tree features. The left
decision tree has linear nodes and the right decision tree has
constant nodes.
</figureCaption>
<subsectionHeader confidence="0.967958">
2.1 Form of induced features
</subsectionHeader>
<bodyText confidence="0.99538275">
We will use the example in Figure 1 to introduce
the form of the new features we induce and to give
an intuition of why such features might be useful.
The new features are expressed by regression de-
cision trees; Figure 2 shows two examples.
One intuition we might have is that, if a phrase
pair has been seen very few times in the training
corpus (for example, the first phrase pair P1 in the
Figure has been seen only one time f2 = 1), we
would like to trust its lexical weighting channel
model score f1 more than its smoothed relative-
frequency channel estimate f0. The first regres-
sion tree feature h1 in Figure 2 captures this in-
tuition. The feature value for a phrase-pair of
this feature is computed as follows: if f2
2, then h1(f0, f1, f2, f3) = 2 f1; otherwise,
h1(f0, f1, f2, f3) = f1. The effect of this new
feature h1 is to boost the importance of the lexi-
cal weighting score for phrase-pairs of low joint
count. More generally, the regression tree fea-
tures we consider have either linear or constant
leaf nodes, and have up to 8 leaves. Deeper trees
can capture more complex conditions on several
input feature values. Each non-leaf node performs
a comparison of some input feature value to a
threshold and each leaf node (for linear nodes) re-
turns the value of some input feature multiplied
by some factor. For a given regression tree with
linear nodes, all leaf nodes are expressions of the
same input feature but have different coefficients
for it (for example, both leaf nodes of h1 return
affine functions of the input feature f1). A deci-
sion tree feature with constant-valued leaf nodes
is illustrated by the right-hand-side tree in Figure
2. For these decision trees, the leaf nodes contain
a constant, which is specific to each leaf. These
kinds of trees can effectively perform conjunctions
of several binary-valued input feature functions; or
they can achieve binning of real-values features to-
gether with conjunctions over binned values.
</bodyText>
<page confidence="0.994649">
407
</page>
<bodyText confidence="0.998849875">
\x0cHaving introduced the form of the new features
we learn, we now turn to the methodology for in-
ducing them. We apply the framework of gradient
boosting for decision tree weak learners (Fried-
man, 2000). To define the framework, we need
to introduce the original input features, the differ-
entiable loss function, and the details of the tree
growing algorithm. We discuss these in turn next.
</bodyText>
<subsectionHeader confidence="0.99417">
2.2 Initial features
</subsectionHeader>
<bodyText confidence="0.99969125">
Our baseline MT system uses relative frequency
and lexical weighting channel model weights, one
or more language models, distortion penalty, word
count, phrase count, and multiple lexicalized re-
ordering weights, one for each distortion type. We
have around 15 features in this base feature set.
We further expand the input set of features to in-
crease the possibility that useful feature combi-
nations could be found by our feature induction
method. The large feature set contains around
190 features, including source and target word
count features, joint phrase count, lexical weight-
ing scores according to alternative word-alignment
model ran over morphemes instead of words, in-
dicator lexicalized features for insertion and dele-
tion of the top 15 words in each language, cluster-
based insertion and deletion indicators using hard
word clustering, and cluster based signatures of
phrase-pairs. This is the feature set we use as a
basis for weak learner induction.
</bodyText>
<subsectionHeader confidence="0.997622">
2.3 Loss function
</subsectionHeader>
<bodyText confidence="0.99835">
We use a pair-wise ranking log-loss as in the
PRO parameter tuning method (Hopkins and May,
2011). The loss is defined by comparing the model
scores of pairs of hypotheses hi and hj where
the BLEU score of the first hypothesis is greater
than the BLEU score of the second hypothesis by
a specified threshold. 1
We denote the sentences in a corpus as
s1, s2, . . . , sN . For each sentence sn, we de-
note the ordered selected pairs of hypotheses as
</bodyText>
<equation confidence="0.97684475">
[hn
i1
, hn
j1
], . . . , [hn
iK
, hn
jK
</equation>
<bodyText confidence="0.636265">
]. The loss-function is
defined in terms of the hypothesis model scores
</bodyText>
<page confidence="0.742706">
1
</page>
<bodyText confidence="0.946021">
In our implementation, for each sentence, we sample
10, 000 pairs of translations and accept a pair of transla-
tions for use with probability proportional to the BLEU score
difference, if that difference is greater than the threshold of
0.04. The top K = 100 or K = 300 hypothesis pairs with
the largest BLEU difference are selected for computation of
the loss. We compute sentence-level BLEUscores by add-
smoothing of the match counts for computation of n-gram
precision. The and K parameters are chosen via cross-
validation.
</bodyText>
<equation confidence="0.8979933">
1: F0(x) = arg min (F(x, ))
2: for m = 1toM do
3: yr = [(F(x))
F(xr) ]F(x)=Fm1(x), r =
1 . . . R
4: m = arg min,
R
r=1[yr h(xi; )]2
5: m = arg min (Fm1(x) + h(x; m)
6: Fm(x) = Fm1(x) + mh(x; m)
</equation>
<figureCaption confidence="0.625239">
7: end for
Figure 3: A gradient boosting algorithm for local
</figureCaption>
<bodyText confidence="0.5834865">
feature functions.
F(h) as follows:
</bodyText>
<equation confidence="0.958527444444444">
n=1...N
k=1...K log(1 +
e
F(hn
jk
)F(hn
ik
)
).
</equation>
<bodyText confidence="0.994471763157895">
The idea of the gradient boosting method is to
induce additional features by computing a func-
tional gradient of the target loss function and itera-
tively selecting the next weak learner (feature) that
is most parallel to the negative gradient. Since we
want to induce features such that the hypothesis
scores decompose locally, we need to formulate
our loss function as a function of local phrase-pair
in context scores. Having the model scores de-
compose locally means that the scores of hypothe-
ses F(h) decompose as F(h) =
prh F(pr)),
where by pr h we denote the enumeration over
phrase pairs in context that are parts of h. If xr de-
notes the input feature vector for a phrase-pair in
context pr, the score of this phrase-pair can be ex-
pressed as F(xr). Appendix A expresses the pair-
wise log-loss as a function of the phrase scores.
We are now ready to introduce the gradient
boosting algorithm, summarized in Figure 3. In
the first step of the algorithm, we start by set-
ting the phrase-pair in context scoring function
F0(x) as a linear function of the input feature val-
ues, by selecting the feature weights to min-
imize the PRO loss (F0(x)) as a function of
. The initial scores have the form F0(x) =
l=1...L lfl(x).This is equivalent to using the
(Hopkins and May, 2011) method of parameter
tuning for a fixed input feature set and a linear
model. We used LBFGS for the optimization in
Line 1. Then we iterate and induce a new de-
cision tree weak learner h(x; m) like the exam-
ples in Figure 2 at each iteration. The parame-
ter vectors m encode the topology and parame-
ters of the decision trees, including which feature
value is tested at each node, what the compari-
son cutoffs are, and the way to compute the val-
ues at the leaf nodes. After a new decision tree
</bodyText>
<page confidence="0.997933">
408
</page>
<table confidence="0.991698333333333">
\x0cLanguage Train Dev-Train Dev-Select Test
Chs-En 999K NIST02+03 2K NIST05
Fin-En 2.2M 12K 2K 4.8K
</table>
<tableCaption confidence="0.968108">
Table 1: Data sets for the two language pairs Chinese-
</tableCaption>
<table confidence="0.95447325">
English and Finnish-English.
Chs-En Fin-En
Features Tune Dev-Train Test Dev-Train Test
base MERT 31.3 30.76 49.8 51.31
base PRO 31.1 31.16 49.7 51.56
large PRO 31.8 31.44 49.8 51.77
boost-global PRO 31.8 31.30 50.0 51.87
boost-local PRO 31.8 31.44 50.1 51.95
</table>
<tableCaption confidence="0.977162">
Table 2: Results for the two language pairs using different
</tableCaption>
<bodyText confidence="0.958960777777778">
weight tuning methods and feature sets.
h(x; m) is induced, it is treated as new feature
and a linear coefficient m for that feature is set
by minimizing the loss as a function of this pa-
rameter (Line 5). The new model scores are set as
the old model scores plus a weighted contribution
from the new feature (Line 6). At the end of learn-
ing, we have a linear model over the input features
and additional decision tree features. FM (x) =
</bodyText>
<equation confidence="0.98893">
l=1...L lfl(x) +
m=1...M mh(x; m). The
</equation>
<bodyText confidence="0.998997631578947">
most time-intensive step of the algorithm is the se-
lection of the next decision tree h. This is done
by first computing the functional gradient of the
loss with respect to the phrase scores F(xr) at the
point of the current model scores Fm1(xr). Ap-
pendix A shows a derivation of this gradient. We
then induce a regression tree using mean-square-
error minimization, setting the direction given by
the negative gradient as a target to be predicted us-
ing the features of each phrase-pair in context in-
stance. This is shown as the setting of the m pa-
rameters by mean-squared-error minimization in
Line 4 of the algorithm. The minimization is done
approximately by a standard greedy tree-growing
algorithm (Breiman et al., 1984). When we tune
weights to minimize the loss, such as the weights
of the initial features, or the weights m of in-
duced learners, we also include an L2 penalty on
the parameters, to prevent overfitting.
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999073389830508">
We report experimental results on two language
pairs: Chinese-English, and Finnish-English. Ta-
ble 1 summarizes statistics about the data. For
each language pair, we used a training set (Train)
for extracting phrase tables and language models,
a Dev-Train set for tuning feature weights and in-
ducing features, a Dev-Select set for selecting hy-
perparameters of PRO tuning and selecting a stop-
ping point and other hyperparameters of the boost-
ing method, and a Test set for reporting final re-
sults. For Chinese-English, the training corpus
consists of approximately one million sentence
pairs from the FBIS and HongKong portions of
the LDC data for the NIST MT evaluation and the
Dev-Train and Test sets are from NIST competi-
tions. The MT system is a phrasal system with a 4-
gram language model, trained on the Xinhua por-
tion of the English Gigaword corpus. The phrase
table has maximum phrase length of 7 words on
either side. For Finnish-English we used a data-
set from a technical domain of software manuals.
For this language pair we used two language mod-
els: one very large model trained on billions of
words, and another language model trained from
the target side of the parallel training set. We re-
port performance using the BLEU-SBP metric pro-
posed in (Chiang et al., 2008a). This is a vari-
ant of BLEU (Papineni et al., 2002) with strict
brevity penalty, where a long translation for one
sentence can not be used to counteract the brevity
penalty for another sentence with a short transla-
tion. Chiang et al. (2008a) showed that this metric
overcomes several undesirable properties of BLEU
and has better correlation with human judgements.
In our experiments with different feature sets and
hyperparameters we observed more stable results
and better correlation of Dev-Train, Dev-Select,
and Test results using BLEU-SBP. For our exper-
iments, we first trained weights for the base fea-
ture sets described in Section 2.2 using MERT. We
then decoded the Dev-Train, Dev-Select, and Test
datasets, generating 500-best lists for each set. All
results in Table 2 report performance of re-ranking
on these 500-best lists using different feature sets
and parameter tuning methods.
The baseline (base feature set) performance us-
ing MERT and PRO tuning on the two language
pairs is shown on the first two lines. In line with
prior work, PRO tuning achieves a bit lower scores
on the tuning set but higher scores on the test set,
compared to MERT. The large feature set addi-
tionally contains over 170 manually specified fea-
tures, described in Section 2.2. It was infeasible
to run MERT training on this feature set. The test
set results using PRO tuning for the large set are
about a quarter of a BLEU-SBP point higher than
the results using the base feature set on both lan-
guage pairs. Finally, the last two rows show the
performance of the gradient boosting method. In
</bodyText>
<page confidence="0.993652">
409
</page>
<bodyText confidence="0.999157740740741">
\x0caddition to learning locally decomposable features
boost-local, we also implemented boost-global,
where we are learning combinations of the global
feature values and lose decomposability. The fea-
tures learned by boost-global can not be com-
puted exactly on partial hypotheses in decoding
and thus this method has a speed disadvantage, but
we wanted to compare the performance of boost-
local and boost-global on n-best list re-ranking
to see the potential accuracy gain of the two meth-
ods. We see that boost-local is slightly better in
performance, in addition to being amenable to ef-
ficient decoder integration.
The gradient boosting results are mixed; for
Finnish-English, we see around .2 gain of the
boost-local model over the large feature set.
There is no improvement on Chinese-English, and
the boost-global method brings slight degrada-
tion. We did not see a large difference in perfor-
mance among models using different decision tree
leaf node types and different maximum numbers
of leaf nodes. The selected boost-local model
for FIN-ENU used trees with maximum of 2 leaf
nodes and linear leaf values; 25 new features were
induced before performance started to degrade
on the Dev-Select set. The induced features for
Finnish included combinations of language model
and channel model scores, combinations of word
count and channel model scores, and combina-
tions of channel and lexicalized reordering scores.
For example, one feature increases the contribu-
tion of the relative frequency channel score for
phrases with many target words, and decreases the
channel model contribution for shorter phrases.
The best boost-local model for Chs-Enu used
trees with a maximum of 2 constant-values leaf
nodes, and induced 24 new tree features. The fea-
tures effectively promoted and demoted phrase-
pairs in context based on whether an input fea-
tures value was smaller than a determined cutoff.
In conclusion, we proposed a new method to
induce feature combinations for machine transla-
tion, which do not increase the decoding complex-
ity. There were small improvements on one lan-
guage pair in a re-ranking setting. Further im-
provements in the original feature set and the in-
duction algorithm, as well as full integration in de-
coding are needed to result in substantial perfor-
mance improvements.
This work did not consider alternative ways
of generating non-linear features, such as taking
products of two or more input features. It would
be interesting to compare such alternatives to the
regression tree features we explored.
</bodyText>
<sectionHeader confidence="0.984931" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.963729931818182">
Leo Breiman, Jerome Friedman, Charles J. Stone, and
R.A. Olshen. 1984. Classification and Regression
Trees. Chapman and Hall.
Chris Burges, Tal Shaked, Erin Renshaw, Matt Deeds,
Nicole Hamilton, and Greg Hullender. 2005. Learn-
ing to rank using gradient descent. In ICML.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In HLT-
NAACL.
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008a. Decomposability of trans-
lation metrics for improved evaluation and efficient
algorithms. In EMNLP.
David Chiang, Yuval Marton, and Philp Resnik. 2008b.
Online large margin training of syntactic and struc-
tural translation features. In EMNLP.
D. Chiang, W. Wang, and K. Knight. 2009. 11,001
new features for statistical machine translation. In
NAACL.
Jonathan Clark, Alon Lavie, and Chris Dyer. 2012.
One system, many domains: Open-domain statisti-
cal machine translation via feature augmentation. In
AMTA.
Hal Daume III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In ACL.
Jerome H. Friedman. 2000. Greedy function approx-
imation: A gradient boosting machine. Annals of
Statistics, 29:11891232.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In EMNLP.
Patrick Nguyen, Milind Mahajan, and Xiaodong He.
2007. Training non-parametric features for statis-
tical machine translation. In Second Workshop on
Statistical Machine Translation.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL.
TaroWatanabe, Jun Suzuki, Hajime Tsukuda, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In EMNLP.
</reference>
<page confidence="0.925287">
410
</page>
<reference confidence="0.9094785">
\x0cQiang Wu, Christopher J. Burges, Krysta M. Svore, and
Jianfeng Gao. 2010. Adapting boosting for infor-
mation retrieval measures. Information Retrieval,
13(3), June.
</reference>
<sectionHeader confidence="0.968014" genericHeader="method">
4 Appendix A: Derivation of derivatives
</sectionHeader>
<bodyText confidence="0.9748281">
Here we express the loss as a function of phrase-
level in context scores and derive the derivative of
the loss with respect to these scores.
Let us number all phrase-pairs in context in
all hypotheses in all sentences as p1, . . . , pR and
denote their input feature vectors as x1, . . . , xR.
We will use F(pr) and F(xr) interchange-
ably, because the score of a phrase-pair in
context is defined by its input feature vec-
tor. The loss (F(xr)) is expressed as follows:
</bodyText>
<figure confidence="0.904166636363636">
N
n=1
K
k=1 log(1 + e
prhn
jk
F(xr)
prhn
ik
F(xr)
).
</figure>
<bodyText confidence="0.9988665">
Next we derive the derivatives of the loss
(F(x)) with respect to the phrase scores. Intu-
itively, we are treating the scores we want to learn
as parameters for the loss function; thus the loss
function has a huge number of parameters, one
for each instance of each phrase pair in context in
each translation. We ask the loss function if these
scores could be set in an arbitrary way, what di-
rection it would like to move them in to be mini-
mized. This is the direction given by the negative
gradient.
Each phrase-pair in context pr occurs in exactly
one hypothesis h in one sentence. It is possible
that two phrase-pairs in context share the same set
of input features, but for ease of implementation
and exposition, we treat these as different train-
ing instances. To express the gradient with respect
to F(xr) we therefore need to focus on the terms
of the loss from a single sentence and to take into
account the hypothesis pairs [hj,k, hi,k] where the
left or the right hypothesis is the hypothesis h con-
taining our focus phrase pair pr. (F(x))
</bodyText>
<figure confidence="0.983581470588235">
F(xr) is ex-
pressed as:
=
k:h=hik
e
prhn
jk
F (xr)
prhn
ik
F (xr)
1+e
prhn
jk
F (xr)
prhn
ik
F (xr)
+
k:h=hjk
e
prhn
jk
F (xr)
prhn
ik
F (xr)
1+e
prhn
jk
F (xr)
prhn
ik
F (xr)
</figure>
<bodyText confidence="0.994625222222222">
Since in the boosting step we induce a deci-
sion tree to fit the negative gradient, we can see
that the feature induction algorithm is trying to in-
crease the scores of phrases that occur in better
hypotheses (the first hypothesis in each pair), and
it increases the scores more if weaker hypotheses
have higher advantage; it is also trying to decrease
the scores of phrases in weaker hypotheses that are
currently receiving high scores.
</bodyText>
<page confidence="0.986295">
411
</page>
<figure confidence="0.250718">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.743335">
<note confidence="0.875663">b&amp;apos;Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 406411, Sofia, Bulgaria, August 4-9 2013. c 2013 Association for Computational Linguistics</note>
<title confidence="0.998575">Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines</title>
<author confidence="0.999708">Kristina Toutanova</author>
<affiliation confidence="0.998523">Microsoft Research</affiliation>
<address confidence="0.999872">Redmond, WA 98502</address>
<email confidence="0.999337">kristout@microsoft.com</email>
<author confidence="0.995727">Byung-Gyu Ahn</author>
<affiliation confidence="0.999827">Johns Hopkins University</affiliation>
<address confidence="0.999885">Baltimore, MD 21218</address>
<email confidence="0.999815">bahn@cs.jhu.edu</email>
<abstract confidence="0.999743105263158">In this paper we show how to automatically induce non-linear features for machine translation. The new features are selected to approximately maximize a BLEU-related objective and decompose on the level of local phrases, which guarantees that the asymptotic complexity of machine translation decoding does not increase. We achieve this by applying gradient boosting machines (Friedman, 2000) to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU. Our results indicate that small gains in performance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
<author>Jerome Friedman</author>
<author>Charles J Stone</author>
<author>R A Olshen</author>
</authors>
<title>Classification and Regression Trees.</title>
<date>1984</date>
<publisher>Chapman and Hall.</publisher>
<contexts>
<context position="14306" citStr="Breiman et al., 1984" startWordPosition="2405" endWordPosition="2408">by first computing the functional gradient of the loss with respect to the phrase scores F(xr) at the point of the current model scores Fm1(xr). Appendix A shows a derivation of this gradient. We then induce a regression tree using mean-squareerror minimization, setting the direction given by the negative gradient as a target to be predicted using the features of each phrase-pair in context instance. This is shown as the setting of the m parameters by mean-squared-error minimization in Line 4 of the algorithm. The minimization is done approximately by a standard greedy tree-growing algorithm (Breiman et al., 1984). When we tune weights to minimize the loss, such as the weights of the initial features, or the weights m of induced learners, we also include an L2 penalty on the parameters, to prevent overfitting. 3 Experiments We report experimental results on two language pairs: Chinese-English, and Finnish-English. Table 1 summarizes statistics about the data. For each language pair, we used a training set (Train) for extracting phrase tables and language models, a Dev-Train set for tuning feature weights and inducing features, a Dev-Select set for selecting hyperparameters of PRO tuning and selecting a</context>
</contexts>
<marker>Breiman, Friedman, Stone, Olshen, 1984</marker>
<rawString>Leo Breiman, Jerome Friedman, Charles J. Stone, and R.A. Olshen. 1984. Classification and Regression Trees. Chapman and Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Burges</author>
<author>Tal Shaked</author>
<author>Erin Renshaw</author>
<author>Matt Deeds</author>
<author>Nicole Hamilton</author>
<author>Greg Hullender</author>
</authors>
<title>Learning to rank using gradient descent.</title>
<date>2005</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1985" citStr="Burges et al., 2005" startWordPosition="298" endWordPosition="301">et al., 2007; Chiang et al., 2009) and parameter tuning methods (Chiang et al., 2008b; Hopkins and May, 2011; Cherry and Foster, 2012) which are better able to scale to the larger parameter space. However, a significant feature engineering effort is still required from practitioners. When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; Clark et al., 2012). In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (Burges et al., 2005; Wu This research was conducted during the authors internship at Microsoft Research et al., 2010). Here we adapt the main insights of such work to the machine translation setting and share results on two language pairs. Some recent works have attempted to relax the linearity assumption on MT features (Nguyen et al., 2007), by defining non-parametric models on complete translation hypotheses, for use in an nbest re-ranking setting. In this paper we develop a framework for inducing non-linear features in the form of regression decision trees, which decompose locally and can be integrated effici</context>
</contexts>
<marker>Burges, Shaked, Renshaw, Deeds, Hamilton, Hullender, 2005</marker>
<rawString>Chris Burges, Tal Shaked, Erin Renshaw, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="1500" citStr="Cherry and Foster, 2012" startWordPosition="221" endWordPosition="224">rm of regression trees, using a differentiable loss function related to BLEU. Our results indicate that small gains in performance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks. 1 Introduction The linear model for machine translation (Och and Ney, 2002) has become the de-facto standard in the field. Recently, researchers have proposed a large number of additional features (TaroWatanabe et al., 2007; Chiang et al., 2009) and parameter tuning methods (Chiang et al., 2008b; Hopkins and May, 2011; Cherry and Foster, 2012) which are better able to scale to the larger parameter space. However, a significant feature engineering effort is still required from practitioners. When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; Clark et al., 2012). In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (Burges et al., 2005; Wu This research was conducted during the authors internship at Microsoft Research et al., 2010). Here we adapt t</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Steve DeNeefe</author>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Decomposability of translation metrics for improved evaluation and efficient algorithms.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1450" citStr="Chiang et al., 2008" startWordPosition="212" endWordPosition="215"> learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU. Our results indicate that small gains in performance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks. 1 Introduction The linear model for machine translation (Och and Ney, 2002) has become the de-facto standard in the field. Recently, researchers have proposed a large number of additional features (TaroWatanabe et al., 2007; Chiang et al., 2009) and parameter tuning methods (Chiang et al., 2008b; Hopkins and May, 2011; Cherry and Foster, 2012) which are better able to scale to the larger parameter space. However, a significant feature engineering effort is still required from practitioners. When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; Clark et al., 2012). In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (Burges et al., 2005; Wu This research was conducted during the authors internship at</context>
<context position="15798" citStr="Chiang et al., 2008" startWordPosition="2660" endWordPosition="2663">valuation and the Dev-Train and Test sets are from NIST competitions. The MT system is a phrasal system with a 4- gram language model, trained on the Xinhua portion of the English Gigaword corpus. The phrase table has maximum phrase length of 7 words on either side. For Finnish-English we used a dataset from a technical domain of software manuals. For this language pair we used two language models: one very large model trained on billions of words, and another language model trained from the target side of the parallel training set. We report performance using the BLEU-SBP metric proposed in (Chiang et al., 2008a). This is a variant of BLEU (Papineni et al., 2002) with strict brevity penalty, where a long translation for one sentence can not be used to counteract the brevity penalty for another sentence with a short translation. Chiang et al. (2008a) showed that this metric overcomes several undesirable properties of BLEU and has better correlation with human judgements. In our experiments with different feature sets and hyperparameters we observed more stable results and better correlation of Dev-Train, Dev-Select, and Test results using BLEU-SBP. For our experiments, we first trained weights for th</context>
</contexts>
<marker>Chiang, DeNeefe, Chan, Ng, 2008</marker>
<rawString>David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee Tou Ng. 2008a. Decomposability of translation metrics for improved evaluation and efficient algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philp Resnik</author>
</authors>
<title>Online large margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1450" citStr="Chiang et al., 2008" startWordPosition="212" endWordPosition="215"> learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU. Our results indicate that small gains in performance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks. 1 Introduction The linear model for machine translation (Och and Ney, 2002) has become the de-facto standard in the field. Recently, researchers have proposed a large number of additional features (TaroWatanabe et al., 2007; Chiang et al., 2009) and parameter tuning methods (Chiang et al., 2008b; Hopkins and May, 2011; Cherry and Foster, 2012) which are better able to scale to the larger parameter space. However, a significant feature engineering effort is still required from practitioners. When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; Clark et al., 2012). In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (Burges et al., 2005; Wu This research was conducted during the authors internship at</context>
<context position="15798" citStr="Chiang et al., 2008" startWordPosition="2660" endWordPosition="2663">valuation and the Dev-Train and Test sets are from NIST competitions. The MT system is a phrasal system with a 4- gram language model, trained on the Xinhua portion of the English Gigaword corpus. The phrase table has maximum phrase length of 7 words on either side. For Finnish-English we used a dataset from a technical domain of software manuals. For this language pair we used two language models: one very large model trained on billions of words, and another language model trained from the target side of the parallel training set. We report performance using the BLEU-SBP metric proposed in (Chiang et al., 2008a). This is a variant of BLEU (Papineni et al., 2002) with strict brevity penalty, where a long translation for one sentence can not be used to counteract the brevity penalty for another sentence with a short translation. Chiang et al. (2008a) showed that this metric overcomes several undesirable properties of BLEU and has better correlation with human judgements. In our experiments with different feature sets and hyperparameters we observed more stable results and better correlation of Dev-Train, Dev-Select, and Test results using BLEU-SBP. For our experiments, we first trained weights for th</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philp Resnik. 2008b. Online large margin training of syntactic and structural translation features. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>W Wang</author>
<author>K Knight</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="1400" citStr="Chiang et al., 2009" startWordPosition="203" endWordPosition="206">ying gradient boosting machines (Friedman, 2000) to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU. Our results indicate that small gains in performance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks. 1 Introduction The linear model for machine translation (Och and Ney, 2002) has become the de-facto standard in the field. Recently, researchers have proposed a large number of additional features (TaroWatanabe et al., 2007; Chiang et al., 2009) and parameter tuning methods (Chiang et al., 2008b; Hopkins and May, 2011; Cherry and Foster, 2012) which are better able to scale to the larger parameter space. However, a significant feature engineering effort is still required from practitioners. When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; Clark et al., 2012). In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (Burges et al., 2005; Wu This resea</context>
</contexts>
<marker>Chiang, Wang, Knight, 2009</marker>
<rawString>D. Chiang, W. Wang, and K. Knight. 2009. 11,001 new features for statistical machine translation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Clark</author>
<author>Alon Lavie</author>
<author>Chris Dyer</author>
</authors>
<title>One system, many domains: Open-domain statistical machine translation via feature augmentation.</title>
<date>2012</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="1831" citStr="Clark et al., 2012" startWordPosition="276" endWordPosition="279">Och and Ney, 2002) has become the de-facto standard in the field. Recently, researchers have proposed a large number of additional features (TaroWatanabe et al., 2007; Chiang et al., 2009) and parameter tuning methods (Chiang et al., 2008b; Hopkins and May, 2011; Cherry and Foster, 2012) which are better able to scale to the larger parameter space. However, a significant feature engineering effort is still required from practitioners. When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; Clark et al., 2012). In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (Burges et al., 2005; Wu This research was conducted during the authors internship at Microsoft Research et al., 2010). Here we adapt the main insights of such work to the machine translation setting and share results on two language pairs. Some recent works have attempted to relax the linearity assumption on MT features (Nguyen et al., 2007), by defining non-parametric models on complete translation hypotheses, for use in an nbest re-ranking setting. In this pa</context>
</contexts>
<marker>Clark, Lavie, Dyer, 2012</marker>
<rawString>Jonathan Clark, Alon Lavie, and Chris Dyer. 2012. One system, many domains: Open-domain statistical machine translation via feature augmentation. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
<author>Jagadeesh Jagarlamudi</author>
</authors>
<title>Domain adaptation for machine translation by mining unseen words.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<marker>Daume, Jagarlamudi, 2011</marker>
<rawString>Hal Daume III and Jagadeesh Jagarlamudi. 2011. Domain adaptation for machine translation by mining unseen words. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome H Friedman</author>
</authors>
<title>Greedy function approximation: A gradient boosting machine.</title>
<date>2000</date>
<journal>Annals of Statistics,</journal>
<pages>29--11891232</pages>
<contexts>
<context position="828" citStr="Friedman, 2000" startWordPosition="111" endWordPosition="112">res for Machine Translation Using Gradient Boosting Machines Kristina Toutanova Microsoft Research Redmond, WA 98502 kristout@microsoft.com Byung-Gyu Ahn Johns Hopkins University Baltimore, MD 21218 bahn@cs.jhu.edu Abstract In this paper we show how to automatically induce non-linear features for machine translation. The new features are selected to approximately maximize a BLEU-related objective and decompose on the level of local phrases, which guarantees that the asymptotic complexity of machine translation decoding does not increase. We achieve this by applying gradient boosting machines (Friedman, 2000) to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU. Our results indicate that small gains in performance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks. 1 Introduction The linear model for machine translation (Och and Ney, 2002) has become the de-facto standard in the field. Recently, researchers have proposed a large number of additional features (TaroWatanabe et al., 2007; Chiang et al., 2009) and parameter tuning method</context>
<context position="2728" citStr="Friedman (2000)" startWordPosition="420" endWordPosition="421">hts of such work to the machine translation setting and share results on two language pairs. Some recent works have attempted to relax the linearity assumption on MT features (Nguyen et al., 2007), by defining non-parametric models on complete translation hypotheses, for use in an nbest re-ranking setting. In this paper we develop a framework for inducing non-linear features in the form of regression decision trees, which decompose locally and can be integrated efficiently in decoding. The regression trees encode nonlinear feature combinations of the original features. We build on the work by Friedman (2000) which shows how to induce features to minimize any differentiable loss function. In our application the features are regression decision trees, and the loss function is the pairwise ranking log-loss from the PRO method for parameter tuning (Hopkins and May, 2011). Additionally, we show how to design the learning process such that the induced features are local on phrase-pairs and their language model and reordering context, and thus can be incorporated in decoding efficiently. Our results using re-ranking on two language pairs show that the feature induction approach can bring small gains in </context>
<context position="8253" citStr="Friedman, 2000" startWordPosition="1328" endWordPosition="1330">cision tree feature with constant-valued leaf nodes is illustrated by the right-hand-side tree in Figure 2. For these decision trees, the leaf nodes contain a constant, which is specific to each leaf. These kinds of trees can effectively perform conjunctions of several binary-valued input feature functions; or they can achieve binning of real-values features together with conjunctions over binned values. 407 \x0cHaving introduced the form of the new features we learn, we now turn to the methodology for inducing them. We apply the framework of gradient boosting for decision tree weak learners (Friedman, 2000). To define the framework, we need to introduce the original input features, the differentiable loss function, and the details of the tree growing algorithm. We discuss these in turn next. 2.2 Initial features Our baseline MT system uses relative frequency and lexical weighting channel model weights, one or more language models, distortion penalty, word count, phrase count, and multiple lexicalized reordering weights, one for each distortion type. We have around 15 features in this base feature set. We further expand the input set of features to increase the possibility that useful feature com</context>
</contexts>
<marker>Friedman, 2000</marker>
<rawString>Jerome H. Friedman. 2000. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:11891232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1474" citStr="Hopkins and May, 2011" startWordPosition="216" endWordPosition="220">rs (features) in the form of regression trees, using a differentiable loss function related to BLEU. Our results indicate that small gains in performance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks. 1 Introduction The linear model for machine translation (Och and Ney, 2002) has become the de-facto standard in the field. Recently, researchers have proposed a large number of additional features (TaroWatanabe et al., 2007; Chiang et al., 2009) and parameter tuning methods (Chiang et al., 2008b; Hopkins and May, 2011; Cherry and Foster, 2012) which are better able to scale to the larger parameter space. However, a significant feature engineering effort is still required from practitioners. When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; Clark et al., 2012). In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (Burges et al., 2005; Wu This research was conducted during the authors internship at Microsoft Research et a</context>
<context position="2992" citStr="Hopkins and May, 2011" startWordPosition="460" endWordPosition="464">ypotheses, for use in an nbest re-ranking setting. In this paper we develop a framework for inducing non-linear features in the form of regression decision trees, which decompose locally and can be integrated efficiently in decoding. The regression trees encode nonlinear feature combinations of the original features. We build on the work by Friedman (2000) which shows how to induce features to minimize any differentiable loss function. In our application the features are regression decision trees, and the loss function is the pairwise ranking log-loss from the PRO method for parameter tuning (Hopkins and May, 2011). Additionally, we show how to design the learning process such that the induced features are local on phrase-pairs and their language model and reordering context, and thus can be incorporated in decoding efficiently. Our results using re-ranking on two language pairs show that the feature induction approach can bring small gains in performance. Overall, even though the method shows some promise, we do not see the dramatic gains that have been seen for the web search ranking task (Wu et al., 2010). Further improvements in the original feature set and the induction algorithm, as well as full i</context>
<context position="9542" citStr="Hopkins and May, 2011" startWordPosition="1532" endWordPosition="1535"> feature set contains around 190 features, including source and target word count features, joint phrase count, lexical weighting scores according to alternative word-alignment model ran over morphemes instead of words, indicator lexicalized features for insertion and deletion of the top 15 words in each language, clusterbased insertion and deletion indicators using hard word clustering, and cluster based signatures of phrase-pairs. This is the feature set we use as a basis for weak learner induction. 2.3 Loss function We use a pair-wise ranking log-loss as in the PRO parameter tuning method (Hopkins and May, 2011). The loss is defined by comparing the model scores of pairs of hypotheses hi and hj where the BLEU score of the first hypothesis is greater than the BLEU score of the second hypothesis by a specified threshold. 1 We denote the sentences in a corpus as s1, s2, . . . , sN . For each sentence sn, we denote the ordered selected pairs of hypotheses as [hn i1 , hn j1 ], . . . , [hn iK , hn jK ]. The loss-function is defined in terms of the hypothesis model scores 1 In our implementation, for each sentence, we sample 10, 000 pairs of translations and accept a pair of translations for use with probab</context>
<context position="12142" citStr="Hopkins and May, 2011" startWordPosition="2021" endWordPosition="2024">eature vector for a phrase-pair in context pr, the score of this phrase-pair can be expressed as F(xr). Appendix A expresses the pairwise log-loss as a function of the phrase scores. We are now ready to introduce the gradient boosting algorithm, summarized in Figure 3. In the first step of the algorithm, we start by setting the phrase-pair in context scoring function F0(x) as a linear function of the input feature values, by selecting the feature weights to minimize the PRO loss (F0(x)) as a function of . The initial scores have the form F0(x) = l=1...L lfl(x).This is equivalent to using the (Hopkins and May, 2011) method of parameter tuning for a fixed input feature set and a linear model. We used LBFGS for the optimization in Line 1. Then we iterate and induce a new decision tree weak learner h(x; m) like the examples in Figure 2 at each iteration. The parameter vectors m encode the topology and parameters of the decision trees, including which feature value is tested at each node, what the comparison cutoffs are, and the way to compute the values at the leaf nodes. After a new decision tree 408 \x0cLanguage Train Dev-Train Dev-Select Test Chs-En 999K NIST02+03 2K NIST05 Fin-En 2.2M 12K 2K 4.8K Table </context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Nguyen</author>
<author>Milind Mahajan</author>
<author>Xiaodong He</author>
</authors>
<title>Training non-parametric features for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Second Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="2309" citStr="Nguyen et al., 2007" startWordPosition="352" endWordPosition="355"> researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; Clark et al., 2012). In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (Burges et al., 2005; Wu This research was conducted during the authors internship at Microsoft Research et al., 2010). Here we adapt the main insights of such work to the machine translation setting and share results on two language pairs. Some recent works have attempted to relax the linearity assumption on MT features (Nguyen et al., 2007), by defining non-parametric models on complete translation hypotheses, for use in an nbest re-ranking setting. In this paper we develop a framework for inducing non-linear features in the form of regression decision trees, which decompose locally and can be integrated efficiently in decoding. The regression trees encode nonlinear feature combinations of the original features. We build on the work by Friedman (2000) which shows how to induce features to minimize any differentiable loss function. In our application the features are regression decision trees, and the loss function is the pairwis</context>
</contexts>
<marker>Nguyen, Mahajan, He, 2007</marker>
<rawString>Patrick Nguyen, Milind Mahajan, and Xiaodong He. 2007. Training non-parametric features for statistical machine translation. In Second Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1230" citStr="Och and Ney, 2002" startWordPosition="176" endWordPosition="179"> and decompose on the level of local phrases, which guarantees that the asymptotic complexity of machine translation decoding does not increase. We achieve this by applying gradient boosting machines (Friedman, 2000) to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU. Our results indicate that small gains in performance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks. 1 Introduction The linear model for machine translation (Och and Ney, 2002) has become the de-facto standard in the field. Recently, researchers have proposed a large number of additional features (TaroWatanabe et al., 2007; Chiang et al., 2009) and parameter tuning methods (Chiang et al., 2008b; Hopkins and May, 2011; Cherry and Foster, 2012) which are better able to scale to the larger parameter space. However, a significant feature engineering effort is still required from practitioners. When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; Clark et al., 2012</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="15851" citStr="Papineni et al., 2002" startWordPosition="2671" endWordPosition="2674"> NIST competitions. The MT system is a phrasal system with a 4- gram language model, trained on the Xinhua portion of the English Gigaword corpus. The phrase table has maximum phrase length of 7 words on either side. For Finnish-English we used a dataset from a technical domain of software manuals. For this language pair we used two language models: one very large model trained on billions of words, and another language model trained from the target side of the parallel training set. We report performance using the BLEU-SBP metric proposed in (Chiang et al., 2008a). This is a variant of BLEU (Papineni et al., 2002) with strict brevity penalty, where a long translation for one sentence can not be used to counteract the brevity penalty for another sentence with a short translation. Chiang et al. (2008a) showed that this metric overcomes several undesirable properties of BLEU and has better correlation with human judgements. In our experiments with different feature sets and hyperparameters we observed more stable results and better correlation of Dev-Train, Dev-Select, and Test results using BLEU-SBP. For our experiments, we first trained weights for the base feature sets described in Section 2.2 using ME</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki TaroWatanabe</author>
<author>Hajime Tsukuda</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1378" citStr="TaroWatanabe et al., 2007" startWordPosition="198" endWordPosition="202">se. We achieve this by applying gradient boosting machines (Friedman, 2000) to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU. Our results indicate that small gains in performance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks. 1 Introduction The linear model for machine translation (Och and Ney, 2002) has become the de-facto standard in the field. Recently, researchers have proposed a large number of additional features (TaroWatanabe et al., 2007; Chiang et al., 2009) and parameter tuning methods (Chiang et al., 2008b; Hopkins and May, 2011; Cherry and Foster, 2012) which are better able to scale to the larger parameter space. However, a significant feature engineering effort is still required from practitioners. When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; Clark et al., 2012). In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (Burges et al</context>
</contexts>
<marker>TaroWatanabe, Tsukuda, Isozaki, 2007</marker>
<rawString>TaroWatanabe, Jun Suzuki, Hajime Tsukuda, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cQiang Wu</author>
<author>Christopher J Burges</author>
<author>Krysta M Svore</author>
<author>Jianfeng Gao</author>
</authors>
<title>Adapting boosting for information retrieval measures.</title>
<date>2010</date>
<journal>Information Retrieval,</journal>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="3495" citStr="Wu et al., 2010" startWordPosition="544" endWordPosition="547">e loss function is the pairwise ranking log-loss from the PRO method for parameter tuning (Hopkins and May, 2011). Additionally, we show how to design the learning process such that the induced features are local on phrase-pairs and their language model and reordering context, and thus can be incorporated in decoding efficiently. Our results using re-ranking on two language pairs show that the feature induction approach can bring small gains in performance. Overall, even though the method shows some promise, we do not see the dramatic gains that have been seen for the web search ranking task (Wu et al., 2010). Further improvements in the original feature set and the induction algorithm, as well as full integration in decoding are needed to potentially result in substantial performance improvements. 2 Feature learning using gradient boosting machines In the linear model for machine translation, the scores of translation hypotheses are weighted sums of a set of input features over the hypotheses. 406 \x0cFigure 1: A Bulgarian source sentence (meaning the conference in Bulgaria, together with a candidate translation. Local and global features for the translation hypothesis are shown. f0=smoothed rela</context>
</contexts>
<marker>Wu, Burges, Svore, Gao, 2010</marker>
<rawString>\x0cQiang Wu, Christopher J. Burges, Krysta M. Svore, and Jianfeng Gao. 2010. Adapting boosting for information retrieval measures. Information Retrieval, 13(3), June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>