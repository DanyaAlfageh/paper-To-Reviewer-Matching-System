 The minimization is done approximately by a standard greedy tree-growing algorithm CITATION,,
, 2007; CITATION) and parameter tuning methods (CITATIONb; CITATION; CITATION) which are better able to scale to the larger parameter space,,
 When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; CITATION),,
 In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (CITATION; Wu This research was conducted during the authors internship at Microsoft Research et al,,
 Some recent works have attempted to relax the linearity assumption on MT features CITATION, by defining non-parametric models on complete translation hypotheses, for use in an nbest re-ranking setting,,
 1 Introduction The linear model for machine translation CITATION has become the de-facto standard in the field,,
 Recently, researchers have proposed a large number of additional features (CITATION; CITATION) and parameter tuning methods (CITATIONb; CITATION; CITATION) which are better able to scale to the larger parameter space,,
 When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; CITATION),,
 In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (CITATION; Wu This research was conducted during the authors internship at Microsoft Research et al,,
 1 Introduction The linear model for machine translation CITATION has become the de-facto standard in the field,,
 Recently, researchers have proposed a large number of additional features (CITATION; CITATION) and parameter tuning methods (CITATIONb; CITATION; CITATION) which are better able to scale to the larger parameter space,,
 When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; CITATION),,
 In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (CITATION; Wu This research was conducted during the authors internship at,,
 We report performance using the BLEU-SBP metric proposed in (CITATIONa),,
 This is a variant of BLEU CITATION with strict brevity penalty, where a long translation for one sentence can not be used to counteract the brevity penalty for another sentence with a short translation,,
 1 Introduction The linear model for machine translation CITATION has become the de-facto standard in the field,,
 Recently, researchers have proposed a large number of additional features (CITATION; CITATION) and parameter tuning methods (CITATIONb; CITATION; CITATION) which are better able to scale to the larger parameter space,,
 When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; CITATION),,
 In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (CITATION; Wu This research was conducted during the authors internship at,,
 We report performance using the BLEU-SBP metric proposed in (CITATIONa),,
 This is a variant of BLEU CITATION with strict brevity penalty, where a long translation for one sentence can not be used to counteract the brevity penalty for another sentence with a short translation,,
ying gradient boosting machines CITATION to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU,,
 1 Introduction The linear model for machine translation CITATION has become the de-facto standard in the field,,
 Recently, researchers have proposed a large number of additional features (CITATION; CITATION) and parameter tuning methods (CITATIONb; CITATION; CITATION) which are better able to scale to the larger parameter space,,
 When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; CITATION),,
 In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (CITATION; Wu This resea,,
CITATION) has become the de-facto standard in the field,,
 Recently, researchers have proposed a large number of additional features (CITATION; CITATION) and parameter tuning methods (CITATIONb; CITATION; CITATION) which are better able to scale to the larger parameter space,,
 When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; CITATION),,
 In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (CITATION; Wu This research was conducted during the authors internship at Microsoft Research et al,,
 Some recent works have attempted to relax the linearity assumption on MT features CITATION, by defining non-parametric models on complete translation hypotheses, for use in an nbest re-ranking setting,,
 We achieve this by applying gradient boosting machines CITATION to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU,,
 1 Introduction The linear model for machine translation CITATION has become the de-facto standard in the field,,
 Recently, researchers have proposed a large number of additional features (CITATION; CITATION) and parameter tuning method,,
 Some recent works have attempted to relax the linearity assumption on MT features CITATION, by defining non-parametric models on complete translation hypotheses, for use in an nbest re-ranking setting,,
 We build on the work by CITATION which shows how to induce features to minimize any differentiable loss function,,
 In our application the features are regression decision trees, and the loss function is the pairwise ranking log-loss from the PRO method for parameter tuning CITATION,,
 We apply the framework of gradient boosting for decision tree weak learners CITATION,,
 1 Introduction The linear model for machine translation CITATION has become the de-facto standard in the field,,
 Recently, researchers have proposed a large number of additional features (CITATION; CITATION) and parameter tuning methods (CITATIONb; CITATION; CITATION) which are better able to scale to the larger parameter space,,
 When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; CITATION),,
 In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (CITATION; Wu This research was conducted during the authors internship at Microsoft Research et a,,
 We build on the work by CITATION which shows how to induce features to minimize any differentiable loss function,,
 In our application the features are regression decision trees, and the loss function is the pairwise ranking log-loss from the PRO method for parameter tuning CITATION,,
 Overall, even though the method shows some promise, we do not see the dramatic gains that have been seen for the web search ranking task CITATION,,
3 Loss function We use a pair-wise ranking log-loss as in the PRO parameter tuning method CITATION,,
This is equivalent to using the CITATION method of parameter tuning for a fixed input feature set and a linear model,,
 researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; CITATION),,
 In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (CITATION; Wu This research was conducted during the authors internship at Microsoft Research et al,,
 Some recent works have attempted to relax the linearity assumption on MT features CITATION, by defining non-parametric models on complete translation hypotheses, for use in an nbest re-ranking setting,,
 We build on the work by CITATION which shows how to induce features to minimize any differentiable loss function,,
 We achieve this by applying gradient boosting machines CITATION to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU,,
 1 Introduction The linear model for machine translation CITATION has become the de-facto standard in the field,,
 Recently, researchers have proposed a large number of additional features (CITATION; CITATION) and parameter tuning methods (CITATIONb; CITATION; CITATION) which are better able to scale to the larger parameter space,,
 When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; CITATION,,
 We report performance using the BLEU-SBP metric proposed in (CITATIONa),,
 This is a variant of BLEU CITATION with strict brevity penalty, where a long translation for one sentence can not be used to counteract the brevity penalty for another sentence with a short translation,,
 We achieve this by applying gradient boosting machines CITATION to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU,,
 1 Introduction The linear model for machine translation CITATION has become the de-facto standard in the field,,
 Recently, researchers have proposed a large number of additional features (CITATION; CITATION) and parameter tuning methods (CITATIONb; CITATION; CITATION) which are better able to scale to the larger parameter space,,
 When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; CITATION),,
e loss function is the pairwise ranking log-loss from the PRO method for parameter tuning CITATION,,
 Overall, even though the method shows some promise, we do not see the dramatic gains that have been seen for the web search ranking task CITATION,,
