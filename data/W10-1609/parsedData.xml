<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.9379555">
b&apos;Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 6267, Los Angeles, California, June 2010. c
</bodyText>
<sectionHeader confidence="0.627923" genericHeader="abstract">
2010 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.9896385">
A Machine Learning Approach for
Recognizing Textual Entailment in Spanish
</title>
<author confidence="0.996865">
Julio Javier Castillo
</author>
<affiliation confidence="0.998165">
National University of Cordoba
</affiliation>
<address confidence="0.6313165">
Ciudad Universitaria, 5000
Cordoba, Argentina
</address>
<email confidence="0.991043">
jotacastillo@gmail.com
</email>
<sectionHeader confidence="0.990444" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.963140375">
This paper presents a system that uses ma-
chine learning algorithms for the task of re-
cognizing textual entailment in Spanish
language. The datasets used include SPARTE
Corpus and a translated version to Spanish of
RTE3, RTE4 and RTE5 datasets. The features
chosen quantify lexical, syntactic and seman-
tic level matching between text and hypothe-
sis sentences. We analyze how the different
sizes of datasets and classifiers could impact
on the final overall performance of the RTE
classification of two-way task in Spanish. The
RTE system yields 60.83% of accuracy and a
competitive result of 66.50% of accuracy is
reported by train and test set taken from
SPARTE Corpus with 70% split.
</bodyText>
<sectionHeader confidence="0.997411" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99720885106383">
The objective of the Recognizing Textual Entail-
ment Challenge is determining whether the mean-
ing of the Hypothesis (H) can be inferred from a
text (T) (Ido Dagan et al., 2006). This challenge
has been organized by NIST in recent years.
Another related antecedent was Answer Valida-
tion Exercise (AVE), part of Cross Language
Evaluation Forum (CLEF), whose objective is to
develop systems which are able to decide whether
the answer to a question is correct or not (Penas et
al, 2006). It was a three year-old track, from 2006
to 2008.
AVE challenge was an evaluation framework
for Question Answering (QA) systems to promote
the development and evaluation of subsystems
aimed at validating the correctness of the answers
given by a QA system. The Answer Validation
task must select the best answer for the final out-
put. There is a subtask for each language involved
in QA, the Spanish is one of these. Thus, AVE task
is very similar to RTE (Recognition of Textual
Entailments).
In this paper, we address the RTE task problem
of determining the entailment value between Text
and Hypothesis pairs in Spanish, applying machine
learning techniques.
In the past, RTEs Challenges machine learning
algorithms were widely used for the task of recog-
nizing textual entailment (Marneffe et al., 2006;
Zanzotto et al., 2007; Castillo, 2009) and they have
reported goods results for English language. Also,
our system applies machine learning algorithms to
the Spanish.
We built a set of datasets based on public avail-
able datasets for English, together to SPARTE
(Penas et al, 2006), an available Corpus in Spanish.
This corpus contains 2962 hypothesis with a doc-
ument label and a True/False value indicating
whether the document entails the hypothesis or not.
Up to our knowledge, SPARTE corpus in the only
corpus aimed at evaluating RTE systems in Span-
ish.
Finally, we generated a feature vector with the
following components for both Text and Hypothe-
sis: Levenshtein distance, a lexical distance based
on Levenshtein, a semantic similarity measure
Wordnet based, and the LCS (longest common
</bodyText>
<page confidence="0.9952">
62
</page>
<bodyText confidence="0.985408555555556">
\x0csubstring) metric; in order to characterize the rela-
tionships between the Text and the Hypothesis.
The remainder of the paper is organized as fol-
lows. Section 2 shows the system description, whe-
reas Section 3 describes the results of experimental
evaluation and discussion of them. Section 4 dis-
cusses opportunities of collaboration. Finally, Sec-
tion 5 summarizes the conclusions and lines for
future work.
</bodyText>
<sectionHeader confidence="0.975654" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.98826475">
This section provides an overview of our system
which is based on a machine learning approach for
recognizing textual entailment to the Spanish. The
system produces feature vectors for the available
development data RTE3, RTE4, RTE5, and
SPARTE(Penas et al, 2006). Weka (Witten and
Frank, 2000) is used to train classifiers on these
feature vectors.
The SPARTE Corpus, was built from the Span-
ish corpora used at Cross-Language Evaluation
Forum (CLEF) for evaluating QA systems during
the years 2003, 2004 and 2005. This corpus con-
tains 2962 hypothesis with a True/False value indi-
cating whether the document entails the hypothesis
or not.
Due to, all available dataset of PASCAL Text
Analysis Conference were in English, we trans-
lated every dataset to Spanish by using an online
translator engine1
. So, we had a Spanish dataset but
with some translation errors provided by the trans-
lator. It is important to note, that the quality of
the translation is given by the Translator engine,
and we suppose that the sense of the sentence
should not be modified by the Translator. Indeed, it
is the situation for the majority of the cases that we
analyzed. The new datasets were named RTE3-Sp
(Spanish), RTE4-Sp, and RTE5-Sp.
The following example is the pair number 799
from RTE3-Sp with False as entailment value.
Text:
Otros dos marines, Tyler Jackson y Juan Jodka
III, ya han se declaro culpables de asalto agra-
vantes y conspiracion para obstruir la justicia y
fueron condenados a 21 meses y 18 meses, respec-
tivamente.
</bodyText>
<equation confidence="0.586967">
1
http://www.microsofttranslator.com/
</equation>
<bodyText confidence="0.9638903">
Hypothesis:
Tyler Jackson ha sido condenado a 18 meses.
This example shows a little noisy (and a minimal
syntactic error) in the translation of the Text to
Spanish (instead of ya han se declaro should be
ya se han declarado); but the whole meaning was
not changed.
Also, we show a pair example (pair id=3) taken
from Sparte Corpus with False as entailment value:
Text: Cual es la capital de Croacia?
</bodyText>
<subsectionHeader confidence="0.732404">
Hypothesis :
</subsectionHeader>
<bodyText confidence="0.984214733333333">
La capital de Croacia es ONU.
In a similar way, all pairs from SPARTE belong to
QA task and these are syntactically simpler than
RTEs Corpus pairs.
Additionally, we generate the following devel-
opment sets: RTE3-Sp+RTE4-Sp, and SPARTE-
Bal+RTE3-Sp+RTE4-Sp in order to train with dif-
ferent corpus and different sizes. In all cases,
RTE5-Sp TAC 2009 gold standard dataset was
used as test-set.
Also, we did additional experiments with
SPARTE, using cross-validation technique and
percentage split method, in order to test the accu-
racy of our system taking only this corpus as de-
velopment and training set.
</bodyText>
<subsectionHeader confidence="0.580143">
2.1 Features
</subsectionHeader>
<bodyText confidence="0.985525705882353">
We experimented with the following four ma-
chine learning algorithms: Support Vector Ma-
chine (SVM), Multilayer Perceptron(MLP),
Decision Trees(DT) and AdaBoost(AB).
The Decision Trees are interesting because we
can see what features were selected from the top
levels of the trees. SVM and AdaBoost were se-
lected because they are known for achieving high
performances, and MLP was used because it has
achieved high performance in others NLP tasks.
We experimented with various settings for the
machine learning algorithms, including only the
results for the best parameters.
We generated a feature vector with the follow-
ing components for every possible &lt;T,H&amp;gt;: Le-
venshtein distance, a lexical distance based on
Levenshtein, a semantic similarity measure Word-
</bodyText>
<page confidence="0.998282">
63
</page>
<bodyText confidence="0.9985725">
\x0cnet based, and the LCS (longest common sub-
string) metric.
We chose only four features in order to learn
the development sets, having into account that
larger feature sets do not necessarily lead to im-
proving classification performance because it
could increase the risk of overfitting the training
data.
Below the motivation for the input features:
Levenshtein distance is motivated by the good re-
sults obtained as a measure of similarity between
two strings. Using stems, this measure improves
the Levenshtein over words. The lexical distance
feature based on Levenshtein distance is interesting
because works to a sentence level. Semantic simi-
larity using WordNet is interesting because of the
capture of the semantic similarity between T and H
to sentence level. Longest common substring is
selected because it is easy to implement and pro-
vides a good measure for word overlap.
</bodyText>
<subsectionHeader confidence="0.999">
2.2 Lexical Distance
</subsectionHeader>
<bodyText confidence="0.987251444444445">
The standard Levenshtein distance is a string me-
tric for measuring the amount of difference be-
tween two strings. This distance quantifies the
number of changes (character based) to generate
one text string (T) from the other (H). The algo-
rithm works independently from the language that
we are analyzing.
We used a Spanish Stemmer that stems words
in Spanish based on a modified version of the
Snowball algorithm2
.
Additionally, by using Levenshtein distance we
defined a lexical distance and the procedure is the
following:
Each string T and H are divided in a list of
tokens.
The similarity between each pair of tokens
in T and H is performed using the Le-
venshtein distance over stems.
The string similarity between two lists of
tokens is reduced to the problem of bipar-
tite graph matching, performed using the
Hungarian algorithm (Kuhn, 1955) over
this bipartite graph. Then, we found the as-
signment that maximizes the sum of rat-
ings of each token. Note that each graph
node is a token of the list.
</bodyText>
<figure confidence="0.890025466666666">
2
http://snowball.tartarus.org/
The final score is calculated by:
))
(
),
(
( H
Length
T
Length
Max
TotalSim
finalscore
Where:
</figure>
<bodyText confidence="0.6444005">
TotalSim is the sum of the similarities with
the optimal assignment in the graph.
Length (T) is the number of tokens in T.
Length (H) is the number of tokens in H.
</bodyText>
<subsectionHeader confidence="0.992784">
2.3 Wordnet Distance
</subsectionHeader>
<bodyText confidence="0.9904956">
Since, all datasets are in Spanish, we need to con-
vert &lt;T, H&amp;gt; pair to English. In the case of RTEs-
Sp datasets, this action will backward to the Eng-
lish language (source).
Our ideal case would be to use EuroWordNet3
to obtain the semantic information that we need,
but we wont be able to access to this resource.
Thus, WordNet is used to calculate the seman-
tic similarity between T and H. The following pro-
cedure is applied:
</bodyText>
<listItem confidence="0.982507">
1. Word sense disambiguation using the Lesk
algorithm (Lesk, 1986), based on Wordnet defini-
tions.
2. A semantic similarity matrix between words
in T and H is defined. Words are used only in syn-
onym and hyperonym relationship. The Breadth
First Search algorithm is used over these tokens;
similarity is calculated by using two factors: length
of the path and orientation of the path.
3. To obtain the final score, we use matching
</listItem>
<bodyText confidence="0.781503666666667">
average.
The semantic similarity between two words is
computed as:
</bodyText>
<equation confidence="0.916256173913043">
)
(
)
(
))
,
(
(
2
)
,
(
t
Depth
s
Depth
t
s
LCS
Depth
t
s
Sim
</equation>
<bodyText confidence="0.934241285714286">
Where: s,t are source and target words that we
are comparing (s is in H and t is in T). Depth(s) is
the shortest distance from the root node to the cur-
rent node. LCS(s,t):is the least common subsume
of s and t.
The matching average (step 3) between two
sentences X and Y is calculated as follows:
</bodyText>
<figure confidence="0.876558105263158">
)
(
)
(
)
,
(
2
Y
Length
X
Length
Y
X
Match
erage
MatchingAv
3
http://www.illc.uva.nl/EuroWordNet/
</figure>
<page confidence="0.771086">
64
</page>
<table confidence="0.185463">
\x0c2.4 Longest Common Substring
</table>
<bodyText confidence="0.9771">
Given two strings, T of length n and H of length m,
the Longest Common Sub-string (LCS) problem
(Dan, 1999) will find the longest string that is a
substring of both T and H. It is found by dynamic
programming.
</bodyText>
<sectionHeader confidence="0.975672" genericHeader="method">
3 Experimental Evaluation and Discus-
</sectionHeader>
<bodyText confidence="0.918105548387097">
sion of the Results
With the aim of exploring the differences among
training sets and machine learning algorithms, we
did many experiments looking for the best result to
our system.
First, we converted the RTE4 and RTE5 data-
sets with Contradiction/Unknown/Entailment pair
information to a binary True/False problem, named
two-way problem.
Then, we used the following combination of da-
tasets: RTE3-Sp, RTE4-Sp, RTE3-Sp+RTE4-Sp,
SPARTE-Bal (balanced SPARTE Corpus with the
same number of true and false cases), and
SPARTE-Bal+ RTE3-Sp+RTE4-Sp. The training
set SPARTE-Balanced was created by taking all
true cases and randomly taking false cases, and
then we build a balanced training set containing
1352 pairs, with 676 true and 676 false pairs.
We used four classifiers to learn every devel-
opment set: (1) Support Vector Machine, (2) Ada
Boost, (3) Multilayer Perceptron (MLP) and (4)
Decision Tree using the open source WEKA Data
Mining Software (Witten &amp; Frank, 2005). In all the
tables results we show only the accuracy of the
best classifier.
The results obtained to predict RTE5-Sp in a
two-way classification task are summarized in Ta-
ble 1 below. In addition, table 2 shows our results
reported in RTE two-way classification task by
using with Cross Validation technique with 10
folds.
</bodyText>
<figure confidence="0.614534105263158">
Dataset Classifier Accuracy%
RTE3-Sp+RTE4-Sp SVM 60.83%
RTE3-Sp SVM 60.50%
RTE4-Sp MLP 60.50%
SPARTE-Bal+
RTE3-Sp+RTE4-Sp
MLP 60.17%
SPARTE-Bal DT 50%
Baseline - 50%
Table 1.Results obtained in two-way classification task.
Dataset Classifier Accuracy%
SPARTE-Bal DT 68.19%
RTE3-Sp SVM 66.50%
RTE3-Sp+RTE4-Sp MLP 61.44%
RTE4-Sp MLP 59.60%
SPARTE-Bal+
RTE3-Sp+RTE4-Sp
AdaBoost 56.83%
Baseline - 50%
</figure>
<tableCaption confidence="0.864622">
Table 2.Results obtained with Cross Validation 10 folds
</tableCaption>
<bodyText confidence="0.995708392857143">
in two-way task.
The performance in all cases was clearly above
those baselines. Only when using SPARTE-Bal we
obtained a result equal to the baseline (50% true
pairs and 50% false pairs).
The SPARTE-Balanced dataset yields the worst
results, maybe because this dataset contains only
pairs with QA task, and an additional reason, could
be that SPARTE is syntactically simpler than
PASCAL RTE. In that sense, some authors have
reported low performance when using syntactically
simpler datasets; for instance, by using BPI4
data-
set to predict RTEs datasets in English. Therefore,
SPARTE seems to be not enough good training set
to predict RTEs test sets.
The best performance of our system was
achieved with SVM classifier with RTE3-
Sp+RTE4-Sp dataset; it was 60.83% of accuracy.
In the majority of the cases, SVM or MLP classifi-
ers appear as favorite in all classification tasks.
Surprisingly, in the two-way task, a slight and
not statistical significant difference of 0.66% be-
tween the best and worst combination (except for
SPARTE-Bal) of datasets and classifiers is found.
So, it suggests that the combination of dataset and
classifiers do not produce a strong impact predict-
ing RTE5-Sp, at least, for these feature sets.
</bodyText>
<figure confidence="0.97853064">
4
http://www.cs.utexas.edu/users/pclark/bpi-test-suite/
))
(
),
(
min(
))
,
(
(
)
,
(
H
Length
T
Length
H
T
MaxComSub
Length
H
T
lcs
</figure>
<page confidence="0.995992">
65
</page>
<bodyText confidence="0.997557">
\x0cAlso, we observed that by including SPARTE-Bal
to RTE3-Sp+RTE4-Sp dataset, the performance
slightly decreases, although this difference was not
statistical significant.
The results obtained in table 2(and table 4) with
SPARTE-Bal and decision tree algorithm, are the
best for cross-validation experiments. In fact, an
accuracy of 68.19% was obtained, which is
18.19% bigger than the result obtained in table 1,
and was statistical significant.
Finally, we assessed our system only over the
SPARTE Corpus. First, we used cross validation
technique with ten folds over SPARTE-Bal, testing
over our four classifiers. Then, we tested
SPARTE-Bal by splitting the corpus in training set
(70%), and test set (30%).
The results are shown in the tables 4 and 5 below.
</bodyText>
<table confidence="0.8873385">
Classifier Accuracy%
DT 68.19%
MLP 62.64%
AdaBoost 61.31%
SVM 60.35%
Baseline 50%
</table>
<tableCaption confidence="0.884079">
Table 4.Results obtained with Cross Validation 10 folds
</tableCaption>
<table confidence="0.869401">
in two-way task to predict SPARTE.
Classifier Accuracy%
DT 66.50%
AdaBoost 62.31%
SVM 59.60%
MLP 52.70%
Baseline 50%
</table>
<tableCaption confidence="0.952091">
Table 5.Results obtained with SPARTE with split 70%.
</tableCaption>
<bodyText confidence="0.949788636363636">
The results on cross-validation are better than
those obtained on test set, which is most probably
due to overfitting of classifiers.
Table 5 shows a good performance of 66.50%,
predicting test set and using Decision trees. These
results are opposed to the bad performance re-
ported by SPARTE to predict RTEs datasets. Here,
in fact, the syntactic complexity and original task
do not change between train and test set; and it
seems to be the main problem with the low per-
formance of SPARTE in Table 1.
</bodyText>
<subsectionHeader confidence="0.76755">
3.1 Related Work
</subsectionHeader>
<bodyText confidence="0.9983444">
Up to our knowledge, there are not available re-
sults of other teams that used SPARTE to predict
RTE, or used RTEs applied to Spanish. However,
some comparison with other results for Spanish
could be done in AVE Challenge (Alberto Tellez-
Valero et al., 2008; Ferrandez et al., 2008; Castillo,
2008), but we will need to modify our system to
test AVE 2008 test set and computing different
metric for the ranking of the result.
On the other hand, comparing the results ob-
tained with English in RTE5 TAC Challenge, we
obtained a result not statistical significant with re-
spect to the median score for English systems that
is 61.17% of accuracy. Also, our system could be
compared to independent-language RTE systems.
To finish, we think that several improvements
could be done in order to improve the accuracy of
the system, using syntactic features, more semantic
information, and new external resources such as
Acronyms database.
</bodyText>
<sectionHeader confidence="0.996559" genericHeader="method">
4 Opportunities for Collaboration
</sectionHeader>
<bodyText confidence="0.998939192307692">
Our work is oriented to create a Textual Entailment
System. Such system could be used by another sys-
tem or teams of others Universities, as an internal
module.
The entailment relations between texts or
strings are very useful for a variety of Natural
Language Processing applications, such as Ques-
tion Answering, Information Extraction, Informa-
tion Retrieval and Document Summarization.
For example, a RTE module could be used in a
Question Answering system, where the answer of a
question must be entailed by the text that supports
the correctness of the answer; or an Automatic
Summarization system could eliminate the passag-
es whose meaning is already entailed by other pas-
sages and, by this way, reduce the size of the
passages.
In addition, a question answering system could
be enhanced by a RTE module, and also, these re-
sults are useful as Answer Validation System.
Our system was designed having in mind the
interoperation among systems. Thus, the system
inputs accept files in .xml format, and the output is
text plain files and .xml files.
On the other hand, one of the resources that
would allow this work advance is the EuroWord-
</bodyText>
<page confidence="0.928858">
66
</page>
<bodyText confidence="0.994745210526316">
\x0cnet, because it could provide additional semantic
information improving our semantic features, and
so the performance of our system. Due to being an
expensive and not freely available resource, we are
avoiding using it, but we expect to be able to use it
in the future. In section 3, we used Wordnet in or-
der to obtain the relationship between two different
concepts. Since Wordnet includes only synsets for
English and not for Spanish, we have translated the
&lt;t,h&amp;gt; pairs to English using the online Microsoft
Bing translator5
, in order to use Wordnet. As a re-
sult, a loss of performance was obtained. We be-
lieve that the use of EuroWordNet could benefit
our semantic features.
Currently, we are keeping improving our sys-
tem, and we are looking forward to get opportuni-
ties for collaboration with other teams of all the
Americas.
</bodyText>
<sectionHeader confidence="0.991748" genericHeader="conclusions">
5 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.997064631578948">
In this paper we present an initial RTE System
based for the Spanish language, based on machine
learning techniques that uses some of the available
textual entailment corpus and yields 60.83% of
accuracy.
One issue found is that SPARTE Corpus seems
to be not useful to predict RTEs-Sp datasets, be-
cause of the syntactic simplicity and the absence of
task information different to QA task.
On the other hand, we found that a competitive
result of 66.50%acc is reported by train and test set
taken from SPARTE Corpus.
Future work is oriented to experiment with ad-
ditional lexical and semantic similarities features
and to test the improvements they may yield. Also,
we must explore how to decrease the computation-
al cost of the system. Our plan is keeping applying
machine learning algorithms, testing with new fea-
tures, and adding new source of knowledge.
</bodyText>
<sectionHeader confidence="0.991279" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996494375">
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danillo
Giampiccolo, and Bernardo Magnini. 2009. The Fifth
PASCAL Recognizing Textual Entailment Challenge.
In proceedings of Textual Analysis Conference
(TAC). NIST, Maryland USA.
Adrian Iftene, Mihai-Alex Moruz.2009. UAIC Partici-
pation at RTE5, TAC 2009, Gaithersburg, Maryland,
USA.
</reference>
<page confidence="0.753285">
5
</page>
<reference confidence="0.994704755102041">
http://www.microsofttranslator.com/
S. Mirkin, R. Bar-Haim, J. Berant, I. Dagan, E. Shnarch,
A. Stern, and I. Szpektor.2009. Bar-Ilan Universitys
submission to RTE5, TAC 2009, Gaithersburg, Mary-
land, USA.
Castillo, Julio. Sagan in TAC2009: Using Support Vec-
tor Machines in Recognizing Textual Entailment and
TE Search Pilot task. TAC 2009, Gaithersburg, Mar-
yland, USA.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty and Christopher
D. Manning. 2006. Learning to distinguish valid tex-
tual entailments. RTE2 Challenge, Italy.
F. Zanzotto, Marco Pennacchiotti and Alessandro Mo-
schitti.2007. Shallow Semantics in Fast Textual En-
tailment Rule Learners, RTE3, Prague.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques&amp;quot;,
2nd Edition, Morgan Kaufmann, San Francisco,
USA.
Anselmo Penas, Alvaro Rodrigo, Felisa Verdejo.
SPARTE, a Test Suite for Recognising Textual En-
tailment in Spanish. Cicling 2006, Mexico.
Penas A., Rodrigo A., Sama V., and Verdejo F. Over-
view of the Answer Validation Exercise 2006, In-
Working notes for the Cross Language Evaluation
Forum Workshop (CLEF 2006), September 2006,
Spain.
Ido Dagan, Oren Glickman and Bernardo Magnini. The
PASCAL Recognising Textual Entailment Challenge.
In Quinonero-Candela, J.; Dagan, I.; Magnini, B.;
d\&apos;Alche-Buc, F. (Eds.) Machine Learning Chal-
lenges. Lecture Notes in Computer Science , Vol.
3944, pp. 177-190, Springer, 2006.
M. Lesk. Automatic sense disambiguation using ma-
chine readable dictionaries: How to tell a pine cone
from a ice cream cone. In SIGDOC 86, 1986.
Harold W. Kuhn, The Hungarian Method for the
assignment problem, Naval Research Logistics Quar-
terly. 1955
Alberto Tellez-Valero, Antonio Juarez-Gonzalez, Ma-
nuel Montes-y-Gomez, Luis Villasenior-Pineda.
INAOE at QA@CLEF 2008:Evaluating Answer Va-
lidation in Spanish Question Answering. CLEF 2008.
Julio J. Castillo. The Contribution of FaMAF at
QA@CLEF 2008.Answer ValidationExercise.CLEF
2008.
Oscar Ferrandez, Rafael Munoz, and Manuel Palomar.
A Lexical Semantic Approach to AVE. CLEF 2008.
</reference>
<page confidence="0.883551">
67
</page>
<figure confidence="0.328374">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.481127">
<note confidence="0.906077666666667">b&apos;Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas, pages 6267, Los Angeles, California, June 2010. c 2010 Association for Computational Linguistics</note>
<title confidence="0.9878435">A Machine Learning Approach for Recognizing Textual Entailment in Spanish</title>
<author confidence="0.99934">Julio Javier Castillo</author>
<affiliation confidence="0.999884">National University of Cordoba</affiliation>
<address confidence="0.9791245">Ciudad Universitaria, 5000 Cordoba, Argentina</address>
<email confidence="0.999916">jotacastillo@gmail.com</email>
<abstract confidence="0.980612294117647">This paper presents a system that uses machine learning algorithms for the task of recognizing textual entailment in Spanish language. The datasets used include SPARTE Corpus and a translated version to Spanish of RTE3, RTE4 and RTE5 datasets. The features chosen quantify lexical, syntactic and semantic level matching between text and hypothesis sentences. We analyze how the different sizes of datasets and classifiers could impact on the final overall performance of the RTE classification of two-way task in Spanish. The RTE system yields 60.83% of accuracy and a competitive result of 66.50% of accuracy is reported by train and test set taken from SPARTE Corpus with 70% split.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
<author>Danillo Giampiccolo</author>
<author>Bernardo Magnini</author>
</authors>
<title>The Fifth PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2009</date>
<booktitle>In proceedings of Textual Analysis Conference (TAC).</booktitle>
<location>NIST, Maryland USA.</location>
<marker>Bentivogli, Dagan, Dang, Giampiccolo, Magnini, 2009</marker>
<rawString>Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danillo Giampiccolo, and Bernardo Magnini. 2009. The Fifth PASCAL Recognizing Textual Entailment Challenge. In proceedings of Textual Analysis Conference (TAC). NIST, Maryland USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrian Iftene</author>
</authors>
<title>Mihai-Alex Moruz.2009. UAIC Participation at RTE5, TAC</title>
<date>2009</date>
<location>Gaithersburg, Maryland, USA. http://www.microsofttranslator.com/</location>
<marker>Iftene, 2009</marker>
<rawString>Adrian Iftene, Mihai-Alex Moruz.2009. UAIC Participation at RTE5, TAC 2009, Gaithersburg, Maryland, USA. http://www.microsofttranslator.com/ S. Mirkin, R. Bar-Haim, J. Berant, I. Dagan, E. Shnarch, A. Stern, and I. Szpektor.2009. Bar-Ilan Universitys submission to RTE5, TAC 2009, Gaithersburg, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Castillo</author>
</authors>
<title>Sagan in TAC2009: Using Support Vector Machines in Recognizing Textual Entailment and TE Search Pilot task. TAC</title>
<date>2009</date>
<location>Gaithersburg, Maryland, USA.</location>
<contexts>
<context position="2451" citStr="Castillo, 2009" startWordPosition="386" endWordPosition="387">answers given by a QA system. The Answer Validation task must select the best answer for the final output. There is a subtask for each language involved in QA, the Spanish is one of these. Thus, AVE task is very similar to RTE (Recognition of Textual Entailments). In this paper, we address the RTE task problem of determining the entailment value between Text and Hypothesis pairs in Spanish, applying machine learning techniques. In the past, RTEs Challenges machine learning algorithms were widely used for the task of recognizing textual entailment (Marneffe et al., 2006; Zanzotto et al., 2007; Castillo, 2009) and they have reported goods results for English language. Also, our system applies machine learning algorithms to the Spanish. We built a set of datasets based on public available datasets for English, together to SPARTE (Penas et al, 2006), an available Corpus in Spanish. This corpus contains 2962 hypothesis with a document label and a True/False value indicating whether the document entails the hypothesis or not. Up to our knowledge, SPARTE corpus in the only corpus aimed at evaluating RTE systems in Spanish. Finally, we generated a feature vector with the following components for both Tex</context>
</contexts>
<marker>Castillo, 2009</marker>
<rawString>Castillo, Julio. Sagan in TAC2009: Using Support Vector Machines in Recognizing Textual Entailment and TE Search Pilot task. TAC 2009, Gaithersburg, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Daniel Cer</author>
<author>Anna Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to distinguish valid textual entailments. RTE2</title>
<date>2006</date>
<location>Challenge, Italy.</location>
<marker>de Marneffe, MacCartney, Grenager, Cer, Rafferty, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, Trond Grenager, Daniel Cer, Anna Rafferty and Christopher D. Manning. 2006. Learning to distinguish valid textual entailments. RTE2 Challenge, Italy.</rawString>
</citation>
<citation valid="false">
<authors>
<author>F Zanzotto</author>
</authors>
<title>Marco Pennacchiotti and Alessandro Moschitti.2007. Shallow Semantics in Fast Textual Entailment Rule Learners,</title>
<location>RTE3, Prague.</location>
<marker>Zanzotto, </marker>
<rawString>F. Zanzotto, Marco Pennacchiotti and Alessandro Moschitti.2007. Shallow Semantics in Fast Textual Entailment Rule Learners, RTE3, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques&amp;quot;, 2nd Edition,</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, USA. Anselmo Penas, Alvaro Rodrigo, Felisa Verdejo.</location>
<contexts>
<context position="11831" citStr="Witten &amp; Frank, 2005" startWordPosition="1952" endWordPosition="1955">used the following combination of datasets: RTE3-Sp, RTE4-Sp, RTE3-Sp+RTE4-Sp, SPARTE-Bal (balanced SPARTE Corpus with the same number of true and false cases), and SPARTE-Bal+ RTE3-Sp+RTE4-Sp. The training set SPARTE-Balanced was created by taking all true cases and randomly taking false cases, and then we build a balanced training set containing 1352 pairs, with 676 true and 676 false pairs. We used four classifiers to learn every development set: (1) Support Vector Machine, (2) Ada Boost, (3) Multilayer Perceptron (MLP) and (4) Decision Tree using the open source WEKA Data Mining Software (Witten &amp; Frank, 2005). In all the tables results we show only the accuracy of the best classifier. The results obtained to predict RTE5-Sp in a two-way classification task are summarized in Table 1 below. In addition, table 2 shows our results reported in RTE two-way classification task by using with Cross Validation technique with 10 folds. Dataset Classifier Accuracy% RTE3-Sp+RTE4-Sp SVM 60.83% RTE3-Sp SVM 60.50% RTE4-Sp MLP 60.50% SPARTE-Bal+ RTE3-Sp+RTE4-Sp MLP 60.17% SPARTE-Bal DT 50% Baseline - 50% Table 1.Results obtained in two-way classification task. Dataset Classifier Accuracy% SPARTE-Bal DT 68.19% RTE3</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical machine learning tools and techniques&amp;quot;, 2nd Edition, Morgan Kaufmann, San Francisco, USA. Anselmo Penas, Alvaro Rodrigo, Felisa Verdejo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>a SPARTE</author>
</authors>
<title>Test Suite for Recognising Textual Entailment in Spanish. Cicling</title>
<date>2006</date>
<marker>SPARTE, 2006</marker>
<rawString>SPARTE, a Test Suite for Recognising Textual Entailment in Spanish. Cicling 2006, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Penas</author>
<author>A Rodrigo</author>
<author>V Sama</author>
<author>F Verdejo</author>
</authors>
<title>Overview of the Answer Validation Exercise 2006, InWorking notes for the Cross Language Evaluation Forum Workshop (CLEF</title>
<date>2006</date>
<journal>Ido Dagan, Oren Glickman</journal>
<contexts>
<context position="1607" citStr="Penas et al, 2006" startWordPosition="246" endWordPosition="249">nd a competitive result of 66.50% of accuracy is reported by train and test set taken from SPARTE Corpus with 70% split. 1 Introduction The objective of the Recognizing Textual Entailment Challenge is determining whether the meaning of the Hypothesis (H) can be inferred from a text (T) (Ido Dagan et al., 2006). This challenge has been organized by NIST in recent years. Another related antecedent was Answer Validation Exercise (AVE), part of Cross Language Evaluation Forum (CLEF), whose objective is to develop systems which are able to decide whether the answer to a question is correct or not (Penas et al, 2006). It was a three year-old track, from 2006 to 2008. AVE challenge was an evaluation framework for Question Answering (QA) systems to promote the development and evaluation of subsystems aimed at validating the correctness of the answers given by a QA system. The Answer Validation task must select the best answer for the final output. There is a subtask for each language involved in QA, the Spanish is one of these. Thus, AVE task is very similar to RTE (Recognition of Textual Entailments). In this paper, we address the RTE task problem of determining the entailment value between Text and Hypoth</context>
<context position="3903" citStr="Penas et al, 2006" startWordPosition="614" endWordPosition="617"> Text and the Hypothesis. The remainder of the paper is organized as follows. Section 2 shows the system description, whereas Section 3 describes the results of experimental evaluation and discussion of them. Section 4 discusses opportunities of collaboration. Finally, Section 5 summarizes the conclusions and lines for future work. 2 System Description This section provides an overview of our system which is based on a machine learning approach for recognizing textual entailment to the Spanish. The system produces feature vectors for the available development data RTE3, RTE4, RTE5, and SPARTE(Penas et al, 2006). Weka (Witten and Frank, 2000) is used to train classifiers on these feature vectors. The SPARTE Corpus, was built from the Spanish corpora used at Cross-Language Evaluation Forum (CLEF) for evaluating QA systems during the years 2003, 2004 and 2005. This corpus contains 2962 hypothesis with a True/False value indicating whether the document entails the hypothesis or not. Due to, all available dataset of PASCAL Text Analysis Conference were in English, we translated every dataset to Spanish by using an online translator engine1 . So, we had a Spanish dataset but with some translation errors p</context>
</contexts>
<marker>Penas, Rodrigo, Sama, Verdejo, 2006</marker>
<rawString>Penas A., Rodrigo A., Sama V., and Verdejo F. Overview of the Answer Validation Exercise 2006, InWorking notes for the Cross Language Evaluation Forum Workshop (CLEF 2006), September 2006, Spain. Ido Dagan, Oren Glickman and Bernardo Magnini. The PASCAL Recognising Textual Entailment Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>In Quinonero-Candela</author>
<author>J Dagan</author>
<author>I Magnini</author>
<author>B d\&apos;Alche-Buc</author>
<author>F</author>
</authors>
<title>(Eds.) Machine Learning Challenges.</title>
<date>2006</date>
<journal>Lecture Notes in Computer Science ,</journal>
<volume>3944</volume>
<pages>177--190</pages>
<publisher>Springer,</publisher>
<marker>Quinonero-Candela, Dagan, Magnini, d\&apos;Alche-Buc, F, 2006</marker>
<rawString>In Quinonero-Candela, J.; Dagan, I.; Magnini, B.; d\&apos;Alche-Buc, F. (Eds.) Machine Learning Challenges. Lecture Notes in Computer Science , Vol. 3944, pp. 177-190, Springer, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from a ice cream cone.</title>
<date>1986</date>
<booktitle>In SIGDOC 86,</booktitle>
<contexts>
<context position="9691" citStr="Lesk, 1986" startWordPosition="1581" endWordPosition="1582">ent in the graph. Length (T) is the number of tokens in T. Length (H) is the number of tokens in H. 2.3 Wordnet Distance Since, all datasets are in Spanish, we need to convert &lt;T, H&amp;gt; pair to English. In the case of RTEsSp datasets, this action will backward to the English language (source). Our ideal case would be to use EuroWordNet3 to obtain the semantic information that we need, but we wont be able to access to this resource. Thus, WordNet is used to calculate the semantic similarity between T and H. The following procedure is applied: 1. Word sense disambiguation using the Lesk algorithm (Lesk, 1986), based on Wordnet definitions. 2. A semantic similarity matrix between words in T and H is defined. Words are used only in synonym and hyperonym relationship. The Breadth First Search algorithm is used over these tokens; similarity is calculated by using two factors: length of the path and orientation of the path. 3. To obtain the final score, we use matching average. The semantic similarity between two words is computed as: ) ( ) ( )) , ( ( 2 ) , ( t Depth s Depth t s LCS Depth t s Sim Where: s,t are source and target words that we are comparing (s is in H and t is in T). Depth(s) is the sho</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from a ice cream cone. In SIGDOC 86, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold W Kuhn</author>
</authors>
<title>The Hungarian Method for the assignment problem, Naval Research Logistics Quarterly.</title>
<date>1955</date>
<contexts>
<context position="8732" citStr="Kuhn, 1955" startWordPosition="1405" endWordPosition="1406">lgorithm works independently from the language that we are analyzing. We used a Spanish Stemmer that stems words in Spanish based on a modified version of the Snowball algorithm2 . Additionally, by using Levenshtein distance we defined a lexical distance and the procedure is the following: Each string T and H are divided in a list of tokens. The similarity between each pair of tokens in T and H is performed using the Levenshtein distance over stems. The string similarity between two lists of tokens is reduced to the problem of bipartite graph matching, performed using the Hungarian algorithm (Kuhn, 1955) over this bipartite graph. Then, we found the assignment that maximizes the sum of ratings of each token. Note that each graph node is a token of the list. 2 http://snowball.tartarus.org/ The final score is calculated by: )) ( ), ( ( H Length T Length Max TotalSim finalscore Where: TotalSim is the sum of the similarities with the optimal assignment in the graph. Length (T) is the number of tokens in T. Length (H) is the number of tokens in H. 2.3 Wordnet Distance Since, all datasets are in Spanish, we need to convert &lt;T, H&amp;gt; pair to English. In the case of RTEsSp datasets, this action will bac</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>Harold W. Kuhn, The Hungarian Method for the assignment problem, Naval Research Logistics Quarterly. 1955 Alberto Tellez-Valero, Antonio Juarez-Gonzalez, Manuel Montes-y-Gomez, Luis Villasenior-Pineda.</rawString>
</citation>
<citation valid="true">
<date>2008</date>
<booktitle>INAOE at QA@CLEF 2008:Evaluating Answer Validation in Spanish Question Answering. CLEF</booktitle>
<marker>2008</marker>
<rawString>INAOE at QA@CLEF 2008:Evaluating Answer Validation in Spanish Question Answering. CLEF 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio J Castillo</author>
</authors>
<title>The Contribution of FaMAF at QA@CLEF 2008.Answer ValidationExercise.CLEF</title>
<date>2008</date>
<note>Oscar</note>
<contexts>
<context position="15854" citStr="Castillo, 2008" startWordPosition="2595" endWordPosition="2596">nd using Decision trees. These results are opposed to the bad performance reported by SPARTE to predict RTEs datasets. Here, in fact, the syntactic complexity and original task do not change between train and test set; and it seems to be the main problem with the low performance of SPARTE in Table 1. 3.1 Related Work Up to our knowledge, there are not available results of other teams that used SPARTE to predict RTE, or used RTEs applied to Spanish. However, some comparison with other results for Spanish could be done in AVE Challenge (Alberto TellezValero et al., 2008; Ferrandez et al., 2008; Castillo, 2008), but we will need to modify our system to test AVE 2008 test set and computing different metric for the ranking of the result. On the other hand, comparing the results obtained with English in RTE5 TAC Challenge, we obtained a result not statistical significant with respect to the median score for English systems that is 61.17% of accuracy. Also, our system could be compared to independent-language RTE systems. To finish, we think that several improvements could be done in order to improve the accuracy of the system, using syntactic features, more semantic information, and new external resour</context>
</contexts>
<marker>Castillo, 2008</marker>
<rawString>Julio J. Castillo. The Contribution of FaMAF at QA@CLEF 2008.Answer ValidationExercise.CLEF 2008. Oscar Ferrandez, Rafael Munoz, and Manuel Palomar.</rawString>
</citation>
<citation valid="true">
<title>A Lexical Semantic Approach to AVE. CLEF</title>
<date>2008</date>
<marker>2008</marker>
<rawString>A Lexical Semantic Approach to AVE. CLEF 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>