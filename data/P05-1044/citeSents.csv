356 \x0cputation, like random sampling (see, e.g., CITATION), will not help to avoid this difficulty; in addition, convergence rates are in general unknown and bounds difficult to prove,,
Even when they avoid local maxima (e.g., through clever initialization) they typically deviate from human ideas of what the right structure is CITATION,,
This 8 CITATION explored the semi-supervised classification problem for spatially-distributed data, where some data are labeled, using a Boltzmann machine to model the dataset,,
a new acyclic WFSA, and sum the u-scores of all its paths CITATION using a simple dynamic programming algorithm akin to the forward algorithm,,
5.1 Comparison with EM Our experiments are inspired by those in CITATION; we train a trigram tagger using only unlabeled data, assuming complete knowledge of the tagging dictionary.5 In our experiments, we varied the amount of data available (12K96K words of WSJ), the heaviness of smoothing, and the estimation criterion,,
In CITATION, we define a sentences neighborhood to be a set of slightly-altered sentences that use the same lexemes, as suggested at the start of this section,,
To compute this, intersect the WFSA and the lattice, obtaining a new acyclic WFSA, and sum the u-scores of all its paths CITATION using a simple dynamic programming algorithm akin to the forward algorithm,,
These gains dwarf the performance of EM on over 1.1M words (66.6% as reported by CITATION), even when the latter uses improved search (70.0%),,
CITATION did so, then resorted to an approximation because the true objective function was hard to normalize,,
k~ k 1; i, y 6= y i , ~ (~ f(xi, y i ) ~ f(xi, y)) expected local accuracy CITATION Y i Y j p \x10 `j(Y ) = `j(y i ) |xi, ~ \x11 Table 1: Various supervised training criteria,,
m CITATION, which locally maximizes Y i p \x10 X = xi |~ \x11 = Y i X yY p \x10 X = xi, Y = y |~ \x11 (1) where X is a random variable over sentences and Y a random variable over analyses (notation is often abbreviated, eliminating the random variables),,
unsupervised learning CITATION,,
354 \x0cWe are particularly interested in log-linear models over sequences, like the conditional random fields (CRFs) of CITATION and weighted CFGs (Miya,,
e examples (CITATION; CITATION; CITATION): Y i &quot; u \x10 xi |~ \x11 , X j u \x10 xj |~ \x11 # (8) Viewed as a CE method, this approach (though effective when there are few hypotheses) seems misguided; the objective says to move mass to each example at the expense of all other training examples,,
6 Future Work Foremost for future work is the minimally supervised paradigm in which a small amount of labeled data is available (see, e.g., CITATION),,
CITATION discuss the latent maximum entropy principle,,
354 \x0cWe are particularly interested in log-linear models over sequences, like the conditional random fields (CRFs) of CITATION and weighted CFGs CITATION,,
The effectiveness of CE (and different neighborhoods) for dependency grammar induction is explored in CITATION with considerable success,,
The classical forward-backward and inside-outside algorithms try to guide probabilistic models to discover structure in text, but they tend to get stuck in local maxima CITATION,,
This approach has been applied to conditional densities CITATION and conditional training of acoustic models with hidden variables CITATION,,
7), we apply a standard numerical optimization method (L-BFGS) that iteratively climbs the function using knowledge of its value and gradient CITATION,,
joint likelihood (JL) Y i p \x10 xi, y i |~ \x11 conditional likelihood (CL) Y i p \x10 y i |xi, ~ \x11 classification accuracy CITATION X i (y i , y(xi)) expected classification accuracy CITATION X i p \x10 y i |xi, ~ \x11 negated boosting loss CITATION X i p \x10 y i |xi, ~ \x111 margin CITATION s.t,,
CE offers an additional way to inject domain knowledge into unsupervised learning CITATION,,
Typically one turns to the EM algorithm CITATION, which locally maximizes Y i p \x10 X = xi |~ \x11 = Y i X yY p \x10 X = xi, Y = y |~ \x11 (1) where X is a random variable over sentences and Y a random variable over analyses (notation is often abbreviated, eliminating the random variables),,
An alternative is to restrict the neighborhood to the set of observed training examples rather than all possible examples (CITATION; CITATION; CITATION): Y i &quot; u \x10 xi |~ \x11 , X j u \x10 xj |~ \x11 # (8) Viewed as a CE method, this approach (though effective when there are few hypotheses) seems misguided; the objective says to move mass to each example at the expense of all other training examples,,
Log-linear models can be so crafted and have already achieved excellent performance when trained on annotated data, where they are known as maximum entropy models (CITATION; CITATION),,
CE with lattice neighborhoods is not confined to the WFSAs of this paper; when estimating weighted CFGs, the key algorithm is the inside algorithm for lattice parsing CITATION,,
When Bi contains only xi paired with the current best competitor (y) to y i , we have a technique that resembles maximum margin training CITATION,,
joint likelihood (JL) Y i p \x10 xi, y i |~ \x11 conditional likelihood (CL) Y i p \x10 y i |xi, ~ \x11 classification accuracy CITATION,,
Unlike well-known bootstrapping approaches CITATION, EM and CE have the possible advantage of maintaining posteriors over hidden labels (or structure) throughout learning; bootstrapping either chooses, for each example, a single label, or remains completely agnostic,,
In future we might wish to apply techniques for avoiding local optima, such as deterministic annealing CITATION,,
 to restrict the neighborhood to the set of observed training examples rather than all possible examples (CITATION; CITATION; CITATION): Y i &quot; u \x10 xi |~ \x11 , X j u \x10 xj |~ \x11 # (8) Viewed as a CE method, this approach (though effective when there are few hypotheses) seems misguided; the objective says to move mass to each example at the expense of all other training examples,,
For a k-state WFSA, this equates to solving a linear system of k equations in k variables CITATION,,
CITATION argue for CL on grounds of accuracy, but see also CITATION,,
Since the forward-backward and inside-outside algorithms are instances of Expectation-Maximization (EM) CITATION, a natural approach is to construct EM algorithms that handle log-linear models,,
1 Not to be confused with contrastive divergence minimization CITATION, a technique for training products of experts,,
Alternatives to exact com2 These are exemplified by CRFs CITATION, which can be viewed alternately as undirected dynamic graphical models with a chain topology, as log-linear models over entire sequences with local features, or as WFSAs,,
