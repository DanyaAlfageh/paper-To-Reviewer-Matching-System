CITATION achieved a gain of 1% over this result by including a classbased lexicalization.,,
A comparison of performance is more di\x1ecult for the lexicalized PCFG of CITATION which was trained by EM on 450,000 sentences of German newspaper text.,,
However, the gain achieved by CITATION due to grammar lexicalizaton is only 2%, compared to about 10% in our case.,,
Such word-based lexicalizations of probability models are used successfully in the statistical parsing models of, e.g., CITATION, CITATION, or CITATION.,,
In all of the cited approaches, the Penn Wall Street Journal Treebank CITATION is used, the availability of which obviates the standard e\x1bort required for treebank training\x16handannotating l,,
However, the gain achieved by CITATION due to grammar lexicalizaton is only 2%, compared to about 10% in our case.,,
A comparison of the performance gain due to grammar lexicalization shows that our results are on a par with that reported in CITATION.,,
Such word-based lexicalizations of probability models are used successfully in the statistical parsing models of, e.g., CITATION, CITATION, or CITATION.,,
In all of the cited approaches, the Penn Wall Street Journal Treebank CITATION is used, the availability of which obviates the standard e\x1bort required for treebank training\x1,,
In all of the cited approaches, the Penn Wall Street Journal Treebank CITATION is used, the availability of which obviates the standard e\x1bort required for treebank training\x16handannotating large corpora of speci\x1cc domains of speci\x1cc languages with speci\x1cc parse types.,,
Moreover, common wisdom is that training from unannotated data via the expectationmaximization (EM) algorithm CITATION yields poor results unless at least partial annotation is applied.,,
Experimental results con\x1crming this wisdom have been presented, e.g., by CITATION and CITATION for EM training of Hidden Markov Models and PCFGs.,,
For data consisting of unannotated sentences\x16so-called incomplete data\x16the iterative method of the EM algorithm CITATION has to be employed.,,
A singly-iterative algorithm interleaving EM and iterative scaling into a mathematically well-de\x1cned estimation method for log-linear models from incomplete data is the IM algorithm of CITATION.,,
In all of the cited approaches, the Penn Wall Street Journal Treebank CITATION is used, the availability of which obviates the standard e\x1bort required for treebank training\x16handannotating large corpora of speci\x1cc domains of speci\x1cc languages with speci\x1cc parse types.,,
Moreover, common wisdom is that training from unannotated data via the expectationmaximization (EM) algorithm CITATION yields poor results unless at least partial annotation is applied.,,
Experimental results con\x1crming this wisdom have been presented, e.g., by CITATION and CITATION for EM training of Hidden Markov Models and PCFGs.,,
We \x1crst parsed with the LFG grammar 550 sentences which are used for illustrative purposes in the foreign language learner&apos;s grammar of CITATION.,,
In a next step, the correct parse was indicated by a human disambiguator, according to the reading intended in CITATION.,,
Note that for a uniformly distributed reference model p0, the minimum divergence model is a maximum entropy model CITATION.,,
3 Property Design and Lexicalization 3.1 Basic Con\x1cgurational Properties The basic 190 properties employed in our models are similar to the properties of CITATION which incorporate general linguistic principles into a log-linear model.,,
5 Discussion The most direct points of comparison of our method are the approaches of CITATION and CITATION.,,
CITATION achieved a gain of 1% over this result by including a classbased lexicalization.,,
In previous work on log-linear models for LFG by CITATION, pseudo\x0clikelihood estimation from annotated corpora has been introduced and experimented with on a small scale.,,
Fortunately, algorithms exist for statistical inference of log-linear models from unannotated data CITATION.,,
Note that for a uniformly distributed reference model p0, the minimum divergence model is a maximum entropy model CITATION.,,
3 Property Design and Lexicalization 3.1 Basic Con\x1cgurational Properties The basic 190 properties employed in our models are similar to the properties of CITATION which incorporate general linguistic principles into a log-linear model.,,
5 Discussion The most direct points of comparison of our method are the approaches of CITATION and CITATION.,,
CITATION achieved a gain of 1% over this result by including a classbased lexicalization.,,
od values is guaranteed to converge to a critical point of the likelihood function L. This is shown for the IM algorithm in CITATION.,,
A way to search for order in this chaos is to search for starting values which are hopefully attracted by the global maximum of L. This problem can best be explained in terms of the minimum divergence paradigm CITATION, which is equivalent to the maximum likelihood paradigm by the following theorem.,,
d lexicalizations of probability models are used successfully in the statistical parsing models of, e.g., CITATION, CITATION, or CITATION.,,
In all of the cited approaches, the Penn Wall Street Journal Treebank CITATION is used, the availability of which obviates the standard e\x1bort required for treebank training\x16handannotating large corpora of speci\x1cc domains of speci\x1cc languages with speci\x1cc parse types.,,
Moreover, common wisdom is that training from unannotated data via the expectationmaximization (EM) algorithm CITATION yields poor results unless at least partial annotation is applied.,,
Experimental results con\x1crming this wisdom have been presented, e.g., by CITATION and CITATION for EM training of Hidden Markov Models and PCFGs.,,
In order to compare the contribution of unambiguous and ambiguous sentences to the estimation results, we extracted a subcorpus of 4,000 sentences, for which the LFG grammar produced a unique parse, from the full train2 The German LFG grammar is being implemented in the Xerox Linguistic Environment (XLE, see CITATION) as part of the Parallel Grammar (ParGram) project at the IMS Stuttgart.,,
In all of the cited approaches, the Penn Wall Street Journal Treebank CITATION is used, the availability of which obviates the standard e\x1bort required for treebank training\x16handannotating large corpora of speci\x1cc domains of speci\x1cc languages with speci\x1cc parse types.,,
Moreover, common wisdom is that training from unannotated data via the expectationmaximization (EM) algorithm CITATION yields poor results unless at least partial annotation is applied.,,
Experimental results con\x1crming this wisdom have been presented, e.g., by CITATION and CITATION for EM training of Hidden Markov Models and PCFGs.,,
Class-based estimated frequencies are introduced in CITATION as the frequency f(v; n) of a (v; n)-pair in the training corpus, weighted by the best estimate of the class-membership probability p(cjv; n) of an EM-based clustering model on (v; n)-pairs, i.e., fc(v; n) = max c2C p(cjv; n)(f(v; n) + 1).,,
As is shown in CITATION in an evaluation on lexical ambiguity resolution, a gain of about 7% can be obtained by using the class-based estimated frequency fc(v; n) as disambiguation criterion instead of classbased probabilities p(njv).,,
Such word-based lexicalizations of probability models are used successfully in the statistical parsing models of, e.g., CITATION, CITATION, or CITATION.,,
In all of the cited approaches, the Penn Wall Street Journal Treebank CITATION is used, the availability of which obviates the standard e\x1bort required for treebank training\x16handannotating large corpora of speci\x,,
In previous work on log-linear models for LFG by CITATION, pseudo\x0clikelihood estimation from annotated corpora has been introduced and experimented with on a small scale.,,
Fortunately, algorithms exist for statistical inference of log-linear models from unannotated data CITATION.,,
For data consisting of unannotated sentences\x16so-called incomplete data\x16the iterative method of the EM algorithm CITATION has to be employed.,,
A singly-iterative algorithm interleaving EM and iterative scaling into a mathematically well-de\x1cned estimation method for log-linear models from incomplete data is the IM algorithm of CITATION.,,
2.3 Searching for Order in Chaos For incomplete-data estimation, a sequence of likelihood values is guaranteed to converge to a critical point of the likelihood function L. This is shown for the IM algorithm in CITATION.,,
A way to search for order in this chaos is to search for starting values which are hopefully attracted by the global maximum of L. This problem can best be explained in terms of the minimum divergence paradigm CITATION, which is equivalent to the maximum likelihood paradigm by the following theorem.,,
