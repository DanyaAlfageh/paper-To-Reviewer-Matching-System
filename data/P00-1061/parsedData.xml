<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.812402">
b&quot;Lexicalized Stochastic Modeling of Constraint-Based Grammars
using Log-Linear Measures and EM Training
</title>
<author confidence="0.777056">
Stefan Riezler
</author>
<affiliation confidence="0.69573">
IMS, Universitat Stuttgart
</affiliation>
<email confidence="0.707947">
riezler@ims.uni-stuttgart.de
</email>
<author confidence="0.412355">
Detlef Prescher
</author>
<affiliation confidence="0.468468">
IMS, Universitat Stuttgart
</affiliation>
<email confidence="0.698866">
prescher@ims.uni-stuttgart.de
</email>
<author confidence="0.756422">
Jonas Kuhn
</author>
<affiliation confidence="0.715936">
IMS, Universitat Stuttgart
</affiliation>
<email confidence="0.834039">
jonas@ims.uni-stuttgart.de
</email>
<author confidence="0.704479">
Mark Johnson
</author>
<bodyText confidence="0.427384">
Cog. &amp; Ling. Sciences, Brown University
</bodyText>
<email confidence="0.885789">
Mark_Johnson@brown.edu
</email>
<sectionHeader confidence="0.98008" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999064176470588">
We present a new approach to
stochastic modeling of constraint-
based grammars that is based on log-
linear models and uses EM for esti-
mation from unannotated data. The
techniques are applied to an LFG
grammar for German. Evaluation on
an exact match task yields 86% pre-
cision for an ambiguity rate of 5.4,
and 90% precision on a subcat frame
match for an ambiguity rate of 25.
Experimental comparison to train-
ing from a parsebank shows a 10%
gain from EM training. Also, a new
class-based grammar lexicalization is
presented, showing a 10% gain over
unlexicalized models.
</bodyText>
<sectionHeader confidence="0.998009" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998538346534653">
Stochastic parsing models capturing contex-
tual constraints beyond the dependencies of
probabilistic context-free grammars (PCFGs)
are currently the subject of intensive research.
An interesting feature common to most such
models is the incorporation of contextual de-
pendencies on individual head words into rule-
based probability models. Such word-based
lexicalizations of probability models are used
successfully in the statistical parsing mod-
els of, e.g., Collins (1997), Charniak (1997),
or Ratnaparkhi (1997). However, it is still
an open question which kind of lexicaliza-
tion, e.g., statistics on individual words or
statistics based upon word classes, is the best
choice. Secondly, these approaches have in
common the fact that the probability models
are trained on treebanks, i.e., corpora of man-
ually disambiguated sentences, and not from
corpora of unannotated sentences. In all of the
cited approaches, the Penn Wall Street Jour-
nal Treebank (Marcus et al., 1993) is used,
the availability of which obviates the standard
e\x1bort required for treebank training\x16hand-
annotating large corpora of speci\x1cc domains
of speci\x1cc languages with speci\x1cc parse types.
Moreover, common wisdom is that training
from unannotated data via the expectation-
maximization (EM) algorithm (Dempster et
al., 1977) yields poor results unless at
least partial annotation is applied. Experi-
mental results con\x1crming this wisdom have
been presented, e.g., by Elworthy (1994) and
Pereira and Schabes (1992) for EM training
of Hidden Markov Models and PCFGs.
In this paper, we present a new lexicalized
stochastic model for constraint-based gram-
mars that employs a combination of head-
word frequencies and EM-based clustering
for grammar lexicalization. Furthermore, we
make crucial use of EM for estimating the
parameters of the stochastic grammar from
unannotated data. Our usage of EM was ini-
tiated by the current lack of large uni\x1ccation-
based treebanks for German. However, our ex-
perimental results also show an exception to
the common wisdom of the insu\x1eciency of EM
for highly accurate statistical modeling.
Our approach to lexicalized stochastic mod-
eling is based on the parametric family of log-
linear probability models, which is used to de-
\x1cne a probability distribution on the parses
of a Lexical-Functional Grammar (LFG) for
German. In previous work on log-linear mod-
els for LFG by Johnson et al. (1999), pseudo-
\x0clikelihood estimation from annotated corpora
has been introduced and experimented with
on a small scale. However, to our knowledge,
to date no large LFG annotated corpora of
unrestricted German text are available. For-
tunately, algorithms exist for statistical infer-
ence of log-linear models from unannotated
data (Riezler, 1999). We apply this algorithm
to estimate log-linear LFG models from large
corpora of newspaper text. In our largest ex-
periment, we used 250,000 parses which were
produced by parsing 36,000 newspaper sen-
tences with the German LFG. Experimental
evaluation of our models on an exact-match
task (i.e. percentage of exact match of most
probable parse with correct parse) on 550
manually examined examples with on average
5.4 analyses gave 86% precision. Another eval-
uation on a verb frame recognition task (i.e.
percentage of agreement between subcatego-
rization frames of main verb of most proba-
ble parse and correct parse) gave 90% pre-
cision on 375 manually disambiguated exam-
ples with an average ambiguity of 25. Clearly,
a direct comparison of these results to state-
of-the-art statistical parsers cannot be made
because of di\x1berent training and test data and
other evaluation measures. However, we would
like to draw the following conclusions from our
experiments:
\x0f The problem of chaotic convergence be-
haviour of EM estimation can be solved
for log-linear models.
\x0f EM does help constraint-based gram-
mars, e.g. using about 10 times more sen-
tences and about 100 times more parses
for EM training than for training from an
automatically constructed parsebank can
improve precision by about 10%.
\x0f Class-based lexicalization can yield a gain
in precision of about 10%.
In the rest of this paper we intro-
duce incomplete-data estimation for log-linear
models (Sec. 2), and present the actual design
of our models (Sec. 3) and report our experi-
mental results (Sec. 4).
</bodyText>
<sectionHeader confidence="0.769771333333333" genericHeader="introduction">
2 Incomplete-Data Estimation for
Log-Linear Models
2.1 Log-Linear Models
</sectionHeader>
<bodyText confidence="0.998343">
A log-linear distribution p\x15(x) on the set of
analyses X of a constraint-based grammar can
be de\x1cned as follows:
</bodyText>
<equation confidence="0.9990812">
p\x15(x) = Z\x15
;1e\x15\x01\x17(x)p0(x)
where Z\x15 =
P
x2X e\x15\x01\x17(x)p0(x) is a normal-
</equation>
<bodyText confidence="0.971885666666667">
izing constant, \x15 = (\x151; : : : ; \x15n) 2 IRn is a
vector of log-parameters, \x17 = (\x171; : : : ; \x17n) is
a vector of property-functions \x17i : X ! IR for
</bodyText>
<equation confidence="0.987399">
i = 1; : : : ; n, \x15 \x01 \x17(x) is the vector dot prod-
uct
Pn
i=1 \x15i\x17i(x), and p0 is a \x1cxed reference
</equation>
<bodyText confidence="0.973825111111111">
distribution.
The task of probabilistic modeling with log-
linear distributions is to build salient proper-
ties of the data as property-functions \x17i into
the probability model. For a given vector \x17 of
property-functions, the task of statistical in-
ference is to tune the parameters \x15 to best
re\x1dect the empirical distribution of the train-
ing data.
</bodyText>
<subsectionHeader confidence="0.969391">
2.2 Incomplete-Data Estimation
</subsectionHeader>
<bodyText confidence="0.998081076923077">
Standard numerical methods for statis-
tical inference of log-linear models from
fully annotated data\x16so-called complete
data\x16are the iterative scaling meth-
ods of Darroch and Ratcli\x1b (1972) and
Della Pietra et al. (1997). For data consisting
of unannotated sentences\x16so-called incom-
plete data\x16the iterative method of the EM
algorithm (Dempster et al., 1977) has to be
employed. However, since even complete-data
estimation for log-linear models requires
iterative methods, an application of EM to
log-linear models results in an algorithm
which is expensive since it is doubly-iterative.
A singly-iterative algorithm interleaving EM
and iterative scaling into a mathematically
well-de\x1cned estimation method for log-linear
models from incomplete data is the IM
algorithm of Riezler (1999). Applying this
algorithm to stochastic constraint-based
grammars, we assume the following to be
given: A training sample of unannotated sen-
tences y from a set Y, observed with empirical
\x0cInput Reference model p0, property-functions vector \x17 with constant \x17#, parses
X(y) for each y in incomplete-data sample from Y.
Output MLE model p\x15\x03 on X.
</bodyText>
<subsubsectionHeader confidence="0.736378">
Procedure
</subsubsectionHeader>
<bodyText confidence="0.941364333333333">
Until convergence do
Compute p\x15; k\x15, based on \x15 = (\x151; : : : ; \x15n),
For i from 1 to n do
</bodyText>
<equation confidence="0.9982125">
i := 1
\x17# ln
Py2Y ~
p(y)Px2X(y)k\x15(xjy)\x17i(x)
Px2X p\x15(x)\x17i(x) ,
\x15i := \x15i +
i,
Return \x15\x03 = (\x151; : : : ; \x15n).
</equation>
<figureCaption confidence="0.714657">
Figure 1: Closed-form version of IM algorithm
probability ~
p(y), a constraint-based grammar
</figureCaption>
<bodyText confidence="0.954311">
yielding a set X(y) of parses for each sentence
y, and a log-linear model p\x15(\x01) on the parses
</bodyText>
<equation confidence="0.971287">
X =
P
y2Yj~
p(y)&amp;gt;0 X(y) for the sentences in
</equation>
<bodyText confidence="0.9546746">
the training corpus, with known values of
property-functions \x17 and unknown values
of \x15. The aim of incomplete-data maximum
likelihood estimation (MLE) is to \x1cnd a value
\x15\x03 that maximizes the incomplete-data log-
</bodyText>
<equation confidence="0.9484878">
likelihood L =
P
y2Y ~
p(y)ln
P
x2X(y) p\x15(x),
i.e.,
\x15\x03 = argmax
\x152IRn
L(\x15):
</equation>
<bodyText confidence="0.650162">
Closed-form parameter-updates for this prob-
lem can be computed by the algorithm of Fig.
</bodyText>
<equation confidence="0.9874595">
1, where \x17#(x) =
Pn
i=1 \x17i(x), and k\x15(xjy) =
p\x15(x)=
P
x2X(y) p\x15(x) is the conditional prob-
</equation>
<bodyText confidence="0.9655134">
ability of a parse x given the sentence y and
the current parameter value \x15.
The constancy requirement on \x17# can be
enforced by adding a \x10correction\x11 property-
function \x17l:
</bodyText>
<equation confidence="0.9914538">
Choose K = maxx2X \x17#(x) and
\x17l(x) = K ;\x17#(x) for all x 2 X.
Then
Pl
i=1 \x17i(x) = K for all x 2 X.
</equation>
<bodyText confidence="0.997912285714286">
Note that because of the restriction of X to
the parses obtainable by a grammar from the
training corpus, we have a log-linear probabil-
ity measure only on those parses and not on
all possible parses of the grammar. We shall
therefore speak of mere log-linear measures in
our application of disambiguation.
</bodyText>
<subsectionHeader confidence="0.93401">
2.3 Searching for Order in Chaos
</subsectionHeader>
<bodyText confidence="0.997262352941176">
For incomplete-data estimation, a sequence
of likelihood values is guaranteed to converge
to a critical point of the likelihood function
L. This is shown for the IM algorithm in
Riezler (1999). The process of \x1cnding likeli-
hood maxima is chaotic in that the \x1cnal likeli-
hood value is extremely sensitive to the start-
ing values of \x15, i.e. limit points can be lo-
cal maxima (or saddlepoints), which are not
necessarily also global maxima. A way to
search for order in this chaos is to search for
starting values which are hopefully attracted
by the global maximum of L. This problem
can best be explained in terms of the mini-
mum divergence paradigm (Kullback, 1959),
which is equivalent to the maximum likeli-
hood paradigm by the following theorem. Let
</bodyText>
<equation confidence="0.992951333333333">
p[f] =
P
x2X p(x)f(x) be the expectation of
</equation>
<bodyText confidence="0.990208571428571">
a function f with respect to a distribution p:
The probability distribution p\x03 that
minimizes the divergence D(pjjp0) to
a reference model p0 subject to the
constraints p[\x17i] = q[\x17i]; i = 1; : : : ; n
is the model in the parametric fam-
ily of log-linear distributions p\x15 that
</bodyText>
<equation confidence="0.418676">
maximizes the likelihood L(\x15) =
q[lnp\x15] of the training data1
.
1
</equation>
<bodyText confidence="0.736282666666667">
If the training sample consists of complete data
\x0cReasonable starting values for minimum di-
vergence estimation is to set \x15i = 0 for
</bodyText>
<equation confidence="0.73716">
i = 1; : : : ; n. This yields a distribution which
</equation>
<bodyText confidence="0.988813714285714">
minimizes the divergence to p0, over the
set of models p to which the constraints
p[\x17i] = q[\x17i]; i = 1; : : : ; n have yet to be ap-
plied. Clearly, this argument applies to both
complete-data and incomplete-data estima-
tion. Note that for a uniformly distributed
reference model p0, the minimum divergence
model is a maximum entropy model (Jaynes,
1957). In Sec. 4, we will demonstrate that
a uniform initialization of the IM algorithm
shows a signi\x1ccant improvement in likelihood
maximization as well as in linguistic perfor-
mance when compared to standard random
initialization.
</bodyText>
<sectionHeader confidence="0.618042" genericHeader="method">
3 Property Design and
</sectionHeader>
<subsectionHeader confidence="0.7843665">
Lexicalization
3.1 Basic Con\x1cgurational Properties
</subsectionHeader>
<bodyText confidence="0.761176285714286">
The basic 190 properties employed in our
models are similar to the properties of
Johnson et al. (1999) which incorporate gen-
eral linguistic principles into a log-linear
model. They refer to both the c(onstituent)-
structure and the f(eature)-structure of the
LFG parses. Examples are properties for
\x0f c-structure nodes, corresponding to stan-
dard production properties,
\x0f c-structure subtrees, indicating argument
versus adjunct attachment,
\x0f f-structure attributes, corresponding to
grammatical functions used in LFG,
\x0f atomic attribute-value pairs in f-
structures,
\x0f complexity of the phrase being attached
to, thus indicating both high and low at-
tachment,
\x0f non-right-branching behavior of nonter-
minal nodes,
\x0f non-parallelism of coordinations.
x 2 X , the expectation q[\x01] corresponds to the em-
pirical expectation ~
p[\x01]. If we observe incomplete data
y 2 Y, the expectation q[\x01] is replaced by the condi-
tional expectation ~
p[k\x150 [\x01]] given the observed data y
and the current parameter value \x150.
</bodyText>
<subsectionHeader confidence="0.984945">
3.2 Class-Based Lexicalization
</subsectionHeader>
<bodyText confidence="0.999473">
Our approach to grammar lexicalization is
class-based in the sense that we use class-
based estimated frequencies fc(v; n) of head-
verbs v and argument head-nouns n in-
stead of pure frequency statistics or class-
based probabilities of head word dependen-
cies. Class-based estimated frequencies are in-
troduced in Prescher et al. (2000) as the fre-
quency f(v; n) of a (v; n)-pair in the train-
ing corpus, weighted by the best estimate of
the class-membership probability p(cjv; n) of
an EM-based clustering model on (v; n)-pairs,
</bodyText>
<equation confidence="0.995132">
i.e., fc(v; n) = max
c2C
p(cjv; n)(f(v; n) + 1).
</equation>
<bodyText confidence="0.995274166666667">
As is shown in Prescher et al. (2000) in an
evaluation on lexical ambiguity resolution, a
gain of about 7% can be obtained by using
the class-based estimated frequency fc(v; n)
as disambiguation criterion instead of class-
based probabilities p(njv). In order to make
the most direct use possible of this fact, we
incorporated the decisions of the disambigua-
tor directly into 45 additional properties for
the grammatical relations of the subject, di-
rect object, indirect object, in\x1cnitival object,
oblique and adjunctival dative and accusative
preposition, for active and passive forms of the
\x1crst three verbs in each parse. Let vr(x) be the
verbal head of grammatical relation r in parse
x, and nr(x) the nominal head of grammatical
relation r in x. Then a lexicalized property \x17r
for grammatical relation r is de\x1cned as
</bodyText>
<equation confidence="0.989966428571429">
\x17r(x) =
8
&amp;lt;
:
1 if fc(vr(x); nr(x)) \x15
fc(vr(x0); nr(x0)) 8x0 2 X(y);
0 otherwise:
</equation>
<bodyText confidence="0.978080555555555">
The property-function \x17r thus pre-
disambiguates the parses x 2 X(y) of a
sentence y according to fc(v; n), and stores
the best parse directly instead of taking the
actual estimated frequencies as its value. In
Sec. 4, we will see that an incorporation of
this pre-disambiguation routine into the mod-
els improves performance in disambiguation
by about 10%.
</bodyText>
<figure confidence="0.85328272">
\x0cexact match
evaluation
basic
model
lexicalized
model
selected
+ lexicalized
model
complete-data
estimation
P: 68
E: 59.6
P: 73.9
E: 71.6
P: 74.3
E: 71.8
incomplete-data
estimation
P: 73
E: 65.4
P: 86
E: 85.2
P: 86.1
E: 85.4
</figure>
<figureCaption confidence="0.982162">
Figure 2: Evaluation on exact match task for 550 examples with average ambiguity 5.4
</figureCaption>
<figure confidence="0.78926904">
frame match
evaluation
basic
model
lexicalized
model
selected
+ lexicalized
model
complete-data
estimation
P: 80.6
E: 70.4
P: 82.7
E: 76.4
P: 83.4
E: 76
incomplete-data
estimation
P: 84.5
E: 73.1
P: 88.5
E: 84.9
P: 90
E: 86.3
</figure>
<figureCaption confidence="0.995679">
Figure 3: Evaluation on frame match task for 375 examples with average ambiguity 25
</figureCaption>
<sectionHeader confidence="0.992518" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999552">
4.1 Incomplete Data and Parsebanks
</subsectionHeader>
<bodyText confidence="0.995357888888889">
In our experiments, we used an LFG grammar
for German2
for parsing unrestricted text.
Since training was faster than parsing, we
parsed in advance and stored the resulting
packed c/f-structures. The low ambiguity rate
of the German LFG grammar allowed us to
restrict the training data to sentences with
at most 20 parses. The resulting training cor-
pus of unannotated, incomplete data consists
of approximately 36,000 sentences of online
available German newspaper text, comprising
approximately 250,000 parses.
In order to compare the contribution of un-
ambiguous and ambiguous sentences to the es-
timation results, we extracted a subcorpus of
4,000 sentences, for which the LFG grammar
produced a unique parse, from the full train-
</bodyText>
<page confidence="0.965246">
2
</page>
<bodyText confidence="0.996456884615385">
The German LFG grammar is being imple-
mented in the Xerox Linguistic Environment (XLE,
see Maxwell and Kaplan (1996)) as part of the Paral-
lel Grammar (ParGram) project at the IMS Stuttgart.
The coverage of the grammar is about 50% for unre-
stricted newspaper text. For the experiments reported
here, the e\x1bective coverage was lower, since the cor-
pus preprocessing we applied was minimal. Note that
for the disambiguation task we were interested in,
the overall grammar coverage was of subordinate rel-
evance.
ing corpus. The average sentence length of
7.5 for this automatically constructed parse-
bank is only slightly smaller than that of
10.5 for the full set of 36,000 training sen-
tences and 250,000 parses. Thus, we conjec-
ture that the parsebank includes a representa-
tive variety of linguistic phenomena. Estima-
tion from this automatically disambiguated
parsebank enjoys the same complete-data es-
timation properties3
as training from manu-
ally disambiguated treebanks. This makes a
comparison of complete-data estimation from
this parsebank to incomplete-data estimation
from the full set of training data interesting.
</bodyText>
<subsectionHeader confidence="0.999737">
4.2 Test Data and Evaluation Tasks
</subsectionHeader>
<bodyText confidence="0.998912666666667">
To evaluate our models, we constructed
two di\x1berent test corpora. We \x1crst parsed
with the LFG grammar 550 sentences
which are used for illustrative purposes in
the foreign language learner&amp;apos;s grammar of
Helbig and Buscha (1996). In a next step, the
correct parse was indicated by a human dis-
ambiguator, according to the reading intended
in Helbig and Buscha (1996). Thus a precise
</bodyText>
<page confidence="0.985451">
3
</page>
<bodyText confidence="0.981940774193548">
For example, convergence to the global maximum
of the complete-data log-likelihood function is guar-
anteed, which is a good condition for highly precise
statistical disambiguation.
\x0cindication of correct c/f-structure pairs was
possible. However, the average ambiguity of
this corpus is only 5.4 parses per sentence, for
sentences with on average 7.5 words. In order
to evaluate on sentences with higher ambigu-
ity rate, we manually disambiguated further
375 sentences of LFG-parsed newspaper text.
The sentences of this corpus have on average
25 parses and 11.2 words.
We tested our models on two evalua-
tion tasks. The statistical disambiguator was
tested on an \x10exact match\x11 task, where ex-
act correspondence of the full c/f-structure
pair of the hand-annotated correct parse and
the most probable parse is checked. Another
evaluation was done on a \x10frame match\x11 task,
where exact correspondence only of the sub-
categorization frame of the main verb of the
most probable parse and the correct parse is
checked. Clearly, the latter task involves a
smaller e\x1bective ambiguity rate, and is thus
to be interpreted as an evaluation of the com-
bined system of highly-constrained symbolic
parsing and statistical disambiguation.
Performance on these two evaluation tasks
was assessed according to the following evalu-
ation measures:
</bodyText>
<figure confidence="0.829936571428571">
Precision = #correct
#correct+#incorrect
,
E\x1bectiveness = #correct
#correct+#incorrect+#don&amp;apos;t know
.
\x10Correct\x11 and \x10incorrect\x11 speci\x1ces a suc-
</figure>
<bodyText confidence="0.99935525">
cess/failure on the respective evaluation tasks;
\x10don&amp;apos;t know\x11 cases are cases where the system
is unable to make a decision, i.e. cases with
more than one most probable parse.
</bodyText>
<subsectionHeader confidence="0.998157">
4.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.996831754098361">
For each task and each test corpus, we cal-
culated a random baseline by averaging over
several models with randomly chosen pa-
rameter values. This baseline measures the
disambiguation power of the pure symbolic
parser. The results of an exact-match evalu-
ation on the Helbig-Buscha corpus is shown
in Fig. 2. The random baseline was around
33% for this case. The columns list di\x1berent
models according to their property-vectors.
\x10Basic\x11 models consist of 190 con\x1cgurational
properties as described in Sec. 3.1. \x10Lexical-
ized\x11 models are extended by 45 lexical pre-
disambiguation properties as described in Sec.
3.2. \x10Selected + lexicalized\x11 models result
from a simple property selection procedure
where a cuto\x1b on the number of parses with
non-negative value of the property-functions
was set. Estimation of basic models from com-
plete data gave 68% precision (P), whereas
training lexicalized and selected models from
incomplete data gave 86.1% precision, which
is an improvement of 18%. Comparing lex-
icalized models in the estimation method
shows that incomplete-data estimation gives
an improvement of 12% precision over train-
ing from the parsebank. A comparison of mod-
els trained from incomplete data shows that
lexicalization yields a gain of 13% in preci-
sion. Note also the gain in e\x1bectiveness (E)
due to the pre-disambigution routine included
in the lexicalized properties. The gain due to
property selection both in precision and e\x1bec-
tiveness is minimal. A similar pattern of per-
formance arises in an exact match evaluation
on the newspaper corpus with an ambiguity
rate of 25. The lexicalized and selected model
trained from incomplete data achieved here
60.1% precision and 57.9% e\x1bectiveness, for a
random baseline of around 17%.
As shown in Fig. 3, the improvement in per-
formance due to both lexicalization and EM
training is smaller for the easier task of frame
evaluation. Here the random baseline is 70%
for frame evaluation on the newspaper corpus
with an ambiguity rate of 25. An overall gain
of roughly 10% can be achieved by going from
unlexicalized parsebank models (80.6% preci-
sion) to lexicalized EM-trained models (90%
precision). Again, the contribution to this im-
provement is about the same for lexicalization
and incomplete-data training. Applying the
same evaluation to the Helbig-Buscha corpus
shows 97.6% precision and 96.7% e\x1bectiveness
for the lexicalized and selected incomplete-
data model, compared to around 80% for the
random baseline.
Optimal iteration numbers were decided by
repeated evaluation of the models at every
\x1cfth iteration. Fig. 4 shows the precision of
lexicalized and selected models on the exact
</bodyText>
<figure confidence="0.984621375">
\x0c68
70
72
74
76
78
80
82
84
86
88
10 20 30 40 50 60 70 80 90
precision
number of iterations
complete-data estimation
incomplete-data estimation
</figure>
<figureCaption confidence="0.999966">
Figure 4: Precision on exact match task in number of training iterations
</figureCaption>
<bodyText confidence="0.962630823529412">
match task plotted against the number of it-
erations of the training algorithm. For parse-
bank training, the maximal precision value
is obtained at 35 iterations. Iterating fur-
ther shows a clear overtraining e\x1bect. For
incomplete-data estimation more iterations
are necessary to reach a maximal precision
value. A comparison of models with random
or uniform starting values shows an increase
in precision of 10% to 40% for the latter.
In terms of maximization of likelihood, this
corresponds to the fact that uniform starting
values immediately push the likelihood up to
nearly its \x1cnal value, whereas random starting
values yield an initial likelihood which has to
be increased by factors of 2 to 20 to an often
lower \x1cnal value.
</bodyText>
<sectionHeader confidence="0.998418" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.998962314285714">
The most direct points of compar-
ison of our method are the ap-
proaches of Johnson et al. (1999) and
Johnson and Riezler (2000). In the \x1crst ap-
proach, log-linear models on LFG grammars
using about 200 con\x1cgurational properties
were trained on treebanks of about 400
sentences by maximum pseudo-likelihood
estimation. Precision was evaluated on an
exact match task in a 10-way cross valida-
tion paradigm for an ambiguity rate of 10,
and achieved 59% for the \x1crst approach.
Johnson and Riezler (2000) achieved a gain
of 1% over this result by including a class-
based lexicalization. Our best models clearly
outperform these results, both in terms of
precision relative to ambiguity and in terms
of relative gain due to lexicalization. A
comparison of performance is more di\x1ecult
for the lexicalized PCFG of Beil et al. (1999)
which was trained by EM on 450,000 sen-
tences of German newspaper text. There, a
70.4% precision is reported on a verb frame
recognition task on 584 examples. However,
the gain achieved by Beil et al. (1999) due to
grammar lexicalizaton is only 2%, compared
to about 10% in our case. A comparison
is di\x1ecult also for most other state-of-the-
art PCFG-based statistical parsers, since
di\x1berent training and test data, and most
importantly, di\x1berent evaluation criteria were
used. A comparison of the performance gain
due to grammar lexicalization shows that our
results are on a par with that reported in
Charniak (1997).
</bodyText>
<sectionHeader confidence="0.997685" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999478909090909">
We have presented a new approach to stochas-
tic modeling of constraint-based grammars.
Our experimental results show that EM train-
ing can in fact be very helpful for accurate
stochastic modeling in natural language pro-
cessing. We conjecture that this result is due
partly to the fact that the space of parses
produced by a constraint-based grammar is
only \x10mildly incomplete\x11, i.e. the ambiguity
rate can be kept relatively low. Another rea-
son may be that EM is especially useful for
log-linear models, where the search space in
maximization can be kept under control. Fur-
thermore, we have introduced a new class-
\x0cbased grammar lexicalization, which again
uses EM training and incorporates a pre-
disambiguation routine into log-linear models.
An impressive gain in performance could also
be demonstrated for this method. Clearly, a
central task of future work is a further explo-
ration of the relation between complete-data
and incomplete-data estimation for larger,
manually disambiguated treebanks. An inter-
esting question is whether a systematic vari-
ation of training data size along the lines
of the EM-experiments of Nigam et al. (2000)
for text classi\x1ccation will show similar results,
namely a systematic dependence of the rela-
tive gain due to EM training from the relative
sizes of unannotated and annotated data. Fur-
thermore, it is important to show that EM-
based methods can be applied successfully
also to other statistical parsing frameworks.
</bodyText>
<sectionHeader confidence="0.962701" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999">
We thank Stefanie Dipper and Bettina
Schrader for help with disambiguation of the
test suites, and the anonymous ACL review-
ers for helpful suggestions. This research was
supported by the ParGram project and the
project B7 of the SFB 340 of the DFG.
</bodyText>
<sectionHeader confidence="0.991581" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997527871428572">
Franz Beil, Glenn Carroll, Detlef Prescher, Stefan
Riezler, and Mats Rooth. 1999. Inside-outside
estimation of a lexicalized PCFG for German.
In Proceedings of the 37th ACL, College Park,
MD.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
Proceedings of the 14th AAAI, Menlo Park, CA.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th ACL, Madrid.
J.N. Darroch and D. Ratcli\x1b. 1972. General-
ized iterative scaling for log-linear models. The
Annals of Mathematical Statistics, 43(5):1470\x15
1480.
Stephen Della Pietra, Vincent Della Pietra, and
John La\x1berty. 1997. Inducing features of ran-
dom \x1celds. IEEE PAMI, 19(4):380\x15393.
A. P. Dempster, N. M. Laird, and D. B. Ru-
bin. 1977. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of
the Royal Statistical Society, 39(B):1\x1538.
David Elworthy. 1994. Does Baum-Welch re-
estimation help taggers? In Proceedings of the
4th ANLP, Stuttgart.
Gerhard Helbig and Joachim Buscha. 1996.
Deutsche Grammatik. Ein Handbuch fur den
Auslanderunterricht. Langenscheidt, Leipzig.
Edwin T. Jaynes. 1957. Information theory
and statistical mechanics. Physical Review,
106:620\x15630.
Mark Johnson and Stefan Riezler. 2000. Ex-
ploiting auxiliary distributions in stochastic
uni\x1ccation-based grammars. In Proceedings of
the 1st NAACL, Seattle, WA.
Mark Johnson, Stuart Geman, Stephen Canon,
Zhiyi Chi, and Stefan Riezler. 1999. Estimators
for stochastic \x10uni\x1ccation-based\x11 grammars. In
Proceedings of the 37th ACL, CollegePark, MD.
Solomon Kullback. 1959. Information Theory and
Statistics. Wiley, New York.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Build-
ing a large annotated corpus of english: The
Penn treebank. Computational Linguistics,
19(2):313\x15330.
John Maxwell and R. Kaplan. 1996. Uni\x1ccation-
based parsers that automatically take ad-
vantage of context freeness. Unpublished
manuscript, Xerox Palo Alto Research Center.
Kamal Nigam, Andrew McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classi-
\x1ccation from labeled and unlabeled documents
using EM. Machine Learning, 39(2/4):103\x15134.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed
corpora. In Proceedings of the 30th ACL,
Newark, Delaware.
Detlef Prescher, Stefan Riezler, and Mats Rooth.
2000. Using a probabilistic class-based lexicon
for lexical ambiguity resolution. In Proceedings
of the 18th COLING, Saarbrucken.
Adwait Ratnaparkhi. 1997. A linear observed
time statistical parser based on maximum en-
tropy models. In Proceedings of EMNLP-2.
Stefan Riezler. 1999. Probabilistic Constraint
Logic Programming Ph.D. thesis, Seminar
fur Sprachwissenschaft, Universitat Tubingen.
AIMS Report, 5(1), IMS, Universitat Stuttgart.
\x0c&quot;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.584356">
<title confidence="0.9889575">b&quot;Lexicalized Stochastic Modeling of Constraint-Based Grammars using Log-Linear Measures and EM Training</title>
<author confidence="0.999697">Stefan Riezler</author>
<affiliation confidence="0.99423">IMS, Universitat Stuttgart</affiliation>
<email confidence="0.98734">riezler@ims.uni-stuttgart.de</email>
<author confidence="0.663381">Detlef Prescher</author>
<affiliation confidence="0.939667">IMS, Universitat Stuttgart</affiliation>
<email confidence="0.968838">prescher@ims.uni-stuttgart.de</email>
<author confidence="0.999965">Jonas Kuhn</author>
<affiliation confidence="0.996597">IMS, Universitat Stuttgart</affiliation>
<email confidence="0.982874">jonas@ims.uni-stuttgart.de</email>
<author confidence="0.999985">Mark Johnson</author>
<affiliation confidence="0.999358">Cog. &amp; Ling. Sciences, Brown University</affiliation>
<email confidence="0.99106">Mark_Johnson@brown.edu</email>
<abstract confidence="0.999109666666667">We present a new approach to stochastic modeling of constraintbased grammars that is based on loglinear models and uses EM for estimation from unannotated data. The techniques are applied to an LFG grammar for German. Evaluation on an exact match task yields 86% precision for an ambiguity rate of 5.4, and 90% precision on a subcat frame match for an ambiguity rate of 25. Experimental comparison to training from a parsebank shows a 10% gain from EM training. Also, a new class-based grammar lexicalization is presented, showing a 10% gain over unlexicalized models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Franz Beil</author>
<author>Glenn Carroll</author>
<author>Detlef Prescher</author>
<author>Stefan Riezler</author>
<author>Mats Rooth</author>
</authors>
<title>Inside-outside estimation of a lexicalized PCFG for German.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th ACL,</booktitle>
<location>College Park, MD.</location>
<contexts>
<context position="23124" citStr="Beil et al. (1999)" startWordPosition="3667" endWordPosition="3670">tional properties were trained on treebanks of about 400 sentences by maximum pseudo-likelihood estimation. Precision was evaluated on an exact match task in a 10-way cross validation paradigm for an ambiguity rate of 10, and achieved 59% for the \x1crst approach. Johnson and Riezler (2000) achieved a gain of 1% over this result by including a classbased lexicalization. Our best models clearly outperform these results, both in terms of precision relative to ambiguity and in terms of relative gain due to lexicalization. A comparison of performance is more di\x1ecult for the lexicalized PCFG of Beil et al. (1999) which was trained by EM on 450,000 sentences of German newspaper text. There, a 70.4% precision is reported on a verb frame recognition task on 584 examples. However, the gain achieved by Beil et al. (1999) due to grammar lexicalizaton is only 2%, compared to about 10% in our case. A comparison is di\x1ecult also for most other state-of-theart PCFG-based statistical parsers, since di\x1berent training and test data, and most importantly, di\x1berent evaluation criteria were used. A comparison of the performance gain due to grammar lexicalization shows that our results are on a par with that r</context>
</contexts>
<marker>Beil, Carroll, Prescher, Riezler, Rooth, 1999</marker>
<rawString>Franz Beil, Glenn Carroll, Detlef Prescher, Stefan Riezler, and Mats Rooth. 1999. Inside-outside estimation of a lexicalized PCFG for German. In Proceedings of the 37th ACL, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th AAAI,</booktitle>
<location>Menlo Park, CA.</location>
<contexts>
<context position="1471" citStr="Charniak (1997)" startWordPosition="205" endWordPosition="206">ing. Also, a new class-based grammar lexicalization is presented, showing a 10% gain over unlexicalized models. 1 Introduction Stochastic parsing models capturing contextual constraints beyond the dependencies of probabilistic context-free grammars (PCFGs) are currently the subject of intensive research. An interesting feature common to most such models is the incorporation of contextual dependencies on individual head words into rulebased probability models. Such word-based lexicalizations of probability models are used successfully in the statistical parsing models of, e.g., Collins (1997), Charniak (1997), or Ratnaparkhi (1997). However, it is still an open question which kind of lexicalization, e.g., statistics on individual words or statistics based upon word classes, is the best choice. Secondly, these approaches have in common the fact that the probability models are trained on treebanks, i.e., corpora of manually disambiguated sentences, and not from corpora of unannotated sentences. In all of the cited approaches, the Penn Wall Street Journal Treebank (Marcus et al., 1993) is used, the availability of which obviates the standard e\x1bort required for treebank training\x16handannotating l</context>
<context position="23750" citStr="Charniak (1997)" startWordPosition="3771" endWordPosition="3772">trained by EM on 450,000 sentences of German newspaper text. There, a 70.4% precision is reported on a verb frame recognition task on 584 examples. However, the gain achieved by Beil et al. (1999) due to grammar lexicalizaton is only 2%, compared to about 10% in our case. A comparison is di\x1ecult also for most other state-of-theart PCFG-based statistical parsers, since di\x1berent training and test data, and most importantly, di\x1berent evaluation criteria were used. A comparison of the performance gain due to grammar lexicalization shows that our results are on a par with that reported in Charniak (1997). 6 Conclusion We have presented a new approach to stochastic modeling of constraint-based grammars. Our experimental results show that EM training can in fact be very helpful for accurate stochastic modeling in natural language processing. We conjecture that this result is due partly to the fact that the space of parses produced by a constraint-based grammar is only \x10mildly incomplete\x11, i.e. the ambiguity rate can be kept relatively low. Another reason may be that EM is especially useful for log-linear models, where the search space in maximization can be kept under control. Furthermore</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the 14th AAAI, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th ACL,</booktitle>
<location>Madrid.</location>
<contexts>
<context position="1454" citStr="Collins (1997)" startWordPosition="203" endWordPosition="204">in from EM training. Also, a new class-based grammar lexicalization is presented, showing a 10% gain over unlexicalized models. 1 Introduction Stochastic parsing models capturing contextual constraints beyond the dependencies of probabilistic context-free grammars (PCFGs) are currently the subject of intensive research. An interesting feature common to most such models is the incorporation of contextual dependencies on individual head words into rulebased probability models. Such word-based lexicalizations of probability models are used successfully in the statistical parsing models of, e.g., Collins (1997), Charniak (1997), or Ratnaparkhi (1997). However, it is still an open question which kind of lexicalization, e.g., statistics on individual words or statistics based upon word classes, is the best choice. Secondly, these approaches have in common the fact that the probability models are trained on treebanks, i.e., corpora of manually disambiguated sentences, and not from corpora of unannotated sentences. In all of the cited approaches, the Penn Wall Street Journal Treebank (Marcus et al., 1993) is used, the availability of which obviates the standard e\x1bort required for treebank training\x1</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th ACL, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcli\x1b</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>43</volume>
<issue>5</issue>
<pages>1480</pages>
<marker>Darroch, Ratcli\x1b, 1972</marker>
<rawString>J.N. Darroch and D. Ratcli\x1b. 1972. Generalized iterative scaling for log-linear models. The Annals of Mathematical Statistics, 43(5):1470\x15 1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John La\x1berty</author>
</authors>
<title>Inducing features of random \x1celds.</title>
<date>1997</date>
<journal>IEEE PAMI,</journal>
<volume>19</volume>
<issue>4</issue>
<marker>Pietra, Pietra, La\x1berty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John La\x1berty. 1997. Inducing features of random \x1celds. IEEE PAMI, 19(4):380\x15393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<contexts>
<context position="2292" citStr="Dempster et al., 1977" startWordPosition="326" endWordPosition="329">these approaches have in common the fact that the probability models are trained on treebanks, i.e., corpora of manually disambiguated sentences, and not from corpora of unannotated sentences. In all of the cited approaches, the Penn Wall Street Journal Treebank (Marcus et al., 1993) is used, the availability of which obviates the standard e\x1bort required for treebank training\x16handannotating large corpora of speci\x1cc domains of speci\x1cc languages with speci\x1cc parse types. Moreover, common wisdom is that training from unannotated data via the expectationmaximization (EM) algorithm (Dempster et al., 1977) yields poor results unless at least partial annotation is applied. Experimental results con\x1crming this wisdom have been presented, e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs. In this paper, we present a new lexicalized stochastic model for constraint-based grammars that employs a combination of headword frequencies and EM-based clustering for grammar lexicalization. Furthermore, we make crucial use of EM for estimating the parameters of the stochastic grammar from unannotated data. Our usage of EM was initiated by the current l</context>
<context position="6631" citStr="Dempster et al., 1977" startWordPosition="1017" endWordPosition="1020">nctions \x17i into the probability model. For a given vector \x17 of property-functions, the task of statistical inference is to tune the parameters \x15 to best re\x1dect the empirical distribution of the training data. 2.2 Incomplete-Data Estimation Standard numerical methods for statistical inference of log-linear models from fully annotated data\x16so-called complete data\x16are the iterative scaling methods of Darroch and Ratcli\x1b (1972) and Della Pietra et al. (1997). For data consisting of unannotated sentences\x16so-called incomplete data\x16the iterative method of the EM algorithm (Dempster et al., 1977) has to be employed. However, since even complete-data estimation for log-linear models requires iterative methods, an application of EM to log-linear models results in an algorithm which is expensive since it is doubly-iterative. A singly-iterative algorithm interleaving EM and iterative scaling into a mathematically well-de\x1cned estimation method for log-linear models from incomplete data is the IM algorithm of Riezler (1999). Applying this algorithm to stochastic constraint-based grammars, we assume the following to be given: A training sample of unannotated sentences y from a set Y, obse</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(B):1\x1538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Elworthy</author>
</authors>
<title>Does Baum-Welch reestimation help taggers?</title>
<date>1994</date>
<booktitle>In Proceedings of the 4th ANLP,</booktitle>
<location>Stuttgart.</location>
<contexts>
<context position="2451" citStr="Elworthy (1994)" startWordPosition="351" endWordPosition="352"> of unannotated sentences. In all of the cited approaches, the Penn Wall Street Journal Treebank (Marcus et al., 1993) is used, the availability of which obviates the standard e\x1bort required for treebank training\x16handannotating large corpora of speci\x1cc domains of speci\x1cc languages with speci\x1cc parse types. Moreover, common wisdom is that training from unannotated data via the expectationmaximization (EM) algorithm (Dempster et al., 1977) yields poor results unless at least partial annotation is applied. Experimental results con\x1crming this wisdom have been presented, e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs. In this paper, we present a new lexicalized stochastic model for constraint-based grammars that employs a combination of headword frequencies and EM-based clustering for grammar lexicalization. Furthermore, we make crucial use of EM for estimating the parameters of the stochastic grammar from unannotated data. Our usage of EM was initiated by the current lack of large uni\x1ccationbased treebanks for German. However, our experimental results also show an exception to the common wisdom of the insu\x1eciency of EM</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>David Elworthy. 1994. Does Baum-Welch reestimation help taggers? In Proceedings of the 4th ANLP, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Helbig</author>
<author>Joachim Buscha</author>
</authors>
<title>Deutsche Grammatik. Ein Handbuch fur den Auslanderunterricht.</title>
<date>1996</date>
<location>Langenscheidt, Leipzig.</location>
<contexts>
<context position="16776" citStr="Helbig and Buscha (1996)" startWordPosition="2670" endWordPosition="2673">esentative variety of linguistic phenomena. Estimation from this automatically disambiguated parsebank enjoys the same complete-data estimation properties3 as training from manually disambiguated treebanks. This makes a comparison of complete-data estimation from this parsebank to incomplete-data estimation from the full set of training data interesting. 4.2 Test Data and Evaluation Tasks To evaluate our models, we constructed two di\x1berent test corpora. We \x1crst parsed with the LFG grammar 550 sentences which are used for illustrative purposes in the foreign language learner&amp;apos;s grammar of Helbig and Buscha (1996). In a next step, the correct parse was indicated by a human disambiguator, according to the reading intended in Helbig and Buscha (1996). Thus a precise 3 For example, convergence to the global maximum of the complete-data log-likelihood function is guaranteed, which is a good condition for highly precise statistical disambiguation. \x0cindication of correct c/f-structure pairs was possible. However, the average ambiguity of this corpus is only 5.4 parses per sentence, for sentences with on average 7.5 words. In order to evaluate on sentences with higher ambiguity rate, we manually disambigua</context>
</contexts>
<marker>Helbig, Buscha, 1996</marker>
<rawString>Gerhard Helbig and Joachim Buscha. 1996. Deutsche Grammatik. Ein Handbuch fur den Auslanderunterricht. Langenscheidt, Leipzig.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edwin T Jaynes</author>
</authors>
<title>Information theory and statistical mechanics. Physical Review,</title>
<date>1957</date>
<contexts>
<context position="10762" citStr="Jaynes, 1957" startWordPosition="1719" endWordPosition="1720">izes the likelihood L(\x15) = q[lnp\x15] of the training data1 . 1 If the training sample consists of complete data \x0cReasonable starting values for minimum divergence estimation is to set \x15i = 0 for i = 1; : : : ; n. This yields a distribution which minimizes the divergence to p0, over the set of models p to which the constraints p[\x17i] = q[\x17i]; i = 1; : : : ; n have yet to be applied. Clearly, this argument applies to both complete-data and incomplete-data estimation. Note that for a uniformly distributed reference model p0, the minimum divergence model is a maximum entropy model (Jaynes, 1957). In Sec. 4, we will demonstrate that a uniform initialization of the IM algorithm shows a signi\x1ccant improvement in likelihood maximization as well as in linguistic performance when compared to standard random initialization. 3 Property Design and Lexicalization 3.1 Basic Con\x1cgurational Properties The basic 190 properties employed in our models are similar to the properties of Johnson et al. (1999) which incorporate general linguistic principles into a log-linear model. They refer to both the c(onstituent)- structure and the f(eature)-structure of the LFG parses. Examples are properties</context>
</contexts>
<marker>Jaynes, 1957</marker>
<rawString>Edwin T. Jaynes. 1957. Information theory and statistical mechanics. Physical Review, 106:620\x15630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stefan Riezler</author>
</authors>
<title>Exploiting auxiliary distributions in stochastic uni\x1ccation-based grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st NAACL,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="22418" citStr="Johnson and Riezler (2000)" startWordPosition="3554" endWordPosition="3557">e iterations are necessary to reach a maximal precision value. A comparison of models with random or uniform starting values shows an increase in precision of 10% to 40% for the latter. In terms of maximization of likelihood, this corresponds to the fact that uniform starting values immediately push the likelihood up to nearly its \x1cnal value, whereas random starting values yield an initial likelihood which has to be increased by factors of 2 to 20 to an often lower \x1cnal value. 5 Discussion The most direct points of comparison of our method are the approaches of Johnson et al. (1999) and Johnson and Riezler (2000). In the \x1crst approach, log-linear models on LFG grammars using about 200 con\x1cgurational properties were trained on treebanks of about 400 sentences by maximum pseudo-likelihood estimation. Precision was evaluated on an exact match task in a 10-way cross validation paradigm for an ambiguity rate of 10, and achieved 59% for the \x1crst approach. Johnson and Riezler (2000) achieved a gain of 1% over this result by including a classbased lexicalization. Our best models clearly outperform these results, both in terms of precision relative to ambiguity and in terms of relative gain due to lex</context>
</contexts>
<marker>Johnson, Riezler, 2000</marker>
<rawString>Mark Johnson and Stefan Riezler. 2000. Exploiting auxiliary distributions in stochastic uni\x1ccation-based grammars. In Proceedings of the 1st NAACL, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic \x10uni\x1ccation-based\x11 grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th ACL, CollegePark, MD.</booktitle>
<contexts>
<context position="3396" citStr="Johnson et al. (1999)" startWordPosition="501" endWordPosition="504">mating the parameters of the stochastic grammar from unannotated data. Our usage of EM was initiated by the current lack of large uni\x1ccationbased treebanks for German. However, our experimental results also show an exception to the common wisdom of the insu\x1eciency of EM for highly accurate statistical modeling. Our approach to lexicalized stochastic modeling is based on the parametric family of loglinear probability models, which is used to de\x1cne a probability distribution on the parses of a Lexical-Functional Grammar (LFG) for German. In previous work on log-linear models for LFG by Johnson et al. (1999), pseudo\x0clikelihood estimation from annotated corpora has been introduced and experimented with on a small scale. However, to our knowledge, to date no large LFG annotated corpora of unrestricted German text are available. Fortunately, algorithms exist for statistical inference of log-linear models from unannotated data (Riezler, 1999). We apply this algorithm to estimate log-linear LFG models from large corpora of newspaper text. In our largest experiment, we used 250,000 parses which were produced by parsing 36,000 newspaper sentences with the German LFG. Experimental evaluation of our mo</context>
<context position="11170" citStr="Johnson et al. (1999)" startWordPosition="1778" endWordPosition="1781">Clearly, this argument applies to both complete-data and incomplete-data estimation. Note that for a uniformly distributed reference model p0, the minimum divergence model is a maximum entropy model (Jaynes, 1957). In Sec. 4, we will demonstrate that a uniform initialization of the IM algorithm shows a signi\x1ccant improvement in likelihood maximization as well as in linguistic performance when compared to standard random initialization. 3 Property Design and Lexicalization 3.1 Basic Con\x1cgurational Properties The basic 190 properties employed in our models are similar to the properties of Johnson et al. (1999) which incorporate general linguistic principles into a log-linear model. They refer to both the c(onstituent)- structure and the f(eature)-structure of the LFG parses. Examples are properties for \x0f c-structure nodes, corresponding to standard production properties, \x0f c-structure subtrees, indicating argument versus adjunct attachment, \x0f f-structure attributes, corresponding to grammatical functions used in LFG, \x0f atomic attribute-value pairs in fstructures, \x0f complexity of the phrase being attached to, thus indicating both high and low attachment, \x0f non-right-branching behav</context>
<context position="22387" citStr="Johnson et al. (1999)" startWordPosition="3549" endWordPosition="3552">mplete-data estimation more iterations are necessary to reach a maximal precision value. A comparison of models with random or uniform starting values shows an increase in precision of 10% to 40% for the latter. In terms of maximization of likelihood, this corresponds to the fact that uniform starting values immediately push the likelihood up to nearly its \x1cnal value, whereas random starting values yield an initial likelihood which has to be increased by factors of 2 to 20 to an often lower \x1cnal value. 5 Discussion The most direct points of comparison of our method are the approaches of Johnson et al. (1999) and Johnson and Riezler (2000). In the \x1crst approach, log-linear models on LFG grammars using about 200 con\x1cgurational properties were trained on treebanks of about 400 sentences by maximum pseudo-likelihood estimation. Precision was evaluated on an exact match task in a 10-way cross validation paradigm for an ambiguity rate of 10, and achieved 59% for the \x1crst approach. Johnson and Riezler (2000) achieved a gain of 1% over this result by including a classbased lexicalization. Our best models clearly outperform these results, both in terms of precision relative to ambiguity and in te</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic \x10uni\x1ccation-based\x11 grammars. In Proceedings of the 37th ACL, CollegePark, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Kullback</author>
</authors>
<title>Information Theory and Statistics.</title>
<date>1959</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="9726" citStr="Kullback, 1959" startWordPosition="1534" endWordPosition="1535">od values is guaranteed to converge to a critical point of the likelihood function L. This is shown for the IM algorithm in Riezler (1999). The process of \x1cnding likelihood maxima is chaotic in that the \x1cnal likelihood value is extremely sensitive to the starting values of \x15, i.e. limit points can be local maxima (or saddlepoints), which are not necessarily also global maxima. A way to search for order in this chaos is to search for starting values which are hopefully attracted by the global maximum of L. This problem can best be explained in terms of the minimum divergence paradigm (Kullback, 1959), which is equivalent to the maximum likelihood paradigm by the following theorem. Let p[f] = P x2X p(x)f(x) be the expectation of a function f with respect to a distribution p: The probability distribution p\x03 that minimizes the divergence D(pjjp0) to a reference model p0 subject to the constraints p[\x17i] = q[\x17i]; i = 1; : : : ; n is the model in the parametric family of log-linear distributions p\x15 that maximizes the likelihood L(\x15) = q[lnp\x15] of the training data1 . 1 If the training sample consists of complete data \x0cReasonable starting values for minimum divergence estimat</context>
</contexts>
<marker>Kullback, 1959</marker>
<rawString>Solomon Kullback. 1959. Information Theory and Statistics. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1954" citStr="Marcus et al., 1993" startWordPosition="280" endWordPosition="283">d lexicalizations of probability models are used successfully in the statistical parsing models of, e.g., Collins (1997), Charniak (1997), or Ratnaparkhi (1997). However, it is still an open question which kind of lexicalization, e.g., statistics on individual words or statistics based upon word classes, is the best choice. Secondly, these approaches have in common the fact that the probability models are trained on treebanks, i.e., corpora of manually disambiguated sentences, and not from corpora of unannotated sentences. In all of the cited approaches, the Penn Wall Street Journal Treebank (Marcus et al., 1993) is used, the availability of which obviates the standard e\x1bort required for treebank training\x16handannotating large corpora of speci\x1cc domains of speci\x1cc languages with speci\x1cc parse types. Moreover, common wisdom is that training from unannotated data via the expectationmaximization (EM) algorithm (Dempster et al., 1977) yields poor results unless at least partial annotation is applied. Experimental results con\x1crming this wisdom have been presented, e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs. In this paper, we pr</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The Penn treebank. Computational Linguistics, 19(2):313\x15330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Maxwell</author>
<author>R Kaplan</author>
</authors>
<title>Uni\x1ccationbased parsers that automatically take advantage of context freeness.</title>
<date>1996</date>
<institution>Xerox Palo Alto Research Center.</institution>
<note>Unpublished manuscript,</note>
<contexts>
<context position="15503" citStr="Maxwell and Kaplan (1996)" startWordPosition="2473" endWordPosition="2476">the German LFG grammar allowed us to restrict the training data to sentences with at most 20 parses. The resulting training corpus of unannotated, incomplete data consists of approximately 36,000 sentences of online available German newspaper text, comprising approximately 250,000 parses. In order to compare the contribution of unambiguous and ambiguous sentences to the estimation results, we extracted a subcorpus of 4,000 sentences, for which the LFG grammar produced a unique parse, from the full train2 The German LFG grammar is being implemented in the Xerox Linguistic Environment (XLE, see Maxwell and Kaplan (1996)) as part of the Parallel Grammar (ParGram) project at the IMS Stuttgart. The coverage of the grammar is about 50% for unrestricted newspaper text. For the experiments reported here, the e\x1bective coverage was lower, since the corpus preprocessing we applied was minimal. Note that for the disambiguation task we were interested in, the overall grammar coverage was of subordinate relevance. ing corpus. The average sentence length of 7.5 for this automatically constructed parsebank is only slightly smaller than that of 10.5 for the full set of 36,000 training sentences and 250,000 parses. Thus,</context>
</contexts>
<marker>Maxwell, Kaplan, 1996</marker>
<rawString>John Maxwell and R. Kaplan. 1996. Uni\x1ccationbased parsers that automatically take advantage of context freeness. Unpublished manuscript, Xerox Palo Alto Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Text classi\x1ccation from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning, 39(2/4):103\x15134.</booktitle>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew McCallum, Sebastian Thrun, and Tom Mitchell. 2000. Text classi\x1ccation from labeled and unlabeled documents using EM. Machine Learning, 39(2/4):103\x15134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Insideoutside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th ACL,</booktitle>
<location>Newark, Delaware.</location>
<contexts>
<context position="2482" citStr="Pereira and Schabes (1992)" startWordPosition="354" endWordPosition="357">ences. In all of the cited approaches, the Penn Wall Street Journal Treebank (Marcus et al., 1993) is used, the availability of which obviates the standard e\x1bort required for treebank training\x16handannotating large corpora of speci\x1cc domains of speci\x1cc languages with speci\x1cc parse types. Moreover, common wisdom is that training from unannotated data via the expectationmaximization (EM) algorithm (Dempster et al., 1977) yields poor results unless at least partial annotation is applied. Experimental results con\x1crming this wisdom have been presented, e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs. In this paper, we present a new lexicalized stochastic model for constraint-based grammars that employs a combination of headword frequencies and EM-based clustering for grammar lexicalization. Furthermore, we make crucial use of EM for estimating the parameters of the stochastic grammar from unannotated data. Our usage of EM was initiated by the current lack of large uni\x1ccationbased treebanks for German. However, our experimental results also show an exception to the common wisdom of the insu\x1eciency of EM for highly accurate statistica</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. Insideoutside reestimation from partially bracketed corpora. In Proceedings of the 30th ACL, Newark, Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detlef Prescher</author>
<author>Stefan Riezler</author>
<author>Mats Rooth</author>
</authors>
<title>Using a probabilistic class-based lexicon for lexical ambiguity resolution.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th COLING, Saarbrucken.</booktitle>
<contexts>
<context position="12466" citStr="Prescher et al. (2000)" startWordPosition="1970" endWordPosition="1973">he expectation q[\x01] corresponds to the empirical expectation ~ p[\x01]. If we observe incomplete data y 2 Y, the expectation q[\x01] is replaced by the conditional expectation ~ p[k\x150 [\x01]] given the observed data y and the current parameter value \x150. 3.2 Class-Based Lexicalization Our approach to grammar lexicalization is class-based in the sense that we use classbased estimated frequencies fc(v; n) of headverbs v and argument head-nouns n instead of pure frequency statistics or classbased probabilities of head word dependencies. Class-based estimated frequencies are introduced in Prescher et al. (2000) as the frequency f(v; n) of a (v; n)-pair in the training corpus, weighted by the best estimate of the class-membership probability p(cjv; n) of an EM-based clustering model on (v; n)-pairs, i.e., fc(v; n) = max c2C p(cjv; n)(f(v; n) + 1). As is shown in Prescher et al. (2000) in an evaluation on lexical ambiguity resolution, a gain of about 7% can be obtained by using the class-based estimated frequency fc(v; n) as disambiguation criterion instead of classbased probabilities p(njv). In order to make the most direct use possible of this fact, we incorporated the decisions of the disambiguator</context>
</contexts>
<marker>Prescher, Riezler, Rooth, 2000</marker>
<rawString>Detlef Prescher, Stefan Riezler, and Mats Rooth. 2000. Using a probabilistic class-based lexicon for lexical ambiguity resolution. In Proceedings of the 18th COLING, Saarbrucken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP-2.</booktitle>
<contexts>
<context position="1494" citStr="Ratnaparkhi (1997)" startWordPosition="208" endWordPosition="209">ss-based grammar lexicalization is presented, showing a 10% gain over unlexicalized models. 1 Introduction Stochastic parsing models capturing contextual constraints beyond the dependencies of probabilistic context-free grammars (PCFGs) are currently the subject of intensive research. An interesting feature common to most such models is the incorporation of contextual dependencies on individual head words into rulebased probability models. Such word-based lexicalizations of probability models are used successfully in the statistical parsing models of, e.g., Collins (1997), Charniak (1997), or Ratnaparkhi (1997). However, it is still an open question which kind of lexicalization, e.g., statistics on individual words or statistics based upon word classes, is the best choice. Secondly, these approaches have in common the fact that the probability models are trained on treebanks, i.e., corpora of manually disambiguated sentences, and not from corpora of unannotated sentences. In all of the cited approaches, the Penn Wall Street Journal Treebank (Marcus et al., 1993) is used, the availability of which obviates the standard e\x1bort required for treebank training\x16handannotating large corpora of speci\x</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of EMNLP-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
</authors>
<title>Probabilistic Constraint Logic Programming Ph.D. thesis, Seminar fur Sprachwissenschaft, Universitat Tubingen.</title>
<date>1999</date>
<journal>AIMS Report,</journal>
<volume>5</volume>
<issue>1</issue>
<institution>IMS, Universitat Stuttgart.</institution>
<note>x0c&quot;</note>
<contexts>
<context position="3736" citStr="Riezler, 1999" startWordPosition="552" endWordPosition="553">stochastic modeling is based on the parametric family of loglinear probability models, which is used to de\x1cne a probability distribution on the parses of a Lexical-Functional Grammar (LFG) for German. In previous work on log-linear models for LFG by Johnson et al. (1999), pseudo\x0clikelihood estimation from annotated corpora has been introduced and experimented with on a small scale. However, to our knowledge, to date no large LFG annotated corpora of unrestricted German text are available. Fortunately, algorithms exist for statistical inference of log-linear models from unannotated data (Riezler, 1999). We apply this algorithm to estimate log-linear LFG models from large corpora of newspaper text. In our largest experiment, we used 250,000 parses which were produced by parsing 36,000 newspaper sentences with the German LFG. Experimental evaluation of our models on an exact-match task (i.e. percentage of exact match of most probable parse with correct parse) on 550 manually examined examples with on average 5.4 analyses gave 86% precision. Another evaluation on a verb frame recognition task (i.e. percentage of agreement between subcategorization frames of main verb of most probable parse and</context>
<context position="7064" citStr="Riezler (1999)" startWordPosition="1079" endWordPosition="1080"> (1972) and Della Pietra et al. (1997). For data consisting of unannotated sentences\x16so-called incomplete data\x16the iterative method of the EM algorithm (Dempster et al., 1977) has to be employed. However, since even complete-data estimation for log-linear models requires iterative methods, an application of EM to log-linear models results in an algorithm which is expensive since it is doubly-iterative. A singly-iterative algorithm interleaving EM and iterative scaling into a mathematically well-de\x1cned estimation method for log-linear models from incomplete data is the IM algorithm of Riezler (1999). Applying this algorithm to stochastic constraint-based grammars, we assume the following to be given: A training sample of unannotated sentences y from a set Y, observed with empirical \x0cInput Reference model p0, property-functions vector \x17 with constant \x17#, parses X(y) for each y in incomplete-data sample from Y. Output MLE model p\x15\x03 on X. Procedure Until convergence do Compute p\x15; k\x15, based on \x15 = (\x151; : : : ; \x15n), For i from 1 to n do i := 1 \x17# ln Py2Y ~ p(y)Px2X(y)k\x15(xjy)\x17i(x) Px2X p\x15(x)\x17i(x) , \x15i := \x15i + i, Return \x15\x03 = (\x151; : : </context>
<context position="9249" citStr="Riezler (1999)" startWordPosition="1450" endWordPosition="1451"> and \x17l(x) = K ;\x17#(x) for all x 2 X. Then Pl i=1 \x17i(x) = K for all x 2 X. Note that because of the restriction of X to the parses obtainable by a grammar from the training corpus, we have a log-linear probability measure only on those parses and not on all possible parses of the grammar. We shall therefore speak of mere log-linear measures in our application of disambiguation. 2.3 Searching for Order in Chaos For incomplete-data estimation, a sequence of likelihood values is guaranteed to converge to a critical point of the likelihood function L. This is shown for the IM algorithm in Riezler (1999). The process of \x1cnding likelihood maxima is chaotic in that the \x1cnal likelihood value is extremely sensitive to the starting values of \x15, i.e. limit points can be local maxima (or saddlepoints), which are not necessarily also global maxima. A way to search for order in this chaos is to search for starting values which are hopefully attracted by the global maximum of L. This problem can best be explained in terms of the minimum divergence paradigm (Kullback, 1959), which is equivalent to the maximum likelihood paradigm by the following theorem. Let p[f] = P x2X p(x)f(x) be the expecta</context>
</contexts>
<marker>Riezler, 1999</marker>
<rawString>Stefan Riezler. 1999. Probabilistic Constraint Logic Programming Ph.D. thesis, Seminar fur Sprachwissenschaft, Universitat Tubingen. AIMS Report, 5(1), IMS, Universitat Stuttgart. \x0c&quot;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>