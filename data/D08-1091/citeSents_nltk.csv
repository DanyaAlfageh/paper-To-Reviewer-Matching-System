At testing time we marginalize out the hidden structure and extract the tree with the highest number of expected correct productions, as in CITATION.,,
Set Test Set ENGLISH-WSJ Sections Section 22 Section 23 CITATION 2-21 ENGLISH-BROWN see 10% of 10% of the (Francis et al.,,
2002) ENGLISH-WSJ the data6 the data6 FRENCH7 Sentences Sentences Sentences CITATION 1-18,609 18,610-19,609 19,609-20,610 GERMAN Sentences Sentences Sentences CITATION 1-18,602 18,603-19,602 19,603-20,602 Table 1: Corpora and standard experimental setups.,,
We compare to a baseline of discriminatively trained latent variable grammars CITATION.,,
For those comparisons we use the grammars from CITATION.,,
As in CITATION, we start with a simple X-bar grammar from an input treebank.,,
The parameters of the grammar (production log-weights for now) are estimated in a log-linear framework by maximizing the penalized log conditional likelihood Lcond R(), where: Lcond() = log Y i P(Ti|wi) R() = X r |r| We directly optimize this non-convex objective function using a numerical gradient based method (LBFGS CITATION in our implementation).,,
To handle the non-diferentiability of the L1-regularization term R() we use the orthant-wise method of CITATION.,,
As we will show below, these expectations can be computed exactly using marginals from the chart of the inside/outside algorithm CITATION.,,
Note that the span features improve the performance of the unsplit baseline grammar by 8%, but not surprisingly their contribution 6 See CITATION for the exact setup.,,
7 This setup contains only sentences without annotation errors, as in CITATION.,,
0c 40 words all Parser F1 EX F1 EX ENGLISH-WSJ CITATION 88.8 35.7 88.3 33.1 Charniak et al.,,
(2005) 90.3 39.6 89.7 37.2 CITATION 90.6 39.1 90.1 37.1 This work w/o span features 89.7 39.6 89.2 37.2 This work w/ span features 90.0 40.1 89.4 37.7 ENGLISH-WSJ (reranked) CITATION 92.3 46.2 91.7 43.5 ENGLISH-BROWN Charniak et al.,,
(2005) 84.5 34.8 82.9 31.7 CITATION 84.9 34.5 83.7 31.2 This work w/o span features 85.3 35.6 84.3 32.1 This work w/ span features 85.6 35.8 84.5 32.3 ENGLISH-BROWN (reranked) Charniak et al.,,
(2005) 86.8 39.9 85.2 37.8 FRENCH CITATION 79.2 21.2 75.6 16.4 This Paper 80.1 24.2 77.2 19.2 GERMAN CITATION 80.8 40.8 80.1 39.1 This Paper 81.5 45.2 80.7 43.9 Table 2: Our final test set parsing accuracies compared to the best previous work on English, French and German.,,
Discriminative parsing has been investigated before, such as in CITATION, CITATION, CITATION, Koo and Collins (2005), CITATION, Finkel et al.,,
(2008), and, most similarly, in CITATION.,,
Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (CITATION; CITATION).,,
6.2 Span Features There are many features beyond local tree configurations which can enhance parsing discrimination; CITATION presents a varied list.,,
In reranking, one can incorporate any such features, of course, but even in our dynamic programming approach it is possible to include features that decompose along the dynamic program structure, as shown by CITATION.,,
We use non-local span features, which condition on properties of input spans CITATION.,,
On WSJ-English, the discriminative grammars perform on par with the generative grammars of CITATION, falling slightly short in terms of F1, but having a higher exact match score.,,
All those methods fall short of reranking parsers like CITATION and CITATION, which, however, have access to many additional features, that cannot be used in our dynamic program.,,
To make this computation practical on large data sets, we use the same approach as CITATION.,,
Therein, the idea of coarse-to-fine parsing CITATION is extended to handle the repeated parsing of the same sentences.,,
7.3 Efficiency CITATION demonstrates how the idea of coarse-to-fine parsing (CITATION; CITATION) can be used in the context of latent variable models.,,
7.3 Efficiency CITATION demonstrates how the idea of coarse-to-fine parsing (CITATION; CITATION) can be used in the context of latent variable models.,,
1 Introduction In latent variable approaches to parsing (CITATION; Petrov et al., 2006), one models an observed treebank of coarse parse trees using a grammar over more refined, but unobserved, derivation trees.,,
In recent years, latent variable methods have been shown to produce grammars which are as good as, or even better than, earlier parsing work (CITATION; CITATION).,,
In particular, in CITATION we exhibited a very accurate category-splitting approach, in which a coarse initial grammar is refined by iteratively splitting each grammar category into two subcategories using the EM algorithm.,,
lity of the discriminative framework both to improve the treatment of unknown words as well as to include span features CITATION, giving the benefit of some input features integrally in our dynamic program.,,
Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of CITATION.,,
Discriminative parsing has been investigated before, such as in CITATION, CITATION, CITATION, Koo and Collins (2005), CITATION, Finkel et al.,,
(2008), and, most similarly, in CITATION.,,
Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (CITATION; CITATION).,,
1 Introduction In latent variable approaches to parsing (CITATION; Petrov et al., 2006), one models an observed treebank of coarse parse trees using a grammar over more refined, but unobserved, derivation trees.,,
In recent years, latent variable methods have been shown to produce grammars which are as good as, or even better than, earlier parsing work (CITATION; CITATION).,,
In particular, in CITATION we exhibited a very accurate category-splitting approach, in which a coarse initial grammar is refined by iteratively splitting each grammar category into two subcategories using the EM algorithm.,,
Typically, a classification of rare words into word classes is used CITATION.,,
The features with the highest negative weight involve single commas: x[x,x+], and x[x+,x+]x and so on (indeed, such spans were structurally disallowed by the CITATION parser).,,
Note that the span features improve the performance of the unsplit baseline grammar by 8%, but not surprisingly their contribution 6 See CITATION for the exact setup.,,
7 This setup contains only sentences without annotation errors, as in CITATION.,,
e framework both to improve the treatment of unknown words as well as to include span features CITATION, giving the benefit of some input features integrally in our dynamic program.,,
Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of CITATION.,,
Discriminative parsing has been investigated before, such as in CITATION, CITATION, CITATION, Koo and Collins (2005), CITATION, Finkel et al.,,
(2008), and, most similarly, in CITATION.,,
Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (CITATION; CITATION).,,
Discriminative parsing has been investigated before, such as in CITATION, CITATION, CITATION, Koo and Collins (2005), CITATION, Finkel et al.,,
(2008), and, most similarly, in CITATION.,,
Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (CITATION; CITATION).,,
Compared to the generative parser of CITATION, parsing with multi-scale grammars requires the evaluation of 29% fewer productions, decreasing the average parsing time per sentence by 36% to 0.36 sec/sentence.,,
873 \x0c 40 words all Parser F1 EX F1 EX ENGLISH-WSJ CITATION 88.8 35.7 88.3 33.1 Charniak et al.,,
(2005) 90.3 39.6 89.7 37.2 CITATION 90.6 39.1 90.1 37.1 This work w/o span features 89.7 39.6 89.2 37.2 This work w/ span features 90.0 40.1 89.4 37.7 ENGLISH-WSJ (reranked) CITATION 92.3 46.2 91.7 43.5 ENGLISH-BROWN Charniak et al.,,
(2005) 84.5 34.8 82.9 31.7 CITATION 84.9 34.5 83.7 31.2 This work w/o span features 85.3 35.6 84.3 32.1 This work w/ span features 85.6 35.8 84.5 32.3 ENGLISH-BROWN (reranked) Charniak et al.,,
(2005) 86.8 39.9 85.2 37.8 FRENCH CITATION 79.2 21.2 75.6 16.4 This Paper 80.1 24.2 77.2 19.2 GERMAN CITATION 80.8 40.8 80.1 39.1 This Paper 81.5 45.2 80.7 43.9 Table 2: Our final test set parsing accuracies compared to the best previous work on English, French and German.,,
On WSJ-English, the discriminative grammars perform on par with the generative grammars of CITATION, falling slightly short in terms of F1, but having a higher exact match score.,,
All those methods fall short of reranking parsers like CITATION and CITATION, which, however, have access to many additional features, that cannot be used in our dynamic program.,,
A manual approach might take the category NP and subdivide it into one subcategory NPS for subjects and another subcategory NPVP for objects (CITATION; CITATION).,,
loit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features CITATION, giving the benefit of some input features integrally in our dynamic program.,,
Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of CITATION.,,
Discriminative parsing has been investigated before, such as in CITATION, CITATION, CITATION, Koo and Collins (2005), CITATION, Finkel et al.,,
(2008), and, most similarly, in CITATION.,,
Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (CITATION; CITATION).,,
ined productions Ax By Cz, where Ax is a subcategory of A, By of B, and Cz of C, can then be estimated in various ways; past work has included both generative (CITATION; CITATION) and discriminative approaches CITATION.,,
Note that the comparison is only between estimation methods, as CITATION show that the model classes are the same.,,
2.2 Log-Linear Latent Variable Grammars In a log-linear latent variable grammar, each production r = Ax By Cz is associated with a multiplicative weight r (CITATION; CITATION) (sometimes we will use the log-weight r when convenient).,,
A manual approach might take the category NP and subdivide it into one subcategory NPS for subjects and another subcategory NPVP for objects (CITATION; CITATION).,,
We use the general framework of hidden variable CRFs (CITATION; CITATION), where gradient-based optimization maximizes the likelihood of the observed variables, here parse trees, summing over log-linearly scored derivations.,,
Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features CITATION, giving the benefit of some input features integrally in our,,
We use the general framework of hidden variable CRFs (CITATION; CITATION), where gradient-based optimization maximizes the likelihood of the observed variables, here parse trees, summing over log-linearly scored derivations.,,
Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features CITATION, giving the benefit of some input fe,,
tiability of the L1-regularization term R() we use the orthant-wise method of CITATION.,,
As we will show below, these expectations can be computed exactly using marginals from the chart of the inside/outside algorithm CITATION.,,
The inside/outside algorithm CITATION is an efficient dynamic program for summing over derivations under a context-free grammar.,,
The parameters of the refined productions Ax By Cz, where Ax is a subcategory of A, By of B, and Cz of C, can then be estimated in various ways; past work has included both generative (CITATION; CITATION) and discriminative approaches CITATION.,,
Note that the comparison is only between estimation methods, as CITATION show that the model classes are the same.,,
2.2 Log-Linear Latent Variable Grammars In a log-linear latent variable grammar, each production r = Ax By Cz is associated with a multiplicative weight r (CITATION; CITATION) (sometimes we will use the log-weight r when convenient).,,
At testing time we marginalize out the hidden structure and extract the tree with the highest number of expected correct productions, as in CITATION.,,
Set Test Set ENGLISH-WSJ Sections Section 22 Section 23 CITATION 2-21 ENGLISH-BROWN see 10% of 10% of the (Francis et al.,,
2002) ENGLISH-WSJ the data6 the data6 FRENCH7 Sentences Sentences Sentences CITATION 1-18,609 18,610-19,609 19,609-20,610 GERMAN Sentences Sentences Sentences CITATION 1-18,602 18,603-19,602 19,603-20,602 Table 1: Corpora and standard experimental setups.,,
We compare to a baseline of discriminatively trained latent variable grammars CITATION.,,
grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of CITATION.,,
1 Introduction In latent variable approaches to parsing (CITATION; Petrov et al., 2006), one models an observed treebank of coarse parse trees using a grammar over more refined, but unobserved, derivation trees.,,
In recent years, latent variable methods have been shown to produce grammars which are as good as, or even better than, earlier parsing work (CITATION; CITATION).,,
In particular, in CITATION we exhibited a very accurate category-splitting approach, in which a coarse ini,,
The parameters of the refined productions Ax By Cz, where Ax is a subcategory of A, By of B, and Cz of C, can then be estimated in various ways; past work has included both generative (CITATION; CITATION) and discriminative approaches CITATION.,,
Note that the comparison is only between estimation methods, as CITATION show that the model classes are the same.,,
2.2 Log-Linear Latent Variable Grammars In a log-linear latent variable grammar, each production r = Ax By Cz is associated with a multiplicative weight r (CITATION; CITATION) (sometimes we will use the log-weight r when convenient).,,
In the present work, we exploit L1-regularization, though other techniques such as structural zeros CITATION could also potentially be used.,,
As in CITATION, we start with a simple X-bar grammar from an input treebank.,,
As in CITATION, we start with a simple X-bar grammar from an input treebank.,,
The parameters of the grammar (production log-weights for now) are estimated in a log-linear framework by maximizing the penalized log conditional likelihood Lcond R(), where: Lcond() = log Y i P(Ti|wi) R() = X r |r| We directly optimize this non-convex objective function using a numerical gradient based method (LBFGS CITATION in our implementation).,,
To handle the non-diferentiability of the L1-regularization term R() we use the orthant-wise method of CITATION.,,
As in CITATION, we arrange our subcategories into a hierarchy, as shown in Figure 1.,,
We will use x x to indicate that the subscript or subcategory x is a refinement of x.1 We 1 Conversely, x is a coarser version of x, or, in the language of CITATION, x is a projection of x.,,
At testing time we marginalize out the hidden structure and extract the tree with the highest number of expected correct productions, as in CITATION.,,
Set Test Set ENGLISH-WSJ Sections Section 22 Section 23 CITATION 2-21 ENGLISH-BROWN see 10% of 10% of the (Francis et al.,,
2002) ENGLISH-WSJ the data6 the data6 FRENCH7 Sentences Sentences Sentences CITATION 1-18,609 18,610-19,609 19,609-20,610 GERMAN Sentences Sentences Sentences CITATION 1-18,602 18,603-19,602 19,603-20,602 Table 1: Corpora and standard experimental setups.,,
7.3 Efficiency CITATION demonstrates how the idea of coarse-to-fine parsing (CITATION; CITATION) can be used in the context of latent variable models.,,
Compared to the generative parser of CITATION, parsing with multi-scale grammars requires the evaluation of 29% fewer productions, decreasing the average parsing time per sentence by 36% to 0.36 sec/sentence.,,
873 \x0c 40 words all Parser F1 EX F1 EX ENGLISH-WSJ CITATION 88.8 35.7 88.3 33.1 Charniak et al.,,
(2005) 90.3 39.6 89.7 37.2 CITATION 90.6 39.1 90.1 37.1 This work w/o span features 89.7 39.6 89.2 37.2 This work w/ span features 90.0 40.1 89.4 37.7 ENGLISH-WSJ (reranked) CITATION 92.3 46.2 91.7 43.5 ENGLISH-BROWN Charniak et al.,,
(2005) 84.5 34.8 82.9 31.7 CITATION 84.9 34.5 83.7 31.2 This work w/o span features 85.3 35.6 84.3 32.1 This work w/ span features 85.6 35.8 84.5 32.3 ENGLISH-BROWN (reranked) Charniak et al.,,
(2005) 86.8 39.9 85.2 37.8 FRENCH CITATION 79.2 21.2 75.6 16.4 This Paper 80.1 24.2 77.2 19.2 GERMAN CITATION 80.8 40.8 80.1 39.1 This Paper 81.5 45.2 80.7 43.9 T,,
Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of CITATION.,,
Discriminative parsing has been investigated before, such as in CITATION, CITATION, CITATION, Koo and Collins (2005), CITATION, Finkel et al.,,
(2008), and, most similarly, in CITATION.,,
Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (CITATION; CITATION).,,
The parameters of the refined productions Ax By Cz, where Ax is a subcategory of A, By of B, and Cz of C, can then be estimated in various ways; past work has included both generative (CITATION; CITATION) and discriminative approaches CITATION.,,
Note that the comparison is only between estimation methods, as CITATION show that the model classes are the same.,,
2.2 Log-Linear Latent Variable Grammars In a log-linear latent variable grammar, each production r = Ax By Cz is associated with a multiplicative weight r (CITATION; CITATION) (sometimes we will use the log-weight r when convenient).,,
We define these marginals 4 These scores lack any probabilistic interpretation, but can be normalized to compute the necessary expectations for training CITATION.,,
To make this computation practical on large data sets, we use the same approach as CITATION.,,
Therein, the idea of coarse-to-fine parsing CITATION is extended to handle the repeated parsing of the same sentences.,,
Set Test Set ENGLISH-WSJ Sections Section 22 Section 23 CITATION 2-21 ENGLISH-BROWN see 10% of 10% of the (Francis et al.,,
2002) ENGLISH-WSJ the data6 the data6 FRENCH7 Sentences Sentences Sentences CITATION 1-18,609 18,610-19,609 19,609-20,610 GERMAN Sentences Sentences Sentences CITATION 1-18,602 18,603-19,602 19,603-20,602 Table 1: Corpora and standard experimental setups.,,
We compare to a baseline of discriminatively trained latent variable grammars CITATION.,,
For those comparisons we use the grammars from CITATION.,,
Compared to the generative parser of CITATION, parsing with multi-scale grammars requires the evaluation of 29% fewer productions, decreasing the average parsing time per sentence by 36% to 0.36 sec/sentence.,,
873 \x0c 40 words all Parser F1 EX F1 EX ENGLISH-WSJ CITATION 88.8 35.7 88.3 33.1 Charniak et al.,,
(2005) 90.3 39.6 89.7 37.2 CITATION 90.6 39.1 90.1 37.1 This work w/o span features 89.7 39.6 89.2 37.2 This work w/ span features 90.0 40.1 89.4 37.7 ENGLISH-WSJ (reranked) CITATION 92.3 46.2 91.7 43.5 ENGLISH-BROWN Charniak et al.,,
(2005) 84.5 34.8 82.9 31.7 CITATION 84.9 34.5 83.7 31.2 This work w/o span features 85.3 35.6 84.3 32.1 This work w/ span features 85.6 35.8 84.5 32.3 ENGLISH-BROWN (reranked) Charniak et al.,,
(2005) 86.8 39.9 85.2 37.8 FRENCH CITATION 79.2 21.2 75.6 16.4 This Paper 80.1 24.2 77.2 1,,
Different regions of the grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of CITATION.,,
1 Introduction In latent variable approaches to parsing (CITATION; Petrov et al., 2006), one models an observed treebank of coarse parse trees using a grammar over more refined, but unobserved, derivation trees.,,
Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features CITATION, giving the benefit of some input features integrally in our dynamic program.,,
Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of CITATION.,,
Discriminative parsing has been investigated before, such as in CITATION, CITATION, CITATION, Koo and Collins (2005), CITATION, Finkel et al.,,
(2008), and, most similarly, in CITATION.,,
The resulting memory limitations alone can prevent the practical learning of highly split grammars CITATION.,,
This issue was partially addressed in CITATION, where categories were repeatedly split and some splits were re-merged if the gains were too small.,,
As in CITATION, we arrange our subcategories into a hierarchy, as shown in Figure 1.,,
In the present work, we exploit L1-regularization, though other techniques such as structural zeros CITATION could also potentially be used.,,
As in CITATION, we start with a simple X-bar grammar from an input treebank.,,
The parameters of the grammar (production log-weights for now) are estimated in a log-linear framework by maximizing the penalized log conditional likelihood Lcond R(), where: Lcond() = log Y i P(Ti|wi) R() = X r |r| We directly optimize this non-convex objective function using a numerical gradient based method (LBFGS CITATION in our implementation).,,
To handle the non-diferentiability of the L1-regularization term R() we use the orthant-wise method of CITATION.,,
16.4 This Paper 80.1 24.2 77.2 19.2 GERMAN CITATION 80.8 40.8 80.1 39.1 This Paper 81.5 45.2 80.7 43.9 Table 2: Our final test set parsing accuracies compared to the best previous work on English, French and German.,,
On WSJ-English, the discriminative grammars perform on par with the generative grammars of CITATION, falling slightly short in terms of F1, but having a higher exact match score.,,
All those methods fall short of reranking parsers like CITATION and CITATION, which, however, have access to many additional features, that cannot be used in our dynamic program.,,
At testing time we marginalize out the hidden structure and extract the tree with the highest number of expected correct productions, as in CITATION.,,
Set Test Set ENGLISH-WSJ Sections Section 22 Section 23 CITATION 2-21 ENGLISH-BROWN see 10% of 10% of the (Francis et al.,,
2002) ENGLISH-WSJ the data6 the data6 FRENCH7 Sentences Sentences Sentences CITATION 1-18,609 18,610-19,609 19,609-20,610 GERMAN Sentences Sentences Sentences CITATION 1-18,602 18,603-19,602 19,603-20,602 Table 1: Corpora and standard experimental setups.,,
We compare to a baseline of discriminatively trained latent variable grammars CITATION.,,
For those comparisons we use the grammars from CITATION.,,
The parameters of the refined productions Ax By Cz, where Ax is a subcategory of A, By of B, and Cz of C, can then be estimated in various ways; past work has included both generative (CITATION; CITATION) and discriminative approaches CITATION.,,
Note that the comparison is only between estimation methods, as CITATION show that the model classes are the same.,,
2.2 Log-Linear Latent Variable Grammars In a log-linear latent variable grammar, each production r = Ax By Cz is associated with a multiplicative weight r (CITATION; CITATION) (sometimes we will use the log-weight r when convenient).,,
 framework of hidden variable CRFs (CITATION; CITATION), where gradient-based optimization maximizes the likelihood of the observed variables, here parse trees, summing over log-linearly scored derivations.,,
Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features CITATION, giving the benefit of some input features integrally in our dynamic program.,,
Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of CITATION.,,
Discriminative parsing has been investigated before, such as in CITATION, CITATION, CITATION, Koo and Collins (2005), CITATION, Finkel et al.,,
6.2 Span Features There are many features beyond local tree configurations which can enhance parsing discrimination; CITATION presents a varied list.,,
In reranking, one can incorporate any such features, of course, but even in our dynamic programming approach it is possible to include features that decompose along the dynamic program structure, as shown by CITATION.,,
We use non-local span features, which condition on properties of input spans CITATION.,,
of unknown words as well as to include span features CITATION, giving the benefit of some input features integrally in our dynamic program.,,
Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of CITATION.,,
Discriminative parsing has been investigated before, such as in CITATION, CITATION, CITATION, Koo and Collins (2005), CITATION, Finkel et al.,,
(2008), and, most similarly, in CITATION.,,
Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (CITATION; CITATION).,,
