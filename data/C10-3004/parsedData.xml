<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<address confidence="0.2319015">
b&apos;Coling 2010: Demonstration Volume, pages 1316,
Beijing, August 2010
</address>
<title confidence="0.534811">
LTP: A Chinese Language Technology Platform
</title>
<author confidence="0.69422">
Wanxiang Che, Zhenghua Li, Ting Liu
</author>
<affiliation confidence="0.82719675">
Research Center for Information Retrieval
MOE-Microsoft Key Laboratory of Natural Language Processing and Speech
School of Computer Science and Technology
Harbin Institute of Technology
</affiliation>
<email confidence="0.930818">
{car, lzh, tliu}@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.878677" genericHeader="abstract">
Abstract
LTP (Language Technology Platform) is
</sectionHeader>
<bodyText confidence="0.996181642857143">
an integrated Chinese processing platform
which includes a suite of high perfor-
mance natural language processing (NLP)
modules and relevant corpora. Espe-
cially for the syntactic and semantic pars-
ing modules, we achieved good results
in some relevant evaluations, such as
CoNLL and SemEval. Based on XML in-
ternal data representation, users can easily
use these modules and corpora by invok-
ing DLL (Dynamic Link Library) or Web
service APIs (Application Program Inter-
face), and view the processing results di-
rectly by the visualization tool.
</bodyText>
<sectionHeader confidence="0.998109" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9954448">
A Chinese natural language processing (NLP)
platform always includes lexical analysis (word
segmentation, part-of-speech tagging, named en-
tity recognition), syntactic parsing and seman-
tic parsing (word sense disambiguation, semantic
role labeling) modules. It is a laborious and time-
consuming work for researchers to develop a full
NLP platform, especially for Chinese, which has
fewer existing NLP tools. Therefore, it should be
of particular concern to build an integrated Chi-
nese processing platform. There are some key
problems for such a platform: providing high per-
formance language processing modules, integrat-
ing these modules smoothly, using processing re-
sults conveniently, and showing processing results
directly.
LTP (Language Technology Platform), a Chi-
nese processing platform, is built to solve the
above mentioned problems. It uses XML to trans-
fer data through modules and provides all sorts
</bodyText>
<equation confidence="0.651466785714286">
k \x12ZZA
&gt;GdzsAu\x03\x04YAuLJs ^LJYAs\x03
WAsYO
^GwAYs\x03WAsYO
WAZI
^GGS\x03
dAOOsYO
EAwGE\x03
\x1cYsLJ\x03
ZGZOYssZY
\x18GGYEGYLJ\x03
WAsYO
tZE\x03^GYG\x03
\x18sAwdsOA
sZY
^GwAYs\x03
ZZuG\x03
&gt;AdGusYO
tZE\x03
^GOwGYAsZY
L yD&gt;dAGE\x03\x18AA\x03\x03
l \x18&gt;&gt;\x03\x04W/ L tGd\x03^GsG
WGGY
AsZY\x03
\x03
WZG
sYO
l ssAusnjAsZY
</equation>
<figureCaption confidence="0.998661">
Figure 1: The architecture of LTP
</figureCaption>
<bodyText confidence="0.972007333333333">
of high performance Chinese processing modules,
some DLL or Web service APIs, visualization
tools, and some relevant corpora.
</bodyText>
<sectionHeader confidence="0.7359785" genericHeader="method">
2 Language Technology Platform
LTP (Language Technology Platform)1 is an inte-
</sectionHeader>
<bodyText confidence="0.9892395">
grated Chinese processing platform. Its architec-
ture is shown in Figure 1. From bottom to up, LTP
comprises 6 components: \x02 Corpora, \x03 Various
Chinese processing modules, \x04 XML based inter-
nal data presentation and processing, \x05 DLL API,
\x06 Web service, and \x07 Visualization tool. In the
following sections, we will introduce these com-
ponents in detail.
</bodyText>
<subsectionHeader confidence="0.969103">
2.1 Corpora
</subsectionHeader>
<bodyText confidence="0.994462444444445">
Many NLP tasks are based on annotated corpora.
We distributed two key corpora used by LTP.
First, WordMap is a Chinese thesaurus which
contains 100,093 words. In WordMap, each word
sense belongs to a five-level categories. There are
12 top, about 100 second and 1,500 third level,
and more fourth and fifth level categories. For in-
stance, the Chinese word \x02\x02 has the follow-
ing two senses:
</bodyText>
<footnote confidence="0.3115395">
1
http://ir.hit.edu.cn/ltp/
</footnote>
<page confidence="0.792582">
13
</page>
<equation confidence="0.99778325">
\x0c1. \x02(entity) \x02\x02(common name) \x02
\x02(goods) \x02\x02(goods) \x02\x02(material)
2. \x02(human beings) \x03\x02(ability) \x02
\x02(hero) \x02\x03(talents) \x02\x03(talents)
</equation>
<bodyText confidence="0.996842357142857">
We can see that the two senses belong to \x02
(entity) and \x02 (human beings) top categories
respectively. In each category, the concept be-
comes more and more specifical.
The second corpus is Chinese Dependency
Treebank (CDT) (Liu et al., 2006). It is annotated
with the dependency structure and contains 24 de-
pendency relation tags, such as SUB, OBJ, and
ADV. It consists of 10,000 sentences randomly ex-
tracted from the first six-month corpus of Peoples
Daily (China) in 1998, which has been annotated
with lexical tags, including word segmentation,
part-of-speech tagging, and named entity recog-
nition tags2.
</bodyText>
<subsectionHeader confidence="0.998337">
2.2 Chinese Processing Modules
</subsectionHeader>
<bodyText confidence="0.9868275">
We have developed 6 state-of-the-art Chinese pro-
cessing modules for LTP.
</bodyText>
<listItem confidence="0.995971666666667">
1. Word Segmentation (WordSeg): A CRF
model (Lafferty et al., 2001) is used to segment
Chinese words. All of the Peoples Daily (China)
corpus is used as training data.
2. Part-of-Speech Tagging (POSTag): We
adopt SVMTool3 for Chinese POS tagging
task (Wang et al., 2009). The Peoples Daily cor-
pus is also used here.
3. Named Entity Recognition (NER): LTP can
</listItem>
<bodyText confidence="0.62551575">
identify six sorts of named entity: Person, Loc,
Org, Time, Date and Quantity. A maximum en-
tropy model (Berger et al., 1996) is adopted here.
We still used the Peoples Daily corpus.
</bodyText>
<listItem confidence="0.621380125">
4. Word Sense Disambiguation (WSD): This
is an all word WSD system, which labels the
WordMap sense of each word. It adopts an SVM
model (Guo et al., 2007), which obtains the best
performance in SemEval 2009 Task 11: English
Lexical Sample Task via English-Chinese Parallel
Text.
5. Syntactic Parsing (Parser): Dependency
</listItem>
<bodyText confidence="0.852813333333333">
grammar is used in our syntactic parser. A high
order graph-based model (Che et al., 2009) is
adopted here which achieved the third place of
</bodyText>
<figure confidence="0.49253225">
2
http://icl.pku.edu.cn/icl res/
3
http://www.lsi.upc.edu/nlp/SVMTool/
Modules Performance Speed
WordSeg F1 = 97.4 185KB/s
POSTag The overall Accuracy =
97.80%, and the out of vo-
cabulary word Accuracy =
85.48%
56.3KB/s
NER The overall F1 = 92.25 14.4KB/s
</figure>
<table confidence="0.968432818181818">
WSD The all word WSD
Accuracy = 94.34%
and the multi-sense word
Accuracy = 91.29%
7.2KB/s
Parser LAS (Labeled Attachment
Score) = 73.91% and UAS
(Unlabeled Attachment
Score) = 78.23%
0.2KB/s
SRL F1 = 77.15 1.3KB/s
</table>
<tableCaption confidence="0.999177">
Table 1: The performance and speed for each
</tableCaption>
<listItem confidence="0.681849538461538">
module.
the dependency syntactic parsing subtask in the
CoNLL-2009 Syntactic and Semantic Dependen-
cies in Multiple Languages Shared Task (Hajic et
al., 2009).
6. Semantic Role Labeling (SRL): SRL is to
identify the relations between predicates in a sen-
tence and their associated arguments. The module
is based on syntactic parser. A maximum entropy
model (Che et al., 2009) is adopted here which
achieved the first place in the joint task of syn-
tactic and semantic dependencies of the CoNLL-
2009 Shared Task.
</listItem>
<bodyText confidence="0.992497357142857">
Table 1 shows the performance and speed of
each module in detail. The performances are ob-
tained with n-fold cross-validation method. The
speed is gotten on a machine with Xeon 2.0GHz
CPU and 4G Memory.
At present, LTP processes these modules with
a cascaded mechanism, i.e., some higher-level
processing modules depend on other lower-level
modules. For example, WSD needs to take the
output of POSTag as input; while before POSTag,
the document must be processed with WordSeg.
LTP can guarantee that the lower-level modules
are invoked automatically when invoking higher-
level modules.
</bodyText>
<subsectionHeader confidence="0.98535">
2.3 LTML
</subsectionHeader>
<bodyText confidence="0.999669">
We adopt eXtensible Markup Language (XML) as
the internal data presentation for some reasons.
First, XML is a simple, flexible text format, and
plays an increasingly important role in the ex-
</bodyText>
<page confidence="0.997712">
14
</page>
<bodyText confidence="0.996586333333333">
\x0cchange of a wide variety of data on the Web and
elsewhere. Second, there exist many powerful and
simple XML parsers. With these tools, we can
easily and effectively achieve all kinds of opera-
tions on XML. Finally, based on XML, we can
easily implement visualization with some script
languages such as JavaScript.
Based on XML, we have designed a tag-set for
NLP platform, named LTML (Language Technol-
ogy Markup Language). Basically, we regard a
word as a unit. The word has attributes such as id,
pos, wsd, etc., which indicate the index, part-of-
speech, word sense, etc. information of the word.
A sentence consists of a word sequence and then
a series of sentences compose a paragraph. The
semantic role labeling arguments are attached to
semantic predicate words. The meaning of each
tag and attribute are explained in Table 2.
</bodyText>
<figure confidence="0.71615925">
Tag Meaning Attr. Meaning
&lt;ltml&gt; Root node
&lt;doc&gt; Document
level
&lt;para&gt; Paragraph
in doc
id Paragraph index
in doc
&lt;sent&gt; Sentence
in para
id Sentence index in
paragraph
id Word index in
sentence
cont Word content
pos Part of speech of
word
&lt;word&gt; Word in
sentence
ne Named entity type
of word
wsd Word sense code
in WordMap
parent Word id of this
word depends on
in syntax tree
relate Syntax relation
type
id Argument index
of this word
Semantic
argu-
ments
type Semantic role of
this argument
&lt;arg&gt; of a word beg Beginning word
id of this argu-
ment
end Ending word id of
this argument
</figure>
<tableCaption confidence="0.995147">
Table 2: Tags and attributes of LTML
</tableCaption>
<subsectionHeader confidence="0.911613">
2.4 DLL API
</subsectionHeader>
<bodyText confidence="0.980541">
In order to gain the analysis results of LTP, we
</bodyText>
<listItem confidence="0.784762727272727">
provide various DLL APIs (implemented in C++
and Python), which can be divided into three
classes: I/O operation, module invoking, and re-
sult extraction.
1. I/O Operation: Load texts or LTML files
and convert them into DOM (Document Object
Model); Save DOM to XML files.
2. Module Invoking: Invoke the 6 Chinese pro-
cessing modules.
3. Result Extraction: Get the results produced
by the modules.
</listItem>
<bodyText confidence="0.981858">
Through invoking these APIs, users can accom-
plish some NLP tasks simply and conveniently.
Assuming that we want to get the part-of-speech
tags of a document, we can implement it with
Python programming language easily as shown in
</bodyText>
<figureCaption confidence="0.894361">
Figure 2.
</figureCaption>
<figure confidence="0.366189875">
from ltp_interface import *
CreateDOMFromTxt(&quot;test.txt&quot;) # Load a text
POStag() # Invoke POS tagger
for i in range( CountSentenceInDocument() ):
# Handle each sentence in a document
word_list = GetWordsFromSentence(i) # Get words
pos_list = GetPOSsFromSentence(i) # Get POS
......
</figure>
<figureCaption confidence="0.986814">
Figure 2: LTP Python API example
</figureCaption>
<bodyText confidence="0.989255428571429">
However, the DLL API has some shortcomings.
First, it only can be used on Microsoft Windows
machines. Second, users must download huge
model files when LTP is updated. Third, LTP
needs a high performance machine to run. All of
above problems prevent from its widespread ap-
plications.
</bodyText>
<subsectionHeader confidence="0.988435">
2.5 Web Service
</subsectionHeader>
<bodyText confidence="0.9907588">
In recent years, the Internet has become a platform
where we can acquire all kinds of services. Users
can build their own applications using LTP Web
services conveniently. The LTP Web service has
the following four advantages:
</bodyText>
<listItem confidence="0.9900035">
1. No need to setup LTP system.
2. No need to burden hardware to run LTP.
</listItem>
<page confidence="0.681961">
15
</page>
<figureCaption confidence="0.423354">
\x0cFigure 3: Sentence processing result
</figureCaption>
<listItem confidence="0.791496">
3. Update promptly and smoothly.
4. Cross most operating systems and program-
ming languages.
</listItem>
<subsectionHeader confidence="0.955592">
2.6 Visualization
</subsectionHeader>
<bodyText confidence="0.999510909090909">
A clear visualization can help researchers to ex-
amine processing results. We develop an cross-
platform and cross-browser visualization tool with
FLEX technology, which can be used easily with-
out installing any excess software.
Figure 3 shows the integrated sentence process-
ing results. The Rows 1 to 4 are the WordSeg,
POSTag, WSD, and NER results. The last rows
are the SRL results for different predicates. The
syntactic dependency Parser tree is shown above
with relation labels.
</bodyText>
<subsectionHeader confidence="0.972968">
2.7 Sharing
</subsectionHeader>
<bodyText confidence="0.998877888888889">
We have been sharing LTP freely for academic
purposes4. Until now, more than 350 worldwide
research institutes have shared LTP with license.
Some famous IT corporations of China, such as
HuaWei5 and Kingsoft6, have bought LTPs com-
mercial license. According to incompletely statis-
tics, there are more than 60 publications which
cited LTP, and the LTP web site has more than 30
unique visitors per day on the average.
</bodyText>
<sectionHeader confidence="0.995187" genericHeader="conclusions">
3 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9678595">
In this paper we describe an integrated Chinese
processing platform, LTP. Based on XML data
</bodyText>
<figure confidence="0.826716166666667">
4
http://ir.hit.edu.cn/demo/ltp/Sharing Plan.htm
5
http://www.huawei.com/
6
http://www.kingsoft.com/
</figure>
<bodyText confidence="0.98208625">
presentation, it provides a suite of high perfor-
mance NLP modules invoked with DLL or Web
service APIs, a visualization environment and a
set of corpora.
</bodyText>
<sectionHeader confidence="0.879609" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<reference confidence="0.901322571428571">
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, the 863 National High-
Tech Research and Development of China via
grant 2008AA01Z144, and Natural Scientific Re-
search Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069).
</reference>
<sectionHeader confidence="0.542142" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998355696969697">
Berger, Adam L., Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput.
Linguist., 22(1):3971.
Che, Wanxiang, Zhenghua Li, Yongqiang Li, Yuhang
Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing.
In CoNLL 2009, pages 4954, Boulder, Colorado,
June.
Guo, Yuhang, Wanxiang Che, Yuxuan Hu, Wei Zhang,
and Ting Liu. 2007. Hit-ir-wsd: A wsd system
for english lexical sample task. In SemEval-2007,
pages 165168.
Hajic, Jan, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antonia Mart, Llus
Marquez, Adam Meyers, Joakim Nivre, Sebastian
Pado, Jan Stepanek, Pavel Stranak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In CoNLL 2009, pages 118,
Boulder, Colorado, June.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML 2001, pages 282289. Mor-
gan Kaufmann, San Francisco, CA.
Liu, Ting, Jinshan Ma, and Sheng Li. 2006. Build-
ing a dependency treebank for improving Chinese
parser. Journal of Chinese Language and Comput-
ing, 16(4):207224.
Wang, Lijie, Wanxiang Che, and Ting Liu. 2009. An
SVMTool-based Chinese POS Tagger. Journal of
Chinese Information Processing, 23(4):1622.
</reference>
<page confidence="0.887403">
16
</page>
<figure confidence="0.319866">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.212924">
<note confidence="0.721507">b&apos;Coling 2010: Demonstration Volume, pages 1316, Beijing, August 2010</note>
<title confidence="0.945892">LTP: A Chinese Language Technology Platform</title>
<author confidence="0.71269">Wanxiang Che</author>
<author confidence="0.71269">Zhenghua Li</author>
<author confidence="0.71269">Ting Liu</author>
<affiliation confidence="0.80519125">Research Center for Information Retrieval MOE-Microsoft Key Laboratory of Natural Language Processing and Speech School of Computer Science and Technology Harbin Institute of Technology</affiliation>
<email confidence="0.886931">car@ir.hit.edu.cn</email>
<email confidence="0.886931">lzh@ir.hit.edu.cn</email>
<email confidence="0.886931">tliu@ir.hit.edu.cn</email>
<abstract confidence="0.9966888125">LTP (Language Technology Platform) is an integrated Chinese processing platform which includes a suite of high performance natural language processing (NLP) modules and relevant corpora. Especially for the syntactic and semantic parsing modules, we achieved good results in some relevant evaluations, such as CoNLL and SemEval. Based on XML internal data representation, users can easily use these modules and corpora by invoking DLL (Dynamic Link Library) or Web service APIs (Application Program Interface), and view the processing results directly by the visualization tool.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Comput. Linguist.,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="4631" citStr="Berger et al., 1996" startWordPosition="682" endWordPosition="685"> named entity recognition tags2. 2.2 Chinese Processing Modules We have developed 6 state-of-the-art Chinese processing modules for LTP. 1. Word Segmentation (WordSeg): A CRF model (Lafferty et al., 2001) is used to segment Chinese words. All of the Peoples Daily (China) corpus is used as training data. 2. Part-of-Speech Tagging (POSTag): We adopt SVMTool3 for Chinese POS tagging task (Wang et al., 2009). The Peoples Daily corpus is also used here. 3. Named Entity Recognition (NER): LTP can identify six sorts of named entity: Person, Loc, Org, Time, Date and Quantity. A maximum entropy model (Berger et al., 1996) is adopted here. We still used the Peoples Daily corpus. 4. Word Sense Disambiguation (WSD): This is an all word WSD system, which labels the WordMap sense of each word. It adopts an SVM model (Guo et al., 2007), which obtains the best performance in SemEval 2009 Task 11: English Lexical Sample Task via English-Chinese Parallel Text. 5. Syntactic Parsing (Parser): Dependency grammar is used in our syntactic parser. A high order graph-based model (Che et al., 2009) is adopted here which achieved the third place of 2 http://icl.pku.edu.cn/icl res/ 3 http://www.lsi.upc.edu/nlp/SVMTool/ Modules P</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, Adam L., Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Comput. Linguist., 22(1):3971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Yongqiang Li</author>
<author>Yuhang Guo</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
</authors>
<title>Multilingual dependency-based syntactic and semantic parsing.</title>
<date>2009</date>
<contexts>
<context position="5100" citStr="Che et al., 2009" startWordPosition="760" endWordPosition="763">ognition (NER): LTP can identify six sorts of named entity: Person, Loc, Org, Time, Date and Quantity. A maximum entropy model (Berger et al., 1996) is adopted here. We still used the Peoples Daily corpus. 4. Word Sense Disambiguation (WSD): This is an all word WSD system, which labels the WordMap sense of each word. It adopts an SVM model (Guo et al., 2007), which obtains the best performance in SemEval 2009 Task 11: English Lexical Sample Task via English-Chinese Parallel Text. 5. Syntactic Parsing (Parser): Dependency grammar is used in our syntactic parser. A high order graph-based model (Che et al., 2009) is adopted here which achieved the third place of 2 http://icl.pku.edu.cn/icl res/ 3 http://www.lsi.upc.edu/nlp/SVMTool/ Modules Performance Speed WordSeg F1 = 97.4 185KB/s POSTag The overall Accuracy = 97.80%, and the out of vocabulary word Accuracy = 85.48% 56.3KB/s NER The overall F1 = 92.25 14.4KB/s WSD The all word WSD Accuracy = 94.34% and the multi-sense word Accuracy = 91.29% 7.2KB/s Parser LAS (Labeled Attachment Score) = 73.91% and UAS (Unlabeled Attachment Score) = 78.23% 0.2KB/s SRL F1 = 77.15 1.3KB/s Table 1: The performance and speed for each module. the dependency syntactic par</context>
</contexts>
<marker>Che, Li, Li, Guo, Qin, Liu, 2009</marker>
<rawString>Che, Wanxiang, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, and Ting Liu. 2009. Multilingual dependency-based syntactic and semantic parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>In CoNLL</author>
</authors>
<date>2009</date>
<pages>4954</pages>
<location>Boulder, Colorado,</location>
<marker>CoNLL, 2009</marker>
<rawString>In CoNLL 2009, pages 4954, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhang Guo</author>
<author>Wanxiang Che</author>
<author>Yuxuan Hu</author>
<author>Wei Zhang</author>
<author>Ting Liu</author>
</authors>
<title>Hit-ir-wsd: A wsd system for english lexical sample task. In</title>
<date>2007</date>
<booktitle>SemEval-2007,</booktitle>
<pages>165168</pages>
<contexts>
<context position="4843" citStr="Guo et al., 2007" startWordPosition="721" endWordPosition="724"> segment Chinese words. All of the Peoples Daily (China) corpus is used as training data. 2. Part-of-Speech Tagging (POSTag): We adopt SVMTool3 for Chinese POS tagging task (Wang et al., 2009). The Peoples Daily corpus is also used here. 3. Named Entity Recognition (NER): LTP can identify six sorts of named entity: Person, Loc, Org, Time, Date and Quantity. A maximum entropy model (Berger et al., 1996) is adopted here. We still used the Peoples Daily corpus. 4. Word Sense Disambiguation (WSD): This is an all word WSD system, which labels the WordMap sense of each word. It adopts an SVM model (Guo et al., 2007), which obtains the best performance in SemEval 2009 Task 11: English Lexical Sample Task via English-Chinese Parallel Text. 5. Syntactic Parsing (Parser): Dependency grammar is used in our syntactic parser. A high order graph-based model (Che et al., 2009) is adopted here which achieved the third place of 2 http://icl.pku.edu.cn/icl res/ 3 http://www.lsi.upc.edu/nlp/SVMTool/ Modules Performance Speed WordSeg F1 = 97.4 185KB/s POSTag The overall Accuracy = 97.80%, and the out of vocabulary word Accuracy = 85.48% 56.3KB/s NER The overall F1 = 92.25 14.4KB/s WSD The all word WSD Accuracy = 94.34</context>
</contexts>
<marker>Guo, Che, Hu, Zhang, Liu, 2007</marker>
<rawString>Guo, Yuhang, Wanxiang Che, Yuxuan Hu, Wei Zhang, and Ting Liu. 2007. Hit-ir-wsd: A wsd system for english lexical sample task. In SemEval-2007, pages 165168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajic</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Antonia Mart</author>
<author>Llus Marquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pado</author>
<author>Jan Stepanek</author>
<author>Pavel Stranak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages. In CoNLL</title>
<date>2009</date>
<pages>118</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="5821" citStr="Hajic et al., 2009" startWordPosition="873" endWordPosition="876">pc.edu/nlp/SVMTool/ Modules Performance Speed WordSeg F1 = 97.4 185KB/s POSTag The overall Accuracy = 97.80%, and the out of vocabulary word Accuracy = 85.48% 56.3KB/s NER The overall F1 = 92.25 14.4KB/s WSD The all word WSD Accuracy = 94.34% and the multi-sense word Accuracy = 91.29% 7.2KB/s Parser LAS (Labeled Attachment Score) = 73.91% and UAS (Unlabeled Attachment Score) = 78.23% 0.2KB/s SRL F1 = 77.15 1.3KB/s Table 1: The performance and speed for each module. the dependency syntactic parsing subtask in the CoNLL-2009 Syntactic and Semantic Dependencies in Multiple Languages Shared Task (Hajic et al., 2009). 6. Semantic Role Labeling (SRL): SRL is to identify the relations between predicates in a sentence and their associated arguments. The module is based on syntactic parser. A maximum entropy model (Che et al., 2009) is adopted here which achieved the first place in the joint task of syntactic and semantic dependencies of the CoNLL2009 Shared Task. Table 1 shows the performance and speed of each module in detail. The performances are obtained with n-fold cross-validation method. The speed is gotten on a machine with Xeon 2.0GHz CPU and 4G Memory. At present, LTP processes these modules with a </context>
</contexts>
<marker>Hajic, Ciaramita, Johansson, Kawahara, Mart, Marquez, Meyers, Nivre, Pado, Stepanek, Stranak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Hajic, Jan, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Antonia Mart, Llus Marquez, Adam Meyers, Joakim Nivre, Sebastian Pado, Jan Stepanek, Pavel Stranak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages. In CoNLL 2009, pages 118, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML</title>
<date>2001</date>
<pages>282289</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="4215" citStr="Lafferty et al., 2001" startWordPosition="610" endWordPosition="613">pecifical. The second corpus is Chinese Dependency Treebank (CDT) (Liu et al., 2006). It is annotated with the dependency structure and contains 24 dependency relation tags, such as SUB, OBJ, and ADV. It consists of 10,000 sentences randomly extracted from the first six-month corpus of Peoples Daily (China) in 1998, which has been annotated with lexical tags, including word segmentation, part-of-speech tagging, and named entity recognition tags2. 2.2 Chinese Processing Modules We have developed 6 state-of-the-art Chinese processing modules for LTP. 1. Word Segmentation (WordSeg): A CRF model (Lafferty et al., 2001) is used to segment Chinese words. All of the Peoples Daily (China) corpus is used as training data. 2. Part-of-Speech Tagging (POSTag): We adopt SVMTool3 for Chinese POS tagging task (Wang et al., 2009). The Peoples Daily corpus is also used here. 3. Named Entity Recognition (NER): LTP can identify six sorts of named entity: Person, Loc, Org, Time, Date and Quantity. A maximum entropy model (Berger et al., 1996) is adopted here. We still used the Peoples Daily corpus. 4. Word Sense Disambiguation (WSD): This is an all word WSD system, which labels the WordMap sense of each word. It adopts an </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, John, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML 2001, pages 282289. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu</author>
<author>Jinshan Ma</author>
<author>Sheng Li</author>
</authors>
<title>Building a dependency treebank for improving Chinese parser.</title>
<date>2006</date>
<journal>Journal of Chinese Language and Computing,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="3677" citStr="Liu et al., 2006" startWordPosition="527" endWordPosition="530"> about 100 second and 1,500 third level, and more fourth and fifth level categories. For instance, the Chinese word \x02\x02 has the following two senses: 1 http://ir.hit.edu.cn/ltp/ 13 \x0c1. \x02(entity) \x02\x02(common name) \x02 \x02(goods) \x02\x02(goods) \x02\x02(material) 2. \x02(human beings) \x03\x02(ability) \x02 \x02(hero) \x02\x03(talents) \x02\x03(talents) We can see that the two senses belong to \x02 (entity) and \x02 (human beings) top categories respectively. In each category, the concept becomes more and more specifical. The second corpus is Chinese Dependency Treebank (CDT) (Liu et al., 2006). It is annotated with the dependency structure and contains 24 dependency relation tags, such as SUB, OBJ, and ADV. It consists of 10,000 sentences randomly extracted from the first six-month corpus of Peoples Daily (China) in 1998, which has been annotated with lexical tags, including word segmentation, part-of-speech tagging, and named entity recognition tags2. 2.2 Chinese Processing Modules We have developed 6 state-of-the-art Chinese processing modules for LTP. 1. Word Segmentation (WordSeg): A CRF model (Lafferty et al., 2001) is used to segment Chinese words. All of the Peoples Daily (C</context>
</contexts>
<marker>Liu, Ma, Li, 2006</marker>
<rawString>Liu, Ting, Jinshan Ma, and Sheng Li. 2006. Building a dependency treebank for improving Chinese parser. Journal of Chinese Language and Computing, 16(4):207224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lijie Wang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>An SVMTool-based Chinese POS Tagger.</title>
<date>2009</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="4418" citStr="Wang et al., 2009" startWordPosition="644" endWordPosition="647">sists of 10,000 sentences randomly extracted from the first six-month corpus of Peoples Daily (China) in 1998, which has been annotated with lexical tags, including word segmentation, part-of-speech tagging, and named entity recognition tags2. 2.2 Chinese Processing Modules We have developed 6 state-of-the-art Chinese processing modules for LTP. 1. Word Segmentation (WordSeg): A CRF model (Lafferty et al., 2001) is used to segment Chinese words. All of the Peoples Daily (China) corpus is used as training data. 2. Part-of-Speech Tagging (POSTag): We adopt SVMTool3 for Chinese POS tagging task (Wang et al., 2009). The Peoples Daily corpus is also used here. 3. Named Entity Recognition (NER): LTP can identify six sorts of named entity: Person, Loc, Org, Time, Date and Quantity. A maximum entropy model (Berger et al., 1996) is adopted here. We still used the Peoples Daily corpus. 4. Word Sense Disambiguation (WSD): This is an all word WSD system, which labels the WordMap sense of each word. It adopts an SVM model (Guo et al., 2007), which obtains the best performance in SemEval 2009 Task 11: English Lexical Sample Task via English-Chinese Parallel Text. 5. Syntactic Parsing (Parser): Dependency grammar </context>
</contexts>
<marker>Wang, Che, Liu, 2009</marker>
<rawString>Wang, Lijie, Wanxiang Che, and Ting Liu. 2009. An SVMTool-based Chinese POS Tagger. Journal of Chinese Information Processing, 23(4):1622.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>