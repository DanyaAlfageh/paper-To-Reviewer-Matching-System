<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.5264105">
b&apos;Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 296303,
New York, June 2006. c
</bodyText>
<sectionHeader confidence="0.414988" genericHeader="abstract">
2006 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.873723">
Integrating Probabilistic Extraction Models and Data Mining
to Discover Relations and Patterns in Text
</title>
<author confidence="0.992072">
Aron Culotta
</author>
<affiliation confidence="0.999202">
University of Massachusetts
</affiliation>
<address confidence="0.959189">
Amherst, MA 01003
</address>
<email confidence="0.995727">
culotta@cs.umass.edu
</email>
<author confidence="0.994674">
Andrew McCallum
</author>
<affiliation confidence="0.999393">
University of Massachusetts
</affiliation>
<address confidence="0.959213">
Amherst, MA 01003
</address>
<email confidence="0.995443">
mccallum@cs.umass.edu
</email>
<author confidence="0.976687">
Jonathan Betz
</author>
<affiliation confidence="0.851541">
Google, Inc.
</affiliation>
<address confidence="0.993759">
New York, NY 10018
</address>
<email confidence="0.995301">
jtb@google.com
</email>
<sectionHeader confidence="0.990751" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.989643235294118">
In order for relation extraction systems
to obtain human-level performance, they
must be able to incorporate relational pat-
terns inherent in the data (for example,
that ones sister is likely ones mothers
daughter, or that children are likely to
attend the same college as their par-
ents). Hand-coding such knowledge can
be time-consuming and inadequate. Addi-
tionally, there may exist many interesting,
unknown relational patterns that both im-
prove extraction performance and provide
insight into text. We describe a probabilis-
tic extraction model that provides mutual
benefits to both top-down relational pat-
tern discovery and bottom-up relation
extraction.
</bodyText>
<sectionHeader confidence="0.968612" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.564617">
Consider these four sentences:
</bodyText>
<listItem confidence="0.8717566">
1. George W. Bushs father is George H. W. Bush.
2. George H. W. Bushs sister is Nancy Bush Ellis.
3. Nancy Bush Elliss son is John Prescott Ellis.
4. John Prescott Ellis analyzed George W. Bushs
campaign.
</listItem>
<bodyText confidence="0.991493">
We would like to build an automated system to
extract the set of relations shown in Figure 1.
</bodyText>
<figure confidence="0.926657125">
cousin
Nancy Ellis Bush
sibling
George HW Bush
George W Bush
son
John Prescott Ellis
son
</figure>
<figureCaption confidence="0.999882">
Figure 1: Bush family tree
</figureCaption>
<bodyText confidence="0.999425538461538">
State of the art extraction algorithms may be able
to detect the son and sibling relations from local lan-
guage clues. However, the cousin relation is only
implied by the text and requires additional knowl-
edge to be extracted. Specifically, the system re-
quires knowledge of familial relation patterns.
One could imagine a system that accepts such
rules as input (e.g. cousin = fathers sisters son)
and applies them to extract implicit relations. How-
ever, exhaustively enumerating all possible rules can
be tedious and incomplete. More importantly, many
relational patterns unknown a priori may both im-
prove extraction accuracy and uncover informative
trends in the data (e.g. that children often adopt the
religion of their parents). Indeed, the goal of data
mining is to learn such patterns from database reg-
ularities. Since these patterns will not always hold,
we would like to handle them probabilistically.
We propose an integrated supervised machine
learning method that learns both contextual and re-
lational patterns to extract relations. In particular,
we construct a linear-chain conditional random field
(Lafferty et al., 2001; Sutton and McCallum, 2006)
to extract relations from biographical texts while si-
multaneously discovering interesting relational pat-
terns that improve extraction performance.
</bodyText>
<page confidence="0.998978">
296
</page>
<sectionHeader confidence="0.972237" genericHeader="related work">
\x0c2 Related Work
</sectionHeader>
<bodyText confidence="0.998294933333333">
This work can be viewed as a step toward the in-
tegration of information extraction and data mining
technology, a direction of growing interest. Nahm
and Mooney (2000) present a system that mines as-
sociation rules from a database constructed from au-
tomatically extracted data, then applies these learned
rules to improve data field recall without revisiting
the text. Our work attempts to more tightly inte-
grate the extraction and mining tasks by learning
relational patterns that can be included probabilis-
tically into extraction to improve its accuracy; also,
our work focuses on mining from relational graphs,
rather than single-table databases.
McCallum and Jensen (2003) argue the theoreti-
cal benefits of an integrated probabilistic model for
extraction and mining, but do not construct such a
system. Our work is a step in the direction of their
proposal, using an inference procedure based on a
closed-loop iteration between extraction and rela-
tional pattern discovery.
Most other work in this area mines raw text, rather
than a database automatically populated via extrac-
tion (Hearst, 1999; Craven et al., 1998).
This work can also be viewed as part of a trend
to perform joint inference across multiple language
processing tasks (Miller et al., 2000; Roth and tau
Yih, 2002; Sutton and McCallum, 2004).
Finally, using relational paths between entities is
also examined in (Richards and Mooney, 1992) to
escape local maxima in a first-order learning system.
</bodyText>
<sectionHeader confidence="0.833976" genericHeader="method">
3 Relation Extraction as Sequence
</sectionHeader>
<subsectionHeader confidence="0.610951">
Labeling
</subsectionHeader>
<bodyText confidence="0.982782553191489">
Relation extraction is the task of discovering seman-
tic connections between entities. In text, this usu-
ally amounts to examining pairs of entities in a doc-
ument and determining (from local language cues)
whether a relation exists between them. Common
approaches to this problem include pattern match-
ing (Brin, 1998; Agichtein and Gravano, 2000),
kernel methods (Zelenko et al., 2003; Culotta and
Sorensen, 2004; Bunescu and Mooney, 2006), lo-
gistic regression (Kambhatla, 2004), and augmented
parsing (Miller et al., 2000).
The pairwise classification approach of kernel
methods and logistic regression is commonly a two-
phase method: first the entities in a document are
identified, then a relation type is predicted for each
pair of entities. This approach presents at least
two difficulties: (1) enumerating all pairs of enti-
ties, even when restricted to pairs within a sentence,
results in a low density of positive relation exam-
ples; and (2) errors in the entity recognition phase
can propagate to errors in the relation classification
stage. As an example of the latter difficulty, if a per-
son is mislabeled as a company, then the relation
classifier will be unsuccessful in finding a brother
relation, despite local evidence.
We avoid these difficulties by restricting our in-
vestigation to biographical texts, e.g. encyclopedia
articles. A biographical text mostly discusses one
entity, which we refer to as the principal entity. We
refer to other mentioned entities as secondary enti-
ties. For each secondary entity, our goal is to predict
what relation, if any, it has to the principal entity.
This formulation allows us to treat relation ex-
traction as a sequence labeling task such as named-
entity recognition or part-of-speech tagging, and we
can now apply models that have been successful on
those tasks. By anchoring one argument of relations
to be the principal entity, we alleviate the difficulty
of enumerating all pairs of entities in a document.
By converting to a sequence labeling task, we fold
the entity recognition step into the relation extrac-
tion task. There is no initial pass to label each entity
as a person or company. Instead, an entitys label is
its relation to the principal entity. Below is an exam-
ple of a labeled article:
George W. Bush
George is the son of George H. W. Bush
</bodyText>
<equation confidence="0.725694">
 |{z }
father
and Barbara Bush
 |{z }
mother
.
</equation>
<bodyText confidence="0.995015333333333">
Additionally, by using a sequence model we can
capture the dependence between adjacent labels. For
example, in our data it is common to see phrases
such as son of the Republican president George H.
W. Bush for which the labels politicalParty, jobTi-
tle, and father occur consecutively. Sequence mod-
els are specifically designed to handle these kinds
of dependencies. We now discuss the details of our
extraction model.
</bodyText>
<page confidence="0.99726">
297
</page>
<sectionHeader confidence="0.394945" genericHeader="method">
\x0c3.1 Conditional Random Fields
</sectionHeader>
<bodyText confidence="0.99912825">
We build a model to extract relations using linear-
chain conditional random fields (CRFs) (Lafferty
et al., 2001; Sutton and McCallum, 2006). CRFs
are undirected graphical models (i.e. Markov net-
works) that are discriminatively-trained to maximize
the conditional probability of a set of output vari-
ables y given a set of input variables x. This condi-
tional distribution has the form
</bodyText>
<equation confidence="0.999118">
p(y|x) =
1
Zx
Y
cC
c(yc, xc; ) (1)
</equation>
<bodyText confidence="0.885207">
where are potential functions parameterized by
</bodyText>
<equation confidence="0.935302">
and Zx =
P
y
Q
cC (yc, xc) is a normalization
</equation>
<bodyText confidence="0.934811333333333">
factor. Assuming c factorizes as a log-linear com-
bination of arbitrary features computed over clique
c, then c(yc, xc; ) = exp (
</bodyText>
<equation confidence="0.777045">
P
k kfk(yc, xc)),
</equation>
<bodyText confidence="0.997589">
where f is a set of arbitrary feature functions over
the input, each of which has an associate model
parameter k. Parameters = {k} are a set
of real-valued weights typically estimated from la-
beled training data by maximizing the data likeli-
hood function using gradient ascent.
In these experiments, we make a first-order
Markov assumption on the dependencies among y,
resulting in a linear-chain CRF.
</bodyText>
<sectionHeader confidence="0.997048" genericHeader="method">
4 Relational Patterns
</sectionHeader>
<bodyText confidence="0.994237842105263">
The modeling flexibility of CRFs permits the fea-
ture functions to be complex, overlapping features of
the input without requiring additional assumptions
on their inter-dependencies. In addition to common
language features (e.g. neighboring words and syn-
tactic information), in this work we explore features
that cull relational patterns from a database of enti-
ties.
As described in the introductory example (Figure
1), context alone is often insufficient to extract re-
lations. Even in simpler examples, it may be the
case that modeling relational patterns can improve
extraction accuracy.
To capture this evidence, we compute features
from a database to indicate relational connections
between entities, similar to the relational path-
finding performed in Richards and Mooney (1992).
Imagine that the four sentence example about the
Bush family is included in a training set, and the en-
</bodyText>
<figure confidence="0.82843775">
cousin
father son
X Y
sibling
</figure>
<figureCaption confidence="0.998126">
Figure 2: A feature template for the cousin relation.
</figureCaption>
<bodyText confidence="0.988107392857143">
tities are labeled with their correct relations. In this
case, the cousin relation in sentence 4 would also be
labeled. From this data, we can create a relational
database that contains the relations in Figure 1.
Assume sentence 4 comes from a biography about
John Ellis. We calculate a feature for the entity
George W. Bush that indicates the path from John
Ellis to George W. Bush in the database, annotat-
ing each edge in the path with its relation label; i.e.
father-sibling-son. By abstracting away the actual
entity names, we have created a cousin template fea-
ture, as shown in Figure 2.
By adding these relational paths as features to
the model, we can learn interesting relational pat-
terns that may have low precision (e.g. people are
likely to be friends with their classmates) without
hampering extraction performance. This is in con-
trast to the system described in Nahm and Mooney
(2000), in which patterns are induced from a noisy
database and then applied directly to extraction. In
our system, since each learned path has an associ-
ated weight, it is simply another piece of evidence
to help the extractor. Low precision patterns may
have lower weights than high precision patterns, but
they will still influence the extractor.
A nice property of this approach is that examin-
ing highly weighted patterns can provide insight into
regularities of the data.
</bodyText>
<subsectionHeader confidence="0.976354">
4.1 Feature Induction
</subsectionHeader>
<bodyText confidence="0.996231888888889">
During CRF training, weights are learned for each
relational pattern. Patterns that increase extraction
performance will receive higher weights, while pat-
terns that have little effect on performance will re-
ceive low weights.
We can explore the space of possible conjunctions
of these patterns using feature induction for CRFs,
as described in McCallum (2003). Search through
the large space of possible conjunctions is guided
</bodyText>
<page confidence="0.991119">
298
</page>
<bodyText confidence="0.999337769230769">
\x0cby adding features that are estimated to increase the
likelihood function most.
When feature induction is used with relational
patterns, we can view this as a type of data mining,
in which patterns are created based on their influ-
ence on an extraction model. This is similar to work
by Dehaspe (1997), where inductive logic program-
ming is embedded as a feature induction technique
for a maximum entropy classifier. Our work restricts
induced features to conjunctions of base features,
rather than using first-order clauses. However, the
patterns we learn are based on information extracted
from natural language.
</bodyText>
<subsectionHeader confidence="0.982547">
4.2 Iterative Database Construction
</subsectionHeader>
<bodyText confidence="0.998535489361702">
The top-down knowledge provided by data min-
ing algorithms has the potential to improve the per-
formance of information extraction systems. Con-
versely, bottom-up knowledge generated by ex-
traction systems can be used to populate a large
database, from which more top-down knowledge
can be discovered. By carefully communicating the
uncertainty between these systems, we hope to iter-
atively expand a knowledge base, while minimizing
fallacious inferences.
In this work, the top-down knowledge consists of
relational patterns describing the database path be-
tween entities in text. The uncertainty of this knowl-
edge is handled by associating a real-valued CRF
weight with each pattern, which increases when the
pattern is predictive of other relations. Thus, the ex-
traction model can adapt to noise in these patterns.
Since we also desire to extract relations between
entities that appear in text but not in the database, we
first populate the database with relations extracted
by a CRF that does not use relational patterns. We
then do further extraction with a CRF that incorpo-
rates the relational patterns found in this automati-
cally generated database. In this manner, we create a
closed-loop system that alternates between bottom-
up extraction and top-down pattern discovery. This
approach can be viewed as a type of alternating opti-
mization, with analogies to formal methods such as
expectation-maximization.
The uncertainty in the bottom-up extraction step
is handled by estimating the confidence of each ex-
traction and pruning the database to remove en-
tries with low confidence. One of the benefits of
a probabilistic extraction model is that confidence
estimates can be straight-forwardly obtained. Cu-
lotta and McCallum (2004) describe the constrained
forward-backward algorithm to efficiently estimate
the conditional probability that a segment of text is
correctly extracted by a CRF.
Using this algorithm, we associate a confidence
value with each relation extracted by the CRF. This
confidence value is then used to limit the noise
introduced by incorrect extractions. This differs
from Nahm and Mooney (2000) and Mooney and
Bunescu (2005), in which standard decision tree rule
learners are applied to the unfiltered output of ex-
traction.
</bodyText>
<subsectionHeader confidence="0.997932">
4.3 Extracting Implicit Relations
</subsectionHeader>
<bodyText confidence="0.997267">
An implicit relation is one that does not have direct
contextual evidence, for example the cousin relation
in our initial example. Implicit relations generally
require some background knowledge to be detected,
such as relational patterns (e.g. rules about familial
relations). These are the sorts of relations on which
current extraction models perform most poorly.
Notably, these are exactly the sorts of relations
that are likely to have the biggest impact on informa-
tion access. A system that can accurately discover
knowledge that is only implied by the text will dra-
matically increase the amount of information a user
can uncover, effectively providing access to the im-
plications of a corpus.
We argue that integrating top-down and bottom-
up knowledge discovery algorithms discussed in
Section 4.2 can enable this technology. By per-
forming pattern discovery in conjunction with infor-
mation extraction, we can collate facts from multi-
ple sources to infer new relations. This is an ex-
ample of cross-document fusion or cross-document
information extraction, a growing area of research
transforming raw extractions into usable knowledge
bases (Mann and Yarowsky, 2005; Masterson and
Kushmerik, 2003).
</bodyText>
<sectionHeader confidence="0.985169" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.550965">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.877573">
We sampled 1127 paragraphs from 271 articles from
</bodyText>
<figure confidence="0.980135">
the online encyclopedia Wikipedia1 and labeled a to-
1
http://www.wikipedia.org
299
\x0cGeorge W. Bush
Dick Cheney
underling
Yale
education
Republican
party
President
jobTitle
George H. W. Bush
son
underling
Harken Energy
executive
education party
jobTitle
Prescott Bush
son
education
Bill Clinton
rival
Bob Dole
rival
education
Democrat
party
jobTitle
Hillary Clinton
husband
education
party
Halliburton
executive
education
Pres Medal of Freedom
award
party
Nelson Rockefeller
award
Elizabeth Dole
wife
WWII
participant
award
party
party
Martin Luther King, Jr.
award
</figure>
<figureCaption confidence="0.7484156">
Figure 3: An example of the connectivity of the entities in the data.
birthday birth year death day
death year nationality visited
birth place death place religion
job title member of cousin
</figureCaption>
<figure confidence="0.421715142857143">
friend discovered education
employer associate opus
participant influence award
brother wife supported idea
executive of political party supported person
founder son father
rival underling superior
role inventor husband
grandfather sister brother-in-law
nephew mother daughter
granddaughter grandson great-grandson
grandmother rival organization owner of
uncle descendant ancestor
great-grandfather aunt
</figure>
<tableCaption confidence="0.99255">
Table 1: The set of labeled relations.
</tableCaption>
<bodyText confidence="0.999328018181818">
tal of 4701 relation instances. In addition to a large
set of person-to-person relations, we also included
links between people and organizations, as well as
biographical facts such as birthday and jobTitle. In
all, there are 53 labels in the training data (Table 1).
We sample articles that result in a high density
of interesting relations by choosing, for example, a
collection of related family members and associates.
Figure 3 shows a small example of the type of con-
nections in the data. We then split the data into train-
ing and testing sets (70-30 split), attempting to sep-
arate the entities into connected components. For
example, all Bush family members were placed in
the training set, while all Kennedy family members
were placed in the testing set. While there are still
occasional paths connecting entities in the training
set to those in the test set, we believe this method-
ology reflects a typical real-world scenario in which
we would like to extend an existing database to a
different, but slightly related, domain.
The structure of the Wikipedia articles somewhat
simplifies the extraction task, since important enti-
ties are hyper-linked within the text. This provides
an automated way to detect entities in the text, al-
though these entities are not classified by type. This
also allows us to easily construct database queries,
since we can reason at the entity level, rather than
the token level. (Although, see Sarawagi and Cohen
(2004) for extensions of CRFs that model the en-
tity length distribution.) The results we report here
are constrained to predict relations only for hyper-
linked entities. Note that despite this property, we
still desire to use a sequence model to capture the
dependencies between adjacent labels.
We use the MALLET CRF implementation (Mc-
Callum, 2002) with the default regularization pa-
rameters.
Based on initial experiments, we restrict relational
path features to length two or three. Paths of length
one will learn trivial paths and can lead to over-
fitting. Paths longer than three can increase compu-
tational costs without adding much new information.
In addition to the relational pattern features de-
scribed in Section 4, the list of local features in-
cludes context words (such as the token identity
within a 6 word window of the target token), lexi-
cons (such as whether a token appears in a list of
cities, people, or companies), regular expressions
(such as whether the token is capitalized or contains
digits or punctuation), part-of-speech (predicted by
a CRF that was trained separately for part of speech
tagging), prefix/suffix (such as whether a word ends
in -ed or begins with ch-), and offset conjunctions
(combinations of adjacent features within a window
of size six).
</bodyText>
<page confidence="0.988497">
300
</page>
<table confidence="0.99858475">
\x0cME CRF0 CRFr CRFr0.9 CRFr0.5 CRFt CRFt0.5
F1 .5489 .5995 .6100 .6008 .6136 .6791 .6363
P .6475 .7019 .6799 .7177 .7095 .7553 .7343
R .4763 .5232 .5531 .5166 .5406 .6169 .5614
</table>
<tableCaption confidence="0.998352">
Table 2: Results comparing the relative benefits of using relational patterns in extraction.
</tableCaption>
<subsectionHeader confidence="0.999003">
5.2 Extraction Results
</subsectionHeader>
<bodyText confidence="0.999535628571428">
We evaluate performance by calculating the preci-
sion (P) and recall (R) of extracted relations, as well
as the F1 measure, which is the harmonic mean of
precision and recall.
CRF0 is the conditional random field constructed
without relational features. Results for CRF0 are
displayed in the second column of Table 2. ME is
a maximum entropy classifier trained on the same
feature set as CRF0. The difference between these
two models is that CRF0 models the dependence of
relations that appear consecutively in the text. The
superior performance of CRF0 suggests that this de-
pendence is important to capture.
The remaining models incorporate the relational
patterns described in Section 4. We compare three
different confidence thresholds for the construction
of the initial testing database, as described in Sec-
tion 4.2. CRFr uses no threshold, while CRFr0.9
and CRFr0.5 restrict the database to extractions with
confidence greater than 0.9 and 0.5, respectively.
As shown by comparing CRF0 and CRFr in Ta-
ble 2, the relational features constructed from the
database with no confidence threshold provides a
considerable boost in recall (reducing error by 7%),
at the cost of a decrease in precision. Here we see
the effect of making fallacious inferences on a noisy
database.
In column four, we see the opposite effect for
the overly conservative threshold of CRFr0.9. Here,
precision improves slightly over CRF0, and consid-
erably over CRFr (12% error reduction), but this is
accompanied by a drop in recall (8% reduction).
Finally, in column five, a confidence of 0.5 results
in the best F1 measure (a 3.5% error reduction over
CRF0). CRFr0.5 also obtains better recall and preci-
sion than CRF0, reducing recall error by 3.6%, pre-
cision error by 2.5%.
Comparing the performance on different relation
types, we find that the biggest increase from CRF0
to CRFr0.5 is on the memberOf relation, for which
the F1 score improves from 0.4211 to 0.6093. We
conjecture that the reason for this is that the patterns
most useful for the memberOf label contain relations
that are well-detected by the first-pass CRF. Also,
the local language context seems inadequate to prop-
erly extract this relation, given the low performance
of CRF0.
To better gauge how much relational pattern fea-
tures are affected by errors in the database, we run
two additional experiments for which the relational
features are fixed to be correct. That is, imagine that
we construct a database from the true labeling of the
testing data, and create the relational pattern features
from this database. Note that this does not trivialize
the problem, since there are no relational path fea-
tures of length one (e.g., if X is the wife of Y, there
will be no feature indicating this).
We construct two experiments under this scheme,
one where the entire test database is used (CRFt),
and another where only half the relations are in-
cluded in the test database, selected uniformly at
random (CRFt0.5).
Column six shows the improvements enabled by
using the complete testing database. More inter-
estingly, column seven shows that even with only
half the database accurately known, performance
improves considerably over both CRF and CRFr0.5.
A realistic scenario for CRFt0.5 is a semi-automated
system, in which a partially-filled database is used to
bootstrap extraction.
</bodyText>
<subsectionHeader confidence="0.997526">
5.3 Mining Results
</subsectionHeader>
<bodyText confidence="0.994910857142857">
Comparing the impact of discovered patterns on ex-
traction is a way to objectively measure mining per-
formance. We now give a brief subjective evaluation
of the learned patterns. By examining relational pat-
terns with high weights for a particular label, we can
glean some regularities from our dataset. Examples
of such patterns are in Table 3.
</bodyText>
<page confidence="0.997146">
301
</page>
<table confidence="0.815222125">
\x0cRelation Relational Path Feature
mother father wife
cousin mother husband nephew
friend education student
education father education
boss boss son
memberOf grandfather memberOf
rival politicalParty member rival
</table>
<tableCaption confidence="0.997091">
Table 3: Examples of highly weighted relational pat-
</tableCaption>
<bodyText confidence="0.981647294117647">
terns.
From the familial relations in our training data, we
are able to discover many equivalences for mothers,
cousins, grandfathers, and husbands. In addition to
these high precision patterns, the system also gener-
ates interesting, low precision patterns. Row 3-7 of
Table 3 can be summarized by the following gener-
alizations: friends tend to be classmates; children of
alumni often attend the same school as their parents;
a boss child often becomes the boss; grandchildren
are often members of the same organizations as their
grandparents; and rivals of a person from one polit-
ical party are often rivals of other members of the
same political party. While many of these patterns
reflect the high concentration of political entities and
familial relations in our training database, many will
have applicability across domains.
</bodyText>
<subsectionHeader confidence="0.969763">
5.4 Implicit Relations
</subsectionHeader>
<bodyText confidence="0.999485563636364">
It is difficult to measure system performance on im-
plicit relations, since our labeled data does not dis-
tinguish between explicit and implicit relations. Ad-
ditionally, accurately labeling all implicit relations
is challenging even for a human annotator.
We perform a simple exploratory analysis to de-
termine how relational patterns can help discover
implicit relations. We construct a small set of syn-
thetic sentences for which CRF0 successfully ex-
tracts relations using contextual features. We then
add sentences with slightly more ambiguous lan-
guage and measure whether CRFr can overcome this
ambiguity using relational pattern features.
For example, we create an article about an en-
tity named Bob Smith that includes the sentences
His brother, Bill Smith, was a biologist and His
companion, Bill Smith, was a biologist. CRF0 suc-
cessfully returns the brother relation in the first sen-
tence, but not the second. After a fact is added to
the database that says Bob and Bill have a brother in
common named John, CRFr is able to correctly label
the second sentence in spite of the ambiguous word
companion, because CRF0 has a highly-weighted
relational pattern feature for brother.
Similar behavior is observed for low precision
patterns like associates tend to win the same
awards. A synthetic article for the entity Tom
Jones contains the sentences He was awarded the
Pulitzer Prize in 1998 and Tom got the Pulitzer
Prize in 1998. Because CRF0 is highly-reliant on
the presence of the verb awarded or won to indi-
cate a prize fact, it fails to label the second sentence
correctly. After the database is augmented to include
the fact that Toms associate Jill received the Pulitzer
Prize, CRFr labels the second sentence correctly.
However, we also observed that CRFr still re-
quires some contextual clues to extract implicit re-
lations. For example, if the Tom Jones article in-
stead contains the sentence The Pulitzer Prize was
awarded to him in 1998, neither CRF labels the
prize fact correctly, since this passive construction
is rarely seen in the training data.
We conclude from this brief analysis that rela-
tional patterns used by CRFr can help extract im-
plicit relations when (1) the database contains ac-
curate relational information, and (2) the sentence
contains limited contextual clues. Since relational
patterns are treated only as additional features by
CRFr, they are generally not powerful enough to
overcome a complete absence of contextual clues.
From this perspective, relational patterns can be seen
as enhancing the signal from contextual clues. This
differs from deterministically applying learned rules
independent of context, which may boost recall at
the cost of precision.
</bodyText>
<sectionHeader confidence="0.997005" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99943025">
We have shown that integrating pattern discovery
with relation extraction can lead to improved per-
formance on each task.
In the future, we wish to explore extending this
methods to larger datasets, where we expect rela-
tional patterns to be even more interesting. Also,
we plan to improve upon iterative database construc-
tion by performing joint inference among distant
</bodyText>
<page confidence="0.983187">
302
</page>
<bodyText confidence="0.9989655">
\x0crelations in an article. Inference in these highly-
connected models will likely require approximate
methods. Additionally, we wish to focus on extract-
ing implicit relations, dealing more formally with
the precision-recall trade-off inherent in applying
noisy rules to improve extraction.
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.995413">
Thanks to the Google internship program, and to Charles Sutton
for providing the CRF POS tagger. This work was supported in
part by the Center for Intelligent Information Retrieval, in part
by U.S. Government contract #NBCH040171 through a sub-
contract with BBNT Solutions LLC, in part by The Central In-
telligence Agency, the National Security Agency and National
Science Foundation under NSF grant #IIS-0326249, and in part
by the Defense Advanced Research Projects Agency (DARPA),
through the Department of the Interior, NBC, Acquisition Ser-
vices Division, under contract number NBCHD030010. Any
opinions, findings and conclusions or recommendations ex-
pressed in this material are the author(s) and do not necessarily
reflect those of the sponsor.
</bodyText>
<sectionHeader confidence="0.988422" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997050975609756">
Eugene Agichtein and Luis Gravano. 2000. Snowball: Extract-
ing relations from large plain-text collections. In Proceed-
ings of the Fifth ACM International Conference on Digital
Libraries.
Sergey Brin. 1998. Extracting patterns and relations from the
world wide web. In WebDB Workshop at 6th International
Conference on Extending Database Technology.
Razvan Bunescu and Raymond Mooney. 2006. Subsequence
kernels for relation extraction. In Y. Weiss, B. Scholkopf,
and J. Platt, editors, Advances in Neural Information Pro-
cessing Systems 18. MIT Press, Cambridge, MA.
Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew K. Mc-
Callum, Tom M. Mitchell, Kamal Nigam, and Sean Slattery.
1998. Learning to extract symbolic knowledge from the
World Wide Web. In Proceedings of AAAI-98, 15th Confer-
ence of the American Association for Artificial Intelligence,
pages 509516, Madison, US. AAAI Press, Menlo Park, US.
Aron Culotta and Andrew McCallum. 2004. Confidence es-
timation for information extraction. In Human Langauge
Technology Conference (HLT 2004), Boston, MA.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In ACL.
L. Dehaspe. 1997. Maximum entropy modeling with clausal
constraints. In Proceedings of the Seventh International
Workshop on Inductive Logic Programming, pages 109125,
Prague, Czech Republic.
M. Hearst. 1999. Untangling text data mining. In 37th Annual
Meeting of the Association for Computational Linguistics.
Nanda Kambhatla. 2004. Combining lexical, syntactic, and se-
mantic features with maximum entropy models for extract-
ing relations. In ACL.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. 18th Interna-
tional Conf. on Machine Learning, pages 282289. Morgan
Kaufmann, San Francisco, CA.
Gideon Mann and David Yarowsky. 2005. Multi-field informa-
tion extraction and cross-document fusion. In ACL.
D. Masterson and N. Kushmerik. 2003. Information extraction
from multi-document threads. In ECML-2003: Workshop on
Adaptive Text Extraction and Mining, pages 3441.
Andrew McCallum and David Jensen. 2003. A note on the
unification of information extraction and data mining us-
ing conditional-probability, relational models. In IJCAI03
Workshop on Learning Statistical Models from Relational
Data.
Andrew McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
Andrew McCallum. 2003. Efficiently inducing features of con-
ditional random fields. In Nineteenth Conference on Uncer-
tainty in Artificial Intelligence (UAI03).
Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing to ex-
tract information from text. In ANLP.
Raymond J. Mooney and Razvan Bunescu. 2005. Mining
knowledge from text using information extraction. SigKDD
Explorations on Text Mining and Natural Language Process-
ing.
Un Yong Nahm and Raymond J. Mooney. 2000. A mutually
beneficial integration of data mining and information extrac-
tion. In AAAI/IAAI.
Bradley L. Richards and Raymond J. Mooney. 1992. Learning
relations by pathfinding. In Proceedings of the Tenth Na-
tional Conference on Artificial Intelligence (AAAI-92), pages
5055, San Jose, CA.
Dan Roth and Wen tau Yih. 2002. Probabilistic reasoning for
entity and relation recognition. In COLING.
Sunita Sarawagi and William W. Cohen. 2004. Semi-markov
conditional random fields for information extraction. In
NIPS 04.
Charles Sutton and Andrew McCallum. 2004. Dynamic condi-
tional random fields: Factorized probabilistic models for la-
beling and segmenting sequence data. In Proceedings of the
Twenty-First International Conference on Machine Learning
(ICML).
Charles Sutton and Andrew McCallum. 2006. An introduction
to conditional random fields for relational learning. In Lise
Getoor and Ben Taskar, editors, Introduction to Statistical
Relational Learning. MIT Press. To appear.
Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella.
2003. Kernel methods for relation extraction. Journal of
Machine Learning Research, 3:10831106.
</reference>
<page confidence="0.993244">
303
</page>
<figure confidence="0.24519">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.748301">
<note confidence="0.969680333333333">b&apos;Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 296303, New York, June 2006. c 2006 Association for Computational Linguistics</note>
<title confidence="0.972218">Integrating Probabilistic Extraction Models and Data Mining to Discover Relations and Patterns in Text</title>
<author confidence="0.999874">Aron Culotta</author>
<affiliation confidence="0.999987">University of Massachusetts</affiliation>
<address confidence="0.999857">Amherst, MA 01003</address>
<email confidence="0.999773">culotta@cs.umass.edu</email>
<author confidence="0.999756">Andrew McCallum</author>
<affiliation confidence="0.999981">University of Massachusetts</affiliation>
<address confidence="0.999605">Amherst, MA 01003</address>
<email confidence="0.999791">mccallum@cs.umass.edu</email>
<author confidence="0.999922">Jonathan Betz</author>
<affiliation confidence="0.958618">Google, Inc.</affiliation>
<address confidence="0.999687">New York, NY 10018</address>
<email confidence="0.999681">jtb@google.com</email>
<abstract confidence="0.993693166666667">In order for relation extraction systems to obtain human-level performance, they must be able to incorporate relational patterns inherent in the data (for example, that ones sister is likely ones mothers daughter, or that children are likely to attend the same college as their parents). Hand-coding such knowledge can be time-consuming and inadequate. Additionally, there may exist many interesting, unknown relational patterns that both improve extraction performance and provide insight into text. We describe a probabilistic extraction model that provides mutual benefits to both top-down relational pattern discovery and bottom-up relation extraction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fifth ACM International Conference on Digital Libraries.</booktitle>
<contexts>
<context position="4824" citStr="Agichtein and Gravano, 2000" startWordPosition="742" endWordPosition="745">essing tasks (Miller et al., 2000; Roth and tau Yih, 2002; Sutton and McCallum, 2004). Finally, using relational paths between entities is also examined in (Richards and Mooney, 1992) to escape local maxima in a first-order learning system. 3 Relation Extraction as Sequence Labeling Relation extraction is the task of discovering semantic connections between entities. In text, this usually amounts to examining pairs of entities in a document and determining (from local language cues) whether a relation exists between them. Common approaches to this problem include pattern matching (Brin, 1998; Agichtein and Gravano, 2000), kernel methods (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2006), logistic regression (Kambhatla, 2004), and augmented parsing (Miller et al., 2000). The pairwise classification approach of kernel methods and logistic regression is commonly a twophase method: first the entities in a document are identified, then a relation type is predicted for each pair of entities. This approach presents at least two difficulties: (1) enumerating all pairs of entities, even when restricted to pairs within a sentence, results in a low density of positive relation examples; and (2)</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of the Fifth ACM International Conference on Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
</authors>
<title>Extracting patterns and relations from the world wide web.</title>
<date>1998</date>
<booktitle>In WebDB Workshop at 6th International Conference on Extending Database Technology.</booktitle>
<contexts>
<context position="4794" citStr="Brin, 1998" startWordPosition="740" endWordPosition="741">anguage processing tasks (Miller et al., 2000; Roth and tau Yih, 2002; Sutton and McCallum, 2004). Finally, using relational paths between entities is also examined in (Richards and Mooney, 1992) to escape local maxima in a first-order learning system. 3 Relation Extraction as Sequence Labeling Relation extraction is the task of discovering semantic connections between entities. In text, this usually amounts to examining pairs of entities in a document and determining (from local language cues) whether a relation exists between them. Common approaches to this problem include pattern matching (Brin, 1998; Agichtein and Gravano, 2000), kernel methods (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2006), logistic regression (Kambhatla, 2004), and augmented parsing (Miller et al., 2000). The pairwise classification approach of kernel methods and logistic regression is commonly a twophase method: first the entities in a document are identified, then a relation type is predicted for each pair of entities. This approach presents at least two difficulties: (1) enumerating all pairs of entities, even when restricted to pairs within a sentence, results in a low density of posit</context>
</contexts>
<marker>Brin, 1998</marker>
<rawString>Sergey Brin. 1998. Extracting patterns and relations from the world wide web. In WebDB Workshop at 6th International Conference on Extending Database Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Subsequence kernels for relation extraction.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 18.</booktitle>
<editor>In Y. Weiss, B. Scholkopf, and J. Platt, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4917" citStr="Bunescu and Mooney, 2006" startWordPosition="756" endWordPosition="759">using relational paths between entities is also examined in (Richards and Mooney, 1992) to escape local maxima in a first-order learning system. 3 Relation Extraction as Sequence Labeling Relation extraction is the task of discovering semantic connections between entities. In text, this usually amounts to examining pairs of entities in a document and determining (from local language cues) whether a relation exists between them. Common approaches to this problem include pattern matching (Brin, 1998; Agichtein and Gravano, 2000), kernel methods (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2006), logistic regression (Kambhatla, 2004), and augmented parsing (Miller et al., 2000). The pairwise classification approach of kernel methods and logistic regression is commonly a twophase method: first the entities in a document are identified, then a relation type is predicted for each pair of entities. This approach presents at least two difficulties: (1) enumerating all pairs of entities, even when restricted to pairs within a sentence, results in a low density of positive relation examples; and (2) errors in the entity recognition phase can propagate to errors in the relation classificatio</context>
</contexts>
<marker>Bunescu, Mooney, 2006</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2006. Subsequence kernels for relation extraction. In Y. Weiss, B. Scholkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Dan DiPasquo</author>
<author>Dayne Freitag</author>
<author>Andrew K McCallum</author>
<author>Tom M Mitchell</author>
<author>Kamal Nigam</author>
<author>Sean Slattery</author>
</authors>
<title>Learning to extract symbolic knowledge from the World Wide Web.</title>
<date>1998</date>
<booktitle>In Proceedings of AAAI-98, 15th Conference of the American Association for Artificial Intelligence,</booktitle>
<pages>509516</pages>
<publisher>AAAI Press,</publisher>
<location>Madison, US.</location>
<contexts>
<context position="4090" citStr="Craven et al., 1998" startWordPosition="626" endWordPosition="629">obabilistically into extraction to improve its accuracy; also, our work focuses on mining from relational graphs, rather than single-table databases. McCallum and Jensen (2003) argue the theoretical benefits of an integrated probabilistic model for extraction and mining, but do not construct such a system. Our work is a step in the direction of their proposal, using an inference procedure based on a closed-loop iteration between extraction and relational pattern discovery. Most other work in this area mines raw text, rather than a database automatically populated via extraction (Hearst, 1999; Craven et al., 1998). This work can also be viewed as part of a trend to perform joint inference across multiple language processing tasks (Miller et al., 2000; Roth and tau Yih, 2002; Sutton and McCallum, 2004). Finally, using relational paths between entities is also examined in (Richards and Mooney, 1992) to escape local maxima in a first-order learning system. 3 Relation Extraction as Sequence Labeling Relation extraction is the task of discovering semantic connections between entities. In text, this usually amounts to examining pairs of entities in a document and determining (from local language cues) whethe</context>
</contexts>
<marker>Craven, DiPasquo, Freitag, McCallum, Mitchell, Nigam, Slattery, 1998</marker>
<rawString>Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew K. McCallum, Tom M. Mitchell, Kamal Nigam, and Sean Slattery. 1998. Learning to extract symbolic knowledge from the World Wide Web. In Proceedings of AAAI-98, 15th Conference of the American Association for Artificial Intelligence, pages 509516, Madison, US. AAAI Press, Menlo Park, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
</authors>
<title>Confidence estimation for information extraction.</title>
<date>2004</date>
<booktitle>In Human Langauge Technology Conference (HLT 2004),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="13538" citStr="Culotta and McCallum (2004)" startWordPosition="2151" endWordPosition="2155">s found in this automatically generated database. In this manner, we create a closed-loop system that alternates between bottomup extraction and top-down pattern discovery. This approach can be viewed as a type of alternating optimization, with analogies to formal methods such as expectation-maximization. The uncertainty in the bottom-up extraction step is handled by estimating the confidence of each extraction and pruning the database to remove entries with low confidence. One of the benefits of a probabilistic extraction model is that confidence estimates can be straight-forwardly obtained. Culotta and McCallum (2004) describe the constrained forward-backward algorithm to efficiently estimate the conditional probability that a segment of text is correctly extracted by a CRF. Using this algorithm, we associate a confidence value with each relation extracted by the CRF. This confidence value is then used to limit the noise introduced by incorrect extractions. This differs from Nahm and Mooney (2000) and Mooney and Bunescu (2005), in which standard decision tree rule learners are applied to the unfiltered output of extraction. 4.3 Extracting Implicit Relations An implicit relation is one that does not have di</context>
</contexts>
<marker>Culotta, McCallum, 2004</marker>
<rawString>Aron Culotta and Andrew McCallum. 2004. Confidence estimation for information extraction. In Human Langauge Technology Conference (HLT 2004), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4890" citStr="Culotta and Sorensen, 2004" startWordPosition="752" endWordPosition="755">d McCallum, 2004). Finally, using relational paths between entities is also examined in (Richards and Mooney, 1992) to escape local maxima in a first-order learning system. 3 Relation Extraction as Sequence Labeling Relation extraction is the task of discovering semantic connections between entities. In text, this usually amounts to examining pairs of entities in a document and determining (from local language cues) whether a relation exists between them. Common approaches to this problem include pattern matching (Brin, 1998; Agichtein and Gravano, 2000), kernel methods (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2006), logistic regression (Kambhatla, 2004), and augmented parsing (Miller et al., 2000). The pairwise classification approach of kernel methods and logistic regression is commonly a twophase method: first the entities in a document are identified, then a relation type is predicted for each pair of entities. This approach presents at least two difficulties: (1) enumerating all pairs of entities, even when restricted to pairs within a sentence, results in a low density of positive relation examples; and (2) errors in the entity recognition phase can propagate to errors in</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Dehaspe</author>
</authors>
<title>Maximum entropy modeling with clausal constraints.</title>
<date>1997</date>
<booktitle>In Proceedings of the Seventh International Workshop on Inductive Logic Programming,</booktitle>
<pages>109125</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="11461" citStr="Dehaspe (1997)" startWordPosition="1836" endWordPosition="1837"> receive higher weights, while patterns that have little effect on performance will receive low weights. We can explore the space of possible conjunctions of these patterns using feature induction for CRFs, as described in McCallum (2003). Search through the large space of possible conjunctions is guided 298 \x0cby adding features that are estimated to increase the likelihood function most. When feature induction is used with relational patterns, we can view this as a type of data mining, in which patterns are created based on their influence on an extraction model. This is similar to work by Dehaspe (1997), where inductive logic programming is embedded as a feature induction technique for a maximum entropy classifier. Our work restricts induced features to conjunctions of base features, rather than using first-order clauses. However, the patterns we learn are based on information extracted from natural language. 4.2 Iterative Database Construction The top-down knowledge provided by data mining algorithms has the potential to improve the performance of information extraction systems. Conversely, bottom-up knowledge generated by extraction systems can be used to populate a large database, from wh</context>
</contexts>
<marker>Dehaspe, 1997</marker>
<rawString>L. Dehaspe. 1997. Maximum entropy modeling with clausal constraints. In Proceedings of the Seventh International Workshop on Inductive Logic Programming, pages 109125, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Untangling text data mining.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4068" citStr="Hearst, 1999" startWordPosition="624" endWordPosition="625">be included probabilistically into extraction to improve its accuracy; also, our work focuses on mining from relational graphs, rather than single-table databases. McCallum and Jensen (2003) argue the theoretical benefits of an integrated probabilistic model for extraction and mining, but do not construct such a system. Our work is a step in the direction of their proposal, using an inference procedure based on a closed-loop iteration between extraction and relational pattern discovery. Most other work in this area mines raw text, rather than a database automatically populated via extraction (Hearst, 1999; Craven et al., 1998). This work can also be viewed as part of a trend to perform joint inference across multiple language processing tasks (Miller et al., 2000; Roth and tau Yih, 2002; Sutton and McCallum, 2004). Finally, using relational paths between entities is also examined in (Richards and Mooney, 1992) to escape local maxima in a first-order learning system. 3 Relation Extraction as Sequence Labeling Relation extraction is the task of discovering semantic connections between entities. In text, this usually amounts to examining pairs of entities in a document and determining (from local</context>
</contexts>
<marker>Hearst, 1999</marker>
<rawString>M. Hearst. 1999. Untangling text data mining. In 37th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4956" citStr="Kambhatla, 2004" startWordPosition="763" endWordPosition="764">examined in (Richards and Mooney, 1992) to escape local maxima in a first-order learning system. 3 Relation Extraction as Sequence Labeling Relation extraction is the task of discovering semantic connections between entities. In text, this usually amounts to examining pairs of entities in a document and determining (from local language cues) whether a relation exists between them. Common approaches to this problem include pattern matching (Brin, 1998; Agichtein and Gravano, 2000), kernel methods (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2006), logistic regression (Kambhatla, 2004), and augmented parsing (Miller et al., 2000). The pairwise classification approach of kernel methods and logistic regression is commonly a twophase method: first the entities in a document are identified, then a relation type is predicted for each pair of entities. This approach presents at least two difficulties: (1) enumerating all pairs of entities, even when restricted to pairs within a sentence, results in a low density of positive relation examples; and (2) errors in the entity recognition phase can propagate to errors in the relation classification stage. As an example of the latter di</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. 18th International Conf. on Machine Learning,</booktitle>
<pages>282289</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="2773" citStr="Lafferty et al., 2001" startWordPosition="423" endWordPosition="426">and incomplete. More importantly, many relational patterns unknown a priori may both improve extraction accuracy and uncover informative trends in the data (e.g. that children often adopt the religion of their parents). Indeed, the goal of data mining is to learn such patterns from database regularities. Since these patterns will not always hold, we would like to handle them probabilistically. We propose an integrated supervised machine learning method that learns both contextual and relational patterns to extract relations. In particular, we construct a linear-chain conditional random field (Lafferty et al., 2001; Sutton and McCallum, 2006) to extract relations from biographical texts while simultaneously discovering interesting relational patterns that improve extraction performance. 296 \x0c2 Related Work This work can be viewed as a step toward the integration of information extraction and data mining technology, a direction of growing interest. Nahm and Mooney (2000) present a system that mines association rules from a database constructed from automatically extracted data, then applies these learned rules to improve data field recall without revisiting the text. Our work attempts to more tightly </context>
<context position="7399" citStr="Lafferty et al., 2001" startWordPosition="1167" endWordPosition="1170"> W. Bush |{z } father and Barbara Bush |{z } mother . Additionally, by using a sequence model we can capture the dependence between adjacent labels. For example, in our data it is common to see phrases such as son of the Republican president George H. W. Bush for which the labels politicalParty, jobTitle, and father occur consecutively. Sequence models are specifically designed to handle these kinds of dependencies. We now discuss the details of our extraction model. 297 \x0c3.1 Conditional Random Fields We build a model to extract relations using linearchain conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006). CRFs are undirected graphical models (i.e. Markov networks) that are discriminatively-trained to maximize the conditional probability of a set of output variables y given a set of input variables x. This conditional distribution has the form p(y|x) = 1 Zx Y cC c(yc, xc; ) (1) where are potential functions parameterized by and Zx = P y Q cC (yc, xc) is a normalization factor. Assuming c factorizes as a log-linear combination of arbitrary features computed over clique c, then c(yc, xc; ) = exp ( P k kfk(yc, xc)), where f is a set of arbitrary feature functions over </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th International Conf. on Machine Learning, pages 282289. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Mann</author>
<author>David Yarowsky</author>
</authors>
<title>Multi-field information extraction and cross-document fusion.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="15256" citStr="Mann and Yarowsky, 2005" startWordPosition="2415" endWordPosition="2418">mplied by the text will dramatically increase the amount of information a user can uncover, effectively providing access to the implications of a corpus. We argue that integrating top-down and bottomup knowledge discovery algorithms discussed in Section 4.2 can enable this technology. By performing pattern discovery in conjunction with information extraction, we can collate facts from multiple sources to infer new relations. This is an example of cross-document fusion or cross-document information extraction, a growing area of research transforming raw extractions into usable knowledge bases (Mann and Yarowsky, 2005; Masterson and Kushmerik, 2003). 5 Experiments 5.1 Data We sampled 1127 paragraphs from 271 articles from the online encyclopedia Wikipedia1 and labeled a to1 http://www.wikipedia.org 299 \x0cGeorge W. Bush Dick Cheney underling Yale education Republican party President jobTitle George H. W. Bush son underling Harken Energy executive education party jobTitle Prescott Bush son education Bill Clinton rival Bob Dole rival education Democrat party jobTitle Hillary Clinton husband education party Halliburton executive education Pres Medal of Freedom award party Nelson Rockefeller award Elizabeth D</context>
</contexts>
<marker>Mann, Yarowsky, 2005</marker>
<rawString>Gideon Mann and David Yarowsky. 2005. Multi-field information extraction and cross-document fusion. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Masterson</author>
<author>N Kushmerik</author>
</authors>
<title>Information extraction from multi-document threads.</title>
<date>2003</date>
<booktitle>In ECML-2003: Workshop on Adaptive Text Extraction and Mining,</booktitle>
<pages>3441</pages>
<contexts>
<context position="15288" citStr="Masterson and Kushmerik, 2003" startWordPosition="2419" endWordPosition="2422">ramatically increase the amount of information a user can uncover, effectively providing access to the implications of a corpus. We argue that integrating top-down and bottomup knowledge discovery algorithms discussed in Section 4.2 can enable this technology. By performing pattern discovery in conjunction with information extraction, we can collate facts from multiple sources to infer new relations. This is an example of cross-document fusion or cross-document information extraction, a growing area of research transforming raw extractions into usable knowledge bases (Mann and Yarowsky, 2005; Masterson and Kushmerik, 2003). 5 Experiments 5.1 Data We sampled 1127 paragraphs from 271 articles from the online encyclopedia Wikipedia1 and labeled a to1 http://www.wikipedia.org 299 \x0cGeorge W. Bush Dick Cheney underling Yale education Republican party President jobTitle George H. W. Bush son underling Harken Energy executive education party jobTitle Prescott Bush son education Bill Clinton rival Bob Dole rival education Democrat party jobTitle Hillary Clinton husband education party Halliburton executive education Pres Medal of Freedom award party Nelson Rockefeller award Elizabeth Dole wife WWII participant award </context>
</contexts>
<marker>Masterson, Kushmerik, 2003</marker>
<rawString>D. Masterson and N. Kushmerik. 2003. Information extraction from multi-document threads. In ECML-2003: Workshop on Adaptive Text Extraction and Mining, pages 3441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>David Jensen</author>
</authors>
<title>A note on the unification of information extraction and data mining using conditional-probability, relational models.</title>
<date>2003</date>
<booktitle>In IJCAI03 Workshop on Learning Statistical Models from Relational Data.</booktitle>
<contexts>
<context position="3646" citStr="McCallum and Jensen (2003)" startWordPosition="554" endWordPosition="557">egration of information extraction and data mining technology, a direction of growing interest. Nahm and Mooney (2000) present a system that mines association rules from a database constructed from automatically extracted data, then applies these learned rules to improve data field recall without revisiting the text. Our work attempts to more tightly integrate the extraction and mining tasks by learning relational patterns that can be included probabilistically into extraction to improve its accuracy; also, our work focuses on mining from relational graphs, rather than single-table databases. McCallum and Jensen (2003) argue the theoretical benefits of an integrated probabilistic model for extraction and mining, but do not construct such a system. Our work is a step in the direction of their proposal, using an inference procedure based on a closed-loop iteration between extraction and relational pattern discovery. Most other work in this area mines raw text, rather than a database automatically populated via extraction (Hearst, 1999; Craven et al., 1998). This work can also be viewed as part of a trend to perform joint inference across multiple language processing tasks (Miller et al., 2000; Roth and tau Yi</context>
</contexts>
<marker>McCallum, Jensen, 2003</marker>
<rawString>Andrew McCallum and David Jensen. 2003. A note on the unification of information extraction and data mining using conditional-probability, relational models. In IJCAI03 Workshop on Learning Statistical Models from Relational Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="18362" citStr="McCallum, 2002" startWordPosition="2893" endWordPosition="2895">des an automated way to detect entities in the text, although these entities are not classified by type. This also allows us to easily construct database queries, since we can reason at the entity level, rather than the token level. (Although, see Sarawagi and Cohen (2004) for extensions of CRFs that model the entity length distribution.) The results we report here are constrained to predict relations only for hyperlinked entities. Note that despite this property, we still desire to use a sequence model to capture the dependencies between adjacent labels. We use the MALLET CRF implementation (McCallum, 2002) with the default regularization parameters. Based on initial experiments, we restrict relational path features to length two or three. Paths of length one will learn trivial paths and can lead to overfitting. Paths longer than three can increase computational costs without adding much new information. In addition to the relational pattern features described in Section 4, the list of local features includes context words (such as the token identity within a 6 word window of the target token), lexicons (such as whether a token appears in a list of cities, people, or companies), regular expressi</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Efficiently inducing features of conditional random fields.</title>
<date>2003</date>
<booktitle>In Nineteenth Conference on Uncertainty in Artificial Intelligence (UAI03).</booktitle>
<contexts>
<context position="11085" citStr="McCallum (2003)" startWordPosition="1773" endWordPosition="1774"> patterns may have lower weights than high precision patterns, but they will still influence the extractor. A nice property of this approach is that examining highly weighted patterns can provide insight into regularities of the data. 4.1 Feature Induction During CRF training, weights are learned for each relational pattern. Patterns that increase extraction performance will receive higher weights, while patterns that have little effect on performance will receive low weights. We can explore the space of possible conjunctions of these patterns using feature induction for CRFs, as described in McCallum (2003). Search through the large space of possible conjunctions is guided 298 \x0cby adding features that are estimated to increase the likelihood function most. When feature induction is used with relational patterns, we can view this as a type of data mining, in which patterns are created based on their influence on an extraction model. This is similar to work by Dehaspe (1997), where inductive logic programming is embedded as a feature induction technique for a maximum entropy classifier. Our work restricts induced features to conjunctions of base features, rather than using first-order clauses. </context>
</contexts>
<marker>McCallum, 2003</marker>
<rawString>Andrew McCallum. 2003. Efficiently inducing features of conditional random fields. In Nineteenth Conference on Uncertainty in Artificial Intelligence (UAI03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Heidi Fox</author>
<author>Lance A Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>A novel use of statistical parsing to extract information from text.</title>
<date>2000</date>
<booktitle>In ANLP.</booktitle>
<contexts>
<context position="4229" citStr="Miller et al., 2000" startWordPosition="650" endWordPosition="653">atabases. McCallum and Jensen (2003) argue the theoretical benefits of an integrated probabilistic model for extraction and mining, but do not construct such a system. Our work is a step in the direction of their proposal, using an inference procedure based on a closed-loop iteration between extraction and relational pattern discovery. Most other work in this area mines raw text, rather than a database automatically populated via extraction (Hearst, 1999; Craven et al., 1998). This work can also be viewed as part of a trend to perform joint inference across multiple language processing tasks (Miller et al., 2000; Roth and tau Yih, 2002; Sutton and McCallum, 2004). Finally, using relational paths between entities is also examined in (Richards and Mooney, 1992) to escape local maxima in a first-order learning system. 3 Relation Extraction as Sequence Labeling Relation extraction is the task of discovering semantic connections between entities. In text, this usually amounts to examining pairs of entities in a document and determining (from local language cues) whether a relation exists between them. Common approaches to this problem include pattern matching (Brin, 1998; Agichtein and Gravano, 2000), ker</context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 2000</marker>
<rawString>Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph Weischedel. 2000. A novel use of statistical parsing to extract information from text. In ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
<author>Razvan Bunescu</author>
</authors>
<title>Mining knowledge from text using information extraction.</title>
<date>2005</date>
<booktitle>SigKDD Explorations on Text Mining and Natural Language Processing.</booktitle>
<contexts>
<context position="13955" citStr="Mooney and Bunescu (2005)" startWordPosition="2215" endWordPosition="2218">pruning the database to remove entries with low confidence. One of the benefits of a probabilistic extraction model is that confidence estimates can be straight-forwardly obtained. Culotta and McCallum (2004) describe the constrained forward-backward algorithm to efficiently estimate the conditional probability that a segment of text is correctly extracted by a CRF. Using this algorithm, we associate a confidence value with each relation extracted by the CRF. This confidence value is then used to limit the noise introduced by incorrect extractions. This differs from Nahm and Mooney (2000) and Mooney and Bunescu (2005), in which standard decision tree rule learners are applied to the unfiltered output of extraction. 4.3 Extracting Implicit Relations An implicit relation is one that does not have direct contextual evidence, for example the cousin relation in our initial example. Implicit relations generally require some background knowledge to be detected, such as relational patterns (e.g. rules about familial relations). These are the sorts of relations on which current extraction models perform most poorly. Notably, these are exactly the sorts of relations that are likely to have the biggest impact on info</context>
</contexts>
<marker>Mooney, Bunescu, 2005</marker>
<rawString>Raymond J. Mooney and Razvan Bunescu. 2005. Mining knowledge from text using information extraction. SigKDD Explorations on Text Mining and Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Un Yong Nahm</author>
<author>Raymond J Mooney</author>
</authors>
<title>A mutually beneficial integration of data mining and information extraction.</title>
<date>2000</date>
<booktitle>In AAAI/IAAI.</booktitle>
<contexts>
<context position="3138" citStr="Nahm and Mooney (2000)" startWordPosition="477" endWordPosition="480">o handle them probabilistically. We propose an integrated supervised machine learning method that learns both contextual and relational patterns to extract relations. In particular, we construct a linear-chain conditional random field (Lafferty et al., 2001; Sutton and McCallum, 2006) to extract relations from biographical texts while simultaneously discovering interesting relational patterns that improve extraction performance. 296 \x0c2 Related Work This work can be viewed as a step toward the integration of information extraction and data mining technology, a direction of growing interest. Nahm and Mooney (2000) present a system that mines association rules from a database constructed from automatically extracted data, then applies these learned rules to improve data field recall without revisiting the text. Our work attempts to more tightly integrate the extraction and mining tasks by learning relational patterns that can be included probabilistically into extraction to improve its accuracy; also, our work focuses on mining from relational graphs, rather than single-table databases. McCallum and Jensen (2003) argue the theoretical benefits of an integrated probabilistic model for extraction and mini</context>
<context position="10235" citStr="Nahm and Mooney (2000)" startWordPosition="1637" endWordPosition="1640">calculate a feature for the entity George W. Bush that indicates the path from John Ellis to George W. Bush in the database, annotating each edge in the path with its relation label; i.e. father-sibling-son. By abstracting away the actual entity names, we have created a cousin template feature, as shown in Figure 2. By adding these relational paths as features to the model, we can learn interesting relational patterns that may have low precision (e.g. people are likely to be friends with their classmates) without hampering extraction performance. This is in contrast to the system described in Nahm and Mooney (2000), in which patterns are induced from a noisy database and then applied directly to extraction. In our system, since each learned path has an associated weight, it is simply another piece of evidence to help the extractor. Low precision patterns may have lower weights than high precision patterns, but they will still influence the extractor. A nice property of this approach is that examining highly weighted patterns can provide insight into regularities of the data. 4.1 Feature Induction During CRF training, weights are learned for each relational pattern. Patterns that increase extraction perf</context>
<context position="13925" citStr="Nahm and Mooney (2000)" startWordPosition="2210" endWordPosition="2213">nce of each extraction and pruning the database to remove entries with low confidence. One of the benefits of a probabilistic extraction model is that confidence estimates can be straight-forwardly obtained. Culotta and McCallum (2004) describe the constrained forward-backward algorithm to efficiently estimate the conditional probability that a segment of text is correctly extracted by a CRF. Using this algorithm, we associate a confidence value with each relation extracted by the CRF. This confidence value is then used to limit the noise introduced by incorrect extractions. This differs from Nahm and Mooney (2000) and Mooney and Bunescu (2005), in which standard decision tree rule learners are applied to the unfiltered output of extraction. 4.3 Extracting Implicit Relations An implicit relation is one that does not have direct contextual evidence, for example the cousin relation in our initial example. Implicit relations generally require some background knowledge to be detected, such as relational patterns (e.g. rules about familial relations). These are the sorts of relations on which current extraction models perform most poorly. Notably, these are exactly the sorts of relations that are likely to h</context>
</contexts>
<marker>Nahm, Mooney, 2000</marker>
<rawString>Un Yong Nahm and Raymond J. Mooney. 2000. A mutually beneficial integration of data mining and information extraction. In AAAI/IAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley L Richards</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning relations by pathfinding.</title>
<date>1992</date>
<booktitle>In Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI-92),</booktitle>
<pages>5055</pages>
<location>San Jose, CA.</location>
<contexts>
<context position="4379" citStr="Richards and Mooney, 1992" startWordPosition="673" endWordPosition="676">construct such a system. Our work is a step in the direction of their proposal, using an inference procedure based on a closed-loop iteration between extraction and relational pattern discovery. Most other work in this area mines raw text, rather than a database automatically populated via extraction (Hearst, 1999; Craven et al., 1998). This work can also be viewed as part of a trend to perform joint inference across multiple language processing tasks (Miller et al., 2000; Roth and tau Yih, 2002; Sutton and McCallum, 2004). Finally, using relational paths between entities is also examined in (Richards and Mooney, 1992) to escape local maxima in a first-order learning system. 3 Relation Extraction as Sequence Labeling Relation extraction is the task of discovering semantic connections between entities. In text, this usually amounts to examining pairs of entities in a document and determining (from local language cues) whether a relation exists between them. Common approaches to this problem include pattern matching (Brin, 1998; Agichtein and Gravano, 2000), kernel methods (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2006), logistic regression (Kambhatla, 2004), and augmented parsing</context>
<context position="9150" citStr="Richards and Mooney (1992)" startWordPosition="1450" endWordPosition="1453"> their inter-dependencies. In addition to common language features (e.g. neighboring words and syntactic information), in this work we explore features that cull relational patterns from a database of entities. As described in the introductory example (Figure 1), context alone is often insufficient to extract relations. Even in simpler examples, it may be the case that modeling relational patterns can improve extraction accuracy. To capture this evidence, we compute features from a database to indicate relational connections between entities, similar to the relational pathfinding performed in Richards and Mooney (1992). Imagine that the four sentence example about the Bush family is included in a training set, and the encousin father son X Y sibling Figure 2: A feature template for the cousin relation. tities are labeled with their correct relations. In this case, the cousin relation in sentence 4 would also be labeled. From this data, we can create a relational database that contains the relations in Figure 1. Assume sentence 4 comes from a biography about John Ellis. We calculate a feature for the entity George W. Bush that indicates the path from John Ellis to George W. Bush in the database, annotating e</context>
</contexts>
<marker>Richards, Mooney, 1992</marker>
<rawString>Bradley L. Richards and Raymond J. Mooney. 1992. Learning relations by pathfinding. In Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI-92), pages 5055, San Jose, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
</authors>
<title>Probabilistic reasoning for entity and relation recognition.</title>
<date>2002</date>
<booktitle>In COLING.</booktitle>
<marker>Roth, Yih, 2002</marker>
<rawString>Dan Roth and Wen tau Yih. 2002. Probabilistic reasoning for entity and relation recognition. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semi-markov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In NIPS 04.</booktitle>
<contexts>
<context position="18020" citStr="Sarawagi and Cohen (2004)" startWordPosition="2837" endWordPosition="2840">ning set to those in the test set, we believe this methodology reflects a typical real-world scenario in which we would like to extend an existing database to a different, but slightly related, domain. The structure of the Wikipedia articles somewhat simplifies the extraction task, since important entities are hyper-linked within the text. This provides an automated way to detect entities in the text, although these entities are not classified by type. This also allows us to easily construct database queries, since we can reason at the entity level, rather than the token level. (Although, see Sarawagi and Cohen (2004) for extensions of CRFs that model the entity length distribution.) The results we report here are constrained to predict relations only for hyperlinked entities. Note that despite this property, we still desire to use a sequence model to capture the dependencies between adjacent labels. We use the MALLET CRF implementation (McCallum, 2002) with the default regularization parameters. Based on initial experiments, we restrict relational path features to length two or three. Paths of length one will learn trivial paths and can lead to overfitting. Paths longer than three can increase computation</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2004. Semi-markov conditional random fields for information extraction. In NIPS 04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2004</date>
<booktitle>In Proceedings of the Twenty-First International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="4281" citStr="Sutton and McCallum, 2004" startWordPosition="659" endWordPosition="662">e theoretical benefits of an integrated probabilistic model for extraction and mining, but do not construct such a system. Our work is a step in the direction of their proposal, using an inference procedure based on a closed-loop iteration between extraction and relational pattern discovery. Most other work in this area mines raw text, rather than a database automatically populated via extraction (Hearst, 1999; Craven et al., 1998). This work can also be viewed as part of a trend to perform joint inference across multiple language processing tasks (Miller et al., 2000; Roth and tau Yih, 2002; Sutton and McCallum, 2004). Finally, using relational paths between entities is also examined in (Richards and Mooney, 1992) to escape local maxima in a first-order learning system. 3 Relation Extraction as Sequence Labeling Relation extraction is the task of discovering semantic connections between entities. In text, this usually amounts to examining pairs of entities in a document and determining (from local language cues) whether a relation exists between them. Common approaches to this problem include pattern matching (Brin, 1998; Agichtein and Gravano, 2000), kernel methods (Zelenko et al., 2003; Culotta and Soren</context>
</contexts>
<marker>Sutton, McCallum, 2004</marker>
<rawString>Charles Sutton and Andrew McCallum. 2004. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. In Proceedings of the Twenty-First International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>An introduction to conditional random fields for relational learning.</title>
<date>2006</date>
<booktitle>In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</booktitle>
<publisher>MIT Press.</publisher>
<note>To appear.</note>
<contexts>
<context position="2801" citStr="Sutton and McCallum, 2006" startWordPosition="427" endWordPosition="430">portantly, many relational patterns unknown a priori may both improve extraction accuracy and uncover informative trends in the data (e.g. that children often adopt the religion of their parents). Indeed, the goal of data mining is to learn such patterns from database regularities. Since these patterns will not always hold, we would like to handle them probabilistically. We propose an integrated supervised machine learning method that learns both contextual and relational patterns to extract relations. In particular, we construct a linear-chain conditional random field (Lafferty et al., 2001; Sutton and McCallum, 2006) to extract relations from biographical texts while simultaneously discovering interesting relational patterns that improve extraction performance. 296 \x0c2 Related Work This work can be viewed as a step toward the integration of information extraction and data mining technology, a direction of growing interest. Nahm and Mooney (2000) present a system that mines association rules from a database constructed from automatically extracted data, then applies these learned rules to improve data field recall without revisiting the text. Our work attempts to more tightly integrate the extraction and</context>
<context position="7427" citStr="Sutton and McCallum, 2006" startWordPosition="1171" endWordPosition="1174">nd Barbara Bush |{z } mother . Additionally, by using a sequence model we can capture the dependence between adjacent labels. For example, in our data it is common to see phrases such as son of the Republican president George H. W. Bush for which the labels politicalParty, jobTitle, and father occur consecutively. Sequence models are specifically designed to handle these kinds of dependencies. We now discuss the details of our extraction model. 297 \x0c3.1 Conditional Random Fields We build a model to extract relations using linearchain conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006). CRFs are undirected graphical models (i.e. Markov networks) that are discriminatively-trained to maximize the conditional probability of a set of output variables y given a set of input variables x. This conditional distribution has the form p(y|x) = 1 Zx Y cC c(yc, xc; ) (1) where are potential functions parameterized by and Zx = P y Q cC (yc, xc) is a normalization factor. Assuming c factorizes as a log-linear combination of arbitrary features computed over clique c, then c(yc, xc; ) = exp ( P k kfk(yc, xc)), where f is a set of arbitrary feature functions over the input, each of which has</context>
</contexts>
<marker>Sutton, McCallum, 2006</marker>
<rawString>Charles Sutton and Andrew McCallum. 2006. An introduction to conditional random fields for relational learning. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning. MIT Press. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--10831106</pages>
<contexts>
<context position="4862" citStr="Zelenko et al., 2003" startWordPosition="748" endWordPosition="751">u Yih, 2002; Sutton and McCallum, 2004). Finally, using relational paths between entities is also examined in (Richards and Mooney, 1992) to escape local maxima in a first-order learning system. 3 Relation Extraction as Sequence Labeling Relation extraction is the task of discovering semantic connections between entities. In text, this usually amounts to examining pairs of entities in a document and determining (from local language cues) whether a relation exists between them. Common approaches to this problem include pattern matching (Brin, 1998; Agichtein and Gravano, 2000), kernel methods (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2006), logistic regression (Kambhatla, 2004), and augmented parsing (Miller et al., 2000). The pairwise classification approach of kernel methods and logistic regression is commonly a twophase method: first the entities in a document are identified, then a relation type is predicted for each pair of entities. This approach presents at least two difficulties: (1) enumerating all pairs of entities, even when restricted to pairs within a sentence, results in a low density of positive relation examples; and (2) errors in the entity recognition phas</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. Journal of Machine Learning Research, 3:10831106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>