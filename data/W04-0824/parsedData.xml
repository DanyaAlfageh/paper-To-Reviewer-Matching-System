<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.847595">
b&amp;apos;Multi-Component Word Sense Disambiguation
</title>
<author confidence="0.961249">
Massimiliano Ciaramita Mark Johnson
</author>
<affiliation confidence="0.97916">
Brown University
Department of Cognitive and Linguistic Sciences
</affiliation>
<address confidence="0.7063975">
Providence, RI 02912
\x01
</address>
<email confidence="0.82952">
massi@brown.edu,mark johnson@brown.edu\x02
</email>
<sectionHeader confidence="0.980976" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993728">
This paper describes the system MC-WSD pre-
sented for the English Lexical Sample task. The
system is based on a multicomponent architecture.
It consists of one classifier with two components.
One is trained on the data provided for the task. The
second is trained on this data and, additionally, on
an external training set extracted from the Wordnet
glosses. The goal of the additional component is to
lessen sparse data problems by exploiting the infor-
mation encoded in the ontology.
</bodyText>
<sectionHeader confidence="0.998207" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987290862068966">
One of the main difficulties in word sense classifi-
cation tasks stems from the fact that word senses,
such as Wordnets synsets (Fellbaum, 1998), de-
fine very specific classes1. As a consequence train-
ing instances are often too few in number to cap-
ture extremely fine-grained semantic distinctions.
Word senses, however, are not just independent enti-
ties but are connected by several semantic relations;
e.g., the is-a, which specifies a relation of inclusion
among classes such as car is-a vehicle. Based on
the is-a relation Wordnet defines large and complex
hierarchies for nouns and verbs.
These hierarchical structures encode potentially
useful world-knowledge that can be exploited for
word sense classification purposes, by providing
means for generalizing beyond the narrowest synset
level. To disambiguate an instance of a noun like
bat a system might be more successful if, in-
stead of limiting itself to applying what it knows
about the concepts bat-mammal and bat-sport-
implement, it could use additional knowledge
about other animals and artifacts.
Our system implements this intuition in two
steps. First, for each sense of an ambiguous word
we generate an additional set of training instances
\x03
We would like to thank Thomas Hofmann and our colleagues
in the Brown Laboratory for Linguistic Information Processing
(BLLIP).
</bodyText>
<page confidence="0.897062">
1
</page>
<bodyText confidence="0.998946">
51% of the noun synsets in Wordnet contain only 1 word.
from the Wordnet glosses. This data is not limited to
the specific synset that represents one of the senses
of the word, but concerns also other synsets that are
semantically similar, i.e., close in the hierarchy, to
that synset. Then, we integrate the task-specific and
the external training data with a multicomponent
classifier that simplifies the system for hierarchical
word sense disambiguation presented in (Ciaramita
et al., 2003). The classifier consists of two com-
ponents based on the averaged multiclass percep-
tron (Collins, 2002; Crammer and Singer, 2003).
The first component is trained on the task-specific
data while the second is trained on the former and
on the external training data. When predicting a la-
bel for an instance the classifier combines the pre-
dictions of the two components. Cross-validation
experiments on the training data show the advan-
tages of the multicomponent architecture.
In the following section we describe the features
used by our system. In Section 3 we explain how we
generated the additional training set. In Section 4
we describe the architecture of the classifier and in
Section 5 we discuss the specifics of the final system
and some experimental results.
</bodyText>
<sectionHeader confidence="0.99918" genericHeader="introduction">
2 Features
</sectionHeader>
<bodyText confidence="0.918740142857143">
We used a set of features similar to that which
was extensively described and evaluated in (Yoong
and Hwee, 2002). The sentence with POS annota-
tion A-DT newspaper-NN and-CC now-RB a-DT
bank-NN have-AUX since-RB taken-VBN over-
RB serves as an example to illustrate them. The
word to disambiguate is bank (or activate for (7)).
</bodyText>
<listItem confidence="0.869973">
1. part of speech of neighboring words \x04\x06\x05 ,
</listItem>
<equation confidence="0.836756">
\x07\t\x08\x0b
\x0c\x0f\x0e\x11\x10\x12\x0c\x0f\x13\x11\x10\x12\x0c\x15\x14\x16\x10\x18\x17\x11\x10\x1a\x19\x1b\x14\x1c\x10\x1a\x19\x1d\x13\x11\x10\x1e\x19\x1d\x0e\x06\x1f ; e.g., \x04!#&amp;quot;%$\&amp;apos;&amp;)( ,
</equation>
<bodyText confidence="0.371283">
\x04)*+$\&amp;apos;,), , \x04.-/&amp;quot;0$21\x163\x064 , ...
2. words in the same sentence WS or passage WC; e.g.,
</bodyText>
<page confidence="0.962475">
576
</page>
<equation confidence="0.586999">
$\&amp;apos;879\x16:.;\x1c&amp;lt; ,
</equation>
<page confidence="0.769883">
576
</page>
<equation confidence="0.7795935">
$&amp;gt;=\x1c:\x06;\x16?
@ ,
</equation>
<page confidence="0.676099">
576
</page>
<equation confidence="0.9437868">
$BA7;\x1cCEDGF\x069\x1cF7;\x16?\x16H , ...
3. n-grams:
I ,.J \x05 , \x07\t\x08\x0b
\x16\x0c\x0f\x13\x11\x10K\x0c\x15\x14\x1c\x10\x1a\x19\x1b\x14\x16\x10\x1e\x19\x1d\x137\x1f ; e.g.,
,.J7ELM$NA\x06=\x1cC , ,.J)-/&amp;quot;0$\&amp;apos;879\x16:.; , ,.J)-OLP$\&amp;apos;Q\x069\x1cR\x06;
</equation>
<table confidence="0.774337833333333">
Association for Computational Linguistics
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
\x0cI ,.J \x05\x01 \x02 ,
\x03 \x07E\x10\x05\x04\x07\x06M\x08\x0b
\x03 \x0c\x0f\x137\x10K\x0c\x15\x14\x08\x06\x1a\x10 \x03 \x0c\x15\x14\x16\x10\x1e\x19\x1b\x14\x08\x06\x1a\x10 \x03 \x19\x1b\x14\x16\x10\x1e\x19\x1d\x13\t\x06\x1e\x1f ;
e.g., ,.J\x11EL
#&amp;quot; $\&amp;apos;A7=\x1cC 9 ,
,.J
- &amp;quot;\x0b -OL%$\&amp;apos;879\x1c:\x06; Q.9\x16R\x06;
4. syntactically governing elements under a phrase J\x11&amp;quot; ;
e.g., J &amp;quot; $2Q.9\x16R\x06;
</table>
<page confidence="0.802114">
6
</page>
<listItem confidence="0.97672675">
5. syntactically governed elements under a phrase J\x06L ;
e.g., J)L+$ 9 ,)\x04 , J)LM$\&amp;apos;A7= C ,)\x04
6. coordinates \x0c\t\x0c ; e.g., \x0c\t\x0c $\&amp;apos;A7;\x1cC!DGF79\x1cF7;\x1c?
7. features for verbs, e.g, ... activate the pressure:
</listItem>
<equation confidence="0.9229755">
I number of arguments
), ; e.g.,
), $ \x14
I syntactic type of arguments
)1 ; e.g.,
)1 $2,)\x04
8. morphology/spelling:
I prefixes/suffixes up to 4
characters \x0e)\x04\x07\x0f\x10\x0e
6
; e.g.,
\x11\x13\x12
$\x15\x14 \x10 \x11\x13\x12
$\x16\x14\x18\x17 \x10 \x11\x1a\x19
$\x16\x1b\x1d\x1c \x10 \x11\x1a\x19
$\x1e\x17\x1f\x1b\x1d\x1c
I uppercase characters \x0e\x1c3 ; e.g., \x0e\x1c3 $ \x17
I number/type of words components \x0e\t!\x0f\x08\x0e#&amp;quot; ;
</equation>
<bodyText confidence="0.964393785714286">
e.g., \x0e\x1f\x1b$ \x14\x16\x10 \x0e#&amp;quot; $%$79 A.R
The same features were extracted from the given
test and training data, and the additional dataset.
POS and other syntactic features were extracted
from parse trees. Training and test data, and
the Wordnet glosses, were parsed with Charniaks
parser (Charniak, 2000). Open class words were
morphologically simplified with the morph func-
tion from the Wordnet library wn.h. When it
was not possible to identify the noun or verb in the
glosses 2 we only extracted a limited set of features:
WS, WC, and morphological features. Each gloss
provides one training instance per synset. Overall
we found approximately 200,000 features.
</bodyText>
<sectionHeader confidence="0.80428" genericHeader="method">
3 External training data
</sectionHeader>
<bodyText confidence="0.73142575">
There are 57 different ambiguous words in the task:
32 verbs, 20 nouns, and 5 adjectives. For each word
&amp;
a training set of pairs \&amp;apos;)(+*-,\x18.#*0/\x0b1*3254 , .#*7698:\&amp;apos;
</bodyText>
<equation confidence="0.7023265">
&amp;
/ , is
</equation>
<bodyText confidence="0.9289678">
generated from the task-specific data; ( * is a vector
of features and 8;\&amp;apos;
&amp;
/ is the set of possible senses for
&amp;
. Nouns are labeled with Wordnet 1.71 synset la-
bels, while verbs and adjectives are annotated with
the Wordsmyths dictionary labels. For nouns and
verbs we used the hierarchies of Wordnet to gener-
ate the additional training data. We used the given
sense map to map Wordsmyth senses to Wordnet
synsets. For adjectives we simply used the task-
specific data and a standard flat classifier.3
For each noun, or verb, synset we generated
a fixed number &amp;lt; of other semantically similar
</bodyText>
<page confidence="0.957404">
2
</page>
<bodyText confidence="0.887277">
E.g., the example sentence for the noun synset relegation
is He has been relegated to a post in Siberia,
</bodyText>
<page confidence="0.985297">
3
</page>
<bodyText confidence="0.947702">
We used Wordnet 2.0 in our experiments using the Word-
</bodyText>
<listItem confidence="0.92579275">
net sense map files to map synsets from 1.71 to 2.0.
Algorithm 1 Find &amp;lt; Closest Neighbors
1: input =?&amp;gt;A@B.DC , EGFH&amp;gt;JI , k
2: repeat
3: KMLON\x1dPRQTS+UV=HW
4: XZY\\[GL^]!_)`\x1daBbca\x10d efbTaB]!bcg5e5hDg+dRaT\&amp;apos;)K5,i&amp;lt;j/
5: for each kG6lXmY\\[ do
6: if n EGFcnpoq&amp;lt; then
7: E F LOE Fsr k
8: end if
9: end for
10: for each t;u\x1ft is a parent of K do
11: ENQUE(Q,v)
12: end for
13: DEQUE(Q)
14: until n EGFvn#&amp;gt;J&amp;lt; or =A&amp;gt;JI
</listItem>
<bodyText confidence="0.993560083333333">
synsets. For each sense we start collecting synsets
among the descendants of the sense itself and work
our way up the hierarchy following the paths from
the sense to the top until we found &amp;lt; synsets. At
each level we look for the closest &amp;lt; descendants
of the current synset as follows - this is the clos-
est descendants() function of Algorithm 1 above.
If there are &amp;lt; or less descendants we collect them
all. Otherwise, we take the closest &amp;lt; around the
synset exploiting the fact that when ordered, using
the synset IDs as keys, similar synsets tend to be
close to each other4. For example, synsets around
</bodyText>
<table confidence="0.895121285714286">
Rhode Islander refer to other American states in-
habitants names:
Synset ID Nouns
109127828 Pennsylvanian
w
109127914 Rhode Islander
109128001 South Carolinian
</table>
<bodyText confidence="0.9535392">
Algorithm 1 presents a schematic description of
the procedure. For each sense . of a noun, or verb,
we produced a set ExF of &amp;lt;M&amp;gt;zy\x10{#{ similar neighbor
synsets of . . We label this set with |. , thus for each
set of labels 8;\&amp;apos;
</bodyText>
<figure confidence="0.944821166666667">
&amp;
/ we induce a set of pseudo-labels
|
8:\&amp;apos;
&amp;
/ .For each synset in E\\F we compiled a train-
</figure>
<bodyText confidence="0.829080666666667">
ing instance from the Wordnet glosses. At the end
of this process, for each noun or verb, there is an
additional training set \&amp;apos;)( * ,}|
</bodyText>
<equation confidence="0.80039">
. * /\x0b~ .
</equation>
<sectionHeader confidence="0.969441" genericHeader="method">
4 Classifier
</sectionHeader>
<subsectionHeader confidence="0.988169">
4.1 Multiclass averaged perceptron
</subsectionHeader>
<bodyText confidence="0.996466666666667">
Our base classifier is the multiclass averaged per-
ceptron. The multiclass perceptron (Crammer and
Singer, 2003) is an on-line learning algorithm which
</bodyText>
<page confidence="0.957989">
4
</page>
<bodyText confidence="0.946135333333333">
This likely depends on the fact that the IDs encode the lo-
cation in the hierarchy, even though we dont know how the IDs
are generated.
</bodyText>
<figure confidence="0.9763234375">
\x0cAlgorithm 2 Multiclass Perceptron
1: input training data \&amp;apos;)( * ,\x18. * /\x0b1* 254 ,
\x01
2: repeat
3: for \x02 &amp;gt; y\t,\x04\x03\x05\x03\x05\x03 ,\x07\x06 do
4: \x08 * &amp;gt;?@
\t\\6 8 u\x0c\x0b)t\x0e
\x1f,\x18( *\x10\x0f\x12\x11 \x0b)t\tF\x14\x13\x18,\x18( *\x15\x0f C
5: if n \x08 * n \x11 { then
6: t F\x14\x13 L t F\x14\x13\x17\x16 (\x1d*
7: for \t 6\x18\x08 * do
8: t
L t
\x1a\x19
4
\x1b \x1c \x13 \x1b (D*
</figure>
<listItem confidence="0.8013685">
9: end for
10: end if
11: end for
12: until no more mistakes
extends to the multiclass case the standard percep-
tron. It takes as input a training set \&amp;apos;)( * ,\x18. * /\x0b1*3254 ,
</listItem>
<equation confidence="0.959274">
( * 6\x1e\x1d \x1f! , and . * 6?8;\&amp;apos;
&amp;
/ . In the multiclass per-
</equation>
<bodyText confidence="0.8645585">
ceptron, one introduces a weight vector t!F96&amp;quot;\x1d \x1f
for every .;6 8;\&amp;apos;
</bodyText>
<equation confidence="0.874198461538461">
&amp;
/ , and defines # by the so-called
winner-take-all rule
#\x13\&amp;apos;)(%$
\x01
/ &amp;gt;\&amp;apos;&amp;\x0e(\x14)+*,&amp;.-
F
/\x0e0
\x0b)t\tF!,\x18( \x0f \x03 (1)
Here
\x01
6 \x1d \x1f
\x1b 0214365 \x1b87
</equation>
<bodyText confidence="0.890709666666667">
refers to the matrix of
weights, with every column corresponding to one of
the weight vectors t\x07F . The algorithm is summarized
in Algorithm 2. Training patterns are presented one
at a time. Whenever #\x13\&amp;apos;)( * $
\x01
/:9&amp;gt;A. * an update step
is performed; otherwise the weight vectors remain
unchanged. To perform the update, one first com-
putes the error set \x08 * containing those class labels
that have received a higher score than the correct
class:
</bodyText>
<equation confidence="0.84483">
\x08 * &amp;gt;?@
\t\\6 8 u\x0c\x0b)t\x0e
\x1f,\x18( *\x10\x0f\x12\x11 \x0b)t\tF\x14\x13\x18,\x18( *\x15\x0f C (2)
</equation>
<bodyText confidence="0.796406">
We use the simplest case of uniform update weights,
</bodyText>
<figure confidence="0.915536307692307">
\x19
4
\x1b \x1c \x13 \x1b for \t 6;\x08 * .
The perceptron algorithm defines a sequence of
weight matrices
\x01 1=&amp;lt;\x145
,\x04\x03\x04\x03\x04\x03B,
\x01 1
1
5
, where
\x01 1 * 5
is the
</figure>
<figureCaption confidence="0.6753745">
weight matrix after the first \x02 training items have
been processed. In the standard perceptron, the
</figureCaption>
<figure confidence="0.7175164">
weight matrix
\x01
&amp;gt;
\x01 1
1
</figure>
<page confidence="0.740768">
5
</page>
<bodyText confidence="0.977704714285714">
is used to classify the un-
labeled test examples. However, a variety of meth-
ods can be used for regularization or smoothing in
order to reduce the effect of overtraining. Here
we used the averaged perceptron (Collins, 2002),
where the weight matrix used to classify the test
data is the average of all of the matrices posited dur-
</bodyText>
<figure confidence="0.827263606060606">
ing training, i.e.,
\x01
&amp;gt;
4
1
&amp;gt;
1*3254
\x01 *
.
4.2 Multicomponent architecture
Task specific and external training data are inte-
grated with a two-component perceptron. The dis-
Algorithm 3 Multicomponent Perceptron
1: input \&amp;apos;)( * ,\x18. * /\x0b1* 254 ,
\x01
&amp;gt;J{ ,\&amp;apos;)(@?#, |
.A?R/\x0b~? 254 , B &amp;gt; { ,
2: for C &amp;gt; y\t,\x04\x03\x05\x03\x05\x03 ,ED do
3: train M on \&amp;apos;)(@?#, |
.A?R/\x0b~? 254 and \&amp;apos;)( * ,\x18. * /\x0b1*3254
4: train V on \&amp;apos;)( * ,\x18. * /\x0b1*3254
5: end for
criminant function is defined as:
#\x13\&amp;apos;)(%$
\x01
,FB / &amp;gt;G&amp;\x0e(F)H*,&amp;.-
F
/\x0e0I1=3J5\x0eK
FL\x0b)t\x1fFT,\x18( \x0f \x16
KNM
F \x0bPO
M
F ,\x18( \x0f
</figure>
<bodyText confidence="0.984831">
The first component is trained on the task-specific
data. The second component learns a separate
weight matrix B , where each column vector rep-
resents the set label |. , and is trained on both the
task-specific and the additional training sets. Each
component is weighted by a parameter
</bodyText>
<figure confidence="0.755709666666667">
K
; here
KQM
F
is simply equal to y \x19
K
F . We experimented with
two values for
K
</figure>
<bodyText confidence="0.998177529411765">
F , namely 1 and 0.5. In the for-
mer case only the first component is used, in the
latter they are both used, and their contributions are
equally weighted.
The training procedure for the multicomponent
classifier is described in Algorithm 3. This is a sim-
plification of the algorithm presented in (Ciaramita
et al., 2003). The two algorithms are similar except
that convergence, if the data is separable, is clear
in this case because the two components are trained
individually with the standard multiclass perceptron
procedure. Convergence is typically achieved in
less than 50 iterations, but the value for D to be used
for evaluation on the unseen test data was chosen by
cross-validation. With this version of the algorithm
the implementation is simpler especially if several
components are included.
</bodyText>
<subsectionHeader confidence="0.998684">
4.3 Multilabel cases
</subsectionHeader>
<bodyText confidence="0.965482625">
Often, several senses of an ambiguous word are very
close in the hierarchy. Thus it can happen that a
synset belongs to the neighbor set of more than one
sense of the ambiguous word. When this is the case
the training instance for that synset is treated as a
multilabeled instance; i.e., |
. * is actually a set of la-
bels for (D* , that is, |
</bodyText>
<figure confidence="0.730088">
.\x07*2R |
8:\&amp;apos;
&amp;
/ . Several methods can
</figure>
<bodyText confidence="0.79784375">
be used to deal with multilabeled instances, here we
use a simple generalization of Algorithm 2. The er-
ror set for a multilabel training instance is defined
as:
</bodyText>
<equation confidence="0.668963666666667">
\x08 * &amp;gt;A@
\t 6M8 u@Sc. 6 . * ,A\x0b)t\x0e
,\x18( *\x10\x0f\x1a\x11 \x0b)t\tF\x07,\x18( *\x10\x0f C (3)
</equation>
<bodyText confidence="0.455578666666667">
which is equivalent to the definition in Equation 2
when n . * nj&amp;gt; y . The positive update of Algorithm 2
(line 6) is also redefined. The update concerns a set
</bodyText>
<figure confidence="0.894135461538462">
\x0cword
K
FZ&amp;gt; y
K
FZ&amp;gt; {@\x03\x01 word
K
F7&amp;gt; y
K
FH&amp;gt; {@\x03\x01 word
K
F7&amp;gt; y
K
F7&amp;gt;J{@\x03\x01
</figure>
<table confidence="0.992189571428571">
appear 86.1 85.5 audience 84.8 86.8 encounter 72.9 75.0
arm 85.9 87.5 bank 82.9 82.1 watch 77.1 77.9
ask 61.9 62.7 begin 57.0 61.5 hear 65.6 68.7
lose 53.1 52.5 eat 85.7 85.0 party 77.1 79.0
expect 76.6 75.9 mean 76.5 77.5 image 66.3 67.8
note 59.6 60.4 difficulty 49.2 54.2 write 68.3 65.0
plan 77.2 78.3 disc 72.1 74.1 paper 56.3 57.7
</table>
<tableCaption confidence="0.998427">
Table 1. Results on several words from the cross-validation experiments on the training data. Accuracies are reported
</tableCaption>
<bodyText confidence="0.616438333333333">
for the best value of \x02 , which is then chosen as the value for the final system, together with the value \x03\x05\x04 that performed
better. On most words the multicomponent model outperforms the flat one
of labels 8 *\x07\x06
</bodyText>
<figure confidence="0.970503727272727">
|
8:\&amp;apos;
&amp;
/ such that there are incorrect
labels wich achieved a better score; i.e., 8 * &amp;gt; @B. 6
. * u\x17SL\t\t\x086 . * ,A\x0b)t\x0e
\t,\x18( *\x10\x0f \x11 \x0b)t\tFT,\x18( *\x15\x0f C . For each .\x1e6 8 *
the update is equal to \x16
4
\x1b 0
\x13 \x1b , which, again, reduces
</figure>
<bodyText confidence="0.855955">
to the former case when n 8 * n!&amp;gt; y .
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
5 Results
</sectionHeader>
<bodyText confidence="0.933462928571428">
Table 1 presents results from a set of experiments
performed by cross-validation on the training data,
for several nouns and verbs.For 37 nouns and verbs,
out of 52, the two-component model was more ac-
curate than the flat model5. We used the results from
these experiments to set, separately for each word,
the parameters D , which was equal to 13.9 on av-
erage, and
K
F . For adjectives we only set the pa-
rameter D and used the standard flat perceptron.
For each word in the task we separately trained one
classifier. The system accuracy on the unseen test
set is summarized in the following table:
</bodyText>
<table confidence="0.966182">
Measure Precision Recall
Fine all POS 71.1 71.1%
Coarse all POS 78.1 78.1%
Fine verbs 72.5 72.5%
Coarse verbs 80.0 80.0%
Fine nouns 71.3 71.3%
Coarse nouns 77.4 77.4%
Fine adjectives 49.7 49.7%
Coarse adjectives 63.5 63.5%
</table>
<bodyText confidence="0.995596714285714">
Overall the system has the following advantages
over that of (Ciaramita et al., 2003). Selecting the
external training data based on the most similar &amp;lt;
synsets has the advantage, over using supersenses,
of generating an equivalent amount of additional
data for each word sense. The additional data for
each synset is also more homogeneous, thus the
</bodyText>
<page confidence="0.909319">
5
</page>
<bodyText confidence="0.927594615384616">
Since \x03 \x04 is an adjustable parameter it is possible that,
with different values for \x03 \x04 , the multicomponent model would
achieve even better performances.
model should have less variance6. The multicom-
ponent architecture is simpler and has an obvious
convergence proof. Convergence is faster and train-
ing is efficient. It takes less than one hour to build
and train all final systems and generate the complete
test results. We used the averaged version of the per-
ceptron and introduced an adjustable parameter
K
to
weigh each components contribution separately.
</bodyText>
<sectionHeader confidence="0.961998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999006851851852">
E. Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of the 38th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL 2000).
M. Ciaramita, T. Hofmann, and M. Johnson.
2003. Hierarchical Semantic Classification:
Word Sense Disambiguation with World Knowl-
edge. In Proceedings of the 18th International
Joint Conference on Artificial Intelligence (IJCAI
2003).
M. Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Ex-
periments with Perceptron Algorithms. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP
2002), pages 18.
K. Crammer and Y. Singer. 2003. Ultraconserva-
tive Online Algorithms for Multiclass Problems.
Journal of Machine Learning Research, 3.
C. Fellbaum. 1998. WordNet: An Electronic Lexi-
cal Database. MIT Press, Cambridge, MA.
K.L Yoong and T.N. Hwee. 2002. An Empirical
Evaluation of Knowledge Sources and Learning
Algorithms for Word Sense Disambiguation. In
Proceedings of the 2002 Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP 2002).
</reference>
<page confidence="0.993913">
6
</page>
<bodyText confidence="0.863261">
Of course the supersense level, or any other level, can sim-
ply be added as an additional component.
\x0c&amp;apos;
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.222455">
<title confidence="0.999879">b&amp;apos;Multi-Component Word Sense Disambiguation</title>
<author confidence="0.999985">Massimiliano Ciaramita Mark Johnson</author>
<affiliation confidence="0.999402">Brown University Department of Cognitive and Linguistic Sciences</affiliation>
<address confidence="0.996284">Providence, RI 02912</address>
<note confidence="0.331234">x01 massi@brown.edu,mark johnson@brown.edu\x02</note>
<abstract confidence="0.992452818181818">This paper describes the system MC-WSD presented for the English Lexical Sample task. The system is based on a multicomponent architecture. It consists of one classifier with two components. One is trained on the data provided for the task. The second is trained on this data and, additionally, on an external training set extracted from the Wordnet glosses. The goal of the additional component is to lessen sparse data problems by exploiting the information encoded in the ontology.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A Maximum-Entropy-Inspired Parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="5898" citStr="Charniak, 2000" startWordPosition="830" endWordPosition="831">aracters \x0e)\x04\x07\x0f\x10\x0e 6 ; e.g., \x11\x13\x12 $\x15\x14 \x10 \x11\x13\x12 $\x16\x14\x18\x17 \x10 \x11\x1a\x19 $\x16\x1b\x1d\x1c \x10 \x11\x1a\x19 $\x1e\x17\x1f\x1b\x1d\x1c I uppercase characters \x0e\x1c3 ; e.g., \x0e\x1c3 $ \x17 I number/type of words components \x0e\t!\x0f\x08\x0e#&amp;quot; ; e.g., \x0e\x1f\x1b$ \x14\x16\x10 \x0e#&amp;quot; $%$79 A.R The same features were extracted from the given test and training data, and the additional dataset. POS and other syntactic features were extracted from parse trees. Training and test data, and the Wordnet glosses, were parsed with Charniaks parser (Charniak, 2000). Open class words were morphologically simplified with the morph function from the Wordnet library wn.h. When it was not possible to identify the noun or verb in the glosses 2 we only extracted a limited set of features: WS, WC, and morphological features. Each gloss provides one training instance per synset. Overall we found approximately 200,000 features. 3 External training data There are 57 different ambiguous words in the task: 32 verbs, 20 nouns, and 5 adjectives. For each word &amp; a training set of pairs \&amp;apos;)(+*-,\x18.#*0/\x0b1*3254 , .#*7698:\&amp;apos; &amp; / , is generated from the task-specific d</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A Maximum-Entropy-Inspired Parser. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>T Hofmann</author>
<author>M Johnson</author>
</authors>
<title>Hierarchical Semantic Classification: Word Sense Disambiguation with World Knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<contexts>
<context position="2556" citStr="Ciaramita et al., 2003" startWordPosition="389" endWordPosition="392">e would like to thank Thomas Hofmann and our colleagues in the Brown Laboratory for Linguistic Information Processing (BLLIP). 1 51% of the noun synsets in Wordnet contain only 1 word. from the Wordnet glosses. This data is not limited to the specific synset that represents one of the senses of the word, but concerns also other synsets that are semantically similar, i.e., close in the hierarchy, to that synset. Then, we integrate the task-specific and the external training data with a multicomponent classifier that simplifies the system for hierarchical word sense disambiguation presented in (Ciaramita et al., 2003). The classifier consists of two components based on the averaged multiclass perceptron (Collins, 2002; Crammer and Singer, 2003). The first component is trained on the task-specific data while the second is trained on the former and on the external training data. When predicting a label for an instance the classifier combines the predictions of the two components. Cross-validation experiments on the training data show the advantages of the multicomponent architecture. In the following section we describe the features used by our system. In Section 3 we explain how we generated the additional </context>
<context position="12713" citStr="Ciaramita et al., 2003" startWordPosition="2030" endWordPosition="2033">component learns a separate weight matrix B , where each column vector represents the set label |. , and is trained on both the task-specific and the additional training sets. Each component is weighted by a parameter K ; here KQM F is simply equal to y \x19 K F . We experimented with two values for K F , namely 1 and 0.5. In the former case only the first component is used, in the latter they are both used, and their contributions are equally weighted. The training procedure for the multicomponent classifier is described in Algorithm 3. This is a simplification of the algorithm presented in (Ciaramita et al., 2003). The two algorithms are similar except that convergence, if the data is separable, is clear in this case because the two components are trained individually with the standard multiclass perceptron procedure. Convergence is typically achieved in less than 50 iterations, but the value for D to be used for evaluation on the unseen test data was chosen by cross-validation. With this version of the algorithm the implementation is simpler especially if several components are included. 4.3 Multilabel cases Often, several senses of an ambiguous word are very close in the hierarchy. Thus it can happen</context>
<context position="16044" citStr="Ciaramita et al., 2003" startWordPosition="2627" endWordPosition="2630">arately for each word, the parameters D , which was equal to 13.9 on average, and K F . For adjectives we only set the parameter D and used the standard flat perceptron. For each word in the task we separately trained one classifier. The system accuracy on the unseen test set is summarized in the following table: Measure Precision Recall Fine all POS 71.1 71.1% Coarse all POS 78.1 78.1% Fine verbs 72.5 72.5% Coarse verbs 80.0 80.0% Fine nouns 71.3 71.3% Coarse nouns 77.4 77.4% Fine adjectives 49.7 49.7% Coarse adjectives 63.5 63.5% Overall the system has the following advantages over that of (Ciaramita et al., 2003). Selecting the external training data based on the most similar &amp;lt; synsets has the advantage, over using supersenses, of generating an equivalent amount of additional data for each word sense. The additional data for each synset is also more homogeneous, thus the 5 Since \x03 \x04 is an adjustable parameter it is possible that, with different values for \x03 \x04 , the multicomponent model would achieve even better performances. model should have less variance6. The multicomponent architecture is simpler and has an obvious convergence proof. Convergence is faster and training is efficient. It </context>
</contexts>
<marker>Ciaramita, Hofmann, Johnson, 2003</marker>
<rawString>M. Ciaramita, T. Hofmann, and M. Johnson. 2003. Hierarchical Semantic Classification: Word Sense Disambiguation with World Knowledge. In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>18</pages>
<contexts>
<context position="2658" citStr="Collins, 2002" startWordPosition="407" endWordPosition="408">ssing (BLLIP). 1 51% of the noun synsets in Wordnet contain only 1 word. from the Wordnet glosses. This data is not limited to the specific synset that represents one of the senses of the word, but concerns also other synsets that are semantically similar, i.e., close in the hierarchy, to that synset. Then, we integrate the task-specific and the external training data with a multicomponent classifier that simplifies the system for hierarchical word sense disambiguation presented in (Ciaramita et al., 2003). The classifier consists of two components based on the averaged multiclass perceptron (Collins, 2002; Crammer and Singer, 2003). The first component is trained on the task-specific data while the second is trained on the former and on the external training data. When predicting a label for an instance the classifier combines the predictions of the two components. Cross-validation experiments on the training data show the advantages of the multicomponent architecture. In the following section we describe the features used by our system. In Section 3 we explain how we generated the additional training set. In Section 4 we describe the architecture of the classifier and in Section 5 we discuss </context>
<context position="11287" citStr="Collins, 2002" startWordPosition="1777" endWordPosition="1778">( *\x15\x0f C (2) We use the simplest case of uniform update weights, \x19 4 \x1b \x1c \x13 \x1b for \t 6;\x08 * . The perceptron algorithm defines a sequence of weight matrices \x01 1=&amp;lt;\x145 ,\x04\x03\x04\x03\x04\x03B, \x01 1 1 5 , where \x01 1 * 5 is the weight matrix after the first \x02 training items have been processed. In the standard perceptron, the weight matrix \x01 &amp;gt; \x01 1 1 5 is used to classify the unlabeled test examples. However, a variety of methods can be used for regularization or smoothing in order to reduce the effect of overtraining. Here we used the averaged perceptron (Collins, 2002), where the weight matrix used to classify the test data is the average of all of the matrices posited during training, i.e., \x01 &amp;gt; 4 1 &amp;gt; 1*3254 \x01 * . 4.2 Multicomponent architecture Task specific and external training data are integrated with a two-component perceptron. The disAlgorithm 3 Multicomponent Perceptron 1: input \&amp;apos;)( * ,\x18. * /\x0b1* 254 , \x01 &amp;gt;J{ ,\&amp;apos;)(@?#, | .A?R/\x0b~? 254 , B &amp;gt; { , 2: for C &amp;gt; y\t,\x04\x03\x05\x03\x05\x03 ,ED do 3: train M on \&amp;apos;)(@?#, | .A?R/\x0b~? 254 and \&amp;apos;)( * ,\x18. * /\x0b1*3254 4: train V on \&amp;apos;)( * ,\x18. * /\x0b1*3254 5: end for criminant function i</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative Online Algorithms for Multiclass Problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="2685" citStr="Crammer and Singer, 2003" startWordPosition="409" endWordPosition="412">1 51% of the noun synsets in Wordnet contain only 1 word. from the Wordnet glosses. This data is not limited to the specific synset that represents one of the senses of the word, but concerns also other synsets that are semantically similar, i.e., close in the hierarchy, to that synset. Then, we integrate the task-specific and the external training data with a multicomponent classifier that simplifies the system for hierarchical word sense disambiguation presented in (Ciaramita et al., 2003). The classifier consists of two components based on the averaged multiclass perceptron (Collins, 2002; Crammer and Singer, 2003). The first component is trained on the task-specific data while the second is trained on the former and on the external training data. When predicting a label for an instance the classifier combines the predictions of the two components. Cross-validation experiments on the training data show the advantages of the multicomponent architecture. In the following section we describe the features used by our system. In Section 3 we explain how we generated the additional training set. In Section 4 we describe the architecture of the classifier and in Section 5 we discuss the specifics of the final </context>
<context position="9038" citStr="Crammer and Singer, 2003" startWordPosition="1383" endWordPosition="1386">hm 1 presents a schematic description of the procedure. For each sense . of a noun, or verb, we produced a set ExF of &amp;lt;M&amp;gt;zy\x10{#{ similar neighbor synsets of . . We label this set with |. , thus for each set of labels 8;\&amp;apos; &amp; / we induce a set of pseudo-labels | 8:\&amp;apos; &amp; / .For each synset in E\\F we compiled a training instance from the Wordnet glosses. At the end of this process, for each noun or verb, there is an additional training set \&amp;apos;)( * ,}| . * /\x0b~ . 4 Classifier 4.1 Multiclass averaged perceptron Our base classifier is the multiclass averaged perceptron. The multiclass perceptron (Crammer and Singer, 2003) is an on-line learning algorithm which 4 This likely depends on the fact that the IDs encode the location in the hierarchy, even though we dont know how the IDs are generated. \x0cAlgorithm 2 Multiclass Perceptron 1: input training data \&amp;apos;)( * ,\x18. * /\x0b1* 254 , \x01 2: repeat 3: for \x02 &amp;gt; y\t,\x04\x03\x05\x03\x05\x03 ,\x07\x06 do 4: \x08 * &amp;gt;?@ \t\\6 8 u\x0c\x0b)t\x0e \x1f,\x18( *\x10\x0f\x12\x11 \x0b)t\tF\x14\x13\x18,\x18( *\x15\x0f C 5: if n \x08 * n \x11 { then 6: t F\x14\x13 L t F\x14\x13\x17\x16 (\x1d* 7: for \t 6\x18\x08 * do 8: t L t \x1a\x19 4 \x1b \x1c \x13 \x1b (D* 9: end for 1</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative Online Algorithms for Multiclass Problems. Journal of Machine Learning Research, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="866" citStr="Fellbaum, 1998" startWordPosition="127" endWordPosition="128">tem MC-WSD presented for the English Lexical Sample task. The system is based on a multicomponent architecture. It consists of one classifier with two components. One is trained on the data provided for the task. The second is trained on this data and, additionally, on an external training set extracted from the Wordnet glosses. The goal of the additional component is to lessen sparse data problems by exploiting the information encoded in the ontology. 1 Introduction One of the main difficulties in word sense classification tasks stems from the fact that word senses, such as Wordnets synsets (Fellbaum, 1998), define very specific classes1. As a consequence training instances are often too few in number to capture extremely fine-grained semantic distinctions. Word senses, however, are not just independent entities but are connected by several semantic relations; e.g., the is-a, which specifies a relation of inclusion among classes such as car is-a vehicle. Based on the is-a relation Wordnet defines large and complex hierarchies for nouns and verbs. These hierarchical structures encode potentially useful world-knowledge that can be exploited for word sense classification purposes, by providing mean</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L Yoong</author>
<author>T N Hwee</author>
</authors>
<title>An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="3447" citStr="Yoong and Hwee, 2002" startWordPosition="536" endWordPosition="539">predicting a label for an instance the classifier combines the predictions of the two components. Cross-validation experiments on the training data show the advantages of the multicomponent architecture. In the following section we describe the features used by our system. In Section 3 we explain how we generated the additional training set. In Section 4 we describe the architecture of the classifier and in Section 5 we discuss the specifics of the final system and some experimental results. 2 Features We used a set of features similar to that which was extensively described and evaluated in (Yoong and Hwee, 2002). The sentence with POS annotation A-DT newspaper-NN and-CC now-RB a-DT bank-NN have-AUX since-RB taken-VBN overRB serves as an example to illustrate them. The word to disambiguate is bank (or activate for (7)). 1. part of speech of neighboring words \x04\x06\x05 , \x07\t\x08\x0b \x0c\x0f\x0e\x11\x10\x12\x0c\x0f\x13\x11\x10\x12\x0c\x15\x14\x16\x10\x18\x17\x11\x10\x1a\x19\x1b\x14\x1c\x10\x1a\x19\x1d\x13\x11\x10\x1e\x19\x1d\x0e\x06\x1f ; e.g., \x04!#&amp;quot;%$\&amp;apos;&amp;)( , \x04)*+$\&amp;apos;,), , \x04.-/&amp;quot;0$21\x163\x064 , ... 2. words in the same sentence WS or passage WC; e.g., 576 $\&amp;apos;879\x16:.;\x1c&amp;lt; , 576 $&amp;gt;=\x1c:\</context>
</contexts>
<marker>Yoong, Hwee, 2002</marker>
<rawString>K.L Yoong and T.N. Hwee. 2002. An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>