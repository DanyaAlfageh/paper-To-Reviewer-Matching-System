<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.3893685">
b&amp;apos;Generative Models for Statistical Parsing with Combinatory Categorial
Grammar
</title>
<author confidence="0.839134">
Julia Hockenmaier and Mark Steedman
</author>
<affiliation confidence="0.9667475">
Division of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.779291">
Edinburgh EH8 9LW, United Kingdom
</address>
<email confidence="0.907345">
fjulia, steedmang@cogsci.ed.ac.uk
</email>
<sectionHeader confidence="0.989948" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.984886666666667">
This paper compares a number of gen-
erative probability models for a wide-
coverage Combinatory Categorial Gram-
mar (CCG) parser. These models are
trained and tested on a corpus obtained by
translating the Penn Treebank trees into
CCG normal-form derivations. According
to an evaluation of unlabeled word-word
dependencies, our best model achieves a
performance of 89.9%, comparable to the
figures given by Collins (1999) for a lin-
guistically less expressive grammar. In
contrast to Gildea (2001), we find a signif-
icant improvement from modeling word-
word dependencies.
</bodyText>
<sectionHeader confidence="0.998002" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996197303030303">
The currently best single-model statistical parser
(Charniak, 1999) achieves Parseval scores of over
89% on the Penn Treebank. However, the grammar
underlying the Penn Treebank is very permissive,
and a parser can do well on the standard Parseval
measures without committing itself on certain se-
mantically significant decisions, such as predicting
null elements arising from deletion or movement.
The potential benefit of wide-coverage parsing with
CCG lies in its more constrained grammar and its
simple and semantically transparent capture of ex-
traction and coordination.
We present a number of models over syntac-
tic derivations of Combinatory Categorial Grammar
(CCG, see Steedman (2000) and Clark et al. (2002),
this conference, for introduction), estimated from
and tested on a translation of the Penn Treebank
to a corpus of CCG normal-form derivations. CCG
grammars are characterized by much larger category
sets than standard Penn Treebank grammars, distin-
guishing for example between many classes of verbs
with different subcategorization frames. As a re-
sult, the categorial lexicon extracted for this purpose
from the training corpus has 1207 categories, com-
pared with the 48 POS-tags of the Penn Treebank.
On the other hand, grammar rules in CCG are lim-
ited to a small number of simple unary and binary
combinatory schemata such as function application
and composition. This results in a smaller and less
overgenerating grammar than standard PCFGs (ca.
3,000 rules when instantiated with the above cate-
gories in sections 02-21, instead of &amp;gt;12,400 in the
original Treebank representation (Collins, 1999)).
</bodyText>
<sectionHeader confidence="0.880219" genericHeader="method">
2 Evaluating a CCG parser
</sectionHeader>
<bodyText confidence="0.947787833333333">
Since CCG produces unary and binary branching
trees with a very fine-grained category set, CCG
Parseval scores cannot be compared with scores
of standard Treebank parsers. Therefore, we also
evaluate performance using a dependency evalua-
tion reported by Collins (1999), which counts word-
word dependencies as determined by local trees and
their labels. According to this metric, a local tree
with parent node P, head daughter H and non-head
daughter S (and position of S relative to P, ie. left
or right, which is implicit in CCG categories) de-
fines a hP; H; Si dependency between the head word
of S, wS, and the head word of H, wH. This measure
is neutral with respect to the branching factor. Fur-
thermore, as noted by Hockenmaier (2001), it does
not penalize equivalent analyses of multiple modi-
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 335-342.
Proceedings of the 40th Annual Meeting of the Association for
</bodyText>
<figure confidence="0.990861">
\x0cPierre Vinken ; 61 years old ; will join the board as a nonexecutive director Nov 29
N=N N ; N=N N (S[adj]nNP)nNP ; (S[dcl]nNP)=(S[b]nNP) ((S[b]nNP)=PP)=NP NP=N N PP=NP NP=N N=N N ((SnNP)n(SnNP))=N N
&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;
N N N N (SnNP)n(SnNP)
&amp;gt;
NP NP NP NP
&amp;lt; &amp;gt; &amp;gt;
NP S[adj]nNP (S[b]nNP)=PP PP
&amp;gt;
NPnNP S[b]nNP
&amp;lt; &amp;lt;
NP S[b]nNP
&amp;gt;
NP S[dcl]nNP
&amp;lt;
S[dcl]
</figure>
<figureCaption confidence="0.999888">
Figure 1: A CCG derivation in our corpus
</figureCaption>
<bodyText confidence="0.9962243">
fiers. In the unlabeled case hi (where it only matters
whether word a is a dependent of word b, not what
the label of the local tree is which defines this depen-
dency), scores can be compared across grammars
with different sets of labels and different kinds of
trees. In order to compare our performance with the
parser of Clark et al. (2002), we also evaluate our
best model according to the dependency evaluation
introduced for that parser. For further discussion we
refer the reader to Clark and Hockenmaier (2002) .
</bodyText>
<sectionHeader confidence="0.777046" genericHeader="method">
3 CCGbanka CCG treebank
</sectionHeader>
<bodyText confidence="0.9935502">
CCGbank is a corpus of CCG normal-form deriva-
tions obtained by translating the Penn Tree-
bank trees using an algorithm described by
Hockenmaier and Steedman (2002). Almost all
types of constructionwith the exception of gap-
ping and UCP (Unlike Coordinate Phrases) are
covered by the translation procedure, which pro-
cesses 98.3% of the sentences in the training corpus
(WSJ sections 02-21) and 98.5% of the sentences
in the test corpus (WSJ section 23). The grammar
contains a set of type-changing rules similar to the
lexical rules described in Carpenter (1992). Figure
1 shows a derivation taken from CCGbank. Cate-
gories, such as ((S[b]nNP)=PP)=NP, encode unsat-
urated subcat frames. The complement-adjunct dis-
tinction is made explicit; for instance as a nonexec-
utive director is marked up as PP-CLR in the Tree-
bank, and hence treated as a PP-complement of join,
whereas Nov. 29 is marked up as an NP-TMP and
therefore analyzed as VP modifier. The -CLR tag
is not in fact a very reliable indicator of whether a
constituent should be treated as a complement, but
the translation to CCG is automatic and must do the
best it can with the information in the Treebank.
The verbal categories in CCGbank carry fea-
tures distinguishing declarative verbs (and auxil-
iaries) from past participles in past tense, past par-
ticiples for passive, bare infinitives and ing-forms.
There is a separate level for nouns and noun phrases,
but, like the nonterminal NP in the Penn Treebank,
noun phrases do not carry any number agreement.
The derivations in CCGbank are normal-form in
the sense that analyses involving the combinatory
rules of type-raising and composition are only used
when syntactically necessary.
</bodyText>
<sectionHeader confidence="0.952451" genericHeader="method">
4 Generative models of CCG derivations
</sectionHeader>
<table confidence="0.903113">
Expansion HeadCat NonHeadCat
P(exp j : : : ) P(H j : : : ) P(S j : : : )
Baseline P P; exp P; exp; H
+ Conj P; con jP P; exp; con jP P; exp; H; con jP
+ Grandparent P; GP P; GP; exp P; GP; exp; H
+ P#L;R
</table>
<equation confidence="0.987992333333333">
P P; exp#L;R
P P; exp; H#L;R
P
</equation>
<tableCaption confidence="0.994149">
Table 1: The unlexicalized models
</tableCaption>
<bodyText confidence="0.997793357142857">
The models described here are all extensions of
a very simple model which models derivations by a
top-down tree-generating process. This model was
originally described in Hockenmaier (2001), where
it was applied to a preliminary version of CCGbank,
and its definition is repeated here in the top row of
Table 1. Given a (parent) node with category P,
choose the expansion exp of P, where exp can be
leaf (for lexical categories), unary (for unary ex-
pansions such as type-raising), left (for binary trees
where the head daughter is left) or right (binary
trees, head right). If P is a leaf node, generate its
head word w. Otherwise, generate the category of
its head daughter H. If P is binary branching, gen-
erate the category of its non-head daughter S (a
complement or modifier of H).
The model itself includes no prior knowledge spe-
cific to CCG other than that it only allows unary and
binary branching trees, and that the sets of nontermi-
nals and terminals are not disjoint (hence the need to
include leaf as a possible expansion, which acts as a
stop probability).
All the experiments reported in this section were
conducted using sections 02-21 of CCGbank as
training corpus, and section 23 as test corpus. We
\x0creplace all rare words in the training data with their
POS-tag. For all experiments reported here and in
section 5, the frequency threshold was set to 5. Like
Collins (1999), we assume that the test data is POS-
tagged, and can therefore replace unknown words in
the test data with their POS-tag, which is more ap-
propriate for a formalism like CCG with a large set
of lexical categories than one generic token for all
unknown words.
The performance of the baseline model is shown
in the top row of table 3. For six out of the 2379
sentences in our test corpus we do not get a parse.1
The reason is that a lexicon consisting of the word-
category pairs observed in the training corpus does
not contain all the entries required to parse the test
corpus. We discuss a simple, but imperfect, solution
to this problem in section 7.
</bodyText>
<sectionHeader confidence="0.709948" genericHeader="method">
5 Extending the baseline model
</sectionHeader>
<bodyText confidence="0.988676375">
State-of-the-art statistical parsers use many other
features, or conditioning variables, such as head
words, subcategorization frames, distance measures
and grandparent nodes. We too can extend the
baseline model described in the previous section
by including more features. Like the models of
Goodman (1997), the additional features in our
model are generated probabilistically, whereas in
the parser of Collins (1997) distance measures are
assumed to be a function of the already generated
structure and are not generated explicitly.
In order to estimate the conditional probabilities
of our model, we recursively smooth empirical es-
timates ei of specific conditional distributions with
(possible smoothed) estimates of less specific distri-
butions ei 1, using linear interpolation:
</bodyText>
<equation confidence="0.702692">
ei = ei + (1 )ei 1
</equation>
<bodyText confidence="0.924833428571429">
is a smoothing weight which depends on the par-
ticular distribution.2
When defining models, we will indicate a back-
off level with a # sign between conditioning vari-
ables, eg. A; B # C # D means that we interpolate
P(::: jA; B;C; D) with P(::: jA; B;C), which is an in-
terpolation of P(::: jA; B;C) and P(::: jA; B).
</bodyText>
<footnote confidence="0.99361">
1We conjecture that the minor variations in coverage among
the other models (except Grandparent) are artefacts of the beam.
2We compute in the same way as Collins (1999), p. 185.
</footnote>
<subsectionHeader confidence="0.928531">
5.1 Adding non-lexical information
</subsectionHeader>
<bodyText confidence="0.992389666666667">
The coordination feature We define a boolean
feature, conj, which is true for constituents which
expand to coordinations on the head path.
</bodyText>
<figure confidence="0.973808315789474">
S, +conj
S=NP, +conj
S=NP, conj
S=(SnNP)
NP
IBM
(SnNP)=NP
buys
S=NP[c], +conj
conj
but
S=NP[c], conj
S=(SnNP)
NP
Lotus
(SnNP)=NP
sells
NP
shares
</figure>
<bodyText confidence="0.987373">
This feature is generated at the root of the sentence
with P(conj j TOP). For binary expansions, conjH
is generated with P(conjH jH; S; conjP) and conjS is
generated with P(conjS j S # P; expP; H; conjP). Ta-
ble 1 shows how conj is used as a conditioning vari-
able. This is intended to allow the model to cap-
ture the fact that, for a sentence without extraction,
a CCG derivation where the subject is type-raised
and composed with the verb is much more likely in
right node raising constructions like the above.
The impact of the grandparent feature
Johnson (1998) showed that a PCFG estimated
from a version of the Penn Treebank in which
the label of a nodes parent is attached to the
nodes own label yields a substantial improvement
(LP/LR: from 73.5%/69.7% to 80.0%/79.2%).
The inclusion of an additional grandparent feature
gives Charniak (1999) a slight improvement in the
Maximum Entropy inspired model, but a slight
decrease in performance for an MLE model. Table
3 (Grandparent) shows that a grammar transfor-
mation like Johnsons does yield an improvement,
but not as dramatic as in the Treebank-CFG case.
At the same time coverage is reduced (which might
not be the case if this was an additional feature in
the model rather than a change in the representation
of the categories). Both of these results are to be
expectedCCG categories encode more contextual
information than Treebank labels, in particular
about parents and grandparents; therefore the his-
tory feature might be expected to have less impact.
Moreover, since our category set is much larger,
appending the parent node will lead to an even more
fine-grained partitioning of the data, which then
results in sparse data problems.
\x0cDistance measures for CCG Our distance mea-
sures are related to those proposed by Goodman
(1997), which are appropriate for binary trees (un-
like those of Collins (1997)). Every node has a left
distance measure, L, measuring the distance from
the head word to the left frontier of the constituent.
There is a similar right distance measure R. We
implemented three different ways of measuring dis-
tance: Adjacency measures string adjacency (0, 1 or
2 and more intervening words); Verb counts inter-
vening verbs (0 or 1 and more); and Pct counts in-
tervening punctuation marks (0, 1, 2 or 3 and more).
These s are generated by the model in the follow-
ing manner: at the root of the sentence, generate L
with P(L j TOP), and R with P(R j TOP; L).
Then, for each expansion, if it is a unary expan-
sion, L
</bodyText>
<equation confidence="0.96582925">
H = L
P and R
H = R
P with a probabil-
</equation>
<bodyText confidence="0.849393">
ity of 1. If it is a binary expansion, only the in
the direction of the sister changes, with a probability
</bodyText>
<figure confidence="0.8865498">
of P(L
H j L
PH#P; S) if exp = right, and analo-
gously for exp=left. L
S and R
S are conditioned
on S and the of H and P in the direction of S:
P(L
S j S#R
P; R
H) and P(R
S jS; L
S#R
P; R
H).
</figure>
<bodyText confidence="0.988708714285714">
They are then used as further conditioning variables
for the other distributions as shown in table 1.
Table 3 also gives the Parseval and dependency
scores obtained with each of these measures. Pct
has the smallest effect. However, our model does
not yet contain anything like the hard constraint on
punctuation marks in Collins (1999).
</bodyText>
<subsectionHeader confidence="0.994101">
5.2 Adding lexical information
</subsectionHeader>
<bodyText confidence="0.99919258">
Gildea (2001) shows that removing the lexical de-
pendencies in Model 1 of Collins (1997) (that is,
not conditioning on wh when generating ws) de-
creases labeled precision and recall by only 0.5%.
It can therefore be assumed that the main influence
of lexical head features (words and preterminals) in
Collins Model 1 is on the structural probabilities.
In CCG, by contrast, preterminals are lexical cat-
egories, encoding complete subcategorization infor-
mation. They therefore encode more information
about the expansion of a nonterminal than Treebank
POS-tags and thus are more constraining.
Generating a constituents lexical category c at its
maximal projection (ie. either at the root of the tree,
TOP, or when generating a non-head daughter S),
and using the lexical category as conditioning vari-
able (LexCat) increases performance of the baseline
model as measured by hP; H; Si by almost 3%. In
this model, cS, the lexical category of S depends on
the category S and on the local tree in which S is
generated. However, slightly worse performance is
obtained for LexCatDep, a model which is identical
to the original LexCat model, except that cS is also
conditioned on cH, the lexical category of the head
node, which introduces a dependency between the
lexical categories.
Since there is so much information in the lexical
categories, one might expect that this would reduce
the effect of conditioning the expansion of a con-
stituent on its head word w. However, we did find a
substantial effect. Generating the head word at the
maximal projection (HeadWord) increases perfor-
mance by a further 2%. Finally, conditioning wS
on wH, hence including word-word dependencies,
(HWDep) increases performance even more, by an-
other 3.5%, or 8.3% overall. This is in stark contrast
to Gildeas findings for Collins Model 1.
We conjecture that the reason why CCG benefits
more from word-word dependencies than Collins
Model 1 is that CCG allows a cleaner parametriza-
tion of these surface dependencies. In Collins
Model 1, wS is conditioned not only on the local
tree hP; H; Si, cH and wH, but also on the distance
between the head and the modifier to be generated.
However, Model 1 does not incorporate the notion
of subcategorization frames. Instead, the distance
measure was found to yield a good, if imperfect, ap-
proximation to subcategorization information.
Using our notation, Collins Model 1 generates wS
with the following probability:
</bodyText>
<equation confidence="0.9513679">
PCollins1(wS jcS; ; P; H; S; cH; wH) =
1P(wS jcS; ; P; H; S; cH; wH)
+(1 1)
\x02
2P(wS jcS; ; P; H; S; cH) + (1 2)P(wS jcS)
\x03
whereas the CCG dependency model generates
wS as follows:
PCCGdep(wS jcS; P; H; S; cH; wH) =
P(wS jcS; P; H; S; cH; wH) + (1 )P(wS jcS)
</equation>
<bodyText confidence="0.9517744">
Since our P, H, S and cH are CCG categories, and
hence encode subcategorization information, the lo-
cal tree always identifies a specific argument slot.
Therefore it is not necessary for us to include a dis-
tance measure in the dependency probabilities.
</bodyText>
<equation confidence="0.972175230769231">
\x0cExpansion HeadCat NonHeadCat LexCat Head word
P(exp j:::) P(H j:::) P(S j:::) P(cS j:::) P(cTOPj:::
) P(wS j:::) P(wTOP j:::
)
LexCat P; cP P; exp; cP P; exp; H#cP S#H; exp; P P=TOP
LexCatDep P; cP P; exp; cP P; exp; H#cP S#H; exp; P#cP P=TOP
HeadWord P; cP#wP P; exp; cP#wP P; exp; H#cP#wP S#H; exp; P P=TOP cS cP
HWDep P; cP#wP P; exp; cP#wP P; exp; H#cP#wP S#H; exp; P P=TOP cS#P; H; S; wP cP
HWDep P; cP#L;R
P#wP P; exp; cP#L;R
P#wP P; exp; H#L;R
P#cP#wP S#H; exp; P P=TOP cS#P; H; S; wP cP
HWDepConj P; cP; conjP#wP P; exp; cP; conjP#wP P; exp; H; conjP#cP#wP S#H; exp; P P=TOP cS#P; H; S; wP cP
</equation>
<tableCaption confidence="0.996867">
Table 2: The lexicalized models
</tableCaption>
<table confidence="0.995719">
Model NoParse LexCat LP LR BP BR hP; H; Si hSi hi CM on hi \x142 CD
Baseline 6 87.7 72.8 72.4 78,3 77.9 75.7 81.1 84.3 23.0 51.1
Conj 9 87.8 73.8 73.9 79.3 79.3 76.7 82.0 85.1 24.3 53.2
Grandparent 91 88.8 77.1 77.6 82.4 82.9 79.9 84.7 87.9 30.9 63.8
Pct 6 88.1 73.7 73.1 79.2 78.6 76.5 81.8 84.9 23.1 53.2
Verb 6 88.0 75.9 75.5 81.6 81.1 76.9 82.3 85.3 25.2 55.1
Adjacency 6 88.6 77.5 77.3 82.9 82.8 78.9 83.8 86.9 24.8 59.6
LexCat 9 88.5 75.8 76.0 81.3 81.5 78.6 83.7 86.8 27.4 57.8
LexCatDep 9 88.5 75.7 75.9 81.2 81.4 78.4 83.5 86.6 26.3 57.9
HeadWord 8 89.6 77.9 78.0 83.0 83.1 80.5 85.2 88.3 30.4 63.0
HWDep 8 92.0 81.6 81.9 85.5 85.9 84.0 87.8 90.1 37.9 69.2
HWDep 8 90.9 81.4 81.6 86.1 86.3 83.0 87.0 89.8 35.7 68.7
HWDepConj 9 91.8 80.7 81.2 84.8 85.3 83.6 87.5 89.9 36.5 68.6
HWDep (+ tagger) 7 91.7 81.4 81.8 85.6 85.9 83.6 87.5 89.9 38.1 69.1
</table>
<tableCaption confidence="0.999021">
Table 3: Performance of the models: LexCat indicates accuracy of the lexical categories; LP, LR, BP and
</tableCaption>
<bodyText confidence="0.958187315789474">
BR (the standard Parseval scores labeled/bracketed precision and recall) are not commensurate with other
Treebank parsers. hP; H; Si, hSi, and hi are as defined in section 2. CM on hi is the percentage of sentences
with complete match on hi, and \x142 CD is the percentage of sentences with under 2 crossing dependencies
as defined by hi.
The hP; H; Si labeled dependencies we report are
not directly comparable with Collins (1999), since
CCG categories encode subcategorization frames.
For instance, if the direct object of a verb has been
recognized as such, but a PP has been mistaken as
a complement (whereas the gold standard says it
is an adjunct), the fully labeled dependency eval-
uation hP; H; Si will not award a point. Therefore,
we also include in Table 3 a more comparable eval-
uation hSi which only takes the correctness of the
non-head category into account. The reported fig-
ures are also deflated by retaining verb features like
tensed/untensed. If this is done (by stripping off
all verb features), an improvement of 0.6% on the
hP; H; Si score for our best model is obtained.
</bodyText>
<subsectionHeader confidence="0.996623">
5.3 Combining lexical and non-lexical
</subsectionHeader>
<bodyText confidence="0.985841105263158">
information
When incorporating the adjacency distance mea-
sure or the coordination feature into the dependency
model (HWDep and HWDepConj), overall per-
formance is lower than with the dependency model
alone. We conjecture that this arises from data
sparseness. It cannot be concluded from these re-
sults alone that the lexical dependencies make struc-
tural information redundant or superfluous. Instead,
it is quite likely that we are facing an estimation
problem similar to Charniak (1999), who reports
that the inclusion of the grandparent feature worsens
performance of an MLE model, but improves per-
formance if the individual distributions are modelled
using Maximum Entropy. This intuition is strength-
ened by the fact that, on casual inspection of the
scores for individual sentences, it is sometimes the
case that the lexicalized models perform worse than
the unlexicalized models.
</bodyText>
<subsectionHeader confidence="0.934628">
5.4 The impact of tagging errors
</subsectionHeader>
<bodyText confidence="0.999221142857143">
All of the experiments described above use the POS-
tags as given by CCGbank (which are the Treebank
tags, with some corrections necessary to acquire cor-
rect features on categories). It is reasonable to as-
sume that this input is of higher quality than can
be produced by a POS-tagger. We therefore ran the
\x0cdependency model on a test corpus tagged with the
POS-tagger of Ratnaparkhi (1996), which is trained
on the original Penn Treebank (see HWDep (+ tag-
ger) in Table 3). Performance degrades slightly,
which is to be expected, since our approach makes
so much use of the POS-tag information for un-
known words. However, a POS-tagger trained on
CCGbank might yield slightly better results.
</bodyText>
<subsectionHeader confidence="0.890827">
5.5 Limitations of the current model
</subsectionHeader>
<bodyText confidence="0.997755">
Unlike Clark et al. (2002), our parser does not al-
ways model the dependencies in the logical form.
For example, in the interpretation of a coordinate
structure like buy and sell shares, shares will head
an object of both buy and sell. Similarly, in examples
like buy the company that wins, the relative con-
struction makes company depend upon both buy as
object and wins as subject. As is well known (Ab-
ney, 1997), DAG-like dependencies cannot in gen-
eral be modeled with a generative approach of the
kind taken here3.
</bodyText>
<subsectionHeader confidence="0.995218">
5.6 Comparison with Clark et al. (2002)
</subsectionHeader>
<bodyText confidence="0.990158">
Clark et al. (2002) presents another statistical CCG
parser, which is based on a conditional (rather
than generative) model of the derived depen-
dency structure, including non-surface dependen-
cies. The following table compares the two parsers
according to the evaluation of surface and deep
dependencies given in Clark et al. (2002). We
use Clark et al.s parser to generate these de-
pendencies from the output of our parser (see
</bodyText>
<table confidence="0.9175998">
Clark and Hockenmaier (2002)) 4.
LP LR UP UR
Clark 81.9% 81.8% 89.1% 90.1%
Hockenmaier 83.7% 84.2% 90.5% 91.1%
6 Performance on specific constructions
</table>
<bodyText confidence="0.960674">
One of the advantages of CCG is that it provides a
simple, surface grammatical analysis of extraction
and coordination. We investigate whether our best
</bodyText>
<footnote confidence="0.944262666666667">
3It remains to be seen whether the more restricted reentran-
cies of CCG will ultimately support a generative model.
4Due to the smaller grammar and lexicon of Clark et al., our
</footnote>
<bodyText confidence="0.99736508">
parser can only be evaluated on slightly over 94% of the sen-
tences in section 23, whereas the figures for Clark et al. (2002)
are on 97%.
model, HWDep, predicts the correct analyses, using
the development section 00.
Coordination There are two instances of argu-
ment cluster coordination (constructions like cost
$5,000 in July and $6,000 in August) in the devel-
opment corpus. Of these, HWDep recovers none
correctly. This is a shortcoming in the model, rather
than in CCG: the relatively high probability both of
the NP modifier analysis of PPs like in July and of
NP coordination is enough to misdirect the parser.
There are 203 instances of verb phrase coordina-
tion (S[:]nNP, with [:] any verbal feature) in the de-
velopment corpus. On these, we obtain a labeled re-
call and precision of 67.0%/67.3%. Interestingly, on
the 24 instances of right node raising (coordination
of (S[:]nNP)=NP), our parser achieves higher per-
formance, with labeled recall and precision of 79.2%
and 73.1%. Figure 2 gives an example of the output
of our parser on such a sentence.
Extraction Long-range dependencies are not cap-
tured by the evaluation used here. However, the ac-
curacy for recovering lexical categories for words
with extraction categories, such as relative pro-
nouns, gives some indication of how well the model
detects the presence of such dependencies.
The most common category for subject relative
pronouns, (NPnNP)=(S[dcl]nNP), has been recov-
ered with precision and recall of 97.1% (232 out of
239) and 94.3% (232/246).
Embedded subject extraction requires the special
lexical category ((S[dcl]nNP)=NP)=(S[dcl]nNP)
for verbs like think. On this category, the model
achieves a precision of 100% (5/5) and recall of
83.3% (5/6). The case the parser misanalyzed is due
to lexical coverage: the verb agree occurs in our lex-
icon, but not with this category.
The most common category for object relative
pronouns, (NPnNP)=(S[dcl]=NP), has a recall of
76.2% (16 out of 21) and precision of 84.2% (16/19).
Free object relatives, NP=(S[dcl]=NP), have a
recall of 84.6% (11/13), and precision of 91.7%
(11/12). However, object extraction appears more
frequently as a reduced relative (the man John saw),
and there are no lexical categories indicating this ex-
traction. Reduced relative clauses are captured by a
type-changing rule NPnNP !S[dcl]=NP. This rule
was applied 56 times in the gold standard, and 70
</bodyText>
<figure confidence="0.998900030303031">
\x0cS[dcl]
NP
the suit
S[dcl]nNP
S[dcl]nNP
(S[dcl]nNP)=NP
seeks
NP
a court order
(SnNP)n(SnNP)
S[ng]nNP
(S[ng]nNP)=PP
((S[ng]nNP)=PP)=NP
preventing
NP
the guild
PP
PP=(S[ng]nNP)
from
S[ng]nNP
(S[ng]nNP)=NP
(S[ng]nNP)=NP
punishing
(S[ng]nNP)=NP[c]
conj
or
(S[ng]nNP)=NP
(S[ng]nNP)=PP
retaliating
PP=NP
against
NP
Mr: Trudeau
</figure>
<figureCaption confidence="0.999624">
Figure 2: Right node raising output produced by our parser. Punishing and retaliating are unknown words.
</figureCaption>
<bodyText confidence="0.993376037037037">
times by the parser, out of which 48 times it corre-
sponded to a rule in the gold standard (or 34 times,
if the exact bracketing of the S[dcl]=NP is taken into
accountthis lower figure is due to attachment de-
cisions made elsewhere in the tree).
These figures are difficult to compare with stan-
dard Treebank parsers. Despite the fact that the
original Treebank does contain traces for move-
ment, none of the existing parsers try to gener-
ate these traces (with the exception of Collins
Model 3, for which he only gives an overall score
of 96.3%/98.8% P/R for subject extraction and
81.4%/59.4% P/R for other cases). The only long
range dependency for which Collins gives numbers
is subject extraction hSBAR, WHNP, SG, Ri, which
has labeled precision and recall of 90.56% and
90.56%, whereas the CCG model achieves a labeled
precision and recall of 94.3% and 96.5% on the most
frequency subject extraction dependency hNPnNP,
(NPnNP)=(S[dcl]nNP), S[dcl]nNPi, which occurs
262 times in the gold standard and was produced
256 times by our parser. However, out of the
15 cases of this relation in the gold standard that
our parser did not return, 8 were in fact analyzed
as subject extraction of bare infinitivals hNPnNP,
(NPnNP)=(S[b]nNP), S[b]nNPi, yielding a com-
bined recall of 97.3%.
</bodyText>
<sectionHeader confidence="0.941309" genericHeader="method">
7 Lexical coverage
</sectionHeader>
<bodyText confidence="0.999876647058824">
The most serious problem facing parsers like the
present one with large category sets is not so much
the standard problem of unseen words, but rather the
problem of words that have been seen, but not with
the necessary category.
For standard Treebank parsers, the latter problem
does not have much impact, if any, since the Penn
Treebank tagset is fairly small, and the grammar un-
derlying the Treebank is very permissive. However,
for CCG this is a serious problem: the first three
rows in table 4 show a significant difference in per-
formance for sentences with complete lexical cover-
age (No missing) and sentences with missing lex-
ical entries (Missing).
Using the POS-tags in the corpus, we can estimate
the lexical probabilities P(w j c) using a linear in-
terpolation between the relative frequency estimates
</bodyText>
<equation confidence="0.7761885">
P(w jc) and the following approximation:5
Ptags(w jc) = t2tagsP(w jt)P(t jc)
</equation>
<bodyText confidence="0.946268">
We smooth the lexical probabilities as follows:
</bodyText>
<equation confidence="0.7518">
P(w jc) = P(w jc) + (1 )Ptags(w jc)
</equation>
<bodyText confidence="0.996490307692308">
Table 4 shows the performance of the baseline
model with a frequency cutoff of 5 and 10 for rare
words and with a smoothed and non-smoothed lexi-
con.6 This frequency cutoff plays an important role
here - smoothing with a small cutoff yields worse
performance than not smoothing, whereas smooth-
ing with a cutoff of 10 does not have a significant
impact on performance. Smoothing the lexicon in
this way does make the parser more robust, result-
ing in complete coverage of the test set. However, it
does not affect overall performance, nor does it alle-
viate the problem for sentences with missing lexical
entries for seen words.
</bodyText>
<footnote confidence="0.969516">
5We compute in the same way as Collins (1999), p. 185.
6Smoothing was only done for categories with a total fre-
</footnote>
<reference confidence="0.84170675">
quency of 100 or more.
\x0cBaseline, Cutoff = 5 Baseline, Cutoff = 10 HWDep, Cutoff = 10
(Missing = 463 sentences) (Missing = 387 sentences) (Missing = 387 sentences)
Non-smoothed Smoothed Non-smoothed Smoothed Smoothed
Parse failures 6 5
hP; H; Si, All 75.7 73.2 76.2 76.3 83.9
hP; H; Si, Missing 66.4 64.2 67.0 67.1 75.1
hP; H; Si, No missing 78.5 75.9 78.5 78.6 86.6
</reference>
<tableCaption confidence="0.972747">
Table 4: The impact of lexical coverage, using a different cutoff for rare words and smoothing (section 23)
</tableCaption>
<sectionHeader confidence="0.959407" genericHeader="conclusions">
8 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999360419354839">
We have compared a number of generative probabil-
ity models of CCG derivations, and shown that our
best model recovers 89.9% of word-word dependen-
cies on section 23 of CCGbank. On section 00, it
recovers 89.7% of word-word dependencies. These
figures are surprisingly close to the figure of 90.9%
reported by Collins (1999) on section 00, given that,
in order to allow a direct comparison, we have used
the same interpolation technique and beam strategy
as Collins (1999), which are very unlikely to be as
well-tuned to our kind of grammar.
As is to be expected, a statistical model of a CCG
extracted from the Treebank is less robust than a
model with an overly permissive grammar such as
Collins (1999). This problem seems to stem mainly
from the incomplete coverage of the lexicon. We
have shown that smoothing can compensate for en-
tirely unknown words. However, this approach does
not help on sentences which require previously un-
seen entries for known words. We would expect a
less naive approach such as applying morphologi-
cal rules to the observed entries, together with better
smoothing techniques, to yield better results.
We have also shown that a statistical model of
CCG benefits from word-word dependencies to a
much greater extent than a less linguistically moti-
vated model such as Collins Model 1. This indi-
cates to us that, although the task faced by a CCG
parser might seem harder prima facie, there are
advantages to using a more linguistically adequate
grammar.
</bodyText>
<sectionHeader confidence="0.963091" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9973005">
Thanks to Stephen Clark, Miles Osborne and the
ACL-02 referees for comments. Various parts of the
research were funded by EPSRC grants GR/M96889
and GR/R02450 and an EPSRC studentship.
</bodyText>
<sectionHeader confidence="0.991709" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997052973684211">
Steven Abney. 1997. Stochastic Attribute-Value Grammars.
Computational Linguistics, 23(4).
Bob Carpenter. 1992. Categorial Grammars, Lexical Rules,
and the English Predicative. In R. Levine, ed., Formal
Grammar: Theory and Implementation. OUP.
Eugene Charniak. 1999. A Maximum-Entropy-Inspired Parser.
TR CS-99-12, Brown University.
David Chiang. 2000. Statistical Parsing with an Automatically-
Extracted Tree Adjoining Grammar 38th ACL, Hong Kong,
pp. 456-463.
Stephen Clark and Julia Hockenmaier. 2002. Evaluating a
Wide-Coverage CCG Parser. LREC Beyond PARSEVAL
workshop, Las Palmas, Spain.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building Deep Dependency Structures Using a Wide-
Coverage CCG Parser. 40th ACL, Philadelphia.
Michael Collins. 1997. Three Generative Lexicalized Models
for Statistical Parsing. 35th ACL, Madrid, pp. 1623.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Daniel Gildea. 2001. Corpus Variation and Parser Perfor-
mance. EMNLP, Pittsburgh, PA.
Julia Hockenmaier. 2001. Statistical Parsing for CCG with
Simple Generative Models. Student Workshop, 39th ACL/
10th EACL, Toulouse, France, pp. 712.
Julia Hockenmaier and Mark Steedman 2002. Acquiring Com-
pact Lexicalized Grammars from a Cleaner Treebank. Third
LREC, Las Palmas, Spain.
Joshua Goodman. 1997. Probabilistic Feature Grammars.
IWPT, Boston.
Mark Johnson. 1998. PCFG Models of Linguistic Tree Repre-
sentations. Computational Linguistics, 24(4).
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-Of-
Speech Tagger. EMNLP, Philadelphia, pp. 133142.
Mark Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge Mass.
\x0c&amp;apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.883982">
<title confidence="0.96492">b&amp;apos;Generative Models for Statistical Parsing with Combinatory Categorial Grammar</title>
<author confidence="0.999967">Julia Hockenmaier</author>
<author confidence="0.999967">Mark Steedman</author>
<affiliation confidence="0.9998535">Division of Informatics University of Edinburgh</affiliation>
<address confidence="0.997341">Edinburgh EH8 9LW, United Kingdom</address>
<email confidence="0.997641">fjulia,steedmang@cogsci.ed.ac.uk</email>
<abstract confidence="0.997119875">This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Grammar (CCG) parser. These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations. According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar. In contrast to Gildea (2001), we find a significant improvement from modeling wordword dependencies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>H hP</author>
<author>Si</author>
</authors>
<title>quency of 100 or more.</title>
<date>1997</date>
<journal>hP; H; Si, All</journal>
<booktitle>x0cBaseline, Cutoff = 5 Baseline, Cutoff = 10 HWDep, Cutoff = 10 (Missing = 463 sentences) (Missing = 387 sentences) (Missing = 387 sentences) Non-smoothed Smoothed Non-smoothed Smoothed Smoothed Parse failures</booktitle>
<volume>6</volume>
<marker>hP, Si, 1997</marker>
<rawString>quency of 100 or more. \x0cBaseline, Cutoff = 5 Baseline, Cutoff = 10 HWDep, Cutoff = 10 (Missing = 463 sentences) (Missing = 387 sentences) (Missing = 387 sentences) Non-smoothed Smoothed Non-smoothed Smoothed Smoothed Parse failures 6 5 hP; H; Si, All 75.7 73.2 76.2 76.3 83.9 hP; H; Si, Missing 66.4 64.2 67.0 67.1 75.1 hP; H; Si, No missing 78.5 75.9 78.5 78.6 86.6 Steven Abney. 1997. Stochastic Attribute-Value Grammars. Computational Linguistics, 23(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>Categorial Grammars, Lexical Rules, and the English Predicative. In</title>
<date>1992</date>
<booktitle>Formal Grammar: Theory and Implementation. OUP.</booktitle>
<editor>R. Levine, ed.,</editor>
<contexts>
<context position="4875" citStr="Carpenter (1992)" startWordPosition="781" endWordPosition="782">o Clark and Hockenmaier (2002) . 3 CCGbanka CCG treebank CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Treebank trees using an algorithm described by Hockenmaier and Steedman (2002). Almost all types of constructionwith the exception of gapping and UCP (Unlike Coordinate Phrases) are covered by the translation procedure, which processes 98.3% of the sentences in the training corpus (WSJ sections 02-21) and 98.5% of the sentences in the test corpus (WSJ section 23). The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter (1992). Figure 1 shows a derivation taken from CCGbank. Categories, such as ((S[b]nNP)=PP)=NP, encode unsaturated subcat frames. The complement-adjunct distinction is made explicit; for instance as a nonexecutive director is marked up as PP-CLR in the Treebank, and hence treated as a PP-complement of join, whereas Nov. 29 is marked up as an NP-TMP and therefore analyzed as VP modifier. The -CLR tag is not in fact a very reliable indicator of whether a constituent should be treated as a complement, but the translation to CCG is automatic and must do the best it can with the information in the Treeban</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Bob Carpenter. 1992. Categorial Grammars, Lexical Rules, and the English Predicative. In R. Levine, ed., Formal Grammar: Theory and Implementation. OUP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A Maximum-Entropy-Inspired Parser.</title>
<date>1999</date>
<tech>TR CS-99-12,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="888" citStr="Charniak, 1999" startWordPosition="123" endWordPosition="124">generative probability models for a widecoverage Combinatory Categorial Grammar (CCG) parser. These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations. According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar. In contrast to Gildea (2001), we find a significant improvement from modeling wordword dependencies. 1 Introduction The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank. However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parseval measures without committing itself on certain semantically significant decisions, such as predicting null elements arising from deletion or movement. The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and its simple and semantically transparent capture of extraction and coordination. We present a number of models over syntactic derivations of Combinatory Categorial Grammar (C</context>
<context position="10850" citStr="Charniak (1999)" startWordPosition="1815" endWordPosition="1816">sed as a conditioning variable. This is intended to allow the model to capture the fact that, for a sentence without extraction, a CCG derivation where the subject is type-raised and composed with the verb is much more likely in right node raising constructions like the above. The impact of the grandparent feature Johnson (1998) showed that a PCFG estimated from a version of the Penn Treebank in which the label of a nodes parent is attached to the nodes own label yields a substantial improvement (LP/LR: from 73.5%/69.7% to 80.0%/79.2%). The inclusion of an additional grandparent feature gives Charniak (1999) a slight improvement in the Maximum Entropy inspired model, but a slight decrease in performance for an MLE model. Table 3 (Grandparent) shows that a grammar transformation like Johnsons does yield an improvement, but not as dramatic as in the Treebank-CFG case. At the same time coverage is reduced (which might not be the case if this was an additional feature in the model rather than a change in the representation of the categories). Both of these results are to be expectedCCG categories encode more contextual information than Treebank labels, in particular about parents and grandparents; th</context>
<context position="19362" citStr="Charniak (1999)" startWordPosition="3329" endWordPosition="3330">b features), an improvement of 0.6% on the hP; H; Si score for our best model is obtained. 5.3 Combining lexical and non-lexical information When incorporating the adjacency distance measure or the coordination feature into the dependency model (HWDep and HWDepConj), overall performance is lower than with the dependency model alone. We conjecture that this arises from data sparseness. It cannot be concluded from these results alone that the lexical dependencies make structural information redundant or superfluous. Instead, it is quite likely that we are facing an estimation problem similar to Charniak (1999), who reports that the inclusion of the grandparent feature worsens performance of an MLE model, but improves performance if the individual distributions are modelled using Maximum Entropy. This intuition is strengthened by the fact that, on casual inspection of the scores for individual sentences, it is sometimes the case that the lexicalized models perform worse than the unlexicalized models. 5.4 The impact of tagging errors All of the experiments described above use the POStags as given by CCGbank (which are the Treebank tags, with some corrections necessary to acquire correct features on c</context>
</contexts>
<marker>Charniak, 1999</marker>
<rawString>Eugene Charniak. 1999. A Maximum-Entropy-Inspired Parser. TR CS-99-12, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical Parsing with an AutomaticallyExtracted Tree Adjoining Grammar 38th ACL, Hong Kong,</title>
<date>2000</date>
<pages>456--463</pages>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical Parsing with an AutomaticallyExtracted Tree Adjoining Grammar 38th ACL, Hong Kong, pp. 456-463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Evaluating a Wide-Coverage CCG Parser.</title>
<date>2002</date>
<booktitle>LREC Beyond PARSEVAL workshop,</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="4289" citStr="Clark and Hockenmaier (2002)" startWordPosition="684" endWordPosition="687">(S[b]nNP)=PP PP &amp;gt; NPnNP S[b]nNP &amp;lt; &amp;lt; NP S[b]nNP &amp;gt; NP S[dcl]nNP &amp;lt; S[dcl] Figure 1: A CCG derivation in our corpus fiers. In the unlabeled case hi (where it only matters whether word a is a dependent of word b, not what the label of the local tree is which defines this dependency), scores can be compared across grammars with different sets of labels and different kinds of trees. In order to compare our performance with the parser of Clark et al. (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser. For further discussion we refer the reader to Clark and Hockenmaier (2002) . 3 CCGbanka CCG treebank CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Treebank trees using an algorithm described by Hockenmaier and Steedman (2002). Almost all types of constructionwith the exception of gapping and UCP (Unlike Coordinate Phrases) are covered by the translation procedure, which processes 98.3% of the sentences in the training corpus (WSJ sections 02-21) and 98.5% of the sentences in the test corpus (WSJ section 23). The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter (1992). Figure 1 sho</context>
<context position="21533" citStr="Clark and Hockenmaier (2002)" startWordPosition="3688" endWordPosition="3691"> is well known (Abney, 1997), DAG-like dependencies cannot in general be modeled with a generative approach of the kind taken here3. 5.6 Comparison with Clark et al. (2002) Clark et al. (2002) presents another statistical CCG parser, which is based on a conditional (rather than generative) model of the derived dependency structure, including non-surface dependencies. The following table compares the two parsers according to the evaluation of surface and deep dependencies given in Clark et al. (2002). We use Clark et al.s parser to generate these dependencies from the output of our parser (see Clark and Hockenmaier (2002)) 4. LP LR UP UR Clark 81.9% 81.8% 89.1% 90.1% Hockenmaier 83.7% 84.2% 90.5% 91.1% 6 Performance on specific constructions One of the advantages of CCG is that it provides a simple, surface grammatical analysis of extraction and coordination. We investigate whether our best 3It remains to be seen whether the more restricted reentrancies of CCG will ultimately support a generative model. 4Due to the smaller grammar and lexicon of Clark et al., our parser can only be evaluated on slightly over 94% of the sentences in section 23, whereas the figures for Clark et al. (2002) are on 97%. model, HWDe</context>
</contexts>
<marker>Clark, Hockenmaier, 2002</marker>
<rawString>Stephen Clark and Julia Hockenmaier. 2002. Evaluating a Wide-Coverage CCG Parser. LREC Beyond PARSEVAL workshop, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Building Deep Dependency Structures Using a WideCoverage CCG Parser. 40th ACL,</title>
<date>2002</date>
<location>Philadelphia.</location>
<contexts>
<context position="1535" citStr="Clark et al. (2002)" startWordPosition="220" endWordPosition="223"> over 89% on the Penn Treebank. However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parseval measures without committing itself on certain semantically significant decisions, such as predicting null elements arising from deletion or movement. The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and its simple and semantically transparent capture of extraction and coordination. We present a number of models over syntactic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al. (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations. CCG grammars are characterized by much larger category sets than standard Penn Treebank grammars, distinguishing for example between many classes of verbs with different subcategorization frames. As a result, the categorial lexicon extracted for this purpose from the training corpus has 1207 categories, compared with the 48 POS-tags of the Penn Treebank. On the other hand, grammar rules in CCG are limited to a small number of simple unary and binary </context>
<context position="4114" citStr="Clark et al. (2002)" startWordPosition="657" endWordPosition="660">(S[adj]nNP)nNP ; (S[dcl]nNP)=(S[b]nNP) ((S[b]nNP)=PP)=NP NP=N N PP=NP NP=N N=N N ((SnNP)n(SnNP))=N N &amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt; N N N N (SnNP)n(SnNP) &amp;gt; NP NP NP NP &amp;lt; &amp;gt; &amp;gt; NP S[adj]nNP (S[b]nNP)=PP PP &amp;gt; NPnNP S[b]nNP &amp;lt; &amp;lt; NP S[b]nNP &amp;gt; NP S[dcl]nNP &amp;lt; S[dcl] Figure 1: A CCG derivation in our corpus fiers. In the unlabeled case hi (where it only matters whether word a is a dependent of word b, not what the label of the local tree is which defines this dependency), scores can be compared across grammars with different sets of labels and different kinds of trees. In order to compare our performance with the parser of Clark et al. (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser. For further discussion we refer the reader to Clark and Hockenmaier (2002) . 3 CCGbanka CCG treebank CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Treebank trees using an algorithm described by Hockenmaier and Steedman (2002). Almost all types of constructionwith the exception of gapping and UCP (Unlike Coordinate Phrases) are covered by the translation procedure, which processes 98.3% of the sentences in the training corpus (WSJ sections 02-21) and 98.5% o</context>
<context position="20547" citStr="Clark et al. (2002)" startWordPosition="3522" endWordPosition="3525">y to acquire correct features on categories). It is reasonable to assume that this input is of higher quality than can be produced by a POS-tagger. We therefore ran the \x0cdependency model on a test corpus tagged with the POS-tagger of Ratnaparkhi (1996), which is trained on the original Penn Treebank (see HWDep (+ tagger) in Table 3). Performance degrades slightly, which is to be expected, since our approach makes so much use of the POS-tag information for unknown words. However, a POS-tagger trained on CCGbank might yield slightly better results. 5.5 Limitations of the current model Unlike Clark et al. (2002), our parser does not always model the dependencies in the logical form. For example, in the interpretation of a coordinate structure like buy and sell shares, shares will head an object of both buy and sell. Similarly, in examples like buy the company that wins, the relative construction makes company depend upon both buy as object and wins as subject. As is well known (Abney, 1997), DAG-like dependencies cannot in general be modeled with a generative approach of the kind taken here3. 5.6 Comparison with Clark et al. (2002) Clark et al. (2002) presents another statistical CCG parser, which is</context>
<context position="22109" citStr="Clark et al. (2002)" startWordPosition="3787" endWordPosition="3790">ur parser (see Clark and Hockenmaier (2002)) 4. LP LR UP UR Clark 81.9% 81.8% 89.1% 90.1% Hockenmaier 83.7% 84.2% 90.5% 91.1% 6 Performance on specific constructions One of the advantages of CCG is that it provides a simple, surface grammatical analysis of extraction and coordination. We investigate whether our best 3It remains to be seen whether the more restricted reentrancies of CCG will ultimately support a generative model. 4Due to the smaller grammar and lexicon of Clark et al., our parser can only be evaluated on slightly over 94% of the sentences in section 23, whereas the figures for Clark et al. (2002) are on 97%. model, HWDep, predicts the correct analyses, using the development section 00. Coordination There are two instances of argument cluster coordination (constructions like cost $5,000 in July and $6,000 in August) in the development corpus. Of these, HWDep recovers none correctly. This is a shortcoming in the model, rather than in CCG: the relatively high probability both of the NP modifier analysis of PPs like in July and of NP coordination is enough to misdirect the parser. There are 203 instances of verb phrase coordination (S[:]nNP, with [:] any verbal feature) in the development</context>
</contexts>
<marker>Clark, Hockenmaier, Steedman, 2002</marker>
<rawString>Stephen Clark, Julia Hockenmaier, and Mark Steedman. 2002. Building Deep Dependency Structures Using a WideCoverage CCG Parser. 40th ACL, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three Generative Lexicalized Models for Statistical Parsing.</title>
<date>1997</date>
<booktitle>35th ACL,</booktitle>
<pages>1623</pages>
<location>Madrid,</location>
<contexts>
<context position="8807" citStr="Collins (1997)" startWordPosition="1466" endWordPosition="1467"> the training corpus does not contain all the entries required to parse the test corpus. We discuss a simple, but imperfect, solution to this problem in section 7. 5 Extending the baseline model State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes. We too can extend the baseline model described in the previous section by including more features. Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly. In order to estimate the conditional probabilities of our model, we recursively smooth empirical estimates ei of specific conditional distributions with (possible smoothed) estimates of less specific distributions ei 1, using linear interpolation: ei = ei + (1 )ei 1 is a smoothing weight which depends on the particular distribution.2 When defining models, we will indicate a backoff level with a # sign between conditioning variables, eg. A; B # C # D means that we interpolate P(:</context>
<context position="11872" citStr="Collins (1997)" startWordPosition="1982" endWordPosition="1983">resentation of the categories). Both of these results are to be expectedCCG categories encode more contextual information than Treebank labels, in particular about parents and grandparents; therefore the history feature might be expected to have less impact. Moreover, since our category set is much larger, appending the parent node will lead to an even more fine-grained partitioning of the data, which then results in sparse data problems. \x0cDistance measures for CCG Our distance measures are related to those proposed by Goodman (1997), which are appropriate for binary trees (unlike those of Collins (1997)). Every node has a left distance measure, L, measuring the distance from the head word to the left frontier of the constituent. There is a similar right distance measure R. We implemented three different ways of measuring distance: Adjacency measures string adjacency (0, 1 or 2 and more intervening words); Verb counts intervening verbs (0 or 1 and more); and Pct counts intervening punctuation marks (0, 1, 2 or 3 and more). These s are generated by the model in the following manner: at the root of the sentence, generate L with P(L j TOP), and R with P(R j TOP; L). Then, for each expansion, if </context>
<context position="13290" citStr="Collins (1997)" startWordPosition="2260" endWordPosition="2261">ght, and analogously for exp=left. L S and R S are conditioned on S and the of H and P in the direction of S: P(L S j S#R P; R H) and P(R S jS; L S#R P; R H). They are then used as further conditioning variables for the other distributions as shown in table 1. Table 3 also gives the Parseval and dependency scores obtained with each of these measures. Pct has the smallest effect. However, our model does not yet contain anything like the hard constraint on punctuation marks in Collins (1999). 5.2 Adding lexical information Gildea (2001) shows that removing the lexical dependencies in Model 1 of Collins (1997) (that is, not conditioning on wh when generating ws) decreases labeled precision and recall by only 0.5%. It can therefore be assumed that the main influence of lexical head features (words and preterminals) in Collins Model 1 is on the structural probabilities. In CCG, by contrast, preterminals are lexical categories, encoding complete subcategorization information. They therefore encode more information about the expansion of a nonterminal than Treebank POS-tags and thus are more constraining. Generating a constituents lexical category c at its maximal projection (ie. either at the root of </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three Generative Lexicalized Models for Statistical Parsing. 35th ACL, Madrid, pp. 1623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="658" citStr="Collins (1999)" startWordPosition="90" endWordPosition="91">with Combinatory Categorial Grammar Julia Hockenmaier and Mark Steedman Division of Informatics University of Edinburgh Edinburgh EH8 9LW, United Kingdom fjulia, steedmang@cogsci.ed.ac.uk Abstract This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Grammar (CCG) parser. These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations. According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar. In contrast to Gildea (2001), we find a significant improvement from modeling wordword dependencies. 1 Introduction The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank. However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parseval measures without committing itself on certain semantically significant decisions, such as predicting null elements arising from deletion or movement. The potential benefit of wide-coverage p</context>
<context position="2434" citStr="Collins, 1999" startWordPosition="362" endWordPosition="363">lasses of verbs with different subcategorization frames. As a result, the categorial lexicon extracted for this purpose from the training corpus has 1207 categories, compared with the 48 POS-tags of the Penn Treebank. On the other hand, grammar rules in CCG are limited to a small number of simple unary and binary combinatory schemata such as function application and composition. This results in a smaller and less overgenerating grammar than standard PCFGs (ca. 3,000 rules when instantiated with the above categories in sections 02-21, instead of &amp;gt;12,400 in the original Treebank representation (Collins, 1999)). 2 Evaluating a CCG parser Since CCG produces unary and binary branching trees with a very fine-grained category set, CCG Parseval scores cannot be compared with scores of standard Treebank parsers. Therefore, we also evaluate performance using a dependency evaluation reported by Collins (1999), which counts wordword dependencies as determined by local trees and their labels. According to this metric, a local tree with parent node P, head daughter H and non-head daughter S (and position of S relative to P, ie. left or right, which is implicit in CCG categories) defines a hP; H; Si dependency</context>
<context position="7707" citStr="Collins (1999)" startWordPosition="1281" endWordPosition="1282"> of H). The model itself includes no prior knowledge specific to CCG other than that it only allows unary and binary branching trees, and that the sets of nonterminals and terminals are not disjoint (hence the need to include leaf as a possible expansion, which acts as a stop probability). All the experiments reported in this section were conducted using sections 02-21 of CCGbank as training corpus, and section 23 as test corpus. We \x0creplace all rare words in the training data with their POS-tag. For all experiments reported here and in section 5, the frequency threshold was set to 5. Like Collins (1999), we assume that the test data is POStagged, and can therefore replace unknown words in the test data with their POS-tag, which is more appropriate for a formalism like CCG with a large set of lexical categories than one generic token for all unknown words. The performance of the baseline model is shown in the top row of table 3. For six out of the 2379 sentences in our test corpus we do not get a parse.1 The reason is that a lexicon consisting of the wordcategory pairs observed in the training corpus does not contain all the entries required to parse the test corpus. We discuss a simple, but </context>
<context position="9674" citStr="Collins (1999)" startWordPosition="1615" endWordPosition="1616">stributions with (possible smoothed) estimates of less specific distributions ei 1, using linear interpolation: ei = ei + (1 )ei 1 is a smoothing weight which depends on the particular distribution.2 When defining models, we will indicate a backoff level with a # sign between conditioning variables, eg. A; B # C # D means that we interpolate P(::: jA; B;C; D) with P(::: jA; B;C), which is an interpolation of P(::: jA; B;C) and P(::: jA; B). 1We conjecture that the minor variations in coverage among the other models (except Grandparent) are artefacts of the beam. 2We compute in the same way as Collins (1999), p. 185. 5.1 Adding non-lexical information The coordination feature We define a boolean feature, conj, which is true for constituents which expand to coordinations on the head path. S, +conj S=NP, +conj S=NP, conj S=(SnNP) NP IBM (SnNP)=NP buys S=NP[c], +conj conj but S=NP[c], conj S=(SnNP) NP Lotus (SnNP)=NP sells NP shares This feature is generated at the root of the sentence with P(conj j TOP). For binary expansions, conjH is generated with P(conjH jH; S; conjP) and conjS is generated with P(conjS j S # P; expP; H; conjP). Table 1 shows how conj is used as a conditioning variable. This is</context>
<context position="13170" citStr="Collins (1999)" startWordPosition="2241" endWordPosition="2242"> binary expansion, only the in the direction of the sister changes, with a probability of P(L H j L PH#P; S) if exp = right, and analogously for exp=left. L S and R S are conditioned on S and the of H and P in the direction of S: P(L S j S#R P; R H) and P(R S jS; L S#R P; R H). They are then used as further conditioning variables for the other distributions as shown in table 1. Table 3 also gives the Parseval and dependency scores obtained with each of these measures. Pct has the smallest effect. However, our model does not yet contain anything like the hard constraint on punctuation marks in Collins (1999). 5.2 Adding lexical information Gildea (2001) shows that removing the lexical dependencies in Model 1 of Collins (1997) (that is, not conditioning on wh when generating ws) decreases labeled precision and recall by only 0.5%. It can therefore be assumed that the main influence of lexical head features (words and preterminals) in Collins Model 1 is on the structural probabilities. In CCG, by contrast, preterminals are lexical categories, encoding complete subcategorization information. They therefore encode more information about the expansion of a nonterminal than Treebank POS-tags and thus a</context>
<context position="18177" citStr="Collins (1999)" startWordPosition="3135" endWordPosition="3136">36.5 68.6 HWDep (+ tagger) 7 91.7 81.4 81.8 85.6 85.9 83.6 87.5 89.9 38.1 69.1 Table 3: Performance of the models: LexCat indicates accuracy of the lexical categories; LP, LR, BP and BR (the standard Parseval scores labeled/bracketed precision and recall) are not commensurate with other Treebank parsers. hP; H; Si, hSi, and hi are as defined in section 2. CM on hi is the percentage of sentences with complete match on hi, and \x142 CD is the percentage of sentences with under 2 crossing dependencies as defined by hi. The hP; H; Si labeled dependencies we report are not directly comparable with Collins (1999), since CCG categories encode subcategorization frames. For instance, if the direct object of a verb has been recognized as such, but a PP has been mistaken as a complement (whereas the gold standard says it is an adjunct), the fully labeled dependency evaluation hP; H; Si will not award a point. Therefore, we also include in Table 3 a more comparable evaluation hSi which only takes the correctness of the non-head category into account. The reported figures are also deflated by retaining verb features like tensed/untensed. If this is done (by stripping off all verb features), an improvement of</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus Variation and Parser Performance.</title>
<date>2001</date>
<location>EMNLP, Pittsburgh, PA.</location>
<contexts>
<context position="733" citStr="Gildea (2001)" startWordPosition="102" endWordPosition="103">sion of Informatics University of Edinburgh Edinburgh EH8 9LW, United Kingdom fjulia, steedmang@cogsci.ed.ac.uk Abstract This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Grammar (CCG) parser. These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations. According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar. In contrast to Gildea (2001), we find a significant improvement from modeling wordword dependencies. 1 Introduction The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank. However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parseval measures without committing itself on certain semantically significant decisions, such as predicting null elements arising from deletion or movement. The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and its simple and sem</context>
<context position="13216" citStr="Gildea (2001)" startWordPosition="2247" endWordPosition="2248"> the sister changes, with a probability of P(L H j L PH#P; S) if exp = right, and analogously for exp=left. L S and R S are conditioned on S and the of H and P in the direction of S: P(L S j S#R P; R H) and P(R S jS; L S#R P; R H). They are then used as further conditioning variables for the other distributions as shown in table 1. Table 3 also gives the Parseval and dependency scores obtained with each of these measures. Pct has the smallest effect. However, our model does not yet contain anything like the hard constraint on punctuation marks in Collins (1999). 5.2 Adding lexical information Gildea (2001) shows that removing the lexical dependencies in Model 1 of Collins (1997) (that is, not conditioning on wh when generating ws) decreases labeled precision and recall by only 0.5%. It can therefore be assumed that the main influence of lexical head features (words and preterminals) in Collins Model 1 is on the structural probabilities. In CCG, by contrast, preterminals are lexical categories, encoding complete subcategorization information. They therefore encode more information about the expansion of a nonterminal than Treebank POS-tags and thus are more constraining. Generating a constituent</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus Variation and Parser Performance. EMNLP, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Statistical Parsing for CCG with Simple Generative Models.</title>
<date>2001</date>
<booktitle>Student Workshop, 39th ACL/ 10th EACL,</booktitle>
<pages>712</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="3200" citStr="Hockenmaier (2001)" startWordPosition="493" endWordPosition="494">e compared with scores of standard Treebank parsers. Therefore, we also evaluate performance using a dependency evaluation reported by Collins (1999), which counts wordword dependencies as determined by local trees and their labels. According to this metric, a local tree with parent node P, head daughter H and non-head daughter S (and position of S relative to P, ie. left or right, which is implicit in CCG categories) defines a hP; H; Si dependency between the head word of S, wS, and the head word of H, wH. This measure is neutral with respect to the branching factor. Furthermore, as noted by Hockenmaier (2001), it does not penalize equivalent analyses of multiple modiComputational Linguistics (ACL), Philadelphia, July 2002, pp. 335-342. Proceedings of the 40th Annual Meeting of the Association for \x0cPierre Vinken ; 61 years old ; will join the board as a nonexecutive director Nov 29 N=N N ; N=N N (S[adj]nNP)nNP ; (S[dcl]nNP)=(S[b]nNP) ((S[b]nNP)=PP)=NP NP=N N PP=NP NP=N N=N N ((SnNP)n(SnNP))=N N &amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt; N N N N (SnNP)n(SnNP) &amp;gt; NP NP NP NP &amp;lt; &amp;gt; &amp;gt; NP S[adj]nNP (S[b]nNP)=PP PP &amp;gt; NPnNP S[b]nNP &amp;lt; &amp;lt; NP S[b]nNP &amp;gt; NP S[dcl]nNP &amp;lt; S[dcl] Figure 1: A CCG derivation in our corpus fiers. In the unlabeled cas</context>
<context position="6504" citStr="Hockenmaier (2001)" startWordPosition="1069" endWordPosition="1070">hat analyses involving the combinatory rules of type-raising and composition are only used when syntactically necessary. 4 Generative models of CCG derivations Expansion HeadCat NonHeadCat P(exp j : : : ) P(H j : : : ) P(S j : : : ) Baseline P P; exp P; exp; H + Conj P; con jP P; exp; con jP P; exp; H; con jP + Grandparent P; GP P; GP; exp P; GP; exp; H + P#L;R P P; exp#L;R P P; exp; H#L;R P Table 1: The unlexicalized models The models described here are all extensions of a very simple model which models derivations by a top-down tree-generating process. This model was originally described in Hockenmaier (2001), where it was applied to a preliminary version of CCGbank, and its definition is repeated here in the top row of Table 1. Given a (parent) node with category P, choose the expansion exp of P, where exp can be leaf (for lexical categories), unary (for unary expansions such as type-raising), left (for binary trees where the head daughter is left) or right (binary trees, head right). If P is a leaf node, generate its head word w. Otherwise, generate the category of its head daughter H. If P is binary branching, generate the category of its non-head daughter S (a complement or modifier of H). The</context>
</contexts>
<marker>Hockenmaier, 2001</marker>
<rawString>Julia Hockenmaier. 2001. Statistical Parsing for CCG with Simple Generative Models. Student Workshop, 39th ACL/ 10th EACL, Toulouse, France, pp. 712.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Acquiring Compact Lexicalized Grammars from a Cleaner Treebank. Third LREC,</title>
<date>2002</date>
<location>Las Palmas,</location>
<contexts>
<context position="4478" citStr="Hockenmaier and Steedman (2002)" startWordPosition="715" endWordPosition="718"> dependent of word b, not what the label of the local tree is which defines this dependency), scores can be compared across grammars with different sets of labels and different kinds of trees. In order to compare our performance with the parser of Clark et al. (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser. For further discussion we refer the reader to Clark and Hockenmaier (2002) . 3 CCGbanka CCG treebank CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Treebank trees using an algorithm described by Hockenmaier and Steedman (2002). Almost all types of constructionwith the exception of gapping and UCP (Unlike Coordinate Phrases) are covered by the translation procedure, which processes 98.3% of the sentences in the training corpus (WSJ sections 02-21) and 98.5% of the sentences in the test corpus (WSJ section 23). The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter (1992). Figure 1 shows a derivation taken from CCGbank. Categories, such as ((S[b]nNP)=PP)=NP, encode unsaturated subcat frames. The complement-adjunct distinction is made explicit; for instance as a nonexecut</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman 2002. Acquiring Compact Lexicalized Grammars from a Cleaner Treebank. Third LREC, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Probabilistic Feature Grammars.</title>
<date>1997</date>
<location>IWPT, Boston.</location>
<contexts>
<context position="8696" citStr="Goodman (1997)" startWordPosition="1450" endWordPosition="1451">st corpus we do not get a parse.1 The reason is that a lexicon consisting of the wordcategory pairs observed in the training corpus does not contain all the entries required to parse the test corpus. We discuss a simple, but imperfect, solution to this problem in section 7. 5 Extending the baseline model State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes. We too can extend the baseline model described in the previous section by including more features. Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly. In order to estimate the conditional probabilities of our model, we recursively smooth empirical estimates ei of specific conditional distributions with (possible smoothed) estimates of less specific distributions ei 1, using linear interpolation: ei = ei + (1 )ei 1 is a smoothing weight which depends on the particular distribution.2 When defining models, we will indica</context>
<context position="11800" citStr="Goodman (1997)" startWordPosition="1970" endWordPosition="1971">s was an additional feature in the model rather than a change in the representation of the categories). Both of these results are to be expectedCCG categories encode more contextual information than Treebank labels, in particular about parents and grandparents; therefore the history feature might be expected to have less impact. Moreover, since our category set is much larger, appending the parent node will lead to an even more fine-grained partitioning of the data, which then results in sparse data problems. \x0cDistance measures for CCG Our distance measures are related to those proposed by Goodman (1997), which are appropriate for binary trees (unlike those of Collins (1997)). Every node has a left distance measure, L, measuring the distance from the head word to the left frontier of the constituent. There is a similar right distance measure R. We implemented three different ways of measuring distance: Adjacency measures string adjacency (0, 1 or 2 and more intervening words); Verb counts intervening verbs (0 or 1 and more); and Pct counts intervening punctuation marks (0, 1, 2 or 3 and more). These s are generated by the model in the following manner: at the root of the sentence, generate L </context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>Joshua Goodman. 1997. Probabilistic Feature Grammars. IWPT, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG Models of Linguistic Tree Representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="10565" citStr="Johnson (1998)" startWordPosition="1769" endWordPosition="1770">], conj S=(SnNP) NP Lotus (SnNP)=NP sells NP shares This feature is generated at the root of the sentence with P(conj j TOP). For binary expansions, conjH is generated with P(conjH jH; S; conjP) and conjS is generated with P(conjS j S # P; expP; H; conjP). Table 1 shows how conj is used as a conditioning variable. This is intended to allow the model to capture the fact that, for a sentence without extraction, a CCG derivation where the subject is type-raised and composed with the verb is much more likely in right node raising constructions like the above. The impact of the grandparent feature Johnson (1998) showed that a PCFG estimated from a version of the Penn Treebank in which the label of a nodes parent is attached to the nodes own label yields a substantial improvement (LP/LR: from 73.5%/69.7% to 80.0%/79.2%). The inclusion of an additional grandparent feature gives Charniak (1999) a slight improvement in the Maximum Entropy inspired model, but a slight decrease in performance for an MLE model. Table 3 (Grandparent) shows that a grammar transformation like Johnsons does yield an improvement, but not as dramatic as in the Treebank-CFG case. At the same time coverage is reduced (which might n</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG Models of Linguistic Tree Representations. Computational Linguistics, 24(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Part-OfSpeech Tagger.</title>
<date>1996</date>
<pages>133142</pages>
<location>EMNLP, Philadelphia,</location>
<contexts>
<context position="20183" citStr="Ratnaparkhi (1996)" startWordPosition="3463" endWordPosition="3464">n is strengthened by the fact that, on casual inspection of the scores for individual sentences, it is sometimes the case that the lexicalized models perform worse than the unlexicalized models. 5.4 The impact of tagging errors All of the experiments described above use the POStags as given by CCGbank (which are the Treebank tags, with some corrections necessary to acquire correct features on categories). It is reasonable to assume that this input is of higher quality than can be produced by a POS-tagger. We therefore ran the \x0cdependency model on a test corpus tagged with the POS-tagger of Ratnaparkhi (1996), which is trained on the original Penn Treebank (see HWDep (+ tagger) in Table 3). Performance degrades slightly, which is to be expected, since our approach makes so much use of the POS-tag information for unknown words. However, a POS-tagger trained on CCGbank might yield slightly better results. 5.5 Limitations of the current model Unlike Clark et al. (2002), our parser does not always model the dependencies in the logical form. For example, in the interpretation of a coordinate structure like buy and sell shares, shares will head an object of both buy and sell. Similarly, in examples like</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-OfSpeech Tagger. EMNLP, Philadelphia, pp. 133142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge Mass. \x0c&amp;apos;</location>
<contexts>
<context position="1511" citStr="Steedman (2000)" startWordPosition="217" endWordPosition="218">s Parseval scores of over 89% on the Penn Treebank. However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parseval measures without committing itself on certain semantically significant decisions, such as predicting null elements arising from deletion or movement. The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and its simple and semantically transparent capture of extraction and coordination. We present a number of models over syntactic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al. (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations. CCG grammars are characterized by much larger category sets than standard Penn Treebank grammars, distinguishing for example between many classes of verbs with different subcategorization frames. As a result, the categorial lexicon extracted for this purpose from the training corpus has 1207 categories, compared with the 48 POS-tags of the Penn Treebank. On the other hand, grammar rules in CCG are limited to a small number of </context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge Mass. \x0c&amp;apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>