<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000421">
<note confidence="0.267912">
b&quot;Coling 2010: Poster Volume, pages 507515,
</note>
<address confidence="0.517547">
Beijing, August 2010
</address>
<title confidence="0.816532">
Challenges from Information Extraction to Information Fusion
</title>
<author confidence="0.966343">
Heng Ji
</author>
<affiliation confidence="0.898229">
Computer Science Department
Queens College and Graduate Center
City University of New York
</affiliation>
<email confidence="0.996519">
hengji@cs.qc.cuny.edu
</email>
<sectionHeader confidence="0.990201" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998067833333333">
Information Extraction (IE) technology is fac-
ing new challenges of dealing with large-scale
heterogeneous data sources from different
documents, languages and modalities. Infor-
mation fusion, a new emerging area derived
from IE, aims to address these challenges. We
specify the requirements and possible solu-
tions to perform information fusion. The is-
sues include redundancy removal, contradic-
tion resolution and uncertainty reduction. We
believe this is a critical step to advance IE to a
higher level of performance and portability.
</bodyText>
<sectionHeader confidence="0.99662" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998205152542373">
Latest development of Information Extraction
(IE) techniques has made it possible to extract
facts (entities, relations and events) from un-
structured documents, and converting them into
structured representations (e.g. databases). Once
the collection grows beyond a certain size, an
issue of critical importance is how a user can
monitor a compact knowledge base or identify
the interesting portions without having to (re)
read large amounts of facts. In this situation us-
ers are often more concerned with the speed in
which they obtain results, rather than obtaining
the exact answers to their queries (Jagadish et
al., 1999). The facts extracted from heterogene-
ous data sources (e.g. text, images, speech and
videos) must then be integrated in a knowledge
base, so that it can be queried in a uniform way.
This provides unparalleled challenges and op-
portunities for improved decision making.
Data can be noisy, incorrect, or misleading.
Unstructured data, mostly text, is difficult to in-
terpret. In practice it is often the case that there
are multiple sources which need to be extracted
and compressed. In a large, diverse, and inter-
connected system, it is difficult to assure accu-
racy or even coherence among the data sources.
In this environment, traditional IE would be of
little value. Most current IE systems focus on
processing a single document and language, and
are customized for a single data modality. In ad-
dition, automatic IE systems are far from perfect
and tend to produce errors.
Achieving really advances in IE requires that
we take a broader view, one that looks outside a
single source. We feel the time is now ripe to
incorporate some information integration tech-
niques in the database community (e.g. Seligman
et al., 2010) to extend the IE paradigm to real-
time information fusion and raise IE to a higher
level of performance and portability. This re-
quires us to work on a more challenging problem
of information fusion - to remove redundancy,
resolve contradictions and uncertainties by mul-
tiple information providers and design a general
framework for the veracity analysis problem.
The goal of this paper is to lay out the current
status and potential challenges of information
fusion, and suggest the following possible re-
search avenues.
Cross-document: We will discuss how to
effectively aggregate facts across documents
via entity and event coreference resolution.
Cross-lingual: A shrinking fraction of the
worlds Web pages are written in English,
and so the ability to access pages across a
range of languages is becoming increasingly
important for many applications. This need
can be addressed in part by cross-lingual in-
formation fusion. We will discuss the chal-
</bodyText>
<page confidence="0.989618">
507
</page>
<bodyText confidence="0.993864333333333">
\x0clenges of extraction and translation respec-
tively.
Cross-media: Advances in speech and im-
age processing make the application of IE
possible on other data modalities, beyond
traditional textual documents.
</bodyText>
<sectionHeader confidence="0.944749" genericHeader="method">
2 Cross-Document Information Fusion
</sectionHeader>
<bodyText confidence="0.988923666666667">
Most current IE systems focus on processing one
document at a time, and except for coreference
resolution, operate one sentence at a time. The
systems make only limited use of facts already
extracted in the current document. The output
contains rich structures about entities, relations
and events involving such entities. However, due
to noise, uncertainty, volatility and unavailability
of IE components, the collected facts may be
incomplete, noisy and erroneous. Several recent
studies have stressed the benefits of using infor-
mation fusion across documents. These methods
investigate quite different angles while follow a
common research theme, namely to exploit
global background knowledge.
</bodyText>
<subsectionHeader confidence="0.915568">
2.1 Information Inference
</subsectionHeader>
<bodyText confidence="0.99848036">
Achieving really high performance (especially,
recall) of IE requires deep semantic knowledge
and large costly hand-labeled data. Many sys-
tems also exploited lexical gazetteers. However,
such knowledge is relatively static (it is not up-
dated during the extraction process), expensive
to construct, and doesnt include any probabilis-
tic information. Error analysis on relation extrac-
tion shows that a majority (about 78%) of errors
occur on nominal mentions, and more than 90%
missing errors occur due to the lack of enough
patterns to capture the context between two en-
tity mentions. For instance, to describe the lo-
cated relation between a bomber and a bus,
there are more than 50 different intervening
strings (e.g. killed many people on a, s at-
tack on a, blew apart a, blew himself up on
a, drove his explosives-laden car into a,
had rigged the, set off a bomb on a, etc.),
but the ACE1
training corpora only cover about
1/3 of these expressions.
Several recent studies have stressed the bene-
fits of using information redundancy on estimat-
ing the correctness of the IE output (Downey et
</bodyText>
<equation confidence="0.531277">
1
http://www.itl.nist.gov/iad/mig/tests/ace/
</equation>
<bodyText confidence="0.998929039215687">
al., 2005), improving disease event extraction
(Yangarber, 2006), Message Understanding
Conference event extraction (Mann, 2007; Pat-
wardhan and Riloff, 2009) and ACE event ex-
traction (Ji and Grishman, 2008). This approach
is based on the premise that many facts will be
reported multiple times from different sources in
different forms. This may occur both within the
same document and within a cluster of topically
related and successive documents. Therefore, by
aggregating similar facts across documents and
conducting statistical global inference by favor-
ing interpretation consistency, enhanced extrac-
tion performance can be achieved with heteroge-
neous data than uniform data.
The underlying hypothesis of cross-document
inference is that the salience of a fact should be
calculated by taking into consideration both its
confidence and the confidence of other facts
connected to it, which is inspired by PageRank
(Page et al., 1998) and LexRank (Erkan and
Radev, 2004). For example, a vote by linked en-
tities which are highly voted on by other entities
is more valuable than a vote from unlinked enti-
ties. There are two major heuristics: (1) an as-
sertion that several information providers agree
on is usually more trustable than that only one
provider suggests; and (2) an information pro-
vider is trustworthy if it provides many pieces of
true information, and a piece of information is
likely to be true if it is provided by many trust-
worthy providers. (Yin et al., 2008) used the
above heuristics in a progressive, iterative en-
hancement process for information fusion.
The results from the previous work are prom-
ising, but the heuristic inferences are highly de-
pendent on the order of applying rules, and the
performance may have been limited by the
thresholds which may overfit a small develop-
ment corpus. One promising method might be
using Markov Logic Networks (Richardson and
Domingos, 2006), a statistical relational learning
language, to model these global inference rules
more declaratively. Markov Logic will make it
possible to compactly specify probability distri-
butions over the complex relational inferences. It
can capture non-deterministic (soft) rules that
tend to hold among facts but do not have to. Ex-
ploiting this approach will also provide greater
flexibility to incorporate additional linguistic and
world knowledge into inference.
</bodyText>
<page confidence="0.993171">
508
</page>
<bodyText confidence="0.995101794871795">
\x0cThe information fused across documents can
be represented as an information network (Ji,
2009) in which entities can be viewed as vertices
on the graph and they can be connected by some
type of static relationship (e.g. those attributes
defined in NIST TAC-KBP task (McNamee and
Dang, 2009)), or as a temporal chain linking dy-
namic events (e.g. Bethard and Martin, 2008;
Chambers and Jurafsky, 2009; Ji et al., 2009a).
The latter representation is more attractive be-
cause business or international affairs analysts
often review many news reports to track people,
companies, and government activities and
trends. The query logs from the commercial
search engines show that there is a fair number
of news related queries (Mishne &amp; de Rijke,
2006), suggesting that blog search users have an
interest in the blogosphere response to news sto-
ries as they develop. For example, (Ji et al.,
2009a) extracted centroid entities and then
linked events centered around the same centroid
entities on a time line.
Temporal ordering is a challenging task in
particular because about half of the event men-
tions dont include explicit time arguments. The
text order by itself is a poor predictor of chrono-
logical order (only 3% temporal correlation with
the true order). Single-document IE technique
can identify and normalize event time arguments
from the texts, which results in a much better
correlation score of 44% (Ji et al., 2009a). But
this is still far from the ideal performance for
real applications. In order to alleviate this bottle-
neck, a possible solution is to exploit global
knowledge from the related documents and
Wikipedia, and related events to recover and
predict some implicit time arguments (Filatova
and Hovy, 2001; Mani et al., 2003; Mann, 2007;
Eidelman, 2008; Gupta and Ji, 2009).
</bodyText>
<subsectionHeader confidence="0.999496">
2.2 Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.999298147540984">
One of the key challenges for information fusion
is cross-document entity coreference precise
clustering of mentions into correct entities.
There are two principal challenges: the same
entity can be referred to by more than one name
string and the same name string can refer to
more than one entity. The recent research has
been mainly promoted in the web people search
task (Artiles et al., 2007) such as (Balog et al.,
2008), ACE2008 such as (Baron and Freedman,
2008) and NIST TAC KBP (McNamee and
Dang, 2009) evaluations. Interestingly, the qual-
ity of information can often be improved by the
fused fact network itself, which can be called as
self-boosting of information fusion. For exam-
ple, if two GPE entities are involved in a con-
flict-attack event, then they are unlikely to be
connected by a part-whole relation; Mah-
moud Abbas and Abu Mazen are likely to be
coreferential if they get involved in the same
life-born event. Some prior work (Ji et al.,
2005; Jing et al., 2007) demonstrated the effec-
tiveness of using semantic relations to improve
entity coreference resolution; while (Downey et
al., 2005; Sutton and McCallum, 2004; Finkel et
al., 2005; Mann, 2007) experimented with in-
formation fusion of relations across multiple
documents. The TextRunner system (Banko et
al., 2007) can collapse and compress redundant
facts extracted from multiple documents based
on coreference resolution (Yates and Etzioni,
2009), semantic similarity computation and nor-
malization.
Two relations are central for event fusion:
contradiction part of one event mention con-
tradicts part of another, and redundancy part of
one event mention conveys the same content as
(or is entailed by) part of another. Once these
central relations are identified they will provide
a basis for identifying more complex relations
such as elaboration, presupposition or conse-
quence. It is important to note that redundancy
and contradiction among event mentions are
logical relations that are not captured by tradi-
tional topic-based techniques for similarity de-
tection (e.g. Brants and Stolle, 2002). Contradic-
tions also arise from complex differences in the
structure of assertions, discrepancies based on
world-knowledge, and lexical contrasts. Ritter et
al. (2009) described a contradiction detection
method based on functional relations and pointed
out that many contradictory fact pairs from the
Web appear consistent, and that requires back-
ground knowledge to predict.
Assessing event coreference is essential: for
texts to contradict, they must refer to the same
event. Event coreference resolution is more chal-
lenging than entity coreference because each
linking decision needs to be made based upon
the overall similarity of the event trigger and
multiple arguments. Hasler and Orasan (2009)
</bodyText>
<page confidence="0.990273">
509
</page>
<bodyText confidence="0.983687411764706">
\x0cfurther found that in many cases even coreferen-
tial even arguments are not good indicators for
event coreference.
Earlier work on event coreference resolution
(e.g. Bagga and Baldwin, 1999) was limited to
several MUC scenarios. Recent work (Chen et
al., 2009) focus on much wider coverage of
event types defined in ACE. The methods from
the knowledge fusion community (e.g. Appriou
et al., 2001; Gregoire, 2006) mostly focus on
resolving conflicts rather than identifying them
(i.e. inconsistency problem rather than ambigu-
ity). These approaches allow the conflicts to be
resolved in a straightforward way but they rely
on the availability of meta-data (e.g., distribution
of weights between attributes, probability as-
signment etc.). However, it is not always clear
where to get this meta-data.
The event attributes such as Modality, Polarity,
Genericity and Tense (Sauri et al., 2006) will
play an important role in event coreference reso-
lution because two event mentions cannot be
coreferential if any of the attributes conflict with
each other. Such attempts have been largely ne-
glected in the prior research due to the low
weights of attribute labeling in the ACE scoring
metric. (Chen et al., 2009) demonstrated that
simple automatic event attribute labeling can
significantly improve event coreference resolu-
tion. In addition, some very recent work includ-
ing (Nicolae and Nicolae, 2006; Ng, 2009; Chen
et al., 2009) found that graph-cut based cluster-
ing can improve coreference resolution. The
challenge lies in computing the affinity matrix.
</bodyText>
<sectionHeader confidence="0.982422" genericHeader="method">
3 Cross-Lingual Information Fusion
</sectionHeader>
<bodyText confidence="0.99914225">
Cross-lingual comparable corpora are also
prevalent now because almost all the influential
events can be reported in multi-languages at the
first time, but probably in different aspects.
Therefore, linked fact networks can be con-
structed and lots of research tasks can benefit
from such structures. Since the two networks are
similar in structure but not homogeneous, we can
do alignment and translation which may advance
information fusion. Cross-lingual information
fusion is concerned with technologies that fuse
the information available in various languages
and present the fused information in the user-
preferred language. The following fundamental
cross-lingual IE pipelines can be employed: (1)
Translate source language texts into target lan-
guage, and then run target language IE on the
translated texts. (2) Run source language IE on
the source language texts, and then use machine
translation (MT) word alignments to translate
(project) extracted information into target lan-
guages. Regardless of the different architectures,
both pipelines are facing the following chal-
lenges from extraction and translation.
</bodyText>
<subsectionHeader confidence="0.999271">
3.1 Extraction Challenges
</subsectionHeader>
<bodyText confidence="0.9966056">
Some recent fusion work focus on cross-lingual
interaction and inference to improve both sides
synchronously, beyond the parallel comparisons
of cross-lingual IE pipelines in (e.g. Riloff et al.,
2002). One of such examples is on cross-lingual
co-training (e.g. Cao et al., 2003; Chen and Ji,
2009). In co-training (Blum and Mitchell, 1998),
the uncertainty of a classifier is defined as the
portion of instances on which it cannot make
classification decisions. Exchanging tagged data
in bootstrapping can help reduce the uncertain-
ties of classifiers. The cross-lingual fusion proc-
ess satisfies the co-training algorithms assump-
tions about two views (in this case, two lan-
guages): (1) the two views are individually suffi-
cient for classification (IE systems in both lan-
guages were learned from annotated corpora
which are enough for reasonable extraction per-
formance); (2) the two views are conditionally
independent given the class (IE systems in dif-
ferent languages may use different features and
resources).
(Cao et al., 2003) indicated that uncertainty
reduction is an important factor for enhancing
the performance of co-training. Its important to
design new uncertainty measures for represent-
ing the degree of uncertainty correlation of the
two classifiers in co-training. (Chen and Ji, 2009)
proposed a new co-training framework using
cross-lingual information projection. They dem-
onstrated that this framework is particularly ef-
fective for a challenging IE task which is situ-
ated at the end of a pipeline and thus suffers
from the errors propagated from upstream proc-
essing and has low-performance baseline.
</bodyText>
<subsectionHeader confidence="0.998794">
3.2 Translation Challenges
</subsectionHeader>
<bodyText confidence="0.998049666666667">
Because the facts are aggregated from multiple
languages, the translation errors will bring us
great challenges. However, in order to extend
</bodyText>
<page confidence="0.934588">
510
</page>
<bodyText confidence="0.998728460317461">
\x0ccross-lingual information fusion techniques to
more language pairs, we can start from the much
more scalable task of information translation
(Etzioni et al., 2007). The additional processing
may take the form of machine translation (MT)
of extracted facts such as names and events. IE
tasks performed notably worse on machine trans-
lated texts than on texts originally written in
English, and error analysis indicated that a major
cause was the low quality of name translation (Ji
et al., 2009b). Traditional MT systems focus on
the overall fluency and accuracy of the transla-
tion but fall short in their ability to translate cer-
tain informationally critical words. In particular,
it appears that better entity name translation can
substantially improve cross-lingual information
fusion.
Some recent work (e.g. Klementiev and Roth,
2006; Ji, 2009) has exploited comparable cor-
pora to enhance information translation. There
are no document-level or sentence-level align-
ments across languages, but important facts such
as names, relations and events in one language in
such corpora tend to co-occur with their coun-
terparts in the other. (Ji, 2009) used a bootstrap-
ping approach to align the information networks
from bilingual comparable corpora, and discover
name translations and extract relations links si-
multaneously. The general idea is to start from a
small seed set of common name pairs, and then
rely on the link attributes to align their related
names. Then the new name translations are
added to the seed set for the next iteration. This
bootstrapping procedure is repeated until no new
translations are produced. This approach is based
on graph traverses and doesnt need a name
transliteration module to serve as baseline, or
compute document-wise temporal distributions.
The novelty of using comparable corpora lies
in constructing and mining multi-lingual infor-
mation fusion framework which is capable of
self-boosting. First, this approach can generate
information translation pairs with high accuracy
by using a small seed set. Second, the shortcom-
ings of traditional approaches are due to their
limited use of IE techniques, and this approach
can effectively integrate extraction and transla-
tion based on reliable confidence estimation.
Third, compared to bitexts this approach can
take advantage of much less expensive compara-
ble corpora. This approach can be extended to
foster the research in other aspects for informa-
tion fusion. For example, the aligned sub-graphs
with names, relations and events can be used to
reduce information redundancy; the outlier (mis-
aligned) sub-graphs can be used to detect the
novel or local information described in one lan-
guage but not in the other after the fusion proc-
ess. It does happen that the two persons have
been explicitly reported as Father and Son rela-
tionship in one language, but in the other lan-
guage, they are just reported as two common
persons.
</bodyText>
<sectionHeader confidence="0.983269" genericHeader="method">
4 Cross-Media Information Fusion
</sectionHeader>
<bodyText confidence="0.996053636363636">
The research challenges discussed so far con-
cerned with textual data. Besides written texts,
ever-increasing human generated data is avail-
able as speech recordings, microblogs, images
and videos. We now discuss how to develop
techniques for fusing a variety of media sources.
State-of-the-art IE techniques have been devel-
oped primarily on newspaper articles and a few
web texts, and it is not clear how systems would
perform on other sources and how to integrate all
available information.
</bodyText>
<subsectionHeader confidence="0.999682">
4.1 Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.999164291666667">
The main challenge is on designing a coherent
information fusion framework that is able to ex-
ploit information across different parts of multi-
media documents and link them via cross-media
coreference resolution. The framework will han-
dle multimedia information by considering not
only the documents text and images data but
also the layout structure which determines how a
given text block is related to a particular image
or video. For example, a Web news page about
Health Care Reform in America is composed
by text describing some event (e.g., Final Senate
vote for the reform plans, Obama signs the re-
form agreement), images (e.g., images about
various government involvements over decades)
and videos (e.g. Obamas speech video about the
decisions) containing additional information re-
garding the real extent of the event or providing
evidence corroborating the text part.
Current state-of-the-art information fusion ap-
proaches can be divided into two groups: formal
top-down methods from the generic knowl-
edge fusion community and quantitative bot-
tom-up techniques from the applied Semantic
</bodyText>
<page confidence="0.984371">
511
</page>
<bodyText confidence="0.999125470588236">
\x0cWeb community (Appriou et al., 2001; Gregoire,
2006). Both approaches have their limitations. It
will be beneficial to combine both types of ap-
proaches so that the fusion decision can be made
depending on the type of problem and the
amount of domain information it possesses. Sag-
gion et al. (2004) described a multimedia extrac-
tion approach to create composite index from
multiple and multi-lingual sources. Magalhaes
et al. (2008) described a semantic similarity met-
ric based on key word vectors for multi-media
fusion. Iria and Magalhaes (2009) exploited in-
formation across different parts of a multimedia
document to improve document classification. It
is important to go beyond key words and attempt
representing the documents by the semantic facts
identified by IE.
One possible solution is to exploit the linkage
information. Specifically, coreference resolution
methods should be applied to four types of cross-
media data: (1) between the captions of images
and context texts; (2) detecting HTML cross-
media associations and quantifying the level of
image and text block correlation (3) between the
texts embedded in images and context texts; (4)
between the transcribed texts from the speech in
video clips (via automatic speech recognition)
and context texts. We can apply a similarity
graph to incorporate virtual linkages. For exam-
ple, when we see images of two web documents
containing the same object, we can raise our
confidence that such documents are semanti-
cally correlated even if the two web documents
are from different sources.
</bodyText>
<subsectionHeader confidence="0.9984">
4.2 Uncertainty Reduction
</subsectionHeader>
<bodyText confidence="0.999455622222222">
When we combine information from images and
their associated texts (e.g. meta-data, captions,
surrounding text, transcription), one of the chal-
lenges lies in the uncertainty of text representa-
tion. Therefore it is important to study both how
to learn good models from different sources with
different kinds of associated uncertainty, and
how to make use of these, along with their level
of uncertainty in supporting coherent decisions,
taking into account characteristics of the data as
well as of its source.
The descriptions are usually generated by hu-
mans and thus are prone to error or subjectiv-
ity. The images, especially the web images, are
typically labeled by different users in different
languages and cultural backgrounds. It is unreal-
istic to expect descriptions to be consistent. In
speech conversations, many facts are often em-
bedded in questions such as It&amp;apos;s OK to put De-
mocratic career politicians at the Pentagon and
the Justice Department if they&amp;apos;re Democrats but
not if they&amp;apos;re Republicans, is that right? This
challenge can be generally addressed by
strengthening semantic attribute classification
methods for Modality, Polarity and Genericity.
And if the data sources are comparable, a more
direct method of committee-based voting can
also be exploited.
However, the fusion process may itself cause
data uncertainties. We can follow the co-training
framework as described in section 3.1 to reduce
uncertainty in fusion. To handle the missing la-
bels, a promising approach is to use graph-based
label propagation (Deshpande et al., 2009),
which can capture complex uncertainties and
correlations in the data in a uniform manner. Its
also worth importing the multi-dimensional un-
certainty analysis framework described in data
mining community (Aggarwal, 2010). The
multi-dimensional uncertainty analysis method
exactly suits the multi-media fusion needs: it
allows us to combine first-order logic with prob-
abilities, modeling inferential uncertainty about
multiple aspects - both the context of facts and
intended meanings.
</bodyText>
<subsectionHeader confidence="0.998802">
4.3 Joint Modeling
</subsectionHeader>
<bodyText confidence="0.999101052631579">
IE is generally applied on top of machine gener-
ated transcription and automatic structuring that
suffer from errors compared to the true content
of relations and events. In the context of infor-
mation fusion we can divide the problem of ad-
aptation into two types: (1) radical adaptation
such as from newswire to biomedical articles;
(2) modest adaptation such as from newswire to
wikipedia or automatic speech recognition
(ASR) output. (1) requires a great deal of new
development such as ontology definition and
data annotation; while (2) can be partially ad-
dressed during the information fusion process.
For example, while dealing with speech input,
IE systems need to be robust to the noise intro-
duced by earlier speech processing tasks such as
ASR, sentence segmentation, salience detection
and and speaker identification. Some earlier
work (Makhoul et al., 2005; Favre et al., 2008)
</bodyText>
<page confidence="0.985786">
512
</page>
<bodyText confidence="0.999561375">
\x0cshowed that using an IE system trained from
newswire, the performance degrades notably
when the system is tested on automatic speech
recognition output. But no general solutions
have been proposed to address the genre-specific
challenges for speech data.
More specifically, pronoun resolution is one
of the major challenges (Jing et al., 2007). For
example, in wikipedia a lot of pronouns may
refer to the entry entity; while in speech conver-
sation we will need to resolve first and second
person pronouns based on automatic speaker role
identification; and improve cross-sentence third
pronoun resolution by exploiting gender and
animacy knowledge discovery methods.
The processing methods of text and other me-
dia are typically organized as a pipeline architec-
ture of processing stages (e.g. from pattern rec-
ognition, to information fusion, and to summari-
zation). Each of these stages has been studied
separately and quite intensively over the past
decade. Its critical to move away from ap-
proaches that make chains of independent local
decisions, and instead toward methods that make
multiple decisions jointly using global informa-
tion. Joint inference techniques (Roth and Yih,
2004; Ji et al., 2005; McCallum, 2006) can trans-
form the integration of multi-media into a bene-
fit by reducing the errors in individual stages. In
doing so, we can take advantage (among other
properties) of the coherence of a discourse: that a
correct analysis of a text discourse reveals a
large number of connections from the image in-
formation in its context, and so (in general) a
more tightly connected analysis is more likely to
be correct. For example, prior work has demon-
strated the benefit of jointly modeling name tag-
ging and n-best hypotheses, ASR lattices or
word confusion networks (Hakkani-Tur et al.,
2006).
</bodyText>
<sectionHeader confidence="0.994367" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998595294117647">
In the current information explosion era, IE
technology is facing new challenges of dealing
with heterogeneous data sources from different
documents, languages and media which may
contain a multiplicity of aspects on particular
entities, relations and events. This new phenom-
ena requires IE to perform both traditional lower
level processing as well as information fusion of
factual data based on implicit inferences. This
paper investigated the issues of information fu-
sion on a massive scale and the challenges have
not been discussed in previous work. We speci-
fied the requirements and possible solutions for
various dimensions to perform information fu-
sion. We also overviewed some recent work to
demonstrate how these goals can be achieved.
The field of information fusion is relatively
new; and the nature of different data sources
provides new ideas and challenges which are not
present in other research. While much research
has been performed in the area of data fusion,
the context of automatic extraction provides a
different perspective in which the fusion is per-
formed in the context of a lot of uncertainty and
noise. This new task will provide connections
between NLP and other areas such as data min-
ing and knowledge discovery. The progress on
this task would save, anybody concerned with
staying informed, an enormous amount of time.
These are certainly ambitious goals and require
long-term development of fusion and adaptation
methods. But we hope that this outline of the
research challenges will bring us closer to the
goal.
</bodyText>
<sectionHeader confidence="0.420195" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<table confidence="0.812004">
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
Number W911NF-09-2-0053, the U.S. NSF
CAREER Award under Grant IIS-0953149,
Google, Inc., DARPA GALE Program, CUNY
Research Enhancement Program, PSC-CUNY
Research Program, Faculty Publication Program
</table>
<bodyText confidence="0.971996222222222">
and GRTI Program. The views and conclusions
contained in this document are those of the au-
thors and should not be interpreted as represent-
ing the official policies, either expressed or im-
plied, of the Army Research Laboratory or the
U.S. Government. The U.S. Government is au-
thorized to reproduce and distribute reprints for
Government purposes notwithstanding any copy-
right notation here on.
</bodyText>
<sectionHeader confidence="0.985756" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9988415">
Charu Aggarwal. 2010. On Multi-dimensional Sharp-
ening of Uncertain Data. SIAM: SIAM Conference
on Data Mining (SDM10).
A. Appriou-, A. Ayoun, et al. 2001. Fusion: General
concepts and characteristics. International Journal
of Intelligent Systems 16(10).
</reference>
<page confidence="0.978237">
513
</page>
<reference confidence="0.999235900900902">
\x0cJavier Artiles, Julio Gonzalo and Satoshi Sekine.
2007. The SemEval-2007 WePS Evaluation: Es-
tablishing a benchmark for the Web People Search
Task. Proc. Semeval-2007.
Amit Bagga and Breck Baldwin. 1999. Cross-
document Event Coreference: Annotations, Ex-
periments, and Observations. Proc. ACL1999
Workshop on Coreference and Its Applications.
K. Balog, L. Azzopardi, M. de Rijke. 2008. Personal
Name Resolution of Web People Search. Proc.
WWW2008 Workshop: NLP Challenges in the In-
formation Explosion Era (NLPIX 2008).
Michele Banko, Michael J Cafarella, Stephen
Soderland and Oren Etzioni. 2007. Open Informa-
tion Extraction from the Web. Proc. IJCAI 2007.
Alex Baron and Marjorie Freedman. 2008. Who is
Who and What is What: Experiments in Cross-
Document Co-Reference. Proc. EMNLP 2008.
Steven Bethard and James H. Martin. 2008. Learning
semantic links from a corpus of parallel temporal
and causal relations. Proc. ACL-HLT 2008.
Avrim Blum and Tom Mitchell. 1998. Combining
Labeled and Unlabeled Data with Co-training.
Proc. of the Workshop on Computational Learning
Theory. Morgan Kaufmann Publishers.
T. Brants and R. Stolle. 2002. Finding Similar Docu-
ments in Document Collections. Proc. LREC
Workshop on Using Semantics for Information
Retrieval and Filtering.
Yunbo Cao, Hang Li and Li Lian. 2003. Uncertainty
Reduction in Collaborative Bootstrapping:
Measure and Algorithm. Proc. ACL 2003.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised Learning of Narrative Schemas and
their Participants. Proc. ACL 09.
Zheng Chen and Heng Ji. 2009. Can One Language
Bootstrap the Other: A Case Study on Event Ex-
traction. Proc. HLT-NAACL Workshop on Semi-
supervised Learning for Natural Language Proc-
essing. Boulder, Co.
Zheng Chen, Heng Ji and Robert Harallick. 2009. A
Pairwise Coreference Model, Feature Impact and
Evaluation for Event Coreference Resolution.
Proc. RANLP 2009 workshop on Events in Emerg-
ing Text Types.
Amol Deshpande, Lise Getoor and Prithviraj Sen.
2009. Graphical Models for Uncertain Data.
Managing and Mining Uncertain Data (Edited
by Charu Aggarwal). Springer.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A Probabilistic Model of Redundancy in In-
formation Extraction. Proc. IJCAI 2005.
Vladimir Eidelman. 2008. Inferring Activity Time in
News through Event Modeling. Proc. ACL-HLT
2008.
Gunes Erkan and Dragomir R. Radev. 2004.
LexPageRank: Prestige in multi-document text
summarization. Proc. EMNLP 2004.
Oren Etzioni, Kobi Reiter, Stephen Soderland and
Marcus Sammer. 2007. Lexical Translation with
Application to Image Search on the Web. Proc.
Machine Translation Summit XI.
Benoit Favre, Ralph Grishman, Dustin Hillard, Heng
Ji, Dilek Hakkani-Tur and Mari Ostendorf. 2008.
Punctuating Speech for Information Extraction.
Proc. ICASSP 2008.
Elena Filatova and Eduard Hovy. 2001. Assigning
Time-Stamps to Event-Clauses. Proc. ACL 2001
Workshop on Temporal and Spatial Information
Processing.
Jenny Rose Finkel, Trond Grenager and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. Proc. ACL 2005.
E. Gregoire. 2006. An unbiased approach to iterated
fusion by weakening. Information Fusion. 7(1).
Prashant Gupta and Heng Ji. 2009. Predicting Un-
known Time Arguments based on Cross-event
propagation. Proc. ACL-IJCNLP 2009.
Dilek Hakkani-Tur, Frederic Bechet, Giuseppe Ric-
cardi, Gokhan Tur. 2006. Beyond ASR 1-Best: Us-
ing Word Confusion Networks in Spoken Lan-
guage Understanding. Journal of Computer Speech
and Language, Vol. 20, No. 4, pp. 495-514.
Laura Hasler and Constantin Orasan. 2009. Do
coreferential arguments make event mentions
coreferential? Proc. the 7th Discourse Anaphora
and Anaphor Resolution Colloquium (DAARC
2009).
Jose Iria and Joao Magalhaes. 2009. Exploiting
Cross-Media Correlations in the Categorization
of Multimedia Web Documents. Proc. CIAM
2009.
H. V. Jagadish, Jason Madar, and Raymond Ng. 1999.
Semantic compression and pattern extraction
with fascicles. VLDB, pages 186197.
Heng Ji, David Westbrook and Ralph Grishman.
2005. Using Semantic Relations to Refine Corefer-
ence Decisions. Proc. HLT/EMNLP 05.
Heng Ji and Ralph Grishman. 2008. Refining Event
Extraction Through Cross-document Inference.
Proc. ACL 2008.
Heng Ji. 2009. Mining Name Translations from
Comparable Corpora by Creating Bilingual In-
formation Networks. Proc. ACL-IJCNLP 2009
workshop on Building and Using Comparable
Corpora (BUCC 2009): from parallel to non-
parallel corpora.
Heng Ji, Ralph Grishman, Dayne Freitag, Matthias
Blume, John Wang, Shahram Khadivi, Richard
Zens and Hermann Ney. 2009a. Name Transla-
</reference>
<page confidence="0.976381">
514
</page>
<reference confidence="0.999312840909091">
\x0ction for Distillation. Book chapter for Global
Automatic Language Exploitation.
Heng Ji, Ralph Grishman, Zheng Chen and Prashant
Gupta. 2009b. Cross-document Event Extraction,
Ranking and Tracking. Proc. RANLP 2009.
Hongyan Jing, Nanda Kambhatla and Salim Roukos.
2007. Extracting Social Networks and Bio-
graphical Facts From Conversational Speech
Transcripts. Proc. ACL 2007.
A. Klementiev and D. Roth. 2006. Named Entity
Transliteration and Discovery from Multilingual
Comparable Corpora. Proc. HLT-NAACL 2006.
Joao Magalhaes, Fabio Ciravegna and Stefan Ruger.
2008. Exploring Multimedia in a Keyword Space.
Proc. ACM Multimedia.
Inderjeet Mani, Barry Schiffman and Jianping Zhang.
2003. Inferring Temporal Ordering of Events in
News. Proc. HLT-NAACL 2003.
John Makhoul, Alex Baron, Ivan Bulyko, Long
Nguyen, Lance Ramshaw, David Stallard, Richard
Schwartz and Bing Xiang. 2005. The Effects of
Speech Recognition and Punctuation on Informa-
tion Extraction Performance. Proc. Interspeech.
Gideon Mann. 2007. Multi-document Relationship
Fusion via Constraints on Probabilistic Data-
bases. Proc. HLT/NAACL 2007.
Andrew McCallum. 2006. Information Extraction,
Data Mining and Joint Inference. Proc. SIGKDD.
Paul McNamee and Hoa Dang. 2009. Overview of
the TAC 2009 Knowledge Base Population
Track. Proc. TAC 2009 Workshop.
Gilad Mishne and Maarten de Rijke. 2006. Capturing
Global Mood Levels using Blog Posts. Proc. AAAI
2006 Spring Symposium on Computational Ap-
proaches to Analysing Weblogs.
Vincent Ng. 2009. Graph-Cut-Based Anaphoricity
Determination for Coreference Resolution. Proc.
HLT-NAACL 2009.
Lawrence Page, Sergey Brin, Rajeev Motwani and
Terry Winograd. 1998. The PageRank Citation
Ranking: Bringing Order to the Web. Proc. WWW.
Siddharth Patwardhan and Ellen Riloff. 2009. A Uni-
fied Model of Phrasal and Sentential Evidence
for Information Extraction. 2009. Proc. EMNLP.
Matt Richardson and Pedro Domingos. 2006. Markov
Logic Networks. Machine Learning. 62:107-136.
Ellen Riloff, Charles Schafer, and David Yarowsky.
2002. Inducing Information Extraction Systems
for New Languages via Cross-Language Projec-
tion. Proc. COLING 2002.
Alan Ritter; Stephen Soderland; Doug Downey; Oren
Etzioni. 2009. Its a Contradiction no, its not:
A Case Study using Functional Relations. Proc.
EMNLP 2009.
Dan Roth and Wen-tau Yih. 2004. A Linear Pro-
gramming Formulation for Global Inference in
Natural Language Tasks. Proc. CONLL2004.
Saggion, H., Cunningham, H., Bontcheva, K., May-
nard, D., Hamza, O., and Wilks, Y. 2004. Multi-
media indexing through multi-source and multi-
language information extraction: the MUMIS pro-
ject. Data Knowlege Engineering, 48, 2, pp. 247-
264.
Roser Saur and Marc Verhagen and James Puste-
jovsky. 2006. Annotating and Recognizing Event
Modality in Text. Proc. FLAIRS 2006.
Len Seligman, Peter Mork, Alon Halevy, Ken Smith,
Michael J. Carey, Kuang Chen, Chris Wolf,
Jayant Madhavan and Akshay Kannan. 2010.
OpenII: An Open Source Information Integration
Toolkit. Proc. the 2010 international conference
on Management of data.
Charles Sutton and Andrew McCallum. 2004.
Collective Segmentation and Labeling of Distant
Entities in Information Extraction. Proc. ICML
Workshop on Statistical Relational Learning and
Its Connections to Other Fields.
Roman Yangarber. 2006. Verification of Facts across
Document Boundaries. Proc. International
Workshop on Intelligent Information Access.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised Methods for Determining Object and Rela-
tion Synonyms on the Web. Journal of Artificial
Intelligence. Res. (JAIR) 34: 255-296.
Xiaoxin Yin, Jiawei Han and Philip S. Yu. 2008.
Truth Discovery with multiple conflicting infor-
mation providers on the web. IEEE Trans.
Knowledge and Data Eng., 20:796-808.
</reference>
<page confidence="0.952612">
515
</page>
<figure confidence="0.267586">
\x0c&quot;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.389160">
<note confidence="0.8739015">b&quot;Coling 2010: Poster Volume, pages 507515, Beijing, August 2010</note>
<title confidence="0.933422">Challenges from Information Extraction to Information Fusion</title>
<author confidence="0.999487">Heng Ji</author>
<affiliation confidence="0.846137">Computer Science Department Queens College and Graduate Center City University of New York</affiliation>
<email confidence="0.999797">hengji@cs.qc.cuny.edu</email>
<abstract confidence="0.999294307692308">Information Extraction (IE) technology is facing new challenges of dealing with large-scale heterogeneous data sources from different documents, languages and modalities. Information fusion, a new emerging area derived from IE, aims to address these challenges. We specify the requirements and possible solutions to perform information fusion. The issues include redundancy removal, contradiction resolution and uncertainty reduction. We believe this is a critical step to advance IE to a higher level of performance and portability.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Charu Aggarwal</author>
</authors>
<title>On Multi-dimensional Sharpening of Uncertain Data.</title>
<date>2010</date>
<booktitle>SIAM: SIAM Conference on Data Mining (SDM10).</booktitle>
<contexts>
<context position="25018" citStr="Aggarwal, 2010" startWordPosition="3888" endWordPosition="3889">y. And if the data sources are comparable, a more direct method of committee-based voting can also be exploited. However, the fusion process may itself cause data uncertainties. We can follow the co-training framework as described in section 3.1 to reduce uncertainty in fusion. To handle the missing labels, a promising approach is to use graph-based label propagation (Deshpande et al., 2009), which can capture complex uncertainties and correlations in the data in a uniform manner. Its also worth importing the multi-dimensional uncertainty analysis framework described in data mining community (Aggarwal, 2010). The multi-dimensional uncertainty analysis method exactly suits the multi-media fusion needs: it allows us to combine first-order logic with probabilities, modeling inferential uncertainty about multiple aspects - both the context of facts and intended meanings. 4.3 Joint Modeling IE is generally applied on top of machine generated transcription and automatic structuring that suffer from errors compared to the true content of relations and events. In the context of information fusion we can divide the problem of adaptation into two types: (1) radical adaptation such as from newswire to biome</context>
</contexts>
<marker>Aggarwal, 2010</marker>
<rawString>Charu Aggarwal. 2010. On Multi-dimensional Sharpening of Uncertain Data. SIAM: SIAM Conference on Data Mining (SDM10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Appriou-</author>
<author>A Ayoun</author>
</authors>
<title>Fusion: General concepts and characteristics.</title>
<date>2001</date>
<journal>International Journal of Intelligent Systems</journal>
<volume>16</volume>
<issue>10</issue>
<marker>Appriou-, Ayoun, 2001</marker>
<rawString>A. Appriou-, A. Ayoun, et al. 2001. Fusion: General concepts and characteristics. International Journal of Intelligent Systems 16(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cJavier Artiles</author>
<author>Julio Gonzalo</author>
<author>Satoshi Sekine</author>
</authors>
<title>The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task.</title>
<date>2007</date>
<booktitle>Proc. Semeval-2007.</booktitle>
<contexts>
<context position="10192" citStr="Artiles et al., 2007" startWordPosition="1598" endWordPosition="1601">elated documents and Wikipedia, and related events to recover and predict some implicit time arguments (Filatova and Hovy, 2001; Mani et al., 2003; Mann, 2007; Eidelman, 2008; Gupta and Ji, 2009). 2.2 Coreference Resolution One of the key challenges for information fusion is cross-document entity coreference precise clustering of mentions into correct entities. There are two principal challenges: the same entity can be referred to by more than one name string and the same name string can refer to more than one entity. The recent research has been mainly promoted in the web people search task (Artiles et al., 2007) such as (Balog et al., 2008), ACE2008 such as (Baron and Freedman, 2008) and NIST TAC KBP (McNamee and Dang, 2009) evaluations. Interestingly, the quality of information can often be improved by the fused fact network itself, which can be called as self-boosting of information fusion. For example, if two GPE entities are involved in a conflict-attack event, then they are unlikely to be connected by a part-whole relation; Mahmoud Abbas and Abu Mazen are likely to be coreferential if they get involved in the same life-born event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated</context>
</contexts>
<marker>Artiles, Gonzalo, Sekine, 2007</marker>
<rawString>\x0cJavier Artiles, Julio Gonzalo and Satoshi Sekine. 2007. The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task. Proc. Semeval-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Crossdocument Event Coreference: Annotations, Experiments, and Observations.</title>
<date>1999</date>
<booktitle>Proc. ACL1999 Workshop on Coreference and Its Applications.</booktitle>
<contexts>
<context position="12770" citStr="Bagga and Baldwin, 1999" startWordPosition="1996" endWordPosition="1999">ictory fact pairs from the Web appear consistent, and that requires background knowledge to predict. Assessing event coreference is essential: for texts to contradict, they must refer to the same event. Event coreference resolution is more challenging than entity coreference because each linking decision needs to be made based upon the overall similarity of the event trigger and multiple arguments. Hasler and Orasan (2009) 509 \x0cfurther found that in many cases even coreferential even arguments are not good indicators for event coreference. Earlier work on event coreference resolution (e.g. Bagga and Baldwin, 1999) was limited to several MUC scenarios. Recent work (Chen et al., 2009) focus on much wider coverage of event types defined in ACE. The methods from the knowledge fusion community (e.g. Appriou et al., 2001; Gregoire, 2006) mostly focus on resolving conflicts rather than identifying them (i.e. inconsistency problem rather than ambiguity). These approaches allow the conflicts to be resolved in a straightforward way but they rely on the availability of meta-data (e.g., distribution of weights between attributes, probability assignment etc.). However, it is not always clear where to get this meta-</context>
</contexts>
<marker>Bagga, Baldwin, 1999</marker>
<rawString>Amit Bagga and Breck Baldwin. 1999. Crossdocument Event Coreference: Annotations, Experiments, and Observations. Proc. ACL1999 Workshop on Coreference and Its Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Balog</author>
<author>L Azzopardi</author>
<author>M de Rijke</author>
</authors>
<title>Personal Name Resolution of Web People Search.</title>
<date>2008</date>
<publisher>Proc.</publisher>
<marker>Balog, Azzopardi, de Rijke, 2008</marker>
<rawString>K. Balog, L. Azzopardi, M. de Rijke. 2008. Personal Name Resolution of Web People Search. Proc.</rawString>
</citation>
<citation valid="true">
<date>2008</date>
<booktitle>WWW2008 Workshop: NLP Challenges in the Information Explosion Era (NLPIX</booktitle>
<contexts>
<context position="22100" citStr="(2008)" startWordPosition="3437" endWordPosition="3437">es can be divided into two groups: formal top-down methods from the generic knowledge fusion community and quantitative bottom-up techniques from the applied Semantic 511 \x0cWeb community (Appriou et al., 2001; Gregoire, 2006). Both approaches have their limitations. It will be beneficial to combine both types of approaches so that the fusion decision can be made depending on the type of problem and the amount of domain information it possesses. Saggion et al. (2004) described a multimedia extraction approach to create composite index from multiple and multi-lingual sources. Magalhaes et al. (2008) described a semantic similarity metric based on key word vectors for multi-media fusion. Iria and Magalhaes (2009) exploited information across different parts of a multimedia document to improve document classification. It is important to go beyond key words and attempt representing the documents by the semantic facts identified by IE. One possible solution is to exploit the linkage information. Specifically, coreference resolution methods should be applied to four types of crossmedia data: (1) between the captions of images and context texts; (2) detecting HTML crossmedia associations and q</context>
</contexts>
<marker>2008</marker>
<rawString>WWW2008 Workshop: NLP Challenges in the Information Explosion Era (NLPIX 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<date>2007</date>
<booktitle>Open Information Extraction from the Web. Proc. IJCAI</booktitle>
<contexts>
<context position="11088" citStr="Banko et al., 2007" startWordPosition="1746" endWordPosition="1749">n fusion. For example, if two GPE entities are involved in a conflict-attack event, then they are unlikely to be connected by a part-whole relation; Mahmoud Abbas and Abu Mazen are likely to be coreferential if they get involved in the same life-born event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007) experimented with information fusion of relations across multiple documents. The TextRunner system (Banko et al., 2007) can collapse and compress redundant facts extracted from multiple documents based on coreference resolution (Yates and Etzioni, 2009), semantic similarity computation and normalization. Two relations are central for event fusion: contradiction part of one event mention contradicts part of another, and redundancy part of one event mention conveys the same content as (or is entailed by) part of another. Once these central relations are identified they will provide a basis for identifying more complex relations such as elaboration, presupposition or consequence. It is important to note that redu</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J Cafarella, Stephen Soderland and Oren Etzioni. 2007. Open Information Extraction from the Web. Proc. IJCAI 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Baron</author>
<author>Marjorie Freedman</author>
</authors>
<title>Who is Who and What is What: Experiments in CrossDocument Co-Reference.</title>
<date>2008</date>
<booktitle>Proc. EMNLP</booktitle>
<contexts>
<context position="10265" citStr="Baron and Freedman, 2008" startWordPosition="1611" endWordPosition="1614">dict some implicit time arguments (Filatova and Hovy, 2001; Mani et al., 2003; Mann, 2007; Eidelman, 2008; Gupta and Ji, 2009). 2.2 Coreference Resolution One of the key challenges for information fusion is cross-document entity coreference precise clustering of mentions into correct entities. There are two principal challenges: the same entity can be referred to by more than one name string and the same name string can refer to more than one entity. The recent research has been mainly promoted in the web people search task (Artiles et al., 2007) such as (Balog et al., 2008), ACE2008 such as (Baron and Freedman, 2008) and NIST TAC KBP (McNamee and Dang, 2009) evaluations. Interestingly, the quality of information can often be improved by the fused fact network itself, which can be called as self-boosting of information fusion. For example, if two GPE entities are involved in a conflict-attack event, then they are unlikely to be connected by a part-whole relation; Mahmoud Abbas and Abu Mazen are likely to be coreferential if they get involved in the same life-born event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity corefere</context>
</contexts>
<marker>Baron, Freedman, 2008</marker>
<rawString>Alex Baron and Marjorie Freedman. 2008. Who is Who and What is What: Experiments in CrossDocument Co-Reference. Proc. EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>James H Martin</author>
</authors>
<title>Learning semantic links from a corpus of parallel temporal and causal relations.</title>
<date>2008</date>
<booktitle>Proc. ACL-HLT</booktitle>
<contexts>
<context position="8346" citStr="Bethard and Martin, 2008" startWordPosition="1299" endWordPosition="1302">al inferences. It can capture non-deterministic (soft) rules that tend to hold among facts but do not have to. Exploiting this approach will also provide greater flexibility to incorporate additional linguistic and world knowledge into inference. 508 \x0cThe information fused across documents can be represented as an information network (Ji, 2009) in which entities can be viewed as vertices on the graph and they can be connected by some type of static relationship (e.g. those attributes defined in NIST TAC-KBP task (McNamee and Dang, 2009)), or as a temporal chain linking dynamic events (e.g. Bethard and Martin, 2008; Chambers and Jurafsky, 2009; Ji et al., 2009a). The latter representation is more attractive because business or international affairs analysts often review many news reports to track people, companies, and government activities and trends. The query logs from the commercial search engines show that there is a fair number of news related queries (Mishne &amp; de Rijke, 2006), suggesting that blog search users have an interest in the blogosphere response to news stories as they develop. For example, (Ji et al., 2009a) extracted centroid entities and then linked events centered around the same cen</context>
</contexts>
<marker>Bethard, Martin, 2008</marker>
<rawString>Steven Bethard and James H. Martin. 2008. Learning semantic links from a corpus of parallel temporal and causal relations. Proc. ACL-HLT 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-training.</title>
<date>1998</date>
<booktitle>Proc. of the Workshop on Computational Learning Theory.</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="15650" citStr="Blum and Mitchell, 1998" startWordPosition="2431" endWordPosition="2434">e texts, and then use machine translation (MT) word alignments to translate (project) extracted information into target languages. Regardless of the different architectures, both pipelines are facing the following challenges from extraction and translation. 3.1 Extraction Challenges Some recent fusion work focus on cross-lingual interaction and inference to improve both sides synchronously, beyond the parallel comparisons of cross-lingual IE pipelines in (e.g. Riloff et al., 2002). One of such examples is on cross-lingual co-training (e.g. Cao et al., 2003; Chen and Ji, 2009). In co-training (Blum and Mitchell, 1998), the uncertainty of a classifier is defined as the portion of instances on which it cannot make classification decisions. Exchanging tagged data in bootstrapping can help reduce the uncertainties of classifiers. The cross-lingual fusion process satisfies the co-training algorithms assumptions about two views (in this case, two languages): (1) the two views are individually sufficient for classification (IE systems in both languages were learned from annotated corpora which are enough for reasonable extraction performance); (2) the two views are conditionally independent given the class (IE sy</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining Labeled and Unlabeled Data with Co-training. Proc. of the Workshop on Computational Learning Theory. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>R Stolle</author>
</authors>
<title>Finding Similar Documents in Document Collections.</title>
<date>2002</date>
<booktitle>Proc. LREC Workshop on Using Semantics for Information Retrieval and Filtering.</booktitle>
<contexts>
<context position="11871" citStr="Brants and Stolle, 2002" startWordPosition="1863" endWordPosition="1866">putation and normalization. Two relations are central for event fusion: contradiction part of one event mention contradicts part of another, and redundancy part of one event mention conveys the same content as (or is entailed by) part of another. Once these central relations are identified they will provide a basis for identifying more complex relations such as elaboration, presupposition or consequence. It is important to note that redundancy and contradiction among event mentions are logical relations that are not captured by traditional topic-based techniques for similarity detection (e.g. Brants and Stolle, 2002). Contradictions also arise from complex differences in the structure of assertions, discrepancies based on world-knowledge, and lexical contrasts. Ritter et al. (2009) described a contradiction detection method based on functional relations and pointed out that many contradictory fact pairs from the Web appear consistent, and that requires background knowledge to predict. Assessing event coreference is essential: for texts to contradict, they must refer to the same event. Event coreference resolution is more challenging than entity coreference because each linking decision needs to be made ba</context>
</contexts>
<marker>Brants, Stolle, 2002</marker>
<rawString>T. Brants and R. Stolle. 2002. Finding Similar Documents in Document Collections. Proc. LREC Workshop on Using Semantics for Information Retrieval and Filtering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunbo Cao</author>
<author>Hang Li</author>
<author>Li Lian</author>
</authors>
<title>Uncertainty Reduction in Collaborative Bootstrapping: Measure and Algorithm.</title>
<date>2003</date>
<booktitle>Proc. ACL</booktitle>
<contexts>
<context position="15588" citStr="Cao et al., 2003" startWordPosition="2421" endWordPosition="2424">exts. (2) Run source language IE on the source language texts, and then use machine translation (MT) word alignments to translate (project) extracted information into target languages. Regardless of the different architectures, both pipelines are facing the following challenges from extraction and translation. 3.1 Extraction Challenges Some recent fusion work focus on cross-lingual interaction and inference to improve both sides synchronously, beyond the parallel comparisons of cross-lingual IE pipelines in (e.g. Riloff et al., 2002). One of such examples is on cross-lingual co-training (e.g. Cao et al., 2003; Chen and Ji, 2009). In co-training (Blum and Mitchell, 1998), the uncertainty of a classifier is defined as the portion of instances on which it cannot make classification decisions. Exchanging tagged data in bootstrapping can help reduce the uncertainties of classifiers. The cross-lingual fusion process satisfies the co-training algorithms assumptions about two views (in this case, two languages): (1) the two views are individually sufficient for classification (IE systems in both languages were learned from annotated corpora which are enough for reasonable extraction performance); (2) the </context>
</contexts>
<marker>Cao, Li, Lian, 2003</marker>
<rawString>Yunbo Cao, Hang Li and Li Lian. 2003. Uncertainty Reduction in Collaborative Bootstrapping: Measure and Algorithm. Proc. ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised Learning of Narrative Schemas and their Participants.</title>
<date>2009</date>
<booktitle>Proc. ACL 09.</booktitle>
<contexts>
<context position="8375" citStr="Chambers and Jurafsky, 2009" startWordPosition="1303" endWordPosition="1306">ure non-deterministic (soft) rules that tend to hold among facts but do not have to. Exploiting this approach will also provide greater flexibility to incorporate additional linguistic and world knowledge into inference. 508 \x0cThe information fused across documents can be represented as an information network (Ji, 2009) in which entities can be viewed as vertices on the graph and they can be connected by some type of static relationship (e.g. those attributes defined in NIST TAC-KBP task (McNamee and Dang, 2009)), or as a temporal chain linking dynamic events (e.g. Bethard and Martin, 2008; Chambers and Jurafsky, 2009; Ji et al., 2009a). The latter representation is more attractive because business or international affairs analysts often review many news reports to track people, companies, and government activities and trends. The query logs from the commercial search engines show that there is a fair number of news related queries (Mishne &amp; de Rijke, 2006), suggesting that blog search users have an interest in the blogosphere response to news stories as they develop. For example, (Ji et al., 2009a) extracted centroid entities and then linked events centered around the same centroid entities on a time line</context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised Learning of Narrative Schemas and their Participants. Proc. ACL 09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Chen</author>
<author>Heng Ji</author>
</authors>
<title>Can One Language Bootstrap the Other: A Case Study on Event Extraction.</title>
<date>2009</date>
<booktitle>Proc. HLT-NAACL Workshop on Semisupervised Learning for Natural Language Processing.</booktitle>
<location>Boulder, Co.</location>
<contexts>
<context position="15608" citStr="Chen and Ji, 2009" startWordPosition="2425" endWordPosition="2428">ce language IE on the source language texts, and then use machine translation (MT) word alignments to translate (project) extracted information into target languages. Regardless of the different architectures, both pipelines are facing the following challenges from extraction and translation. 3.1 Extraction Challenges Some recent fusion work focus on cross-lingual interaction and inference to improve both sides synchronously, beyond the parallel comparisons of cross-lingual IE pipelines in (e.g. Riloff et al., 2002). One of such examples is on cross-lingual co-training (e.g. Cao et al., 2003; Chen and Ji, 2009). In co-training (Blum and Mitchell, 1998), the uncertainty of a classifier is defined as the portion of instances on which it cannot make classification decisions. Exchanging tagged data in bootstrapping can help reduce the uncertainties of classifiers. The cross-lingual fusion process satisfies the co-training algorithms assumptions about two views (in this case, two languages): (1) the two views are individually sufficient for classification (IE systems in both languages were learned from annotated corpora which are enough for reasonable extraction performance); (2) the two views are condit</context>
</contexts>
<marker>Chen, Ji, 2009</marker>
<rawString>Zheng Chen and Heng Ji. 2009. Can One Language Bootstrap the Other: A Case Study on Event Extraction. Proc. HLT-NAACL Workshop on Semisupervised Learning for Natural Language Processing. Boulder, Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Chen</author>
<author>Heng Ji</author>
<author>Robert Harallick</author>
</authors>
<title>A Pairwise Coreference Model, Feature Impact and Evaluation for Event Coreference Resolution.</title>
<date>2009</date>
<contexts>
<context position="12840" citStr="Chen et al., 2009" startWordPosition="2008" endWordPosition="2011">nd knowledge to predict. Assessing event coreference is essential: for texts to contradict, they must refer to the same event. Event coreference resolution is more challenging than entity coreference because each linking decision needs to be made based upon the overall similarity of the event trigger and multiple arguments. Hasler and Orasan (2009) 509 \x0cfurther found that in many cases even coreferential even arguments are not good indicators for event coreference. Earlier work on event coreference resolution (e.g. Bagga and Baldwin, 1999) was limited to several MUC scenarios. Recent work (Chen et al., 2009) focus on much wider coverage of event types defined in ACE. The methods from the knowledge fusion community (e.g. Appriou et al., 2001; Gregoire, 2006) mostly focus on resolving conflicts rather than identifying them (i.e. inconsistency problem rather than ambiguity). These approaches allow the conflicts to be resolved in a straightforward way but they rely on the availability of meta-data (e.g., distribution of weights between attributes, probability assignment etc.). However, it is not always clear where to get this meta-data. The event attributes such as Modality, Polarity, Genericity and </context>
</contexts>
<marker>Chen, Ji, Harallick, 2009</marker>
<rawString>Zheng Chen, Heng Ji and Robert Harallick. 2009. A Pairwise Coreference Model, Feature Impact and Evaluation for Event Coreference Resolution.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RANLP</author>
</authors>
<title>workshop on Events in Emerging Text Types.</title>
<date>2009</date>
<marker>RANLP, 2009</marker>
<rawString>Proc. RANLP 2009 workshop on Events in Emerging Text Types.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amol Deshpande</author>
<author>Lise Getoor</author>
<author>Prithviraj Sen</author>
</authors>
<title>Graphical Models for Uncertain Data. Managing and Mining Uncertain Data (Edited by Charu Aggarwal).</title>
<date>2009</date>
<publisher>Springer.</publisher>
<contexts>
<context position="24797" citStr="Deshpande et al., 2009" startWordPosition="3855" endWordPosition="3858"> the Justice Department if they&amp;apos;re Democrats but not if they&amp;apos;re Republicans, is that right? This challenge can be generally addressed by strengthening semantic attribute classification methods for Modality, Polarity and Genericity. And if the data sources are comparable, a more direct method of committee-based voting can also be exploited. However, the fusion process may itself cause data uncertainties. We can follow the co-training framework as described in section 3.1 to reduce uncertainty in fusion. To handle the missing labels, a promising approach is to use graph-based label propagation (Deshpande et al., 2009), which can capture complex uncertainties and correlations in the data in a uniform manner. Its also worth importing the multi-dimensional uncertainty analysis framework described in data mining community (Aggarwal, 2010). The multi-dimensional uncertainty analysis method exactly suits the multi-media fusion needs: it allows us to combine first-order logic with probabilities, modeling inferential uncertainty about multiple aspects - both the context of facts and intended meanings. 4.3 Joint Modeling IE is generally applied on top of machine generated transcription and automatic structuring tha</context>
</contexts>
<marker>Deshpande, Getoor, Sen, 2009</marker>
<rawString>Amol Deshpande, Lise Getoor and Prithviraj Sen. 2009. Graphical Models for Uncertain Data. Managing and Mining Uncertain Data (Edited by Charu Aggarwal). Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Oren Etzioni</author>
<author>Stephen Soderland</author>
</authors>
<title>A Probabilistic Model of Redundancy in Information Extraction.</title>
<date>2005</date>
<booktitle>Proc. IJCAI</booktitle>
<contexts>
<context position="10907" citStr="Downey et al., 2005" startWordPosition="1719" endWordPosition="1722">mee and Dang, 2009) evaluations. Interestingly, the quality of information can often be improved by the fused fact network itself, which can be called as self-boosting of information fusion. For example, if two GPE entities are involved in a conflict-attack event, then they are unlikely to be connected by a part-whole relation; Mahmoud Abbas and Abu Mazen are likely to be coreferential if they get involved in the same life-born event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007) experimented with information fusion of relations across multiple documents. The TextRunner system (Banko et al., 2007) can collapse and compress redundant facts extracted from multiple documents based on coreference resolution (Yates and Etzioni, 2009), semantic similarity computation and normalization. Two relations are central for event fusion: contradiction part of one event mention contradicts part of another, and redundancy part of one event mention conveys the same content as (or is entailed by) part of another. Once these ce</context>
</contexts>
<marker>Downey, Etzioni, Soderland, 2005</marker>
<rawString>Doug Downey, Oren Etzioni, and Stephen Soderland. 2005. A Probabilistic Model of Redundancy in Information Extraction. Proc. IJCAI 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
</authors>
<title>Inferring Activity Time in News through Event Modeling.</title>
<date>2008</date>
<booktitle>Proc. ACL-HLT</booktitle>
<contexts>
<context position="9745" citStr="Eidelman, 2008" startWordPosition="1527" endWordPosition="1528">elf is a poor predictor of chronological order (only 3% temporal correlation with the true order). Single-document IE technique can identify and normalize event time arguments from the texts, which results in a much better correlation score of 44% (Ji et al., 2009a). But this is still far from the ideal performance for real applications. In order to alleviate this bottleneck, a possible solution is to exploit global knowledge from the related documents and Wikipedia, and related events to recover and predict some implicit time arguments (Filatova and Hovy, 2001; Mani et al., 2003; Mann, 2007; Eidelman, 2008; Gupta and Ji, 2009). 2.2 Coreference Resolution One of the key challenges for information fusion is cross-document entity coreference precise clustering of mentions into correct entities. There are two principal challenges: the same entity can be referred to by more than one name string and the same name string can refer to more than one entity. The recent research has been mainly promoted in the web people search task (Artiles et al., 2007) such as (Balog et al., 2008), ACE2008 such as (Baron and Freedman, 2008) and NIST TAC KBP (McNamee and Dang, 2009) evaluations. Interestingly, the quali</context>
</contexts>
<marker>Eidelman, 2008</marker>
<rawString>Vladimir Eidelman. 2008. Inferring Activity Time in News through Event Modeling. Proc. ACL-HLT 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<date>2004</date>
<contexts>
<context position="6580" citStr="Erkan and Radev, 2004" startWordPosition="1013" endWordPosition="1016"> both within the same document and within a cluster of topically related and successive documents. Therefore, by aggregating similar facts across documents and conducting statistical global inference by favoring interpretation consistency, enhanced extraction performance can be achieved with heterogeneous data than uniform data. The underlying hypothesis of cross-document inference is that the salience of a fact should be calculated by taking into consideration both its confidence and the confidence of other facts connected to it, which is inspired by PageRank (Page et al., 1998) and LexRank (Erkan and Radev, 2004). For example, a vote by linked entities which are highly voted on by other entities is more valuable than a vote from unlinked entities. There are two major heuristics: (1) an assertion that several information providers agree on is usually more trustable than that only one provider suggests; and (2) an information provider is trustworthy if it provides many pieces of true information, and a piece of information is likely to be true if it is provided by many trustworthy providers. (Yin et al., 2008) used the above heuristics in a progressive, iterative enhancement process for information fusi</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>Gunes Erkan and Dragomir R. Radev. 2004.</rawString>
</citation>
<citation valid="true">
<title>LexPageRank: Prestige in multi-document text summarization.</title>
<date>2004</date>
<booktitle>Proc. EMNLP</booktitle>
<contexts>
<context position="21966" citStr="(2004)" startWordPosition="3418" endWordPosition="3418">g the real extent of the event or providing evidence corroborating the text part. Current state-of-the-art information fusion approaches can be divided into two groups: formal top-down methods from the generic knowledge fusion community and quantitative bottom-up techniques from the applied Semantic 511 \x0cWeb community (Appriou et al., 2001; Gregoire, 2006). Both approaches have their limitations. It will be beneficial to combine both types of approaches so that the fusion decision can be made depending on the type of problem and the amount of domain information it possesses. Saggion et al. (2004) described a multimedia extraction approach to create composite index from multiple and multi-lingual sources. Magalhaes et al. (2008) described a semantic similarity metric based on key word vectors for multi-media fusion. Iria and Magalhaes (2009) exploited information across different parts of a multimedia document to improve document classification. It is important to go beyond key words and attempt representing the documents by the semantic facts identified by IE. One possible solution is to exploit the linkage information. Specifically, coreference resolution methods should be applied to</context>
</contexts>
<marker>2004</marker>
<rawString>LexPageRank: Prestige in multi-document text summarization. Proc. EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Kobi Reiter</author>
<author>Stephen Soderland</author>
<author>Marcus Sammer</author>
</authors>
<title>Lexical Translation with Application to Image Search on the Web.</title>
<date>2007</date>
<booktitle>Proc. Machine Translation</booktitle>
<location>Summit XI.</location>
<contexts>
<context position="17266" citStr="Etzioni et al., 2007" startWordPosition="2675" endWordPosition="2678">k using cross-lingual information projection. They demonstrated that this framework is particularly effective for a challenging IE task which is situated at the end of a pipeline and thus suffers from the errors propagated from upstream processing and has low-performance baseline. 3.2 Translation Challenges Because the facts are aggregated from multiple languages, the translation errors will bring us great challenges. However, in order to extend 510 \x0ccross-lingual information fusion techniques to more language pairs, we can start from the much more scalable task of information translation (Etzioni et al., 2007). The additional processing may take the form of machine translation (MT) of extracted facts such as names and events. IE tasks performed notably worse on machine translated texts than on texts originally written in English, and error analysis indicated that a major cause was the low quality of name translation (Ji et al., 2009b). Traditional MT systems focus on the overall fluency and accuracy of the translation but fall short in their ability to translate certain informationally critical words. In particular, it appears that better entity name translation can substantially improve cross-ling</context>
</contexts>
<marker>Etzioni, Reiter, Soderland, Sammer, 2007</marker>
<rawString>Oren Etzioni, Kobi Reiter, Stephen Soderland and Marcus Sammer. 2007. Lexical Translation with Application to Image Search on the Web. Proc. Machine Translation Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Favre</author>
<author>Ralph Grishman</author>
<author>Dustin Hillard</author>
<author>Heng Ji</author>
<author>Dilek Hakkani-Tur</author>
<author>Mari Ostendorf</author>
</authors>
<title>Punctuating Speech for Information Extraction.</title>
<date>2008</date>
<contexts>
<context position="26188" citStr="Favre et al., 2008" startWordPosition="4067" endWordPosition="4070">radical adaptation such as from newswire to biomedical articles; (2) modest adaptation such as from newswire to wikipedia or automatic speech recognition (ASR) output. (1) requires a great deal of new development such as ontology definition and data annotation; while (2) can be partially addressed during the information fusion process. For example, while dealing with speech input, IE systems need to be robust to the noise introduced by earlier speech processing tasks such as ASR, sentence segmentation, salience detection and and speaker identification. Some earlier work (Makhoul et al., 2005; Favre et al., 2008) 512 \x0cshowed that using an IE system trained from newswire, the performance degrades notably when the system is tested on automatic speech recognition output. But no general solutions have been proposed to address the genre-specific challenges for speech data. More specifically, pronoun resolution is one of the major challenges (Jing et al., 2007). For example, in wikipedia a lot of pronouns may refer to the entry entity; while in speech conversation we will need to resolve first and second person pronouns based on automatic speaker role identification; and improve cross-sentence third pron</context>
</contexts>
<marker>Favre, Grishman, Hillard, Ji, Hakkani-Tur, Ostendorf, 2008</marker>
<rawString>Benoit Favre, Ralph Grishman, Dustin Hillard, Heng Ji, Dilek Hakkani-Tur and Mari Ostendorf. 2008. Punctuating Speech for Information Extraction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ICASSP</author>
</authors>
<date>2008</date>
<marker>ICASSP, 2008</marker>
<rawString>Proc. ICASSP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Eduard Hovy</author>
</authors>
<title>Assigning Time-Stamps to Event-Clauses.</title>
<date>2001</date>
<booktitle>Proc. ACL 2001 Workshop on Temporal and Spatial Information Processing.</booktitle>
<contexts>
<context position="9698" citStr="Filatova and Hovy, 2001" startWordPosition="1517" endWordPosition="1520">t include explicit time arguments. The text order by itself is a poor predictor of chronological order (only 3% temporal correlation with the true order). Single-document IE technique can identify and normalize event time arguments from the texts, which results in a much better correlation score of 44% (Ji et al., 2009a). But this is still far from the ideal performance for real applications. In order to alleviate this bottleneck, a possible solution is to exploit global knowledge from the related documents and Wikipedia, and related events to recover and predict some implicit time arguments (Filatova and Hovy, 2001; Mani et al., 2003; Mann, 2007; Eidelman, 2008; Gupta and Ji, 2009). 2.2 Coreference Resolution One of the key challenges for information fusion is cross-document entity coreference precise clustering of mentions into correct entities. There are two principal challenges: the same entity can be referred to by more than one name string and the same name string can refer to more than one entity. The recent research has been mainly promoted in the web people search task (Artiles et al., 2007) such as (Balog et al., 2008), ACE2008 such as (Baron and Freedman, 2008) and NIST TAC KBP (McNamee and Da</context>
</contexts>
<marker>Filatova, Hovy, 2001</marker>
<rawString>Elena Filatova and Eduard Hovy. 2001. Assigning Time-Stamps to Event-Clauses. Proc. ACL 2001 Workshop on Temporal and Spatial Information Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>Proc. ACL</booktitle>
<contexts>
<context position="10955" citStr="Finkel et al., 2005" startWordPosition="1727" endWordPosition="1730">the quality of information can often be improved by the fused fact network itself, which can be called as self-boosting of information fusion. For example, if two GPE entities are involved in a conflict-attack event, then they are unlikely to be connected by a part-whole relation; Mahmoud Abbas and Abu Mazen are likely to be coreferential if they get involved in the same life-born event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007) experimented with information fusion of relations across multiple documents. The TextRunner system (Banko et al., 2007) can collapse and compress redundant facts extracted from multiple documents based on coreference resolution (Yates and Etzioni, 2009), semantic similarity computation and normalization. Two relations are central for event fusion: contradiction part of one event mention contradicts part of another, and redundancy part of one event mention conveys the same content as (or is entailed by) part of another. Once these central relations are identified they will provide</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proc. ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gregoire</author>
</authors>
<title>An unbiased approach to iterated fusion by weakening.</title>
<date>2006</date>
<journal>Information Fusion.</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="12992" citStr="Gregoire, 2006" startWordPosition="2035" endWordPosition="2036"> is more challenging than entity coreference because each linking decision needs to be made based upon the overall similarity of the event trigger and multiple arguments. Hasler and Orasan (2009) 509 \x0cfurther found that in many cases even coreferential even arguments are not good indicators for event coreference. Earlier work on event coreference resolution (e.g. Bagga and Baldwin, 1999) was limited to several MUC scenarios. Recent work (Chen et al., 2009) focus on much wider coverage of event types defined in ACE. The methods from the knowledge fusion community (e.g. Appriou et al., 2001; Gregoire, 2006) mostly focus on resolving conflicts rather than identifying them (i.e. inconsistency problem rather than ambiguity). These approaches allow the conflicts to be resolved in a straightforward way but they rely on the availability of meta-data (e.g., distribution of weights between attributes, probability assignment etc.). However, it is not always clear where to get this meta-data. The event attributes such as Modality, Polarity, Genericity and Tense (Sauri et al., 2006) will play an important role in event coreference resolution because two event mentions cannot be coreferential if any of the </context>
<context position="21721" citStr="Gregoire, 2006" startWordPosition="3374" endWordPosition="3375">e.g., Final Senate vote for the reform plans, Obama signs the reform agreement), images (e.g., images about various government involvements over decades) and videos (e.g. Obamas speech video about the decisions) containing additional information regarding the real extent of the event or providing evidence corroborating the text part. Current state-of-the-art information fusion approaches can be divided into two groups: formal top-down methods from the generic knowledge fusion community and quantitative bottom-up techniques from the applied Semantic 511 \x0cWeb community (Appriou et al., 2001; Gregoire, 2006). Both approaches have their limitations. It will be beneficial to combine both types of approaches so that the fusion decision can be made depending on the type of problem and the amount of domain information it possesses. Saggion et al. (2004) described a multimedia extraction approach to create composite index from multiple and multi-lingual sources. Magalhaes et al. (2008) described a semantic similarity metric based on key word vectors for multi-media fusion. Iria and Magalhaes (2009) exploited information across different parts of a multimedia document to improve document classification.</context>
</contexts>
<marker>Gregoire, 2006</marker>
<rawString>E. Gregoire. 2006. An unbiased approach to iterated fusion by weakening. Information Fusion. 7(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prashant Gupta</author>
<author>Heng Ji</author>
</authors>
<title>Predicting Unknown Time Arguments based on Cross-event propagation.</title>
<date>2009</date>
<booktitle>Proc. ACL-IJCNLP</booktitle>
<contexts>
<context position="9766" citStr="Gupta and Ji, 2009" startWordPosition="1529" endWordPosition="1532">edictor of chronological order (only 3% temporal correlation with the true order). Single-document IE technique can identify and normalize event time arguments from the texts, which results in a much better correlation score of 44% (Ji et al., 2009a). But this is still far from the ideal performance for real applications. In order to alleviate this bottleneck, a possible solution is to exploit global knowledge from the related documents and Wikipedia, and related events to recover and predict some implicit time arguments (Filatova and Hovy, 2001; Mani et al., 2003; Mann, 2007; Eidelman, 2008; Gupta and Ji, 2009). 2.2 Coreference Resolution One of the key challenges for information fusion is cross-document entity coreference precise clustering of mentions into correct entities. There are two principal challenges: the same entity can be referred to by more than one name string and the same name string can refer to more than one entity. The recent research has been mainly promoted in the web people search task (Artiles et al., 2007) such as (Balog et al., 2008), ACE2008 such as (Baron and Freedman, 2008) and NIST TAC KBP (McNamee and Dang, 2009) evaluations. Interestingly, the quality of information can</context>
</contexts>
<marker>Gupta, Ji, 2009</marker>
<rawString>Prashant Gupta and Heng Ji. 2009. Predicting Unknown Time Arguments based on Cross-event propagation. Proc. ACL-IJCNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Hakkani-Tur</author>
<author>Frederic Bechet</author>
<author>Giuseppe Riccardi</author>
<author>Gokhan Tur</author>
</authors>
<title>Beyond ASR 1-Best: Using Word Confusion Networks in Spoken Language Understanding.</title>
<date>2006</date>
<journal>Journal of Computer Speech and Language,</journal>
<volume>20</volume>
<pages>495--514</pages>
<contexts>
<context position="27999" citStr="Hakkani-Tur et al., 2006" startWordPosition="4356" endWordPosition="4359">ih, 2004; Ji et al., 2005; McCallum, 2006) can transform the integration of multi-media into a benefit by reducing the errors in individual stages. In doing so, we can take advantage (among other properties) of the coherence of a discourse: that a correct analysis of a text discourse reveals a large number of connections from the image information in its context, and so (in general) a more tightly connected analysis is more likely to be correct. For example, prior work has demonstrated the benefit of jointly modeling name tagging and n-best hypotheses, ASR lattices or word confusion networks (Hakkani-Tur et al., 2006). 5 Conclusion In the current information explosion era, IE technology is facing new challenges of dealing with heterogeneous data sources from different documents, languages and media which may contain a multiplicity of aspects on particular entities, relations and events. This new phenomena requires IE to perform both traditional lower level processing as well as information fusion of factual data based on implicit inferences. This paper investigated the issues of information fusion on a massive scale and the challenges have not been discussed in previous work. We specified the requirements </context>
</contexts>
<marker>Hakkani-Tur, Bechet, Riccardi, Tur, 2006</marker>
<rawString>Dilek Hakkani-Tur, Frederic Bechet, Giuseppe Riccardi, Gokhan Tur. 2006. Beyond ASR 1-Best: Using Word Confusion Networks in Spoken Language Understanding. Journal of Computer Speech and Language, Vol. 20, No. 4, pp. 495-514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Hasler</author>
<author>Constantin Orasan</author>
</authors>
<title>Do coreferential arguments make event mentions coreferential?</title>
<date>2009</date>
<booktitle>Proc. the 7th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC</booktitle>
<contexts>
<context position="12572" citStr="Hasler and Orasan (2009)" startWordPosition="1966" endWordPosition="1969">rtions, discrepancies based on world-knowledge, and lexical contrasts. Ritter et al. (2009) described a contradiction detection method based on functional relations and pointed out that many contradictory fact pairs from the Web appear consistent, and that requires background knowledge to predict. Assessing event coreference is essential: for texts to contradict, they must refer to the same event. Event coreference resolution is more challenging than entity coreference because each linking decision needs to be made based upon the overall similarity of the event trigger and multiple arguments. Hasler and Orasan (2009) 509 \x0cfurther found that in many cases even coreferential even arguments are not good indicators for event coreference. Earlier work on event coreference resolution (e.g. Bagga and Baldwin, 1999) was limited to several MUC scenarios. Recent work (Chen et al., 2009) focus on much wider coverage of event types defined in ACE. The methods from the knowledge fusion community (e.g. Appriou et al., 2001; Gregoire, 2006) mostly focus on resolving conflicts rather than identifying them (i.e. inconsistency problem rather than ambiguity). These approaches allow the conflicts to be resolved in a strai</context>
</contexts>
<marker>Hasler, Orasan, 2009</marker>
<rawString>Laura Hasler and Constantin Orasan. 2009. Do coreferential arguments make event mentions coreferential? Proc. the 7th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jose Iria</author>
<author>Joao Magalhaes</author>
</authors>
<title>Exploiting Cross-Media Correlations in the Categorization of Multimedia Web Documents.</title>
<date>2009</date>
<booktitle>Proc. CIAM</booktitle>
<contexts>
<context position="22215" citStr="Iria and Magalhaes (2009)" startWordPosition="3452" endWordPosition="3455">munity and quantitative bottom-up techniques from the applied Semantic 511 \x0cWeb community (Appriou et al., 2001; Gregoire, 2006). Both approaches have their limitations. It will be beneficial to combine both types of approaches so that the fusion decision can be made depending on the type of problem and the amount of domain information it possesses. Saggion et al. (2004) described a multimedia extraction approach to create composite index from multiple and multi-lingual sources. Magalhaes et al. (2008) described a semantic similarity metric based on key word vectors for multi-media fusion. Iria and Magalhaes (2009) exploited information across different parts of a multimedia document to improve document classification. It is important to go beyond key words and attempt representing the documents by the semantic facts identified by IE. One possible solution is to exploit the linkage information. Specifically, coreference resolution methods should be applied to four types of crossmedia data: (1) between the captions of images and context texts; (2) detecting HTML crossmedia associations and quantifying the level of image and text block correlation (3) between the texts embedded in images and context texts</context>
</contexts>
<marker>Iria, Magalhaes, 2009</marker>
<rawString>Jose Iria and Joao Magalhaes. 2009. Exploiting Cross-Media Correlations in the Categorization of Multimedia Web Documents. Proc. CIAM 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H V Jagadish</author>
<author>Jason Madar</author>
<author>Raymond Ng</author>
</authors>
<title>Semantic compression and pattern extraction with fascicles. VLDB,</title>
<date>1999</date>
<pages>186197</pages>
<contexts>
<context position="1431" citStr="Jagadish et al., 1999" startWordPosition="207" endWordPosition="210">st development of Information Extraction (IE) techniques has made it possible to extract facts (entities, relations and events) from unstructured documents, and converting them into structured representations (e.g. databases). Once the collection grows beyond a certain size, an issue of critical importance is how a user can monitor a compact knowledge base or identify the interesting portions without having to (re) read large amounts of facts. In this situation users are often more concerned with the speed in which they obtain results, rather than obtaining the exact answers to their queries (Jagadish et al., 1999). The facts extracted from heterogeneous data sources (e.g. text, images, speech and videos) must then be integrated in a knowledge base, so that it can be queried in a uniform way. This provides unparalleled challenges and opportunities for improved decision making. Data can be noisy, incorrect, or misleading. Unstructured data, mostly text, is difficult to interpret. In practice it is often the case that there are multiple sources which need to be extracted and compressed. In a large, diverse, and interconnected system, it is difficult to assure accuracy or even coherence among the data sour</context>
</contexts>
<marker>Jagadish, Madar, Ng, 1999</marker>
<rawString>H. V. Jagadish, Jason Madar, and Raymond Ng. 1999. Semantic compression and pattern extraction with fascicles. VLDB, pages 186197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>David Westbrook</author>
<author>Ralph Grishman</author>
</authors>
<title>Using Semantic Relations to Refine Coreference Decisions.</title>
<date>2005</date>
<booktitle>Proc. HLT/EMNLP 05.</booktitle>
<contexts>
<context position="10759" citStr="Ji et al., 2005" startWordPosition="1697" endWordPosition="1700">he web people search task (Artiles et al., 2007) such as (Balog et al., 2008), ACE2008 such as (Baron and Freedman, 2008) and NIST TAC KBP (McNamee and Dang, 2009) evaluations. Interestingly, the quality of information can often be improved by the fused fact network itself, which can be called as self-boosting of information fusion. For example, if two GPE entities are involved in a conflict-attack event, then they are unlikely to be connected by a part-whole relation; Mahmoud Abbas and Abu Mazen are likely to be coreferential if they get involved in the same life-born event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007) experimented with information fusion of relations across multiple documents. The TextRunner system (Banko et al., 2007) can collapse and compress redundant facts extracted from multiple documents based on coreference resolution (Yates and Etzioni, 2009), semantic similarity computation and normalization. Two relations are central for event fusion: contradiction part of one event mention </context>
<context position="27399" citStr="Ji et al., 2005" startWordPosition="4255" endWordPosition="4258">pronoun resolution by exploiting gender and animacy knowledge discovery methods. The processing methods of text and other media are typically organized as a pipeline architecture of processing stages (e.g. from pattern recognition, to information fusion, and to summarization). Each of these stages has been studied separately and quite intensively over the past decade. Its critical to move away from approaches that make chains of independent local decisions, and instead toward methods that make multiple decisions jointly using global information. Joint inference techniques (Roth and Yih, 2004; Ji et al., 2005; McCallum, 2006) can transform the integration of multi-media into a benefit by reducing the errors in individual stages. In doing so, we can take advantage (among other properties) of the coherence of a discourse: that a correct analysis of a text discourse reveals a large number of connections from the image information in its context, and so (in general) a more tightly connected analysis is more likely to be correct. For example, prior work has demonstrated the benefit of jointly modeling name tagging and n-best hypotheses, ASR lattices or word confusion networks (Hakkani-Tur et al., 2006)</context>
</contexts>
<marker>Ji, Westbrook, Grishman, 2005</marker>
<rawString>Heng Ji, David Westbrook and Ralph Grishman. 2005. Using Semantic Relations to Refine Coreference Decisions. Proc. HLT/EMNLP 05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Refining Event Extraction Through Cross-document Inference.</title>
<date>2008</date>
<contexts>
<context position="5813" citStr="Ji and Grishman, 2008" startWordPosition="896" endWordPosition="899">illed many people on a, s attack on a, blew apart a, blew himself up on a, drove his explosives-laden car into a, had rigged the, set off a bomb on a, etc.), but the ACE1 training corpora only cover about 1/3 of these expressions. Several recent studies have stressed the benefits of using information redundancy on estimating the correctness of the IE output (Downey et 1 http://www.itl.nist.gov/iad/mig/tests/ace/ al., 2005), improving disease event extraction (Yangarber, 2006), Message Understanding Conference event extraction (Mann, 2007; Patwardhan and Riloff, 2009) and ACE event extraction (Ji and Grishman, 2008). This approach is based on the premise that many facts will be reported multiple times from different sources in different forms. This may occur both within the same document and within a cluster of topically related and successive documents. Therefore, by aggregating similar facts across documents and conducting statistical global inference by favoring interpretation consistency, enhanced extraction performance can be achieved with heterogeneous data than uniform data. The underlying hypothesis of cross-document inference is that the salience of a fact should be calculated by taking into con</context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>Heng Ji and Ralph Grishman. 2008. Refining Event Extraction Through Cross-document Inference.</rawString>
</citation>
<citation valid="true">
<date>2008</date>
<booktitle>Proc. ACL</booktitle>
<contexts>
<context position="22100" citStr="(2008)" startWordPosition="3437" endWordPosition="3437">es can be divided into two groups: formal top-down methods from the generic knowledge fusion community and quantitative bottom-up techniques from the applied Semantic 511 \x0cWeb community (Appriou et al., 2001; Gregoire, 2006). Both approaches have their limitations. It will be beneficial to combine both types of approaches so that the fusion decision can be made depending on the type of problem and the amount of domain information it possesses. Saggion et al. (2004) described a multimedia extraction approach to create composite index from multiple and multi-lingual sources. Magalhaes et al. (2008) described a semantic similarity metric based on key word vectors for multi-media fusion. Iria and Magalhaes (2009) exploited information across different parts of a multimedia document to improve document classification. It is important to go beyond key words and attempt representing the documents by the semantic facts identified by IE. One possible solution is to exploit the linkage information. Specifically, coreference resolution methods should be applied to four types of crossmedia data: (1) between the captions of images and context texts; (2) detecting HTML crossmedia associations and q</context>
</contexts>
<marker>2008</marker>
<rawString>Proc. ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
</authors>
<title>Mining Name Translations from Comparable Corpora by Creating Bilingual Information Networks.</title>
<date>2009</date>
<booktitle>Proc. ACL-IJCNLP 2009 workshop on Building and Using Comparable Corpora (BUCC</booktitle>
<contexts>
<context position="8071" citStr="Ji, 2009" startWordPosition="1253" endWordPosition="1254">ov Logic Networks (Richardson and Domingos, 2006), a statistical relational learning language, to model these global inference rules more declaratively. Markov Logic will make it possible to compactly specify probability distributions over the complex relational inferences. It can capture non-deterministic (soft) rules that tend to hold among facts but do not have to. Exploiting this approach will also provide greater flexibility to incorporate additional linguistic and world knowledge into inference. 508 \x0cThe information fused across documents can be represented as an information network (Ji, 2009) in which entities can be viewed as vertices on the graph and they can be connected by some type of static relationship (e.g. those attributes defined in NIST TAC-KBP task (McNamee and Dang, 2009)), or as a temporal chain linking dynamic events (e.g. Bethard and Martin, 2008; Chambers and Jurafsky, 2009; Ji et al., 2009a). The latter representation is more attractive because business or international affairs analysts often review many news reports to track people, companies, and government activities and trends. The query logs from the commercial search engines show that there is a fair number</context>
<context position="9766" citStr="Ji, 2009" startWordPosition="1531" endWordPosition="1532"> chronological order (only 3% temporal correlation with the true order). Single-document IE technique can identify and normalize event time arguments from the texts, which results in a much better correlation score of 44% (Ji et al., 2009a). But this is still far from the ideal performance for real applications. In order to alleviate this bottleneck, a possible solution is to exploit global knowledge from the related documents and Wikipedia, and related events to recover and predict some implicit time arguments (Filatova and Hovy, 2001; Mani et al., 2003; Mann, 2007; Eidelman, 2008; Gupta and Ji, 2009). 2.2 Coreference Resolution One of the key challenges for information fusion is cross-document entity coreference precise clustering of mentions into correct entities. There are two principal challenges: the same entity can be referred to by more than one name string and the same name string can refer to more than one entity. The recent research has been mainly promoted in the web people search task (Artiles et al., 2007) such as (Balog et al., 2008), ACE2008 such as (Baron and Freedman, 2008) and NIST TAC KBP (McNamee and Dang, 2009) evaluations. Interestingly, the quality of information can</context>
<context position="15608" citStr="Ji, 2009" startWordPosition="2427" endWordPosition="2428">ge IE on the source language texts, and then use machine translation (MT) word alignments to translate (project) extracted information into target languages. Regardless of the different architectures, both pipelines are facing the following challenges from extraction and translation. 3.1 Extraction Challenges Some recent fusion work focus on cross-lingual interaction and inference to improve both sides synchronously, beyond the parallel comparisons of cross-lingual IE pipelines in (e.g. Riloff et al., 2002). One of such examples is on cross-lingual co-training (e.g. Cao et al., 2003; Chen and Ji, 2009). In co-training (Blum and Mitchell, 1998), the uncertainty of a classifier is defined as the portion of instances on which it cannot make classification decisions. Exchanging tagged data in bootstrapping can help reduce the uncertainties of classifiers. The cross-lingual fusion process satisfies the co-training algorithms assumptions about two views (in this case, two languages): (1) the two views are individually sufficient for classification (IE systems in both languages were learned from annotated corpora which are enough for reasonable extraction performance); (2) the two views are condit</context>
<context position="17949" citStr="Ji, 2009" startWordPosition="2784" endWordPosition="2785">of extracted facts such as names and events. IE tasks performed notably worse on machine translated texts than on texts originally written in English, and error analysis indicated that a major cause was the low quality of name translation (Ji et al., 2009b). Traditional MT systems focus on the overall fluency and accuracy of the translation but fall short in their ability to translate certain informationally critical words. In particular, it appears that better entity name translation can substantially improve cross-lingual information fusion. Some recent work (e.g. Klementiev and Roth, 2006; Ji, 2009) has exploited comparable corpora to enhance information translation. There are no document-level or sentence-level alignments across languages, but important facts such as names, relations and events in one language in such corpora tend to co-occur with their counterparts in the other. (Ji, 2009) used a bootstrapping approach to align the information networks from bilingual comparable corpora, and discover name translations and extract relations links simultaneously. The general idea is to start from a small seed set of common name pairs, and then rely on the link attributes to align their re</context>
</contexts>
<marker>Ji, 2009</marker>
<rawString>Heng Ji. 2009. Mining Name Translations from Comparable Corpora by Creating Bilingual Information Networks. Proc. ACL-IJCNLP 2009 workshop on Building and Using Comparable Corpora (BUCC 2009): from parallel to nonparallel corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
<author>Dayne Freitag</author>
<author>Matthias Blume</author>
<author>John Wang</author>
<author>Shahram Khadivi</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Name Transla\x0ction for Distillation. Book chapter for Global Automatic Language Exploitation.</title>
<date>2009</date>
<contexts>
<context position="8392" citStr="Ji et al., 2009" startWordPosition="1307" endWordPosition="1310">rules that tend to hold among facts but do not have to. Exploiting this approach will also provide greater flexibility to incorporate additional linguistic and world knowledge into inference. 508 \x0cThe information fused across documents can be represented as an information network (Ji, 2009) in which entities can be viewed as vertices on the graph and they can be connected by some type of static relationship (e.g. those attributes defined in NIST TAC-KBP task (McNamee and Dang, 2009)), or as a temporal chain linking dynamic events (e.g. Bethard and Martin, 2008; Chambers and Jurafsky, 2009; Ji et al., 2009a). The latter representation is more attractive because business or international affairs analysts often review many news reports to track people, companies, and government activities and trends. The query logs from the commercial search engines show that there is a fair number of news related queries (Mishne &amp; de Rijke, 2006), suggesting that blog search users have an interest in the blogosphere response to news stories as they develop. For example, (Ji et al., 2009a) extracted centroid entities and then linked events centered around the same centroid entities on a time line. Temporal orderi</context>
<context position="17595" citStr="Ji et al., 2009" startWordPosition="2730" endWordPosition="2733">egated from multiple languages, the translation errors will bring us great challenges. However, in order to extend 510 \x0ccross-lingual information fusion techniques to more language pairs, we can start from the much more scalable task of information translation (Etzioni et al., 2007). The additional processing may take the form of machine translation (MT) of extracted facts such as names and events. IE tasks performed notably worse on machine translated texts than on texts originally written in English, and error analysis indicated that a major cause was the low quality of name translation (Ji et al., 2009b). Traditional MT systems focus on the overall fluency and accuracy of the translation but fall short in their ability to translate certain informationally critical words. In particular, it appears that better entity name translation can substantially improve cross-lingual information fusion. Some recent work (e.g. Klementiev and Roth, 2006; Ji, 2009) has exploited comparable corpora to enhance information translation. There are no document-level or sentence-level alignments across languages, but important facts such as names, relations and events in one language in such corpora tend to co-oc</context>
</contexts>
<marker>Ji, Grishman, Freitag, Blume, Wang, Khadivi, Zens, Ney, 2009</marker>
<rawString>Heng Ji, Ralph Grishman, Dayne Freitag, Matthias Blume, John Wang, Shahram Khadivi, Richard Zens and Hermann Ney. 2009a. Name Transla\x0ction for Distillation. Book chapter for Global Automatic Language Exploitation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
<author>Zheng Chen</author>
<author>Prashant Gupta</author>
</authors>
<title>Cross-document Event Extraction, Ranking and Tracking.</title>
<date>2009</date>
<booktitle>Proc. RANLP</booktitle>
<contexts>
<context position="8392" citStr="Ji et al., 2009" startWordPosition="1307" endWordPosition="1310">rules that tend to hold among facts but do not have to. Exploiting this approach will also provide greater flexibility to incorporate additional linguistic and world knowledge into inference. 508 \x0cThe information fused across documents can be represented as an information network (Ji, 2009) in which entities can be viewed as vertices on the graph and they can be connected by some type of static relationship (e.g. those attributes defined in NIST TAC-KBP task (McNamee and Dang, 2009)), or as a temporal chain linking dynamic events (e.g. Bethard and Martin, 2008; Chambers and Jurafsky, 2009; Ji et al., 2009a). The latter representation is more attractive because business or international affairs analysts often review many news reports to track people, companies, and government activities and trends. The query logs from the commercial search engines show that there is a fair number of news related queries (Mishne &amp; de Rijke, 2006), suggesting that blog search users have an interest in the blogosphere response to news stories as they develop. For example, (Ji et al., 2009a) extracted centroid entities and then linked events centered around the same centroid entities on a time line. Temporal orderi</context>
<context position="17595" citStr="Ji et al., 2009" startWordPosition="2730" endWordPosition="2733">egated from multiple languages, the translation errors will bring us great challenges. However, in order to extend 510 \x0ccross-lingual information fusion techniques to more language pairs, we can start from the much more scalable task of information translation (Etzioni et al., 2007). The additional processing may take the form of machine translation (MT) of extracted facts such as names and events. IE tasks performed notably worse on machine translated texts than on texts originally written in English, and error analysis indicated that a major cause was the low quality of name translation (Ji et al., 2009b). Traditional MT systems focus on the overall fluency and accuracy of the translation but fall short in their ability to translate certain informationally critical words. In particular, it appears that better entity name translation can substantially improve cross-lingual information fusion. Some recent work (e.g. Klementiev and Roth, 2006; Ji, 2009) has exploited comparable corpora to enhance information translation. There are no document-level or sentence-level alignments across languages, but important facts such as names, relations and events in one language in such corpora tend to co-oc</context>
</contexts>
<marker>Ji, Grishman, Chen, Gupta, 2009</marker>
<rawString>Heng Ji, Ralph Grishman, Zheng Chen and Prashant Gupta. 2009b. Cross-document Event Extraction, Ranking and Tracking. Proc. RANLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>Extracting Social Networks and Biographical Facts From Conversational Speech Transcripts.</title>
<date>2007</date>
<booktitle>Proc. ACL</booktitle>
<contexts>
<context position="10779" citStr="Jing et al., 2007" startWordPosition="1701" endWordPosition="1704">rch task (Artiles et al., 2007) such as (Balog et al., 2008), ACE2008 such as (Baron and Freedman, 2008) and NIST TAC KBP (McNamee and Dang, 2009) evaluations. Interestingly, the quality of information can often be improved by the fused fact network itself, which can be called as self-boosting of information fusion. For example, if two GPE entities are involved in a conflict-attack event, then they are unlikely to be connected by a part-whole relation; Mahmoud Abbas and Abu Mazen are likely to be coreferential if they get involved in the same life-born event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007) experimented with information fusion of relations across multiple documents. The TextRunner system (Banko et al., 2007) can collapse and compress redundant facts extracted from multiple documents based on coreference resolution (Yates and Etzioni, 2009), semantic similarity computation and normalization. Two relations are central for event fusion: contradiction part of one event mention contradicts part of </context>
<context position="26540" citStr="Jing et al., 2007" startWordPosition="4120" endWordPosition="4123">ile dealing with speech input, IE systems need to be robust to the noise introduced by earlier speech processing tasks such as ASR, sentence segmentation, salience detection and and speaker identification. Some earlier work (Makhoul et al., 2005; Favre et al., 2008) 512 \x0cshowed that using an IE system trained from newswire, the performance degrades notably when the system is tested on automatic speech recognition output. But no general solutions have been proposed to address the genre-specific challenges for speech data. More specifically, pronoun resolution is one of the major challenges (Jing et al., 2007). For example, in wikipedia a lot of pronouns may refer to the entry entity; while in speech conversation we will need to resolve first and second person pronouns based on automatic speaker role identification; and improve cross-sentence third pronoun resolution by exploiting gender and animacy knowledge discovery methods. The processing methods of text and other media are typically organized as a pipeline architecture of processing stages (e.g. from pattern recognition, to information fusion, and to summarization). Each of these stages has been studied separately and quite intensively over th</context>
</contexts>
<marker>Jing, Kambhatla, Roukos, 2007</marker>
<rawString>Hongyan Jing, Nanda Kambhatla and Salim Roukos. 2007. Extracting Social Networks and Biographical Facts From Conversational Speech Transcripts. Proc. ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Klementiev</author>
<author>D Roth</author>
</authors>
<date>2006</date>
<booktitle>Named Entity Transliteration and Discovery from Multilingual Comparable Corpora. Proc. HLT-NAACL</booktitle>
<contexts>
<context position="17938" citStr="Klementiev and Roth, 2006" startWordPosition="2780" endWordPosition="2783">f machine translation (MT) of extracted facts such as names and events. IE tasks performed notably worse on machine translated texts than on texts originally written in English, and error analysis indicated that a major cause was the low quality of name translation (Ji et al., 2009b). Traditional MT systems focus on the overall fluency and accuracy of the translation but fall short in their ability to translate certain informationally critical words. In particular, it appears that better entity name translation can substantially improve cross-lingual information fusion. Some recent work (e.g. Klementiev and Roth, 2006; Ji, 2009) has exploited comparable corpora to enhance information translation. There are no document-level or sentence-level alignments across languages, but important facts such as names, relations and events in one language in such corpora tend to co-occur with their counterparts in the other. (Ji, 2009) used a bootstrapping approach to align the information networks from bilingual comparable corpora, and discover name translations and extract relations links simultaneously. The general idea is to start from a small seed set of common name pairs, and then rely on the link attributes to ali</context>
</contexts>
<marker>Klementiev, Roth, 2006</marker>
<rawString>A. Klementiev and D. Roth. 2006. Named Entity Transliteration and Discovery from Multilingual Comparable Corpora. Proc. HLT-NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joao Magalhaes</author>
<author>Fabio Ciravegna</author>
<author>Stefan Ruger</author>
</authors>
<title>Exploring Multimedia in a Keyword Space.</title>
<date>2008</date>
<booktitle>Proc. ACM Multimedia.</booktitle>
<contexts>
<context position="22100" citStr="Magalhaes et al. (2008)" startWordPosition="3434" endWordPosition="3437">n fusion approaches can be divided into two groups: formal top-down methods from the generic knowledge fusion community and quantitative bottom-up techniques from the applied Semantic 511 \x0cWeb community (Appriou et al., 2001; Gregoire, 2006). Both approaches have their limitations. It will be beneficial to combine both types of approaches so that the fusion decision can be made depending on the type of problem and the amount of domain information it possesses. Saggion et al. (2004) described a multimedia extraction approach to create composite index from multiple and multi-lingual sources. Magalhaes et al. (2008) described a semantic similarity metric based on key word vectors for multi-media fusion. Iria and Magalhaes (2009) exploited information across different parts of a multimedia document to improve document classification. It is important to go beyond key words and attempt representing the documents by the semantic facts identified by IE. One possible solution is to exploit the linkage information. Specifically, coreference resolution methods should be applied to four types of crossmedia data: (1) between the captions of images and context texts; (2) detecting HTML crossmedia associations and q</context>
</contexts>
<marker>Magalhaes, Ciravegna, Ruger, 2008</marker>
<rawString>Joao Magalhaes, Fabio Ciravegna and Stefan Ruger. 2008. Exploring Multimedia in a Keyword Space. Proc. ACM Multimedia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Barry Schiffman</author>
<author>Jianping Zhang</author>
</authors>
<title>Inferring Temporal Ordering of Events in News.</title>
<date>2003</date>
<booktitle>Proc. HLT-NAACL</booktitle>
<contexts>
<context position="9717" citStr="Mani et al., 2003" startWordPosition="1521" endWordPosition="1524">rguments. The text order by itself is a poor predictor of chronological order (only 3% temporal correlation with the true order). Single-document IE technique can identify and normalize event time arguments from the texts, which results in a much better correlation score of 44% (Ji et al., 2009a). But this is still far from the ideal performance for real applications. In order to alleviate this bottleneck, a possible solution is to exploit global knowledge from the related documents and Wikipedia, and related events to recover and predict some implicit time arguments (Filatova and Hovy, 2001; Mani et al., 2003; Mann, 2007; Eidelman, 2008; Gupta and Ji, 2009). 2.2 Coreference Resolution One of the key challenges for information fusion is cross-document entity coreference precise clustering of mentions into correct entities. There are two principal challenges: the same entity can be referred to by more than one name string and the same name string can refer to more than one entity. The recent research has been mainly promoted in the web people search task (Artiles et al., 2007) such as (Balog et al., 2008), ACE2008 such as (Baron and Freedman, 2008) and NIST TAC KBP (McNamee and Dang, 2009) evaluatio</context>
</contexts>
<marker>Mani, Schiffman, Zhang, 2003</marker>
<rawString>Inderjeet Mani, Barry Schiffman and Jianping Zhang. 2003. Inferring Temporal Ordering of Events in News. Proc. HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Makhoul</author>
<author>Alex Baron</author>
<author>Ivan Bulyko</author>
<author>Long Nguyen</author>
<author>Lance Ramshaw</author>
<author>David Stallard</author>
<author>Richard Schwartz</author>
<author>Bing Xiang</author>
</authors>
<title>The Effects of Speech Recognition and Punctuation on Information Extraction Performance.</title>
<date>2005</date>
<tech>Proc. Interspeech.</tech>
<contexts>
<context position="26167" citStr="Makhoul et al., 2005" startWordPosition="4063" endWordPosition="4066">n into two types: (1) radical adaptation such as from newswire to biomedical articles; (2) modest adaptation such as from newswire to wikipedia or automatic speech recognition (ASR) output. (1) requires a great deal of new development such as ontology definition and data annotation; while (2) can be partially addressed during the information fusion process. For example, while dealing with speech input, IE systems need to be robust to the noise introduced by earlier speech processing tasks such as ASR, sentence segmentation, salience detection and and speaker identification. Some earlier work (Makhoul et al., 2005; Favre et al., 2008) 512 \x0cshowed that using an IE system trained from newswire, the performance degrades notably when the system is tested on automatic speech recognition output. But no general solutions have been proposed to address the genre-specific challenges for speech data. More specifically, pronoun resolution is one of the major challenges (Jing et al., 2007). For example, in wikipedia a lot of pronouns may refer to the entry entity; while in speech conversation we will need to resolve first and second person pronouns based on automatic speaker role identification; and improve cros</context>
</contexts>
<marker>Makhoul, Baron, Bulyko, Nguyen, Ramshaw, Stallard, Schwartz, Xiang, 2005</marker>
<rawString>John Makhoul, Alex Baron, Ivan Bulyko, Long Nguyen, Lance Ramshaw, David Stallard, Richard Schwartz and Bing Xiang. 2005. The Effects of Speech Recognition and Punctuation on Information Extraction Performance. Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Mann</author>
</authors>
<title>Multi-document Relationship Fusion via Constraints on Probabilistic Databases.</title>
<date>2007</date>
<booktitle>Proc. HLT/NAACL</booktitle>
<contexts>
<context position="5734" citStr="Mann, 2007" startWordPosition="884" endWordPosition="885">a bus, there are more than 50 different intervening strings (e.g. killed many people on a, s attack on a, blew apart a, blew himself up on a, drove his explosives-laden car into a, had rigged the, set off a bomb on a, etc.), but the ACE1 training corpora only cover about 1/3 of these expressions. Several recent studies have stressed the benefits of using information redundancy on estimating the correctness of the IE output (Downey et 1 http://www.itl.nist.gov/iad/mig/tests/ace/ al., 2005), improving disease event extraction (Yangarber, 2006), Message Understanding Conference event extraction (Mann, 2007; Patwardhan and Riloff, 2009) and ACE event extraction (Ji and Grishman, 2008). This approach is based on the premise that many facts will be reported multiple times from different sources in different forms. This may occur both within the same document and within a cluster of topically related and successive documents. Therefore, by aggregating similar facts across documents and conducting statistical global inference by favoring interpretation consistency, enhanced extraction performance can be achieved with heterogeneous data than uniform data. The underlying hypothesis of cross-document i</context>
<context position="9729" citStr="Mann, 2007" startWordPosition="1525" endWordPosition="1526">order by itself is a poor predictor of chronological order (only 3% temporal correlation with the true order). Single-document IE technique can identify and normalize event time arguments from the texts, which results in a much better correlation score of 44% (Ji et al., 2009a). But this is still far from the ideal performance for real applications. In order to alleviate this bottleneck, a possible solution is to exploit global knowledge from the related documents and Wikipedia, and related events to recover and predict some implicit time arguments (Filatova and Hovy, 2001; Mani et al., 2003; Mann, 2007; Eidelman, 2008; Gupta and Ji, 2009). 2.2 Coreference Resolution One of the key challenges for information fusion is cross-document entity coreference precise clustering of mentions into correct entities. There are two principal challenges: the same entity can be referred to by more than one name string and the same name string can refer to more than one entity. The recent research has been mainly promoted in the web people search task (Artiles et al., 2007) such as (Balog et al., 2008), ACE2008 such as (Baron and Freedman, 2008) and NIST TAC KBP (McNamee and Dang, 2009) evaluations. Interest</context>
<context position="10968" citStr="Mann, 2007" startWordPosition="1731" endWordPosition="1732">ation can often be improved by the fused fact network itself, which can be called as self-boosting of information fusion. For example, if two GPE entities are involved in a conflict-attack event, then they are unlikely to be connected by a part-whole relation; Mahmoud Abbas and Abu Mazen are likely to be coreferential if they get involved in the same life-born event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007) experimented with information fusion of relations across multiple documents. The TextRunner system (Banko et al., 2007) can collapse and compress redundant facts extracted from multiple documents based on coreference resolution (Yates and Etzioni, 2009), semantic similarity computation and normalization. Two relations are central for event fusion: contradiction part of one event mention contradicts part of another, and redundancy part of one event mention conveys the same content as (or is entailed by) part of another. Once these central relations are identified they will provide a basis for </context>
</contexts>
<marker>Mann, 2007</marker>
<rawString>Gideon Mann. 2007. Multi-document Relationship Fusion via Constraints on Probabilistic Databases. Proc. HLT/NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Information Extraction, Data Mining and Joint Inference.</title>
<date>2006</date>
<booktitle>Proc. SIGKDD.</booktitle>
<contexts>
<context position="27416" citStr="McCallum, 2006" startWordPosition="4259" endWordPosition="4260">n by exploiting gender and animacy knowledge discovery methods. The processing methods of text and other media are typically organized as a pipeline architecture of processing stages (e.g. from pattern recognition, to information fusion, and to summarization). Each of these stages has been studied separately and quite intensively over the past decade. Its critical to move away from approaches that make chains of independent local decisions, and instead toward methods that make multiple decisions jointly using global information. Joint inference techniques (Roth and Yih, 2004; Ji et al., 2005; McCallum, 2006) can transform the integration of multi-media into a benefit by reducing the errors in individual stages. In doing so, we can take advantage (among other properties) of the coherence of a discourse: that a correct analysis of a text discourse reveals a large number of connections from the image information in its context, and so (in general) a more tightly connected analysis is more likely to be correct. For example, prior work has demonstrated the benefit of jointly modeling name tagging and n-best hypotheses, ASR lattices or word confusion networks (Hakkani-Tur et al., 2006). 5 Conclusion In</context>
</contexts>
<marker>McCallum, 2006</marker>
<rawString>Andrew McCallum. 2006. Information Extraction, Data Mining and Joint Inference. Proc. SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Hoa Dang</author>
</authors>
<title>Knowledge Base Population Track.</title>
<date>2009</date>
<journal>Overview of the TAC</journal>
<booktitle>Proc. TAC</booktitle>
<note>Workshop.</note>
<contexts>
<context position="8267" citStr="McNamee and Dang, 2009" startWordPosition="1285" endWordPosition="1288">sible to compactly specify probability distributions over the complex relational inferences. It can capture non-deterministic (soft) rules that tend to hold among facts but do not have to. Exploiting this approach will also provide greater flexibility to incorporate additional linguistic and world knowledge into inference. 508 \x0cThe information fused across documents can be represented as an information network (Ji, 2009) in which entities can be viewed as vertices on the graph and they can be connected by some type of static relationship (e.g. those attributes defined in NIST TAC-KBP task (McNamee and Dang, 2009)), or as a temporal chain linking dynamic events (e.g. Bethard and Martin, 2008; Chambers and Jurafsky, 2009; Ji et al., 2009a). The latter representation is more attractive because business or international affairs analysts often review many news reports to track people, companies, and government activities and trends. The query logs from the commercial search engines show that there is a fair number of news related queries (Mishne &amp; de Rijke, 2006), suggesting that blog search users have an interest in the blogosphere response to news stories as they develop. For example, (Ji et al., 2009a) </context>
<context position="10307" citStr="McNamee and Dang, 2009" startWordPosition="1619" endWordPosition="1622">and Hovy, 2001; Mani et al., 2003; Mann, 2007; Eidelman, 2008; Gupta and Ji, 2009). 2.2 Coreference Resolution One of the key challenges for information fusion is cross-document entity coreference precise clustering of mentions into correct entities. There are two principal challenges: the same entity can be referred to by more than one name string and the same name string can refer to more than one entity. The recent research has been mainly promoted in the web people search task (Artiles et al., 2007) such as (Balog et al., 2008), ACE2008 such as (Baron and Freedman, 2008) and NIST TAC KBP (McNamee and Dang, 2009) evaluations. Interestingly, the quality of information can often be improved by the fused fact network itself, which can be called as self-boosting of information fusion. For example, if two GPE entities are involved in a conflict-attack event, then they are unlikely to be connected by a part-whole relation; Mahmoud Abbas and Abu Mazen are likely to be coreferential if they get involved in the same life-born event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005</context>
</contexts>
<marker>McNamee, Dang, 2009</marker>
<rawString>Paul McNamee and Hoa Dang. 2009. Overview of the TAC 2009 Knowledge Base Population Track. Proc. TAC 2009 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilad Mishne</author>
<author>Maarten de Rijke</author>
</authors>
<title>Capturing Global Mood Levels using Blog Posts.</title>
<date>2006</date>
<booktitle>Proc. AAAI</booktitle>
<marker>Mishne, de Rijke, 2006</marker>
<rawString>Gilad Mishne and Maarten de Rijke. 2006. Capturing Global Mood Levels using Blog Posts. Proc. AAAI 2006 Spring Symposium on Computational Approaches to Analysing Weblogs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Graph-Cut-Based Anaphoricity Determination for Coreference Resolution.</title>
<date>2009</date>
<publisher>Proc.</publisher>
<contexts>
<context position="13982" citStr="Ng, 2009" startWordPosition="2188" endWordPosition="2189">. The event attributes such as Modality, Polarity, Genericity and Tense (Sauri et al., 2006) will play an important role in event coreference resolution because two event mentions cannot be coreferential if any of the attributes conflict with each other. Such attempts have been largely neglected in the prior research due to the low weights of attribute labeling in the ACE scoring metric. (Chen et al., 2009) demonstrated that simple automatic event attribute labeling can significantly improve event coreference resolution. In addition, some very recent work including (Nicolae and Nicolae, 2006; Ng, 2009; Chen et al., 2009) found that graph-cut based clustering can improve coreference resolution. The challenge lies in computing the affinity matrix. 3 Cross-Lingual Information Fusion Cross-lingual comparable corpora are also prevalent now because almost all the influential events can be reported in multi-languages at the first time, but probably in different aspects. Therefore, linked fact networks can be constructed and lots of research tasks can benefit from such structures. Since the two networks are similar in structure but not homogeneous, we can do alignment and translation which may adv</context>
</contexts>
<marker>Ng, 2009</marker>
<rawString>Vincent Ng. 2009. Graph-Cut-Based Anaphoricity Determination for Coreference Resolution. Proc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>HLT-NAACL</author>
</authors>
<date>2009</date>
<marker>HLT-NAACL, 2009</marker>
<rawString>HLT-NAACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The PageRank Citation Ranking: Bringing Order to the Web.</title>
<date>1998</date>
<booktitle>Proc. WWW.</booktitle>
<contexts>
<context position="6544" citStr="Page et al., 1998" startWordPosition="1007" endWordPosition="1010"> different forms. This may occur both within the same document and within a cluster of topically related and successive documents. Therefore, by aggregating similar facts across documents and conducting statistical global inference by favoring interpretation consistency, enhanced extraction performance can be achieved with heterogeneous data than uniform data. The underlying hypothesis of cross-document inference is that the salience of a fact should be calculated by taking into consideration both its confidence and the confidence of other facts connected to it, which is inspired by PageRank (Page et al., 1998) and LexRank (Erkan and Radev, 2004). For example, a vote by linked entities which are highly voted on by other entities is more valuable than a vote from unlinked entities. There are two major heuristics: (1) an assertion that several information providers agree on is usually more trustable than that only one provider suggests; and (2) an information provider is trustworthy if it provides many pieces of true information, and a piece of information is likely to be true if it is provided by many trustworthy providers. (Yin et al., 2008) used the above heuristics in a progressive, iterative enha</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1998</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani and Terry Winograd. 1998. The PageRank Citation Ranking: Bringing Order to the Web. Proc. WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>A Unified Model of Phrasal and Sentential Evidence for Information Extraction.</title>
<date>2009</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="5764" citStr="Patwardhan and Riloff, 2009" startWordPosition="886" endWordPosition="890"> are more than 50 different intervening strings (e.g. killed many people on a, s attack on a, blew apart a, blew himself up on a, drove his explosives-laden car into a, had rigged the, set off a bomb on a, etc.), but the ACE1 training corpora only cover about 1/3 of these expressions. Several recent studies have stressed the benefits of using information redundancy on estimating the correctness of the IE output (Downey et 1 http://www.itl.nist.gov/iad/mig/tests/ace/ al., 2005), improving disease event extraction (Yangarber, 2006), Message Understanding Conference event extraction (Mann, 2007; Patwardhan and Riloff, 2009) and ACE event extraction (Ji and Grishman, 2008). This approach is based on the premise that many facts will be reported multiple times from different sources in different forms. This may occur both within the same document and within a cluster of topically related and successive documents. Therefore, by aggregating similar facts across documents and conducting statistical global inference by favoring interpretation consistency, enhanced extraction performance can be achieved with heterogeneous data than uniform data. The underlying hypothesis of cross-document inference is that the salience </context>
</contexts>
<marker>Patwardhan, Riloff, 2009</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2009. A Unified Model of Phrasal and Sentential Evidence for Information Extraction. 2009. Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov Logic Networks.</title>
<date>2006</date>
<journal>Machine Learning.</journal>
<pages>62--107</pages>
<contexts>
<context position="7511" citStr="Richardson and Domingos, 2006" startWordPosition="1171" endWordPosition="1174">) an information provider is trustworthy if it provides many pieces of true information, and a piece of information is likely to be true if it is provided by many trustworthy providers. (Yin et al., 2008) used the above heuristics in a progressive, iterative enhancement process for information fusion. The results from the previous work are promising, but the heuristic inferences are highly dependent on the order of applying rules, and the performance may have been limited by the thresholds which may overfit a small development corpus. One promising method might be using Markov Logic Networks (Richardson and Domingos, 2006), a statistical relational learning language, to model these global inference rules more declaratively. Markov Logic will make it possible to compactly specify probability distributions over the complex relational inferences. It can capture non-deterministic (soft) rules that tend to hold among facts but do not have to. Exploiting this approach will also provide greater flexibility to incorporate additional linguistic and world knowledge into inference. 508 \x0cThe information fused across documents can be represented as an information network (Ji, 2009) in which entities can be viewed as vert</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matt Richardson and Pedro Domingos. 2006. Markov Logic Networks. Machine Learning. 62:107-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Inducing Information Extraction Systems for New Languages via Cross-Language Projection.</title>
<date>2002</date>
<booktitle>Proc. COLING</booktitle>
<contexts>
<context position="15511" citStr="Riloff et al., 2002" startWordPosition="2408" endWordPosition="2411">e texts into target language, and then run target language IE on the translated texts. (2) Run source language IE on the source language texts, and then use machine translation (MT) word alignments to translate (project) extracted information into target languages. Regardless of the different architectures, both pipelines are facing the following challenges from extraction and translation. 3.1 Extraction Challenges Some recent fusion work focus on cross-lingual interaction and inference to improve both sides synchronously, beyond the parallel comparisons of cross-lingual IE pipelines in (e.g. Riloff et al., 2002). One of such examples is on cross-lingual co-training (e.g. Cao et al., 2003; Chen and Ji, 2009). In co-training (Blum and Mitchell, 1998), the uncertainty of a classifier is defined as the portion of instances on which it cannot make classification decisions. Exchanging tagged data in bootstrapping can help reduce the uncertainties of classifiers. The cross-lingual fusion process satisfies the co-training algorithms assumptions about two views (in this case, two languages): (1) the two views are individually sufficient for classification (IE systems in both languages were learned from annota</context>
</contexts>
<marker>Riloff, Schafer, Yarowsky, 2002</marker>
<rawString>Ellen Riloff, Charles Schafer, and David Yarowsky. 2002. Inducing Information Extraction Systems for New Languages via Cross-Language Projection. Proc. COLING 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Stephen Soderland</author>
<author>Doug Downey</author>
<author>Oren Etzioni</author>
</authors>
<title>Its a Contradiction no, its not: A Case Study using Functional Relations.</title>
<date>2009</date>
<publisher>Proc.</publisher>
<contexts>
<context position="12039" citStr="Ritter et al. (2009)" startWordPosition="1886" endWordPosition="1889">t mention conveys the same content as (or is entailed by) part of another. Once these central relations are identified they will provide a basis for identifying more complex relations such as elaboration, presupposition or consequence. It is important to note that redundancy and contradiction among event mentions are logical relations that are not captured by traditional topic-based techniques for similarity detection (e.g. Brants and Stolle, 2002). Contradictions also arise from complex differences in the structure of assertions, discrepancies based on world-knowledge, and lexical contrasts. Ritter et al. (2009) described a contradiction detection method based on functional relations and pointed out that many contradictory fact pairs from the Web appear consistent, and that requires background knowledge to predict. Assessing event coreference is essential: for texts to contradict, they must refer to the same event. Event coreference resolution is more challenging than entity coreference because each linking decision needs to be made based upon the overall similarity of the event trigger and multiple arguments. Hasler and Orasan (2009) 509 \x0cfurther found that in many cases even coreferential even a</context>
</contexts>
<marker>Ritter, Soderland, Downey, Etzioni, 2009</marker>
<rawString>Alan Ritter; Stephen Soderland; Doug Downey; Oren Etzioni. 2009. Its a Contradiction no, its not: A Case Study using Functional Relations. Proc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>EMNLP</author>
</authors>
<date>2009</date>
<marker>EMNLP, 2009</marker>
<rawString>EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A Linear Programming Formulation for Global Inference in Natural Language Tasks.</title>
<date>2004</date>
<booktitle>Proc. CONLL2004.</booktitle>
<contexts>
<context position="27382" citStr="Roth and Yih, 2004" startWordPosition="4251" endWordPosition="4254">ross-sentence third pronoun resolution by exploiting gender and animacy knowledge discovery methods. The processing methods of text and other media are typically organized as a pipeline architecture of processing stages (e.g. from pattern recognition, to information fusion, and to summarization). Each of these stages has been studied separately and quite intensively over the past decade. Its critical to move away from approaches that make chains of independent local decisions, and instead toward methods that make multiple decisions jointly using global information. Joint inference techniques (Roth and Yih, 2004; Ji et al., 2005; McCallum, 2006) can transform the integration of multi-media into a benefit by reducing the errors in individual stages. In doing so, we can take advantage (among other properties) of the coherence of a discourse: that a correct analysis of a text discourse reveals a large number of connections from the image information in its context, and so (in general) a more tightly connected analysis is more likely to be correct. For example, prior work has demonstrated the benefit of jointly modeling name tagging and n-best hypotheses, ASR lattices or word confusion networks (Hakkani-</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-tau Yih. 2004. A Linear Programming Formulation for Global Inference in Natural Language Tasks. Proc. CONLL2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>H Cunningham</author>
<author>K Bontcheva</author>
<author>D Maynard</author>
<author>O Hamza</author>
<author>Y Wilks</author>
</authors>
<title>Multimedia indexing through multi-source and multilanguage information extraction: the MUMIS project.</title>
<date>2004</date>
<journal>Data Knowlege Engineering,</journal>
<volume>48</volume>
<pages>247--264</pages>
<contexts>
<context position="21966" citStr="Saggion et al. (2004)" startWordPosition="3414" endWordPosition="3418">mation regarding the real extent of the event or providing evidence corroborating the text part. Current state-of-the-art information fusion approaches can be divided into two groups: formal top-down methods from the generic knowledge fusion community and quantitative bottom-up techniques from the applied Semantic 511 \x0cWeb community (Appriou et al., 2001; Gregoire, 2006). Both approaches have their limitations. It will be beneficial to combine both types of approaches so that the fusion decision can be made depending on the type of problem and the amount of domain information it possesses. Saggion et al. (2004) described a multimedia extraction approach to create composite index from multiple and multi-lingual sources. Magalhaes et al. (2008) described a semantic similarity metric based on key word vectors for multi-media fusion. Iria and Magalhaes (2009) exploited information across different parts of a multimedia document to improve document classification. It is important to go beyond key words and attempt representing the documents by the semantic facts identified by IE. One possible solution is to exploit the linkage information. Specifically, coreference resolution methods should be applied to</context>
</contexts>
<marker>Saggion, Cunningham, Bontcheva, Maynard, Hamza, Wilks, 2004</marker>
<rawString>Saggion, H., Cunningham, H., Bontcheva, K., Maynard, D., Hamza, O., and Wilks, Y. 2004. Multimedia indexing through multi-source and multilanguage information extraction: the MUMIS project. Data Knowlege Engineering, 48, 2, pp. 247-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Saur</author>
<author>Marc Verhagen</author>
<author>James Pustejovsky</author>
</authors>
<title>Annotating and Recognizing Event Modality in Text.</title>
<date>2006</date>
<booktitle>Proc. FLAIRS</booktitle>
<marker>Saur, Verhagen, Pustejovsky, 2006</marker>
<rawString>Roser Saur and Marc Verhagen and James Pustejovsky. 2006. Annotating and Recognizing Event Modality in Text. Proc. FLAIRS 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Len Seligman</author>
<author>Peter Mork</author>
<author>Alon Halevy</author>
<author>Ken Smith</author>
<author>Michael J Carey</author>
<author>Kuang Chen</author>
<author>Chris Wolf</author>
<author>Jayant Madhavan</author>
<author>Akshay Kannan</author>
</authors>
<date>2010</date>
<contexts>
<context position="2552" citStr="Seligman et al., 2010" startWordPosition="394" endWordPosition="397">, and interconnected system, it is difficult to assure accuracy or even coherence among the data sources. In this environment, traditional IE would be of little value. Most current IE systems focus on processing a single document and language, and are customized for a single data modality. In addition, automatic IE systems are far from perfect and tend to produce errors. Achieving really advances in IE requires that we take a broader view, one that looks outside a single source. We feel the time is now ripe to incorporate some information integration techniques in the database community (e.g. Seligman et al., 2010) to extend the IE paradigm to realtime information fusion and raise IE to a higher level of performance and portability. This requires us to work on a more challenging problem of information fusion - to remove redundancy, resolve contradictions and uncertainties by multiple information providers and design a general framework for the veracity analysis problem. The goal of this paper is to lay out the current status and potential challenges of information fusion, and suggest the following possible research avenues. Cross-document: We will discuss how to effectively aggregate facts across docume</context>
</contexts>
<marker>Seligman, Mork, Halevy, Smith, Carey, Chen, Wolf, Madhavan, Kannan, 2010</marker>
<rawString>Len Seligman, Peter Mork, Alon Halevy, Ken Smith, Michael J. Carey, Kuang Chen, Chris Wolf, Jayant Madhavan and Akshay Kannan. 2010.</rawString>
</citation>
<citation valid="false">
<title>OpenII: An Open Source Information Integration Toolkit.</title>
<booktitle>Proc. the 2010 international conference on Management of data.</booktitle>
<marker></marker>
<rawString>OpenII: An Open Source Information Integration Toolkit. Proc. the 2010 international conference on Management of data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Collective Segmentation and Labeling of Distant Entities in Information Extraction.</title>
<date>2004</date>
<booktitle>Proc. ICML Workshop on Statistical Relational Learning and Its Connections to Other</booktitle>
<location>Fields.</location>
<contexts>
<context position="10934" citStr="Sutton and McCallum, 2004" startWordPosition="1723" endWordPosition="1726">valuations. Interestingly, the quality of information can often be improved by the fused fact network itself, which can be called as self-boosting of information fusion. For example, if two GPE entities are involved in a conflict-attack event, then they are unlikely to be connected by a part-whole relation; Mahmoud Abbas and Abu Mazen are likely to be coreferential if they get involved in the same life-born event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007) experimented with information fusion of relations across multiple documents. The TextRunner system (Banko et al., 2007) can collapse and compress redundant facts extracted from multiple documents based on coreference resolution (Yates and Etzioni, 2009), semantic similarity computation and normalization. Two relations are central for event fusion: contradiction part of one event mention contradicts part of another, and redundancy part of one event mention conveys the same content as (or is entailed by) part of another. Once these central relations are identif</context>
</contexts>
<marker>Sutton, McCallum, 2004</marker>
<rawString>Charles Sutton and Andrew McCallum. 2004. Collective Segmentation and Labeling of Distant Entities in Information Extraction. Proc. ICML Workshop on Statistical Relational Learning and Its Connections to Other Fields.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
</authors>
<title>Verification of Facts across Document Boundaries.</title>
<date>2006</date>
<booktitle>Proc. International Workshop on Intelligent Information Access.</booktitle>
<contexts>
<context position="5671" citStr="Yangarber, 2006" startWordPosition="877" endWordPosition="878"> For instance, to describe the located relation between a bomber and a bus, there are more than 50 different intervening strings (e.g. killed many people on a, s attack on a, blew apart a, blew himself up on a, drove his explosives-laden car into a, had rigged the, set off a bomb on a, etc.), but the ACE1 training corpora only cover about 1/3 of these expressions. Several recent studies have stressed the benefits of using information redundancy on estimating the correctness of the IE output (Downey et 1 http://www.itl.nist.gov/iad/mig/tests/ace/ al., 2005), improving disease event extraction (Yangarber, 2006), Message Understanding Conference event extraction (Mann, 2007; Patwardhan and Riloff, 2009) and ACE event extraction (Ji and Grishman, 2008). This approach is based on the premise that many facts will be reported multiple times from different sources in different forms. This may occur both within the same document and within a cluster of topically related and successive documents. Therefore, by aggregating similar facts across documents and conducting statistical global inference by favoring interpretation consistency, enhanced extraction performance can be achieved with heterogeneous data t</context>
</contexts>
<marker>Yangarber, 2006</marker>
<rawString>Roman Yangarber. 2006. Verification of Facts across Document Boundaries. Proc. International Workshop on Intelligent Information Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised Methods for Determining Object and Relation Synonyms on the Web.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence. Res. (JAIR)</journal>
<volume>34</volume>
<pages>255--296</pages>
<contexts>
<context position="11222" citStr="Yates and Etzioni, 2009" startWordPosition="1764" endWordPosition="1767">art-whole relation; Mahmoud Abbas and Abu Mazen are likely to be coreferential if they get involved in the same life-born event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007) experimented with information fusion of relations across multiple documents. The TextRunner system (Banko et al., 2007) can collapse and compress redundant facts extracted from multiple documents based on coreference resolution (Yates and Etzioni, 2009), semantic similarity computation and normalization. Two relations are central for event fusion: contradiction part of one event mention contradicts part of another, and redundancy part of one event mention conveys the same content as (or is entailed by) part of another. Once these central relations are identified they will provide a basis for identifying more complex relations such as elaboration, presupposition or consequence. It is important to note that redundancy and contradiction among event mentions are logical relations that are not captured by traditional topic-based techniques for si</context>
</contexts>
<marker>Yates, Etzioni, 2009</marker>
<rawString>Alexander Yates and Oren Etzioni. 2009. Unsupervised Methods for Determining Object and Relation Synonyms on the Web. Journal of Artificial Intelligence. Res. (JAIR) 34: 255-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoxin Yin</author>
<author>Jiawei Han</author>
<author>Philip S Yu</author>
</authors>
<title>Truth Discovery with multiple conflicting information providers on the web.</title>
<date>2008</date>
<journal>IEEE Trans. Knowledge and Data Eng.,</journal>
<pages>20--796</pages>
<contexts>
<context position="7085" citStr="Yin et al., 2008" startWordPosition="1103" endWordPosition="1106">her facts connected to it, which is inspired by PageRank (Page et al., 1998) and LexRank (Erkan and Radev, 2004). For example, a vote by linked entities which are highly voted on by other entities is more valuable than a vote from unlinked entities. There are two major heuristics: (1) an assertion that several information providers agree on is usually more trustable than that only one provider suggests; and (2) an information provider is trustworthy if it provides many pieces of true information, and a piece of information is likely to be true if it is provided by many trustworthy providers. (Yin et al., 2008) used the above heuristics in a progressive, iterative enhancement process for information fusion. The results from the previous work are promising, but the heuristic inferences are highly dependent on the order of applying rules, and the performance may have been limited by the thresholds which may overfit a small development corpus. One promising method might be using Markov Logic Networks (Richardson and Domingos, 2006), a statistical relational learning language, to model these global inference rules more declaratively. Markov Logic will make it possible to compactly specify probability di</context>
</contexts>
<marker>Yin, Han, Yu, 2008</marker>
<rawString>Xiaoxin Yin, Jiawei Han and Philip S. Yu. 2008. Truth Discovery with multiple conflicting information providers on the web. IEEE Trans. Knowledge and Data Eng., 20:796-808.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>