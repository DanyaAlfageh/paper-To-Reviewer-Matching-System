b'Experiments with Open-Domain Textual Question Answering
Sanda M. Harabagiu and Marius A. Pa\x18
sca and Steven J. Maiorano
Department of Computer Science and Engineering
Southern Methodist University
Dallas, TX 75275-0122
fsanda,marius,steveg@renoir.seas.smu.edu
Abstract
This paper describes the integration of several
knowledge-based natural language processing tech-
niques into a Question Answering system, capable
of mining textual answers from large collections of
texts. Surprizing quality is achieved when several
lightweight knowledge-based NLP techniques com-
plement mostly shallow, surface-based approaches.
1 Background
The last decade has witnessed great advances and
interest in the area of Information Extraction (IE)
from real-world texts. Systems that participated in
the TIPSTER MUC competitions have been quite suc-
cessful at extracting information from newswire mes-
sages and \x0clling templates with information pertain-
ing to events or situations of interest. Typically, the
templates model queries regarding who did what to
whom, when and where, and eventually why.
Recently, a new trend in information processing
from texts has emerged. Textual Question Answer-
ing (Q/A) aims at identifying the answer of a ques-
tion in large collections of on-line documents. In-
stead of extracting all events of interest and their re-
lated entities, a Q/A system highlights only a short
piece of text, accounting for the answer. Moreover,
questions are expressed in natural language, are not
constrained to a speci\x0cc domain and are not limited
to the six question types sought by IE systems (i.e.
who1 did what2 to whom3, when4 and where5, and
eventually why6).
In open-domain Q/A systems, the \x0cnite-state
technology and domain knowledge that made IE sys-
tems successful are replaced by a combination of (1)
knowledge-based question processing, (2) new forms
of text indexing and (3) lightweight abduction of
queries. More generally, these systems combine cre-
atively components of the NLP basic research in-
frastructure developed in the 80s (e.g. the compu-
tational theory of Q/A reported in (Lehnert 1978)
and the theory of abductive interpretation of texts
reported in (Hobbs et al.1993)) with other shallow
techniques that make possible the open-domain pro-
cessing on real-world texts.
The idea of building open-domain Q/A systems
that perform on real-world document collections was
initiated by the eighth Text REtrieval Conference
(TREC-8), by organizing the \x0crst competition of an-
swering fact-based questions such as \\Who came up
with the name, El Nino?". Resisting the tempta-
tion of merely porting and integrating existing IE
and IR technologies into Q/A systems, the develop-
ers of the TREC Q/A systems have not only shaped
new processing methods, but also inspired new re-
search in the challenging integration of surface-text-
based methods with knowledge-based text inference.
In particular, two clear knowledge processing needs
are presented: (1) capturing the semantics of open-
domain questions and (2) justifying the correctness
of answers.
In this paper, we present our experiments with
integrating knowledge-based NLP with shallow pro-
cessing techniques for these two aspects of Q/A. Our
research was motivated by the need to enhance the
precision of an implemented Q/A system and by the
requirement to prepare it for scaling to more com-
plex questions than those presented in the TREC
competition. In the remaining of the paper, we de-
scribe a Q/A architecture that allows the integra-
tion of knowledge-based NLP processing with shal-
low processing and we detail their interactions. Sec-
tion 2 presents the functionality of several knowledge
processing modules and describes the NLP tech-
niques for question and answer processing. Section
3 explains the semantic and logical interactions of
processing questions and answers whereas Section
4 highlights the inference aspects that implement
the justi\x0ccation option of a Q/A system. Section
5 presents the results and the evaluations whereas
Section 6 concludes the paper.
2 The NLP Techniques
Surprising quality for open-domain textual Q/A can
be achieved when several lightweight knowledge-
based NLP techniques complement mostly shallow,
surface-based approaches. The processing imposed
by Q/A systems must be distinguished, on the one
hand, from IR techniques, that locate sets of doc-
\x0cWorld Knowledge
Axioms
Rules
Abductive
Lexico-semantic
Patterns
Question
Taxonomies
Word Classes
Parser
Transformation
Question
Semantic
Transformation
Question
Logic Form
Expansion
Question
Question
Recognition
Class
Document
Collection
Index
Keywords 1
Paragrah
Ordering
Combine /
Rerank
Answers
Answer set 1
Answer set 2
Answer set n
Parser
Prover
Question
Expanded Question 2
Expanded Question n
Expanded Question 1
IR Search
Engine
Keywords 2
Keywords 1
Documents
Answer
Extraction
Answer(s)
Answers
True?
False?
Relax
Knowledge-Based Answer Processing
Transformation
Logic Form
Transformation
Semantic
Answer
Answer
Shallow Document Processing
Knowledge-Based Question Processing
Figure 1: An architecture for knowledge-based Question/Answering
uments containing the required information, based
on keywords techniques. Q/A systems are presented
with natural language questions, far richer in seman-
tics than a set of keywords eventually structured
around some operators. Furthermore, the output
of Q/A systems is either the actual answer identi-
\x0ced in a text or small text fragments containing the
answer. This eliminates the user\'s trouble of \x0cnd-
ing the required information in sometimes large sets
of retrieved documents. Open-domain Q/A systems
must also be distinguished, on the other hand, from
IE systems that model the information need through
database templates, thus less naturally than a tex-
tual answer. Moreover, open-domain IE is still di-
cult to achieve, because its linguistic pattern recog-
nition relies on domain-dependent lexico-semantic
knowledge.
To be able to satisfy the open-domain constraints,
textual Q/A systems replace the linguistic pattern
matching capabilities of IE systems with methods
that rely on the recognition of the question type and
of the expected answer type. Generally, this informa-
tion is available by accessing a classi\x0ccation based
on the question stem (i.e. what, how much, who)
and the head of the \x0crst noun phrase of the ques-
tion. Question processing also includes the identi\x0c-
cation of the question keywords. Empirical methods,
based on a set of ordered heuristics operating on the
phrasal parse of the question, extract keywords that
are passed to the search engine. The overall preci-
sion of the Q/A system depends also on the recogni-
tion of the question focus, since the answer extrac-
tion, succeeding the IR phase, is centered around
the question focus. Unfortunately, empirical meth-
ods for focus recognition are hard to develop without
the availability of richer semantic knowledge.
Special requirements are set on the document pro-
cessing component of a Q/A system. To speed-up
the answer extraction, the search engine returns only
those paragraphs from a document that contain all
queried keywords. The paragraphs are ordered to
promote the cases when the keywords not only are as
close as possible, but also preserve the syntactic de-
pendencies recognized in the question. Answers are
extracted whenever the question topic and the an-
swer type are recognized in a paragraph. Thereafter
the answers are scored based on several bag-of-words
heuristics. Throughout all this processing, the NLP
techniques are limited to (a) named entity recogni-
tion; (b) semantic classi\x0ccation of the question type,
based on information provided by an o\x0b-line ques-
tion taxonomy and semantic class information avail-
able from WordNet (Fellbaum 1998); and (c) phrasal
parsing produced by enhancing Brill\'s part-of-speech
tagger with some rules for phrase formation.
However simple, this technology surpasses 75%
precision on trivia questions, as posed in the TREC-
8 competition (cf. (Moldovan et al.1999)). An im-
pressive improvement of 14% is achieved when more
knowledge-intensive NLP techniques are applied at
both question and answer processing level. Figure 1
illustrates the architecture of a system that has en-
hanced Q/A performance.
As represented in Figure 1, all three modules
of the Q/A system preserve the shallow processing
components that determine good performance. In
the Question Processing module, the Question Class
recognizer, working against a taxonomy of questions,
\x0cstill constitutes the central processing that takes
place at this stage. However, a far richer representa-
tion of the question classes is employed. To be able
to classify against the new question taxonomy each
question is \x0crst fully parsed and transformed into
a semantic representation that captures all relation-
ships between phrase heads.
The recognition of the question class is based on
the comparison of the question semantic representa-
tion with the semantic representation of the nodes
from the question taxonomy. Taxonomy nodes en-
code also the answer type, the question focus and
the semantic class of question keywords. Multiple
sets of keywords are generated based on their seman-
tic class, all pertaining to the same original ques-
tion. This feature enables the search engine to re-
trieve multiple sets of documents, pertaining to mul-
tiple sets of answers, that are extracted, combined
and ranked based on several heuristics, reported in
(Moldovan et al.1999). This process of obtaining
multiple sets of answers increases the likelihood of
\x0cnding the correct answer.
However, the big boost in the precision of the
knowledge-based Q/A system is provided by the op-
tion of enabling the justi\x0ccation of the extracted an-
swer. All extracted answers are parsed and trans-
formed in semantic representations. Thereafter,
both semantic transformations for questions and an-
swers are translated into logic forms and presented
to a simpli\x0ced theorem prover. The proof back-
chains from the question to the answer, its trace
generating a justi\x0ccation. The prover may access
a set of abduction rules that relax the justi\x0ccation
process. Whenever an answer cannot be proven, it
is discarded. This option solves multiple situations
when the correct answer is not ranked as the \x0crst
return, due to stronger surface-text-based indicators
in some other answers, which unfortunately are not
correct.
This architecture allows for simple integration of
semantic and axiomatic knowledge sources in a Q/A
system and determines ecient interaction of text-
surface-based and knowledge-based NLP techniques.
3 Interactions
Three main interactions between text-surface-based
and knowledge-based NLP techniques are designed
in our Q/A architecture:
1. When multiple sets of question keywords are passed
to the search engine, increasing the chance of \x0cnd-
ing the text paragraph containing the answer.
2. When the question focus and the answer type, re-
sulting from the knowledge-based processing of the
question, are used in the extraction of the answer,
based on several empirical scores.
3. When the justi\x0ccation option of the Q/A system is
available. Instead of returning answers scored by
some empirical measures, a proof of the correctness
of the answer is produced, by accessing the logical
transformations of the question and the answer, as
well as axioms encoding world knowledge.
All these interactions depend on two factors: (1)
the transformations of the question or answer into
semantic or logical representations; and (2) the avail-
ability of knowledge resources, e.g. the question tax-
onomy and the world knowledge axioms. The avail-
ability of new, high-performace parsers that operate
on real world texts determines the transformation
into semantic and logic formulae quite simple. In ad-
dition, the acquisition of question taxonomies is alle-
viated by machine learning techniques inspired from
bootstrapping methods that learn linguistic patterns
and semantic dictionaries for IE (cf. (Rilo\x0b and
Jones, 1999)). World knowledge axioms can also be
easily derived by processing the gloss de\x0cnitions of
WordNet (Fellbaum 1998).
3.1 Semantic and Logic Transformations
Semantic Transformations
Instead of producing only a phrasal parse for the
question and answer, we make use of one of the new
statistical parsers of large real-world text coverage
(Collins, 1996). The parse trees produced by such a
parser can be easily translated into a semantic repre-
sentation that (1) comprises all the phrase heads and
(2) captures their inter-relationships by anonymous
links. Figure 2 illustrates both the parse tree and
the associated semantic representation of a TREC-8
question.
Why did David Koresh ask the FBI for a word processor
WRB VBD NNP NNP VB NN
DT
IN NN
NNP
DT
NP
WHADVP NP NP
PP
SQ
VP
SBARQ
REASON
ask
processor
word
FBI
Koresh
David
Parse:
Semantic representation:
Question: Why did David Koresh ask the FBI for a word processor?
Figure 2: Question semantic transformation
The actual transformation into semantic repre-
sentation of a question or an answer is obtained
as a by-product of the parse tree traversal. Ini-
tially, all leaves of the parse tree are classi\x0ced as
\x0cskipnodes or non-skipnodes. All nouns, non-auxiliary
verbs, adjectives and adverbs are categorized as
non-skipnodes. All the other leaves are skipnodes.
Bottom-up traversal of the parse tree entails the
propagation of leaf labels whenever the parent node
has more than one non-skipnode child. A rule based
on the syntactic category of the father selects one of
the children to propagate its label at the next level
in the tree. The winning node will then be consid-
ered linked to all the other former siblings that are
non-skipnodes. The propagation continues until the
parse tree root receives a label, and thus a semantic
graph is created as a by-product. Part of the la-
bel propagation, we also consider that whenever all
children of a non-terminal are skipnodes, the parent
becomes a skipnode as well.
Figure 3 represents the label propagation for the
parse tree of the question represented in Figure 2.
The labels of Koresh, ask, FBI and processor are
propagated to the next level. This entails that Ko-
resh is linked to David, ask to FBI and processor and
processor to word. As ask becomes the label of the
tree root, it is also linked to REASON, the question
type determined by the question stem: why. The
label propagation rules are identical to the rules for
mapping from trees to dependency structures used
my Michael Collins (cf. (Collins, 1996)). These rules
identify the head-child, and propagate its label up
in the tree.
ask
ask
ask
Why did David Koresh the FBI for a word processor
WRB VBD NNP NNP VB NN
DT
IN NN
NNP
DT
NP
WHADVP NP NP
PP
SQ
VP
SBARQ
ask
processor
processor
FBI
Koresh
REASON
Figure 3: Parse tree traversal
Logical Transformations
The logical formulae in which questions or answers
are translated are inspired by the notation proposed
in (Hobbs, 1986-1) and implemented in TACITUS
(Hobbs, 1986-2).
Based on the davidsonian treatment of action sen-
tences, in which events are treated as individuals,
every question and every answer are transformed in
a \x0crst-order predicate formula for which (1) verbs
are mapped in predicates verb(e,x,y,z,...) with the
convention that variable e represents the eventual-
ity of that action or event to take place, whereas the
other arguments (e.g. x, y, z, ...) represent the pred-
icate arguments of the verb; (2) nouns are mapped
into their lexicalized predicates; and, (3) modi\x0cers
have the same argument as the predicate they mod-
ify. For example, the question illustrated in Figure 2
has the following logical form transformation (LFT):
[REASON(x)&David(y)&Koresh(y)&ask(e,x,y,z,p)&
&FBI(z)&processor(p)&word(p)]
The process of translating a semantic representa-
tion into a logic form has the following steps:
1. For each node in the semantic representation,
create a predicate with a distinct argument.
2.a. If a noun and an adjective predicate are linked
they should have the same argument.
2.b. The same for verbs and adverbs, pairs of nouns
or an adjective and an adverb.
3. For each verb predicate, add arguments corresponding
to each predicate to which it is directly
linked in the semantic representation.
Predicate arguments can be identi\x0ced because the
semantic representation using anonymous relations
represents uniformly adjuncts and thematic roles.
However, step 2 of the translation procedure recog-
nizes the adjuncts, making predicate arguments the
remaining connections of the verb in the semantic
representation.
3.2 Question Taxonomy
The question taxonomy represents each question
node as a quintuple: (1) a semantic representation
of a question; (2) the question type; (3) the answer
type; (4) the question focus and (5) the question
keywords. By using over 1500 questions provided by
Remedia, as well as other 2000 questions retrieved
from FAQFinder, we have been able to learn classi\x0c-
cation rules and build a complex question taxonomy.
Date
Number
Instance
Product
Name
Name
Person
Author
Location
Name
Artwork
Name
Definition
Reason
Organization
Currency
Location
Figure 4: A snapshot of the top Question Taxonomy
Initially, we started with a seed hierarchy of 25
question classes, manually built, in which all the se-
mantic classes of the nodes from the semantic repre-
sentations were decided o\x0b-line, by a human expert.
300 questions were processed to create this seed hi-
erarchy. Figure 4 illustrates some of the nodes of
\x0cthe top of this hierarchy. Later, as 500 more ques-
tions were considered, we started classifying them
semi-automatically, using the following two steps:
(1) \x0crst a human would decide the semantic class of
each node in the semantic representation of the new
question; (2) then a classi\x0ccation procedure would
decide whether the question belongs to one of the
existing classes or a new class should be considered.
To be able to classify a new question in one of the ex-
isting question classes, two conditions must be satis-
\x0ced: (a) all nodes from the taxonomy question must
correspond to new question nodes with the same
semantic classes; and (b) unifyable nodes must be
linked in the same way in both representations. The
hierarchy grew to 68 question nodes.
Later, 2700 more questions were classi\x0ced fully
automatically. To decide the semantic classes of the
nodes, we used the WordNet semantic hierarchies,
by simply assigning to each semantic representation
node the same class as that of any other question
term from its WordNet hierarchy.
The semantic representation, having the same for-
mat for questions and answers, is a case frame with
anonymous relations, that allows the uni\x0ccation of
the answer to the question regardless of the case re-
lation. Figure 5 illustrates four nodes from the ques-
tion taxonomy, two for the \\currency" question type
and two for the \\person name" question type. The
Figure also represents the mappings of four TREC-
8 questions in these hierarchy nodes. The mappings
are represented by dashed arcs. In this Figure, the
nodes from the semantic representations that con-
tain a question mark are place holders for the ex-
pected answer type.
An additional set of classi\x0ccation rules is associ-
ated with this taxonomy. Initially, all rules are based
on the recognition of the question stem and of the
answer type, obtained with class information from
WordNet. However we could learn new rules when
morphological and semantic variations of the seman-
tic nodes are allowed. Moreover, along with the new
rules, we enrich the taxonomy, because often the new
questions unify only partially with the current tax-
onomy. All semantic and morphologic variations of
the semantic representation nodes are grouped into
word classes. Several of the word classes we used are
listed in Table 1.
Word Class Words
Value words \\monetary value", \\money", \\price"
Expenditure words \\spend", \\buy", \\rent", \\invest"
Creation words \\author", \\designer", \\invent"...
Table 1: Examples of word classes.
The bootstrapping algorithm that learns new
classi\x0ccation rules and new classes of questions
is based on an information extraction measure:
score(rulei)=Ai \x03 log2(Ni), where Ai stands for the
debts
group
Qintex
leave ?
?
timestamp
?
timestamp
?
?
of action
object timestamp
author
of action
word
expenditure
action
value
word
object
with value
timestamp
entity
object
possessing
entity type
specifier
?
Class Name:
Q32:
Q12:
?
spend
Manchester
United
1993
players
Q7:
received
Award
Will Rogers 1989
Q92:
late
1980s
released
Internet
worm
?
competition
word
award
name
word
create
creation
word
creation
title
Class Name:
Class Name:
subclasses
3
name person
semantic representation
semantic representation
Rogers Award in 1989?
Who received the Will
semantic representation
How much did Manchester
United spend on players in 1993?
semantic representation
group leave?
What debts did Qintex
worm in the late 1980s?
Who released the Internet
semantic representation
semantic representation
currency
subclasses
1
2
semantic representation
semantic representation
4
author
Figure 5: Mapping Questions in the Taxonomy
number of di\x0berent lexicon entries for the answer
type of the question, whereas Ni = Ai=Fi, where Fi
is the number of di\x0berent focus categories classi\x0ced.
The steps of the bootstrapping algorithm are:
1. Retrieve concepts morphologically/semantically related
to the semantic representations
2. Apply the classi\x0ccation rules to all questions that
contain any newly retrieved concepts.
3. New Classi\x0ccatiton Rules=fg
MUTUAL BOOTSTRAPPING LOOP
4. Score all new classi\x0ccation rules
5. best CR=the highest scoring classi\x0ccation rule
6. Add best CR to the classi\x0ccation rules
7. Add the questions classi\x0ced by best CR to the taxonomy
8. Goto step 4 three times.
9. Discard all new rules but the best scoring three.
10. Goto 4. until the Q/A performance improves.
\x0c4 The Justi\x0ccation Option
A Q/A system that provides with the option of jus-
tifying the answer has the advantage that erroneous
answers can be ruled out systematically. In our quest
of enhancing the precision of a Q/A system by incor-
porating additional knowledge, we found this option
very helpful. However, the generation of justi\x0cca-
tions for open-domain textual Q/A systems poses
some challenges. First, we needed to develop a very
ecient prover, operating on logical form transfor-
mations. Our proofs are backchaining from the ques-
tions through a mixture of axioms. We use three
forms of axioms: (1) axioms derived from the facts
stated in the textual answer; (2) axioms represent-
ing world knowledge; and (3) axioms determined by
coreference resolution in the answer text. For ex-
ample, some of the axioms employed to prove the
answer to the TREC-8 question Q6: Why did David
Koresh ask the FBI for a word processor?" are:
________________________________________________
SET 1
Mr(71):= null. Koresh(71):=null. word(72):=null.
processor(72):=null. sent(77 76 78 71):=null.
________________________________________________
SET 2
David(1):=Mr(1). _REASON(5):= enable(5 3 6).
________________________________________________
SET 3
FBI(1):=null.
________________________________________________
The \x0crst set represents facts extracted through
LFT predicates of the textual answer: \\Over the
weekend Mr Koresh sent a request for a word proces-
sor to enable him to record his revelations". The sec-
ond set represents world knowledge axioms that we
acquired semi-automatically. For instance we know
that David is a male name, thus that person can be
addressed with Mr.. Similarly, events are enabled
for some reason. The third set of axioms represent
the fact that the FBI is in the context of the text
answer. To be noted that the axioms derived from
the answer have constant arguments, represented by
convention with numbers larger than 70. All the
other arguments are variables.
Q52 Who invented the road trac cone?
Answer Smiling proudly for the cameras , Governor
(shallow Pete Wilson, US Transportation Secretary
methods) Federico Pena and Mayor Richard Riordan
removed a half - dozen plastic orange cones
from the roadway and the \x0crst cars passed
Answer David Morgan, the company\'s managing
(kb-based director and inventor of the plastic
methods) cone even collects them.
Table 2: Examples of improved answer correctness.
The justi\x0ccation of this answer is provided by the
following proof trace. The prover attempts to prove
the LFT of the question (QLF) correct by proving
from left to right each term of QLF.
-------------------------------------------------------------
->Answer:Over the weekend Mr Koresh sent a request for a word
processor to enable him to record his revelations.
->QLF:David(1)^Koresh(1)^word(2)^processor(2)^FBI(4)^
ask(3 4 2 1 5)^_REASON(5)^_PER(1)^_ORG(4)
->ALF:Mr(71)^Koresh(71)^word(72)^processor(72)^revelations(74)^
record(73 74 75)^enable(75 73 76)^request(76)^sent(77 76 78 71)
^weekend(78)^_PER(71)^_DATE(78)
..............................................................
-->Proving:David(1)^Koresh(1)^word(2)^processor(2)^FBI(4)^
ask(3 4 2 1 5)^_REASON(5)^_PER(1)^_ORG(4)
There are 1 target axioms. Selected axiom: David(1):= Mr(1).
Unifying: 1 to 1. Ok
--> Proving: Mr(1)^Koresh(1)^word(2)^processor(2)^FBI(4)
^ask(3 4 2 1 5)^_REASON(5)^_PER(1)^_ORG(4)
There are 1 target axioms. Selected axiom: Mr(71):= null.
Unifying: 1 to 71. Ok
--> Proving: Koresh(71)^word(2)^processor(2)^FBI(4)^
ask(3 4 2 71 5)^_REASON(5)^_PER(71)^_ORG(4)
There are 1 target axioms. Selected axiom: Koresh(71):= null.
Unifying: 71 to 71. Ok
--> Proving: word(2)^processor(2)^FBI(4)^ask(3 4 2 71 5)^
_REASON(5)^_PER(71)^_ORG(4)
There are 1 target axioms. Selected axiom: word(72):= null.
Unifying: 2 to 72. Ok
--> Proving: processor(72)^FBI(4)^ask(3 4 72 71 5)^_REASON(5)
^_PER(71)^_ORG(4)
There are 1 target axioms. Selected axiom: processor(72):= null.
Unifying: 72 to 72. Ok
--> Proving: FBI(4)^ask(3 4 72 71 5)^_REASON(5)^_PER(71)^_ORG(4)
There are 1 target axioms. Selected axiom: FBI(1):= null.
Unifying: 4 to 1. Ok
--> Proving: ask(3 4 72 71 5)^_REASON(5)^_PER(71)^_ORG(4)
There are 2 target axioms. Selected axiom: ask(1 2 3 4 5):=
sent(1 6 7 4)^request(6).
Unifying: 1 to 2. 3 to 1. 5 to 5. 71 to 4. 72 to 3. Ok
--> Proving: sent(1 6 7 71)^request(6)^_REASON(5)^_PER(71)^_ORG(2)
There are 1 target axioms. Selected axiom: sent(77 76 78 71):= null.
Unifying: 1 to 77. 6 to 76. 7 to 78. 71 to 71. Ok
--> Proving: request(76)^_REASON(5)^_PER(71)^_ORG(2)
There are 1 target axioms. Selected axiom: request(76):= null.
Unifying: 76 to 76. Ok
--> Proving: _REASON(5)^_PER(71)^_ORG(2)
There are 1 target axioms. Selected axiom: _REASON(5):= enable(5 3 6).
Unifying: 5 to 5. Ok
--> Proving: enable(5 3 6)^_PER(71)^_ORG(2)
There are 1 target axioms. Selected axiom: enable(75 73 76):= null.
Unifying: 3 to 73. 5 to 75. 6 to 76. Ok
--> Proving: _PER(71)^_ORG(2)
There are 3 target axioms. Selected axiom: _PER(71):= null.
Unifying: 71 to 71. Ok
--> Proving: _ORG(2)
There are 1 target axioms. Selected axiom: _ORG(1):= FBI(1).
Unifying: 2 to 1. Ok
--> Proving: null|||| We found:Success.
------------------------------------------------------------------
There are cases when our simple prover fails to
prove a correct answer. We have noticed that this
happens because in the answer semantic representa-
tion, some concepts that are connected in the ques-
tion semantic representation are no longer directly
linked. This is due to the fact that there are either
parser errors or there are new syntactic dependen-
cies between the two concepts. To accommodate
this situation, we allow di\x0berent constants that are
arguments of the same predicate to be uni\x0cable. The
special cases in which this relaxation of the uni\x0cca-
tion procedure is allowed constitute our abduction
rules.
\x0c5 Evaluation
Both qualitative and quantitative evaluation of the
integration of surface text-based and knowledge-
based methods for Q/A is imposed. Quantitatively,
Table 3 summarizes the scores obtained when only
shallow methods were employed, in contrast with
the results when knowledge-based methods were in-
tegrated. We have separately measured the e\x0bect
of the integration of the knowledge-based methods
at question processing and answer processing level.
We have also evaluated the precision of the sys-
tem when both integrations were implemented. The
results were the \x0crst \x0cve answers returned within
250 bytes of text, when approximatively half mil-
lion TREC documents are mined. We have used the
200 questions from TREC-8, and the correct answers
provided by NIST. The performance was measured
both with the NIST scoring method employed in the
TREC-8 and by simply assigning a score of 1 for the
question having a correct answer, regardless of its
position.
Percentage of NIST score
correct answers
in top 5 returns
Text-surface-based 77.7% 64.5%
Knowledge-based 83.2% 71.5%
Question Processing
(only)
Text-surface-based 77.7% 73%
only with Answer
Justi\x0ccation
Knowledge-based 89.5% 84.75%
Question Processing
with Answer
Justi\x0ccation
Table 3: Accuracy performance
When using the NIST scoring method to eval-
uate an individual answer, we used only six
values:(1; :5; :33; :25; :2; 0), representing the score the
answer\'s question obtains. If the \x0crst answer is cor-
rect, it obtains a score of 1, if the second one is cor-
rect, it is scored with :5, if the third one is correct,
the score becomes :33, if the fourth is correct, the
score is :25 and if the \x0cfth one is correct, the score
is :2. Otherwise, it is scored with 0. No credit is
given if multiple answers are correct. Table 3 shows
that both knowledge-based methods enhanced the
precision, regardless of the scoring method.
To further evaluate the contribution of the justi-
\x0ccation option, we evaluated separately the preci-
sion of the prover for those questions for which the
surface-text-based methods of our system, when op-
erating alone, cannot \x0cnd correct answers. We had
45 TREC-8 questions for which the evaluation of the
prover was performed. Table 4 summarizes the ac-
curacy of the prover.
Proven Proven Precision
correct incorrect
Incorrect answers 3 210 98.5%
(no knowledge)
Correct answers 127 5 96.2%
(KB-based)
Incorrect answers 4 38 90.04%
(KB-based)
Table 4: Prover performance
Qualitatively, we \x0cnd that the integration of
knowledge-based methods is very bene\x0ccial. Table 2
illustrates the correct answer obtained with these
methods, in contrast to the incorrect answer pro-
vided when only the shallow techniques are applied.
6 Conclusions
We believe that the performance of a Q/A system
depends on the knowledge sources it employs. In
this paper we have presented the e\x0bect of the in-
tegration of knowledge derived from question tax-
onomies and produced by answer justi\x0ccations on
the Q/A precision. Our knowledge-based methods
are lightweight, since we do not generate precise se-
mantic representations of questions or answers, but
mere approximations determined by syntactic de-
pendencies. Furthermore, our prover operates on
very simple logical representations, in which syntac-
tic and semantic ambiguities are completely ignored.
Nevertheless, we have shown that these approxima-
tions are functional, since we implemented a prover
that justi\x0ces answers with high precision. Similarly,
our knowledge-based question processing is a mere
combination of word class information and syntactic
dependencies.
References
Michael Collins. A New Statistical Parser Based on Bigram
Lexical Dependencies. In Proceedings of the 34st Annual
Meeting of the Association for Computational Linguistics,
ACL-96, pages 184{191, 1996.
Christiane Fellbaum (Ed). WordNet - An Electronic Lexical
Database. MIT Press, 1998.
Jerry R. Hobbs. Discourse and Inference. Unpublished
manuscript, 1986.
Jerry R. Hobbs. Overview of the TACITUS Project. In Com-
putational Linguistics, 12:(3), 1986.
Jerry Hobbs, Mark Stickel, Doug Appelt, and Paul Mar-
tin. Interpretation as abduction. Arti\x0ccial Intelligence, 63,
pages 69{142, 1993.
Wendy Lehnert. The processing of question answering.
Lawrence Erlbaum Publishers, 1978.
Dan Moldovan, Sanda Harabagiu, Marius Pa\x18
sca, Rada Mi-
halcea, Richard Goodrum, Roxana G^
\x10rju and Vasile Rus.
Lasso: a tool for sur\x0cng the answer net. In Proceedings of
TREC-8, 1999.
Ellen Rilo\x0b and Rosie Jones. Learning Dictionaries for Infor-
mation Extraction by Multi-Level Bootstrapping. In Pro-
ceedings of the 16th National Conference on Arti\x0ccial In-
telligence, AAAI-99, 1999.
\x0c'