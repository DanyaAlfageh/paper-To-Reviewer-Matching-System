b'Probabilistic Parsing and Psychological Plausibility
Thorsten Brants and Matthew Crocker
Saarland University, Computational Linguistics
D-66041 Saarbr\x7f
ucken, Germany
fbrants,crockerg@coli.uni-sb.de
Abstract
Given the recent evidence for probabilistic
mechanisms in models of human ambiguity res-
olution, this paper investigates the plausibil-
ity of exploiting current wide-coverage, prob-
abilistic parsing techniques to model human
linguistic performance. In particular, we in-
vestigate the performance of standard stochas-
tic parsers when they are revised to operate
incrementally, and with reduced memory re-
sources. We present techniques for ranking
and \x0cltering analyses, together with experimen-
tal results. Our results con\x0crm that stochas-
tic parsers which adhere to these psychologi-
cally motivated constraints achieve good per-
formance. Memory can be reduced down to
1% (compared to exhausitve search) without re-
ducing recall and precision. Additionally, these
models exhibit substantially faster performance.
Finally, we argue that this general result is likely
to hold for more sophisticated, and psycholin-
guistically plausible, probabilistic parsing mod-
els.
1 Introduction
Language engineering and computational psy-
cholinguistics are often viewed as distinct re-
search programmes: engineering solutions aim
at practical methods which can achieve good
performance, typically paying little attention
to linguistic or cognitive modelling. Compu-
tational psycholinguistics, on the other hand,
is often focussed on detailed modelling of hu-
man behaviour for a relatively small number
of well-studied constructions. In this paper we
suggest that, broadly, the human sentence pro-
cessing mechanism (HSPM) and current statis-
tical parsing technology can be viewed as having
similar objectives: to optimally (i.e. rapidly and
accurately) understand the text and utterances
they encounter.
Our aim is to show that large scale probabilis-
tic parsers, when subjected to basic cognitive
constraints, can still achieve high levels of pars-
ing accuracy. If successful, this will contribute
to a plausible explanation of the fact that peo-
ple, in general, are also extremely accurate and
robust. Such a result would also strengthen ex-
isting results showing that related probabilistic
mechanisms can explain speci\x0cc psycholinguis-
tic phenomena.
To investigate this issue, we construct a stan-
dard \'baseline\' stochastic parser, which mir-
rors the performance of a similar systems (e.g.
(Johnson, 1998)). We then consider an incre-
mental version of the parser, and evaluate the
e\x0bects of several probabilistic \x0cltering strate-
gies which are used to prune the parser\'s search
space, and thereby reduce memory load.
To assess the generality of our results for
more sophisticated probabilistic models, we also
conduct experiments using a model in which
parent-node information is encoded on the
daughters. This increase in contextual informa-
tion has been shown to improve performance
(Johnson, 1998), and the model is also shown
to be robust to the incrementality and memory
constraints investigated here.
We present the results of parsing perfor-
mance experiments, showing the accuracy of
these systems with respect to both a parsed
corpus and the baseline parser. Our experi-
ments suggest that a strictly incremental model,
in which memory resources are substantially
reduced through \x0cltering, can achieve preci-
sion and recall which equals that of \'unre-
stricted\' systems. Furthermore, implementa-
tion of these restrictions leads to substantially
faster performance. In conclusion, we argue
that such broad-coverage probabilistic parsing
\x0cmodels provide a valuable framework for ex-
plaining the human capacity to rapidly, accu-
rately, and robustly understand \\garden va-
riety" language. This lends further support
to psycholinguistic accounts which posit proba-
bilistic ambiguity resolution mechanisms to ex-
plain \\garden path" phenomena.
It is important to reiterate that our intention
here is only to investigate the performance of
probabilistic parsers under psycholinguistically
motivated constraints. We do not argue for the
psychological plausibility of SCFG parsers (or
the parent-encoded variant) per se. Our inves-
tigation of these models was motivated rather
by our desire to obtain a generalizable result
for these simple and well-understood models,
since obtaining similar results for more sophisti-
cated models (e.g. (Collins, 1996; Ratnaparkhi,
1997)) might have been attributed to special
properties of these models. Rather, the current
result should be taken as support for the poten-
tial scaleability and performance of probabilistic
psychological models such as those proposed by
(Jurafsky, 1996) and (Crocker and Brants, to
appear).
2 Psycholinguistic Motivation
Theories of human sentence processing have
largely been shaped by the study of pathologies
in human language processing behaviour. Most
psycholinguistic models seek to explain the dif-
\x0cculty people have in comprehending structures
that are ambiguous or memory-intensive (see
(Crocker, 1999) for a recent overview). While
often insightful, this approach diverts attention
from the fact that people are in fact extremely
accurate and e\x0bective in understanding the
vast majority of their \\linguistic experience".
This observation, combined with the mounting
psycholinguistic evidence for statistically-based
mechanisms, leads us to investigate the merit of
exploiting robust, broad coverage, probabilistic
parsing systems as models of human linguistic
performance.
The view that human language processing
can be viewed as an optimally adapted sys-
tem, within a probabilistic framework, is ad-
vanced by (Chater et al., 1998), while (Juraf-
sky, 1996) has proposed a speci\x0cc probabilis-
tic parsing model of human sentence process-
ing. In work on human lexical category dis-
ambiguation, (Crocker and Corley, to appear),
have demonstrated that a standard (incremen-
tal) HMM-based part-of-speech tagger mod-
els the \x0cnding from a range of psycholinguis-
tic experiments. In related research, (Crocker
and Brants, 1999) present evidence that an
incremental stochastic parser based on Cas-
caded Markov Models (Brants, 1999) can ac-
count for a range of experimentally observed
local ambiguity preferences. These include
NP/S complement ambiguities, reduced relative
clauses, noun-verb category ambiguities, and
\'that\'-ambiguities (where \'that\' can be either a
complementizer or a determiner) (Crocker and
Brants, to appear).
Crucially, however, there are di\x0berences be-
tween the classes of mechanisms which are psy-
chologically plausible, and those which prevail
in current language technology. We suggest that
two of the most important di\x0berences concern
incrementality, and memory resources. There is
overwhelming experimental evidence that peo-
ple construct connected (i.e. semantically in-
terpretable) analyses for each initial substring
of an utterance, as it is encountered. That is,
processing takes place incrementally, from left
to right, on a word by word basis.
Secondly, it is universally accecpted that peo-
ple can at most consider a relatively small
number of competing analyses (indeed, some
would argue that number is one, i.e. process-
ing is strictly serial). In contrast, many exist-
ing stochastic parsers are \\unrestricted", in that
they are optimised for accuracy, and ignore such
psychologically motivated constraints. Thus the
appropriateness of using broad-coverage proba-
bilistic parsers to model the high level of hu-
man performance is contingent upon being able
to maintain these levels of accuracy when the
constraints of incrementality and resource limi-
tations are imposed.
3 Incremental Stochastic
Context-Free Parsing
The following assumes that the reader is fa-
miliar with stochastic context-free grammars
(SCFG) and stochastic chart-parsing tech-
niques. A good introduction can be found, e.g.,
in (Manning and Sch\x7f
utze, 1999). We use stan-
dard abbreviations for terminial nodes, non-
terminal nodes, rules and probabilities.
\x0cThis paper investigates stochastic context-
free parsing based on a grammar that is derived
from a treebank, starting with part-of-speech
tags as terminals. The grammar is derived by
collecting all rules X ! \x0b that occur in the tree-
bank and their frequencies f. The probability
of a rule is set to
P(X ! \x0b) =
f(X ! \x0b)
P
\x0c
f(X ! \x0b)
(1)
For a description of treebank grammars see
(Charniak, 1996). The grammar does not con-
tain \x0f-rules, otherwise there is no restriction
on the rules. In particular, we do not require
Chomsky-Normal-Form.
In addition to the rules that correspond
to structures in the corpus, we add a new
start symbol ROOT to the grammar and rules
ROOT ! X for all non-terminals X together
with probabilities derived from the root nodes
in the corpus1.
For parsing these grammars, we rely upon
a standard bottom-up chart-parsing technique
with a modi\x0ccation for incremental parsing, i.e.,
for each word, all edges are processed and possi-
bly pruned before proceeding to the next word.
The outline of the algorithm is as follows.
A chart entry E consists of a start and end po-
sition i and j, a dotted rule X ! \x0b:
, the inside
probability \x0c(Xi;j) that X generates the termi-
nal string from position i to j, and information
about the most probable inside structure. If the
dot of the dotted rule is at the rightmost posi-
tion, the corresponding edge is an inactive edge.
If the dot is at any other position, it is an active
edge. Inactive edges represent recognized hypo-
thetical constituents, while active edges repre-
sent pre\x0cxes of hypothetical constituents.
The ith terminal node ti that enters the chart
generates an inactive edge for the span (i 1;i).
Based on this, new active and inactive edges are
generated according to the standard algorithm.
Since we are interested in the most probable
parse, the chart can be minimized in the fol-
lowing way while still performing an exhaustive
search. If there is more than one edge that cov-
ers a span (i;j) having the same non-terminal
symbol on the left-hand side of the dotted rule,
1
The ROOT node is used internally for parsing; it is
neither emitted nor counted for recall and precision.
only the one with the highest inside probability
is kept in the chart. The others cannot con-
tribute to the most probable parse.
For an inactive edge spanning i to j and rep-
resenting the rule X ! Y 1 :::Y k, the inside
probability \x0cI is set to
\x0cI(Xi;j) = P(X ! Y1 :::Yk)
k
Y
l=1
\x0cI(Y l
il;jl
) (2)
where il and jl mark the start and end postition
of Y l, having i = i1 and j = jl. The inside
probability for an active edge \x0cA with the dot
after the kth symbol of the right-hand side is
set to
\x0cA(Xi;j) =
k
Y
l=1
\x0cI(Y k
il;jl
) (3)
We do not use the probability of the rule at this
point. This allows us to combine all edges with
the same span and the dot at the same position
but with di\x0berent symbols on the left-hand side.
Introducing a distinguished left-hand side only
for inactive edges signi\x0ccantly reduces the num-
ber of active edges in the chart. This goes one
step further than implicitly right-binarizing the
grammar; not only suxes of right-hand sides
are joined, but also the corresponding left-hand
sides.
4 Memory Restrictions
We investigate the elimination (pruning) of
edges from the chart in our incremental pars-
ing scheme. After processing a word and before
proceeding to the next word during incremental
parsing, low ranked edges are removed. This is
equivalent to imposing memory restrictions on
the processing system.
The original algorithm keeps one edge in the
chart for each combination of span (start and
end position) and non-terminal symbol (for in-
active edges) or right-hand side pre\x0cxes of dot-
ted rules (for active edges). With pruning, we
restrict the number of edges allowed per span.
The limitation can be expressed in two ways:
1. Variable beam. Select a threshold \x12 \x15 1.
Edge e is removed, if its probability is pe,
the best probability for the span is p1, and
pe <
p1
\x12
: (4)
\x0c2. Fixed beam. Select a maximum number of
edges per span m. An edge e is removed, if
its probability is not in the \x0crst m highest
probabilities for edges with the same span.
We performed experiments using both types
of beams. Fixed beams yielded consistently bet-
ter results than variable beams when plotting
chart size vs. F-score. Therefore, the following
results are reported for \x0cxed beams.
We compare and rank edges covering the
same span only, and we rank active and inactive
edges separately. This is in contrast to (Char-
niak et al., 1998) who rank all edges. They
use normalization in order to account for dif-
ferent spans since in general, edges for longer
spans involve more multiplications of probabil-
ities, yielding lower probabilities. Charniak et
al.\'s normalization value is calculated by a dif-
ferent probability model than the inside proba-
bilities of the edges. So, in addition to the nor-
malization for di\x0berent span lengths, they need
a normalization constant that accounts for the
di\x0berent probability models.
This investigation is based on a much simpler
ranking formula. We use what can be described
as the unigram probability of a non-terminal
node, i.e., the a priori probability of the cor-
responding non-terminal symbol(s) times the
inside probability. Thus, for an inactive edge
hi;j;X ! \x0b;\x0cI(Xi;j)i, we use the probability
PRI(Xi;j) = P(X) \x01P(ti :::tj 1jX) (5)
= P(X) \x01\x0cI(Xi;j) (6)
for ranking. This is the probability of the node
and its yield being present in a parse. The
higher this value, the better is this node. \x0cI is
the inside probability for inactive edges as given
in equation 2, P(X) is the a priori probability
for non-terminal X, (as estimated from the fre-
quency in the training corpus) and PRI is the
probability of the edge for the non-terminal X
spanning positions i to j that is used for rank-
ing.
For an active edge hi;j;X ! Y 1 \x01\x01\x01Y k:
Y k+1 \x01\x01\x01Y m;\x0cA(Y 1
i1;j1
\x01\x01\x01Y k
ik;jk
)i (the dot is af-
ter the kth symbol of the right-hand side) we
use:
PRA(Y 1
i1;j1
:::Y k
ik;jk
) (7)
= P(Y 1 \x01\x01\x01Y k) \x01P(ti :::tj 1jY 1 \x01\x01\x01Y k)(8)
= P(Y 1 \x01\x01\x01Y k) \x01\x0cA(Y 1
i1;j1
:::Y k
ik;jk
) (9)
P(Y 1 \x01\x01\x01Y k) can be read o\x0b the corpus. It is
the a priori probability that the right-hand side
of a production has the pre\x0cx Y 1 :::Y k, which
is estimated by
f(Y 1 \x01\x01\x01Y k is pre\x0cx)
N
(10)
where N is the total number of productions in
the corpus2, i = i1, j = jk and \x0cA is the inside
probability of the pre\x0cx.
5 Experiments
5.1 Data
We use sections 2 { 21 of the Wall Street Jour-
nal part of the Penn Treebank (Marcus et al.,
1993) to generate a treebank grammar. Traces,
functional tags and other tag extensions that do
not mark syntactic category are removed before
training3. No other modi\x0ccations are made. For
testing, we use the 1578 sentences of length 40
or less of section 22. The input to the parser is
the sequence of part-of-speech tags.
5.2 Evaluation
For evaluation, we use the parseval measures
and report labeld F-score (the harmonic mean
of labeled recall and labeled precision). Report-
ing the F-score makes our results comparable to
those of other previous experiments using the
same data sets. As a measure of the amount
of work done by the parser, we report the size
of the chart. The number of active and inac-
tive edges that enter the chart is given for the
exhaustive search, not counting those hypothet-
ical edges that are replaced or rejected because
there is an alternative edge with higher proba-
bility4. For pruned search, we give the percent-
age of edges required.
5.3 Fixed Beam
For our experiments, we de\x0cne the beam by a
maximum number of edges per span. Beams
for active and inactive edges are set separately.
The beams run from 2 to 12, and we test all
2
Here, we use proper pre\x0cxes, i.e., all pre\x0cxes not
including the last element.
3
As an example, PP-TMP=3 is replaced by PP.
4
The size of the chart is comparable to the \
umber
of edges popped" as given in (Charniak et al., 1998).
\x0cResults with Original and Parent Encoding
1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0
71
72
73
74
75
76
77
78
79
Labeled
F-Score
% chart size

inactive: 2
active: 3

inactive: 6
active: 4

inactive: 8
active: 7

inactive: 4
active: 3

inactive: 8
active: 6

inactive: 12
active: 9
Figure 1: Experimental results for incremental parsing and pruning. The \x0cgure shows the percent-
age of edges relative to exhaustive search and the F-score achieved with this chart size. Exhaustive
search yielded 71.21% for the original encoding and 79.28% for the parent encoding. Results in the
grey areas are equivalent with a con\x0cdence degree of \x0b = 0:99.
121 combinations of these beams for active and
inactive edges. Each setting results in a partic-
ular average size of the chart and an F-score,
which are reported in the following section.
5.4 Experimental Results
The results of our 121 test runs with di\x0berent
settings for active and inactive beams are given
in \x0cgure 1. The diagram shows chart sizes vs.
labeled F-scores. It sorts chart sizes across dif-
ferent settings of the beams. If several beam
settings result in equivalent chart sizes, the di-
agram contains the one yielding the highest F-
score.
The main \x0cnding is that we can reduce the
size of the chart to between 1% and 3% of
the size required for exhaustive search without
a\x0becting the results. Only very small beams
degrade performance5. The e\x0bect occurs for
both models despite the simple ranking formula.
This signi\x0ccantly reduces memory requirements
5
Given the amount of test data (26,322 non-terminal
nodes), results within a range of around 0.7% are equiv-
alent with a con\x0cdence degree of \x0b = 99%.
(given as size of the chart) and increases parsing
speed.
Exhaustive search yields an F-Score of
71.21% when using the original Penn Treebank
encoding. Only around 1% the edges are re-
quired to yield equivalent results with incremen-
tal processing and pruning after each word is
added to the chart. This result is, among other
settings, obtained by a \x0cxed beam of 2 for in-
active edges and 3 for active edges6
For the parent encoding, exhaustive search
yields an F-Score of 79.28%. Only between 2
and 3% of the edges are required to yield an
equivalent result with incremental processing
and pruning. As an example, the point at size
= 3.0% F-score = 79.1% is generated by the
beam setting of 12 for inactive and 9 for active
edges. The parent encoding yields around 8%
higher F-scores but it also imposes a higher ab-
solute and relative memory load on the process.
The higher degree of parallelism in the inactive
6
Using variable beams, we would need 1.95% of the
chart entries to achieve an equivalent F-score.
\x0cchart stems from the parent hypothesis in each
node. In terms of pure node categories, the av-
erage number of parallel nodes at this point is
3.57.
Exhaustive search for the base encoding needs
in average 140,000 edges per sentence, for the
parent encoding 200,000 edges; equivalent re-
sults for the base encoding can be achieved with
around 1% of these edges, equivalent results for
the parent encoding need between 2 and 3%.
The lower number of edges signi\x0ccantly in-
creases parsing speed. Using exhaustive search
for the base model, the parser processes 3.0 to-
kens per second (measured on a Pentium III
500; no serious e\x0borts of optimization have gone
into the parser). With a chart size of 1%, speed
is 630 tokens/second. This is a factor of 210
without decreasing accuracy. Speed for the par-
ent model is 0.5 tokens/second (exhaustive) and
111 tokens/seconds (3.0% chart size), yielding
an improvement by factor 220.
6 Related Work
Probably mostly related to the work reported
here are (Charniak et al., 1998) and (Roark and
Johnson, 1999). Both report on signi\x0ccantly
improved parsing eciency by selecting only a
subset of edges for processing. There are three
main di\x0berences to our approach. One is that
they use a ranking for best-\x0crst search while
we immediately prune hypotheses. They need
to store a large number edges because it is not
known in advance how many of the edges will be
used until a parse is found. The second di\x0ber-
ence is that we proceed strictly incrementally
without look-ahead. (Charniak et al., 1998)
use a non-incremental procedure, (Roark and
Johnson, 1999) use a look-ahead of one word.
Thirdly, we use a much simpler ranking formula.
Additionally, (Charniak et al., 1998) and
(Roark and Johnson, 1999) do not use the
original Penntree encoding for the context-free
structures. Before training and parsing, they
change/remove some of the productions and in-
troduce new part-of-speech tags for auxiliaries.
The exact e\x0bect of these modi\x0ccations is un-
known, and it is unclear if these a\x0bect compa-
7
For the active chart, paralellism cannot be given for
di\x0berent nodes types since active edges are introduced
for right-hand side pre\x0cxes, collapsing all possible left-
hand sides.
rability to our results.
The heavy restrictions in our method (imme-
diate pruning, no look-ahead, very simple rank-
ing formula) have consequences on the accuracy.
Using right context and sorting instead of prun-
ing yields roughly 2% higher results (compared
to our base encoding8). But our work shows
that even with these massive restrictions, the
chart size can be reduced to 1% without a de-
crease in accuracy when compared to exhaustive
search.
7 Conclusions
A central challenge in computational psycholin-
guistics is to explain how it is that people are
so accurate and robust in processing language.
Given the substantial psycholinguistic evidence
for statistical cognitive mechanisms, our objec-
tive in this paper was to assess the plausibility
of using wide-coverage probabilistic parsers to
model human linguistic performance. In par-
ticular, we set out to investigate the e\x0bects of
imposing incremental processing and signi\x0ccant
memory limitations on such parsers.
The central \x0cnding of our experiments is that
incremental parsing with massive (97% { 99%)
pruning of the search space does not impair
the accuracy of stochastic context-free parsers.
This basic \x0cnding was robust across di\x0berent
settings of the beams and for the original Penn
Treebank encoding as well as the parent encod-
ing. We did however, observe signi\x0ccantly re-
duced memory and time requirements when us-
ing combined active/inactive edge \x0cltering. To
our knowledge, this is the \x0crst investigation on
tree-bank grammars that systematically varies
the beam for pruning.
Our aim in this paper is not to challenge
state-of-the-art parsing accuracy results. For
our experiments we used a purely context-free
stochastic parser combined with a very sim-
ple pruning scheme based on simple \\unigram"
probabilities, and no use of right context. We
do, however suggest that our result should ap-
ply to richer, more sophistacted probabilistic
8
Comparison of results is not straight-forward since
(Roark and Johnson, 1999) report accuracies only for
those sentences for which a parse tree was generated (be-
tween 93 and 98% of the sentences), while our parser
(except for very small beams) generates parses for vir-
tually all sentences, hence we report accuracies for all
sentences.
\x0cmodels, e.g. when adding word statistics to the
model (Charniak, 1997).
We therefore conclude that wide-coverage,
probabilistic parsers do not su\x0ber impaired ac-
curacy when subject to strict cognitive memory
limitations and incremental processing. Fur-
thermore, parse times are substantially reduced.
This suggests that it may be fruitful to pursue
the use of these models within computational
psycholinguistics, where it is necessary to ex-
plain not only the relatively rare \'pathologies\' of
the human parser, but also its more frequently
observed accuracy and robustness.
References
Thorsten Brants. 1999. Cascaded Markov mod-
els. In Proceedings of 9th Conference of
the European Chapter of the Association for
Computational Linguistics EACL-99, Bergen,
Norway.
Eugene Charniak, Sharon Goldwater, and Mark
Johnson. 1998. Edge-based best-\x0crst chart
parsing. In Proceedings of the Sixth Work-
shop on Very Large Corpora (WVLC-98),
Montreal, Kanada.
Eugene Charniak. 1996. Tree-bank grammars.
In Proceedings of the Thirteenth National
Conference on Arti\x0ccial Intelligence, pages
1031{1036, Menlo Park: AAAI Press/MIT
Press.
Eugene Charniak. 1997. Statistical pars-
ing with a context-free grammar and word
statistics. In Proceedings of the Fourteenth
National Conference on Arti\x0ccial Intelli-
gence, pages 1031{1036, Menlo Park: AAAI
Press/MIT Press.
Nicholas Chater, Matthew Crocker, and Martin
Pickering. 1998. The rational analysis of in-
quiry: The case for parsign. In Chater and
Oaksford, editors, Rational Models of Cogni-
tion. Oxford University Press.
Michael Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In
Proceedings of ACL-96, Santa Cruz, CA,
USA.
Matthew Crocker and Thorsten Brants. 1999.
Incremental probabilistic models of human
linguistic performance. In The 5th Confer-
ence on Architectures and Mechanisms for
Language Processing, Edinburgh, U.K.
Matthew Crocker and Thorsten Brants. to ap-
pear. Wide coverage probabilistic sentence
processing. Journal of Psycholinguistic Re-
search, November 2000.
Matthew Crocker and Ste\x0ban Corley. to ap-
pear. Modular architectures and statistical
mechanisms: The case from lexical category
disambiguation. In Merlo and Stevenson, ed-
itors, The Lexical Basis of Sentence Process-
ing. John Benjamins.
Matthew Crocker. 1999. Mechanisms for sen-
tence processing. In Garrod and Picker-
ing, editors, Language Processing. Psychology
Press, London, UK.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Lin-
guistics, 24(4):613{632.
Daniel Jurafsky. 1996. A probabilistic model of
lexical and syntactic access and disambigua-
tion. Cognitive Science, 20:137{194.
Christopher Manning and Hinrich Sch\x7f
utze.
1999. Foundations of Statistical Natural Lan-
guage Processing. MIT Press, Cambridge,
Massachusetts.
Mitchell Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313{330.
Adwait Ratnaparkhi. 1997. A linear observed
time statistical parser based on maximum en-
tropy models. In Proceedings of the Confer-
ence on Empirical Methods in Natural Lan-
guage Processing EMNLP-97, Providence,
RI.
Brian Roark and Mark Johnson. 1999. Ecient
probabilistic top-down and left-corner pars-
ing. In Proceedings of the 37th Annual Meet-
ing of the Association for Computation Lin-
guistics ACL-99, Maryland.
\x0c'