<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<bodyText confidence="0.430468333333333">
b&apos;Assigning Function Tags to Parsed Text*
Don Blaheta and Eugene Charniak
{dpb, ec}@cs, brown, edu
</bodyText>
<affiliation confidence="0.928874">
Department of Computer Science
</affiliation>
<address confidence="0.62948">
Box 1910 / 115 Waterman St.--4th floor
</address>
<affiliation confidence="0.708207">
Brown University
</affiliation>
<address confidence="0.654638">
Providence, RI 02912
</address>
<sectionHeader confidence="0.966594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998244714285714">
It is generally recognized that the common non-
terminal labels for syntactic constituents (NP,
VP, etc.) do not exhaust the syntactic and se-
mantic information one would like about parts
of a syntactic tree. For example, the Penn Tree-
bank gives each constituent zero or more \&apos;func-
tion tags\&apos; indicating semantic roles and other
related information not easily encapsulated in
the simple constituent labels. We present a sta-
tistical algorithm for assigning these function
tags that, on text already parsed to a simple-
label level, achieves an F-measure of 87%, which
rises to 99% when considering \&apos;no tag\&apos; as a valid
choice.
</bodyText>
<sectionHeader confidence="0.997483" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99497179661017">
Parsing sentences using statistical information
gathered from a treebank was first examined a
decade ago in (Chitrao and Grishman, 1990)
and is by now a fairly well-studied problem
((Charniak, 1997), (Collins, 1997), (Ratna-
parkhi, 1997)). But to date, the end product of
the parsing process has for the most part been
a bracketing with simple constituent labels like
NP, VP, or SBAR.The Penn treebank contains a
great deal of additional syntactic and seman-
tic information from which to gather statistics;
reproducing more of this information automat-
ically is a goal which has so far been mostly
ignored. This paper details a process by which
some of this information--the function tags--
may be recovered automatically.
In the Penn treebank, there are 20 tags (fig-
ure 1) that can be appended to constituent la-
bels in order to indicate additional information
about the syntactic or semantic role of the con-
* This researchwas funded in part by NSF grants LIS-
SBR-9720368 and IGERT-9870676.
stituent. We have divided them into four cate-
gories (given in figure 2) based on those in the
bracketing guidelines (Bies et al., 1995). A con-
stituent can be tagged with multiple tags, but
never with two tags from the same category.1
In actuality, the case where a constituent has
tags from all four categories never happens, but
constituents with three tags do occur (rarely).
At a high level, we can simply say that hav-
ing the function tag information for a given text
is useful just because any further information
would help. But specifically, there are distinct
advantages for each of the various categories.
Grammatical tags are useful for any application
trying to follow the thread of the text--they find
the \&apos;who does what\&apos; of each clause, which can
be useful to gain information about the situa-
tion or to learn more about the behaviour of
the words in the sentence. The form/function
tags help to find those constituents behaving in
ways not conforming to their labelled type, as
well as further clarifying the behaviour of ad-
verbial phrases. Information retrieval applica-
tions specialising in describing events, as with a
number of the MUC applications, could greatly
benefit from some of these in determining the
where-when-whyof things. Noting a topicalised
constituent could also prove useful to these ap-
plications, and it might also help in discourse
analysis, or pronoun resolution. Finally, the
\&apos;miscellaneous\&apos; tags are convenient at various
times; particularly the CLI~\&apos;closely related\&apos; tag,
which among other things marks phrasal verbs
and prepositional ditransitives.
To our knowledge, there has been no attempt
so far to recover the function tags in pars-
ing treebank text. In fact, we know of only
</bodyText>
<footnote confidence="0.523697666666667">
1There is a single exception in the corpus: one con-
stituent is tagged with -LOC-I~R.This appears to be an
error.
</footnote>
<page confidence="0.997548">
234
</page>
<figure confidence="0.950673185185185">
\x0cADV Non-specific adverbial
BNF Benefemtive
CLF It-cleft
CLR \&apos;Closely related\&apos;
DIR Direction
DTV Dative
EXT Extent
HLN Headline
LGS Logical subject
L0C Location
MNI~ Manner
N0M Nominal
PRD Predicate
PRP Purpose
PUT Locative complement of \&apos;put\&apos;
SBJ Subject
TMP Temporal
TPC Topic
TTL Title
V0C Vocative
Grammatical
DTV 0.48%
LGS 3.0%
PRD 18.%
PUT 0.26%
SBJ 78.%
v0c 0.025%
</figure>
<figureCaption confidence="0.988566">
Figure 1: Penn treebank function tags
</figureCaption>
<table confidence="0.9980802">
53.% Form/Function 37.% Topicalisation 2.2%
0.25% NOM 6.8% 2.5% TPC 100% 2.2%
1.5% ADV 11.% 4.2%
9.3% BN\&apos;F 0.072% 0.026%
0.13% DIR 8.3% 3.0%
41.% EXT 3.2% 1.2%
0.013% LOC 25.% 9.2%
MNR 6.2% 2.3%
PI~ 5.2% 1.9%
33.% 12.%
Miscellaneous 9.5%
CLR 94.% 8.8%
CLF 0.34% 0.03%
HLN 2.6% 0.25%
TTL 3.1% 0.29%
</table>
<figureCaption confidence="0.990764">
Figure 2: Categories of function tags and their relative frequencies
</figureCaption>
<bodyText confidence="0.992439">
one project that used them at all: (Collins,
1997) defines certain constituents as comple-
ments based on a combination of label and func-
tion tag information. This boolean condition is
then used to train an improved parser.
</bodyText>
<sectionHeader confidence="0.999154" genericHeader="introduction">
2 Features
</sectionHeader>
<bodyText confidence="0.9997204">
We have found it useful to define our statisti-
cal model in terms of features. A \&apos;feature\&apos;, in
this context, is a boolean-valued function, gen-
erally over parse tree nodes and either node la-
bels or lexical items. Features can be fairly sim-
ple and easily read off the tree (e.g. \&apos;this node\&apos;s
label is X\&apos;, \&apos;this node\&apos;s parent\&apos;s label is Y\&apos;), or
slightly more complex (\&apos;this node\&apos;s head\&apos;s part-
of-speech is Z\&apos;). This is concordant with the us-
age in the maximum entropy literature (Berger
et al., 1996).
When using a number of known features to
guess an unknown one, the usual procedure is
to calculate the value of each feature, and then
essentially look up the empirically most proba-
ble value for the feature to be guessed based on
those known values. Due to sparse data, some
of the features later in the list may need to be
ignored; thus the probability of an unknown fea-
ture value would be estimated as
</bodyText>
<equation confidence="0.6362475">
P(flYl, , Y,)
P(flfl, f2,...,fj), j &lt; n, (1)
</equation>
<bodyText confidence="0.9745678">
where/3 refers to an empirically observed prob-
ability. Of course, if features 1 through i only
co-occur a few times in the training, this value
may not be reliable, so the empirical probability
is usually smoothed:
</bodyText>
<equation confidence="0.969082666666667">
P(flfl, Ii)
AiP(flfl, fa,..., fi)
+ (2)
</equation>
<bodyText confidence="0.978801076923077">
The values for )~i can then be determined ac-
cording to the number of occurrences of features
1 through i together in the training.
One way to think about equation 1 (and
specifically, the notion that j will depend on
the values of fl... fn) is as follows: We begin
with the prior probability of f. If we have data
indicating P(flfl), we multiply in that likeli-
hood, while dividing out the original prior. If
we have data for/3(flfl, f2), we multiply that
in while dividing out the P(flfl) term. This is
repeated for each piece of feature data we have;
at each point, we are adjusting the probability
</bodyText>
<page confidence="0.913576">
235
</page>
<equation confidence="0.990512333333333">
\x0cP(flfl,f2,... ,fn) p(/) P(SlA) P(SlSl, S:)
P(f) P(flfl)
P(flfl,..., Yi-1,A)
-,_-o
&quot; p- ff,
P(flft, $2,..., f~)
P(flA, A,...,f-x)
j&lt;n
(3)
</equation>
<bodyText confidence="0.9747631875">
we already have estimated. If knowledge about
feature fi makes S more likely than with just
fl... fi-1, the term where fi is added will be
greater than one and the running probability
will be adjusted upward. This gives us the new
probability shown in equation 3, which is ex-
actly equivalent to equation 1 since everything
except the last numerator cancels out of the
equation. The value of j is chosen such that
features fl..-fj are sufficiently represented in
the training data; sometimes all n features are
used, but often that would cause sparse data
problems. Smoothing is performed on this equa-
tion exactly as before: each term is interpolated
between the empirical value and the prior esti-
mated probability, according to a value of Ai
that estimates confidence. But aside from per-
haps providing a new way to think about the
problem, equation 3 is not particularly useful
as it is--it is exactly the same as what we had
before. Its real usefulness comes, as shown in
(Charniak, 1999), when we move from the no-
tion of a feature chain to a feature tree.
These feature chains don\&apos;t capture everything
we\&apos;d like them to. If there are two independent
features that are each relatively sparse but occa-
sionally carry a lot of information, then putting
one before the other in a chain will effectively
block the second from having any effect, since
its information is (uselessly) conditioned on the
first one, whose sparseness will completely di-
lute any gain. What we\&apos;d really like is to be able
to have a feature tree, whereby we can condition
those two sparse features independently on one
common predecessor feature. As we said be-
fore, equation 3 represents, for each feature fi,
the probability of f based on fi and all its pre-
decessors, divided by the probability of f based
only on the predecessors. In the chain case, this
means that the denominator is conditioned on
every feature from 1 to i - 1, but if we use a
feature tree, it is conditioned only on those fea-
tures along the path to the root of the tree.
A notable issue with feature trees as opposed
to feature chains is that the terms do not all
cancel out. Every leaf on the tree will be repre-
target ~
feature
</bodyText>
<figureCaption confidence="0.988714">
Figure 3: A small example feature tree
</figureCaption>
<bodyText confidence="0.950478222222222">
sented in the numerator, and every fork in the
tree (from which multiple nodes depend) will
be represented at least once in the denomina-
tor. For example: in figure 3 we have a small
feature tree that has one target feature and four
conditioning features. Features b and d are in-
dependent of each other, but each depends on a;
c depends directly only on b. The unsmoothed
version of the corresponding equation would be
</bodyText>
<equation confidence="0.907571666666667">
P(fla, b,c,d) ,~
p,~ P(fla) ~)(f]a, b) P(f[a, b,c) P(fla, d)
which, after cancelling of terms and smoothing,
results in
P(fla, b,c,d) P(fla, b,c)P(fla, d)
P(fla) (4)
</equation>
<bodyText confidence="0.999307272727273">
Note that strictly speaking the result is not a
probability distribution. It could be made into
one with an appropriate normalisation--the
so-called partition function in the maximum-
entropy literature. However, if the indepen-
dence assumptions made in the derivation of
equation 4 are good ones, the partition func-
tion will be close to 1.0. We assume this to be
the case for our feature trees.
Now we return the discussion to function tag-
ging. There are a number of features that seem
</bodyText>
<page confidence="0.981815">
236
</page>
<figure confidence="0.985921285714286">
\x0cfunction
tag label
succeeding preceding
, ,./-d~el laf)el
pare_p~
gra-\&apos;n~arent\&apos;s parent\&apos;s
label head\&apos;s POS
grandparent\&apos;s
h ~ P O S
headS~ parent\&apos;s
P ~ e a d
head
alt-head\&apos;s
POs alt-~ead
</figure>
<figureCaption confidence="0.999901">
Figure 4: The feature tree used to guess function tags
</figureCaption>
<bodyText confidence="0.99905625">
to condition strongly for one function tag or an-
other; we have assembled them into the feature
tree shown in figure 4.2 This figure should be
relatively self-explanatory, except for the notion
of an \&apos;alternate head\&apos;; currently, an alternate
head is only defined for prepositional phrases,
and is the head of the object of the preposi-
tional phrase. This data is very important in
distinguishing, for example, \&apos;by John\&apos; (where
John might be a logical subject) from \&apos;by next
year\&apos; (a temporal modifier) and \&apos;by selling it\&apos;
(an adverbial indicating manner).
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="method">
3 Experiment
</sectionHeader>
<bodyText confidence="0.9762232">
In the training phase of our experiment, we
gathered statistics on the occurrence of func-
tion tags in sections 2-21 of the Penn treebank.
Specifically, for every constituent in the tree-
bank, we recorded the presence of its function
tags (or lack thereof) along with its condition-
ing information. From this we calculated the
empirical probabilities of each function tag ref-
erenced in section 2 of this paper. Values of )~
were determined using EM on the development
corpus (treebank section 24).
To test, then, we simply took the output of
our parser on the test corpus (treebank section
23), and applied a postprocessing step to add
function tags. For each constituent in the tree,
we calculated the likelihood of each function tag
according to the feature tree in figure 4, and
for each category (see figure 2) we assigned the
most likely function tag (which might be the
null tag).
2The reader will note that the \&apos;features\&apos; listed in the
tree are in fact not boolean-valued; each node in the
given tree can be assumed to stand for a chain of boolean
features, one per potential value at that node, exactly
one of which will be true.
</bodyText>
<sectionHeader confidence="0.995996" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999337542857143">
To evaluate our results, we first need to deter-
mine what is \&apos;correct\&apos;. The definition we chose
is to call a constituent correct if there exists in
the correct parse a constituent with the same
start and end points, label, and function tag
(or lack thereof). Since we treated each of the
four function tag categories as a separate fea-
ture for the purpose of tagging, evaluation was
also done on a per-category basis.
The denominator of the accuracy measure
should be the maximum possible number we
could get correct. In this case, that means
excluding those constituents that were already
wrong in the parser output; the parser we used
attains 89% labelled precision-recall, so roughly
11% of the constituents are excluded from the
function tag accuracy evaluation. (For refer-
ence, we have also included the performance of
our function tagger directly on treebank parses;
the slight gain that resulted is discussed below.)
Another consideration is whether to count
non-tagged constituents in our evaluation. On
the one hand, we could count as correct any
constituent with the correct tag as well as any
correctly non-tagged constituent, and use as
our denominator the number of all correctly-
labelled constituents. (We will henceforth refer
to this as the \&apos;with-null\&apos; measure.) On the other
hand, we could just count constituents with the
correct tag, and use as our denominators the
total number of tagged, correctly-labelled con-
stituents. We believe the latter number (\&apos;no-
null\&apos;) to be a better performance metric, as it
is not overwhelmed by the large number of un-
tagged constituents. Both are reported below.
</bodyText>
<page confidence="0.947162">
237
</page>
<figure confidence="0.9805165">
\x0cCategory
Grammatical
Form/Function
Topicalisation
Miscellaneous
Overall
</figure>
<tableCaption confidence="0.873405">
Table 1: Baseline performance
</tableCaption>
<table confidence="0.651587928571429">
Baseline 1
(never tag) Tag Precision
86.935% SBJ 10.534%
91.786% THP 3.105%
99.406% TPC 0.594%
98.436% CLR 1.317%
94.141% -- 3.887%
Baseline 2 (always choose most likely tag)
Recall F-measure
80.626% 18.633%
37.795% 5.738%
100.00% 1.181%
84.211% 2.594%
66.345% 7.344%
</table>
<tableCaption confidence="0.93599">
Table 2: Performance within each category
</tableCaption>
<table confidence="0.993587333333333">
With-null ---No-null--
Category Accuracy Precision Recall F-measure
Grammatical 98.909% 95.472% 95.837% 95.654%
Form/Function 97.104% 80.415% 77.595% 78.980%
Topicalisation 99.915% 92.195% 93.564% 92.875%
Miscellaneous 98.645% 55.644% 65.789% 60.293%
</table>
<sectionHeader confidence="0.994462" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.915682">
5.1 Baselines
</subsectionHeader>
<bodyText confidence="0.997958478260869">
There are, it seems, two reasonable baselines
for this and future work. First of all, most con-
stituents in the corpus have no tags at all, so
obviously one baseline is to simply guess no tag
for any constituent. Even for the most com-
mon type of function tag (grammatical), this
method performs with 87% accuracy. Thus the
with-null accuracy of a function tagger needs to
be very high to be significant here.
The second baseline might be useful in ex-
amining the no-null accuracy values (particu-
larly the recall): always guess the most common
tag in a category. This means that every con-
stituent gets labelled with \&apos;-SBJ-THP-TPC-CLR\&apos;
(meaning that it is a topicalised temporal sub-
ject that is \&apos;closely related\&apos; to its verb). This
combination of tags is in fact entirely illegal
by the treebank guidelines, but performs ad-
equately for a baseline. The precision is, of
course, abysmal, for the same reasons the first
baseline did so well; but the recall is (as one
might expect) substantial. The performances
of the two baseline measures are given in Table
</bodyText>
<page confidence="0.327657">
1.
</page>
<subsectionHeader confidence="0.798826">
5.2 Performance in individual
categories
</subsectionHeader>
<bodyText confidence="0.999887954545455">
In table 2, we give the results for each category.
The first column is the with-null accuracy, and
the precision and recall values given are the no-
null accuracy, as noted in section 4.
Grammatical tagging performs the best of the
four categories. Even using the more difficult
no-null accuracy measure, it has a 96% accu-
racy. This seems to reflect the fact that gram-
matical relations can often be guessed based on
constituent labels, parts of speech, and high-
frequency lexical items, largely avoiding sparse-
data problems. Topicalisation can similarly be
guessed largely on high-frequency information,
and performed almost as well (93%).
On the other hand, we have the
form/function tags and the \&apos;miscellaneous\&apos;
tags. These are characterised by much more
semantic information, and the relationships
between lexical items are very important,
making sparse data a real problem. All the
same, it should be noted that the performance
is still far better than the baselines.
</bodyText>
<subsectionHeader confidence="0.774366">
5.3 Performance with other feature
trees
</subsectionHeader>
<bodyText confidence="0.9984465">
The feature tree given in figure 4 is by no means
the only feature tree we could have used. In-
</bodyText>
<page confidence="0.998411">
238
</page>
<tableCaption confidence="0.786078">
\x0cTable 3: Overall performance on different inputs
</tableCaption>
<table confidence="0.99350975">
With-null --No-null-
Category Accuracy Precision Recall F-measure
Parsed 98.643% 87.173% 87.381% 87.277%
Treebank 98.805% 88.450% 88.493% 88.472%
</table>
<bodyText confidence="0.998017125">
deed, we tried a number of different trees on the
development corpus; this tree gave among the
best overall results, with no category perform-
ing too badly. However, there is no reason to
use only one feature tree for all four categories;
the best results can be got by using a separate
tree for each one. One can thus achieve slight
(one to three point) gains in each category.
</bodyText>
<subsectionHeader confidence="0.988647">
5.4 Overall performance
</subsectionHeader>
<bodyText confidence="0.998631151515151">
The overall performance, given in table 3, ap-
pears promising. With a tagging accuracy of
about 87%, various information retrieval and
knowledge base applications can reasonably ex-
pect to extract useful information.
The performance given in the first row is (like
all previously given performance values) the
function-tagger\&apos;s performance on the correctly-
labelled constituents output by our parser. For
comparison, we also give its performance when
run directly on the original treebank parse; since
the parser\&apos;s accuracy is about 89%, working di-
rectly with the treebank means our statistics
are over roughly 12% more constituents. This
second version does slightly better.
The main reason that tagging does worse on
the parsed version is that although the con-
stituent itself may be correctly bracketed and la-
belled, its exterior conditioning information can
still be incorrect. An example of this that ac-
tually occurred in the development corpus (sec-
tion 24 of the treebank) is the \&apos;that\&apos; clause in
the phrase \&apos;can swallow the premise that the re-
wards for such ineptitude are six-figure salaries\&apos;,
correctly diagrammed in figure 5. The function
tagger gave this SBARan ADV tag, indicating an
unspecified adverbial function. This seems ex-
tremely odd, given that its conditioning infor-
mation (nodes circled in the figure) clearly show
that it is part of an NP, and hence probably mod-
ifies the preceding NN.Indeed, the statistics give
the probability of an ADVtag in this condition-
ing environment as vanishingly small.
</bodyText>
<figure confidence="0.580362">
vP
the ( premise ) ~
</figure>
<figureCaption confidence="0.999849666666667">
Figure 5: SBAR and conditioning info
the premise ~ ...
Figure 6: SBARand conditioning info, as parsed
</figureCaption>
<bodyText confidence="0.993216933333333">
However, this was not the conditioning infor-
mation that the tagger received. The parser
had instead decided on the (incorrect) parse in
figure 6. As such, the tagger\&apos;s decision makes
much more sense, since an SBARunder two VPs
whose heads are VB and MDis rather likely to be
an ADV. (For instance, the \&apos;although\&apos; clause of
the sentence \&apos;he can help, although he doesn\&apos;t
want to.\&apos; has exactly the conditioning environ-
ment given in figure 6, except that its prede-
cessor is a comma; and this SBARwould be cor-
rectly tagged ADV.) The SBARitself is correctly
bracketed and labelled, so it still gets counted
in the statistics. Happily, this sort of case seems
to be relatively rare.
</bodyText>
<page confidence="0.973681">
239
</page>
<bodyText confidence="0.999579192307692">
\x0cAnother thing that lowers the overall perfor-
mance somewhat is the existence of error and in-
consistency in the treebank tagging. Some tags
seem to have been relatively easy for the human
treebank taggers, and have few errors. Other
tags have explicit caveats that, however well-
justified, proved difficult to remember for the
taggers--for instance, there are 37 instances of
a PP being tagged with LGS (logical subject) in
spite of the guidelines specifically saying, \&apos;[LGS]
attaches to the NP object of by and not to the
PP node itself.\&apos; (Bies et al., 1995) Each mistag-
ging in the test corpus can cause up to two spu-
rious errors, one in precision and one in recall.
Still another source of difficulty comes when the
guidelines are vague or silent on a specific issue.
To return to logical subjects, it is clear that \&apos;the
loss\&apos; is a logical subject in \&apos;The company was
hurt by the loss\&apos;, but what about in \&apos;The com-
pany was unperturbed by the loss\&apos; ? In addition,
a number of the function tags are authorised for
\&apos;metaphorical use\&apos;, but what exactly constitutes
such a use is somewhat inconsistently marked.
It is as yet unclear just to what degree these
tagging errors in the corpus are affecting our
results.
</bodyText>
<sectionHeader confidence="0.998735" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999834028571428">
This work presents a method for assigning func-
tion tags to text that has been parsed to the
simple label level. Because of the lack of prior
research on this task, we are unable to com-
pare our results to those of other researchers;
but the results do seem promising. However, a
great deal of future work immediately suggests
itself:
Although we tested twenty or so feature
trees besides the one given in figure 4, the
space of possible trees is still rather un-
explored. A more systematic investiga-
tion into the advantages of different feature
trees would be useful.
We could add to the feature tree the val-
ues of other categories of function tag, or
the function tags of various tree-relatives
(parent, sibling).
One of the weaknesses of the lexical fea-
tures is sparse data; whereas the part of
speech is too coarse to distinguish \&apos;by John\&apos;
(LGS) from \&apos;by Monday\&apos; (TMP), the lexi-
cal information may be too sparse. This
could be assisted by clustering the lexical
items into useful categories (names, dates,
etc.), and adding those categories as an ad-
ditional feature type.
There is no reason to think that this work
could not be integrated directly into the
parsing process, particularly if one\&apos;s parser
is already geared partially or entirely to-
wards feature-based statistics; the func-
tion tag information could prove quite use-
ful within the parse itself, to rank several
parses to find the most plausible.
</bodyText>
<sectionHeader confidence="0.992087" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9960038125">
Adam L. Berger, Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural lan-
guage processing. Computational Linguistics,
22(1):39-71.
Ann Bies, Mark Ferguson, Karen Katz, and
Robert MacIntyre, 1995. Bracketing Guide-
lines for Treebank H Style Penn Treebank
Project, January.
Eugene Charniak. 1997. Statistical pars-
ing with a context-free grammar and word
statistics. In Proceedings of the Fourteenth
National Conference on Artificial Intelli-
gence, pages 598-603, Menlo Park. AAAI
Press/MIT Press.
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. Technical Report CS-99-12,
Brown University, August.
Mahesh V. Chitrao and Ralph Grishman. 1990.
Statistical parsing of messages. In DARPA
Speech and Language Workshop, pages 263-
266.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the
Association for Computational Linguistics,
pages 16-23.
Adwait Ratnaparkhi. 1997. A linear observed
time statistical parser based on maximum en-
tropy models. In Proceedings of the Second
Annual Conference on Empirical Methods in
Natural Language Processing, pages 1-10.
</reference>
<page confidence="0.826376">
240
</page>
<figure confidence="0.349256">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.655639">
<title confidence="0.999887">b&apos;Assigning Function Tags to Parsed Text*</title>
<author confidence="0.998724">Don Blaheta</author>
<author confidence="0.998724">Eugene Charniak</author>
<email confidence="0.980393">dpb@cs,brown,edu</email>
<email confidence="0.980393">ec@cs,brown,edu</email>
<affiliation confidence="0.999087">Department of Computer Science</affiliation>
<address confidence="0.72086">Box 1910 / 115 Waterman St.--4th floor</address>
<affiliation confidence="0.99953">Brown University</affiliation>
<address confidence="0.999333">Providence, RI 02912</address>
<abstract confidence="0.994713">It is generally recognized that the common nonterminal labels for syntactic constituents (NP, VP, etc.) do not exhaust the syntactic and semantic information one would like about parts of a syntactic tree. For example, the Penn Treebank gives each constituent zero or more \&apos;function tags\&apos; indicating semantic roles and other related information not easily encapsulated in the simple constituent labels. We present a statistical algorithm for assigning these function tags that, on text already parsed to a simplelabel level, achieves an F-measure of 87%, which rises to 99% when considering \&apos;no tag\&apos; as a valid choice.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="5209" citStr="Berger et al., 1996" startWordPosition="852" endWordPosition="855">label and function tag information. This boolean condition is then used to train an improved parser. 2 Features We have found it useful to define our statistical model in terms of features. A \&apos;feature\&apos;, in this context, is a boolean-valued function, generally over parse tree nodes and either node labels or lexical items. Features can be fairly simple and easily read off the tree (e.g. \&apos;this node\&apos;s label is X\&apos;, \&apos;this node\&apos;s parent\&apos;s label is Y\&apos;), or slightly more complex (\&apos;this node\&apos;s head\&apos;s partof-speech is Z\&apos;). This is concordant with the usage in the maximum entropy literature (Berger et al., 1996). When using a number of known features to guess an unknown one, the usual procedure is to calculate the value of each feature, and then essentially look up the empirically most probable value for the feature to be guessed based on those known values. Due to sparse data, some of the features later in the list may need to be ignored; thus the probability of an unknown feature value would be estimated as P(flYl, , Y,) P(flfl, f2,...,fj), j &lt; n, (1) where/3 refers to an empirically observed probability. Of course, if features 1 through i only co-occur a few times in the training, this value may n</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Robert MacIntyre</author>
</authors>
<title>Bracketing Guidelines for Treebank H Style Penn Treebank Project,</title>
<date>1995</date>
<contexts>
<context position="1976" citStr="Bies et al., 1995" startWordPosition="317" endWordPosition="320"> reproducing more of this information automatically is a goal which has so far been mostly ignored. This paper details a process by which some of this information--the function tags-- may be recovered automatically. In the Penn treebank, there are 20 tags (figure 1) that can be appended to constituent labels in order to indicate additional information about the syntactic or semantic role of the con* This researchwas funded in part by NSF grants LISSBR-9720368 and IGERT-9870676. stituent. We have divided them into four categories (given in figure 2) based on those in the bracketing guidelines (Bies et al., 1995). A constituent can be tagged with multiple tags, but never with two tags from the same category.1 In actuality, the case where a constituent has tags from all four categories never happens, but constituents with three tags do occur (rarely). At a high level, we can simply say that having the function tag information for a given text is useful just because any further information would help. But specifically, there are distinct advantages for each of the various categories. Grammatical tags are useful for any application trying to follow the thread of the text--they find the \&apos;who does what\&apos; </context>
<context position="19939" citStr="Bies et al., 1995" startWordPosition="3317" endWordPosition="3320">ily, this sort of case seems to be relatively rare. 239 \x0cAnother thing that lowers the overall performance somewhat is the existence of error and inconsistency in the treebank tagging. Some tags seem to have been relatively easy for the human treebank taggers, and have few errors. Other tags have explicit caveats that, however welljustified, proved difficult to remember for the taggers--for instance, there are 37 instances of a PP being tagged with LGS (logical subject) in spite of the guidelines specifically saying, \&apos;[LGS] attaches to the NP object of by and not to the PP node itself.\&apos; (Bies et al., 1995) Each mistagging in the test corpus can cause up to two spurious errors, one in precision and one in recall. Still another source of difficulty comes when the guidelines are vague or silent on a specific issue. To return to logical subjects, it is clear that \&apos;the loss\&apos; is a logical subject in \&apos;The company was hurt by the loss\&apos;, but what about in \&apos;The company was unperturbed by the loss\&apos; ? In addition, a number of the function tags are authorised for \&apos;metaphorical use\&apos;, but what exactly constitutes such a use is somewhat inconsistently marked. It is as yet unclear just to what degree th</context>
</contexts>
<marker>Bies, Ferguson, Katz, MacIntyre, 1995</marker>
<rawString>Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre, 1995. Bracketing Guidelines for Treebank H Style Penn Treebank Project, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence,</booktitle>
<pages>598--603</pages>
<publisher>AAAI Press/MIT Press.</publisher>
<location>Menlo Park.</location>
<contexts>
<context position="1052" citStr="Charniak, 1997" startWordPosition="164" endWordPosition="165">e Penn Treebank gives each constituent zero or more \&apos;function tags\&apos; indicating semantic roles and other related information not easily encapsulated in the simple constituent labels. We present a statistical algorithm for assigning these function tags that, on text already parsed to a simplelabel level, achieves an F-measure of 87%, which rises to 99% when considering \&apos;no tag\&apos; as a valid choice. 1 Introduction Parsing sentences using statistical information gathered from a treebank was first examined a decade ago in (Chitrao and Grishman, 1990) and is by now a fairly well-studied problem ((Charniak, 1997), (Collins, 1997), (Ratnaparkhi, 1997)). But to date, the end product of the parsing process has for the most part been a bracketing with simple constituent labels like NP, VP, or SBAR.The Penn treebank contains a great deal of additional syntactic and semantic information from which to gather statistics; reproducing more of this information automatically is a goal which has so far been mostly ignored. This paper details a process by which some of this information--the function tags-- may be recovered automatically. In the Penn treebank, there are 20 tags (figure 1) that can be appended to con</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pages 598-603, Menlo Park. AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>1999</date>
<tech>Technical Report CS-99-12,</tech>
<institution>Brown University,</institution>
<contexts>
<context position="7649" citStr="Charniak, 1999" startWordPosition="1283" endWordPosition="1284">on. The value of j is chosen such that features fl..-fj are sufficiently represented in the training data; sometimes all n features are used, but often that would cause sparse data problems. Smoothing is performed on this equation exactly as before: each term is interpolated between the empirical value and the prior estimated probability, according to a value of Ai that estimates confidence. But aside from perhaps providing a new way to think about the problem, equation 3 is not particularly useful as it is--it is exactly the same as what we had before. Its real usefulness comes, as shown in (Charniak, 1999), when we move from the notion of a feature chain to a feature tree. These feature chains don\&apos;t capture everything we\&apos;d like them to. If there are two independent features that are each relatively sparse but occasionally carry a lot of information, then putting one before the other in a chain will effectively block the second from having any effect, since its information is (uselessly) conditioned on the first one, whose sparseness will completely dilute any gain. What we\&apos;d really like is to be able to have a feature tree, whereby we can condition those two sparse features independently on </context>
</contexts>
<marker>Charniak, 1999</marker>
<rawString>Eugene Charniak. 1999. A maximum-entropyinspired parser. Technical Report CS-99-12, Brown University, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh V Chitrao</author>
<author>Ralph Grishman</author>
</authors>
<title>Statistical parsing of messages.</title>
<date>1990</date>
<booktitle>In DARPA Speech and Language Workshop,</booktitle>
<pages>263--266</pages>
<contexts>
<context position="990" citStr="Chitrao and Grishman, 1990" startWordPosition="152" endWordPosition="155">nformation one would like about parts of a syntactic tree. For example, the Penn Treebank gives each constituent zero or more \&apos;function tags\&apos; indicating semantic roles and other related information not easily encapsulated in the simple constituent labels. We present a statistical algorithm for assigning these function tags that, on text already parsed to a simplelabel level, achieves an F-measure of 87%, which rises to 99% when considering \&apos;no tag\&apos; as a valid choice. 1 Introduction Parsing sentences using statistical information gathered from a treebank was first examined a decade ago in (Chitrao and Grishman, 1990) and is by now a fairly well-studied problem ((Charniak, 1997), (Collins, 1997), (Ratnaparkhi, 1997)). But to date, the end product of the parsing process has for the most part been a bracketing with simple constituent labels like NP, VP, or SBAR.The Penn treebank contains a great deal of additional syntactic and semantic information from which to gather statistics; reproducing more of this information automatically is a goal which has so far been mostly ignored. This paper details a process by which some of this information--the function tags-- may be recovered automatically. In the Penn tree</context>
</contexts>
<marker>Chitrao, Grishman, 1990</marker>
<rawString>Mahesh V. Chitrao and Ralph Grishman. 1990. Statistical parsing of messages. In DARPA Speech and Language Workshop, pages 263-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="1069" citStr="Collins, 1997" startWordPosition="166" endWordPosition="167">ves each constituent zero or more \&apos;function tags\&apos; indicating semantic roles and other related information not easily encapsulated in the simple constituent labels. We present a statistical algorithm for assigning these function tags that, on text already parsed to a simplelabel level, achieves an F-measure of 87%, which rises to 99% when considering \&apos;no tag\&apos; as a valid choice. 1 Introduction Parsing sentences using statistical information gathered from a treebank was first examined a decade ago in (Chitrao and Grishman, 1990) and is by now a fairly well-studied problem ((Charniak, 1997), (Collins, 1997), (Ratnaparkhi, 1997)). But to date, the end product of the parsing process has for the most part been a bracketing with simple constituent labels like NP, VP, or SBAR.The Penn treebank contains a great deal of additional syntactic and semantic information from which to gather statistics; reproducing more of this information automatically is a goal which has so far been mostly ignored. This paper details a process by which some of this information--the function tags-- may be recovered automatically. In the Penn treebank, there are 20 tags (figure 1) that can be appended to constituent labels i</context>
<context position="4518" citStr="Collins, 1997" startWordPosition="735" endWordPosition="736">ocative complement of \&apos;put\&apos; SBJ Subject TMP Temporal TPC Topic TTL Title V0C Vocative Grammatical DTV 0.48% LGS 3.0% PRD 18.% PUT 0.26% SBJ 78.% v0c 0.025% Figure 1: Penn treebank function tags 53.% Form/Function 37.% Topicalisation 2.2% 0.25% NOM 6.8% 2.5% TPC 100% 2.2% 1.5% ADV 11.% 4.2% 9.3% BN\&apos;F 0.072% 0.026% 0.13% DIR 8.3% 3.0% 41.% EXT 3.2% 1.2% 0.013% LOC 25.% 9.2% MNR 6.2% 2.3% PI~ 5.2% 1.9% 33.% 12.% Miscellaneous 9.5% CLR 94.% 8.8% CLF 0.34% 0.03% HLN 2.6% 0.25% TTL 3.1% 0.29% Figure 2: Categories of function tags and their relative frequencies one project that used them at all: (Collins, 1997) defines certain constituents as complements based on a combination of label and function tag information. This boolean condition is then used to train an improved parser. 2 Features We have found it useful to define our statistical model in terms of features. A \&apos;feature\&apos;, in this context, is a boolean-valued function, generally over parse tree nodes and either node labels or lexical items. Features can be fairly simple and easily read off the tree (e.g. \&apos;this node\&apos;s label is X\&apos;, \&apos;this node\&apos;s parent\&apos;s label is Y\&apos;), or slightly more complex (\&apos;this node\&apos;s head\&apos;s partof-speech is Z\&apos;)</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Annual Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="1090" citStr="Ratnaparkhi, 1997" startWordPosition="168" endWordPosition="170">ent zero or more \&apos;function tags\&apos; indicating semantic roles and other related information not easily encapsulated in the simple constituent labels. We present a statistical algorithm for assigning these function tags that, on text already parsed to a simplelabel level, achieves an F-measure of 87%, which rises to 99% when considering \&apos;no tag\&apos; as a valid choice. 1 Introduction Parsing sentences using statistical information gathered from a treebank was first examined a decade ago in (Chitrao and Grishman, 1990) and is by now a fairly well-studied problem ((Charniak, 1997), (Collins, 1997), (Ratnaparkhi, 1997)). But to date, the end product of the parsing process has for the most part been a bracketing with simple constituent labels like NP, VP, or SBAR.The Penn treebank contains a great deal of additional syntactic and semantic information from which to gather statistics; reproducing more of this information automatically is a goal which has so far been mostly ignored. This paper details a process by which some of this information--the function tags-- may be recovered automatically. In the Penn treebank, there are 20 tags (figure 1) that can be appended to constituent labels in order to indicate a</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of the Second Annual Conference on Empirical Methods in Natural Language Processing, pages 1-10.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>