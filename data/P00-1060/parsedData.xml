<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.3410015">
b&amp;quot;An Information-Theory-Based Feature Type Analysis for the
Modelling of Statistical Parsing
</title>
<sectionHeader confidence="0.548024666666667" genericHeader="method">
SUI Zhifang
, ZHAO Jun
, Dekai WU
</sectionHeader>
<affiliation confidence="0.974262">
Hong Kong University of Science &amp; Technology
Department of Computer Science
</affiliation>
<title confidence="0.496374">
Human Language Technology Center
</title>
<author confidence="0.913598">
Clear Water Bay, Hong Kong
</author>
<affiliation confidence="0.993847">
Peking University
Department of Computer Science &amp; Technology
Institute of Computational Linguistics
</affiliation>
<address confidence="0.854058">
Beijing, China
</address>
<email confidence="0.968027">
suizf@icl.pku.edu.cn, zhaojun@cs.ust.hk, dekai@cs.ust.hk
</email>
<sectionHeader confidence="0.990321" genericHeader="method">
Abstract
</sectionHeader>
<bodyText confidence="0.995695181818182">
The paper proposes an information-theory-
based method for feature types analysis in
probabilistic evaluation modelling for
statistical parsing. The basic idea is that we
use entropy and conditional entropy to
measure whether a feature type grasps some
of the information for syntactic structure
prediction. Our experiment quantitatively
analyzes several feature types power for
syntactic structure prediction and draws a
series of interesting conclusions.
</bodyText>
<sectionHeader confidence="0.998202" genericHeader="method">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99925237037037">
In the field of statistical parsing, various
probabilistic evaluation models have been
proposed where different models use different
feature types [Black, 1992] [Briscoe, 1993]
[Brown, 1991] [Charniak, 1997] [Collins, 1996]
[Collins, 1997] [Magerman, 1991] [Magerman,
1992] [Magerman, 1995] [Eisner, 1996]. How to
evaluate the different feature types effects for
syntactic parsing? The paper proposes an
information-theory-based feature types analysis
model, which uses the measures of predictive
information quantity, predictive information
gain, predictive information redundancy and
predictive information summation to
quantitatively analyse the different contextual
feature types or feature types combinations
predictive power for syntactic structure.
In the following, Section 2 describes the
probabilistic evaluation model for syntactic trees;
Section 3 proposes an information-theory-based
feature type analysis model; Section 4
introduces several experimental issues; Section 5
quantitatively analyses the different contextual
feature types or feature types combination in the
view of information theory and draws a series of
conclusion on their predictive powers for
syntactic structures.
</bodyText>
<sectionHeader confidence="0.7172" genericHeader="method">
2 The probabilistic evaluation model
</sectionHeader>
<bodyText confidence="0.998989">
for statistical syntactic parsing
Given a sentence, the task of statistical syntactic
parsing is to assign a probability to each
candidate parsing tree that conforms to the
grammar and select the one with highest
probability as the final analysis result. That is:
</bodyText>
<equation confidence="0.9192071">
)
|
(
max
arg S
T
P
T
T
best = (1)
</equation>
<bodyText confidence="0.99517075">
where S denotes the given sentence, T denotes
the set of all the candidate parsing trees that
conform to the grammar, P(T|S) denotes the
probability of parsing tree T for the given
sentence S.
The task of probabilistic evaluation model in
syntactic parsing is the estimation of P(T|S). In
the syntactic parsing model which uses rule-
based grammar, the probability of a parsing tree
can be defined as the probability of the
derivation which generates the current parsing
tree for the given sentence. That is,
</bodyText>
<equation confidence="0.966502827586207">
=
=
=
=
=
n
i
i
i
n
i
i
i
n
S
h
r
P
S
r
r
r
r
P
S
r
r
r
P
</equation>
<figure confidence="0.930444263157895">
S
T
P
1
1
1
2
1
2
1
)
,
|
(
)
,
,
,
,
|
(
)
|
,
,
,
(
)
|
(
\x16
\x16
(2)
\x0cWhere, 1
2
1 ,
,
,
</figure>
<equation confidence="0.912371">
i
r
r
</equation>
<bodyText confidence="0.939616333333333">
r \x16 denotes a derivation rule
sequence, hi denotes the partial parsing tree
derived from 1
</bodyText>
<page confidence="0.702097">
2
</page>
<equation confidence="0.946357142857143">
1 ,
,
,
i
r
r
r \x16 .
</equation>
<bodyText confidence="0.9565535">
In order to accurately estimate the parameters,
we need to select some feature types
</bodyText>
<figure confidence="0.69412605">
m
F
F
F ,
,
, 2
1 \x16 , depending on which we can
divide the contextual condition S
hi , for
predicting rule ri into some equivalence classes,
that is, ]
,
[
,
,
,
, 2
1 S
h
S
</figure>
<equation confidence="0.973233028571429">
h i
F
F
F
i
m
\x16 , so that
=
=
n
i
i
i
n
i
i
i S
h
r
P
S
h
r
P
1
1
])
,
[
|
(
)
,
|
( (3)
</equation>
<bodyText confidence="0.9914365">
According to the equation of (2) and (3), we
have the following equation:
</bodyText>
<equation confidence="0.9972508">
=
n
i
i
i S
h
r
P
S
T
P
1
])
,
[
|
(
)
|
( (4)
</equation>
<bodyText confidence="0.999692535714286">
In this way, we can get a unite expression of
probabilistic evaluation model for statistical
syntactic parsing. The difference among the
different parsing models lies mainly in that they
use different feature types or feature type
combination to divide the contextual condition
into equivalent classes. Our ultimate aim is to
determine which combination of feature types is
optimal for the probabilistic evaluation model of
statistical syntactic parsing. Unfortunately, the
state of knowledge in this regard is very limited.
Many probabilistic evaluation models have been
published inspired by one or more of these
feature types [Black, 1992] [Briscoe, 1993]
[Charniak, 1997] [Collins, 1996] [Collins, 1997]
[Magerman, 1995] [Eisner, 1996], but
discrepancies between training sets, algorithms,
and hardware environments make it difficult, if
not impossible, to compare the models
objectively. In the paper, we propose an
information-theory-based feature type analysis
model by which we can quantitatively analyse
the predictive power of different feature types or
feature type combinations for syntactic structure
in a systematic way. The conclusion is expected
to provide reliable reference for feature type
selection in the probabilistic evaluation
modelling for statistical syntactic parsing.
</bodyText>
<sectionHeader confidence="0.995885" genericHeader="method">
3 The information-theory-based
</sectionHeader>
<bodyText confidence="0.9878935">
feature type analysis model for statistical
syntactic parsing
In the prediction of stochastic events, entropy
and conditional entropy can be used to evaluate
the predictive power of different feature types.
To predict a stochastic event, if the entropy of
the event is much larger than its conditional
entropy on condition that a feature type is
known, it indicates that the feature type grasps
some of the important information for the
predicted event.
According to the above idea, we build the
information-theory-based feature type analysis
model, which is composed of four concepts:
predictive information quantity, predictive
information gain, predictive information
redundancy and predictive information
summation.
</bodyText>
<equation confidence="0.9173104">
z Predictive Information Quantity (PIQ)
)
;
( R
F
</equation>
<bodyText confidence="0.9963422">
PIQ , the predictive information quantity
of feature type F to predict derivation rule R, is
defined as the difference between the entropy of
R and the conditional entropy of R on condition
that the feature type F is known.
</bodyText>
<equation confidence="0.997600767441861">
=
=
R
r
F
f r
P
f
P
r
f
P
r
f
P
F
R
H
R
H
R
F
PIQ
, )
(
)
(
)
,
(
log
)
,
(
)
|
(
)
(
)
;
(
(5)
</equation>
<bodyText confidence="0.995682">
Predictive information quantity can be used to
measure the predictive power of a feature type in
feature type analysis.
z Predictive Information Gain (PIG)
For the prediction of rule R,
PIG(Fx;R|F1,F2,...,Fi), the predictive information
gain of taking Fx as a variant model on top of a
baseline model employing F1,F2,...,Fi as feature
type combination, is defined as the difference
between the conditional entropy of predicting R
based on feature type combination F1,F2,...,Fi
and the conditional entropy of predicting R
based on feature type combination F1,F2,...,Fi,Fx.
</bodyText>
<equation confidence="0.849970510638298">
)
6
(
)
,
,
,
(
)
,
,
(
)
,
,
,
(
)
,
,
,
,
(
log
)
,
,
,
,
(
)
,
,
,
|
(
)
,
,
|
(
)
,
,
|
;
(
</equation>
<figure confidence="0.9692106">
1
1
1
1
1
1
1
1
1
1
</figure>
<equation confidence="0.941947106796117">
r
f
f
P
f
f
P
f
f
f
P
r
f
f
f
P
r
f
f
f
P
F
F
F
R
H
F
F
R
H
F
F
R
F
PIG
i
i
x
i
x
i
R
r
F
f
F
f
F
f
x
i
x
i
i
i
x
x
x
i
i
\x16
\x16
\x16
\x16
\x16
\x16
\x16
\x16
\x16
=
=
If )
,
,
,
|
;
(
)
,
,
,
|
;
( 2
1
2
1 i
y
i
x F
F
F
R
F
PIG
F
F
F
R
F
PIG \x16
\x16 &amp;gt; ,
</equation>
<bodyText confidence="0.88912125">
then Fx is deemed to be more informative than
Fy for predicting R on top of F1,F2,...,Fi, no
matter whether PIQ(Fx;R) is larger than
PIQ(Fy;R) or not.
z Predictive Information Redundancy(PIR)
Based on the above two definitions, we can
further draw the definition of predictive
\x0cinformation redundancy as follows.
PIR(Fx,{F1,F2,...,Fi};R) denotes the redundant
information between feature type Fx and feature
type combination {F1,F2,...,Fi} in predicting R,
which is defined as the difference between
</bodyText>
<equation confidence="0.960528042553192">
PIQ(Fx;R) and PIG(Fx;R|F1,F2,...,Fi). That is,
)
,
,
,
|
;
(
)
;
(
)
};
,
,
,
{
,
(
2
1
2
1
i
x
x
i
x
F
F
F
R
F
PIG
R
F
PIQ
R
F
F
F
F
PIR
\x16
\x16
=
(7)
</equation>
<bodyText confidence="0.8391608">
Predictive information redundancy can be
used as a measure of the redundancy between
the predictive information of a feature type and
that of a feature type combination.
z Predictive Information Summation (PIS)
PIS(F1,F2,...,Fm;R), the predictive information
summation of feature type combination
F1,F2,...,Fm, is defined as the total information
that F1,F2,...,Fm can provide for the prediction of
a derivation rule. Exactly,
</bodyText>
<equation confidence="0.98595225">
=
+
=
m
i
i
i
m
</equation>
<figure confidence="0.868289526315789">
F
F
R
F
PIG
R
F
PIQ
R
F
F
F
PIS
2
1
1
1
2
1
)
,
,
|
;
(
)
;
(
)
;
,
,
,
(
\x16
\x16
(8)
4 Experimental Issues
</figure>
<subsectionHeader confidence="0.761885">
4.1 The classification of the feature
types
</subsectionHeader>
<bodyText confidence="0.998667761904762">
The predicted event of our experiment is the
derivation rule to extend the current non-
terminal node. The feature types for prediction
can be classified into two classes, history feature
types and objective feature types. In the
following, we will take the parsing tree shown in
Figure-1 as the example to explain the
classification of the feature types.
In Figure-1, the current predicted event is the
derivation rule to extend the framed non-
terminal node VP, the part connected by the
solid line belongs to history feature types, which
is the already derived partial parsing tree,
representing the structural environment of the
current non-terminal node. The part framed by
the larger rectangle belongs to the objective
feature types, which is the word sequence
containing the leaf nodes of the partial parsing
tree rooted by the current node, representing the
final objectives to be derived from the current
node.
</bodyText>
<subsectionHeader confidence="0.73598">
4.2 The corpus used in the experiment
</subsectionHeader>
<bodyText confidence="0.991389133333333">
The experimental corpus is derived from Penn
TreeBank[Marcus,1993]. We semi-
automatically assign a headword and a POS tag
to each non-terminal node. 80% of the corpus
(979,767 words) is taken as the training set, used
for estimating the various co-occurrence
probabilities, 10% of the corpus (133,814 words)
is taken as the testing set, used to calculate
predictive information quantity, predictive
information gain, predictive information
redundancy and predictive information
summation. The other 10% of the corpus
(133,814 words) is taken as the held-out set. The
grammar rule set is composed of 8,126 CFG
rules extracted from Penn TreeBank.
</bodyText>
<figure confidence="0.930861647058824">
S
VP
VP
NNP
Pierre
NNP
Vinken
M D
will
VB
join
DT
the
N N
board
IN
as
DT
a
JJ
nonexecutive
N N
director
NNP
Nov.
CD
29
.
.
NP NP NP
PP
NP
Figure-1: The classification of feature types
\x0c4.3 The smoothing method used in the
experiment
In the information-theory-based feature type
analysis model, we need to estimate joint
probability )
,
,
,
,
( 2
1 r
f
f
f
P i
\x16 . Let F1,F2,...,Fi be
the feature type series selected till now,
R
r
F
f
F
f
F
f i
i
,
,
,
, 2
2
1
1 \x16 , we use a
blended probability )
,
,
,
,
(
~
2
1 r
f
f
f
P i
\x16 to
approximate probability )
,
,
,
,
</figure>
<equation confidence="0.9008516">
( 2
1 r
f
f
f
P i
\x16 in
order to solve the sparse data problem[Bell,
1992].
=
+
+
=
i
j
j
j
i
r
f
f
f
P
w
r
P
w
r
P
w
r
f
f
f
P
</equation>
<figure confidence="0.950154666666667">
1
2
1
0
0
1
1
2
1
</figure>
<equation confidence="0.900498538461539">
)
,
,
,
,
(
)
(
)
(
)
,
,
,
,
(
~
\x16
\x16
(9)
In the above formula,
=
R
r
r
c
r
P
1
)
(
1
)
( (10)
=
R
r
r
c
r
c
r
P
0
)
(
)
(
)
( (11)
where )
(r
</equation>
<bodyText confidence="0.99545675">
c is the total number of time that r has
been seen in the corpus.
According to the escape mechanism in [Bell,
1992], we define the weights wk )
</bodyText>
<equation confidence="0.987852625">
1
( i
k
&lt;
in
the formula (9) as follows.
i
i
i
k
s
s
k
k
e
w
i
k
e
e
w
=
=
+
=
1
1
,
)
1
(
1 (12)
</equation>
<bodyText confidence="0.713274">
where ek denotes the escape probability of
context )
</bodyText>
<equation confidence="0.974545">
,
,
,
( 2
1 k
f
f
f \x16 , that is, the probability
</equation>
<bodyText confidence="0.992588666666667">
in which (f1 , f2 , ... , fk , r) is unseen in the corpus.
In such case, the blending model has to escape
to the lower contexts to approximate
</bodyText>
<figure confidence="0.962795727891156">
)
,
,
,
,
( 2
1 r
f
f
f
P k
\x16 . Exactly, escape probability is
defined as
=
=
1
,
0
0
,
)
,
,...,
,
(
)
,
,...,
,
(
2
1
2
1
k
i
k
r
f
f
f
c
r
f
f
f
d
e
R
r
k
R
r
k
k (13)
where
=
&amp;gt;
=
0
)
,
,...,
,
(
,
0
0
)
,
,...,
,
(
,
1
)
,
,...,
,
(
2
1
2
1
2
1
r
f
f
f
c
if
r
f
f
f
c
if
r
f
f
f
d
k
k
k
(14)
In the above blending model, a special
probability
=
R
r
r
c
r
P
1
)
(
1
)
( is used, where all
derivation rules are given an equal probability.
As a result, 0
)
,
,
,
,
(
~
2
1 &amp;gt;
r
f
f
f
P i
\x16 as long as
0
)
(
&amp;gt;
R
r
r
c .
</figure>
<sectionHeader confidence="0.772851" genericHeader="method">
5 The information-theory-based
</sectionHeader>
<bodyText confidence="0.992865833333333">
feature type analysis
The experiments led to a number of interesting
conclusions on the predictive power of various
feature types and feature type combinations,
which is expected to provide reliable reference
for the modelling of probabilistic parsing.
</bodyText>
<subsectionHeader confidence="0.568697">
5.1 The analysis to the predictive
</subsectionHeader>
<bodyText confidence="0.972598875">
information quantities of lexical feature
types, part-of-speech feature types and
constituent label feature types
z Goal
One of the most important variation in statistical
parsing over the last few years is that statistical
lexical information is incorporated into the
probabilistic evaluation model. Some statistical
parsing systems show that the performance is
improved after the lexical information is added.
Our research aims at a quantitative analysis of
the differences among the predictive information
quantities provided by the lexical feature types,
part-of-speech feature types and constituent
label feature types from the view of information
theory.
z Data
The experiment is conducted on the history
feature types of the nodes whose structural
distance to the current node is within 2.
In Table-1, Y in PIQ(X of Y; R) represents
the node, X represents the constitute label, the
headword or POS of the headword of the node.
In the following, the units of PIQ are bits.
</bodyText>
<subsectionHeader confidence="0.56826">
z Conclusion
</subsectionHeader>
<bodyText confidence="0.991356315789474">
Among the feature types in the same structural
position of the parsing tree, the predictive
information quantity of lexical feature type is
larger than that of part-of-speech feature type,
and the predictive information quantity of part-
of-speech feature type is larger than that of the
constituent label feature type.
\x0cTable-1: The predictive information quantity of the history feature type candidates
PIQ(X of Y; R) X= constituent label X= headword X= POS of
the headword
Y= the current node 2.3609 3.7333 2.7708
Y= the parent 1.1598 2.3253 1.1784
Y= the grandpa 0.6483 1.6808 0.6612
Y= the first right brother of the current node 0.4730 1.1525 0.7502
Y= the first left brother of the current node 0.5832 2.1511 1.2186
Y= the second right brother of the current node 0.1066 0.5044 0.2525
Y= the second left brother of the current node 0.0949 0.6171 0.2697
Y= the first right brother of the parent 0.1068 0.3717 0.2133
Y= the first left brother of the parent 0.2505 1.5603 0.6145
</bodyText>
<subsectionHeader confidence="0.888365">
5.2 The analysis to the influence of the
</subsectionHeader>
<bodyText confidence="0.974704941176471">
structural relation and the structural
distance to the predictive information
quantities of the history feature types
z Goal:
In this experiment, we wish to find out the
influence of the structural relation and structural
distance between the current node and the node
that the given feature type related to has to the
predictive information quantities of these feature
types.
z Data:
In Table-2, SR represents the structural relation
between the current node and the node that the
given feature type related to. SD represents the
structural distance between the current node and
the node that the given feature type related to.
Table-2: The predictive information quantity of the selected history feature types
</bodyText>
<figure confidence="0.978442565217391">
PIQ(constituent label
of Y; R)
SR= parent relation SR= brother relation SR= mixed parent and
brother relation
0.5832
(Y= the first left brother)
SD=1 1.1598
(Y= the parent)
0.4730
(Y= the first right brother)
0.2505
(Y= the first left brother
of the parent)
0.0949
(Y= the second left brother)
SD=2 0.6483
(Y= the grandpa)
0.1066
(Y= the second right brother)
0.1068
(Y= the first right
brother of the parent)
z Conclusion
</figure>
<bodyText confidence="0.977501908045978">
Among the history feature types which have the
same structural relation with the current node
(the relations are both parent-child relation, or
both brother relation, etc), the one which has
closer structural distance to the current node will
provide larger predictive information quantity;
Among the history feature types which have the
same structural distance to the current node, the
one which has parent relation with the current
node will provide larger predictive information
quantity than the one that has brother relation or
mixed parent and brother relation to the current
node (such as the parent&amp;apos;s brother node).
5.3 The analysis to the predictive
information quantities of the history
feature types and the objective feature
types
z Goal
Many of the existing probabilistic evaluation
models prefer to use history feature types other
than objective feature types. We select some of
history feature types and objective feature types,
and quantitatively compare their predictive
information quantities.
z Data
The history feature type we use here is the
headword of the parent, which has the largest
predictive information quantity among all the
history feature types. The objective feature types
are selected stochastically, which are the first
\x0cword and the second word in the objective word
sequence of the current node (Please see 4.1 and
Figure-1 for detailed descriptions on the selected
feature types).
Table-3: The predictive information quantity of the selected history and objective feature types
Class Feature type PIQ(Y;R)
History feature type Y= headword of the parent 2.3253
Y= the first word in the objective word sequence 3.2398
Objective feature type
Y= the second word in the objective word sequence 3.0071
z Conclusion
Either of the predictive information quantity of
the first word and the second word in the
objective word sequence is larger than that of
the headword of the parent node which has the
largest predictive information quantity among all
of the history feature type candidates. That is to
say, objective feature types may have larger
predictive power than that of the history feature
type.
5.4 The analysis to the predictive
information quantities of the objective
features types selected respectively on the
physical position information, the
heuristic information of headword and
modifier, and the exact headword
information
z Goal
Not alike the structural history feature types, the
objective feature types are sequential. Generally,
the candidates of the objective feature types are
selected according to the physical position.
However, from the linguistic viewpoint, the
physical position information can hardly grasp
the relations between the linguistic structures.
Therefore, besides the physical position
information, our research try to select the
objective feature types respectively according to
the exact headword information and the heuristic
information of headword and modifier. Through
the experiment, we hope to find out what
influence the exact headword information, the
heuristic information of headword and modifier,
and the physical position information have
respectively to the predictive information
quantities of the feature types.
z Data:
Table-4 gives the evidence for the claim.
Table-4: the predictive information quantity of the selected objective feature types
the information used to select the objective
feature types
PIQ(Y;R)
the physical position information 3.2398
(Y= the first word in the objective word sequence)
Heuristic information 1: determine whether a
word has the possibility to act as the headword of
the current constitute according to its POS
</bodyText>
<page confidence="0.585241">
3.1401
</page>
<bodyText confidence="0.999542833333333">
(Y= the first word in the objective word sequence
which has the possibility to act as the headword of
the current constitute)
Heuristic information 2: determine whether a
word has the possibility to act as the modifier of
the current constitute according to its POS
</bodyText>
<page confidence="0.715993">
3.1374
</page>
<bodyText confidence="0.996526666666667">
(Y= the first word in the objective word sequence
which has the possibility to act as the modifier of the
current constitute)
Heuristic information 3: given the current
headword, determine whether a word has the
possibility to modify the headword
</bodyText>
<page confidence="0.687666">
2.8757
</page>
<bodyText confidence="0.9748274375">
(Y= the first word in the objective word sequence
which has the possibility to modify the headword)
the exact headword information 3.7333
(Y= the headword of the current constitute)
z Conclusion
The predictive information quantity of the
headword of the current node is larger than that
of a feature type selected according to the
selected heuristic information of headword or
modifier, and larger than that of a feature type
selected according to the physical positions; The
\x0cpredictive information quantity of a feature type
selected according to the physical positions is
larger than that of a feature types selected
according to the selected heuristic information
of headword or modifier.
</bodyText>
<subsectionHeader confidence="0.663457">
5.5 The selection of the feature type
</subsectionHeader>
<bodyText confidence="0.9868435">
combination which has the optimal
predictive information summation
z Goal:
We aim at proposing a method to select the
feature types combination that has the optimal
predictive information summation for prediction.
z Approach
We use the following greedy algorithm to select
the optimal feature type combination.
In building a model, the first feature type to
be selected is the feature type which has the
largest predictive information quantity for the
prediction of the derivation rule among all of the
feature type candidates, that is,
</bodyText>
<figure confidence="0.9207671">
)
;
(
max
arg
1 R
F
PIQ
F i
Fi
</figure>
<equation confidence="0.746555">
= (15)
</equation>
<bodyText confidence="0.906472">
Where is the set of candidate feature types.
Given that the model has selected feature type
</bodyText>
<figure confidence="0.955855650793651">
combination j
F
F
F ,
,
, 2
1 \x10 , the next feature
type to be added into the model is the feature
type which has the largest predictive information
gain in all of the feature type candidate except
j
F
F
F ,
,
, 2
1 \x10 , on condition that j
F
F
F ,
,
, 2
1 \x10
is known. That is,
)
16
(
)
,
,
,
|
;
( 2
1
}
,
,
2
,
1
{
1 max
arg j
i
j
F
F
F
i
F
i
F
j F
F
F
R
F
PIG
F \x16
\x16
+ =
z Data:
</figure>
<bodyText confidence="0.992625">
Among the feature types mentioned above, the
optimal feature type combination (i.e. the feature
type combination with the largest predictive
information summation) which is composed of 6
feature types is, the headword of the current
node (type1), the headword of the parent node
(type2), the headword of the grandpa node
(type3), the first word in the objective word
sequence(type4), the first word in the objective
word sequence which have the possibility to act
as the headword of the current constitute(type5),
the headword of the right brother node(type6).
The cumulative predictive information
summation is showed in Figure-2
</bodyText>
<figure confidence="0.9774578">
0
1
2
3
4
5
6
7
type1 type2 type3 type4 type5 type6
feature type
cummulative
predicting
information
summation
Figure-2: The cumulative predictive information summation of the feature type combinations
</figure>
<sectionHeader confidence="0.878588" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998806242424243">
The paper proposes an information-theory-based
feature type analysis method, which not only
presents a series of heuristic conclusion on the
predictive power of the different feature types
and feature type combination for syntactic
parsing, but also provides a guide for the
modeling of syntactic parsing in the view of
methodology, that is, we can quantitatively
analyse the different contextual feature types or
feature types combination&amp;apos;s effect for syntactic
\x0cstructure prediction in advance. Based on these
analysis, we can select the feature type or feature
types combination that has the optimal
predictive information summation to build the
probabilistic parsing model.
However, there are still some questions to be
answered in this paper. For example, what is the
beneficial improvement in the performance after
using this method in a real parser? Whether the
improvements in PIQ will lead to the
improvement of parsing accuracy or not? In the
following research, we will incorporate these
conclusions into a real parser to see whether the
parsing accuracy can be improved or not.
Another work we will do is to do some
experimental analysis to find the impact of data
sparseness on feature type analysis, which is
critical to the performance of real systems.
The proposed feature type analysis method
can be used in not only the probabilistic
modelling for statistical syntactic parsing, but
also language modelling in more general fields
[WU, 1999a] [WU, 1999b].
</bodyText>
<sectionHeader confidence="0.989062" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993976214285714">
Bell, T.C., Cleary, J.G., Witten,I.H. 1992. Text
Compression, PRENTICE HALL, Englewood
Cliffs, New Jersey 07632, 1992
Black, E., Jelinek, F.,Lafferty, J.,Magerman, D.M.,
Mercer, R. and Roukos, S. 1992. Towards
history-based grammars: using richer models of
context in probabilistic parsing. In Proceedings of
the February 1992 DARPA Speech and Natural
Language Workshop, Arden House, NY.
Brown, P., Jelinek, F., &amp; Mercer, R. 1991. Basic
method of probabilistic context-free grammars.
IBM internal Report, Yorktown Heights, NY.
T.Briscoe and J. Carroll. 1993. Generalized LR
parsing of natural language (corpora) with
unification-based grammars. Computational
Linguistics, 19(1): 25-60
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statics. In
Proceedings of the Fourteenth National Conference
on Artificial Intelligence, AAAI Press/MIT Press,
Menlo Park.
Stanley F. Chen and Joshua Goodman. 1999. An
Empirical Study of Smoothing Techniques for
Language Modeling. Computer Speech and
Language, Vol.13, 1999
Michael John Collins. 1996. A new statistical
parser based on bigram lexical dependencies. In
Proceedings of the 34
th
Annual Meeting of the
ACL.
Michael John Collins. 1997. Three generative
lexicalised models for statistical parsing. In
Proceedings of the 35
th
Annual Meeting of the
ACL.
J.Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In
Proceedings of COLING-96, pages 340-345
Joshua Goodman. 1998. Parsing Inside-Out. PhD.
Thesis, Harvard University, 1998
Magerman, D.M. and Marcus, M.P. 1991. Pearl: a
probabilistic chart parser. In Proceedings of the
European ACL Conference, Berlin, Germany.
Magerman, D.M. and Weir, C. 1992. Probabilistic
prediction and Picky chart parsing. In Proceedings
of the February 1992 DARPA Speech and Natural
Language Workshop, Arden House, NY.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33
th
Annual Meeting of the ACL.
Mitchell P. Marcus, Beatrice Santorini &amp; Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn treebank.
Computational Linguistics 19, pages 313-330
C. E. Shannon. 1951. Prediction and Entropy of
Printed English. Bell System Technical Journal,
1951
Dekai,Wu, Sui Zhifang, Zhao Jun. 1999a. An
Information-Based Method for Selecting Feature
Types for Word Prediction. Proceedings of
Eurospeech&amp;apos;99, Budapest Hungary
Dekai, Wu, Zhao Jun, Sui Zhifang. 1999b. An
Information-Theoretic Empirical Analysis of
Dependency-Based Feature Types for Word
Prediction Models. Proceedings of EMNLP&amp;apos;99,
University of Maryland, USA
\x0c&amp;quot;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.326582">
<title confidence="0.9922415">b&amp;quot;An Information-Theory-Based Feature Type Analysis for the Modelling of Statistical Parsing</title>
<author confidence="0.733523">Dekai WU Hong Kong University of Science</author>
<author confidence="0.733523">Technology</author>
<affiliation confidence="0.99759">Department of Computer Science Human Language Technology Center</affiliation>
<address confidence="0.95119">Clear Water Bay, Hong Kong</address>
<affiliation confidence="0.999874333333333">Peking University Department of Computer Science &amp; Technology Institute of Computational Linguistics</affiliation>
<address confidence="0.979467">Beijing, China</address>
<email confidence="0.997012">suizf@icl.pku.edu.cn,zhaojun@cs.ust.hk,dekai@cs.ust.hk</email>
<abstract confidence="0.999715583333333">The paper proposes an information-theorybased method for feature types analysis in probabilistic evaluation modelling for statistical parsing. The basic idea is that we use entropy and conditional entropy to measure whether a feature type grasps some of the information for syntactic structure prediction. Our experiment quantitatively analyzes several feature types power for syntactic structure prediction and draws a series of interesting conclusions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T C Bell</author>
<author>J G Cleary</author>
<author>I H Witten</author>
</authors>
<title>Text Compression, PRENTICE HALL, Englewood Cliffs,</title>
<date>1992</date>
<booktitle>In Proceedings of the February 1992 DARPA Speech and Natural Language Workshop,</booktitle>
<location>New Jersey 07632,</location>
<marker>Bell, Cleary, Witten, 1992</marker>
<rawString>Bell, T.C., Cleary, J.G., Witten,I.H. 1992. Text Compression, PRENTICE HALL, Englewood Cliffs, New Jersey 07632, 1992 Black, E., Jelinek, F.,Lafferty, J.,Magerman, D.M., Mercer, R. and Roukos, S. 1992. Towards history-based grammars: using richer models of context in probabilistic parsing. In Proceedings of the February 1992 DARPA Speech and Natural Language Workshop, Arden House, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>F Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Basic method of probabilistic context-free grammars. IBM internal Report,</title>
<date>1991</date>
<location>Yorktown Heights, NY.</location>
<marker>Brown, Jelinek, Mercer, 1991</marker>
<rawString>Brown, P., Jelinek, F., &amp; Mercer, R. 1991. Basic method of probabilistic context-free grammars. IBM internal Report, Yorktown Heights, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalized LR parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence, AAAI</booktitle>
<volume>19</volume>
<issue>1</issue>
<pages>25--60</pages>
<publisher>Press/MIT Press,</publisher>
<location>Eugene Charniak.</location>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>T.Briscoe and J. Carroll. 1993. Generalized LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1): 25-60 Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statics. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, AAAI Press/MIT Press, Menlo Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling. Computer Speech and Language, Vol.13,</title>
<date>1999</date>
<booktitle>In Proceedings of the 34 th Annual Meeting of the ACL.</booktitle>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1999. An Empirical Study of Smoothing Techniques for Language Modeling. Computer Speech and Language, Vol.13, 1999 Michael John Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34 th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>Three generative lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35 th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1153" citStr="Collins, 1997" startWordPosition="152" endWordPosition="153">c evaluation modelling for statistical parsing. The basic idea is that we use entropy and conditional entropy to measure whether a feature type grasps some of the information for syntactic structure prediction. Our experiment quantitatively analyzes several feature types power for syntactic structure prediction and draws a series of interesting conclusions. 1 Introduction In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1991] [Magerman, 1992] [Magerman, 1995] [Eisner, 1996]. How to evaluate the different feature types effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types or feature types combinations predictive power for syntactic structure. In the following, Section 2 describes the probabilistic evaluation model for synta</context>
<context position="4417" citStr="Collins, 1997" startWordPosition="758" endWordPosition="759">syntactic parsing. The difference among the different parsing models lies mainly in that they use different feature types or feature type combination to divide the contextual condition into equivalent classes. Our ultimate aim is to determine which combination of feature types is optimal for the probabilistic evaluation model of statistical syntactic parsing. Unfortunately, the state of knowledge in this regard is very limited. Many probabilistic evaluation models have been published inspired by one or more of these feature types [Black, 1992] [Briscoe, 1993] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996], but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively. In the paper, we propose an information-theory-based feature type analysis model by which we can quantitatively analyse the predictive power of different feature types or feature type combinations for syntactic structure in a systematic way. The conclusion is expected to provide reliable reference for feature type selection in the probabilistic evaluation modelling for statistical syntactic parsing. 3 The info</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael John Collins. 1997. Three generative lexicalised models for statistical parsing. In Proceedings of the 35 th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING-96,</booktitle>
<pages>340--345</pages>
<publisher>PhD.</publisher>
<contexts>
<context position="1219" citStr="Eisner, 1996" startWordPosition="160" endWordPosition="161">hat we use entropy and conditional entropy to measure whether a feature type grasps some of the information for syntactic structure prediction. Our experiment quantitatively analyzes several feature types power for syntactic structure prediction and draws a series of interesting conclusions. 1 Introduction In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1991] [Magerman, 1992] [Magerman, 1995] [Eisner, 1996]. How to evaluate the different feature types effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types or feature types combinations predictive power for syntactic structure. In the following, Section 2 describes the probabilistic evaluation model for syntactic trees; Section 3 proposes an information-theory-based feature</context>
<context position="4449" citStr="Eisner, 1996" startWordPosition="762" endWordPosition="763"> among the different parsing models lies mainly in that they use different feature types or feature type combination to divide the contextual condition into equivalent classes. Our ultimate aim is to determine which combination of feature types is optimal for the probabilistic evaluation model of statistical syntactic parsing. Unfortunately, the state of knowledge in this regard is very limited. Many probabilistic evaluation models have been published inspired by one or more of these feature types [Black, 1992] [Briscoe, 1993] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996], but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively. In the paper, we propose an information-theory-based feature type analysis model by which we can quantitatively analyse the predictive power of different feature types or feature type combinations for syntactic structure in a systematic way. The conclusion is expected to provide reliable reference for feature type selection in the probabilistic evaluation modelling for statistical syntactic parsing. 3 The information-theory-based feature typ</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J.Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of COLING-96, pages 340-345 Joshua Goodman. 1998. Parsing Inside-Out. PhD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thesis</author>
</authors>
<title>Pearl: a probabilistic chart parser.</title>
<date>1998</date>
<booktitle>In Proceedings of the European ACL Conference,</booktitle>
<institution>Harvard University,</institution>
<location>Berlin, Germany.</location>
<marker>Thesis, 1998</marker>
<rawString>Thesis, Harvard University, 1998 Magerman, D.M. and Marcus, M.P. 1991. Pearl: a probabilistic chart parser. In Proceedings of the European ACL Conference, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
<author>C Weir</author>
</authors>
<title>Probabilistic prediction and Picky chart parsing.</title>
<date>1992</date>
<booktitle>In Proceedings of the February 1992 DARPA Speech and Natural Language Workshop,</booktitle>
<location>Arden House, NY.</location>
<marker>Magerman, Weir, 1992</marker>
<rawString>Magerman, D.M. and Weir, C. 1992. Probabilistic prediction and Picky chart parsing. In Proceedings of the February 1992 DARPA Speech and Natural Language Workshop, Arden House, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33 th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1204" citStr="Magerman, 1995" startWordPosition="158" endWordPosition="159">e basic idea is that we use entropy and conditional entropy to measure whether a feature type grasps some of the information for syntactic structure prediction. Our experiment quantitatively analyzes several feature types power for syntactic structure prediction and draws a series of interesting conclusions. 1 Introduction In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1991] [Magerman, 1992] [Magerman, 1995] [Eisner, 1996]. How to evaluate the different feature types effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types or feature types combinations predictive power for syntactic structure. In the following, Section 2 describes the probabilistic evaluation model for syntactic trees; Section 3 proposes an information-theor</context>
<context position="4434" citStr="Magerman, 1995" startWordPosition="760" endWordPosition="761">g. The difference among the different parsing models lies mainly in that they use different feature types or feature type combination to divide the contextual condition into equivalent classes. Our ultimate aim is to determine which combination of feature types is optimal for the probabilistic evaluation model of statistical syntactic parsing. Unfortunately, the state of knowledge in this regard is very limited. Many probabilistic evaluation models have been published inspired by one or more of these feature types [Black, 1992] [Briscoe, 1993] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996], but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively. In the paper, we propose an information-theory-based feature type analysis model by which we can quantitatively analyse the predictive power of different feature types or feature type combinations for syntactic structure in a systematic way. The conclusion is expected to provide reliable reference for feature type selection in the probabilistic evaluation modelling for statistical syntactic parsing. 3 The information-theory-ba</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical decision-tree models for parsing. In Proceedings of the 33 th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini &amp; Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn treebank.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C E Shannon</author>
</authors>
<title>Prediction and Entropy of Printed English. Bell System Technical Journal,</title>
<date>1951</date>
<journal>Computational Linguistics</journal>
<booktitle>Proceedings of Eurospeech&amp;apos;99,</booktitle>
<volume>19</volume>
<pages>313--330</pages>
<institution>University of Maryland, USA</institution>
<location>Dekai,Wu, Sui Zhifang, Zhao</location>
<note>x0c&amp;quot;</note>
<marker>Shannon, 1951</marker>
<rawString>Computational Linguistics 19, pages 313-330 C. E. Shannon. 1951. Prediction and Entropy of Printed English. Bell System Technical Journal, Dekai,Wu, Sui Zhifang, Zhao Jun. 1999a. An Information-Based Method for Selecting Feature Types for Word Prediction. Proceedings of Eurospeech&amp;apos;99, Budapest Hungary Dekai, Wu, Zhao Jun, Sui Zhifang. 1999b. An Information-Theoretic Empirical Analysis of Dependency-Based Feature Types for Word Prediction Models. Proceedings of EMNLP&amp;apos;99, University of Maryland, USA \x0c&amp;quot;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>