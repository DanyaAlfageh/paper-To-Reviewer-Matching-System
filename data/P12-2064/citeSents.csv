 More specifically, it is shown by cross validation that meta-learning performs at a level that is comparable to the best base classifier CITATION,,
 We followed the structural learning approach CITATION, which trains a model from both a native corpus and a GE tagged corpus (Dahlmeire and Ng, 2011), to improve the base classifiers by the additional information extracted from a native corpus,,
 Normally, different classifiers successfully predict results on different parts of the input space, so researchers have often tried to combine different classifiers together (CITATION; CITATION; CITATION; Aydn, 2009; CITATION),,
 Normally, different classifiers successfully predict results on different parts of the input space, so researchers have often tried to combine different classifiers together (CITATION; CITATION; CITATION; Aydn, 2009; CITATION),,
 Some researchers focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; CITATION; CITATION; CITATION; CITATION; De Felice, 2008),,
 Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (CITATION; CITATION),,
 For example, CITATION proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone,,
 More specifically, it is shown by cross validation that meta-learning performs at a level that is comparable to the best base classifier CITATION,,
 We followed the structural learning approach CITATION, which trains a model from both a native corpus and a GE tagged corpus (Dahlmeire and Ng, 2011), to improve the base classifiers by the additional information extracted from a native corpus,,
 We use the Stanford coreNLP toolkit1 (CITATION; CITATIONa; CITATIONb; CITATION) to extract the features,,
 Some researchers focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; CITATION; CITATION; CITATION; CITATION; De Felice, 2008),,
 Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (CITATION; CITATION),,
 For example, CITATION proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone,,
 Some researchers focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; CITATION; CITATION; CITATION; CITATION; De Felice, 2008),,
 Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (CITATION; CITATION),,
 For example, CITATION proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone,,
 We use the Stanford coreNLP toolkit1 (CITATION; CITATIONa; CITATIONb; CITATION) to extract the features,,
 We use the Stanford coreNLP toolkit1 (CITATION; CITATIONa; CITATIONb; CITATION) to extract the features,,
 Some researchers focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; CITATION; CITATION; CITATION; CITATION; De Felice, 2008),,
 Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (CITATION; CITATION),,
 For example, CITATION proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone,,
 Some researchers focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; CITATION; CITATION; CITATION; CITATION; De Felice, 2008),,
 Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (CITATION; CITATION),,
 For example, CITATION proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone,,
 Normally, different classifiers successfully predict results on different parts of the input space, so researchers have often tried to combine different classifiers together (CITATION; CITATION; CITATION; Aydn, 2009; CITATION),,
 Some researchers focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; CITATION; CITATION; CITATION; CITATION; De Felice, 2008),,
 Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (CITATION; CITATION),,
 For example, CITATION proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone,,
 Some researchers focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; CITATION; CITATION; CITATION; CITATION; De Felice, 2008),,
 Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (CITATION; CITATION),,
 For example, CITATION proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone,,
 We use the Stanford coreNLP toolkit1 (CITATION; CITATIONa; CITATIONb; CITATION) to extract the features,,
 Normally, different classifiers successfully predict results on different parts of the input space, so researchers have often tried to combine different classifiers together (CITATION; CITATION; CITATION; Aydn, 2009; CITATION),,
