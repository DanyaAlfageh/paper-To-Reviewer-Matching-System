<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.7628215">
b&apos;Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 905912,
Sydney, July 2006. c
</bodyText>
<sectionHeader confidence="0.532826" genericHeader="abstract">
2006 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.7412795">
Methods for Using Textual Entailment in
Open-Domain Question Answering
</title>
<author confidence="0.858323">
Sanda Harabagiu and Andrew Hickl
</author>
<affiliation confidence="0.890661">
Language Computer Corporation
</affiliation>
<address confidence="0.884713">
1701 North Collins Boulevard
Richardson, Texas 75080 USA
</address>
<email confidence="0.994297">
sanda@languagecomputer.com
</email>
<sectionHeader confidence="0.990649" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.980206071428571">
Work on the semantics of questions has
argued that the relation between a ques-
tion and its answer(s) can be cast in terms
of logical entailment. In this paper, we
demonstrate how computational systems
designed to recognize textual entailment
can be used to enhance the accuracy of
current open-domain automatic question
answering (Q/A) systems. In our experi-
ments, we show that when textual entail-
ment information is used to either filter or
rank answers returned by a Q/A system,
accuracy can be increased by as much as
20% overall.
</bodyText>
<sectionHeader confidence="0.628925" genericHeader="introduction">
1 Introduction
Open-Domain Question Answering (Q/A) sys-
</sectionHeader>
<bodyText confidence="0.990500181818182">
tems return a textual expression, identified from
a vast document collection, as a response to a
question asked in natural language. In the quest
for producing accurate answers, the open-domain
Q/A problem has been cast as: (1) a pipeline of
linguistic processes pertaining to the processing
of questions, relevant passages and candidate an-
swers, interconnected by several types of lexico-
semantic feedback (cf. (Harabagiu et al., 2001;
Moldovan et al., 2002)); (2) a combination of
language processes that transform questions and
candidate answers in logic representations such
that reasoning systems can select the correct an-
swer based on their proofs (cf. (Moldovan et al.,
2003)); (3) a noisy-channel model which selects
the most likely answer to a question (cf. (Echi-
habi and Marcu, 2003)); or (4) a constraint sat-
isfaction problem, where sets of auxiliary ques-
tions are used to provide more information and
better constrain the answers to individual ques-
tions (cf. (Prager et al., 2004)). While different in
their approach, each of these frameworks seeks to
approximate the forms of semantic inference that
will allow them to identify valid textual answers to
natural language questions.
Recently, the task of automatically recog-
nizing one form of semantic inference tex-
tual entailment has received much attention
from groups participating in the 2005 and 2006
PASCAL Recognizing Textual Entailment (RTE)
Challenges (Dagan et al., 2005). 1
As currently defined, the RTE task requires sys-
tems to determine whether, given two text frag-
ments, the meaning of one text could be reason-
ably inferred, or textually entailed, from the mean-
ing of the other text. We believe that systems de-
veloped specifically for this task can provide cur-
rent question-answering systems with valuable se-
mantic information that can be leveraged to iden-
tify exact answers from ranked lists of candidate
answers. By replacing the pairs of texts evaluated
in the RTE Challenge with combinations of ques-
tions and candidate answers, we expect that textual
entailment could provide yet another mechanism
for approximating the types of inference needed
in order answer questions accurately.
In this paper, we present three different methods
for incorporating systems for textual entailment
into the traditional Q/A architecture employed by
many current systems. Our experimental results
indicate that (even at their current level of per-
formance) textual entailment systems can substan-
tially improve the accuracy of Q/A, even when no
other form of semantic inference is employed.
The remainder of the paper is organized as fol-
</bodyText>
<figure confidence="0.956155860465116">
1
http://www.pascal-network.org/Challenges/RTE
905
\x0cProcessing
Question
Module
(QP)
Passage
Retrieval
Module
(PR)
Answer Type
Expected Keywords
Module
Answer
Processing
(AP)
Ranked
List
of
Answers
TEXTUAL
ENTAILMENT
Method 1
TEXTUAL
ENTAILMENT
Method 2
List of Questions
Generation
AUTOQUAB
Ranked List of Paragraphs
TEXTUAL
ENTAILMENT
Method 3
Entailed Questions
Entailed Paragraphs
List of Entailed Paragraphs
Question
Documents
Answers
AnswersM1
AnswersM2
AnswersM3
</figure>
<sectionHeader confidence="0.582446" genericHeader="method">
QUESTION ANSWERING SYSTEM
</sectionHeader>
<figureCaption confidence="0.992537">
Figure 1: Integrating Textual Entailment in Q/A.
</figureCaption>
<bodyText confidence="0.975896375">
lows. Section 2 describes the three methods of
using textual entailment in open-domain question
answering that we have identified, while Section 3
presents the textual entailment system we have
used. Section 4 details our experimental methods
and our evaluation results. Finally, Section 5 pro-
vides a discussion of our findings, and Section 6
summarizes our conclusions.
</bodyText>
<sectionHeader confidence="0.754187" genericHeader="method">
2 Integrating Textual Entailment in
</sectionHeader>
<subsectionHeader confidence="0.73952">
Question Answering
</subsectionHeader>
<bodyText confidence="0.998200811320754">
In this section, we describe three different meth-
ods for integrating a textual entailment (TE) sys-
tem into the architecture of an open-domain Q/A
system.
Work on the semantics of questions (Groe-
nendijk, 1999; Lewis, 1988) has argued that the
formal answerhood relation found between a ques-
tion and a set of (correct) answers can be cast
in terms of logical entailment. Under these ap-
proaches (referred to as licensing by (Groenendijk,
1999) and aboutness by (Lewis, 1988)), p is con-
sidered to be an answer to a question ?q iff ?q logi-
cally entails the set of worlds in which p is true(i.e.
?p). While the notion of textual entailment has
been defined far less rigorously than logical en-
tailment, we believe that the recognition of textual
entailment between a question and a set of candi-
date answers or between a question and ques-
tions generated from answers can enable Q/A
systems to identify correct answers with greater
precision than current keyword- or pattern-based
techniques.
As illustrated in Figure 1, most open-domain
Q/A systems generally consist of a sequence of
three modules: (1) a question processing (QP)
module; (2) a passage retrieval (PR) module; and
(3) an answer processing (AP) module. Questions
are first submitted to a QP module, which extracts
a set of relevant keywords from the text of the
question and identifies the questions expected an-
swer type (EAT). Keywords along with the ques-
tions EAT are then used by a PR module to re-
trieve a ranked list of paragraphs which may con-
tain answers to the question. These paragraphs are
then sent to an AP module, which extracts an ex-
act candidate answer from each passage and then
ranks each candidate answer according to the like-
lihood that it is a correct answer to the original
question.
Method 1. In Method 1, each of a ranked list of
answers that do not meet the minimum conditions
for TE are removed from consideration and then
re-ranked based on the entailment confidence (a
real-valued number ranging from 0 to 1) assigned
by the TE system to each remaining example. The
system then outputs a new set of ranked answers
which do not contain any answers that are not en-
tailed by the users question.
Table 1 provides an example where Method 1
could be used to make the right prediction for a set
of answers. Even though A1 was ranked in sixth
position, the identification of a high-confidence
positive entailment enabled it to be returned as the
</bodyText>
<page confidence="0.997542">
906
</page>
<bodyText confidence="0.782806">
\x0ctop answer. In contrast, the recognition of a neg-
ative entailment for A2 caused this answer to be
dropped from consideration altogether.
Q1: What did Peter Minuit buy for the equivalent of $24.00?
</bodyText>
<figure confidence="0.496191">
Rank1 TE Rank2 Answer Text
A1 6th
YES
(0.89)
</figure>
<page confidence="0.702822">
1st
</page>
<bodyText confidence="0.894464666666667">
Everyone knows that, back in 1626, Peter Mi-
nuit bought Manhattan from the Indians for $24
worth of trinkets.
</bodyText>
<equation confidence="0.817000666666667">
A2 1st
NO
(0.81)
</equation>
<bodyText confidence="0.941932">
In 1626, an enterprising Peter Minuit flagged
down some passing locals, plied them with
beads, cloth and trinkets worth an estimated
$24, and walked away with the whole island.
</bodyText>
<tableCaption confidence="0.954602">
Table 1: Re-ranking of answers by Method 1.
Method 2. Since AP is often a resource-
</tableCaption>
<bodyText confidence="0.961997736842105">
intensive process for most Q/A systems, we ex-
pect that TE information can be used to limit the
number of passages considered during AP. As il-
lustrated in Method 2 in Figure 1, lists of passages
retrieved by a PR module can either be ranked (or
filtered) using TE information. Once ranking is
complete, answer extraction takes place only on
the set of entailed passages that the system consid-
ers likely to contain a correct answer to the users
question.
Method 3. In previous work (Harabagiu et al.,
2005b), we have described techniques that can be
used to automatically generate well-formed natu-
ral language questions from the text of paragraphs
retrieved by a PR module. In our current system,
sets of automatically-generated questions (AGQ)
are created using a stand-alone AutoQUAB gen-
eration module, which assembles question-answer
pairs (known as QUABs) from the top-ranked pas-
sages returned in response to a question. Table 2
lists some of the questions that this module has
produced for the question Q2: How hot does the
inside of an active volcano get?.
Q2: How hot does the inside of an active volcano get?
A2 Tamagawa University volcano expert Takeyo Kosaka said lava frag-
ments belched out of the mountain on January 31 were as hot as 300
degrees Fahrenheit. The intense heat from a second eruption on
Tuesday forced rescue operations to stop after 90 minutes. Because
of the high temperatures, the bodies of only five of the volcanos
initial victims were retrieved.
Positive Entailment
AGQ1 What temperature were the lava fragments belched out of the moun-
tain on January 31?
AGQ2 How many degrees Fahrenheit were the lava fragments belched out
of the mountain on January 31?
Negative Entailment
AGQ3 When did rescue operations have to stop?
AGQ4 How many bodies of the volcanos initial victims were retrieved?
</bodyText>
<tableCaption confidence="0.850705">
Table 2: TE between AGQs and user question.
</tableCaption>
<bodyText confidence="0.99027146875">
Following (Groenendijk, 1999), we expect that
if a question ?q logically entails another question
?q0, then some subset of the answers entailed by
?q0 should also be interpreted as valid answers to
?q. By establishing TE between a question and
AGQs derived from passages identified by the Q/A
system for that question, we expect we can iden-
tify a set of answer passages that contain correct
answers to the original question. For example, in
Table 2, we find that entailment between questions
indicates the correctness of a candidate answer:
here, establishing that Q2 entails AGQ1 and AGQ2
(but not AGQ3 or AGQ4) enables the system to se-
lect A2 as the correct answer.
When at least one of the AGQs generated by
the AutoQUAB module is entailed by the original
question, all AGQs that do not reach TE are fil-
tered from consideration; remaining passages are
assigned an entailment confidence score and are
sent to the AP module in order to provide an ex-
act answer to the question. Following this pro-
cess, candidate answers extracted from the AP
module were then re-associated with their AGQs
and resubmitted to the TE system (as in Method
1). Question-answer pairs deemed to be posi-
tive instances of entailment were then stored in a
database and used as additional training data for
the AutoQUAB module. When no AGQs were
found to be entailed by the original question, how-
ever, passages were ranked according to their en-
tailment confidence and sent to AP for further pro-
cessing and validation.
</bodyText>
<sectionHeader confidence="0.967161" genericHeader="method">
3 The Textual Entailment System
</sectionHeader>
<bodyText confidence="0.973908">
Processing textual entailment, or recognizing
whether the information expressed in a text can be
inferred from the information expressed in another
text, can be performed in four ways. We can try to
</bodyText>
<listItem confidence="0.9597802">
(1) derive linguistic information from the pair of
texts, and cast the inference recognition as a clas-
sification problem; or (2) evaluate the probability
that an entailment can exist between the two texts;
(3) represent the knowledge from the pair of texts
</listItem>
<bodyText confidence="0.993687733333333">
in some representation language that can be asso-
ciated with an inferential mechanism; or (4) use
the classical AI definition of entailment and build
models of the world in which the two texts are re-
spectively true, and then check whether the models
associated with one text are included in the mod-
els associated with the other text. Although we be-
lieve that each of these methods should be inves-
tigated fully, we decided to focus only on the first
method, which allowed us to build the TE system
illustrated in Figure 2.
Our TE system consists of (1) a Preprocess-
ing Module, which derives linguistic knowledge
from the text pair; (2) an Alignment Module, which
takes advantage of the notions of lexical alignment
</bodyText>
<page confidence="0.948377">
907
</page>
<figure confidence="0.994048804878049">
\x0cClassifier
YES
NO
Textual
Input 2
Textual
Input 1
Preprocessing Training
Corpora
Features
Alignment
Dependency
Features
Paraphrase
Features
Semantic/
Pragmatic
Features
Coreference
Coreference
NE
Aliasing
Concept
Paraphrase Acquisition
WWW
Lexical Alignment
Alignment Module
Feature Extraction
Classification Module
LexicoSemantic
PoS/ NER
Synonyms/
Antonyms
Normalization
Syntactic
Semantic
Temporal
Parsing
Modality Detection Speech Act Recognition
Pragmatics
Factivity Detection Belief Recognition
</figure>
<figureCaption confidence="0.993771">
Figure 2: Textual Entailment Architecture.
</figureCaption>
<bodyText confidence="0.995375375">
and textual paraphrases; and (3) a Classification
Module, which uses a machine learning classifier
(based on decision trees) to make an entailment
judgment for each pair of texts.
As described in (Hickl et al., 2006), the Prepro-
cessing module is used to syntactically parse texts,
identify the semantic dependencies of predicates,
label named entities, normalize temporal and spa-
tial expressions, resolve instances of coreference,
and annotate predicates with polarity, tense, and
modality information.
Following preprocessing, texts are sent to
an Alignment Module which uses a Maximum
Entropy-based classifier in order to estimate the
probability that pairs of constituents selected from
texts encode corresponding information that could
be used to inform an entailment judgment. This
module assumes that since sets of entailing texts
necessarily predicate about the same set of indi-
viduals or events, systems should be able to iden-
tify elements from each text that convey similar
types of presuppositions. Examples of predicates
and arguments aligned by this module are pre-
sented in Figure 3.
</bodyText>
<figure confidence="0.9848846">
Pred: Pred:
ArgMLOC
the inside of an active volcano
an active volcano
How hot
the mountain
the lava fragments
Original Question
AutoQUAB
What temperature
get hot
be temperature
Arg1
Answer Type
Arg1
</figure>
<figureCaption confidence="0.999121">
Figure 3: Alignment Graph
</figureCaption>
<bodyText confidence="0.993684125">
Aligned constituents are then used to extract
sets of phrase-level alternations (or paraphrases)
from the WWW that could be used to capture cor-
respondences between texts longer than individual
constituents. The top 8 candidate paraphrases for
two of the aligned elements from Figure 3 are pre-
sented in Table 3.
Finally, the Classification Module employs a
</bodyText>
<subsectionHeader confidence="0.89691">
Judgment Paraphrase
</subsectionHeader>
<bodyText confidence="0.845453">
YES lava fragments in pyroclastic flows can reach 400 degrees
YES an active volcano can get up to 2000 degrees
NO an active volcano above you are slopes of 30 degrees
YES the active volcano with steam reaching 80 degrees
YES lava fragments such as cinders may still be as hot as 300 degrees
NO lava is a liquid at high temperature: typically from 700 degrees
</bodyText>
<tableCaption confidence="0.881793">
Table 3: Phrase-Level Alternations
</tableCaption>
<bodyText confidence="0.998876857142857">
decision tree classifier in order to determine
whether an entailment relationship exists for each
pair of texts. This classifier is learned using fea-
tures extracted from the previous modules, includ-
ing features derived from (1) the (lexical) align-
ment of the texts, (2) syntactic and semantic de-
pendencies discovered in each text passage, (3)
paraphrases derived from web documents, and (4)
semantic and pragmatic annotations. (A complete
list of features can be found in Figure 4.) Based on
these features, the classifier outputs both an entail-
ment judgment (either yes or no) and a confidence
value, which is used to rank answers or paragraphs
in the architecture illustrated in Figure 1.
</bodyText>
<subsectionHeader confidence="0.99916">
3.1 Lexical Alignment
</subsectionHeader>
<bodyText confidence="0.998710111111111">
Several approaches to the RTE task have argued
that the recognition of textual entailment can be
enhanced when systems are able to identify
or align corresponding entities, predicates, or
phrases found in a pair of texts. In this section,
we show that by using a machine learning-based
classifier which combines lexico-semantic infor-
mation from a wide range of sources, we are able
to accurately identify aligned constituents in pairs
of texts with over 90% accuracy.
We believe the alignment of corresponding en-
tities can be cast as a classification problem which
uses lexico-semantic features in order to compute
an alignment probability p(a), which corresponds
to the likelihood that a term selected from one text
entails a term from another text. We used con-
stituency information from a chunk parser to de-
compose the pair of texts into a set of disjoint seg-
</bodyText>
<page confidence="0.991091">
908
</page>
<table confidence="0.947176">
\x0cALIGNMENT FEATURES: These three features are derived from the
results of the lexical alignment classification.
\x051\x05 LONGEST COMMON STRING: This feature represents the longest
contiguous string common to both texts.
\x052\x05 UNALIGNED CHUNK: This feature represents the number of
chunks in one text that are not aligned with a chunk from the other
\x053\x05 LEXICAL ENTAILMENT PROBABILITY: This feature is defined in
(Glickman and Dagan, 2005).
DEPENDENCY FEATURES: These four features are computed
</table>
<bodyText confidence="0.906584642857143">
from the PropBank-style annotations assigned by the semantic
parser.
\x051\x05 ENTITY-ARG MATCH: This is a boolean feature which fires when
aligned entities were assigned the same argument role label.
\x052\x05 ENTITY-NEAR-ARG MATCH: This feature is collapsing the ar-
guments Arg1 and Arg2 (as well as the ArgM subtypes) into single
categories for the purpose of counting matches.
\x053\x05 PREDICATE-ARG MATCH: This boolean feature is flagged when
at least two aligned arguments have the same role.
\x054\x05 PREDICATE-NEAR-ARG MATCH: This feature is collapsing the ar-
guments Arg1 and Arg2 (as well as the ArgM subtypes) into single
categories for the purpose of counting matches.
PARAPHRASE FEATURES: These three features are derived from
the paraphrases acquired for each pair.
</bodyText>
<listItem confidence="0.906134045454546">
\x051\x05 SINGLE PATTERN MATCH: This is a boolean feature which fired
when a paraphrase matched either of the texts.
\x052\x05 BOTH PATTERN MATCH: This is a boolean feature which fired
when paraphrases matched both texts.
\x053\x05 CATEGORY MATCH: This is a boolean feature which fired when
paraphrases could be found from the same paraphrase cluster that
matched both texts.
SEMANTIC/PRAGMATIC FEATURES: These six features are ex-
tracted by the preprocessing module.
\x051\x05 NAMED ENTITY CLASS: This feature has a different value for
each of the 150 named entity classes.
\x052\x05 TEMPORAL NORMALIZATION: This boolean feature is flagged
when the temporal expressions are normalized to the same ISO
9000 equivalents.
\x053\x05 MODALITY MARKER: This boolean feature is flagged when the
two texts use the same modal verbs.
\x054\x05 SPEECH-ACT: This boolean feature is flagged when the lexicons
indicate the same speech act in both texts.
\x055\x05 FACTIVITY MARKER: This boolean feature is flagged when the
factivity markers indicate either TRUE or FALSE in both texts simul-
taneously.
\x056\x05 BELIEF MARKER: This boolean feature is set when the belief
markers indicate either TRUE or FALSE in both texts simultaneously.
CONTRAST FEATURES: These six features are derived from the
opposing information provided by antonymy relations or chains.
\x051\x05 NUMBER OF LEXICAL ANTONYMY RELATIONS: This feature
counts the number of antonyms from WordNet that are discovered
between the two texts.
\x052\x05 NUMBER OF ANTONYMY CHAINS: This feature counts the num-
ber of antonymy chains that are discovered between the two texts.
\x053\x05 CHAIN LENGTH: This feature represents a vector with the
lengths of the antonymy chains discovered between the two texts.
\x054\x05 NUMBER OF GLOSSES: This feature is a vector representing the
number of Gloss relations used in each antonymy chain.
\x055\x05 NUMBER OF MORPHOLOGICAL CHANGES: This feature is a vector
representing the number of Morphological-Derivation relations found
in each antonymy chain.
\x056\x05 NUMBER OF NODES WITH DEPENDENCIES: This feature is a vec-
tor indexing the number of nodes in each antonymy chain that con-
tain dependency relations.
\x057\x05 TRUTH-VALUE MISMATCH: This is a boolean feature which fired
when two aligned predicates differed in any truth value.
\x058\x05 POLARITY MISMATCH: This is a boolean feature which fired
when predicates were assigned opposite polarity values.
</listItem>
<figureCaption confidence="0.993437">
Figure 4: Features Used in Classifying Entailment
</figureCaption>
<bodyText confidence="0.982397375">
ments known as alignable chunks. Alignable
chunks from one text (Ct) and the other text (Ch)
are then assembled into an alignment matrix (Ct
Ch). Each pair of chunks (p Ct Ch) is then
submitted to a Maximum Entropy-based classi-
fier which determines whether or not the pair of
chunks represents a case of lexical entailment.
Three classes of features were used in the
Alignment Classifier: (1) a set of statistical fea-
tures (e.g. cosine similarity), (2) a set of lexico-
semantic features (including WordNet Similar-
ity (Pedersen et al., 2004), named entity class
equality, and part-of-speech equality), and (3) a set
of string-based features (such as Levenshtein edit
distance and morphological stem equality).
As in (Hickl et al., 2006), we used a two-
step approach to obtain sufficient training data
for the Alignment Classifier. First, humans were
tasked with annotating a total of 10,000 align-
ment pairs (extracted from the 2006 PASCAL De-
velopment Set) as either positive or negative in-
stances of alignment. These annotations were then
used to train a hillclimber that was used to anno-
tate a larger set of 450,000 alignment pairs se-
lected at random from the training corpora de-
scribed in Section 3.3. These machine-annotated
examples were then used to train the Maximum
Entropy-based classifier that was used in our TE
system. Table 4 presents results from TEs linear-
and Maximum Entropy-based Alignment Classi-
fiers on a sample of 1000 alignment pairs selected
at random from the 2006 PASCAL Test Set.
</bodyText>
<table confidence="0.96712">
Classifier Training Set Precision Recall F-Measure
Linear 10K pairs 0.837 0.774 0.804
Maximum Entropy 10K pairs 0.881 0.851 0.866
Maximum Entropy 450K pairs 0.902 0.944 0.922
</table>
<tableCaption confidence="0.99934">
Table 4: Performance of Alignment Classifier
</tableCaption>
<subsectionHeader confidence="0.999243">
3.2 Paraphrase Acquisition
</subsectionHeader>
<bodyText confidence="0.999611238095238">
Much recent work on automatic paraphras-
ing (Barzilay and Lee, 2003) has used relatively
simple statistical techniques to identify text pas-
sages that contain the same information from par-
allel corpora. Since sentence-level paraphrases are
generally assumed to contain information about
the same event, these approaches have generally
assumed that all of the available paraphrases for
a given sentence will include at least one pair of
entities which can be used to extract sets of para-
phrases from text.
The TE system uses a similar approach to gather
phrase-level alternations for each entailment pair.
In our system, the two highest-confidence en-
tity alignments returned by the Lexical Alignment
module were used to construct a query which
was used to retrieve the top 500 documents from
Google, as well as all matching instances from our
training corpora described in Section 3.3. This
method did not always extract true paraphrases of
either texts. In order increase the likelihood that
</bodyText>
<page confidence="0.988124">
909
</page>
<bodyText confidence="0.9972162">
\x0conly true paraphrases were considered as phrase-
level alternations for an example, extracted sen-
tences were clustered using complete-link cluster-
ing using a technique proposed in (Barzilay and
Lee, 2003).
</bodyText>
<subsectionHeader confidence="0.998128">
3.3 Creating New Sources of Training Data
</subsectionHeader>
<bodyText confidence="0.997949416666667">
In order to obtain more training data for our TE
system, we extracted more than 200,000 examples
of textual entailment from large newswire corpora.
Positive Examples. Following an idea pro-
posed in (Burger and Ferro, 2005), we created a
corpus of approximately 101,000 textual entail-
ment examples by pairing the headline and first
sentence from newswire documents. In order to
increase the likelihood of including only positive
examples, pairs were filtered that did not share an
entity (or an NP) in common between the headline
and the first sentence
</bodyText>
<subsectionHeader confidence="0.834988">
Judgment Example
</subsectionHeader>
<bodyText confidence="0.9124287">
YES Text-1: Sydney newspapers made a secret deal not to report
on the fawning and spending during the citys successful bid
for the 2000 Olympics, former Olympics Minister Bruce Baird
said today.
Text-2: Papers Said To Protect Sydney Bid
YES Text-1: An IOC member expelled in the Olympic bribery
scandal was consistently drunk as he checked out Stockholms
bid for the 2004 Games and got so offensive that he was
thrown out of a dinner party, Swedish officials said.
Text-2: Officials Say IOC Member Was Drunk
</bodyText>
<tableCaption confidence="0.796397">
Table 5: Positive Examples
</tableCaption>
<bodyText confidence="0.96640375">
Negative Examples. Two approaches were
used to gather negative examples for our training
set. First, we extracted 98,000 pairs of sequen-
tial sentences that included mentions of the same
named entity from a large newswire corpus. We
also extracted 21,000 pairs of sentences linked by
connectives such as even though, in contrast and
but.
</bodyText>
<table confidence="0.889698285714286">
Judgment Example
NO Text-1: One player losing a close friend is Japanese pitcher
Hideki Irabu, who was befriended by Wells during spring
training last year.
Text-2: Irabu said he would take Wells out to dinner when the
Yankees visit Toronto.
NO Text-1: According to the professor, present methods of clean-
</table>
<tableCaption confidence="0.4309854">
ing up oil slicks are extremely costly and are never completely
efficient.
Text-2: In contrast, he stressed, Clean Mag has a 100 percent
pollution retrieval rate, is low cost and can be recycled.
Table 6: Negative Examples
</tableCaption>
<sectionHeader confidence="0.996633" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999530772727273">
In this section, we describe results from four sets
of experiments designed to explore how textual
entailment information can be used to enhance the
quality of automatic Q/A systems. We show that
by incorporating features from TE into a Q/A sys-
tem which employs no other form of textual infer-
ence, we can improve accuracy by more than 20%
over a baseline.
We conducted our evaluations on a set of
500 factoid questions selected randomly from
questions previously evaluated during the annual
TREC Q/A evaluations. 2 Of these 500 questions,
335 (67.0%) were automatically assigned an an-
swer type from our systems answer type hierar-
chy ; the remaining 165 (33.0%) questions were
classified as having an unknown answer type. In
order to provide a baseline for our experiments,
we ran a version of our Q/A system, known as
FERRET (Harabagiu et al., 2005a), that does not
make use of textual entailment information when
identifying answers to questions. Results from this
baseline are presented in Table 7.
</bodyText>
<table confidence="0.982772333333333">
Question Set Questions Correct Accuracy MRR
Known Answer Types 335 107 32.0% 0.3001
Unknown Answer Types 265 81 30.6% 0.2987
</table>
<tableCaption confidence="0.97878">
Table 7: Q/A Accuracy without TE
</tableCaption>
<bodyText confidence="0.966455222222222">
The performance of the TE system described
in Section 3 was first evaluated in the 2006 PAS-
CAL RTE Challenge. In this task, systems were
tasked with determining whether the meaning of
a sentence (referred to as a hypothesis) could be
reasonably inferred from the meaning of another
sentence (known as a text). Four types of sen-
tence pairs were evaluated in the 2006 RTE Chal-
lenge, including: pairs derived from the output of
</bodyText>
<listItem confidence="0.8909745">
(1) automatic question-answering (QA) systems,
(2) information extraction systems (IE), (3) in-
</listItem>
<bodyText confidence="0.70518575">
formation retrieval (IR) systems, and (4) multi-
document summarization (SUM) systems. The ac-
curacy of our TE system across these four tasks is
presented in Table 8.
</bodyText>
<table confidence="0.899084">
Training Data
Development Set Additional Corpora
Number of Examples 800 201,000
Task
QA-test 0.5750 0.6950
IE-test 0.6450 0.7300
IR-test 0.6200 0.7450
SUM-test 0.7700 0.8450
Overall Accuracy 0.6525 0.7538
</table>
<tableCaption confidence="0.998744">
Table 8: Accuracy on the 2006 RTE Test Set
</tableCaption>
<bodyText confidence="0.989526142857143">
In previous work (Hickl et al., 2006), we have
found that the type and amount of training data
available to our TE system significantly (p &lt; 0.05)
impacted its performance on the 2006 RTE Test
Set. When our system was trained on the training
corpora described in Section 3.3, the overall accu-
racy of the system increased by more than 10%,
</bodyText>
<page confidence="0.614282">
2
</page>
<table confidence="0.281908">
Text Retrieval Conference (http://trec.nist.gov)
</table>
<page confidence="0.946085">
910
</page>
<bodyText confidence="0.998809583333333">
\x0cfrom 65.25% to 75.38%. In order to provide train-
ing data that replicated the task of recognizing en-
tailment between a question and an answer, we as-
sembled a corpus of 5000 question-answer pairs
selected from answers that our baseline Q/A sys-
tem returned in response to a new set of 1000 ques-
tions selected from the TREC test sets. 2500 posi-
tive training examples were created from answers
identified by human annotators to be correct an-
swers to a question, while 2500 negative examples
were created by pairing questions with incorrect
answers returned by the Q/A system.
After training our TE system on this corpus, we
performed the following four experiments:
Method 1. In the first experiment, the ranked
lists of answers produced by the Q/A system were
submitted to the TE system for validation. Un-
der this method, answers that were not entailed
by the question were removed from consideration;
the top-ranked entailed answer was then returned
as the systems answer to the question. Results
from this method are presented in Table 9.
Method 2. In this experiment, entailment in-
formation was used to rank passages returned by
the PR module. After an initial relevance rank-
ing was determined from the PR engine, the top
50 passages were paired with the original question
and were submitted to the TE system. Passages
were re-ranked using the entailment judgment and
the entailment confidence computed for each pair
and then submitted to the AP module. Features
derived from the entailment confidence were then
combined with the keyword- and relation-based
features described in (Harabagiu et al., 2005a) in
order to produce a final ranking of candidate an-
swers. Results from this method are presented in
</bodyText>
<tableCaption confidence="0.782141">
Table 9.
Method 3. In the third experiment, TE was used
</tableCaption>
<bodyText confidence="0.987072807692308">
to select AGQs that were entailed by the question
submitted to the Q/A system. Here, AutoQUAB
was used to generate questions for the top 50 can-
didate answers identified by the system. When at
least one of the top 50 AGQs were entailed by
the original question, the answer passage associ-
ated with the top-ranked entailed question was re-
turned as the answer. When none of the top 50
AGQs were entailed by the question, question-
answer pairs were re-ranked based on the entail-
ment confidence, and the top-ranked answer was
returned. Results for both of these conditions are
presented in Table 9.
Hybrid Method. Finally, we found that the
best results could be obtained by combining as-
pects of each of these three strategies. Under this
approach, candidate answers were initially ranked
using features derived from entailment classifica-
tions performed between (1) the original question
and each candidate answer and (2) the original
question and the AGQ generated from each can-
didate answer. Once a ranking was established,
answers that were not judged to be entailed by
the question were also removed from final rank-
ing. Results from this hybrid method are provided
in Table 9.
</bodyText>
<table confidence="0.995782142857143">
Known EAT Unknown EAT
Acc MRR Acc MRR
Baseline 32.0% 0.3001 30.6% 0.2978
Method 1 44.1% 0.4114 39.5% 0.3833
Method 2 52.4% 0.5558 42.7% 0.4135
Method 3 41.5% 0.4257 37.5% 0.3575
Hybrid 53.9% 0.5640 41.9% 0.4010
</table>
<tableCaption confidence="0.969346">
Table 9: Q/A Performance with TE
</tableCaption>
<sectionHeader confidence="0.997761" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999524766666667">
The experiments reported in this paper suggest
that current TE systems may be able to provide
open-domain Q/A systems with the forms of se-
mantic inference needed to perform accurate an-
swer validation. While probabilistic or web-based
methods for answer validation have been previ-
ously explored in the literature (Magnini et al.,
2002), these approaches have modeled the rela-
tionship between a question and a (correct) answer
in terms of relevance and have not tried to approx-
imate the deeper semantic phenomena that are in-
volved in determining answerhood.
Our work suggests that considerable gains in
performance can be obtained by incorporating TE
during both answer processing and passage re-
trieval. While best results were obtained using
the Hybrid Method (which boosted performance
by nearly 28% for questions with known EATs),
each of the individual methods managed to boost
the overall accuracy of the Q/A system by at least
7%. When TE was used to filter non-entailed an-
swers from consideration (Method 1), the over-
all accuracy of the Q/A system increased by 12%
over the baseline (when an EAT could be iden-
tified) and by nearly 9% (when no EAT could
be identified). In contrast, when entailment in-
formation was used to rank passages and candi-
date answers, performance increased by 22% and
10% respectively. Somewhat smaller performance
gains were achieved when TE was used to select
</bodyText>
<page confidence="0.969399">
911
</page>
<bodyText confidence="0.994499166666667">
\x0camongst AGQs generated by our Q/A systems
AutoQUAB module (Method 3). We expect that
by adding features to TE system specifically de-
signed to account for the semantic contributions
of a questions EAT, we may be able to boost the
performance of this method.
</bodyText>
<sectionHeader confidence="0.999311" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9992312">
In this paper, we discussed three different ways
that a state-of-the-art textual entailment system
could be used to enhance the performance of an
open-domain Q/A system. We have shown that
when textual entailment information is used to ei-
ther filter or rank candidate answers returned by a
Q/A system, Q/A accuracy can be improved from
32% to 52% (when an answer type can be de-
tected) and from 30% to 40% (when no answer
type can be detected). We believe that these results
suggest that current supervised machine learning
approaches to the recognition of textual entailment
may provide open-domain Q/A systems with the
inferential information needed to develop viable
answer validation systems.
</bodyText>
<sectionHeader confidence="0.999151" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9740295">
This material is based upon work funded in whole
or in part by the U.S. Government and any opin-
ions, findings, conclusions, or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the views of the U.S.
Government.
</bodyText>
<sectionHeader confidence="0.983843" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999457739726027">
Regina Barzilay and Lillian Lee. 2003. Learning
to paraphrase: An unsupervised approach using
multiple-sequence alignment. In HLT-NAACL.
John Burger and Lisa Ferro. 2005. Generating an En-
tailment Corpus from News Headlines. In Proceed-
ings of the ACL Workshop on Empirical Modeling of
Semantic Equivalence and Entailment, pages 4954.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL Recognizing Textual Entail-
ment Challenge. In Proceedings of the PASCAL
Challenges Workshop.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of the 41st Meeting of the Association
for Computational Linguistics.
Oren Glickman and Ido Dagan. 2005. A Probabilistic
Setting and Lexical Co-occurrence Model for Tex-
tual Entailment. In Proceedings of the ACL Work-
shop on Empirical Modeling of Semantic Equiva-
lence and Entailment, Ann Arbor, USA.
Jeroen Groenendijk. 1999. The logic of interrogation:
Classical version. In Proceedings of the Ninth Se-
mantics and Linguistics Theory Conference (SALT
IX), Ithaca, NY.
Sanda Harabagiu, Dan Moldovan, Marius Pasca, Rada
Mihalcea, Mihai Surdeanu, Razvan Bunsecu, Rox-
ana Girju, Vasile Rus, and Paul Morarescu. 2001.
The Role of Lexico-Semantic Feedback in Open-
Domain Textual Question-Answering. In Proceed-
ings of the 39th Meeting of the Association for Com-
putational Linguistics.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden,
A. Hickl, and P. Wang. 2005a. Employing Two
Question Answering Systems in TREC 2005. In
Proceedings of the Fourteenth Text REtrieval Con-
ference.
Sanda Harabagiu, Andrew Hickl, John Lehmann, and
Dan Moldovan. 2005b. Experiments with Inter-
active Question-Answering. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics (ACL05).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Rec-
ognizing Textual Entailment with LCCs Ground-
hog System. In Proceedings of the Second PASCAL
Challenges Workshop.
David Lewis. 1988. Relevant Implication. Theoria,
54(3):161174.
Bernardo Magnini, Matteo Negri, Roberto Prevete, and
Hristo Tanev. 2002. Is it the right answer? ex-
ploiting web redundancy for answer validation. In
Proceedings of the Fortieth Annual Meeting of the
Association for Computational Linguistics (ACL),
Philadelphia, PA.
Dan Moldovan, Marius Pasca, Sanda Harabagiu, and
Mihai Surdeanu. 2002. Performance Issues and Er-
ror Analysis in an Open-Domain Question Answer-
ing System. In Proceedings of the 4Oth Meeting of
the Association for Computational Linguistics.
Dan Moldovan, Christine Clark, Sanda Harabagiu,
and Steve Maiorano. 2003. COGEX: A Logic
Prover for Question Answering. In Proceedings of
HLT/NAACL-2003.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - Measuring the Relatedness of
Concepts. In Proceedings of the Nineteenth Na-
tional Conference on Artificial Intelligence (AAAI-
04), San Jose, CA.
John Prager, Jennifer Chu-Carroll, and Krzysztof
Czuba. 2004. Question answering using con-
straint satisfaction: Qa-by-dossier-with-contraints.
In Proceedings of the ACL-2004, pages 574581,
Barcelona, Spain, July.
</reference>
<page confidence="0.961411">
912
</page>
<figure confidence="0.258698">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.672212">
<note confidence="0.917668333333333">b&apos;Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 905912, Sydney, July 2006. c 2006 Association for Computational Linguistics</note>
<title confidence="0.9538025">Methods for Using Textual Entailment in Open-Domain Question Answering</title>
<author confidence="0.99827">Sanda Harabagiu</author>
<author confidence="0.99827">Andrew Hickl</author>
<affiliation confidence="0.999932">Language Computer Corporation</affiliation>
<address confidence="0.9988215">1701 North Collins Boulevard Richardson, Texas 75080 USA</address>
<email confidence="0.999904">sanda@languagecomputer.com</email>
<abstract confidence="0.9932586">Work on the semantics of questions has argued that the relation between a question and its answer(s) can be cast in terms of logical entailment. In this paper, we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering (Q/A) systems. In our experiments, we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system, accuracy can be increased by as much as 20% overall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="22018" citStr="Barzilay and Lee, 2003" startWordPosition="3496" endWordPosition="3499">ion 3.3. These machine-annotated examples were then used to train the Maximum Entropy-based classifier that was used in our TE system. Table 4 presents results from TEs linearand Maximum Entropy-based Alignment Classifiers on a sample of 1000 alignment pairs selected at random from the 2006 PASCAL Test Set. Classifier Training Set Precision Recall F-Measure Linear 10K pairs 0.837 0.774 0.804 Maximum Entropy 10K pairs 0.881 0.851 0.866 Maximum Entropy 450K pairs 0.902 0.944 0.922 Table 4: Performance of Alignment Classifier 3.2 Paraphrase Acquisition Much recent work on automatic paraphrasing (Barzilay and Lee, 2003) has used relatively simple statistical techniques to identify text passages that contain the same information from parallel corpora. Since sentence-level paraphrases are generally assumed to contain information about the same event, these approaches have generally assumed that all of the available paraphrases for a given sentence will include at least one pair of entities which can be used to extract sets of paraphrases from text. The TE system uses a similar approach to gather phrase-level alternations for each entailment pair. In our system, the two highest-confidence entity alignments retu</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiple-sequence alignment. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Burger</author>
<author>Lisa Ferro</author>
</authors>
<title>Generating an Entailment Corpus from News Headlines.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>4954</pages>
<contexts>
<context position="23416" citStr="Burger and Ferro, 2005" startWordPosition="3716" endWordPosition="3719">aining corpora described in Section 3.3. This method did not always extract true paraphrases of either texts. In order increase the likelihood that 909 \x0conly true paraphrases were considered as phraselevel alternations for an example, extracted sentences were clustered using complete-link clustering using a technique proposed in (Barzilay and Lee, 2003). 3.3 Creating New Sources of Training Data In order to obtain more training data for our TE system, we extracted more than 200,000 examples of textual entailment from large newswire corpora. Positive Examples. Following an idea proposed in (Burger and Ferro, 2005), we created a corpus of approximately 101,000 textual entailment examples by pairing the headline and first sentence from newswire documents. In order to increase the likelihood of including only positive examples, pairs were filtered that did not share an entity (or an NP) in common between the headline and the first sentence Judgment Example YES Text-1: Sydney newspapers made a secret deal not to report on the fawning and spending during the citys successful bid for the 2000 Olympics, former Olympics Minister Bruce Baird said today. Text-2: Papers Said To Protect Sydney Bid YES Text-1: An I</context>
</contexts>
<marker>Burger, Ferro, 2005</marker>
<rawString>John Burger and Lisa Ferro. 2005. Generating an Entailment Corpus from News Headlines. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 4954.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop.</booktitle>
<contexts>
<context position="2455" citStr="Dagan et al., 2005" startWordPosition="370" endWordPosition="373">ction problem, where sets of auxiliary questions are used to provide more information and better constrain the answers to individual questions (cf. (Prager et al., 2004)). While different in their approach, each of these frameworks seeks to approximate the forms of semantic inference that will allow them to identify valid textual answers to natural language questions. Recently, the task of automatically recognizing one form of semantic inference textual entailment has received much attention from groups participating in the 2005 and 2006 PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005). 1 As currently defined, the RTE task requires systems to determine whether, given two text fragments, the meaning of one text could be reasonably inferred, or textually entailed, from the meaning of the other text. We believe that systems developed specifically for this task can provide current question-answering systems with valuable semantic information that can be leveraged to identify exact answers from ranked lists of candidate answers. By replacing the pairs of texts evaluated in the RTE Challenge with combinations of questions and candidate answers, we expect that textual entailment c</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL Recognizing Textual Entailment Challenge. In Proceedings of the PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdessamad Echihabi</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel approach to question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1806" citStr="Echihabi and Marcu, 2003" startWordPosition="269" endWordPosition="273">e answers, the open-domain Q/A problem has been cast as: (1) a pipeline of linguistic processes pertaining to the processing of questions, relevant passages and candidate answers, interconnected by several types of lexicosemantic feedback (cf. (Harabagiu et al., 2001; Moldovan et al., 2002)); (2) a combination of language processes that transform questions and candidate answers in logic representations such that reasoning systems can select the correct answer based on their proofs (cf. (Moldovan et al., 2003)); (3) a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)); or (4) a constraint satisfaction problem, where sets of auxiliary questions are used to provide more information and better constrain the answers to individual questions (cf. (Prager et al., 2004)). While different in their approach, each of these frameworks seeks to approximate the forms of semantic inference that will allow them to identify valid textual answers to natural language questions. Recently, the task of automatically recognizing one form of semantic inference textual entailment has received much attention from groups participating in the 2005 and 2006 PASCAL Recognizing Textual</context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>Abdessamad Echihabi and Daniel Marcu. 2003. A noisy-channel approach to question answering. In Proceedings of the 41st Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
</authors>
<title>A Probabilistic Setting and Lexical Co-occurrence Model for Textual Entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="16871" citStr="Glickman and Dagan, 2005" startWordPosition="2701" endWordPosition="2704">m selected from one text entails a term from another text. We used constituency information from a chunk parser to decompose the pair of texts into a set of disjoint seg908 \x0cALIGNMENT FEATURES: These three features are derived from the results of the lexical alignment classification. \x051\x05 LONGEST COMMON STRING: This feature represents the longest contiguous string common to both texts. \x052\x05 UNALIGNED CHUNK: This feature represents the number of chunks in one text that are not aligned with a chunk from the other \x053\x05 LEXICAL ENTAILMENT PROBABILITY: This feature is defined in (Glickman and Dagan, 2005). DEPENDENCY FEATURES: These four features are computed from the PropBank-style annotations assigned by the semantic parser. \x051\x05 ENTITY-ARG MATCH: This is a boolean feature which fires when aligned entities were assigned the same argument role label. \x052\x05 ENTITY-NEAR-ARG MATCH: This feature is collapsing the arguments Arg1 and Arg2 (as well as the ArgM subtypes) into single categories for the purpose of counting matches. \x053\x05 PREDICATE-ARG MATCH: This boolean feature is flagged when at least two aligned arguments have the same role. \x054\x05 PREDICATE-NEAR-ARG MATCH: This feat</context>
</contexts>
<marker>Glickman, Dagan, 2005</marker>
<rawString>Oren Glickman and Ido Dagan. 2005. A Probabilistic Setting and Lexical Co-occurrence Model for Textual Entailment. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeroen Groenendijk</author>
</authors>
<title>The logic of interrogation: Classical version.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Semantics and Linguistics Theory Conference (SALT IX),</booktitle>
<location>Ithaca, NY.</location>
<contexts>
<context position="4790" citStr="Groenendijk, 1999" startWordPosition="719" endWordPosition="721">describes the three methods of using textual entailment in open-domain question answering that we have identified, while Section 3 presents the textual entailment system we have used. Section 4 details our experimental methods and our evaluation results. Finally, Section 5 provides a discussion of our findings, and Section 6 summarizes our conclusions. 2 Integrating Textual Entailment in Question Answering In this section, we describe three different methods for integrating a textual entailment (TE) system into the architecture of an open-domain Q/A system. Work on the semantics of questions (Groenendijk, 1999; Lewis, 1988) has argued that the formal answerhood relation found between a question and a set of (correct) answers can be cast in terms of logical entailment. Under these approaches (referred to as licensing by (Groenendijk, 1999) and aboutness by (Lewis, 1988)), p is considered to be an answer to a question ?q iff ?q logically entails the set of worlds in which p is true(i.e. ?p). While the notion of textual entailment has been defined far less rigorously than logical entailment, we believe that the recognition of textual entailment between a question and a set of candidate answers or betw</context>
<context position="9535" citStr="Groenendijk, 1999" startWordPosition="1533" endWordPosition="1534">ense heat from a second eruption on Tuesday forced rescue operations to stop after 90 minutes. Because of the high temperatures, the bodies of only five of the volcanos initial victims were retrieved. Positive Entailment AGQ1 What temperature were the lava fragments belched out of the mountain on January 31? AGQ2 How many degrees Fahrenheit were the lava fragments belched out of the mountain on January 31? Negative Entailment AGQ3 When did rescue operations have to stop? AGQ4 How many bodies of the volcanos initial victims were retrieved? Table 2: TE between AGQs and user question. Following (Groenendijk, 1999), we expect that if a question ?q logically entails another question ?q0, then some subset of the answers entailed by ?q0 should also be interpreted as valid answers to ?q. By establishing TE between a question and AGQs derived from passages identified by the Q/A system for that question, we expect we can identify a set of answer passages that contain correct answers to the original question. For example, in Table 2, we find that entailment between questions indicates the correctness of a candidate answer: here, establishing that Q2 entails AGQ1 and AGQ2 (but not AGQ3 or AGQ4) enables the syst</context>
</contexts>
<marker>Groenendijk, 1999</marker>
<rawString>Jeroen Groenendijk. 1999. The logic of interrogation: Classical version. In Proceedings of the Ninth Semantics and Linguistics Theory Conference (SALT IX), Ithaca, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Dan Moldovan</author>
<author>Marius Pasca</author>
<author>Rada Mihalcea</author>
<author>Mihai Surdeanu</author>
<author>Razvan Bunsecu</author>
<author>Roxana Girju</author>
<author>Vasile Rus</author>
<author>Paul Morarescu</author>
</authors>
<title>The Role of Lexico-Semantic Feedback in OpenDomain Textual Question-Answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1448" citStr="Harabagiu et al., 2001" startWordPosition="213" endWordPosition="216"> information is used to either filter or rank answers returned by a Q/A system, accuracy can be increased by as much as 20% overall. 1 Introduction Open-Domain Question Answering (Q/A) systems return a textual expression, identified from a vast document collection, as a response to a question asked in natural language. In the quest for producing accurate answers, the open-domain Q/A problem has been cast as: (1) a pipeline of linguistic processes pertaining to the processing of questions, relevant passages and candidate answers, interconnected by several types of lexicosemantic feedback (cf. (Harabagiu et al., 2001; Moldovan et al., 2002)); (2) a combination of language processes that transform questions and candidate answers in logic representations such that reasoning systems can select the correct answer based on their proofs (cf. (Moldovan et al., 2003)); (3) a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)); or (4) a constraint satisfaction problem, where sets of auxiliary questions are used to provide more information and better constrain the answers to individual questions (cf. (Prager et al., 2004)). While different in their approach, each </context>
</contexts>
<marker>Harabagiu, Moldovan, Pasca, Mihalcea, Surdeanu, Bunsecu, Girju, Rus, Morarescu, 2001</marker>
<rawString>Sanda Harabagiu, Dan Moldovan, Marius Pasca, Rada Mihalcea, Mihai Surdeanu, Razvan Bunsecu, Roxana Girju, Vasile Rus, and Paul Morarescu. 2001. The Role of Lexico-Semantic Feedback in OpenDomain Textual Question-Answering. In Proceedings of the 39th Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>C Clark</author>
<author>M Bowden</author>
<author>A Hickl</author>
<author>P Wang</author>
</authors>
<title>Employing Two Question Answering Systems in TREC</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourteenth Text REtrieval Conference.</booktitle>
<contexts>
<context position="8145" citStr="Harabagiu et al., 2005" startWordPosition="1305" endWordPosition="1308">d walked away with the whole island. Table 1: Re-ranking of answers by Method 1. Method 2. Since AP is often a resourceintensive process for most Q/A systems, we expect that TE information can be used to limit the number of passages considered during AP. As illustrated in Method 2 in Figure 1, lists of passages retrieved by a PR module can either be ranked (or filtered) using TE information. Once ranking is complete, answer extraction takes place only on the set of entailed passages that the system considers likely to contain a correct answer to the users question. Method 3. In previous work (Harabagiu et al., 2005b), we have described techniques that can be used to automatically generate well-formed natural language questions from the text of paragraphs retrieved by a PR module. In our current system, sets of automatically-generated questions (AGQ) are created using a stand-alone AutoQUAB generation module, which assembles question-answer pairs (known as QUABs) from the top-ranked passages returned in response to a question. Table 2 lists some of the questions that this module has produced for the question Q2: How hot does the inside of an active volcano get?. Q2: How hot does the inside of an active v</context>
<context position="26034" citStr="Harabagiu et al., 2005" startWordPosition="4149" endWordPosition="4152"> TE into a Q/A system which employs no other form of textual inference, we can improve accuracy by more than 20% over a baseline. We conducted our evaluations on a set of 500 factoid questions selected randomly from questions previously evaluated during the annual TREC Q/A evaluations. 2 Of these 500 questions, 335 (67.0%) were automatically assigned an answer type from our systems answer type hierarchy ; the remaining 165 (33.0%) questions were classified as having an unknown answer type. In order to provide a baseline for our experiments, we ran a version of our Q/A system, known as FERRET (Harabagiu et al., 2005a), that does not make use of textual entailment information when identifying answers to questions. Results from this baseline are presented in Table 7. Question Set Questions Correct Accuracy MRR Known Answer Types 335 107 32.0% 0.3001 Unknown Answer Types 265 81 30.6% 0.2987 Table 7: Q/A Accuracy without TE The performance of the TE system described in Section 3 was first evaluated in the 2006 PASCAL RTE Challenge. In this task, systems were tasked with determining whether the meaning of a sentence (referred to as a hypothesis) could be reasonably inferred from the meaning of another sentenc</context>
<context position="29276" citStr="Harabagiu et al., 2005" startWordPosition="4680" endWordPosition="4683">tion. Results from this method are presented in Table 9. Method 2. In this experiment, entailment information was used to rank passages returned by the PR module. After an initial relevance ranking was determined from the PR engine, the top 50 passages were paired with the original question and were submitted to the TE system. Passages were re-ranked using the entailment judgment and the entailment confidence computed for each pair and then submitted to the AP module. Features derived from the entailment confidence were then combined with the keyword- and relation-based features described in (Harabagiu et al., 2005a) in order to produce a final ranking of candidate answers. Results from this method are presented in Table 9. Method 3. In the third experiment, TE was used to select AGQs that were entailed by the question submitted to the Q/A system. Here, AutoQUAB was used to generate questions for the top 50 candidate answers identified by the system. When at least one of the top 50 AGQs were entailed by the original question, the answer passage associated with the top-ranked entailed question was returned as the answer. When none of the top 50 AGQs were entailed by the question, questionanswer pairs wer</context>
</contexts>
<marker>Harabagiu, Moldovan, Clark, Bowden, Hickl, Wang, 2005</marker>
<rawString>S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang. 2005a. Employing Two Question Answering Systems in TREC 2005. In Proceedings of the Fourteenth Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Andrew Hickl</author>
<author>John Lehmann</author>
<author>Dan Moldovan</author>
</authors>
<title>Experiments with Interactive Question-Answering.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05).</booktitle>
<contexts>
<context position="8145" citStr="Harabagiu et al., 2005" startWordPosition="1305" endWordPosition="1308">d walked away with the whole island. Table 1: Re-ranking of answers by Method 1. Method 2. Since AP is often a resourceintensive process for most Q/A systems, we expect that TE information can be used to limit the number of passages considered during AP. As illustrated in Method 2 in Figure 1, lists of passages retrieved by a PR module can either be ranked (or filtered) using TE information. Once ranking is complete, answer extraction takes place only on the set of entailed passages that the system considers likely to contain a correct answer to the users question. Method 3. In previous work (Harabagiu et al., 2005b), we have described techniques that can be used to automatically generate well-formed natural language questions from the text of paragraphs retrieved by a PR module. In our current system, sets of automatically-generated questions (AGQ) are created using a stand-alone AutoQUAB generation module, which assembles question-answer pairs (known as QUABs) from the top-ranked passages returned in response to a question. Table 2 lists some of the questions that this module has produced for the question Q2: How hot does the inside of an active volcano get?. Q2: How hot does the inside of an active v</context>
<context position="26034" citStr="Harabagiu et al., 2005" startWordPosition="4149" endWordPosition="4152"> TE into a Q/A system which employs no other form of textual inference, we can improve accuracy by more than 20% over a baseline. We conducted our evaluations on a set of 500 factoid questions selected randomly from questions previously evaluated during the annual TREC Q/A evaluations. 2 Of these 500 questions, 335 (67.0%) were automatically assigned an answer type from our systems answer type hierarchy ; the remaining 165 (33.0%) questions were classified as having an unknown answer type. In order to provide a baseline for our experiments, we ran a version of our Q/A system, known as FERRET (Harabagiu et al., 2005a), that does not make use of textual entailment information when identifying answers to questions. Results from this baseline are presented in Table 7. Question Set Questions Correct Accuracy MRR Known Answer Types 335 107 32.0% 0.3001 Unknown Answer Types 265 81 30.6% 0.2987 Table 7: Q/A Accuracy without TE The performance of the TE system described in Section 3 was first evaluated in the 2006 PASCAL RTE Challenge. In this task, systems were tasked with determining whether the meaning of a sentence (referred to as a hypothesis) could be reasonably inferred from the meaning of another sentenc</context>
<context position="29276" citStr="Harabagiu et al., 2005" startWordPosition="4680" endWordPosition="4683">tion. Results from this method are presented in Table 9. Method 2. In this experiment, entailment information was used to rank passages returned by the PR module. After an initial relevance ranking was determined from the PR engine, the top 50 passages were paired with the original question and were submitted to the TE system. Passages were re-ranked using the entailment judgment and the entailment confidence computed for each pair and then submitted to the AP module. Features derived from the entailment confidence were then combined with the keyword- and relation-based features described in (Harabagiu et al., 2005a) in order to produce a final ranking of candidate answers. Results from this method are presented in Table 9. Method 3. In the third experiment, TE was used to select AGQs that were entailed by the question submitted to the Q/A system. Here, AutoQUAB was used to generate questions for the top 50 candidate answers identified by the system. When at least one of the top 50 AGQs were entailed by the original question, the answer passage associated with the top-ranked entailed question was returned as the answer. When none of the top 50 AGQs were entailed by the question, questionanswer pairs wer</context>
</contexts>
<marker>Harabagiu, Hickl, Lehmann, Moldovan, 2005</marker>
<rawString>Sanda Harabagiu, Andrew Hickl, John Lehmann, and Dan Moldovan. 2005b. Experiments with Interactive Question-Answering. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing Textual Entailment with LCCs Groundhog System.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop.</booktitle>
<contexts>
<context position="12966" citStr="Hickl et al., 2006" startWordPosition="2084" endWordPosition="2087">emantic/ Pragmatic Features Coreference Coreference NE Aliasing Concept Paraphrase Acquisition WWW Lexical Alignment Alignment Module Feature Extraction Classification Module LexicoSemantic PoS/ NER Synonyms/ Antonyms Normalization Syntactic Semantic Temporal Parsing Modality Detection Speech Act Recognition Pragmatics Factivity Detection Belief Recognition Figure 2: Textual Entailment Architecture. and textual paraphrases; and (3) a Classification Module, which uses a machine learning classifier (based on decision trees) to make an entailment judgment for each pair of texts. As described in (Hickl et al., 2006), the Preprocessing module is used to syntactically parse texts, identify the semantic dependencies of predicates, label named entities, normalize temporal and spatial expressions, resolve instances of coreference, and annotate predicates with polarity, tense, and modality information. Following preprocessing, texts are sent to an Alignment Module which uses a Maximum Entropy-based classifier in order to estimate the probability that pairs of constituents selected from texts encode corresponding information that could be used to inform an entailment judgment. This module assumes that since set</context>
<context position="20937" citStr="Hickl et al., 2006" startWordPosition="3322" endWordPosition="3325">o an alignment matrix (Ct Ch). Each pair of chunks (p Ct Ch) is then submitted to a Maximum Entropy-based classifier which determines whether or not the pair of chunks represents a case of lexical entailment. Three classes of features were used in the Alignment Classifier: (1) a set of statistical features (e.g. cosine similarity), (2) a set of lexicosemantic features (including WordNet Similarity (Pedersen et al., 2004), named entity class equality, and part-of-speech equality), and (3) a set of string-based features (such as Levenshtein edit distance and morphological stem equality). As in (Hickl et al., 2006), we used a twostep approach to obtain sufficient training data for the Alignment Classifier. First, humans were tasked with annotating a total of 10,000 alignment pairs (extracted from the 2006 PASCAL Development Set) as either positive or negative instances of alignment. These annotations were then used to train a hillclimber that was used to annotate a larger set of 450,000 alignment pairs selected at random from the training corpora described in Section 3.3. These machine-annotated examples were then used to train the Maximum Entropy-based classifier that was used in our TE system. Table 4</context>
<context position="27313" citStr="Hickl et al., 2006" startWordPosition="4355" endWordPosition="4358">ated in the 2006 RTE Challenge, including: pairs derived from the output of (1) automatic question-answering (QA) systems, (2) information extraction systems (IE), (3) information retrieval (IR) systems, and (4) multidocument summarization (SUM) systems. The accuracy of our TE system across these four tasks is presented in Table 8. Training Data Development Set Additional Corpora Number of Examples 800 201,000 Task QA-test 0.5750 0.6950 IE-test 0.6450 0.7300 IR-test 0.6200 0.7450 SUM-test 0.7700 0.8450 Overall Accuracy 0.6525 0.7538 Table 8: Accuracy on the 2006 RTE Test Set In previous work (Hickl et al., 2006), we have found that the type and amount of training data available to our TE system significantly (p &lt; 0.05) impacted its performance on the 2006 RTE Test Set. When our system was trained on the training corpora described in Section 3.3, the overall accuracy of the system increased by more than 10%, 2 Text Retrieval Conference (http://trec.nist.gov) 910 \x0cfrom 65.25% to 75.38%. In order to provide training data that replicated the task of recognizing entailment between a question and an answer, we assembled a corpus of 5000 question-answer pairs selected from answers that our baseline Q/A s</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2006. Recognizing Textual Entailment with LCCs Groundhog System. In Proceedings of the Second PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lewis</author>
</authors>
<title>Relevant Implication.</title>
<date>1988</date>
<journal>Theoria,</journal>
<volume>54</volume>
<issue>3</issue>
<contexts>
<context position="4804" citStr="Lewis, 1988" startWordPosition="722" endWordPosition="723"> methods of using textual entailment in open-domain question answering that we have identified, while Section 3 presents the textual entailment system we have used. Section 4 details our experimental methods and our evaluation results. Finally, Section 5 provides a discussion of our findings, and Section 6 summarizes our conclusions. 2 Integrating Textual Entailment in Question Answering In this section, we describe three different methods for integrating a textual entailment (TE) system into the architecture of an open-domain Q/A system. Work on the semantics of questions (Groenendijk, 1999; Lewis, 1988) has argued that the formal answerhood relation found between a question and a set of (correct) answers can be cast in terms of logical entailment. Under these approaches (referred to as licensing by (Groenendijk, 1999) and aboutness by (Lewis, 1988)), p is considered to be an answer to a question ?q iff ?q logically entails the set of worlds in which p is true(i.e. ?p). While the notion of textual entailment has been defined far less rigorously than logical entailment, we believe that the recognition of textual entailment between a question and a set of candidate answers or between a question</context>
</contexts>
<marker>Lewis, 1988</marker>
<rawString>David Lewis. 1988. Relevant Implication. Theoria, 54(3):161174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Matteo Negri</author>
<author>Roberto Prevete</author>
<author>Hristo Tanev</author>
</authors>
<title>Is it the right answer? exploiting web redundancy for answer validation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Fortieth Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="31200" citStr="Magnini et al., 2002" startWordPosition="5003" endWordPosition="5006">thod are provided in Table 9. Known EAT Unknown EAT Acc MRR Acc MRR Baseline 32.0% 0.3001 30.6% 0.2978 Method 1 44.1% 0.4114 39.5% 0.3833 Method 2 52.4% 0.5558 42.7% 0.4135 Method 3 41.5% 0.4257 37.5% 0.3575 Hybrid 53.9% 0.5640 41.9% 0.4010 Table 9: Q/A Performance with TE 5 Discussion The experiments reported in this paper suggest that current TE systems may be able to provide open-domain Q/A systems with the forms of semantic inference needed to perform accurate answer validation. While probabilistic or web-based methods for answer validation have been previously explored in the literature (Magnini et al., 2002), these approaches have modeled the relationship between a question and a (correct) answer in terms of relevance and have not tried to approximate the deeper semantic phenomena that are involved in determining answerhood. Our work suggests that considerable gains in performance can be obtained by incorporating TE during both answer processing and passage retrieval. While best results were obtained using the Hybrid Method (which boosted performance by nearly 28% for questions with known EATs), each of the individual methods managed to boost the overall accuracy of the Q/A system by at least 7%.</context>
</contexts>
<marker>Magnini, Negri, Prevete, Tanev, 2002</marker>
<rawString>Bernardo Magnini, Matteo Negri, Roberto Prevete, and Hristo Tanev. 2002. Is it the right answer? exploiting web redundancy for answer validation. In Proceedings of the Fortieth Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Marius Pasca</author>
<author>Sanda Harabagiu</author>
<author>Mihai Surdeanu</author>
</authors>
<title>Performance Issues and Error Analysis in an Open-Domain Question Answering System.</title>
<date>2002</date>
<booktitle>In Proceedings of the 4Oth Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1472" citStr="Moldovan et al., 2002" startWordPosition="217" endWordPosition="220">either filter or rank answers returned by a Q/A system, accuracy can be increased by as much as 20% overall. 1 Introduction Open-Domain Question Answering (Q/A) systems return a textual expression, identified from a vast document collection, as a response to a question asked in natural language. In the quest for producing accurate answers, the open-domain Q/A problem has been cast as: (1) a pipeline of linguistic processes pertaining to the processing of questions, relevant passages and candidate answers, interconnected by several types of lexicosemantic feedback (cf. (Harabagiu et al., 2001; Moldovan et al., 2002)); (2) a combination of language processes that transform questions and candidate answers in logic representations such that reasoning systems can select the correct answer based on their proofs (cf. (Moldovan et al., 2003)); (3) a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)); or (4) a constraint satisfaction problem, where sets of auxiliary questions are used to provide more information and better constrain the answers to individual questions (cf. (Prager et al., 2004)). While different in their approach, each of these frameworks seek</context>
</contexts>
<marker>Moldovan, Pasca, Harabagiu, Surdeanu, 2002</marker>
<rawString>Dan Moldovan, Marius Pasca, Sanda Harabagiu, and Mihai Surdeanu. 2002. Performance Issues and Error Analysis in an Open-Domain Question Answering System. In Proceedings of the 4Oth Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Christine Clark</author>
<author>Sanda Harabagiu</author>
<author>Steve Maiorano</author>
</authors>
<title>COGEX: A Logic Prover for Question Answering.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL-2003.</booktitle>
<contexts>
<context position="1695" citStr="Moldovan et al., 2003" startWordPosition="251" endWordPosition="254">cument collection, as a response to a question asked in natural language. In the quest for producing accurate answers, the open-domain Q/A problem has been cast as: (1) a pipeline of linguistic processes pertaining to the processing of questions, relevant passages and candidate answers, interconnected by several types of lexicosemantic feedback (cf. (Harabagiu et al., 2001; Moldovan et al., 2002)); (2) a combination of language processes that transform questions and candidate answers in logic representations such that reasoning systems can select the correct answer based on their proofs (cf. (Moldovan et al., 2003)); (3) a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)); or (4) a constraint satisfaction problem, where sets of auxiliary questions are used to provide more information and better constrain the answers to individual questions (cf. (Prager et al., 2004)). While different in their approach, each of these frameworks seeks to approximate the forms of semantic inference that will allow them to identify valid textual answers to natural language questions. Recently, the task of automatically recognizing one form of semantic inference textual e</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, Maiorano, 2003</marker>
<rawString>Dan Moldovan, Christine Clark, Sanda Harabagiu, and Steve Maiorano. 2003. COGEX: A Logic Prover for Question Answering. In Proceedings of HLT/NAACL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI04),</booktitle>
<location>San Jose, CA.</location>
<contexts>
<context position="20742" citStr="Pedersen et al., 2004" startWordPosition="3293" endWordPosition="3296">signed opposite polarity values. Figure 4: Features Used in Classifying Entailment ments known as alignable chunks. Alignable chunks from one text (Ct) and the other text (Ch) are then assembled into an alignment matrix (Ct Ch). Each pair of chunks (p Ct Ch) is then submitted to a Maximum Entropy-based classifier which determines whether or not the pair of chunks represents a case of lexical entailment. Three classes of features were used in the Alignment Classifier: (1) a set of statistical features (e.g. cosine similarity), (2) a set of lexicosemantic features (including WordNet Similarity (Pedersen et al., 2004), named entity class equality, and part-of-speech equality), and (3) a set of string-based features (such as Levenshtein edit distance and morphological stem equality). As in (Hickl et al., 2006), we used a twostep approach to obtain sufficient training data for the Alignment Classifier. First, humans were tasked with annotating a total of 10,000 alignment pairs (extracted from the 2006 PASCAL Development Set) as either positive or negative instances of alignment. These annotations were then used to train a hillclimber that was used to annotate a larger set of 450,000 alignment pairs selected </context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI04), San Jose, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
<author>Jennifer Chu-Carroll</author>
<author>Krzysztof Czuba</author>
</authors>
<title>Question answering using constraint satisfaction: Qa-by-dossier-with-contraints.</title>
<date>2004</date>
<contexts>
<context position="2005" citStr="Prager et al., 2004" startWordPosition="303" endWordPosition="306">l types of lexicosemantic feedback (cf. (Harabagiu et al., 2001; Moldovan et al., 2002)); (2) a combination of language processes that transform questions and candidate answers in logic representations such that reasoning systems can select the correct answer based on their proofs (cf. (Moldovan et al., 2003)); (3) a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)); or (4) a constraint satisfaction problem, where sets of auxiliary questions are used to provide more information and better constrain the answers to individual questions (cf. (Prager et al., 2004)). While different in their approach, each of these frameworks seeks to approximate the forms of semantic inference that will allow them to identify valid textual answers to natural language questions. Recently, the task of automatically recognizing one form of semantic inference textual entailment has received much attention from groups participating in the 2005 and 2006 PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005). 1 As currently defined, the RTE task requires systems to determine whether, given two text fragments, the meaning of one text could be reasonably in</context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, 2004</marker>
<rawString>John Prager, Jennifer Chu-Carroll, and Krzysztof Czuba. 2004. Question answering using constraint satisfaction: Qa-by-dossier-with-contraints.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>In Proceedings of the ACL-2004,</booktitle>
<pages>574581</pages>
<location>Barcelona, Spain,</location>
<marker></marker>
<rawString>In Proceedings of the ACL-2004, pages 574581, Barcelona, Spain, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>