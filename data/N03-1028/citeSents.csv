uential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy (McCallum et al., 2000; CITATION) and a variety of other linear classifiers, including winnow CITATION, AdaBoost CITATION, and support-vector machines CITATION,,
CITATION showed that CRFs beat related classification models as well as HMMs on synthetic data and on a part-of-speech tagging task,,
Our work shows that preconditioned conjugate-gradient (CG) CITATION or limited-memory quasi-Newton (L-BFGS) CITATION perform comparably on very large problems (around 3.8 million featur,,
d-looking support-vector classifiers CITATION,,
In the present work, we show that CRFs beat all reported single-model NP chunking results on the standard evaluation dataset, and are statistically indistinguishable from the previous best performer, a voting arrangement of 24 forward- and backward-looking support-vector classifiers CITATION,,
In other words, sequential classifiers are myopic about the impact of their current decision on later decisions (Bottou, 1991; CITATION),,
r classifiers CITATION,,
Our work shows that preconditioned conjugate-gradient (CG) CITATION or limited,,
Our work shows that preconditioned conjugate-gradient (CG) CITATION or limited-memory quasi-Newton (L-BFGS,,
The generalized perceptron proposed by CITATION is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron,,
 CITATION, the input to the NP chunker consists of the words in a sentence annotated automatically with part-of-speech (POS) tags,,
3.3 Voted Perceptron Unlike other methods discussed so far, voted perceptron training CITATION attempts to minimize the difference between the global feature vector for a training instance and the sa,,
To avoid overfitting, we penalize the likelihood with a spherical Gaussian weight prior CITATION: L0 = X k [ F (yk, xk) log Z(xk)] kk2 22 + const with gradient L0 = X k \x02 F (yk, xk) Ep(Y |xk)F (Y , xk) \x03 2 3 Training Methods CITATION used iterative scaling algorithms for CRF training, following earlier work on maximumentropy models for natural language (CITATION; Della CITATION),,
 bootstrap variances in preliminary experiments were too high to allow any conclusions, so we used instead a McNemar paired test on labeling disagreements CITATION,,
CITATION showed that CRFs beat related c,,
The relative slowness of iterative scaling is also documented in a recent evaluation of training methods for maximum-entropy classification CITATION,,
In contrast, generative models are trained to maximize the joint probability of the training data, which is 1 CITATION used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classification-based meth,,
On the application side, (log-)linear parsing models have the potential to supplant the currently dominant lexicalized PCFG models for parsing by allowing much richer feature sets and simpler smoothing, while avoiding the label bias problem that may have hindered earlier classifier-based parsers CITATION,,
An earlier study indicates that L-BFGS performs well in maximum-entropy classifier training CITATION,,
3.1 Preconditioned Conjugate Gradient Conjugate-gradient (CG) methods have been shown to be very effective in linear and non-linear optimization CITATION,,
This forced the best sequential classifier systems to resort to heuristic combinations of forward-moving and backward-moving sequential classifiers CITATION,,
Full discriminative parser training faces significant algorithmic challenges in the relationship between parsing alternatives and feature values CITATION and in computing feature expectations,,
Shallow parsing identifies the non-recursive cores of various phrase types in text, possibly as a precursor to full parsing or information extraction CITATION,,
4.1 Data Preparation NP chunking results have been reported on two slightly different data sets: the original RM data set of CITATION, and the modified CoNLL-2000 version of Tjong Kim Sang and Buchholz CITATION,,
\x0cModel F score SVM combination 94.39% CITATION CRF 94.38% Generalized winnow 93.89% CITATION Voted perceptron 94.09% MEMM 93.70% Table 2: NP chunking F scores 5 Results All the experiments were performed with our Java implementation of CRFs,designed to handle millions of features, on 1.7 GHz Pentium IV processors with Linux and IBM Java 1.3.0,,
The sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy (McCallum et al., 2000; CITATION) and a variety of other linear classifiers, including winnow CITATION, AdaBoost CITATION, and support-vector machines CITATION,,
2 Conditional Random Fields We focus here on conditional random fields on sequences, although the notion can be used more generally (CITATION; CITATION),,
The published F score for voted perceptron is 93.53% with a different feature set CITATION,,
In contrast, generative models are trained to maximize the joint probability of the training data, which is 1 CITATION used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classification-based method,,
Those methods are very simple and guaranteed to converge, but as CITATION and CITATION showed for classification, their convergence is much slower than that of general-purposeconvex optimization algorithms when many correlated features are involved,,
Following CITATION, the input to the NP chunker consists of the words in a sentence annotated automatically with part-of-speech (POS) tags,,
The first approach relies on k-order generative probabilistic models of paired input sequences and label sequences, for instance hidden Markov models (HMMs) (CITATION; CITATION) or multilevel Markov models CITATION,,
hose methods are very simple and guaranteed to converge, but as CITATION and CITATION showed for classification, their convergence is much slower than that of general-purposeconvex optimization algorithms when many correlated features are involved,,
log Z(xk)] kk2 22 + const with gradient L0 = X k \x02 F (yk, xk) Ep(Y |xk)F (Y , xk) \x03 2 3 Training Methods CITATION used iterative scaling algorithms for CRF training, following earlier work on maximumentropy models for natural language (CITATION; Della CITATION),,
The task was extended to additional phrase types for the CoNLL2000 shared task (Tjong Kim CITATION), which is now the standard evaluation task for shallow parsing,,
neralize the ideas of large-margin classification to sequence models, strengthening the results of CITATION and leading to new optimal training algorithms with stronger guarantees against overfitting,,
\x0cModel F score SVM combination 94.39% CITATION CRF 94.38% Generalized winnow 93.89% CITATION Voted perceptron 94.09% MEMM 93.70% Table 2: NP chunking F scores 5 Results All the experiments were performed with our Ja,,
CITATION reported and we confirmed that this averaging reduces overfitting considerably,,
Minor variants support voted perceptron CITATION and MEMMs (McCallum et al., 2000) with the same efficient feature encoding,,
3.3 Voted Perceptron Unlike other methods discussed so far, voted perceptron training CITATION attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model,,
However, bootstrap variances in preliminary experiments were too high to allow any conclusions, so we used instead a McNemar paired test on labeling disagreements CITATION,,
Concurrently with the present work, CITATION tested conjugate gradient and second-order methods for CRF training, showing significant training speed advantages over iterative scaling on a small shallow parsing problem,,
A more detailed description of this method can be found elsewhere CITATION,,
The pioneering work of CITATION introduced NP chunking as a machine-learning problem, with standard datasets and evaluation metrics,,
However, work in that direction has so far addressed only parse reranking (CITATION; CITATION),,
On the machine-learning side, it would be interesting to generalize the ideas of large-margin classification to sequence models, strengthening the results of CITATION and leading to new optimal training algorithms with stronger guarantees against overfitting,,
ge-margin classification to sequence models, strengthening the results of CITATION and leading to new optimal training algorithms with stronger guarantees against overfitting,,
Concurrently with the present work, CITATION tested conjugate gradient and second-order methods for CRF training, showing significant training speed advantages ove,,
training time such test is McNemar test on paired observations CITATION,,
Our work shows that preconditioned conjugate-gradient (CG) CITATION or limited-memory quasi-Newton (L-BFGS) CITATION perform comparably on very large problems (around ,,
The first approach relies on k-order generative probabilistic models of paired input sequences and label sequences, for instance hidden Markov models (HMMs) (CITATION; CITATION) or multilevel Markov models (Bikel e,,
guage (CITATION; Della CITATION),,
Yeh CITATION examined randomized tests for estimating the significance of F scores, and in particular the bootstrap over the test set (Efron and Tibshirani, 1993; CITATION),,
The conditional probability distribution defined by the CRF is then p(Y |X) = exp F (Y , X) Z(X) (1) where Z(x) = X y exp F (y, x) Any positive conditional distribution p(Y |X) that obeys the Markov property p(Yi|{Yj}j6=i, X) = p(Yi|Yi1, Yi+1, X) can be written in the form (1) for appropriate choice of feature functions and weight vector CITATION,,
Our work shows that preconditioned conjugate-gradient (CG) CITATION or limited-memory quasi-Newton (L-BFGS) CITATION perform comparably on very large problems (around 3.8 million features),,
We compare those algorithms to generalized iterative scaling (GIS) CITATION, non-preconditioned CG, and voted perceptron training CITATION,,
