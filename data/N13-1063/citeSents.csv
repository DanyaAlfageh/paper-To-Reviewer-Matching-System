fthe-art supervised systems on a variety of tasks (CITATION; CITATION; CITATION),,
, 2003; CITATION), recently researchers become interested in word representations (also called word embeddings) learned by these models,,
 Word embeddings are usually served as the first layer in deep learning systems for NLP (CITATION; CITATIONa, 2011b) and help these systems perform comparably with the state-of-the-art models based on handcrafted features,,
 They also have been directly added as features to the state-of-the-art models of chunking and NER, and have achieved significant improvements CITATION,,
 Although the direct usage of continuous embeddings has been proved to be an effective method for enhancing the state-of-the-art supervised models, it has some disadvantages, which made them be out-performed by simpler Brown cluster features CITATION and made them computationally complic,,
 These methods are very effective in utilizing large-scale unlabeled data and have successfully improved performances of state-ofthe-art supervised systems on a variety of tasks (CITATION; CITATION; CITATION),,
, 2003; CITATION), recently researchers become interested in word representations (also called word embeddings) learned by these models,,
 Word embeddings are usually served as the first layer in deep learning systems for NLP (CITATION; CITATIONa, 2011b) and help these systems perform co,,
 These methods are very effective in utilizing large-scale unlabeled data and have successfully improved performances of state-ofthe-art supervised systems on a variety of tasks (CITATION; CITATION; CITATION),,
, 2003; CITATION), recently researchers become interested in word representations (also called word embeddings) learned by these models,,
 Word embeddings are usually served as the first layer in deep learning systems for NLP (CITATION; CITATIONa, 2011b) and help t,,
 CITATION also observed that compound features of Brown clusters achieved more improvements on parsing,,
 These methods are very effective in utilizing large-scale unlabeled data and have successfully improved performances of state-ofthe-art supervised systems on a variety of tasks (CITATION; CITATION; CITATION),,
, 2003; CITATION), recently researchers become interested in word representations (also called word embeddings) learned by these models,,
 Word embeddings are usually served as the first layer in deep learning systems for NLP (CITATION; CITATIONa, 2011b) and help these systems perform comparably with the state-of-the-art models based on handcrafted features,,
 To compare with the work of CITATION, which aimed to solve the same problem but using embedding directly, we used the same word embeddings (CW 50) and Brown clusters (1000 clusters) they provided,,
 The embeddings in CITATION are trained on RCV corpus, while the CoNLL2000 data is a part of the WSJ corpus,,
 Since we believe that word representations trained on similar domain may better help to improve the results, we also used embeddings and Brown clusters trained on unlabeled WSJ data from CITATION for comparison,,
 1998) and converted them following the style of Penn CTB CITATION,,
 The method in CITATION was used to accelerate the training processes of NLMs,,
 We chose the Sofia-ml toolkit CITATION for clustering of embeddings in order to save time,,
 For comparison we re-implemented the direct usage of embeddings in CITATION with CRFsuite CITATION since their features contain continuous values,,
 The results reported in CITATION were denoted as direct,,
 The method in CITATION was used to accelerate the training processes of NLMs,,
 We chose the Sofia-ml toolkit CITATION for clustering of embeddings in order to save time,,
 For comparison we re-implemented the direct usage of embeddings in CITATION with CRFsuite CITATION since their features contain continuous values,,
 The method in CITATION was used to accelerate the training processes of NLMs,,
 We chose the Sofia-ml toolkit CITATION for clustering of embeddings in order to save time,,
 For comparison we re-implemented the direct usage of embeddings in CITATION with CRFsuite CITATION since their features contain continuous values,,
 The results reported in CITATION were denoted as direct,,
on a variety of tasks (CITATION; CITATION; CITATION),,
, 2003; CITATION), recently researchers become interested in word representations (also called word embeddings) learned by these models,,
 Word embeddings are usually served as the first layer in deep learning systems for NLP (CITATION; CITATIONa, 2011b) and help these systems perform comparably with the state-of-the-art models based on handcrafted features,,
 They also have been directly added as features to the state-of-the-art models of chunking and NER, and have achieved significant improvements CITATION,,
 Although the direct usage of continuous embeddings has been proved to be an effective method for enhancing the state-of-the-art supervised models, it has some disadvantages, which made them be out-performed by simpler Brown cluster features CITATION and made them computationally complicated,,
on a variety of tasks (CITATION; CITATION; CITATION),,
, 2003; CITATION), recently researchers become interested in word representations (also called word embeddings) learned by these models,,
 Word embeddings are usually served as the first layer in deep learning systems for NLP (CITATION; CITATIONa, 2011b) and help these systems perform comparably with the state-of-the-art models based on handcrafted features,,
 They also have been directly added as features to the state-of-the-art models of chunking and NER, and have achieved significant improvements CITATION,,
 Although the direct usage of continuous embeddings has been proved to be an effective method for enhancing the state-of-the-art supervised models, it has some disadvantages, which made them be out-performed by simpler Brown cluster features CITATION and made them computationally complicated,,
 These methods are very effective in utilizing large-scale unlabeled data and have successfully improved performances of state-ofthe-art supervised systems on a variety of tasks (CITATION; CITATION; CITATION),,
, 2003; CITATION), recently researchers become interested in word representations (also called word embeddings) learned by these models,,
 Word embeddings are usually served as the first layer in deep learning systems for NLP (CITATION; CITATIONa, 2011b) and help these systems perform comparably with the state-o,,
 Word embeddings are usually served as the first layer in deep learning systems for NLP (CITATION; CITATIONa, 2011b) and help these systems perform comparably with the state-of-the-art models based on handcrafted features,,
 They also have been directly added as features to the state-of-the-art models of chunking and NER, and have achieved significant improvements CITATION,,
 Although the direct usage of continuous embeddings has been proved to be an effective method for enhancing the state-of-the-art supervised models, it has some disadvantages, which made them be out-performed by simpler Brown cluster features CITATION and made them computationally complicated,,
 As shown in CITATION, this is the main reason that models with embedding features made more errors than those with Brown cluster feature,,
 In addition, although Brown clustering was considered better in CITATION, our experiment results and comparisons showed that our compound features from embedding clustering is at least comparable with those from Brown clustering,,
1 Experimental settings We tested our compound features on the same chunking (CoNLL2000) and NER (CoNLL2003) tasks in CITATION,,
 To compare with the work of CITATION, which aimed to solve the same problem but using embedding directly, we used the same word embeddings (CW 50) and Brown clusters (1000 clusters) they provided,,
 The embeddings in CITATION are trained on RCV corpus, while the CoNLL2000 data is a part of the WSJ corpus,,
 The method in CITATION was used to accelerate the training processes of NLMs,,
 We chose the Sofia-ml toolkit CITATION for clustering of embeddings in order to save time,,
 For comparison we re-implemented the direct usage of embeddings in CITATION with CRFsuite CITATION since their features contain continuous values,,
 The results reported in CITATION were denoted as direct,,
 Our baseline is slightly lower than that in CITATION, because the first-order CRF cannot utilize context information of NE tags,,
 Here the word frequencies are from the results of Brown clustering provided by CITATION,,
 Since we believe that word representations trained on similar domain may better help to improve the results, we also used embeddings and Brown clusters trained on unlabeled WSJ data from CITATION for comparison,,
 1998) and converted them following the style of Penn CTB CITATION,,
