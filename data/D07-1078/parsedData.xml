<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.588958">
b&amp;apos;Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 746754, Prague, June 2007. c
</bodyText>
<sectionHeader confidence="0.4185" genericHeader="abstract">
2007 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.766866333333333">
Binarizing Syntax Trees to Improve
Syntax-Based Machine Translation Accuracy
Wei Wang and Kevin Knight and Daniel Marcu
</title>
<author confidence="0.816679">
Language Weaver, Inc.
</author>
<affiliation confidence="0.828591">
4640 Admiralty Way, Suite 1210
</affiliation>
<address confidence="0.653384">
Marina del Rey, CA, 90292
</address>
<email confidence="0.995114">
{wwang,kknight,dmarcu}@languageweaver.com
</email>
<sectionHeader confidence="0.990711" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.997659583333333">
We show that phrase structures in Penn Tree-
bank style parses are not optimal for syntax-
based machine translation. We exploit a se-
ries of binarization methods to restructure
the Penn Treebank style trees such that syn-
tactified phrases smaller than Penn Treebank
constituents can be acquired and exploited in
translation. We find that by employing the
EM algorithm for determining the binariza-
tion of a parse tree among a set of alternative
binarizations gives us the best translation re-
sult.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993885823529412">
Syntax-based translation models (Eisner, 2003; Gal-
ley et al., 2006; Marcu et al., 2006) are usually built
directly from Penn Treebank (PTB) (Marcus et al.,
1993) style parse trees by composing treebank gram-
mar rules. As a result, often no substructures corre-
sponding to partial PTB constituents are extracted to
form translation rules.
Syntax translation models acquired by composing
treebank grammar rules assume that long rewrites
are not decomposable into smaller steps. This ef-
fectively restricts the generalization power of the in-
duced model. For example, suppose we have an
xRs (Knight and Graehl, 2004) rule R1 in Figure 1
that translates the Chinese phrase RUSSIA MINISTER
VIKTOR-CHERNOMYRDIN into an English NPB tree
fragment yielding an English phrase. Also suppose
that we want to translate a Chinese phrase
</bodyText>
<sectionHeader confidence="0.990801" genericHeader="method">
VIKTOR-CHERNOMYRDIN AND HIS COLLEAGUE
</sectionHeader>
<bodyText confidence="0.996895131578947">
into English. What we desire is that if we have
another rule R2 as shown in Figure 1, we could
somehow compose it with R1 to obtain the desir-
able translation. We unfortunately cannot do this
because R1 and R2 are not further decomposable
and their substructures cannot be re-used. The re-
quirement that all translation rules have exactly one
root node does not enable us to use the translation of
VIKTOR-CHERNOMYRDIN in any other contexts than
those seen in the training corpus.
A solution to overcome this problem is to right-
binarize the left-hand side (LHS) (or the English-
side) tree of R1 such that we can decompose
R1 into R3 and R4 by factoring NNP(viktor)
NNP(chernomyrdin) out as R4 according to the
word alignments; and left-binarize the LHS of R2 by
introducing a new tree node that collapses the two
NNPs, so as to generalize this rule, getting rule R5
and rule R6. We also need to consistently syntact-
ify the root labels of R4 and the new frontier label
of R6 such that these two rules can be composed.
Since labeling is not a concern of this paper, we sim-
ply label new nodes with X-bar where X here is the
parent label. With all these in place, we now can
translate the foreign sentence by composing R6 and
R4 in Figure 1.
Binarizing the syntax trees for syntax-based ma-
chine translation is similar in spirit to generalizing
parsing models via markovization (Collins, 1997;
Charniak, 2000). But in translation modeling, it is
unclear how to effectively markovize the translation
rules, especially when the rules are complex like
those proposed by Galley et al. (2006).
In this paper, we explore the generalization abil-
ity of simple binarization methods like left-, right-,
and head-binarization, and also their combinations.
Simple binarization methods binarize syntax trees
in a consistent fashion (left-, right-, or head-) and
</bodyText>
<page confidence="0.995884">
746
</page>
<figure confidence="0.78324430952381">
\x0cNPB
JJ NNP NNP NNP
russia minister viktor chernomyrdin
RUSSIA MINISTER VC
NPB
?
NNP NNP AND HIS COLLEAGUE
VC AND HIS COLLEAGUE
NPB
JJ
russia
RUSSIA
minister
MINISTER NNP NNP
viktor chernomyrdin
VC
NPB
colleague
and his colleague
COLLEAGUE
PRB$
his
HIS
CC
and
AND
NNP NNP
x0:NNP x1:NNP VC AND HIS COLLEAGUE
NNS
x0:NNP x1:NNP CC PRB$ NNS
R1
R2
NPB
NPB
R3
NPB
NPB
R4 R5
R6
R6
R4
NPB
</figure>
<figureCaption confidence="0.999398">
Figure 1: Generalizing translation rules by binarizing trees.
</figureCaption>
<bodyText confidence="0.99118247368421">
thus cannot guarantee that all the substructures can
be factored out. For example, right binarization on
the LHS of R1 makes available R4, but misses R6
on R2. We then introduce a parallel restructuring
method, that is, one can binarize both to the left and
right at the same time, resulting in a binarization for-
est. We employ the EM (Dempster et al., 1977) algo-
rithm to learn the binarization bias for each tree node
from the parallel alternatives. The EM-binarization
yields best translation performance.
The rest of the paper is organized as follows.
Section 2 describes related research. Section 3 de-
fines the concepts necessary for describing the bina-
rizations methods. Section 4 describes the tree bina-
rization methods in details. Section 5 describes the
forest-based rule extraction algorithm, and section 6
explains how we restructure the trees using the EM
algorithm. The last two sections are for experiments
and conclusions.
</bodyText>
<sectionHeader confidence="0.998762" genericHeader="method">
2 Related Research
</sectionHeader>
<bodyText confidence="0.995532272727273">
Several researchers (Melamed et al., 2004; Zhang
et al., 2006) have already proposed methods for bi-
narizing synchronous grammars in the context of
machine translation. Grammar binarization usually
maintains an equivalence to the original grammar
such that binarized grammars generate the same lan-
guage and assign the same probability to each string
as the original grammar does. Grammar binarization
is often employed to make the grammar fit in a CKY
parser. In our work, we are focused on binarization
of parse trees. Tree binarization generalizes the re-
sulting grammar and changes its probability distri-
bution. In tree binarization, synchronous grammars
built from restructured (binarized) training trees still
contain non-binary, multi-level rules and thus still
require the binarization transformation so as to be
employed by a CKY parser.
The translation model we are using in this paper
belongs to the xRs formalism (Knight and Graehl,
2004), which has been proved successful for ma-
chine translation in (Galley et al., 2004; Galley et
al., 2006; Marcu et al., 2006).
</bodyText>
<sectionHeader confidence="0.99807" genericHeader="method">
3 Concepts
</sectionHeader>
<bodyText confidence="0.999345125">
We focus on tree-to-string (in noisy-channel model
sense) translation models. Translation models of
this type are typically trained on tuples of a source-
language sentence f, a target language (e.g., English)
parse tree that yields e and translates from f, and
the word alignments a between e and f. Such a tuple
is called an alignment graph in (Galley et al., 2004).
The graph (1) in Figure 2 is such an alignment graph.
</bodyText>
<page confidence="0.983029">
747
</page>
<figure confidence="0.997732371428572">
\x0c(1) unbinarized tree
NPB
viktor chernomyrdin
VIKTORCHERNOMYRDIN
NNP1 NNP2 NNP3 NNP4*
(2) left-binarization (3) right-/head-binarization
NPB
NPB
NNP1 NNP2 NNP3
viktor
NNP
4
chernomyrdin
NPB
NNP1 NPB
NNP2 NNP3
viktor
NNP4
chernomyrdin
(4) left-binarization (5) right-binarization (6) left-binarization (7) right-/head-binarization
NPB
NPB
NPB
NNP1 NNP2
NNP3
viktor
NNP4
chernomyrdin
- - NPB
NNP1 NPB
NNP2 NPB
NNP3
viktor
NNP4
chernomyrdin
</figure>
<figureCaption confidence="0.998375">
Figure 2: Left, right, and head binarizations. Heads are marked with
</figureCaption>
<bodyText confidence="0.994517884615384">
s. New nonterminals introduced by binarization are
denoted by X-bars.
A tree node in is admissible if the f string cov-
ered by the node is contiguous but not empty, and
if the f string does not align to any e string that is
not covered by . An xRs rule can be extracted only
from an admissible tree node, so that we do not have
to deal with dis-contiguous f spans in decoding (or
synchronous parsing). For example, in tree (2) in
Figure 2, node NPB is not admissible because the
f string that the node covers also aligns to NNP4,
which is not covered by the NPB. Node NPB in tree
(3), on the other hand, is admissible.
A set of sibling tree nodes is called factorizable
if we can form an admissible new node dominating
them. For example, in tree (1) of Figure 2, sibling
nodes NNP2 NNP3 and NNP4 are factorizable be-
cause we can factorize them out and form a new
node NPB, resulting in tree (3). Sibling tree nodes
NNP1 NNP2 and NNP3 are not factorizable. In syn-
chronous parse trees, not all sibling nodes are fac-
torizable, thus not all sub-phrases can be acquired
and syntactified. The main purpose of our paper is
to restructure parse trees by factorization such that
syntactified sub-phrases can be employed in transla-
tion.
</bodyText>
<sectionHeader confidence="0.960552" genericHeader="method">
4 Binarizing Syntax Trees
</sectionHeader>
<bodyText confidence="0.998692727272727">
We are going to binarize a tree node n that domi-
nates r children n1, ..., nr. Restructuring will be
performed by introducing new tree nodes to domi-
nate a subset of the children nodes. To avoid over-
generalization, we allow ourselves to form only one
new node at a time. For example, in Figure 2, we
can binarize tree (1) into tree (2), but we are not
allowed to form two new nodes, one dominating
NNP1 NNP2 and the other dominating NNP3 NNP4.
Since labeling is not the concern of this paper, we re-
label the newly formed nodes as n.
</bodyText>
<subsectionHeader confidence="0.993789">
4.1 Simple binarization methods
</subsectionHeader>
<bodyText confidence="0.994268333333333">
The left binarization of node n (i.e., the NPB in
tree (1) of Figure 2) factorizes the leftmost r 1
children by forming a new node n (i.e., NPB in
tree (2)) to dominate them, leaving the last child
nr untouched; and then makes the new node n the
left child of n. The method then recursively left-
binarizes the newly formed node n until two leaves
are reached. In Figure 2, we left-binarize tree (1)
into (2) and then into (4).
The right binarization of node n factorizes the
rightmost r 1 children by forming a new node n
(i.e., NPB in tree (3)) to dominate them, leaving the
</bodyText>
<page confidence="0.98996">
748
</page>
<bodyText confidence="0.997756695652174">
\x0cfirst child n1 untouched; and then makes the new
node n the right child of n. The method then recur-
sively right-binarizes the newly formed node n. In
Figure 2, we right-binarize tree (1) into (3) and then
into (7).
The head binarization of node n left-binarizes
n if the head is the first child; otherwise, right-
binarizes n. We prefer right-binarization to left-
binarization when both are applicable under the head
restriction because our initial motivation was to gen-
eralize the NPB-rooted translation rules. As we will
show in the experiments, binarization of other types
of phrases contribute to the translation accuracy im-
provement as well.
Any of these simple binarization methods is easy
to implement, but is incapable of giving us all the
factorizable sub-phrases. Binarizing all the way to
the left, for example, from tree (1) to tree (2) and to
tree (4) in Figure 2, does not enable us to acquire a
substructure that yields NNP3 NNP4 and their trans-
lational equivalences. To obtain more factorizable
sub-phrases, we need to parallel-binarize in both di-
rections.
</bodyText>
<subsectionHeader confidence="0.996869">
4.2 Parallel binarization
</subsectionHeader>
<bodyText confidence="0.984050681818182">
Simple binarizations transform a parse tree into an-
other single parse tree. Parallel binarization will
transform a parse tree into a binarization forest,
desirably packed to enable dynamic programming
when extracting translation rules from it.
Borrowing terms from parsing semirings (Good-
man, 1999), a packed forest is composed of addi-
tive forest nodes (-nodes) and multiplicative forest
nodes (-nodes). In the binarization forest, a -
node corresponds to a tree node in the unbinarized
tree; and this -node composes several -nodes,
forming a one-level substructure that is observed in
the unbinarized tree. A -node corresponds to al-
ternative ways of binarizing the same tree node in
the unbinarized tree and it contains one or more -
nodes. The same -node can appear in more than
one place in the packed forest, enabling sharing.
Figure 3 shows a packed forest obtained by pack-
ing trees (4) and (7) in Figure 2 via the following
parallel binarization algorithm.
To parallel-binarize a tree node n that has children
n1, ..., nr, we employ the following steps:
</bodyText>
<equation confidence="0.9942758">
1(NPB)
2(NPB)
3(NPB)
4(NPB)
5(NPB)
6(NPB)
7(NNP1) 8(NNP2)
9(NNP3)
10(NNP4)
11(NPB)
7(NNP1) 12(NPB)
13(NPB)
8(NNP2) 14(NPB)
15(NPB)
9(NNP3) 10(NNP4)
</equation>
<figureCaption confidence="0.976171">
Figure 3: Packed forest obtained by packing trees (4) and (7)
</figureCaption>
<bodyText confidence="0.970540366666667">
in Figure 2
We recursively parallel-binarize children nodes
n1, ..., nr, producing binarization -nodes
(n1), ..., (nr), respectively.
We right-binarize n, if any contiguous1 subset
of children n2, ..., nr is factorizable, by intro-
ducing an intermediate tree node labeled as n.
We recursively parallel-binarize n to generate
a binarization forest node (n). We form a
multiplicative forest node R as the parent of
(n1) and (n).
We left-binarize n if any contiguous subset
of n1, ..., nr1 is factorizable and if this sub-
set contains n1. Similar to the above right-
binarization, we introduce an intermediate tree
node labeled as n, recursively parallel-binarize
n to generate a binarization forest node (n),
form a multiplicative forest node L as the par-
ent of (n) and (n1).
We form an additive node (n) as the parent
of the two already formed multiplicative nodes
L and R.
The (left and right) binarization conditions con-
sider any subset to enable the factorization of small
constituents. For example, in tree (1) of Figure 2,
although NNP1 NNP2 NNP3 of NPB are not factor-
izable, the subset NNP1 NNP2 is factorizable. The
binarization from tree (1) to tree (2) serves as a re-
laying step for us to factorize NNP1 NNP2 in tree
(4). The left-binarization condition is stricter than
</bodyText>
<page confidence="0.705883">
1
</page>
<bodyText confidence="0.96963025">
We factorize only subsets that cover contiguous spans to
avoid introducing dis-contiguous constituents for practical pur-
pose. In principle, the algorithm works fine without this bina-
rization condition.
</bodyText>
<page confidence="0.987083">
749
</page>
<bodyText confidence="0.997795113636364">
\x0cthe right-binarization condition to avoid spurious bi-
narization; i.e., to avoid the same subconstituent be-
ing reached via both binarizations. We could trans-
form tree (1) directly into tree (4) without bother-
ing to generate tree (3). However, skipping tree (3)
will create us difficulty in applying the EM algo-
rithm to choose a better binarization for each tree
node, since tree (4) can neither be classified as left
binarization nor as right binarization of the original
tree (1) it is the result of the composition of two
left-binarizations.
In parallel binarization, nodes are not always bi-
narizable in both directions. For example, we do not
need to right-binarize tree (2) because NNP2 NNP3
are not factorizable, and thus cannot be used to form
sub-phrases. It is still possible to right-binarize tree
(2) without affecting the correctness of the parallel
binarization algorithm, but that will spuriously in-
crease the branching factor of the search for the rule
extraction, because we will have to expand more tree
nodes.
A restricted version of parallel binarization is the
headed parallel binarization, where both the left and
the right binarization must respect the head propaga-
tion property at the same time.
A nice property of parallel binarization is that
for any factorizable substructure in the unbinarized
tree, we can always find a corresponding admissi-
ble -node in the parallel-binarized packed forest.
A leftmost substructure like the lowest NPB-subtree
in tree (4) of Figure 2 can be made factorizable
by several successive left binarizations, resulting in
5(NPB)-node in the packed forest in Figure 3. A
substructure in the middle can be factorized by the
composition of several left- and right-binarizations.
Therefore, after a tree is parallel-binarized, to make
the sub-phrases available to the MT system, all we
need to do is to extract rules from the admissible
nodes in the packed forest. Rules that can be ex-
tracted from the original unrestructured tree can be
extracted from the packed forest as well.
Parallel binarization results in parse forests. Thus
translation rules need to be extracted from training
data consisting of (e-forest, f, a)-tuples.
</bodyText>
<sectionHeader confidence="0.66727" genericHeader="method">
5 Extracting translation rules from
</sectionHeader>
<bodyText confidence="0.99109485">
(e-forest, f, a)-tuples
The algorithm to extract rules from (e-forest, f, a)-
tuples is a natural generalization of the (e-parse, f,
a)-based rule extraction algorithm in (Galley et al.,
2006). The input to the forest-based algorithm is a
(e-forest, f, a)-triple. The output of the algorithm is
a derivation forest (Galley et al., 2006) composed of
xRs rules. The algorithm recursively traverses the e-
forest top-down and extracts rules only at admissible
forest nodes.
The following procedure transforms the packed e-
forest in Figure 3 into a packed synchronous deriva-
tion in Figure 4.
Condition 1: Suppose we reach an additive
e-forest node, e.g. 1(NPB) in Figure 3. For
each of 1(NPB)s children, e-forest nodes
2(NPB) and 11(NPB), we go to condi-
tion 2 to recursively extract rules on these
two e-forest nodes, generating multiplicative
derivation forest nodes, i.e., (NPB(NPB :
</bodyText>
<equation confidence="0.993473333333333">
x0 NNP3(viktor) NNP4(chernomyrdin)4)
x0 V-C) and (NPB(NNP1 NPB(NNP2 : x0 NPB :
x1)) x0 x1 x2) in Figure 4. We make these
</equation>
<bodyText confidence="0.987184142857143">
new nodes children of (NPB) in the derivation
forest.
Condition 2: Suppose we reach a multiplicative
parse forest node, i.e., 11(NPB) in Figure 3. We
extract rules rooted at it using the procedure in
(Galley et al., 2006), forming multiplicative deriva-
tion forest nodes, i.e., (NPB(NNP1 NPB(NNP2 :
</bodyText>
<equation confidence="0.920562">
x0 NPB : x1)) x0 x1 x2) We then go
</equation>
<bodyText confidence="0.9994392">
to condition 1 to form the derivation forest on
the additive frontier e-forest nodes of the newly
extracted rules, generating additive derivation for-
est nodes, i.e., (NNP1), (NNP2) and (NPB).
We make these nodes the children of node
</bodyText>
<equation confidence="0.995727">
(NPB(NNP1 NPB(NNP2 : x0 NPB : x1))
x0 x1 x2) in the derivation forest.
</equation>
<bodyText confidence="0.995076833333333">
This algorithm is a natural extension of the extrac-
tion algorithm in (Galley et al., 2006) in the sense
that we have an extra condition (1) to relay rule ex-
traction on additive e-forest nodes.
It is worthwhile to eliminate the spuriously am-
biguous rules that are introduced by the parallel bi-
</bodyText>
<page confidence="0.821924">
750
</page>
<equation confidence="0.981767909090909">
\x0c(NPB)
NPB(NPB : x0 NNP(viktor) NNP(chernomyrdin)) x0 V-C
\x01
(NPB)
NPB(NNP : x0 NNP : x1 x0 x1)
\x01
NPB(NNP : x0 NPB(NNP : x1 NPB : x2)) x0 x1 x2
\x01
(NNP) (NNP) (NPB)
NPB(NNP(viktor) NNP(chernomyrdin)) V-C
\x01
</equation>
<figureCaption confidence="0.999012">
Figure 4: Derivation forest.
</figureCaption>
<bodyText confidence="0.4797805">
narization. For example, we may extract the follow-
ing two rules:
</bodyText>
<equation confidence="0.9892255">
- A(A(B:x0 C:x1)D:x2) x1 x0 x2
- A(B:x0 A(C:x1 D:x2)) x1 x0 x2
</equation>
<bodyText confidence="0.978892333333333">
These two rules, however, are not really distinct.
They both converge to the following rules if we
delete the auxiliary nodes A.
</bodyText>
<equation confidence="0.901742">
- A(B:x0 C:x1 D:x2) x1 x0 x2
</equation>
<bodyText confidence="0.989718269230769">
The forest-base rule extraction algorithm pro-
duces much larger grammars than the tree-based
one, making it difficult to scale to very large training
data. From a 50M-word Chinese-to-English parallel
corpus, we can extract more than 300 million trans-
lation rules, while the tree-based rule extraction al-
gorithm gives approximately 100 million. However,
the restructured trees from the simple binarization
methods are not guaranteed to give the best trees for
syntax-based machine translation. What we desire is
a binarization method that still produces single parse
trees, but is able to mix left binarization and right
binarization in the same tree. In the following, we
shall use the EM algorithm to learn the desirable bi-
narization on the forest of binarization alternatives
proposed by the parallel binarization algorithm.
6 Learning how to binarize via the EM
algorithm
The basic idea of applying the EM algorithm to
choose a restructuring is as follows. We perform a
set {} of binarization operations on a parse tree .
Each binarization is the sequence of binarizations
on the necessary (i.e., factorizable) nodes in in pre-
order. Each binarization results in a restructured
tree . We extract rules from (, f, a), generating a
translation model consisting of parameters (i.e., rule
</bodyText>
<figure confidence="0.886248764705882">
eparse
(Galley et al., 2006)
composed rule extraction
1
2
parallel binarization eforest
forestbased rule extraction
of minimal rules
f,a
synchronous derivation forests
EM
3
4
viterbi derivations
project eparse
model
syntax translation
</figure>
<figureCaption confidence="0.999809">
Figure 5: Using the EM algorithm to choose restructuring.
</figureCaption>
<bodyText confidence="0.857157">
probabilities) . Our aim is to obtain the binarization
that gives the best likelihood of the restructured
training data consisting of (, f , a)-tuples. That is
= arg max
p(, f, a|
</bodyText>
<equation confidence="0.744026">
) (1)
</equation>
<bodyText confidence="0.998125538461538">
In practice, we cannot enumerate all the exponen-
tial number of binarized trees for a given e-parse.
We therefore use the packed forest to store all the
binarizations that operate on an e-parse in a com-
pact way, and then use the inside-outside algorithm
(Lari and Young, 1990; Knight and Graehl, 2004)
for model estimation.
The probability p(, f, a) of a (, f, a)-tuple
is what the basic syntax-based translation model is
concerned with. It can be further computed by ag-
gregating the rule probabilities p(r) in each deriva-
tion in the set of all derivations (Galley et al.,
2004; Marcu et al., 2006). That is
</bodyText>
<equation confidence="0.989115">
p(, f, a) =
X
Y
r
p(r) (2)
</equation>
<bodyText confidence="0.9984335">
Since it has been well-known that applying EM
with tree fragments of different sizes causes over-
fitting (Johnson, 1998), and since it is also known
that syntax MT models with larger composed rules
in the mix significantly outperform rules that min-
imally explain the training data (minimal rules) in
</bodyText>
<page confidence="0.987322">
751
</page>
<bodyText confidence="0.995487235294118">
\x0ctranslation accuracy (Galley et al., 2006), we decom-
pose p(b, f, a) using minimal rules during running
of the EM algorithm, but, after the EM restructuring
is finished, we build the final translation model using
composed rules for evaluation.
Figure 5 is the actual pipeline that we use for
EM binarization. We first generate a packed e-forest
via parallel binarization. We then extract minimal
translation rules from the (e-forest, f, a)-tuples, pro-
ducing synchronous derivation forests. We run the
inside-outside algorithm on the derivation forests
until convergence. We obtain the Viterbi derivations
and project the English parses from the derivations.
Finally, we extract composed rules using Galley et
al. (2006)s (e-tree, f, a)-based rule extraction algo-
rithm. This procedure corresponds to the path 1342
in the pipeline.
</bodyText>
<sectionHeader confidence="0.998504" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.99908725">
We carried out a series of experiments to compare
the performance of different binarization methods
in terms of BLEU on Chinese-to-English translation
tasks.
</bodyText>
<subsectionHeader confidence="0.993222">
7.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999213578947368">
Our bitext consists of 16M words, all in the
mainland-news domain. Our development set is a
925-line subset of the 993-line NIST02 evaluation
set. We removed long sentences from the NIST02
evaluation set to speed up discriminative training.
The test set is the full 919-line NIST03 evaluation
set.
We used a bottom-up, CKY-style decoder that
works with binary xRs rules obtained via a syn-
chronous binarization procedure (Zhang et al.,
2006). The decoder prunes hypotheses using strate-
gies described in (Chiang, 2007).
The parse trees on the English side of the bitexts
were generated using a parser (Soricut, 2004) imple-
menting the Collins parsing models (Collins, 1997).
We used the EM procedure described in (Knight
and Graehl, 2004) to perform the inside-outside al-
gorithm on synchronous derivation forests and to
generate the Viterbi derivation forest.
We used the rule extractor described in (Galley et
al., 2006) to extract rules from (e-parse, f, a)-tuples,
but we made an important modification: new nodes
introduced by binarization will not be counted when
computing the rule size limit unless they appear as
the rule roots. The motivation is that binarization
deepens the parses and increases the number of tree
nodes. In (Galley et al., 2006), a composed rule
is extracted only if the number of internal nodes it
contains does not exceed a limit (i.e., 4), similar
to the phrase length limit in phrase-based systems.
This means that rules extracted from the restructured
trees will be smaller than those from the unrestruc-
tured trees, if the X nodes are deleted. As shown in
(Galley et al., 2006), smaller rules lose context, and
thus give lower translation performance. Ignoring X
nodes when computing the rule sizes preserves the
unstructured rules in the resulting translation model
and adds substructures as bonuses.
</bodyText>
<subsectionHeader confidence="0.9676">
7.2 Experiment results
</subsectionHeader>
<bodyText confidence="0.925044448275862">
Table 1 shows the BLEU scores of mixed-cased and
detokenized translations of different systems. We
see that all the binarization methods improve the
baseline system that does not apply any binarization
algorithm. The EM-binarization performs the best
among all the restructuring methods, leading to 1.0
BLEU point improvement. We also computed the
bootstrap p-values (Riezler and Maxwell, 2005) for
the pairwise BLEU comparison between the base-
line system and any of the system trained from bina-
rized trees. The significance test shows that the EM
binarization result is statistically significant better
than the baseline system (p &amp;gt; 0.005), even though
the baseline is already quite strong. To our best
knowledge, 37.94 is the highest BLEU score on this
test set to date.
Also as shown in Table 1, the grammars trained
from the binarized training trees are almost two
times of the grammar size with no binarization. The
extra rules are substructures factored out by these bi-
narization methods.
How many more substructures (or translation
rules) can be acquired is partially determined by
how many more admissible nodes each binariza-
tion method can factorize, since rules are extractable
only from admissible tree nodes. According to
Table 1, binarization methods significantly increase
the number of admissible nodes in the training trees.
The EM binarization makes available the largest
</bodyText>
<page confidence="0.944771">
752
</page>
<table confidence="0.9980035">
\x0cEXPERIMENT NIST03-BLEU # RULES # ADMISSIBLE NODES IN TRAINING
no-bin 36.94 63.4M 7,995,569
left binarization 37.47 (p = 0.047) 114.0M 10,463,148
right binarization 37.49 (p = 0.044) 113.0M 10,413,194
head binarization 37.54 (p = 0.086) 113.8M 10,534,339
EM binarization 37.94 (p = 0.0047) 115.6M 10,658,859
</table>
<tableCaption confidence="0.835198666666667">
Table 1: Translation performance, grammar size and # admissible nodes versus binarization algorithms. BLEU scores are for
mixed-cased and detokenized translations, as we usually do for NIST MT evaluations.
nonterminal left-binarization right-binarization
</tableCaption>
<table confidence="0.998565611111111">
NP 96.97% 3.03%
NP-C 97.49% 2.51%
NPB 0.25% 99.75%
VP 93.90% 6.10%
PP 83.75% 16.25%
ADJP 87.83% 12.17%
ADVP 82.74% 17.26%
S 85.91% 14.09%
S-C 18.88% 81.12%
SBAR 96.69% 3.31%
QP 86.40% 13.60%
PRN 85.18% 14.82%
WHNP 97.93% 2.07%
NX 100% 0
SINV 87.78% 12.22%
PRT 100% 0
SQ 93.53% 6.47%
CONJP 18.08% 81.92%
</table>
<tableCaption confidence="0.997576">
Table 2: Binarization bias learned by EM.
</tableCaption>
<bodyText confidence="0.992448863636364">
number of admissible nodes, and thus results in the
most rules.
The EM binarization factorizes more admissible
nodes because it mixes both left and right binariza-
tions in the same tree. We computed the binarization
biases learned by the EM algorithm for each nonter-
minal from the binarization forest of headed-parallel
binarizations of the training trees, getting the statis-
tics in Table 2. Of course, the binarization bias
chosen by left-/right-binarization methods would be
100% deterministic. One noticeable message from
Table 2 is that most of the categories are actually bi-
ased toward left-binarization, although our motivat-
ing example in our introduction section is for NPB,
which needed right binarization. The main reason
might be that the head sub-constituents of most cat-
egories tend to be on the left, but according to the
performance comparison between head binarization
and EM binarization, head binarization does not suf-
fice because we still need to choose the binarization
between left and right if they both are head binariza-
tions.
</bodyText>
<sectionHeader confidence="0.998976" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999824">
In this paper, we not only studied the impact of
simple tree binarization algorithms on the perfor-
mance of end-to-end syntax-based MT, but also pro-
posed binarization methods that mix more than one
simple binarization in the binarization of the same
parse tree. Binarizing a tree node whether to the left
or to the right was learned by employing the EM
algorithm on a set of alternative binarizations and
by choosing the Viterbi one. The EM binarization
method is informed by word alignments such that
unnecessary new tree nodes will not be blindly in-
troduced.
To our best knowledge, our research is the first
work that aims to generalize a syntax-based trans-
lation model by restructuring and achieves signifi-
cant improvement on a strong baseline. Our work
differs from traditional work on binarization of syn-
chronous grammars in that we are not concerned
with the equivalence of the binarized grammar to the
original grammar, but intend to generalize the orig-
inal grammar via restructuring of the training parse
trees to improve translation performance.
</bodyText>
<sectionHeader confidence="0.978374" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993026333333333">
The authors would like to thank David Chiang,
Bryant Huang, and the anonymous reviewers for
their valuable feedbacks.
</bodyText>
<sectionHeader confidence="0.988554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997597">
E. Charniak. 2000. A maximum-entropy-inspiredparser.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, Seattle, May.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of the
</reference>
<page confidence="0.974798">
753
</page>
<reference confidence="0.999740568965517">
\x0c35th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 1623, Madrid, Spain,
July.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):138.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 205208, Sapporo,
July.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
Whats in a Translation Rule? In Proceedings of
the Human Language Technology Conference and the
North American Association for Computational Lin-
guistics (HLT-NAACL), Boston, Massachusetts.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable Inference and
Training of Context-Rich Syntactic Models. In Pro-
ceedings of the 44th Annual Meeting of the Association
for Computational Linguistics (ACL).
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573605.
M. Johnson. 1998. The DOP estimation method is
biased and inconsistent. Computational Linguistics,
28(1):7176.
K. Knight and J. Graehl. 2004. Training Tree Transduc-
ers. In Proceedings of NAACL-HLT.
K. Lari and S. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algo-
rithm. Computer Speech and Language, pages 3556.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phraases.
In Proceedings of EMNLP-2006, pp. 44-52, Sydney,
Australia.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313330.
I. Dan Melamed, Giorgio Satta, and Benjamin Welling-
ton. 2004. Generalized multitext grammars. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Barcelona,
Spain.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proc. ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or Summa-
rization.
Radu Soricut. 2004. A reimplementation of Collinss
parsing models. Technical report, Information Sci-
ences Institute, Department of Computer Science Uni-
versity of Southern California.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the HLT-NAACL.
</reference>
<page confidence="0.995503">
754
</page>
<figure confidence="0.244502">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.730656">
<note confidence="0.984513666666667">b&amp;apos;Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 746754, Prague, June 2007. c 2007 Association for Computational Linguistics</note>
<title confidence="0.937852">Binarizing Syntax Trees to Improve Syntax-Based Machine Translation Accuracy</title>
<author confidence="0.999364">Wei Wang</author>
<author confidence="0.999364">Kevin Knight</author>
<author confidence="0.999364">Daniel Marcu</author>
<affiliation confidence="0.932778">Language Weaver, Inc.</affiliation>
<address confidence="0.9973835">4640 Admiralty Way, Suite 1210 Marina del Rey, CA, 90292</address>
<email confidence="0.999527">wwang@languageweaver.com</email>
<email confidence="0.999527">kknight@languageweaver.com</email>
<email confidence="0.999527">dmarcu@languageweaver.com</email>
<abstract confidence="0.994117">We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspiredparser.</title>
<date>2000</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Seattle,</location>
<contexts>
<context position="3233" citStr="Charniak, 2000" startWordPosition="523" endWordPosition="524"> that collapses the two NNPs, so as to generalize this rule, getting rule R5 and rule R6. We also need to consistently syntactify the root labels of R4 and the new frontier label of R6 such that these two rules can be composed. Since labeling is not a concern of this paper, we simply label new nodes with X-bar where X here is the parent label. With all these in place, we now can translate the foreign sentence by composing R6 and R4 in Figure 1. Binarizing the syntax trees for syntax-based machine translation is similar in spirit to generalizing parsing models via markovization (Collins, 1997; Charniak, 2000). But in translation modeling, it is unclear how to effectively markovize the translation rules, especially when the rules are complex like those proposed by Galley et al. (2006). In this paper, we explore the generalization ability of simple binarization methods like left-, right-, and head-binarization, and also their combinations. Simple binarization methods binarize syntax trees in a consistent fashion (left-, right-, or head-) and 746 \x0cNPB JJ NNP NNP NNP russia minister viktor chernomyrdin RUSSIA MINISTER VC NPB ? NNP NNP AND HIS COLLEAGUE VC AND HIS COLLEAGUE NPB JJ russia RUSSIA mini</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspiredparser. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Seattle, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="22275" citStr="Chiang, 2007" startWordPosition="3675" endWordPosition="3676">ization methods in terms of BLEU on Chinese-to-English translation tasks. 7.1 Experimental setup Our bitext consists of 16M words, all in the mainland-news domain. Our development set is a 925-line subset of the 993-line NIST02 evaluation set. We removed long sentences from the NIST02 evaluation set to speed up discriminative training. The test set is the full 919-line NIST03 evaluation set. We used a bottom-up, CKY-style decoder that works with binary xRs rules obtained via a synchronous binarization procedure (Zhang et al., 2006). The decoder prunes hypotheses using strategies described in (Chiang, 2007). The parse trees on the English side of the bitexts were generated using a parser (Soricut, 2004) implementing the Collins parsing models (Collins, 1997). We used the EM procedure described in (Knight and Graehl, 2004) to perform the inside-outside algorithm on synchronous derivation forests and to generate the Viterbi derivation forest. We used the rule extractor described in (Galley et al., 2006) to extract rules from (e-parse, f, a)-tuples, but we made an important modification: new nodes introduced by binarization will not be counted when computing the rule size limit unless they appear a</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the \x0c35th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1623</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="3216" citStr="Collins, 1997" startWordPosition="521" endWordPosition="522">a new tree node that collapses the two NNPs, so as to generalize this rule, getting rule R5 and rule R6. We also need to consistently syntactify the root labels of R4 and the new frontier label of R6 such that these two rules can be composed. Since labeling is not a concern of this paper, we simply label new nodes with X-bar where X here is the parent label. With all these in place, we now can translate the foreign sentence by composing R6 and R4 in Figure 1. Binarizing the syntax trees for syntax-based machine translation is similar in spirit to generalizing parsing models via markovization (Collins, 1997; Charniak, 2000). But in translation modeling, it is unclear how to effectively markovize the translation rules, especially when the rules are complex like those proposed by Galley et al. (2006). In this paper, we explore the generalization ability of simple binarization methods like left-, right-, and head-binarization, and also their combinations. Simple binarization methods binarize syntax trees in a consistent fashion (left-, right-, or head-) and 746 \x0cNPB JJ NNP NNP NNP russia minister viktor chernomyrdin RUSSIA MINISTER VC NPB ? NNP NNP AND HIS COLLEAGUE VC AND HIS COLLEAGUE NPB JJ r</context>
<context position="22429" citStr="Collins, 1997" startWordPosition="3700" endWordPosition="3701">ws domain. Our development set is a 925-line subset of the 993-line NIST02 evaluation set. We removed long sentences from the NIST02 evaluation set to speed up discriminative training. The test set is the full 919-line NIST03 evaluation set. We used a bottom-up, CKY-style decoder that works with binary xRs rules obtained via a synchronous binarization procedure (Zhang et al., 2006). The decoder prunes hypotheses using strategies described in (Chiang, 2007). The parse trees on the English side of the bitexts were generated using a parser (Soricut, 2004) implementing the Collins parsing models (Collins, 1997). We used the EM procedure described in (Knight and Graehl, 2004) to perform the inside-outside algorithm on synchronous derivation forests and to generate the Viterbi derivation forest. We used the rule extractor described in (Galley et al., 2006) to extract rules from (e-parse, f, a)-tuples, but we made an important modification: new nodes introduced by binarization will not be counted when computing the rule size limit unless they appear as the rule roots. The motivation is that binarization deepens the parses and increases the number of tree nodes. In (Galley et al., 2006), a composed rule</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalized models for statistical parsing. In Proceedings of the \x0c35th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1623, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="4481" citStr="Dempster et al., 1977" startWordPosition="732" endWordPosition="735">or chernomyrdin VC NPB colleague and his colleague COLLEAGUE PRB$ his HIS CC and AND NNP NNP x0:NNP x1:NNP VC AND HIS COLLEAGUE NNS x0:NNP x1:NNP CC PRB$ NNS R1 R2 NPB NPB R3 NPB NPB R4 R5 R6 R6 R4 NPB Figure 1: Generalizing translation rules by binarizing trees. thus cannot guarantee that all the substructures can be factored out. For example, right binarization on the LHS of R1 makes available R4, but misses R6 on R2. We then introduce a parallel restructuring method, that is, one can binarize both to the left and right at the same time, resulting in a binarization forest. We employ the EM (Dempster et al., 1977) algorithm to learn the binarization bias for each tree node from the parallel alternatives. The EM-binarization yields best translation performance. The rest of the paper is organized as follows. Section 2 describes related research. Section 3 defines the concepts necessary for describing the binarizations methods. Section 4 describes the tree binarization methods in details. Section 5 describes the forest-based rule extraction algorithm, and section 6 explains how we restructure the trees using the EM algorithm. The last two sections are for experiments and conclusions. 2 Related Research Se</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>205208</pages>
<location>Sapporo,</location>
<contexts>
<context position="1021" citStr="Eisner, 2003" startWordPosition="147" endWordPosition="148">CA, 90292 {wwang,kknight,dmarcu}@languageweaver.com Abstract We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R1 in Figure 1 that translates</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 205208, Sapporo, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>M Hopkins</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Whats in a Translation Rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics (HLT-NAACL),</booktitle>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="6108" citStr="Galley et al., 2004" startWordPosition="982" endWordPosition="985">mployed to make the grammar fit in a CKY parser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, synchronous grammars built from restructured (binarized) training trees still contain non-binary, multi-level rules and thus still require the binarization transformation so as to be employed by a CKY parser. The translation model we are using in this paper belongs to the xRs formalism (Knight and Graehl, 2004), which has been proved successful for machine translation in (Galley et al., 2004; Galley et al., 2006; Marcu et al., 2006). 3 Concepts We focus on tree-to-string (in noisy-channel model sense) translation models. Translation models of this type are typically trained on tuples of a sourcelanguage sentence f, a target language (e.g., English) parse tree that yields e and translates from f, and the word alignments a between e and f. Such a tuple is called an alignment graph in (Galley et al., 2004). The graph (1) in Figure 2 is such an alignment graph. 747 \x0c(1) unbinarized tree NPB viktor chernomyrdin VIKTORCHERNOMYRDIN NNP1 NNP2 NNP3 NNP4* (2) left-binarization (3) right</context>
<context position="20370" citStr="Galley et al., 2004" startWordPosition="3371" endWordPosition="3374">-tuples. That is = arg max p(, f, a| ) (1) In practice, we cannot enumerate all the exponential number of binarized trees for a given e-parse. We therefore use the packed forest to store all the binarizations that operate on an e-parse in a compact way, and then use the inside-outside algorithm (Lari and Young, 1990; Knight and Graehl, 2004) for model estimation. The probability p(, f, a) of a (, f, a)-tuple is what the basic syntax-based translation model is concerned with. It can be further computed by aggregating the rule probabilities p(r) in each derivation in the set of all derivations (Galley et al., 2004; Marcu et al., 2006). That is p(, f, a) = X Y r p(r) (2) Since it has been well-known that applying EM with tree fragments of different sizes causes overfitting (Johnson, 1998), and since it is also known that syntax MT models with larger composed rules in the mix significantly outperform rules that minimally explain the training data (minimal rules) in 751 \x0ctranslation accuracy (Galley et al., 2006), we decompose p(b, f, a) using minimal rules during running of the EM algorithm, but, after the EM restructuring is finished, we build the final translation model using composed rules for eval</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004. Whats in a Translation Rule? In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics (HLT-NAACL), Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefe</author>
<author>W Wang</author>
<author>I Thayer</author>
</authors>
<title>Scalable Inference and Training of Context-Rich Syntactic Models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1042" citStr="Galley et al., 2006" startWordPosition="149" endWordPosition="153">ng,kknight,dmarcu}@languageweaver.com Abstract We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R1 in Figure 1 that translates the Chinese phrase R</context>
<context position="3411" citStr="Galley et al. (2006)" startWordPosition="549" endWordPosition="552">label of R6 such that these two rules can be composed. Since labeling is not a concern of this paper, we simply label new nodes with X-bar where X here is the parent label. With all these in place, we now can translate the foreign sentence by composing R6 and R4 in Figure 1. Binarizing the syntax trees for syntax-based machine translation is similar in spirit to generalizing parsing models via markovization (Collins, 1997; Charniak, 2000). But in translation modeling, it is unclear how to effectively markovize the translation rules, especially when the rules are complex like those proposed by Galley et al. (2006). In this paper, we explore the generalization ability of simple binarization methods like left-, right-, and head-binarization, and also their combinations. Simple binarization methods binarize syntax trees in a consistent fashion (left-, right-, or head-) and 746 \x0cNPB JJ NNP NNP NNP russia minister viktor chernomyrdin RUSSIA MINISTER VC NPB ? NNP NNP AND HIS COLLEAGUE VC AND HIS COLLEAGUE NPB JJ russia RUSSIA minister MINISTER NNP NNP viktor chernomyrdin VC NPB colleague and his colleague COLLEAGUE PRB$ his HIS CC and AND NNP NNP x0:NNP x1:NNP VC AND HIS COLLEAGUE NNS x0:NNP x1:NNP CC PRB</context>
<context position="6129" citStr="Galley et al., 2006" startWordPosition="986" endWordPosition="989">rammar fit in a CKY parser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, synchronous grammars built from restructured (binarized) training trees still contain non-binary, multi-level rules and thus still require the binarization transformation so as to be employed by a CKY parser. The translation model we are using in this paper belongs to the xRs formalism (Knight and Graehl, 2004), which has been proved successful for machine translation in (Galley et al., 2004; Galley et al., 2006; Marcu et al., 2006). 3 Concepts We focus on tree-to-string (in noisy-channel model sense) translation models. Translation models of this type are typically trained on tuples of a sourcelanguage sentence f, a target language (e.g., English) parse tree that yields e and translates from f, and the word alignments a between e and f. Such a tuple is called an alignment graph in (Galley et al., 2004). The graph (1) in Figure 2 is such an alignment graph. 747 \x0c(1) unbinarized tree NPB viktor chernomyrdin VIKTORCHERNOMYRDIN NNP1 NNP2 NNP3 NNP4* (2) left-binarization (3) right-/head-binarization N</context>
<context position="15754" citStr="Galley et al., 2006" startWordPosition="2597" endWordPosition="2600"> sub-phrases available to the MT system, all we need to do is to extract rules from the admissible nodes in the packed forest. Rules that can be extracted from the original unrestructured tree can be extracted from the packed forest as well. Parallel binarization results in parse forests. Thus translation rules need to be extracted from training data consisting of (e-forest, f, a)-tuples. 5 Extracting translation rules from (e-forest, f, a)-tuples The algorithm to extract rules from (e-forest, f, a)- tuples is a natural generalization of the (e-parse, f, a)-based rule extraction algorithm in (Galley et al., 2006). The input to the forest-based algorithm is a (e-forest, f, a)-triple. The output of the algorithm is a derivation forest (Galley et al., 2006) composed of xRs rules. The algorithm recursively traverses the eforest top-down and extracts rules only at admissible forest nodes. The following procedure transforms the packed eforest in Figure 3 into a packed synchronous derivation in Figure 4. Condition 1: Suppose we reach an additive e-forest node, e.g. 1(NPB) in Figure 3. For each of 1(NPB)s children, e-forest nodes 2(NPB) and 11(NPB), we go to condition 2 to recursively extract rules on these t</context>
<context position="17289" citStr="Galley et al., 2006" startWordPosition="2856" endWordPosition="2859">forest node, i.e., 11(NPB) in Figure 3. We extract rules rooted at it using the procedure in (Galley et al., 2006), forming multiplicative derivation forest nodes, i.e., (NPB(NNP1 NPB(NNP2 : x0 NPB : x1)) x0 x1 x2) We then go to condition 1 to form the derivation forest on the additive frontier e-forest nodes of the newly extracted rules, generating additive derivation forest nodes, i.e., (NNP1), (NNP2) and (NPB). We make these nodes the children of node (NPB(NNP1 NPB(NNP2 : x0 NPB : x1)) x0 x1 x2) in the derivation forest. This algorithm is a natural extension of the extraction algorithm in (Galley et al., 2006) in the sense that we have an extra condition (1) to relay rule extraction on additive e-forest nodes. It is worthwhile to eliminate the spuriously ambiguous rules that are introduced by the parallel bi750 \x0c(NPB) NPB(NPB : x0 NNP(viktor) NNP(chernomyrdin)) x0 V-C \x01 (NPB) NPB(NNP : x0 NNP : x1 x0 x1) \x01 NPB(NNP : x0 NPB(NNP : x1 NPB : x2)) x0 x1 x2 \x01 (NNP) (NNP) (NPB) NPB(NNP(viktor) NNP(chernomyrdin)) V-C \x01 Figure 4: Derivation forest. narization. For example, we may extract the following two rules: - A(A(B:x0 C:x1)D:x2) x1 x0 x2 - A(B:x0 A(C:x1 D:x2)) x1 x0 x2 These two rules, h</context>
<context position="19343" citStr="Galley et al., 2006" startWordPosition="3198" endWordPosition="3201">thm to learn the desirable binarization on the forest of binarization alternatives proposed by the parallel binarization algorithm. 6 Learning how to binarize via the EM algorithm The basic idea of applying the EM algorithm to choose a restructuring is as follows. We perform a set {} of binarization operations on a parse tree . Each binarization is the sequence of binarizations on the necessary (i.e., factorizable) nodes in in preorder. Each binarization results in a restructured tree . We extract rules from (, f, a), generating a translation model consisting of parameters (i.e., rule eparse (Galley et al., 2006) composed rule extraction 1 2 parallel binarization eforest forestbased rule extraction of minimal rules f,a synchronous derivation forests EM 3 4 viterbi derivations project eparse model syntax translation Figure 5: Using the EM algorithm to choose restructuring. probabilities) . Our aim is to obtain the binarization that gives the best likelihood of the restructured training data consisting of (, f , a)-tuples. That is = arg max p(, f, a| ) (1) In practice, we cannot enumerate all the exponential number of binarized trees for a given e-parse. We therefore use the packed forest to store all t</context>
<context position="20777" citStr="Galley et al., 2006" startWordPosition="3442" endWordPosition="3445">tuple is what the basic syntax-based translation model is concerned with. It can be further computed by aggregating the rule probabilities p(r) in each derivation in the set of all derivations (Galley et al., 2004; Marcu et al., 2006). That is p(, f, a) = X Y r p(r) (2) Since it has been well-known that applying EM with tree fragments of different sizes causes overfitting (Johnson, 1998), and since it is also known that syntax MT models with larger composed rules in the mix significantly outperform rules that minimally explain the training data (minimal rules) in 751 \x0ctranslation accuracy (Galley et al., 2006), we decompose p(b, f, a) using minimal rules during running of the EM algorithm, but, after the EM restructuring is finished, we build the final translation model using composed rules for evaluation. Figure 5 is the actual pipeline that we use for EM binarization. We first generate a packed e-forest via parallel binarization. We then extract minimal translation rules from the (e-forest, f, a)-tuples, producing synchronous derivation forests. We run the inside-outside algorithm on the derivation forests until convergence. We obtain the Viterbi derivations and project the English parses from th</context>
<context position="22677" citStr="Galley et al., 2006" startWordPosition="3737" endWordPosition="3740">We used a bottom-up, CKY-style decoder that works with binary xRs rules obtained via a synchronous binarization procedure (Zhang et al., 2006). The decoder prunes hypotheses using strategies described in (Chiang, 2007). The parse trees on the English side of the bitexts were generated using a parser (Soricut, 2004) implementing the Collins parsing models (Collins, 1997). We used the EM procedure described in (Knight and Graehl, 2004) to perform the inside-outside algorithm on synchronous derivation forests and to generate the Viterbi derivation forest. We used the rule extractor described in (Galley et al., 2006) to extract rules from (e-parse, f, a)-tuples, but we made an important modification: new nodes introduced by binarization will not be counted when computing the rule size limit unless they appear as the rule roots. The motivation is that binarization deepens the parses and increases the number of tree nodes. In (Galley et al., 2006), a composed rule is extracted only if the number of internal nodes it contains does not exceed a limit (i.e., 4), similar to the phrase length limit in phrase-based systems. This means that rules extracted from the restructured trees will be smaller than those fro</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe, W. Wang, and I. Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Models. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="10889" citStr="Goodman, 1999" startWordPosition="1810" endWordPosition="1812">g all the way to the left, for example, from tree (1) to tree (2) and to tree (4) in Figure 2, does not enable us to acquire a substructure that yields NNP3 NNP4 and their translational equivalences. To obtain more factorizable sub-phrases, we need to parallel-binarize in both directions. 4.2 Parallel binarization Simple binarizations transform a parse tree into another single parse tree. Parallel binarization will transform a parse tree into a binarization forest, desirably packed to enable dynamic programming when extracting translation rules from it. Borrowing terms from parsing semirings (Goodman, 1999), a packed forest is composed of additive forest nodes (-nodes) and multiplicative forest nodes (-nodes). In the binarization forest, a - node corresponds to a tree node in the unbinarized tree; and this -node composes several -nodes, forming a one-level substructure that is observed in the unbinarized tree. A -node corresponds to alternative ways of binarizing the same tree node in the unbinarized tree and it contains one or more - nodes. The same -node can appear in more than one place in the packed forest, enabling sharing. Figure 3 shows a packed forest obtained by packing trees (4) and (7</context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>Joshua Goodman. 1999. Semiring parsing. Computational Linguistics, 25(4):573605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>The DOP estimation method is biased and inconsistent.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="20547" citStr="Johnson, 1998" startWordPosition="3407" endWordPosition="3408">tore all the binarizations that operate on an e-parse in a compact way, and then use the inside-outside algorithm (Lari and Young, 1990; Knight and Graehl, 2004) for model estimation. The probability p(, f, a) of a (, f, a)-tuple is what the basic syntax-based translation model is concerned with. It can be further computed by aggregating the rule probabilities p(r) in each derivation in the set of all derivations (Galley et al., 2004; Marcu et al., 2006). That is p(, f, a) = X Y r p(r) (2) Since it has been well-known that applying EM with tree fragments of different sizes causes overfitting (Johnson, 1998), and since it is also known that syntax MT models with larger composed rules in the mix significantly outperform rules that minimally explain the training data (minimal rules) in 751 \x0ctranslation accuracy (Galley et al., 2006), we decompose p(b, f, a) using minimal rules during running of the EM algorithm, but, after the EM restructuring is finished, we build the final translation model using composed rules for evaluation. Figure 5 is the actual pipeline that we use for EM binarization. We first generate a packed e-forest via parallel binarization. We then extract minimal translation rules</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>M. Johnson. 1998. The DOP estimation method is biased and inconsistent. Computational Linguistics, 28(1):7176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<title>Training Tree Transducers.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="1585" citStr="Knight and Graehl, 2004" startWordPosition="235" endWordPosition="238">1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R1 in Figure 1 that translates the Chinese phrase RUSSIA MINISTER VIKTOR-CHERNOMYRDIN into an English NPB tree fragment yielding an English phrase. Also suppose that we want to translate a Chinese phrase VIKTOR-CHERNOMYRDIN AND HIS COLLEAGUE into English. What we desire is that if we have another rule R2 as shown in Figure 1, we could somehow compose it with R1 to obtain the desirable translation. We unfortunately cannot do this because R1 and R2 are not further decomposable and their substructures cannot be re-used. The requirement that all translation rules have exactly one root node d</context>
<context position="6026" citStr="Knight and Graehl, 2004" startWordPosition="968" endWordPosition="971">robability to each string as the original grammar does. Grammar binarization is often employed to make the grammar fit in a CKY parser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, synchronous grammars built from restructured (binarized) training trees still contain non-binary, multi-level rules and thus still require the binarization transformation so as to be employed by a CKY parser. The translation model we are using in this paper belongs to the xRs formalism (Knight and Graehl, 2004), which has been proved successful for machine translation in (Galley et al., 2004; Galley et al., 2006; Marcu et al., 2006). 3 Concepts We focus on tree-to-string (in noisy-channel model sense) translation models. Translation models of this type are typically trained on tuples of a sourcelanguage sentence f, a target language (e.g., English) parse tree that yields e and translates from f, and the word alignments a between e and f. Such a tuple is called an alignment graph in (Galley et al., 2004). The graph (1) in Figure 2 is such an alignment graph. 747 \x0c(1) unbinarized tree NPB viktor ch</context>
<context position="20094" citStr="Knight and Graehl, 2004" startWordPosition="3322" endWordPosition="3325">tion forests EM 3 4 viterbi derivations project eparse model syntax translation Figure 5: Using the EM algorithm to choose restructuring. probabilities) . Our aim is to obtain the binarization that gives the best likelihood of the restructured training data consisting of (, f , a)-tuples. That is = arg max p(, f, a| ) (1) In practice, we cannot enumerate all the exponential number of binarized trees for a given e-parse. We therefore use the packed forest to store all the binarizations that operate on an e-parse in a compact way, and then use the inside-outside algorithm (Lari and Young, 1990; Knight and Graehl, 2004) for model estimation. The probability p(, f, a) of a (, f, a)-tuple is what the basic syntax-based translation model is concerned with. It can be further computed by aggregating the rule probabilities p(r) in each derivation in the set of all derivations (Galley et al., 2004; Marcu et al., 2006). That is p(, f, a) = X Y r p(r) (2) Since it has been well-known that applying EM with tree fragments of different sizes causes overfitting (Johnson, 1998), and since it is also known that syntax MT models with larger composed rules in the mix significantly outperform rules that minimally explain the </context>
<context position="22494" citStr="Knight and Graehl, 2004" startWordPosition="3709" endWordPosition="3712">the 993-line NIST02 evaluation set. We removed long sentences from the NIST02 evaluation set to speed up discriminative training. The test set is the full 919-line NIST03 evaluation set. We used a bottom-up, CKY-style decoder that works with binary xRs rules obtained via a synchronous binarization procedure (Zhang et al., 2006). The decoder prunes hypotheses using strategies described in (Chiang, 2007). The parse trees on the English side of the bitexts were generated using a parser (Soricut, 2004) implementing the Collins parsing models (Collins, 1997). We used the EM procedure described in (Knight and Graehl, 2004) to perform the inside-outside algorithm on synchronous derivation forests and to generate the Viterbi derivation forest. We used the rule extractor described in (Galley et al., 2006) to extract rules from (e-parse, f, a)-tuples, but we made an important modification: new nodes introduced by binarization will not be counted when computing the rule size limit unless they appear as the rule roots. The motivation is that binarization deepens the parses and increases the number of tree nodes. In (Galley et al., 2006), a composed rule is extracted only if the number of internal nodes it contains do</context>
</contexts>
<marker>Knight, Graehl, 2004</marker>
<rawString>K. Knight and J. Graehl. 2004. Training Tree Transducers. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>3556</pages>
<contexts>
<context position="20068" citStr="Lari and Young, 1990" startWordPosition="3318" endWordPosition="3321">f,a synchronous derivation forests EM 3 4 viterbi derivations project eparse model syntax translation Figure 5: Using the EM algorithm to choose restructuring. probabilities) . Our aim is to obtain the binarization that gives the best likelihood of the restructured training data consisting of (, f , a)-tuples. That is = arg max p(, f, a| ) (1) In practice, we cannot enumerate all the exponential number of binarized trees for a given e-parse. We therefore use the packed forest to store all the binarizations that operate on an e-parse in a compact way, and then use the inside-outside algorithm (Lari and Young, 1990; Knight and Graehl, 2004) for model estimation. The probability p(, f, a) of a (, f, a)-tuple is what the basic syntax-based translation model is concerned with. It can be further computed by aggregating the rule probabilities p(r) in each derivation in the set of all derivations (Galley et al., 2004; Marcu et al., 2006). That is p(, f, a) = X Y r p(r) (2) Since it has been well-known that applying EM with tree fragments of different sizes causes overfitting (Johnson, 1998), and since it is also known that syntax MT models with larger composed rules in the mix significantly outperform rules t</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, pages 3556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phraases.</title>
<date>2006</date>
<contexts>
<context position="1063" citStr="Marcu et al., 2006" startWordPosition="154" endWordPosition="157">nguageweaver.com Abstract We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R1 in Figure 1 that translates the Chinese phrase RUSSIA MINISTER VIKTOR</context>
<context position="6150" citStr="Marcu et al., 2006" startWordPosition="990" endWordPosition="993">arser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, synchronous grammars built from restructured (binarized) training trees still contain non-binary, multi-level rules and thus still require the binarization transformation so as to be employed by a CKY parser. The translation model we are using in this paper belongs to the xRs formalism (Knight and Graehl, 2004), which has been proved successful for machine translation in (Galley et al., 2004; Galley et al., 2006; Marcu et al., 2006). 3 Concepts We focus on tree-to-string (in noisy-channel model sense) translation models. Translation models of this type are typically trained on tuples of a sourcelanguage sentence f, a target language (e.g., English) parse tree that yields e and translates from f, and the word alignments a between e and f. Such a tuple is called an alignment graph in (Galley et al., 2004). The graph (1) in Figure 2 is such an alignment graph. 747 \x0c(1) unbinarized tree NPB viktor chernomyrdin VIKTORCHERNOMYRDIN NNP1 NNP2 NNP3 NNP4* (2) left-binarization (3) right-/head-binarization NPB NPB NNP1 NNP2 NNP3</context>
<context position="20391" citStr="Marcu et al., 2006" startWordPosition="3375" endWordPosition="3378">g max p(, f, a| ) (1) In practice, we cannot enumerate all the exponential number of binarized trees for a given e-parse. We therefore use the packed forest to store all the binarizations that operate on an e-parse in a compact way, and then use the inside-outside algorithm (Lari and Young, 1990; Knight and Graehl, 2004) for model estimation. The probability p(, f, a) of a (, f, a)-tuple is what the basic syntax-based translation model is concerned with. It can be further computed by aggregating the rule probabilities p(r) in each derivation in the set of all derivations (Galley et al., 2004; Marcu et al., 2006). That is p(, f, a) = X Y r p(r) (2) Since it has been well-known that applying EM with tree fragments of different sizes causes overfitting (Johnson, 1998), and since it is also known that syntax MT models with larger composed rules in the mix significantly outperform rules that minimally explain the training data (minimal rules) in 751 \x0ctranslation accuracy (Galley et al., 2006), we decompose p(b, f, a) using minimal rules during running of the EM algorithm, but, after the EM restructuring is finished, we build the final translation model using composed rules for evaluation. Figure 5 is t</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phraases.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of EMNLP-2006,</booktitle>
<pages>44--52</pages>
<location>Sydney, Australia.</location>
<marker></marker>
<rawString>In Proceedings of EMNLP-2006, pp. 44-52, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1137" citStr="Marcus et al., 1993" startWordPosition="166" endWordPosition="169"> style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R1 in Figure 1 that translates the Chinese phrase RUSSIA MINISTER VIKTOR-CHERNOMYRDIN into an English NPB tree fragment yielding an English phrase</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Giorgio Satta</author>
<author>Benjamin Wellington</author>
</authors>
<title>Generalized multitext grammars.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="5120" citStr="Melamed et al., 2004" startWordPosition="829" endWordPosition="832"> the binarization bias for each tree node from the parallel alternatives. The EM-binarization yields best translation performance. The rest of the paper is organized as follows. Section 2 describes related research. Section 3 defines the concepts necessary for describing the binarizations methods. Section 4 describes the tree binarization methods in details. Section 5 describes the forest-based rule extraction algorithm, and section 6 explains how we restructure the trees using the EM algorithm. The last two sections are for experiments and conclusions. 2 Related Research Several researchers (Melamed et al., 2004; Zhang et al., 2006) have already proposed methods for binarizing synchronous grammars in the context of machine translation. Grammar binarization usually maintains an equivalence to the original grammar such that binarized grammars generate the same language and assign the same probability to each string as the original grammar does. Grammar binarization is often employed to make the grammar fit in a CKY parser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, synchr</context>
</contexts>
<marker>Melamed, Satta, Wellington, 2004</marker>
<rawString>I. Dan Melamed, Giorgio Satta, and Benjamin Wellington. 2004. Generalized multitext grammars. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proc. ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</booktitle>
<contexts>
<context position="24008" citStr="Riezler and Maxwell, 2005" startWordPosition="3947" endWordPosition="3950">lose context, and thus give lower translation performance. Ignoring X nodes when computing the rule sizes preserves the unstructured rules in the resulting translation model and adds substructures as bonuses. 7.2 Experiment results Table 1 shows the BLEU scores of mixed-cased and detokenized translations of different systems. We see that all the binarization methods improve the baseline system that does not apply any binarization algorithm. The EM-binarization performs the best among all the restructuring methods, leading to 1.0 BLEU point improvement. We also computed the bootstrap p-values (Riezler and Maxwell, 2005) for the pairwise BLEU comparison between the baseline system and any of the system trained from binarized trees. The significance test shows that the EM binarization result is statistically significant better than the baseline system (p &amp;gt; 0.005), even though the baseline is already quite strong. To our best knowledge, 37.94 is the highest BLEU score on this test set to date. Also as shown in Table 1, the grammars trained from the binarized training trees are almost two times of the grammar size with no binarization. The extra rules are substructures factored out by these binarization methods.</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proc. ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
</authors>
<title>A reimplementation of Collinss parsing models.</title>
<date>2004</date>
<tech>Technical report,</tech>
<institution>Information Sciences Institute, Department of Computer Science University of Southern California.</institution>
<contexts>
<context position="22373" citStr="Soricut, 2004" startWordPosition="3692" endWordPosition="3693">Our bitext consists of 16M words, all in the mainland-news domain. Our development set is a 925-line subset of the 993-line NIST02 evaluation set. We removed long sentences from the NIST02 evaluation set to speed up discriminative training. The test set is the full 919-line NIST03 evaluation set. We used a bottom-up, CKY-style decoder that works with binary xRs rules obtained via a synchronous binarization procedure (Zhang et al., 2006). The decoder prunes hypotheses using strategies described in (Chiang, 2007). The parse trees on the English side of the bitexts were generated using a parser (Soricut, 2004) implementing the Collins parsing models (Collins, 1997). We used the EM procedure described in (Knight and Graehl, 2004) to perform the inside-outside algorithm on synchronous derivation forests and to generate the Viterbi derivation forest. We used the rule extractor described in (Galley et al., 2006) to extract rules from (e-parse, f, a)-tuples, but we made an important modification: new nodes introduced by binarization will not be counted when computing the rule size limit unless they appear as the rule roots. The motivation is that binarization deepens the parses and increases the number </context>
</contexts>
<marker>Soricut, 2004</marker>
<rawString>Radu Soricut. 2004. A reimplementation of Collinss parsing models. Technical report, Information Sciences Institute, Department of Computer Science University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL.</booktitle>
<contexts>
<context position="5141" citStr="Zhang et al., 2006" startWordPosition="833" endWordPosition="836"> for each tree node from the parallel alternatives. The EM-binarization yields best translation performance. The rest of the paper is organized as follows. Section 2 describes related research. Section 3 defines the concepts necessary for describing the binarizations methods. Section 4 describes the tree binarization methods in details. Section 5 describes the forest-based rule extraction algorithm, and section 6 explains how we restructure the trees using the EM algorithm. The last two sections are for experiments and conclusions. 2 Related Research Several researchers (Melamed et al., 2004; Zhang et al., 2006) have already proposed methods for binarizing synchronous grammars in the context of machine translation. Grammar binarization usually maintains an equivalence to the original grammar such that binarized grammars generate the same language and assign the same probability to each string as the original grammar does. Grammar binarization is often employed to make the grammar fit in a CKY parser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, synchronous grammars built </context>
<context position="22199" citStr="Zhang et al., 2006" startWordPosition="3662" endWordPosition="3665"> carried out a series of experiments to compare the performance of different binarization methods in terms of BLEU on Chinese-to-English translation tasks. 7.1 Experimental setup Our bitext consists of 16M words, all in the mainland-news domain. Our development set is a 925-line subset of the 993-line NIST02 evaluation set. We removed long sentences from the NIST02 evaluation set to speed up discriminative training. The test set is the full 919-line NIST03 evaluation set. We used a bottom-up, CKY-style decoder that works with binary xRs rules obtained via a synchronous binarization procedure (Zhang et al., 2006). The decoder prunes hypotheses using strategies described in (Chiang, 2007). The parse trees on the English side of the bitexts were generated using a parser (Soricut, 2004) implementing the Collins parsing models (Collins, 1997). We used the EM procedure described in (Knight and Graehl, 2004) to perform the inside-outside algorithm on synchronous derivation forests and to generate the Viterbi derivation forest. We used the rule extractor described in (Galley et al., 2006) to extract rules from (e-parse, f, a)-tuples, but we made an important modification: new nodes introduced by binarization</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of the HLT-NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>