<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.720957">
b&amp;apos;Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 819826,
Sydney, July 2006. c
</bodyText>
<sectionHeader confidence="0.506559" genericHeader="abstract">
2006 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.993504">
A Logic-based Semantic Approach to Recognizing Textual Entailment
</title>
<author confidence="0.967329">
Marta Tatu and Dan Moldovan
</author>
<affiliation confidence="0.949983">
Language Computer Corporation
</affiliation>
<address confidence="0.7067105">
Richardson, Texas, 75080
United States of America
</address>
<email confidence="0.984355">
marta,moldovan@languagecomputer.com
</email>
<sectionHeader confidence="0.99038" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.998536727272727">
This paper proposes a knowledge repre-
sentation model and a logic proving set-
ting with axioms on demand success-
fully used for recognizing textual entail-
ments. It also details a lexical inference
system which boosts the performance of
the deep semantic oriented approach on
the RTE data. The linear combination of
two slightly different logical systems with
the third lexical inference system achieves
73.75% accuracy on the RTE 2006 data.
</bodyText>
<sectionHeader confidence="0.997392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991601814814815">
While communicating, humans use different ex-
pressions to convey the same meaning. One of
the central challenges for natural language under-
standing systems is to determine whether different
text fragments have the same meaning or, more
generally, if the meaning of one text can be de-
rived from the meaning of another. A module
that recognizes the semantic entailment between
two text snippets can be employed by many NLP
applications. For example, Question Answering
systems have to identify texts that entail expected
answers. In Multi-document Summarization, the
redundant information should be recognized and
omitted from the summary.
Trying to boost research in textual inferences,
the PASCAL Network proposed the Recognizing
Textual Entailment (RTE) challenges (Dagan et al.,
2005; Bar-Haim et al., 2006). For a pair of two text
fragments, the task is to determine if the meaning
of one text (the entailed hypothesis denoted by )
can be inferred from the meaning of the other text
(the entailing text or \x01 ).
In this paper, we propose a model to represent
the knowledge encoded in text and a logical set-
ting suitable to a recognizing semantic entailment
system. We cast the textual inference problem as
a logic implication between meanings. Text \x01 se-
mantically entails if its meaning logically im-
plies the meaning of . Thus, we, first, transform
both text fragments into logic form, capture their
meaning by detecting the semantic relations that
hold between their constituents and load these rich
logic representations into a natural language logic
prover to decide if the entailment holds or not.
Figure 1 illustrates our approach to RTE. The fol-
lowing sections of the paper shall detail the logic
proving methodology, our logical representation
of text and the various types of axioms that the
prover uses.
To our knowledge, there are few logical ap-
proaches to RTE. (Bos and Markert, 2005) rep-
resents \x01 and into a first-order logic trans-
lation of the DRS language used in Discourse
Representation Theory (Kamp and Reyle, 1993)
and uses a theorem prover and a model builder
with some generic, lexical and geographical back-
ground knowledge to prove the entailment be-
tween the two texts. (de Salvo Braz et al., 2005)
proposes a Description Logic-based knowledge
representation language used to induce the repre-
sentations of \x01 and and uses an extended sub-
sumption algorithm to check if any of \x01 s rep-
resentations obtained through equivalent transfor-
mations entails .
</bodyText>
<sectionHeader confidence="0.817616" genericHeader="method">
2 Cogex - A Logic Prover for NLP
</sectionHeader>
<bodyText confidence="0.9845656">
Our system uses COGEX (Moldovan et al., 2003),
a natural language prover originating from OT-
TER (McCune, 1994). Once its set of support is
loaded with \x01 and the negated hypothesis (\x02 )
and its usable list with the axioms needed to gener-
</bodyText>
<page confidence="0.998691">
819
</page>
<figureCaption confidence="0.815267">
\x0cFigure 1: COGEXs Architecture
</figureCaption>
<bodyText confidence="0.980877111111111">
ate inferences, COGEX begins to search for proofs.
To every inference, an appropriate weight is as-
signed depending on the axiom used for its deriva-
tion. If a refutation is found, the proof is complete;
if a refutation cannot be found, then predicate ar-
guments are relaxed. When argument relaxation
fails to produce a refutation, entire predicates are
dropped from the negated hypothesis until a refu-
tation is found.
</bodyText>
<subsectionHeader confidence="0.995964">
2.1 Proof scoring algorithm
</subsectionHeader>
<bodyText confidence="0.999508433333333">
Once a proof by contradiction is found, its score is
computed by starting with an initial perfect score
and deducting points for each axiom utilized in the
proof, every relaxed argument, and dropped predi-
cate. The computed score is a measure of the kinds
of axioms used in the proof and the significance of
the dropped arguments and predicates. If we as-
sume that both text fragments are existential, then
\x01\x01 if and only if \x01 s entities are a subset of
s entities (Some smart people read Some peo-
ple read) and penalizing a pair whose contains
predicates that cannot be inferred is a correct way
to ensure entailment (Some people read \x02 Some
smart people read). But, if both \x01 and are uni-
versally quantified, then the groups mentioned in
must be a subset of the ones from \x01 (All people
read All smart people read and All smart people
read \x02 All people read). Thus, the scoring mod-
ule adds back the points for the modifiers dropped
from and subtracts points for \x01 s modifiers not
present in . The remaining two cases are sum-
marized in Table 1.
Because \x03\x01\x05\x04 \x07\x06 pairs with longer sentences can
potentially drop more predicates and receive a
lower score, COGEX normalizes the proof scores
by dividing the assessed penalty by the maximum
assessable penalty (all the predicates from are
dropped). If this final proof score is above a
threshold learned on the development data, then
the pair is labeled as positive entailment.
</bodyText>
<sectionHeader confidence="0.996351" genericHeader="method">
3 Knowledge Representation
</sectionHeader>
<bodyText confidence="0.99880375">
For the textual entailment task, our logic prover
uses a two-layered logical representation which
captures the syntactic and semantic propositions
encoded in a text fragment.
</bodyText>
<subsectionHeader confidence="0.99458">
3.1 Logic Form Transformation
</subsectionHeader>
<bodyText confidence="0.999766444444444">
In the first stage of our representation pro-
cess, COGEX converts \x01 and into logic
forms (Moldovan and Rus, 2001). More specifi-
cally, a predicate is created for each noun, verb,
adjective and adverb. The nouns that form a noun
compound are gathered under a nn NNC predi-
cate. Each named entity class of a noun has a
corresponding predicate which shares its argument
with the noun predicate it modifies. Predicates for
</bodyText>
<page confidence="0.99553">
820
</page>
<table confidence="0.98134075">
\x0c(\x02\x01 ,\x03\x05\x04 ) (\x03\x06\x01 ,\x07\x04 )
All people read Some smart people read Some people read \x02 All smart people read
All smart people read Some people read Some smart people read \x02 All people read
Add the dropped points for s modifiers Subtract points for modifiers not present in
</table>
<tableCaption confidence="0.999382">
Table 1: The quantification of \x01 and influences the proof scoring algorithm
</tableCaption>
<bodyText confidence="0.98471315">
prepositions and conjunctions are also added to
link the texts constituents. This syntactic layer of
the logic representation is, automatically, derived
from a full parse tree and acknowledges syntax-
based relationships such as: syntactic subjects,
syntactic objects, prepositional attachments, com-
plex nominals, and adjectival/adverbial adjuncts.
In order to objectively evaluate our represen-
tation, we derived it from two different sources:
constituency parse trees (generated with our
implementation of (Collins, 1997)) and depen-
dency parse trees (created using Minipar (Lin,
1998))1. The two logic forms are slightly dif-
ferent. The dependency representation captures
more accurately the syntactic dependencies
between the concepts, but lacks the semantic
information that our semantic parser extracts from
the constituency parse trees. For instance, the
sentence Gilda Flores was kidnapped on the 13th
of January 19902 is constituency represented
</bodyText>
<table confidence="0.870767916666667">
as Gilda NN(x1) &amp; Flores NN(x2) &amp;
nn NNC(x3,x1,x2) &amp; human NE(x3) &amp;
kidnap VB(e1,x9,x3) &amp; on IN(e1,x8)
&amp; 13th NN(x4) &amp; of NN(x5) &amp;
January (x6) &amp; 1990 NN(x7)
&amp; nn NNC(x8,x4,x5,x6,x7) &amp;
date NE(x8) and its dependency
logic form is Gilda Flores NN(x2)
&amp; human NE(x2) &amp;
kidnap VB(e1,x4,x2) &amp; on IN(e1,x3)
&amp; 13th NN(x3) &amp; of IN(x3,x1) &amp;
January 1990 NN(x1).
</table>
<subsubsectionHeader confidence="0.342867">
3.1.1 Negation
</subsubsectionHeader>
<bodyText confidence="0.997233">
The exceptions to the one-predicate-per-
open-class-word rule include the adverbs not
and never. In cases similar to further de-
tails were not released, the system removes
</bodyText>
<page confidence="0.937895">
1
</page>
<bodyText confidence="0.98946">
The experimental results described in this paper were
performed using two systems: the logic prover when
it receives as input the constituency logic representation
(COGEX\x08 ) and the dependency representation (COGEX\t ).
</bodyText>
<page confidence="0.978333">
2
</page>
<bodyText confidence="0.992331272727273">
All examples shown in this paper are from the entail-
ment corpus released as part of the Second RTE challenge
(www.pascal-network.org/Challenges/RTE2).
The RTE datasets will be described in Section 7.
not RB(x3,e1) and negates the verbs
predicate (-release VB(e1,x1,x2)).
Similarly, for nouns whose determiner is no,
for example, No case of indigenously ac-
quired rabies infection has been confirmed, the
verbs predicate is negated (case NN(x1) &amp;
-confirm VB(e2,x15,x1)).
</bodyText>
<subsectionHeader confidence="0.998977">
3.2 Semantic Relations
</subsectionHeader>
<bodyText confidence="0.999539736842105">
The second layer of our logic representation adds
the semantic relations, the underlying relation-
ships between concepts. They provide the se-
mantic background for the text, which allows for
a denser connectivity between the concepts ex-
pressed in text. Our semantic parser takes free En-
glish text or parsed sentences and extracts a rich
set of semantic relations3 between words or con-
cepts in each sentence. It focuses not only on
the verb and its arguments, but also on seman-
tic relations encoded in syntactic patterns such as
complex nominals, genitives, adjectival phrases,
and adjectival clauses. Our representation mod-
ule maps each semantic relation identified by the
parser to a predicate whose arguments are the
events and entities that participate in the rela-
tion and it adds these semantic predicates to the
logic form. For example, the previous logic form
is augmented with the THEME SR(x3,e1) &amp;
</bodyText>
<equation confidence="0.505498">
TIME SR(x8,e1) relations4 (Gilda Flores is
</equation>
<bodyText confidence="0.991691">
the theme of the kidnap event and 13th of January
1990 shows the time of the kidnapping).
</bodyText>
<subsectionHeader confidence="0.998975">
3.3 Temporal Representation
</subsectionHeader>
<bodyText confidence="0.997043142857143">
In addition to the semantic predicates, we
represent every date/time into a normal-
ized form time TMP(BeginFn(event),
year, month, date, hour, minute,
second) &amp; time TMP(EndFn(event),
year, month, date, hour, minute,
second). Furthermore, temporal reasoning
</bodyText>
<page confidence="0.988286">
3
</page>
<bodyText confidence="0.941601">
We consider relations such as AGENT,
</bodyText>
<construct confidence="0.439423666666667">
THEME, TIME, LOCATION, MANNER, CAUSE,
INSTRUMENT, POSSESSION, PURPOSE,
MEASURE, KINSHIP, ATTRIBUTE, etc.
</construct>
<page confidence="0.94635">
4
</page>
<bodyText confidence="0.869583">
R(x,y) should be read as x is R of y.
</bodyText>
<page confidence="0.988466">
821
</page>
<bodyText confidence="0.9940627">
\x0cpredicates are derived from both the detected
semantic relations as well as from a module
which utilizes a learning algorithm to detect
temporally ordered events (\x03\x01 \x04\x03\x02\x05\x04 \x04\x03\x02\x07\x06
\x06, where
is the temporal signal linking two events
\x02 \x04 and \x02 \x06 ) (Moldovan et al., 2005). From
each triple, temporally related SUMO predicates
are generated based on hand-coded rules for
the signal classes (\x03\x01 sequence, \x02\x05\x04 \x04\x03\x02\x07\x06
</bodyText>
<equation confidence="0.874221">
\x06 \x08
earlier TMP(e1,e2), \x03\x01 contain, \x02 \x04 \x04\x03\x02 \x06
\x06\t\x08
</equation>
<bodyText confidence="0.8401655">
during TMP(e1,e2), etc.). In the above
example, 13th of January 1990 is normalized
to the interval time TMP(BeginFn(e2),
1990, 1, 13, 0, 0, 0) &amp;
time TMP(EndFn(e2), 1990, 1, 13,
23, 59, 59) and during TMP(e1,e2) is
added to the logical representation to show when
the kidnapping occurred.
</bodyText>
<sectionHeader confidence="0.691452" genericHeader="method">
4 Axioms on Demand
</sectionHeader>
<bodyText confidence="0.910236454545455">
COGEXs usable list consists of all the axioms
generated either automatically or by hand. The
system generates axioms on demand for a given
\x03\x01\x05\x04 \x07\x06 pair whenever the semantic connectivity
between two concepts needs to be established in
a proof. The axioms on demand are lexical chains
and world knowledge axioms. We are keen on the
idea of axioms on demand since it is not possible
to derive apriori all axioms needed in an arbitrary
proof. This brings a considerable level of robust-
ness to our entailment system.
</bodyText>
<subsectionHeader confidence="0.993377">
4.1 eXtended WordNet lexical chains
</subsectionHeader>
<bodyText confidence="0.998026076923077">
For the semantic entailment task, the ability to
recognize two semantically-related words is an
important requirement. Therefore, we automat-
ically construct lexical chains of WordNet rela-
tions from \x01 s constituents to s (Moldovan and
Novischi, 2002). In order to avoid errors intro-
duced by a Word Sense Disambiguation system,
we used the first
senses for each word5 un-
less the source and the target of the chain are
synonyms. If a chain exists6, the system gener-
ates, on demand, an axiom with the predicates
of the source (from \x01 ) and the target (from ).
</bodyText>
<page confidence="0.954014">
5
</page>
<bodyText confidence="0.906557">
Because WordNet senses are ranked based on their fre-
quency, the correct sense is most likely among the first \x0b . In
</bodyText>
<footnote confidence="0.5818945">
our experiments, \x0b
\x0c\x0f\x0e .
</footnote>
<page confidence="0.982871">
6
</page>
<bodyText confidence="0.949607777777778">
Each lexical chain is assigned a weight based on its prop-
erties: shorter chains are better than longer ones, the relations
are not equally important and their order in the chain influ-
ences its strength. If the weight of a chain is above a given
threshold, the lexical chain is discarded.
For example, given the ISA relation between mur-
der#1 and kill#1, the system generates, when
needed, the axiom murder VB(e1,x1,x2)
\x10
kill VB(e1,x1,x2). The remaining of
this section details some of the requirements for
creating accurate lexical chains.
Because our extended version of Word-
Net has attached named entities to each noun
synset, the lexical chain axioms append the
entity name of the target concept, whenever
it exists. For example, the logic prover uses
the axiom Nicaraguan JJ(x1,x2)
</bodyText>
<equation confidence="0.8337605">
\x10
Nicaragua NN(x1) &amp; country NE(x1)
</equation>
<bodyText confidence="0.989623333333333">
when it tries to infer electoral campaign is held in
Nicaragua from Nicaraguan electoral campaign.
We ensured the relevance of the lexical chains
by limiting the path length to three relations and
the set of WordNet relations used to create the
chains by discarding the paths that contain certain
relations in a particular order. For example, the
automatic axiom generation module does not con-
sider chains with an IS-A relation followed by a
</bodyText>
<figure confidence="0.5955795">
HYPONYMY link (\x11\x13\x12\x15\x14\x17\x16\x19\x18\x1b\x1a\x1d\x1c
\x1e \x1f\x03!#&quot;
$ \x10
\x16%\x14\x01&amp;(\&amp;apos;
)+*(,+-/.0*213*
$ \x10
</figure>
<page confidence="0.993121">
465
</page>
<bodyText confidence="0.959229153846154">
&amp;/78\x1c9\x14\x17&amp; ). Similarly, the system rejected chains
with more than one HYPONYMY relations. Al-
though these relations link semantically related
concepts, the type of semantic similarity they in-
troduce is not suited for inferences. Another re-
striction imposed on the lexical chains generated
for entailment is not to start from or include too
general concepts7. Therefore, we assigned to each
noun and verb synset from WordNet a generality
weight based on its relative position within its hi-
erarchy and on its frequency in a large corpus. If
:
\x1e is the depth of concept \x16 \x1e ,
</bodyText>
<page confidence="0.819863">
4
</page>
<bodyText confidence="0.350546666666667">
\x04&lt;; is the max-
imum depth in \x16 \x1e
s hierarchy \x1e
</bodyText>
<equation confidence="0.52302325">
and =&amp;gt;\x11 \x03?\x16 \x1e
\x06A@
$CB \x1c9\x1a \x03ED \x03?\x16 \x1e
\x06 \x06 is the information content of \x16 \x1e mea-
</equation>
<bodyText confidence="0.502018">
sured on the British National Corpus, then
</bodyText>
<equation confidence="0.653374181818182">
\x1a
5GF&lt;5
78\x18 B \x14\x01&amp;(\&amp;apos;IH \x03?\x16
\x1e
\x06J@ K
L
;NM \x04
OQP
;SR
=T\x11 \x03?\x16 \x1e
\x06\x1dU
</equation>
<bodyText confidence="0.997490666666667">
In our experiments, we discarded the chains with
concepts whose generality weight exceeded 0.8
such as object NN#1, act VB#1, be VB#1, etc.
Another important change that we intro-
duced in our extension of WordNet is the re-
finement of the DERIVATION relation which
links verbs with their corresponding nominal-
ized nouns. Because the relation is ambigu-
ous regarding the role of the noun, we split
</bodyText>
<page confidence="0.989762">
7
</page>
<bodyText confidence="0.992149">
There are no restrictions on the target concept.
</bodyText>
<page confidence="0.983983">
822
</page>
<bodyText confidence="0.7834944">
\x0cthis relation in three: ACT-DERIVATION, AGENT-
DERIVATION and THEME-DERIVATION. The
role of the nominalization determines the ar-
gument given to the noun predicate. For in-
stance, the axioms act VB(e1,x1,x2)
</bodyText>
<equation confidence="0.807411">
\x10
acting NN(e1) (ACT), act VB(e1,x1,x2)
\x10
actor NN(x1) (AGENT) reflect different
</equation>
<bodyText confidence="0.806344">
types of derivation.
</bodyText>
<subsectionHeader confidence="0.950751">
4.2 NLP Axioms
</subsectionHeader>
<bodyText confidence="0.998351090909091">
Our NLP axioms are linguistic rewriting rules that
help break down complex logic structures and
express syntactic equivalence. After analyzing
the logic form and the parse trees of each text
fragment, the system, automatically, generates
axioms to break down complex nominals and
coordinating conjunctions into their constituents
so that other axioms can be applied, individually,
to the components. These axioms are made avail-
able only to the \x03\x01\x05\x04 \x07\x06 pair that generated them.
For example, the axiom nn NNC(x3,x1,x2)
</bodyText>
<table confidence="0.822376">
&amp; francisco NN(x1) &amp; merino NN(x2)
\x10
merino NN(x3) breaks down the noun
compound Francisco Merino into Francisco and
Merino and helps COGEX infer Merinos home
from Francisco Merinos home.
</table>
<subsectionHeader confidence="0.959325">
4.3 World Knowledge Axioms
</subsectionHeader>
<bodyText confidence="0.82356075">
Because, sometimes, the lexical or the syntactic
knowledge cannot solve an entailment pair, we
exploit the WordNet glosses, an abundant source
of world knowledge. We used the logic forms
of the glosses provided by eXtended WordNet8
to, automatically, create our world knowledge
axioms. For example, the first sense of noun Pope
and its definition the head of the Roman Catholic
Church introduces the axiom Pope NN(x1)
head NN(x1) &amp; of IN(x1,x2) &amp;
Roman Catholic Church NN(x2) which is
used by prover to show the entailment between
</bodyText>
<equation confidence="0.249304">
\x01 : A place of sorrow, after Pope John Paul II
</equation>
<bodyText confidence="0.966221777777778">
died, became a place of celebration, as Roman
Catholic faithful gathered in downtown Chicago
to mark the installation of new Pope Benedict
XVI. and : Pope Benedict XVI is the new leader
of the Roman Catholic Church.
We also incorporate in our system a small
common-sense knowledge base of 383 hand-
coded world knowledge axioms, where 153 have
been manually designed based on the entire de-
</bodyText>
<page confidence="0.971921">
8
</page>
<bodyText confidence="0.9016938">
http://xwn.hlt.utdallas.edu
velopment set data, and 230 originate from pre-
vious projects. These axioms express knowledge
that could not be derived from WordNet regarding
employment9, family relations, awards, etc.
</bodyText>
<sectionHeader confidence="0.960623" genericHeader="method">
5 Semantic Calculus
</sectionHeader>
<bodyText confidence="0.9854098">
The Semantic Calculus axioms combine two se-
mantic relations identified within a text fragment
and increase the semantic connectivity of the
text (Tatu and Moldovan, 2005). A semantic ax-
iom which combines two relations,
</bodyText>
<equation confidence="0.917962666666667">
\x01 \x1e
and
\x01\x03\x02 , is
</equation>
<bodyText confidence="0.829371666666667">
devised by observing the semantic connection be-
tween the \x04 \x04 and \x04\x06\x05 words for which there exists
at least one other word, \x04 \x06 , such that
</bodyText>
<equation confidence="0.957742857142857">
\x01 \x1e
\x03\x07\x04 \x04 \x04\x08\x04 \x06
\x06
(\x04 \x04
\t ;
\x10
\x04 \x06 ) and
\x01
\x02 \x03\x07\x04 \x06 \x04\x08\x04
\x05 \x06 (\x04 \x06
\t\x0c\x0b
\x10
\x04
\x05 ) hold true.
</equation>
<bodyText confidence="0.953733647058824">
We note that not any two semantic relations can
be combined:
\x01 \x1e
and
\x01\x03\x02 have to be compatible
with respect to the part-of-speech of the common
argument. Depending on their properties, there
are up to 8 combinations between any two se-
mantic relations and their inverses, not counting
the combinations between a semantic relation and
itself10. Many combinations are not semantically
significant, for example, KINSHIP SR(x1,x2)
&amp; TEMPORAL SR(x2,e1) is unlikely to be
found in text. Trying to solve the semantic
combinations one comes upon in text corpora,
we analyzed the RTE development corpora and
devised rules for some of the
</bodyText>
<equation confidence="0.9125185">
\x01 \x1e\x0e
\x01\x03\x02 combina-
</equation>
<bodyText confidence="0.98963">
tions encountered. We validated these axioms
by checking all the \x03\x07\x04S\x04 \x04\x08\x04 \x05 \x06 pairs from the LA
Times text collection such that \x03\x01 \x1e\x0f
</bodyText>
<equation confidence="0.9645535">
\x01\x10\x02 \x06 \x03\x07\x04 \x04 \x04\x08\x04
\x05 \x06
</equation>
<bodyText confidence="0.859446">
holds. We have identified 82 semantic axioms
that show how semantic relations can be com-
bined. These axioms enable inference of unstated
meaning from the semantics detected in text.
For example, if \x01 states explicitly the KINSHIP
(KIN) relations between Nicholas Cage and
Alice Kim Cage and between Alice Kim Cage
and Kal-el Coppola Cage, the logic prover uses
</bodyText>
<equation confidence="0.587326666666667">
the KIN SR(x1,x2) &amp; KIN SR(x2,x3)
\x10
KIN SR(x1,x3) semantic axiom (the
</equation>
<bodyText confidence="0.8994805">
transitivity of the blood relation) and the sym-
metry of this relationship (KIN SR(x1,x2)
</bodyText>
<page confidence="0.968496">
9
</page>
<bodyText confidence="0.6156832">
For example, the axiom country NE(x1) &amp;
negotiator NN(x2) &amp; nn NNC(x3,x1,x2) \x11
work VB(e1,x2,x4) &amp; for IN(e1,x1) helps the
prover infer that Christopher Hill works for the US from top
US negotiator, Christopher Hill.
</bodyText>
<page confidence="0.987928">
10
</page>
<bodyText confidence="0.994672">
Harabagiu and Moldovan (1998) lists the exact number
of possible combinations for several WordNet relations and
part-of-speech classes.
</bodyText>
<page confidence="0.976172">
823
</page>
<figure confidence="0.721954666666667">
\x0c\x10
KIN SR(x2,x1)) to infer s statement
(KIN(Kal-el Coppola Cage, Nicholas Cage)). An-
other frequent axiom is LOCATION SR(x1,x2)
&amp; PARTWHOLE SR(x2,x3)
\x10
</figure>
<figureCaption confidence="0.283268">
LOCATION SR(x1,x3). Given the text
</figureCaption>
<bodyText confidence="0.94322025">
John lives in Dallas, Texas and using the axiom,
the system infers that John lives in Texas. The
system applies the 82 axioms independent of
the concepts involved in the semantic compo-
sition. There are rules that can be applied only
if the concepts that participate satisfy a certain
condition or if the relations are of a certain
type. For example, LOCATION SR(x1,x2)
</bodyText>
<figure confidence="0.545092">
&amp; LOCATION SR(x2,x3)
\x10
LOCATION SR(x1,x3) only if the LOCATION
relation shows inclusion (John is in the car in the
garage
\x10
</figure>
<sectionHeader confidence="0.433361" genericHeader="method">
LOCATION SR(John,garage).
</sectionHeader>
<bodyText confidence="0.656451">
John is near the car behind the garage \x02\x10
</bodyText>
<sectionHeader confidence="0.9381995" genericHeader="method">
LOCATION SR(John,garage)).
6 Temporal Axioms
</sectionHeader>
<bodyText confidence="0.996239272727273">
One of the types of temporal axioms that we load
in our logic prover links specific dates to more
general time intervals. For example, October 2000
entails the year 2000. These axioms are automati-
cally generated before the search for a proof starts.
Additionally, the prover uses a SUMO knowledge
base of temporal reasoning axioms that consists
of axioms for a representation of time points and
time intervals, Allen (Allen, 1991) primitives, and
temporal functions. For example, during is a tran-
sitive Allen primitive: during TMP(e1,e2)
</bodyText>
<table confidence="0.556368666666667">
&amp; during TMP(e2,e3)
\x10
during TMP(e1,e3).
</table>
<sectionHeader confidence="0.968726" genericHeader="evaluation">
7 Experiments and Results
</sectionHeader>
<bodyText confidence="0.534677733333333">
The benchmark corpus for the RTE 2005 task con-
sists of seven subsets with a 50%-50% split be-
tween the positive entailment examples and the
negative ones. Each subgroup corresponds to a
different NLP application: Information Retrival
(IR), Comparable Documents (CD), Reading Com-
prehension (RC), Question Answering (QA), Infor-
mation Extraction (IE), Machine Translation (MT),
and Paraphrase Acquisition (PP). The RTE data
set includes 1367 English \x03\x01\x05\x04 \x07\x06 pairs from the
news domain (political, economical, etc.). The
RTE 2006 data covered only four NLP tasks (IE, IR,
QA and Multi-document Summarization (SUM))
with an identical split between positive and nega-
tive examples. Table 2 presents the data statistics.
</bodyText>
<table confidence="0.971810333333333">
Development set Test set
RTE 2005 567 800
RTE 2006 800 800
</table>
<tableCaption confidence="0.957818">
Table 2: Datasets Statistics
</tableCaption>
<subsectionHeader confidence="0.99256">
7.1 COGEXs Results
</subsectionHeader>
<bodyText confidence="0.970757860465116">
Tables 3 and 4 summarize COGEXs performance
on the RTE datasets, when it received as input the
different-source logic forms11.
On the RTE 2005 data, the overall performance
on the test set is similar for both logic proving
runs, COGEX and COGEXO . On the development
set, the semantically enhanced logic forms helped
the prover distinguish better the positive entail-
ments (COGEX has an overall higher precision
than COGEXO
). If we analyze the performance on
the test data, then COGEX performs slightly bet-
ter on MT, CD and PP and worse on the RC, IR and
QA tasks. The major differences between the two
logic forms are the semantic content (incomplete
for the dependency-derived logic forms) and, be-
cause the texts tokenization is different, the num-
ber of predicates in s logic forms is different
which leads to completely different proof scores.
On the RTE 2006 test data, the system which
uses the dependency logic forms outperforms
COGEX . COGEXO
performs better on almost all
tasks (except SUM) and brings a significant im-
provement over COGEX on the IR task. Some
of the positive examples that the systems did not
label correctly require world knowledge that we
do not have encoded in our axiom set. One ex-
ample for which both systems returned the wrong
answer is pair 353 (test 2006) where, from Chinas
decade-long practice of keeping its currency val-
ued at around 8.28 yuan to the dollar, the system
should recognize the relation between the yuan
and Chinas currency and infer that the currency
used in China is the yuan because a countrys cur-
rency currency used in the country. Some of
the pairs that the prover, currently, cannot handle
involve numeric calculus and human-oriented es-
timations. Consider, for example, pair 359 (dev
set, RTE 2006) labeled as positive, for which the
logic prover could not determine that 15 safety vi-
olations numerous safety violations.
The deeper analysis of the systems output
</bodyText>
<page confidence="0.998497">
11
</page>
<bodyText confidence="0.995893666666667">
For the RTE 2005 data, we list the confidence-weighted
score (cws) (Dagan et al., 2005) and, for the RTE 2006 data,
the average precision (ap) measure (Bar-Haim et al., 2006).
</bodyText>
<page confidence="0.998934">
824
</page>
<table confidence="0.999118727272727">
\x0cTask COGEX\x08 COGEX\t LEXALIGN COMBINATION
acc cws f acc cws f acc cws f acc cws f
IE 58.33 60.90 60.31 57.50 57.03 51.42 56.66 53.41 59.99 62.50 67.63 57.14
IR 52.22 62.41 15.68 53.33 59.67 27.58 50.00 55.92 0.00 68.88 75.77 64.10
CD 82.00 88.90 79.69 79.33 87.15 74.38 82.00 88.04 80.57 84.66 91.73 82.70
QA 50.00 56.27 0.00 51.53 42.37 64.80 53.07 43.76 63.90 60.76 55.05 63.82
RC 53.57 56.38 38.09 57.14 59.32 58.33 57.85 60.26 49.57 60.00 62.89 50.00
MT 55.83 55.83 53.91 52.50 58.17 27.84 51.66 45.94 67.04 64.16 63.80 66.66
PP 56.00 63.11 26.66 54.00 58.15 30.30 50.00 47.03 0.00 68.00 75.27 63.63
TEST 59.37 63.09 48.00 59.12 57.17 54.52 59.12 55.74 59.17 67.25 67.64 64.69
DEV 63.66 63.44 64.48 61.19 63.63 57.52 62.08 59.94 60.83 70.37 71.89 66.66
</table>
<tableCaption confidence="0.587727">
Table 3: RTE 2005 data results (accuracy, confidence-weighted score, and f-measure for the true class)
</tableCaption>
<table confidence="0.99390625">
Task COGEX\x08 COGEX\t LEXALIGN COMBINATION
acc ap f acc ap f acc ap f acc ap f
IE 58.00 49.71 57.57 59.00 59.74 63.71 54.00 49.70 67.14 71.50 62.99 71.36
IR 62.50 65.91 56.14 73.50 72.50 73.89 64.50 69.45 65.02 74.00 74.30 72.92
QA 62.00 67.30 48.64 64.00 68.16 57.64 58.50 55.78 57.86 70.50 75.10 66.67
SUM 74.50 77.60 74.62 74.00 79.68 73.73 70.50 76.82 73.05 79.00 80.33 78.13
TEST 64.25 66.31 60.16 67.62 70.69 67.50 61.87 57.64 66.07 73.75 71.33 72.37
DEV 64.50 64.05 66.19 69.00 70.92 69.31 62.25 62.66 62.72 75.12 76.28 76.83
</table>
<tableCaption confidence="0.992574">
Table 4: RTE 2006 data results (accuracy, average precision, and f-measure for the true class)
</tableCaption>
<bodyText confidence="0.997109">
showed that while WordNet lexical chains and
NLP axioms are the most frequently used axioms
throughout the proofs, the semantic and tempo-
ral axioms bring the highest improvement in ac-
curacy, for the RTE data.
</bodyText>
<subsectionHeader confidence="0.994696">
7.2 Lexical Alignment
</subsectionHeader>
<bodyText confidence="0.994459">
Inspired by the positive examples whose is in
a high degree lexically subsumed by \x01 , we de-
veloped a shallow system which measures their
overlap by computing an edit distance between the
text and the hypothesis. The cost of deleting a
</bodyText>
<table confidence="0.683262071428571">
word from \x01 \x03\x07\x04 \x01 \x10
R
\x06 is equal to 0, the cost
of replacing a word from \x01 with another from
\x03\x07\x04 \x01 \x10
\x04 \x04 , where \x04 \x01 \x02@
\x04 \x04 and \x04 \x01 and \x04 \x04 are
not synonyms in WordNet
\x06 equal to (we do not
allow replace operations) and the cost of inserting
a word from \x03 R
\x10
\x04 \x04
\x06 varies with the part-
</table>
<bodyText confidence="0.999261785714286">
of-speech of the inserted word (higher values for
WordNet nouns, adjectives or adverbs, lower for
verbs and a minimum value for everything else).
Table 5 shows a minimum cost alignment.
The performance of this lexical method (LEX-
ALIGN) is shown in Tables 3 and 4. The align-
ment technique performs significantly better on
the \x03\x01 \x04 \x06 pairs in the CD (RTE 2005) and SUM
(RTE 2006) tasks. For these tasks, all three sys-
tems performed the best because the text of false
pairs is not entailing the hypothesis even at the lex-
ical level. For pair 682 (test set, RTE 2006), \x01
and have very few words overlapping and there
are no axioms that can be used to derive knowl-
edge that supports the hypothesis. Contrarily, for
the IE task, the systems were fooled by the high
word overlap between \x01 and . For example, pair
678s text (test set, RTE 2006) contains the entire
hypothesis in its if clause. For this task, we had the
highest number of false positives, around double
when compared to the other applications. LEX-
ALIGN works surprisingly well on the RTE data. It
outperforms the semantic systems on the 2005 QA
test data, but it has its limitations. The logic rep-
resentations are generated from parse trees which
are not always accurate (\x01 86% accuracy). Once
syntactic and semantic parsers are perfected, the
logical semantic approach shall prove its potential.
</bodyText>
<subsectionHeader confidence="0.924299">
7.3 Merging three systems
</subsectionHeader>
<bodyText confidence="0.999479125">
Because the two logical representations and the
lexical method are very different and perform
better on different sets of tasks, we combined
the scores returned by each system12 to see if a
mixed approach performs better than each individ-
ual method. For each NLP task, we built a classi-
fier based on the linear combination of the three
scores. Each tasks classifier labels pair \x14 as pos-
</bodyText>
<figure confidence="0.987705166666667">
itive if
\x02\x04\x03 -\x06\x05\x08\x07
\t \x08\x0c\x0b \x16%\x1c07
5
\x03\x14
\x06\x0e
\x02\x0f\x03 -\x06\x05\x08\x07
\t \t\x10\x0b \x16\x19\x1c07
5
O
\x03\x14
\x06\x0e
</figure>
<page confidence="0.753793">
12
</page>
<bodyText confidence="0.849774333333333">
Each system returns a score between 0 and 1, a number
close to 0 indicating a probable negative example and a num-
ber close to 1 indicating a probable positive example. Each
</bodyText>
<figure confidence="0.966754272727273">
\x11\x13\x12\x15\x14\x17\x16\x19\x18
pairs lexical alignment score, \x1a\x1c\x1b
\x1d\x1f\x1e\x1f\x1f!#&quot;%$\&amp;apos;&amp;)( ;+*
, , is the
normalized average edit distance cost.
825
\x0c\x12
: The Council of Europe has * 45 member states. Three countries from ...
DEL INS DEL
\x16
: The Council of Europe * is made up by 45 member states. *
</figure>
<tableCaption confidence="0.59162">
Table 5: The lexical alignment for RTE 2006 pair 615 (test set)
</tableCaption>
<figure confidence="0.8016961875">
\x02\x01 \x07
\t\x03\x02\x05\x04 \x1e \x05\x03.
\x0b \x16%\x1c07
5
\x07
\t\x03\x02\x05\x04 \x1e \x052.
\x03\x14
\x06\x07\x06\t\x08
U\x0b
, where the op-
timum values of the classifiers real-valued pa-
rameters (
\x02 \x03 -\x06\x05\x08\x07
\t \x08 \x04 \x02\x0f\x03 -\x06\x05\x08\x07
\t \t \x04 \x02\x01 \x07
\t\x03\x02\x05\x04 \x1e \x05\x03. ) were deter-
</figure>
<bodyText confidence="0.98202775">
mined using a grid search on each development
set. Given the different nature of each application,
the
\x02
parameters vary with each task. For exam-
ple, the final score given to each IE 2006 pair is
highly dependent on the score given by COGEX
when it received as input the logic forms created
from the constituency parse trees with a small cor-
rection from the dependency parse trees logic form
system13. For the IE task, the lexical alignment
performs the worst among the three systems. On
the other hand, for the IR task, the score given by
LEXALIGN is taken into account14. Tables 3 and
4 summarize the performance of the three system
combination. This hybrid approach performs bet-
ter than all other systems for all measures on all
tasks. It displays the same behavior as its depen-
dents: high accuracy on the CD and SUM tasks and
many false positives for the IE task.
</bodyText>
<sectionHeader confidence="0.997833" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999939833333333">
In this paper, we present a logic form represen-
tation of knowledge which captures syntactic de-
pendencies as well as semantic relations between
concepts and includes special temporal predicates.
We implemented several changes to our Word-
Net lexical chains module which lead to fewer un-
sound axioms and incorporated in our logic prover
semantic and temporal axioms which decrease its
dependence on world knowledge. We plan to im-
prove our logic prover to detect false entailments
even when the two texts have a high word overlap
and expand our axiom set.
</bodyText>
<sectionHeader confidence="0.992061" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998438">
J. Allen. 1991. Time and Time Again: The Many Ways
to Represent Time. Internatinal Journal of Intelli-
gent Systems, 4(6):341355.
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Gi-
ampiccolo, B. Magnini, and I. Szpektor. 2006. The
</reference>
<figure confidence="0.949645153846154">
Second PASCAL Recognising Textual Entailment
13\x0c\x0e
\x10\x0f * &quot; $\x12\x11 \x0c\x14\x13\x16\x15\x0b\x13
\x14 \x0c\x0e
\x17\x0f * &quot; $\x12\x18 \x0c\x1a\x19\x03\x15 \x0e
\x14 \x0c !#&quot;%$\&amp;apos;&amp;)( ;+*
, \x0c\x14\x1b\x1c\x19\x1d\x15 \x1e
14\x0c
\x10\x0f * &quot; $\x12\x11 \x0c\x1f\x19\x03\x15 \x0e
\x14 \x0c
\x17\x0f * &quot; $\x12\x18 \x0c\x1a\x19\x03\x15 \x13
\x14 \x0c !#&quot;%$\&amp;apos;&amp;)( ;+*
, \x0c\x1f\x19\x1d\x15 \x1e
</figure>
<reference confidence="0.999666152173913">
Challenge. In Proceedings of the Second PASCAL
Challenges Workshop.
J. Bos and K. Markert. 2005. Recognizing Textual
Entailment with Logical Inference. In Proceedings
of HLT/EMNLP 2005, Vancouver, Canada, October.
M. Collins. 1997. Three Generative, Lexicalized Mod-
els for Statistical Parsing. In Proceedings of the
ACL-97.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Chal-
lenge. In Proceedings of the PASCAL Challenges
Workshop, Southampton, U.K., April.
R. de Salvo Braz, R. Girju, V. Punyakanok, D. Roth,
and M. Sammons. 2005. An Inference Model for
Semantic Entailment in Natural Language. In Pro-
ceedings of AAAI-2005.
S. Harabagiu and D. Moldovan. 1998. Knowledge
Processing on Extended WordNet. In Christiane
Fellbaum, editor, WordNet: an Electronic Lexical
Database and Some of its Applications, pages 379
405. MIT Press.
H. Kamp and U. Reyle. 1993. From Discourse to
Logic: Introduction to Model-theoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Kluwer Academic Publish-
ers.
D. Lin. 1998. Dependency-based Evaluation of MINI-
PAR. In Workshop on the Evaluation of Parsing Sys-
tems, Granada, Spain, May.
William W. McCune, 1994. OTTER 3.0 Reference
Manual and Guide.
D. Moldovan and A. Novischi. 2002. Lexical chains
for Question Answering. In Proceedings of COL-
ING, Taipei, Taiwan, August.
D. Moldovan and V. Rus. 2001. Logic Form Transfor-
mation of WordNet and its Applicability to Question
Answering. In Proceedings of ACL, France.
D. Moldovan, C. Clark, S. Harabagiu, and S. Maio-
rano. 2003. COGEX A Logic Prover for Question
Answering. In Proceedings of the HLT/NAACL.
D. Moldovan, C. Clark, and S. Harabagiu. 2005. Tem-
poral Context Representation and Reasoning. In
Proceedings of IJCAI, Edinburgh, Scotland.
M. Tatu and D. Moldovan. 2005. A Semantic Ap-
proach to Recognizing Textual Entailment. In Pro-
ceedings of HLT/EMNLP.
</reference>
<page confidence="0.988816">
826
</page>
<figure confidence="0.247272">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.584664">
<note confidence="0.846336">b&amp;apos;Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 819826, Sydney, July 2006. c 2006 Association for Computational Linguistics</note>
<title confidence="0.960365">A Logic-based Semantic Approach to Recognizing Textual Entailment</title>
<author confidence="0.999972">Marta Tatu</author>
<author confidence="0.999972">Dan Moldovan</author>
<affiliation confidence="0.999923">Language Computer Corporation</affiliation>
<address confidence="0.999355">Richardson, Texas, 75080</address>
<affiliation confidence="0.973638">United States of America</affiliation>
<email confidence="0.998512">marta,moldovan@languagecomputer.com</email>
<abstract confidence="0.995202666666667">This paper proposes a knowledge representation model and a logic proving setting with axioms on demand successfully used for recognizing textual entailments. It also details a lexical inference system which boosts the performance of the deep semantic oriented approach on the RTE data. The linear combination of two slightly different logical systems with the third lexical inference system achieves 73.75% accuracy on the RTE 2006 data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
</authors>
<title>Time and Time Again: The Many Ways to Represent Time.</title>
<date>1991</date>
<journal>Internatinal Journal of Intelligent Systems,</journal>
<volume>4</volume>
<issue>6</issue>
<contexts>
<context position="21175" citStr="Allen, 1991" startWordPosition="3370" endWordPosition="3371">elation shows inclusion (John is in the car in the garage \x10 LOCATION SR(John,garage). John is near the car behind the garage \x02\x10 LOCATION SR(John,garage)). 6 Temporal Axioms One of the types of temporal axioms that we load in our logic prover links specific dates to more general time intervals. For example, October 2000 entails the year 2000. These axioms are automatically generated before the search for a proof starts. Additionally, the prover uses a SUMO knowledge base of temporal reasoning axioms that consists of axioms for a representation of time points and time intervals, Allen (Allen, 1991) primitives, and temporal functions. For example, during is a transitive Allen primitive: during TMP(e1,e2) &amp; during TMP(e2,e3) \x10 during TMP(e1,e3). 7 Experiments and Results The benchmark corpus for the RTE 2005 task consists of seven subsets with a 50%-50% split between the positive entailment examples and the negative ones. Each subgroup corresponds to a different NLP application: Information Retrival (IR), Comparable Documents (CD), Reading Comprehension (RC), Question Answering (QA), Information Extraction (IE), Machine Translation (MT), and Paraphrase Acquisition (PP). The RTE data se</context>
</contexts>
<marker>Allen, 1991</marker>
<rawString>J. Allen. 1991. Time and Time Again: The Many Ways to Represent Time. Internatinal Journal of Intelligent Systems, 4(6):341355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bar-Haim</author>
<author>I Dagan</author>
<author>B Dolan</author>
<author>L Ferro</author>
<author>D Giampiccolo</author>
<author>B Magnini</author>
<author>I Szpektor</author>
</authors>
<title>The Challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop.</booktitle>
<contexts>
<context position="1632" citStr="Bar-Haim et al., 2006" startWordPosition="235" endWordPosition="238">nt text fragments have the same meaning or, more generally, if the meaning of one text can be derived from the meaning of another. A module that recognizes the semantic entailment between two text snippets can be employed by many NLP applications. For example, Question Answering systems have to identify texts that entail expected answers. In Multi-document Summarization, the redundant information should be recognized and omitted from the summary. Trying to boost research in textual inferences, the PASCAL Network proposed the Recognizing Textual Entailment (RTE) challenges (Dagan et al., 2005; Bar-Haim et al., 2006). For a pair of two text fragments, the task is to determine if the meaning of one text (the entailed hypothesis denoted by ) can be inferred from the meaning of the other text (the entailing text or \x01 ). In this paper, we propose a model to represent the knowledge encoded in text and a logical setting suitable to a recognizing semantic entailment system. We cast the textual inference problem as a logic implication between meanings. Text \x01 semantically entails if its meaning logically implies the meaning of . Thus, we, first, transform both text fragments into logic form, capture their m</context>
<context position="24285" citStr="Bar-Haim et al., 2006" startWordPosition="3876" endWordPosition="3879">and infer that the currency used in China is the yuan because a countrys currency currency used in the country. Some of the pairs that the prover, currently, cannot handle involve numeric calculus and human-oriented estimations. Consider, for example, pair 359 (dev set, RTE 2006) labeled as positive, for which the logic prover could not determine that 15 safety violations numerous safety violations. The deeper analysis of the systems output 11 For the RTE 2005 data, we list the confidence-weighted score (cws) (Dagan et al., 2005) and, for the RTE 2006 data, the average precision (ap) measure (Bar-Haim et al., 2006). 824 \x0cTask COGEX\x08 COGEX\t LEXALIGN COMBINATION acc cws f acc cws f acc cws f acc cws f IE 58.33 60.90 60.31 57.50 57.03 51.42 56.66 53.41 59.99 62.50 67.63 57.14 IR 52.22 62.41 15.68 53.33 59.67 27.58 50.00 55.92 0.00 68.88 75.77 64.10 CD 82.00 88.90 79.69 79.33 87.15 74.38 82.00 88.04 80.57 84.66 91.73 82.70 QA 50.00 56.27 0.00 51.53 42.37 64.80 53.07 43.76 63.90 60.76 55.05 63.82 RC 53.57 56.38 38.09 57.14 59.32 58.33 57.85 60.26 49.57 60.00 62.89 50.00 MT 55.83 55.83 53.91 52.50 58.17 27.84 51.66 45.94 67.04 64.16 63.80 66.66 PP 56.00 63.11 26.66 54.00 58.15 30.30 50.00 47.03 0.00 68</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo, B. Magnini, and I. Szpektor. 2006. The Challenge. In Proceedings of the Second PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
<author>K Markert</author>
</authors>
<title>Recognizing Textual Entailment with Logical Inference.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP 2005,</booktitle>
<location>Vancouver, Canada,</location>
<contexts>
<context position="2726" citStr="Bos and Markert, 2005" startWordPosition="421" endWordPosition="424">f its meaning logically implies the meaning of . Thus, we, first, transform both text fragments into logic form, capture their meaning by detecting the semantic relations that hold between their constituents and load these rich logic representations into a natural language logic prover to decide if the entailment holds or not. Figure 1 illustrates our approach to RTE. The following sections of the paper shall detail the logic proving methodology, our logical representation of text and the various types of axioms that the prover uses. To our knowledge, there are few logical approaches to RTE. (Bos and Markert, 2005) represents \x01 and into a first-order logic translation of the DRS language used in Discourse Representation Theory (Kamp and Reyle, 1993) and uses a theorem prover and a model builder with some generic, lexical and geographical background knowledge to prove the entailment between the two texts. (de Salvo Braz et al., 2005) proposes a Description Logic-based knowledge representation language used to induce the representations of \x01 and and uses an extended subsumption algorithm to check if any of \x01 s representations obtained through equivalent transformations entails . 2 Cogex - A Logic</context>
</contexts>
<marker>Bos, Markert, 2005</marker>
<rawString>J. Bos and K. Markert. 2005. Recognizing Textual Entailment with Logical Inference. In Proceedings of HLT/EMNLP 2005, Vancouver, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three Generative, Lexicalized Models for Statistical Parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL-97.</booktitle>
<contexts>
<context position="7086" citStr="Collins, 1997" startWordPosition="1136" endWordPosition="1137">not present in Table 1: The quantification of \x01 and influences the proof scoring algorithm prepositions and conjunctions are also added to link the texts constituents. This syntactic layer of the logic representation is, automatically, derived from a full parse tree and acknowledges syntaxbased relationships such as: syntactic subjects, syntactic objects, prepositional attachments, complex nominals, and adjectival/adverbial adjuncts. In order to objectively evaluate our representation, we derived it from two different sources: constituency parse trees (generated with our implementation of (Collins, 1997)) and dependency parse trees (created using Minipar (Lin, 1998))1. The two logic forms are slightly different. The dependency representation captures more accurately the syntactic dependencies between the concepts, but lacks the semantic information that our semantic parser extracts from the constituency parse trees. For instance, the sentence Gilda Flores was kidnapped on the 13th of January 19902 is constituency represented as Gilda NN(x1) &amp; Flores NN(x2) &amp; nn NNC(x3,x1,x2) &amp; human NE(x3) &amp; kidnap VB(e1,x9,x3) &amp; on IN(e1,x8) &amp; 13th NN(x4) &amp; of NN(x5) &amp; January (x6) &amp; 1990 NN(x7) &amp; nn NNC(x8,</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three Generative, Lexicalized Models for Statistical Parsing. In Proceedings of the ACL-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop,</booktitle>
<location>Southampton, U.K.,</location>
<contexts>
<context position="1608" citStr="Dagan et al., 2005" startWordPosition="231" endWordPosition="234">mine whether different text fragments have the same meaning or, more generally, if the meaning of one text can be derived from the meaning of another. A module that recognizes the semantic entailment between two text snippets can be employed by many NLP applications. For example, Question Answering systems have to identify texts that entail expected answers. In Multi-document Summarization, the redundant information should be recognized and omitted from the summary. Trying to boost research in textual inferences, the PASCAL Network proposed the Recognizing Textual Entailment (RTE) challenges (Dagan et al., 2005; Bar-Haim et al., 2006). For a pair of two text fragments, the task is to determine if the meaning of one text (the entailed hypothesis denoted by ) can be inferred from the meaning of the other text (the entailing text or \x01 ). In this paper, we propose a model to represent the knowledge encoded in text and a logical setting suitable to a recognizing semantic entailment system. We cast the textual inference problem as a logic implication between meanings. Text \x01 semantically entails if its meaning logically implies the meaning of . Thus, we, first, transform both text fragments into log</context>
<context position="24198" citStr="Dagan et al., 2005" startWordPosition="3861" endWordPosition="3864">llar, the system should recognize the relation between the yuan and Chinas currency and infer that the currency used in China is the yuan because a countrys currency currency used in the country. Some of the pairs that the prover, currently, cannot handle involve numeric calculus and human-oriented estimations. Consider, for example, pair 359 (dev set, RTE 2006) labeled as positive, for which the logic prover could not determine that 15 safety violations numerous safety violations. The deeper analysis of the systems output 11 For the RTE 2005 data, we list the confidence-weighted score (cws) (Dagan et al., 2005) and, for the RTE 2006 data, the average precision (ap) measure (Bar-Haim et al., 2006). 824 \x0cTask COGEX\x08 COGEX\t LEXALIGN COMBINATION acc cws f acc cws f acc cws f acc cws f IE 58.33 60.90 60.31 57.50 57.03 51.42 56.66 53.41 59.99 62.50 67.63 57.14 IR 52.22 62.41 15.68 53.33 59.67 27.58 50.00 55.92 0.00 68.88 75.77 64.10 CD 82.00 88.90 79.69 79.33 87.15 74.38 82.00 88.04 80.57 84.66 91.73 82.70 QA 50.00 56.27 0.00 51.53 42.37 64.80 53.07 43.76 63.90 60.76 55.05 63.82 RC 53.57 56.38 38.09 57.14 59.32 58.33 57.85 60.26 49.57 60.00 62.89 50.00 MT 55.83 55.83 53.91 52.50 58.17 27.84 51.66 4</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>I. Dagan, O. Glickman, and B. Magnini. 2005. The PASCAL Recognising Textual Entailment Challenge. In Proceedings of the PASCAL Challenges Workshop, Southampton, U.K., April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R de Salvo Braz</author>
<author>R Girju</author>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>M Sammons</author>
</authors>
<title>An Inference Model for Semantic Entailment in Natural Language.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI-2005.</booktitle>
<contexts>
<context position="3053" citStr="Braz et al., 2005" startWordPosition="477" endWordPosition="480">gure 1 illustrates our approach to RTE. The following sections of the paper shall detail the logic proving methodology, our logical representation of text and the various types of axioms that the prover uses. To our knowledge, there are few logical approaches to RTE. (Bos and Markert, 2005) represents \x01 and into a first-order logic translation of the DRS language used in Discourse Representation Theory (Kamp and Reyle, 1993) and uses a theorem prover and a model builder with some generic, lexical and geographical background knowledge to prove the entailment between the two texts. (de Salvo Braz et al., 2005) proposes a Description Logic-based knowledge representation language used to induce the representations of \x01 and and uses an extended subsumption algorithm to check if any of \x01 s representations obtained through equivalent transformations entails . 2 Cogex - A Logic Prover for NLP Our system uses COGEX (Moldovan et al., 2003), a natural language prover originating from OTTER (McCune, 1994). Once its set of support is loaded with \x01 and the negated hypothesis (\x02 ) and its usable list with the axioms needed to gener819 \x0cFigure 1: COGEXs Architecture ate inferences, COGEX begins to</context>
</contexts>
<marker>Braz, Girju, Punyakanok, Roth, Sammons, 2005</marker>
<rawString>R. de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, and M. Sammons. 2005. An Inference Model for Semantic Entailment in Natural Language. In Proceedings of AAAI-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
</authors>
<title>Knowledge Processing on Extended WordNet.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: an Electronic Lexical Database and Some of its Applications,</booktitle>
<pages>379--405</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="19821" citStr="Harabagiu and Moldovan (1998)" startWordPosition="3153" endWordPosition="3156">he semantics detected in text. For example, if \x01 states explicitly the KINSHIP (KIN) relations between Nicholas Cage and Alice Kim Cage and between Alice Kim Cage and Kal-el Coppola Cage, the logic prover uses the KIN SR(x1,x2) &amp; KIN SR(x2,x3) \x10 KIN SR(x1,x3) semantic axiom (the transitivity of the blood relation) and the symmetry of this relationship (KIN SR(x1,x2) 9 For example, the axiom country NE(x1) &amp; negotiator NN(x2) &amp; nn NNC(x3,x1,x2) \x11 work VB(e1,x2,x4) &amp; for IN(e1,x1) helps the prover infer that Christopher Hill works for the US from top US negotiator, Christopher Hill. 10 Harabagiu and Moldovan (1998) lists the exact number of possible combinations for several WordNet relations and part-of-speech classes. 823 \x0c\x10 KIN SR(x2,x1)) to infer s statement (KIN(Kal-el Coppola Cage, Nicholas Cage)). Another frequent axiom is LOCATION SR(x1,x2) &amp; PARTWHOLE SR(x2,x3) \x10 LOCATION SR(x1,x3). Given the text John lives in Dallas, Texas and using the axiom, the system infers that John lives in Texas. The system applies the 82 axioms independent of the concepts involved in the semantic composition. There are rules that can be applied only if the concepts that participate satisfy a certain condition </context>
</contexts>
<marker>Harabagiu, Moldovan, 1998</marker>
<rawString>S. Harabagiu and D. Moldovan. 1998. Knowledge Processing on Extended WordNet. In Christiane Fellbaum, editor, WordNet: an Electronic Lexical Database and Some of its Applications, pages 379 405. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kamp</author>
<author>U Reyle</author>
</authors>
<title>From Discourse to Logic: Introduction to Model-theoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory.</title>
<date>1993</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="2866" citStr="Kamp and Reyle, 1993" startWordPosition="444" endWordPosition="447">ecting the semantic relations that hold between their constituents and load these rich logic representations into a natural language logic prover to decide if the entailment holds or not. Figure 1 illustrates our approach to RTE. The following sections of the paper shall detail the logic proving methodology, our logical representation of text and the various types of axioms that the prover uses. To our knowledge, there are few logical approaches to RTE. (Bos and Markert, 2005) represents \x01 and into a first-order logic translation of the DRS language used in Discourse Representation Theory (Kamp and Reyle, 1993) and uses a theorem prover and a model builder with some generic, lexical and geographical background knowledge to prove the entailment between the two texts. (de Salvo Braz et al., 2005) proposes a Description Logic-based knowledge representation language used to induce the representations of \x01 and and uses an extended subsumption algorithm to check if any of \x01 s representations obtained through equivalent transformations entails . 2 Cogex - A Logic Prover for NLP Our system uses COGEX (Moldovan et al., 2003), a natural language prover originating from OTTER (McCune, 1994). Once its set</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>H. Kamp and U. Reyle. 1993. From Discourse to Logic: Introduction to Model-theoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based Evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems,</booktitle>
<location>Granada, Spain,</location>
<contexts>
<context position="7149" citStr="Lin, 1998" startWordPosition="1146" endWordPosition="1147">he proof scoring algorithm prepositions and conjunctions are also added to link the texts constituents. This syntactic layer of the logic representation is, automatically, derived from a full parse tree and acknowledges syntaxbased relationships such as: syntactic subjects, syntactic objects, prepositional attachments, complex nominals, and adjectival/adverbial adjuncts. In order to objectively evaluate our representation, we derived it from two different sources: constituency parse trees (generated with our implementation of (Collins, 1997)) and dependency parse trees (created using Minipar (Lin, 1998))1. The two logic forms are slightly different. The dependency representation captures more accurately the syntactic dependencies between the concepts, but lacks the semantic information that our semantic parser extracts from the constituency parse trees. For instance, the sentence Gilda Flores was kidnapped on the 13th of January 19902 is constituency represented as Gilda NN(x1) &amp; Flores NN(x2) &amp; nn NNC(x3,x1,x2) &amp; human NE(x3) &amp; kidnap VB(e1,x9,x3) &amp; on IN(e1,x8) &amp; 13th NN(x4) &amp; of NN(x5) &amp; January (x6) &amp; 1990 NN(x7) &amp; nn NNC(x8,x4,x5,x6,x7) &amp; date NE(x8) and its dependency logic form is Gil</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Dependency-based Evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems, Granada, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W McCune</author>
</authors>
<date>1994</date>
<booktitle>OTTER 3.0 Reference Manual and Guide.</booktitle>
<contexts>
<context position="3452" citStr="McCune, 1994" startWordPosition="544" endWordPosition="545">eory (Kamp and Reyle, 1993) and uses a theorem prover and a model builder with some generic, lexical and geographical background knowledge to prove the entailment between the two texts. (de Salvo Braz et al., 2005) proposes a Description Logic-based knowledge representation language used to induce the representations of \x01 and and uses an extended subsumption algorithm to check if any of \x01 s representations obtained through equivalent transformations entails . 2 Cogex - A Logic Prover for NLP Our system uses COGEX (Moldovan et al., 2003), a natural language prover originating from OTTER (McCune, 1994). Once its set of support is loaded with \x01 and the negated hypothesis (\x02 ) and its usable list with the axioms needed to gener819 \x0cFigure 1: COGEXs Architecture ate inferences, COGEX begins to search for proofs. To every inference, an appropriate weight is assigned depending on the axiom used for its derivation. If a refutation is found, the proof is complete; if a refutation cannot be found, then predicate arguments are relaxed. When argument relaxation fails to produce a refutation, entire predicates are dropped from the negated hypothesis until a refutation is found. 2.1 Proof scor</context>
</contexts>
<marker>McCune, 1994</marker>
<rawString>William W. McCune, 1994. OTTER 3.0 Reference Manual and Guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>A Novischi</author>
</authors>
<title>Lexical chains for Question Answering.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="11991" citStr="Moldovan and Novischi, 2002" startWordPosition="1894" endWordPosition="1897"> connectivity between two concepts needs to be established in a proof. The axioms on demand are lexical chains and world knowledge axioms. We are keen on the idea of axioms on demand since it is not possible to derive apriori all axioms needed in an arbitrary proof. This brings a considerable level of robustness to our entailment system. 4.1 eXtended WordNet lexical chains For the semantic entailment task, the ability to recognize two semantically-related words is an important requirement. Therefore, we automatically construct lexical chains of WordNet relations from \x01 s constituents to s (Moldovan and Novischi, 2002). In order to avoid errors introduced by a Word Sense Disambiguation system, we used the first senses for each word5 unless the source and the target of the chain are synonyms. If a chain exists6, the system generates, on demand, an axiom with the predicates of the source (from \x01 ) and the target (from ). 5 Because WordNet senses are ranked based on their frequency, the correct sense is most likely among the first \x0b . In our experiments, \x0b \x0c\x0f\x0e . 6 Each lexical chain is assigned a weight based on its properties: shorter chains are better than longer ones, the relations are not</context>
</contexts>
<marker>Moldovan, Novischi, 2002</marker>
<rawString>D. Moldovan and A. Novischi. 2002. Lexical chains for Question Answering. In Proceedings of COLING, Taipei, Taiwan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>V Rus</author>
</authors>
<title>Logic Form Transformation of WordNet and its Applicability to Question Answering.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<contexts>
<context position="5873" citStr="Moldovan and Rus, 2001" startWordPosition="949" endWordPosition="952">score, COGEX normalizes the proof scores by dividing the assessed penalty by the maximum assessable penalty (all the predicates from are dropped). If this final proof score is above a threshold learned on the development data, then the pair is labeled as positive entailment. 3 Knowledge Representation For the textual entailment task, our logic prover uses a two-layered logical representation which captures the syntactic and semantic propositions encoded in a text fragment. 3.1 Logic Form Transformation In the first stage of our representation process, COGEX converts \x01 and into logic forms (Moldovan and Rus, 2001). More specifically, a predicate is created for each noun, verb, adjective and adverb. The nouns that form a noun compound are gathered under a nn NNC predicate. Each named entity class of a noun has a corresponding predicate which shares its argument with the noun predicate it modifies. Predicates for 820 \x0c(\x02\x01 ,\x03\x05\x04 ) (\x03\x06\x01 ,\x07\x04 ) All people read Some smart people read Some people read \x02 All smart people read All smart people read Some people read Some smart people read \x02 All people read Add the dropped points for s modifiers Subtract points for modifiers n</context>
</contexts>
<marker>Moldovan, Rus, 2001</marker>
<rawString>D. Moldovan and V. Rus. 2001. Logic Form Transformation of WordNet and its Applicability to Question Answering. In Proceedings of ACL, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>C Clark</author>
<author>S Harabagiu</author>
<author>S Maiorano</author>
</authors>
<title>COGEX A Logic Prover for Question Answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT/NAACL.</booktitle>
<contexts>
<context position="3387" citStr="Moldovan et al., 2003" startWordPosition="532" endWordPosition="535"> logic translation of the DRS language used in Discourse Representation Theory (Kamp and Reyle, 1993) and uses a theorem prover and a model builder with some generic, lexical and geographical background knowledge to prove the entailment between the two texts. (de Salvo Braz et al., 2005) proposes a Description Logic-based knowledge representation language used to induce the representations of \x01 and and uses an extended subsumption algorithm to check if any of \x01 s representations obtained through equivalent transformations entails . 2 Cogex - A Logic Prover for NLP Our system uses COGEX (Moldovan et al., 2003), a natural language prover originating from OTTER (McCune, 1994). Once its set of support is loaded with \x01 and the negated hypothesis (\x02 ) and its usable list with the axioms needed to gener819 \x0cFigure 1: COGEXs Architecture ate inferences, COGEX begins to search for proofs. To every inference, an appropriate weight is assigned depending on the axiom used for its derivation. If a refutation is found, the proof is complete; if a refutation cannot be found, then predicate arguments are relaxed. When argument relaxation fails to produce a refutation, entire predicates are dropped from t</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, Maiorano, 2003</marker>
<rawString>D. Moldovan, C. Clark, S. Harabagiu, and S. Maiorano. 2003. COGEX A Logic Prover for Question Answering. In Proceedings of the HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>C Clark</author>
<author>S Harabagiu</author>
</authors>
<title>Temporal Context Representation and Reasoning.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="10605" citStr="Moldovan et al., 2005" startWordPosition="1680" endWordPosition="1683">nute, second) &amp; time TMP(EndFn(event), year, month, date, hour, minute, second). Furthermore, temporal reasoning 3 We consider relations such as AGENT, THEME, TIME, LOCATION, MANNER, CAUSE, INSTRUMENT, POSSESSION, PURPOSE, MEASURE, KINSHIP, ATTRIBUTE, etc. 4 R(x,y) should be read as x is R of y. 821 \x0cpredicates are derived from both the detected semantic relations as well as from a module which utilizes a learning algorithm to detect temporally ordered events (\x03\x01 \x04\x03\x02\x05\x04 \x04\x03\x02\x07\x06 \x06, where is the temporal signal linking two events \x02 \x04 and \x02 \x06 ) (Moldovan et al., 2005). From each triple, temporally related SUMO predicates are generated based on hand-coded rules for the signal classes (\x03\x01 sequence, \x02\x05\x04 \x04\x03\x02\x07\x06 \x06 \x08 earlier TMP(e1,e2), \x03\x01 contain, \x02 \x04 \x04\x03\x02 \x06 \x06\t\x08 during TMP(e1,e2), etc.). In the above example, 13th of January 1990 is normalized to the interval time TMP(BeginFn(e2), 1990, 1, 13, 0, 0, 0) &amp; time TMP(EndFn(e2), 1990, 1, 13, 23, 59, 59) and during TMP(e1,e2) is added to the logical representation to show when the kidnapping occurred. 4 Axioms on Demand COGEXs usable list consists of al</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, 2005</marker>
<rawString>D. Moldovan, C. Clark, and S. Harabagiu. 2005. Temporal Context Representation and Reasoning. In Proceedings of IJCAI, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tatu</author>
<author>D Moldovan</author>
</authors>
<title>A Semantic Approach to Recognizing Textual Entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="17701" citStr="Tatu and Moldovan, 2005" startWordPosition="2815" endWordPosition="2818">s the new leader of the Roman Catholic Church. We also incorporate in our system a small common-sense knowledge base of 383 handcoded world knowledge axioms, where 153 have been manually designed based on the entire de8 http://xwn.hlt.utdallas.edu velopment set data, and 230 originate from previous projects. These axioms express knowledge that could not be derived from WordNet regarding employment9, family relations, awards, etc. 5 Semantic Calculus The Semantic Calculus axioms combine two semantic relations identified within a text fragment and increase the semantic connectivity of the text (Tatu and Moldovan, 2005). A semantic axiom which combines two relations, \x01 \x1e and \x01\x03\x02 , is devised by observing the semantic connection between the \x04 \x04 and \x04\x06\x05 words for which there exists at least one other word, \x04 \x06 , such that \x01 \x1e \x03\x07\x04 \x04 \x04\x08\x04 \x06 \x06 (\x04 \x04 \t ; \x10 \x04 \x06 ) and \x01 \x02 \x03\x07\x04 \x06 \x04\x08\x04 \x05 \x06 (\x04 \x06 \t\x0c\x0b \x10 \x04 \x05 ) hold true. We note that not any two semantic relations can be combined: \x01 \x1e and \x01\x03\x02 have to be compatible with respect to the part-of-speech of the common argument. D</context>
</contexts>
<marker>Tatu, Moldovan, 2005</marker>
<rawString>M. Tatu and D. Moldovan. 2005. A Semantic Approach to Recognizing Textual Entailment. In Proceedings of HLT/EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>