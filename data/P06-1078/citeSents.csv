Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (CITATION; CITATION; CITATIONb; CITATION; CITATION),,
On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (CITATION; CITATION), support vector machines (SVMs) CITATION, and conditional random fields (CRFs) CITATION,,
CITATION applied a text-level ME-based NER to ASR results,,
Generative NER models were used for multipass ASR and NER searches using word lattices (CITATIONb; CITATION; CITATION),,
CITATION applied text-based NER to N-best ASR results, and merged the N-best NER results by weighted voting based on several sentence-level results such as ASR and NER scores,,
Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (CITATION; CITATION; CITATIONb; CITATION; CITATION),,
On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (CITATION; CITATION), support vector machines (SVMs) CITATION, and conditional random fields (CRFs) CITATION,,
CITATION applied a text-level ME-based NER to ASR results,,
Many discriminative methods have been applied to NER, such as decision trees CITATION, ME models (CITATION; CITATION), and CRFs CITATION,,
In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese CITATION,,
Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (CITATION; CITATION; CITATIONb; CITATION; CITATION),,
On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (CITATION; CITATION), support vector machines (SVMs) CITATION, and conditional random fields (CRFs) CITATION,,
CITATION applied a text-level ME-based NER to ASR results,,
Many discriminative methods have been applied to NER, such as decision trees CITATION, ME models (CITATION; CITATION), and CRFs CITATION,,
In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese CITATION,,
Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (CITATION; CITATION; CITATIONb; CITATION; CITATION),,
On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (CITATION; CITATION), support vector machines (SVMs) CITATION, and conditional random fields (CRFs) CITATION,,
CITATION applied a text-level ME-based NER to ASR results,,
Generative NER models were used for multipass ASR and NER searches using word lattices (CITATIONb; CITATION; CITATION),,
CITATION applied text-based NER to N-best ASR results, and merged the N-best NER results by weighted voting based on several sentence-level results such as ASR and NER scores,,
We used an ASR engine CITATION with a speaker-independent acoustic model,,
Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (CITATION; CITATION; CITATIONb; CITATION; CITATION),,
On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (CITATION; CITATION), support vector machines (SVMs) CITATION, and conditional random fields (CRFs) CITATION,,
CITATION applied a text-level ME-based NER to ASR results,,
Generative NER models were used for multipass ASR and NER searches using word lattices (CITATIONb; CITATION; CITATION),,
CITATION applied text-based NER to N-best ASR results, and merged the N-best NER results by weighted voting based on several sentence-level results such as ASR and NER scores,,
Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (CITATION; CITATION; CITATIONb; CITATION; CITATION),,
On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (CITATION; CITATION), support vector machines (SVMs) CITATION, and conditional random fields (CRFs) CITATION,,
CITATION applied a text-level ME-based NER to ASR results,,
Generative NER models were used for multipass ASR and NER searches using word lattices (CITATIONb; CITATION; CITATION),,
CITATION applied text-based NER to N-best ASR results, and merged the N-best NER results by weighted voting based on several sentence-level results such as ASR and NER scores,,
Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (CITATION; CITATION; CITATIONb; CITATION; CITATION),,
On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (CITATION; CITATION), support vector machines (SVMs) CITATION, and conditional random fields (CRFs) CITATION,,
CITATION applied a text-level ME-based NER to ASR results,,
Many discriminative methods have been applied to NER, such as decision trees CITATION, ME models (CITATION; CITATION), and CRFs CITATION,,
In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese CITATION,,
3.3 ASR confidence scoring for using the proposed NER model ASR confidence scoring is an important technique in many ASR applications, and many methods have been proposed including using word posterior probabilities on word graphs CITATION, integrating several confidence measures using neural networks CITATION, using linear discriminant analysis CITATION, and using SVMs CITATION,,
Word posterior probability p([w; , t]|X) of word w at time interval [, t] for speech signal X is calculated as follows CITATION: p([w; , t]|X) = X W W [w;,t] n p(X|W ) (p(W )) o p(X) , (1) where W is a sentence hypothesis, W [w; , t] is the set of sentence hypotheses that include w in [, t], p(X|W ) is a acoustic model score, p(W ) is a language model score, is a scaling parameter (&lt;1), and is a language model weight,,
Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (CITATION; CITATION; CITATIONb; CITATION; CITATION),,
On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (CITATION; CITATION), support vector machines (SVMs) CITATION, and conditional random fields (CRFs) CITATION,,
CITATION applied a text-level ME-based NER to ASR results,,
Many discriminative methods have been applied to NER, such as decision trees CITATION, ME models (CITATION; CITATION), and CRFs CITATION,,
In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese CITATION,,
Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (CITATION; CITATION; CITATIONb; CITATION; CITATION),,
On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (CITATION; CITATION), support vector machines (SVMs) CITATION, and conditional random fields (CRFs) CITATION,,
CITATION applied a text-level ME-based NER to ASR results,,
Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (CITATION; CITATION; CITATIONb; CITATION; CITATION),,
On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (CITATION; CITATION), support vector machines (SVMs) CITATION, and conditional random fields (CRFs) CITATION,,
CITATION applied a text-level ME-based NER to ASR results,,
3.3 ASR confidence scoring for using the proposed NER model ASR confidence scoring is an important technique in many ASR applications, and many methods have been proposed including using word posterior probabilities on word graphs CITATION, integrating several confidence measures using neural networks CITATION, using linear discriminant analysis CITATION, and using SVMs CITATION,,
Word posterior probability p([w; , t]|X) of word w at time interval [, t] for speech signal X is calculated as follows CITATION: p([w; , t]|X) = X W W [w;,t] n p(X|W ) (p(W )) o p(X) , (1) where W is a sentence hypothesis, W [w; , t] is the set of sentence hypotheses that include w in [, t], p(X|W ) is a acoustic model score, p(W ) is a language model score, is a scaling parameter (&lt;1), and i,,
We used the training data of the Information Retrieval and Extraction Exercise (IREX) workshop CITATION as the text corpus, which consisted of 1,174 Japanese newspaper articles (10,718 sentences) and 18,200 NEs in eight categories (artifact, organization, location, person, date, time, money, and percent),,
Many discriminative methods have been applied to NER, such as decision trees CITATION, ME models (CITATION; CITATION), and CRFs CITATION,,
In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese CITATION,,
3.3 ASR confidence scoring for using the proposed NER model ASR confidence scoring is an important technique in many ASR applications, and many methods have been proposed including using word posterior probabilities on word graphs CITATION, integrating several confidence measures using neural networks CITATION, using linear discriminant analysis CITATION, and using SVMs CITATION,,
Word posterior probability p([w; , t]|X) of word w at time interval [, t] for speech signal X is calculated as follows CITATION: p([w; , t]|X) = X W W [w;,t] n p(X|W ) (p(W )) o p(X) , (1) where W is a sentence hypothesis, W [w; , t] is the set of sentence hypotheses that include w in [, t], p(X|W ) is a ac,,
Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (CITATION; CITATION; CITATIONb; CITATION; CITATION),,
On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (CITATION; CITATION), support vector machines (SVMs) CITATION, and conditional random fields (CRFs) CITATION,,
CITATION applied a text-level ME-based NER to ASR results,,
Generative NER models were used for multipass ASR and NER searches using word lattices (CITATIONb; CITATION; CITATION),,
CITATION applied text-based NER to N-best ASR results, and merged the N-best NER results by weighted voting based on several sentence-level results such as ASR and NER scores,,
3.3 ASR confidence scoring for using the proposed NER model ASR confidence scoring is an important technique in many ASR applications, and many methods have been proposed including using word posterior probabilities on word graphs CITATION, integrating several confidence measures using neural networks CITATION, using linear discriminant analysis CITATION, and using SVMs CITATION,,
Word posterior probability p([w; , t]|X) of word w at time interval [, t] for speech signal X is calculated as follows CITATION: p([w; , t]|X) = X W W [w;,t] n p(X|W ) (p(W )) o p(X) , (1) where W is a sentence hypothesis, W [w; , t] is the set of sentence hypotheses that include w in [, t], p(X|W ) is a acoustic model score, p(W ) is a language model score, is a scaling parameter (&lt;1), and is a language model weight,,
