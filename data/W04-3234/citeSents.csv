In particular, supervised learning can be used to produce a system with performance at or near the state of the art CITATION,,
3 Co-Clustering As in CITATION, we seek a partition of the vocabulary that maximizes the mutual information between term categories and their contexts,,
To achieve this, we use information theoretic coclustering CITATION, in which a space of entities, on the one hand, and their contexts, on the other, are alternately clustered to maximize mutual information between the two spaces,,
It is known, moreover, that greedy agglomerative clustering leads to partitions that are sub-optimal in terms of a mutual information objective function (see, for example, CITATION),,
While global statistical approaches, such as se\x0cquential averaged perceptrons or CRFs CITATION, appear better suited to the NER,,
paradigm for NER in several languages (CITATION; CITATION), one of them achieving the best overall performance in a comparison of several systems CITATION,,
uments, typically by bootstrapping a classifier using automatic labellings (CITATION; CITATION; CITATION),,
Several papers have shown that distributional clustering yields categories that have high agreement with part of speech (CITATION; CITATION),,
of researchers have therefore sought to exploit the availability of unlabeled documents, typically by bootstrapping a classifier using automatic labellings (CITATION; CITATION; CITATION),,
Several papers have shown that distributional clustering yields categories that have high agreement with part of speech (CITATION; CITATION),,
Examples of this are CITATION, CITATION, and CITATION,,
Strong corroboration for the approach advocated in this paper is provided by CITATION, in which cluster-based features are combined with a sequential maximum entropy model proposed in CITATION to advance the state of the art,,
of researchers have therefore sought to exploit the availability of unlabeled documents, typically by bootstrapping a classifier using automatic labellings (CITATION; CITATION; CITATION),,
Several papers have shown that distributional clustering yields categories that have high agreement with part of speech (CITATION; CITATION),,
We can also exploit what CITATION call the one sense per discourse phenomenon, the tendency of terms to have a fixed meaning within a single document,,
Examples of this are CITATION, CITATION, and CITATION,,
3 Co-Clustering As in CITATION, we seek a partition of the vocabulary that maximizes the mutual information between term categories and their contexts,,
To achieve this, we use information theoretic coclustering CITATION, in which a space of entities, on the one hand, and their contexts, on the other, are alternately clustered to maximize mutual information between the two spaces,,
More details are available in the paper in which BWI was defined CITATION,,
2.1 Boosting BWI uses generalized AdaBoost to produce each boundary classifier CITATION,,
jective function (see, for example, CITATION),,
While global statistical approaches, such as se\x0cquential averaged perceptrons or CRFs CITATION, appear better suited to the NER problem than local symbolic learners, the two approaches search different hypothesis spaces,,
Strong corroboration for the approach advocated in this paper is provided by CITATION, in which cluster-based features are combined with a sequential maximum entropy model proposed in CITATION to advance the state of the art,,
paradigm for NER in several languages (CITATION; CITATION), one of them achieving the best overall performance in a comparison of several systems CITATION,,
More details are available in the paper in which BWI was defined CITATION,,
2.1 Boosting BWI uses generalized AdaBoost to produce each boundary classifier CITATION,,
f unlabeled documents, typically by bootstrapping a classifier using automatic labellings (CITATION; CITATION; CITATION),,
Several papers have shown that distributional clustering yields categories that have high agreement with part of speech (CITATION; CITATION),,
of researchers have therefore sought to exploit the availability of unlabeled documents, typically by bootstrapping a classifier using automatic labellings (CITATION; CITATION; CITATION),,
Several papers have shown that distributional clustering yields categories that have high agreement with part of speech (CITATION; CITATION),,
Examples of this are CITATION, CITATION, and CITATION,,
paradigm for NER in several languages (CITATION; CITATION), one of them achieving the best overall performance in a comparison of several systems CITATION,,
