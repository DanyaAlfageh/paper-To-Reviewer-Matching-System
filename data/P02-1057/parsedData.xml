<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000028">
<title confidence="0.484796">
b&apos;A Noisy-Channel Model for Document Compression
</title>
<author confidence="0.571216">
Hal Daume III and Daniel Marcu
</author>
<affiliation confidence="0.754873333333333">
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.574045">
Marina del Rey, CA 90292
</address>
<email confidence="0.874219">
hdaume,marcu\x01 @isi.edu
</email>
<sectionHeader confidence="0.985575" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997241368421053">
We present a document compression sys-
tem that uses a hierarchical noisy-channel
model of text production. Our compres-
sion system first automatically derives the
syntactic structure of each sentence and
the overall discourse structure of the text
given as input. The system then uses a sta-
tistical hierarchical model of text produc-
tion in order to drop non-important syn-
tactic and discourse constituents so as to
generate coherent, grammatical document
compressions of arbitrary length. The sys-
tem outperforms both a baseline and a
sentence-based compression system that
operates by simplifying sequentially all
sentences in a text. Our results support
the claim that discourse knowledge plays
an important role in document summariza-
tion.
</bodyText>
<sectionHeader confidence="0.996703" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994900380952381">
Single document summarization systems proposed
to date fall within one of the following three classes:
Extractive summarizers simply select and present
to the user the most important sentences in
a text see (Mani and Maybury, 1999;
Marcu, 2000; Mani, 2001) for comprehensive
overviews of the methods and algorithms used
to accomplish this.
Headline generators are noisy-channel probabilis-
tic systems that are trained on large corpora
of \x02Headline, Text\x03 pairs (Banko et al., 2000;
Berger and Mittal, 2000). These systems pro-
duce short sequences of words that are indica-
tive of the content of the text given as input.
Sentence simplification systems (Chandrasekar et
al., 1996; Mahesh, 1997; Carroll et al., 1998;
Grefenstette, 1998; Jing, 2000; Knight and
Marcu, 2000) are capable of compressing long
sentences by deleting unimportant words and
phrases.
Extraction-based summarizers often produce out-
puts that contain non-important sentence fragments.
For example, the hypothetical extractive summary
of Text (1), which is shown in Table 1, can be com-
pacted further by deleting the clause which is al-
ready almost enough to win. Headline-based sum-
maries, such as that shown in Table 1, are usually
indicative of a texts content but not informative,
grammatical, or coherent. By repeatedly applying a
sentence-simplification algorithm one sentence at a
time, one can compress a text; yet, the outputs gen-
erated in this way are likely to be incoherent and
to contain unimportant information. When summa-
rizing text, some sentences should be dropped alto-
gether.
Ideally, we would like to build systems that have
the strengths of all these three classes of approaches.
The Document Compression entry in Table 1
shows a grammatical, coherent summary of Text (1),
which was generated by a hypothetical document
compression system that preserves the most impor-
tant information in a text while deleting sentences,
phrases, and words that are subsidiary to the main
message of the text. Obviously, generating coher-
ent, grammatical summaries such as that produced
by the hypothetical document compression system
in Table 1 is not trivial because of many conflicting
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 449-456.
Proceedings of the 40th Annual Meeting of the Association for
\x0cType of Hypothetical output Output Output is Output is
Summarizer contains only coherent grammatical
important info
Extractive John Doe has already secured the vote of most \x04
summarizer democrats in his constituency, which is already
almost enough to win. But without the support
of the governer, he is still on shaky ground.
Headline mayor vote constituency governer \x04
generator
Sentence The mayor is now looking for re-election. John Doe \x04
simplifier has already secured the vote of most democrats
in his constituency. He is still on shaky ground.
Document John Doe has secured the vote of most democrats. \x04 \x04 \x04
compressor But he is still on shaky ground.
</bodyText>
<tableCaption confidence="0.754209">
Table 1: Hypothetical outputs generated by various types of summarizers.
</tableCaption>
<bodyText confidence="0.992163">
goals1. The deletion of certain sentences may result
in incoherence and information loss. The deletion of
certain words and phrases may also lead to ungram-
maticality and information loss.
The mayor is now looking for re-election. John Doe
has already secured the vote of most democrats in his
constituency, which is already almost enough to win.
But without the support of the governer, he is still on
shaky grounds.
</bodyText>
<equation confidence="0.805938">
(1)
</equation>
<bodyText confidence="0.995757428571429">
In this paper, we present a document compression
system that uses hierarchical models of discourse
and syntax in order to simultaneously manage all
these conflicting goals. Our compression system
first automatically derives the syntactic structure of
each sentence and the overall discourse structure of
the text given as input. The system then uses a sta-
tistical hierarchical model of text production in or-
der to drop non-important syntactic and discourse
units so as to generate coherent, grammatical doc-
ument compressions of arbitrary length. The system
outperforms both a baseline and a sentence-based
compression system that operates by simplifying se-
quentially all sentences in a text.
</bodyText>
<sectionHeader confidence="0.978527" genericHeader="method">
2 Document Compression
</sectionHeader>
<bodyText confidence="0.954755333333333">
The document compression task is conceptually
simple. Given a document \x05\x07\x06\x08\x02
\t\x0c\x0b
\t\x0f\x0e\x11\x10\x12\x10\x13\x10\x14\t\x16\x15\x17\x03 , our
goal is to produce a new document \x05\x19\x18 by dropping
words \t\x16\x1a from \x05 . In order to achieve this goal, we
</bodyText>
<page confidence="0.937129">
1
</page>
<bodyText confidence="0.999243121951219">
A number of other systems use the outputs of extrac-
tive summarizers and repair them to improve coherence (DUC,
2001; DUC, 2002). Unfortunately, none of these seems flexible
enough to produce in one shot good summaries that are simul-
taneously coherent and grammatical.
extent the noisy-channel model proposed by Knight
&amp; Marcu (2000). Their system compressed sen-
tences by dropping syntactic constituents, but could
be applied to entire documents only on a sentence-
by-sentence basis. As discussed in Section 1, this
is not adequate because the resulting summary may
contain many compressed sentences that are irrele-
vant. In order to extend Knight &amp; Marcus approach
beyond the sentence level, we need to glue sen-
tences together in a tree structure similar to that used
at the sentence level. Rhetorical Structure Theory
(RST) (Mann and Thompson, 1988) provides us this
glue.
The tree in Figure 1 depicts the RST structure
of Text (1). In RST, discourse structures are non-
binary trees whose leaves correspond to elementary
discourse units (EDUs), and whose internal nodes
correspond to contiguous text spans. Each internal
node in an RST tree is characterized by a rhetor-
ical relation. For example, the first sentence in
Text (1) provides BACKGROUND information for inter-
preting the information in sentences 2 and 3, which
are in a CONTRAST relation (see Figure 1). Each re-
lation holds between two adjacent non-overlapping
text spans called NUCLEUS and SATELLITE. (There are
a few exceptions to this rule: some relations, such
as LIST and CONTRAST, are multinuclear.) The dis-
tinction between nuclei and satellites comes from
the empirical observation that the nucleus expresses
what is more essential to the writers purpose than
the satellite.
Our system is able to analyze both the discourse
structure of a document and the syntactic structure
of each of its sentences or EDUs. It then compresses
\x0cthe document by dropping either syntactic or dis-
course constituents.
</bodyText>
<sectionHeader confidence="0.998896" genericHeader="method">
3 A Noisy-Channel Model
</sectionHeader>
<bodyText confidence="0.995895016129032">
For a given document \x05 , we want to find the
summary text \x1b that maximizes \x1c\x19\x1d\x1e\x1b \x1f!\x05#&amp;quot; . Using
Bayes rule, we flip this so we end up maximizing
\x1c\x19\x1d\x1e\x05$\x1f%\x1b&amp;&amp;quot;\&apos;\x1c\x19\x1d\x1e\x1b&amp;&amp;quot;. Thus, we are left with modelling two
probability distributions: \x1c\x19\x1d\x1e\x05$\x1f%\x1b&amp;&amp;quot; , the probability of
a document \x05 given a summary \x1b , and \x1c\x19\x1d\x1e\x1b(&amp;quot; , the
probability of a summary. We assume that we are
given the discourse structure of each document and
the syntactic structures of each of its EDUs.
The intuitive way of thinking about this applica-
tion of Bayes rule, reffered to as the noisy-channel
model, is that we start with a summary \x1b and add
noise to it, yielding a longer document \x05 . The
noise added in our model consists of words, phrases
and discourse units.
For instance, given the document John Doe has
secured the vote of most democrats. we could add
words to it (namely the word already) to gener-
ate John Doe has already secured the vote of most
democrats. We could also choose to add an en-
tire syntactic constituent, for instance a prepositional
phrase, to generate John Doe has secured the vote
of most democrats in his constituency. These are
both examples of sentence expansion as used previ-
ously by Knight &amp; Marcu (2000).
Our system, however, also has the ability to ex-
pand on a core message by adding discourse con-
stituents. For instance, it could decide to add another
discourse constituent to the original summary John
Doe has secured the vote of most democrats by
CONTRASTing the information in the summary with
the uncertainty regarding the support of the gover-
nor, thus yielding the text: John Doe has secured
the vote of most democrats. But without the support
of the governor, he is still on shaky ground.
As in any noisy-channel application, there are
three parts that we have to account for if we are to
build a complete document compression system: the
channel model, the source model and the decoder.
We describe each of these below.
The source model assigns to a string the probabil-
ity \x1c\x19\x1d\x1e\x1b(&amp;quot; , the probability that the summary \x1b
is good English. Ideally, the source model
should disfavor ungrammatical sentences and
documents containing incoherently juxtaposed
sentences.
The channel model assigns to any docu-
ment/summary pair a probability \x1c\x19\x1d)\x05*\x1f%\x1b(&amp;quot; .
This models the extent to which \x05 is a good
expansion of \x1b . For instance, if \x1b is The
mayor is now looking for re-election., \x05+\x0b is
The mayor is now looking for re-election.
He has to secure the vote of the democrats.
and \x05\x19\x0e is The major is now looking for
re-election. Sharks have sharp teeth., we
expect \x1c\x19\x1d\x1e\x05,\x0b-\x1f%\x1b(&amp;quot; to be higher than \x1c\x19\x1d\x1e\x05\x19\x0e.\x1f%\x1b&amp;&amp;quot;
because \x05,\x0b expands on \x1b by elaboration,
while \x05\x19\x0e shifts to a different topic, yielding an
incoherent text.
The decoder searches through all possible sum-
maries of a document \x05 for the summary
\x1b that maximizes the posterior probability
</bodyText>
<equation confidence="0.3810645">
\x1c\x19\x1d)\x05*\x1f%\x1b(&amp;quot;
\x1c\x19\x1d\x1e\x1b(&amp;quot; .
</equation>
<bodyText confidence="0.69797">
Each of these parts is described below.
</bodyText>
<subsectionHeader confidence="0.989879">
3.1 Source model
</subsectionHeader>
<bodyText confidence="0.996070076923077">
The job of the source model is to assign a score
\x1c\x19\x1d\x1e\x1b(&amp;quot; to a compression independent of the original
document. That is, the source model should measure
how good English a summary is (independent of
whether it is a good compression or not). Currently,
we use a bigram measure of quality (trigram scores
were also tested but failed to make a difference),
combined with non-lexicalized context-free syntac-
tic probabilities and context-free discourse probabil-
ities, giving \x1c\x19\x1d\x1e\x1b(&amp;quot;/\x06 \x1c102\x1a436587\&apos;9\x19\x1d\x1e\x1b&amp;&amp;quot;\x0c:\x19\x1c&lt;;&amp;gt;=@?\x11A\x0f\x1d\x1e\x1b(&amp;quot;\x0c:
\x1c&lt;BC;&amp;gt;=@?\x11A \x1d)\x1b&amp;&amp;quot; . It would be better to use a lexical-
ized context free grammar, but that was not possible
given the decoder used.
</bodyText>
<subsectionHeader confidence="0.997161">
3.2 Channel model
</subsectionHeader>
<bodyText confidence="0.9905227">
The channel model is allowed to add syntactic
constituents (through a stochastic operation called
constituent-expand) or discourse units (through an-
other stochastic operation called EDU-expand).
Both of these operations are performed on a com-
bined discourse/syntax tree called the DS-tree. The
DS-tree for Text (1) is shown in Figure 1 for refer-
ence.
Suppose we start with the summary \x1bD\x06 The
mayor is looking for re-election. A constituent-
</bodyText>
<figure confidence="0.995552777777778">
\x0cEFFG
S
NPB
DT NN
VP
VBZ ADVP
RB
VPA
VBG PP
NPB
NN PUNC.
IN
The mayor
now looking
for
is
reelection .
HIGJKILMNOFPQR
TOP
John Doe has already
secured the vote of
most democrats in his
constituency,
which is already
almost enough
to win.
But without the
support of the
governer,
he is still
on shaky
ground.
SPLJHTIQ HIGJUVIWPIGXFQ HIGJLFQRXGXFQ SPLJHTIQ
SPLJYFQGOIZG SPLJYFQGOIZG
SPLJHTIQ
* *
</figure>
<figureCaption confidence="0.999938">
Figure 1: The discourse (full)/syntax (partial) tree for Text (1).
</figureCaption>
<bodyText confidence="0.982611741935484">
expand operation could insert a syntactic con-
stituent, such as this year anywhere in the syntac-
tic tree of \x1b . A constituent-expand operation could
also add single words: for instance the word now
could be added between is and looking, yielding
\x05[\x06 The mayor is now looking for re-election.
The probability of inserting this word is based on
the syntactic structure of the node into which its in-
serted.
Knight and Marcu (2000) describe in detail a
noisy-channel model that explains how short sen-
tences can be expanded into longer ones by inserting
and expanding syntactic constituents (and words).
Since our constituent-expand stochastic operation
simply reimplements Knight and Marcus model, we
do not focus on them here. We refer the reader
to (Knight and Marcu, 2000) for the details.
In addition to adding syntactic constituents, our
system is also able to add discourse units. Consider
the summary \x1b\\\x06 John Doe has already secured the
vote of most democrats in his consituency. Through
a sequence of discourse expansions, we can expand
upon this summary to reach the original text. A com-
plete discourse expansion process that would occur
starting from this initial summary to generate the
original document is shown in Figure 2.
In this figure, we can follow the sequence of
steps required to generate our original text, begin-
ning with our summary \x1b . First, through an op-
eration D-Project (D for Discourse), we in-
crease the depth of the tree, adding an intermediate
NUC=SPAN node. This projection adds a factor of
\x1c\x19\x1dNuc=Span ] Nuc=Span\x1f Nuc=Span&amp;quot; to the probabil-
ity of this sequence of operations (as is shown under
the arrow).
We are now able to perform the second operation,
D-Expand, with which we expand on the core mes-
sage contained in \x1b by adding a satellite which eval-
uates the information presented in \x1b . This expansion
adds the probability of performing the expansion
(called the discourse expansion probabilities, \x1c&lt;BC^ .
An example discourse expansion probability, writ-
ten \x1c\x19\x1dNuc=Span ] Nuc=Span Sat=Eval\x1f Nuc=Span ]
Nuc=Span&amp;quot; , reflects the probability of adding an eval-
uation satellite onto a nuclear span).
The rest of Figure 2 shows some of the remaining
steps to produce the original document, each step la-
beled with the appropriate probability factors. Then,
the probability of the entire expansion is the prod-
uct of all those listed probabilities combined with
the appropriate probabilities from the syntax side of
things. In order to produce the final score \x1c\x19\x1d\x1e\x05$\x1f%\x1b&amp;&amp;quot;
for a document/summary pair, we multiply together
each of the expansion probabilities in the path lead-
ing from \x1b to \x05 .
For estimating the parameters for the discourse
models, we used an RST corpus of 385 Wall Street
Journal articles from the Penn Treebank, which we
obtained from LDC. The documents in the corpus
range in size from 31 to 2124 words, with an av-
erage of 458 words per document. Each document
is paired with a discourse structure that was manu-
</bodyText>
<figure confidence="0.942613666666666">
\x0c_``a
John Doe has already
secured the vote of
most democrats in his
constituency,
bcdefghi which is already
almost enough
to win.
fhaejkhlcham`i
bcdefghi nopqrstuv
nowxyz{|
nopqrstuv
_``a
John Doe has already
secured the vote of
most democrats in his
constituency,
bcdefghi
}~\x7f
_``a
John Doe has already
secured the vote of
most democrats in his
constituency,
bcdefghi
bcdefghi
John Doe has already
secured the vote of
most democrats in his
constituency,
bcdefghi which is already
almost enough
to win.
fhaejkhlcham`i But without the
support of the
governer,
fhaed`imam`i he is still
on shaky
ground.
bcdefghi
bcdefghi
bcde`iaha
bcde`iaha
_``a
nowxyz{|
John Doe has already
secured the vote of
most democrats in his
constituency,
bcdefghi which is already
almost enough
to win.
fhaejkhlcham`i But without the
support of the
governer,
fhaed`imam`i he is still
on shaky
ground.
bcdefghi
The mayor is
now looking
for reelection.
fhaehd`ci bcdefghi
bcde`iaha
bcde`iaha
_``a
_``a
John Doe has already
secured the vote of
most democrats in his
constituency,
bcdefghi which is already
almost enough
to win.
fhaejkhlcham`i
bcde`iaha
bcdefghi
John Doe has already
secured the vote of
most democrats in his
constituency,
bcdefghi which is already
almost enough
to win.
fhaejkhlcham`i
bcdefghi
bcde`iaha
_``a
he is still
on shaky
ground.
bcde`iaha
P(Nuc=Span &amp;gt; Nuc=Span
Sat=evaluation
Nuc=Span &amp;gt; Nuc=Span)
P(Nuc=Span &amp;gt; Nuc=Span |
P(Nuc=Span &amp;gt; Nuc=Contrast
Nuc=Contrast |
P(Root &amp;gt; Sat=Background Nuc=Span |
Root &amp;gt; Nuc=Span)
Nuc=Span)
P(Nuc=Span &amp;gt; Nuc=Contrast  |Nuc=Span)
Nuc=Span &amp;gt; Nuc=Contrast)
P(Nuc=Contrast &amp;gt; Sat=condiation Nuc=Span |
Nuc=Contrast &amp;gt; Nuc=Span)
nowxyz{|
nopqrstuv
P(Nuc=Contrast &amp;gt; Nuc=Span  |Nuc=Contrast)*
</figure>
<figureCaption confidence="0.995981">
Figure 2: A sequence of discourse expansions for Text (1) (with probability factors).
</figureCaption>
<bodyText confidence="0.97280444">
ally built in the style of RST. (See (Carlson et al.,
2001) for details concerning the corpus and the an-
notation process.) From this corpus, we were able
to estimate parameters for a discourse PCFG using
standard maximum likelihood methods.
Furthermore, 150 document from the same corpus
are paired with extractive summaries on the EDU
level. Human annotators were asked which EDUs
were most important; suppose in the example DS-
tree (Figure 1) the annotators marked the second
and fifth EDUs (the starred ones). These stars are
propagated up, so that any discourse unit that has
a descendent considered important is also consid-
ered important. From these annotations, we could
deduce that, to compress a NUC=CONTRAST that has
two children, NUC=SPAN and SAT=EVALUATION, we
can drop the evaluation satellite. Similarly, we can
compress a NUC=CONTRAST that has two children,
SAT=CONDITION and NUC=SPAN by dropping the first
discourse constituent. Finally, we can compress the
ROOT deriving into SAT=BACKGROUND NUC=SPAN by
dropping the SAT=BACKGROUND constituent. We keep
counts of each of these examples and, once col-
lected, we normalize them to get the discourse ex-
pansion probabilities.
</bodyText>
<subsectionHeader confidence="0.996637">
3.3 Decoder
</subsectionHeader>
<bodyText confidence="0.999723227272727">
The goal of the decoder is to combine \x1c\x19\x1d)\x1b&amp;&amp;quot; with
\x1c\x19\x1d\x1e\x05$\x1f%\x1b&amp;&amp;quot; to get \x1c\x19\x1d\x1e\x1b\x0c\x1f%\x05,&amp;quot; . There are a vast number
of potential compressions of a large DS-tree, but
we can efficiently pack them into a shared-forest
structure, as described in detail by Knight &amp; Marcu
(2000). Each entry in the shared-forest structure has
three associated probabilities, one from the source
syntax PCFG, one from the source discourse PCFG
and one from the expansion-template probabilities
described in Section 3.2. Once we have generated a
forest representing all possible compressions of the
original document, we want to extract the best (or
the -best) trees, taking into account both the ex-
pansion probabilities of the channel model and the
bigram and syntax and discourse PCFG probabili-
ties of the source model. Thankfully, such a generic
extractor has already been built (Langkilde, 2000).
For our purposes, the extractor selects the trees with
the best combination of LM and expansion scores
after performing an exhaustive search over all possi-
ble summaries. It returns a list of such trees, one for
each possible length.
</bodyText>
<sectionHeader confidence="0.994074" genericHeader="method">
4 System
</sectionHeader>
<bodyText confidence="0.999340333333333">
The system developed works in a pipelined fash-
ion as shown in Figure 3. The first step along the
pipeline is to generate the discourse structure. To
do this, we use the decision-based discourse parser
described by Marcu (2000)2. Once we have the dis-
course structure, we send each EDU off to a syn-
</bodyText>
<page confidence="0.945118">
2
</page>
<bodyText confidence="0.6974915">
The discourse parser achieves an f-score of \x12 for EDU
identification,
</bodyText>
<tableCaption confidence="0.798065666666667">
\x12 for identifying hierarchical spans, \x12 for
nuclearity identification and
\x12 for relation tagging.
</tableCaption>
<figure confidence="0.9968261">
\x0cParser
Discourse Syntax
Parser
Forest
Generator
Decoder
Chooser
Length
Output Summary
Input Document
</figure>
<figureCaption confidence="0.999915">
Figure 3: The pipeline of system components.
</figureCaption>
<bodyText confidence="0.994848884615385">
tactic parser (Collins, 1997). The syntax trees of
the EDUs are then merged with the discourse tree
in the forest generator to create a DS-tree similar to
that shown in Figure 1. From this DS-tree we gener-
ate a forest that subsumes all possible compressions.
This forest is then passed on to the forest ranking
system which is used as decoder (Langkilde, 2000).
The decoder gives us a list of possible compressions,
for each possible length. Example compressions of
Text (1) are shown in Figure 4 together with their
respective log-probabilities.
In order to choose the best compression at
any possible length, we cannot rely only on the
log-probabilities, lest the system always choose the
shortest possible compression. In order to compen-
sate for this, we normalize by length. However, in
practice, simply dividing the log-probability by the
length of the compression is insufficient for longer
documents. Experimentally, we found a reasonable
metric was to, for a compression of length , divide
each log-probability by \x0b\&apos;\x0e . This was the job of
the length chooser from Figure 3, and enabled us
to choose a single compression for each document,
which was used for evaluation. (In Figure 4, the
compression chosen by the length selector is itali-
cized and was the shortest one3.)
</bodyText>
<sectionHeader confidence="0.999938" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9999532">
For testing, we began with two sets of data. The
first set is drawn from the Wall Street Journal (WSJ)
portion of the Penn Treebank and consists of \x13 doc-
uments, each containing between and words.
The second set is drawn from a collection of stu-
</bodyText>
<page confidence="0.985516">
3
</page>
<bodyText confidence="0.998766978723404">
This tends to be the case for very short documents, as the
compressions never get sufficiently long for the length normal-
ization to have an effect.
dent compositions and consists of documents, each
containing between . and words. We call this
set the MITRE corpus (Hirschman et al., 1999). We
would liked to have run evaluations on longer docu-
ments. Unfortunately, the forests generated even for
relatively small documents are huge. Because there
are an exponential number of summaries that can be
generated for any given text4
, the decoder runs out
of memory for longer documents; therefore, we se-
lected shorter subtexts from the original documents.
We used both the WSJ and Mitre data for eval-
uation because we wanted to see whether the per-
formance of our system varies with text genre. The
Mitre data consists mostly of short sentences (av-
erage document length from Mitre is sentences),
quite in constrast to the typically long sentences in
the Wall Street Journal articles (average document
length from WSJ is \x104 sentences).
For purpose of comparison, the Mitre data was
compressed using five systems:
Random: Drops random words (each word has a
50% chance of being dropped (baseline).
Hand: Hand compressions done by a human.
Concat: Each sentence is compressed individually;
the results are concatenated together, using
Knight &amp; Marcus (2000) system here for com-
parison.
EDU: The system described in this paper.
Sent: Because syntactic parsers tend not to work
well parsing just clauses, this system merges
together leaves in the discourse tree which are
in the same sentence, and then proceeds as de-
scribed in this paper.
The Wall Street Journal data was evaluated on the
above five systems as well as two additions. Since
the correct discourse trees were known for these
data, we thought it wise to test the systems using
these human-built discourse trees, instead of the au-
tomatically derived ones. The additionall two sys-
tems were:
PD-EDU: Same as EDU except using the perfect
discourse trees, available from the RST corpus
(Carlson et al., 2001).
</bodyText>
<page confidence="0.919803">
4
</page>
<bodyText confidence="0.814465">
In theory, a text of words has 6 possible compressions.
\x0clen log prob best compression
C\x14\x1266 Mayor is now looking which is enough.
C a4\x14\x13\x14 The mayor is now looking which is already almost enough to win.
C a\&apos;\&apos; The mayor is now looking but without support, he is still on shaky ground.
C 6\x126\x13\x14 Mayor is now looking but without the support of governer, he is still on shaky ground.
C8\&apos;\x124\x146 The mayor is now looking for re-election but without the support of the governer, he is still on shaky
ground.
6\x1266 The mayor is now looking which is already almost enough to win. But without the support of the
governer, he is still on shaky ground.
</bodyText>
<figureCaption confidence="0.828792666666667">
Figure 4: Possible compressions for Text (1).
PD-Sent: The same as Sent except using the perfect
discourse trees.
</figureCaption>
<bodyText confidence="0.993048848484849">
Six human evaluators rated the systems according to
three metrics. The first two, presented together to
the evaluators, were grammaticality and coherence;
the third, presented separately, was summary qual-
ity. Grammaticality was a judgment of how good
the English of the compressions were; coherence
included how well the compression flowed (for in-
stance, anaphors lacking an antecedent would lower
coherence). Summary quality, on the other hand,
was a judgment of how well the compression re-
tained the meaning of the original document. Each
measure was rated on a scale from (worst) to
(best).
We can draw several conclusions from the eval-
uation results shown in Table 2 along with aver-
age compression rate (Cmp, the length of the com-
pressed document divided by the original length).5
First, it is clear that genre influences the results.
Because the Mitre data contained mostly short sen-
tences, the syntax and discourse parsers made fewer
errors, which allowed for better compressions to be
generated. For the Mitre corpus, compressions ob-
tained starting from discourse trees built above the
sentence level were better than compressions ob-
tained starting from discourse trees built above the
EDU level. For the WSJ corpus, compression ob-
tained starting from discourse trees built above the
sentence level were more grammatical, but less co-
herent than compressions obtained starting from dis-
course trees built above the EDU level. Choosing the
manner in which the discourse and syntactic repre-
sentations of texts are mixed should be influenced by
the genre of the texts one is interested to compress.
</bodyText>
<page confidence="0.88972">
5
</page>
<bodyText confidence="0.979452333333333">
We did not run the system on the MITRE data with perfect
discourse trees because we did not have hand-built discourse
trees for this corpus.
</bodyText>
<table confidence="0.979002">
WSJ Mitre
Cmp Grm Coh Qual Cmp Grm Coh Qual
Random 0.51 1.60 1.58 2.13 0.47 1.43 1.77 1.80
Concat 0.44 3.30 2.98 2.70 0.42 2.87 2.50 2.08
EDU 0.49 3.36 3.33 3.03 0.47 3.40 3.30 2.60
Sent 0.47 3.45 3.16 2.88 0.44 4.27 3.63 3.36
PD-EDU 0.47 3.61 3.23 2.95
PD-Sent 0.48 3.96 3.65 2.84
Hand 0.59 4.65 4.48 4.53 0.46 4.97 4.80 4.52
</table>
<tableCaption confidence="0.9922">
Table 2: Evaluation Results
</tableCaption>
<bodyText confidence="0.998392844444445">
The compressions obtained starting from per-
fectly derived discourse trees indicate that perfect
discourse structures help greatly in improving coher-
ence and grammaticality of generated summaries. It
was surprising to see that the summary quality was
affected negatively by the use of perfect discourse
structures (although not statistically significant). We
believe this happened because the text fragments we
summarized were extracted from longer documents.
It is likely that had the discourse structures been built
specifically for these short text snippets, they would
have been different. Moreover, there was no compo-
nent designed to handle cohesion; thus it is to be ex-
pected that many compressions would contain dan-
gling references.
Overall, all our systems outperformed both the
Random baseline and the Concat systems, which
empirically show that discourse has an important
role in document summarization. We performed -
tests on the results and found that on the Wall Street
Journal data, the differences in score between the
Concat and Sent systems for grammaticality and
coherence were statistically significant at the 95%
level, but the difference in score for summary quality
was not. For the Mitre data, the differences in score
between the Concat and Sent systems for grammati-
cality and summary quality were statistically signif-
icant at the 95% level, but the difference in score for
\x0ccoherence was not. The score differences for gram-
maticality, coherence, and summary quality between
our systems and the baselines were statistically sig-
nificant at the 95% level.
The results in Table 2, which can be also as-
sessed by inspecting the compressions in Figure 4
show that, in spite of our success, we are still far
away from human performance levels. An error that
our system makes often is that of dropping comple-
ments that cannot be dropped, such as the phrase
for re-election, which is the complement of is
looking. We are currently experimenting with lex-
icalized models of syntax that would prevent our
compression system from dropping required verb ar-
guments. We also consider methods for scaling up
the decoder to handling documents of more realistic
length.
</bodyText>
<sectionHeader confidence="0.909967" genericHeader="conclusions">
Acknoledgements
</sectionHeader>
<bodyText confidence="0.995495">
This work was partially supported by DARPA-ITO
grant N66001-00-1-9814, NSF grant IIS-0097846,
and a USC Dean Fellowship to Hal Daume III.
Thanks to Kevin Knight for discussions related to
the project.
</bodyText>
<sectionHeader confidence="0.990253" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998407973333333">
Michele Banko, Vibhu Mittal, and Michael Witbrock.
2000. Headline generation based on statistical trans-
lation. In Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics (ACL
2000), pages 318325, Hong Kong, October 18.
Adam Berger and Vibhu Mittal. 2000. Query-relevant
summarization using FAQs. In Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics (ACL2000), pages 294301, Hong
Kong, October 18.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Pro-
ceedings of the 2nd SIGDIAL Workshop on Discourse
and Dialogue, Eurospeech 2001, Aalborg, Denmark,
September.
John Carroll, Guidon Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of english newspaper text to assist aphasic readers. In
Proceedings of the AAAI-98 Workshop on Integrating
Artificial Intelligence and Assistive Technology.
R. Chandrasekar, Christy Doran, and Srinivas Bangalore.
1996. Motivations and methods for text simplifica-
tion. In Proceedings of the Sixteenth International
Conference on Computational Linguistics (COLING
96), Copenhagen, Denmark.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics (ACL97), pages 1623, Madrid,
Spain, July 7-12.
Proceedings of the First Document Understanding Con-
ference (DUC-2001), New Orleans, LA, September.
Proceedings of the Second Document Understanding
Conference (DUC-2002), Philadelphia, PA, July.
Gregory Grefenstette. 1998. Producing intelligent tele-
graphic text reduction to provide an audio scanning
service for the blind. In Working Notes of the AAAI
Spring Symposium on Intelligent Text Summarization,
pages 111118, Stanford University, CA, March 23-
25.
L. Hirschman, M. Light, E. Breck, and J. Burger. 1999.
Deep read: A reading comprehension system. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistics.
H. Jing. 2000. Sentence reduction for automatic text
summarization. In Proceedings of the First Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics NAACL-2000,
pages 310315, Seattle, WA.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization step one: Sentence compression.
In The 17th National Conference on Artificial Intelli-
gence (AAAI2000), pages 703710, Austin, TX, July
30th August 3rd.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of the 1st Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics, Seattle, Washington, April
30May 3.
Kavi Mahesh. 1997. Hypertext summary extraction for
fast document browsing. In Proceedings of the AAAI
Spring Symposium on Natural Language Processing
for the World Wide Web, pages 95103.
Inderjeet Mani and Mark Maybury, editors. 1999. Ad-
vances in Automatic Text Summarization. The MIT
Press.
Inderjeet Mani. 2001. Automatic summarization.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243281.
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. The MIT Press,
Cambridge, Massachusetts.
\x0c&apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.797544">
<title confidence="0.99915">b&apos;A Noisy-Channel Model for Document Compression</title>
<author confidence="0.999323">Hal Daume</author>
<author confidence="0.999323">Daniel Marcu</author>
<affiliation confidence="0.9986335">Information Sciences Institute University of Southern California</affiliation>
<address confidence="0.9990305">4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292</address>
<email confidence="0.97722">hdaume,marcu\x01@isi.edu</email>
<abstract confidence="0.99103715">We present a document compression system that uses a hierarchical noisy-channel model of text production. Our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input. The system then uses a statistical hierarchical model of text production in order to drop non-important syntactic and discourse constituents so as to generate coherent, grammatical document compressions of arbitrary length. The system outperforms both a baseline and a sentence-based compression system that operates by simplifying sequentially all sentences in a text. Our results support the claim that discourse knowledge plays an important role in document summarization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Vibhu Mittal</author>
<author>Michael Witbrock</author>
</authors>
<title>Headline generation based on statistical translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>318325</pages>
<location>Hong Kong,</location>
<contexts>
<context position="1474" citStr="Banko et al., 2000" startWordPosition="215" endWordPosition="218"> in a text. Our results support the claim that discourse knowledge plays an important role in document summarization. 1 Introduction Single document summarization systems proposed to date fall within one of the following three classes: Extractive summarizers simply select and present to the user the most important sentences in a text see (Mani and Maybury, 1999; Marcu, 2000; Mani, 2001) for comprehensive overviews of the methods and algorithms used to accomplish this. Headline generators are noisy-channel probabilistic systems that are trained on large corpora of \x02Headline, Text\x03 pairs (Banko et al., 2000; Berger and Mittal, 2000). These systems produce short sequences of words that are indicative of the content of the text given as input. Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases. Extraction-based summarizers often produce outputs that contain non-important sentence fragments. For example, the hypothetical extractive summary of Text (1), which is shown in Table 1, can be compacted further by deleting t</context>
</contexts>
<marker>Banko, Mittal, Witbrock, 2000</marker>
<rawString>Michele Banko, Vibhu Mittal, and Michael Witbrock. 2000. Headline generation based on statistical translation. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL 2000), pages 318325, Hong Kong, October 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Vibhu Mittal</author>
</authors>
<title>Query-relevant summarization using FAQs.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL2000),</booktitle>
<pages>294301</pages>
<location>Hong Kong,</location>
<contexts>
<context position="1500" citStr="Berger and Mittal, 2000" startWordPosition="219" endWordPosition="222">lts support the claim that discourse knowledge plays an important role in document summarization. 1 Introduction Single document summarization systems proposed to date fall within one of the following three classes: Extractive summarizers simply select and present to the user the most important sentences in a text see (Mani and Maybury, 1999; Marcu, 2000; Mani, 2001) for comprehensive overviews of the methods and algorithms used to accomplish this. Headline generators are noisy-channel probabilistic systems that are trained on large corpora of \x02Headline, Text\x03 pairs (Banko et al., 2000; Berger and Mittal, 2000). These systems produce short sequences of words that are indicative of the content of the text given as input. Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases. Extraction-based summarizers often produce outputs that contain non-important sentence fragments. For example, the hypothetical extractive summary of Text (1), which is shown in Table 1, can be compacted further by deleting the clause which is already</context>
</contexts>
<marker>Berger, Mittal, 2000</marker>
<rawString>Adam Berger and Vibhu Mittal. 2000. Query-relevant summarization using FAQs. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL2000), pages 294301, Hong Kong, October 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd SIGDIAL Workshop on Discourse and Dialogue, Eurospeech</booktitle>
<location>Aalborg, Denmark,</location>
<contexts>
<context position="17219" citStr="Carlson et al., 2001" startWordPosition="2680" endWordPosition="2683">almost enough to win. fhaejkhlcham`i bcdefghi bcde`iaha _``a he is still on shaky ground. bcde`iaha P(Nuc=Span &amp;gt; Nuc=Span Sat=evaluation Nuc=Span &amp;gt; Nuc=Span) P(Nuc=Span &amp;gt; Nuc=Span | P(Nuc=Span &amp;gt; Nuc=Contrast Nuc=Contrast | P(Root &amp;gt; Sat=Background Nuc=Span | Root &amp;gt; Nuc=Span) Nuc=Span) P(Nuc=Span &amp;gt; Nuc=Contrast |Nuc=Span) Nuc=Span &amp;gt; Nuc=Contrast) P(Nuc=Contrast &amp;gt; Sat=condiation Nuc=Span | Nuc=Contrast &amp;gt; Nuc=Span) nowxyz{| nopqrstuv P(Nuc=Contrast &amp;gt; Nuc=Span |Nuc=Contrast)* Figure 2: A sequence of discourse expansions for Text (1) (with probability factors). ally built in the style of RST. (See (Carlson et al., 2001) for details concerning the corpus and the annotation process.) From this corpus, we were able to estimate parameters for a discourse PCFG using standard maximum likelihood methods. Furthermore, 150 document from the same corpus are paired with extractive summaries on the EDU level. Human annotators were asked which EDUs were most important; suppose in the example DStree (Figure 1) the annotators marked the second and fifth EDUs (the starred ones). These stars are propagated up, so that any discourse unit that has a descendent considered important is also considered important. From these annot</context>
<context position="23760" citStr="Carlson et al., 2001" startWordPosition="3737" endWordPosition="3740">tactic parsers tend not to work well parsing just clauses, this system merges together leaves in the discourse tree which are in the same sentence, and then proceeds as described in this paper. The Wall Street Journal data was evaluated on the above five systems as well as two additions. Since the correct discourse trees were known for these data, we thought it wise to test the systems using these human-built discourse trees, instead of the automatically derived ones. The additionall two systems were: PD-EDU: Same as EDU except using the perfect discourse trees, available from the RST corpus (Carlson et al., 2001). 4 In theory, a text of words has 6 possible compressions. \x0clen log prob best compression C\x14\x1266 Mayor is now looking which is enough. C a4\x14\x13\x14 The mayor is now looking which is already almost enough to win. C a\&apos;\&apos; The mayor is now looking but without support, he is still on shaky ground. C 6\x126\x13\x14 Mayor is now looking but without the support of governer, he is still on shaky ground. C8\&apos;\x124\x146 The mayor is now looking for re-election but without the support of the governer, he is still on shaky ground. 6\x1266 The mayor is now looking which is already almost enoug</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Proceedings of the 2nd SIGDIAL Workshop on Discourse and Dialogue, Eurospeech 2001, Aalborg, Denmark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guidon Minnen</author>
<author>Yvonne Canning</author>
<author>Siobhan Devlin</author>
<author>John Tait</author>
</authors>
<title>Practical simplification of english newspaper text to assist aphasic readers.</title>
<date>1998</date>
<booktitle>In Proceedings of the AAAI-98 Workshop on Integrating Artificial Intelligence and Assistive Technology.</booktitle>
<contexts>
<context position="1706" citStr="Carroll et al., 1998" startWordPosition="253" endWordPosition="256">sses: Extractive summarizers simply select and present to the user the most important sentences in a text see (Mani and Maybury, 1999; Marcu, 2000; Mani, 2001) for comprehensive overviews of the methods and algorithms used to accomplish this. Headline generators are noisy-channel probabilistic systems that are trained on large corpora of \x02Headline, Text\x03 pairs (Banko et al., 2000; Berger and Mittal, 2000). These systems produce short sequences of words that are indicative of the content of the text given as input. Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases. Extraction-based summarizers often produce outputs that contain non-important sentence fragments. For example, the hypothetical extractive summary of Text (1), which is shown in Table 1, can be compacted further by deleting the clause which is already almost enough to win. Headline-based summaries, such as that shown in Table 1, are usually indicative of a texts content but not informative, grammatical, or coherent. By repeatedly applying a sentence-sim</context>
</contexts>
<marker>Carroll, Minnen, Canning, Devlin, Tait, 1998</marker>
<rawString>John Carroll, Guidon Minnen, Yvonne Canning, Siobhan Devlin, and John Tait. 1998. Practical simplification of english newspaper text to assist aphasic readers. In Proceedings of the AAAI-98 Workshop on Integrating Artificial Intelligence and Assistive Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chandrasekar</author>
<author>Christy Doran</author>
<author>Srinivas Bangalore</author>
</authors>
<title>Motivations and methods for text simplification.</title>
<date>1996</date>
<booktitle>In Proceedings of the Sixteenth International Conference on Computational Linguistics (COLING 96),</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1670" citStr="Chandrasekar et al., 1996" startWordPosition="247" endWordPosition="250">all within one of the following three classes: Extractive summarizers simply select and present to the user the most important sentences in a text see (Mani and Maybury, 1999; Marcu, 2000; Mani, 2001) for comprehensive overviews of the methods and algorithms used to accomplish this. Headline generators are noisy-channel probabilistic systems that are trained on large corpora of \x02Headline, Text\x03 pairs (Banko et al., 2000; Berger and Mittal, 2000). These systems produce short sequences of words that are indicative of the content of the text given as input. Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases. Extraction-based summarizers often produce outputs that contain non-important sentence fragments. For example, the hypothetical extractive summary of Text (1), which is shown in Table 1, can be compacted further by deleting the clause which is already almost enough to win. Headline-based summaries, such as that shown in Table 1, are usually indicative of a texts content but not informative, grammatical, or coherent. B</context>
</contexts>
<marker>Chandrasekar, Doran, Bangalore, 1996</marker>
<rawString>R. Chandrasekar, Christy Doran, and Srinivas Bangalore. 1996. Motivations and methods for text simplification. In Proceedings of the Sixteenth International Conference on Computational Linguistics (COLING 96), Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL97),</booktitle>
<pages>1623</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="20183" citStr="Collins, 1997" startWordPosition="3141" endWordPosition="3142">ashion as shown in Figure 3. The first step along the pipeline is to generate the discourse structure. To do this, we use the decision-based discourse parser described by Marcu (2000)2. Once we have the discourse structure, we send each EDU off to a syn2 The discourse parser achieves an f-score of \x12 for EDU identification, \x12 for identifying hierarchical spans, \x12 for nuclearity identification and \x12 for relation tagging. \x0cParser Discourse Syntax Parser Forest Generator Decoder Chooser Length Output Summary Input Document Figure 3: The pipeline of system components. tactic parser (Collins, 1997). The syntax trees of the EDUs are then merged with the discourse tree in the forest generator to create a DS-tree similar to that shown in Figure 1. From this DS-tree we generate a forest that subsumes all possible compressions. This forest is then passed on to the forest ranking system which is used as decoder (Langkilde, 2000). The decoder gives us a list of possible compressions, for each possible length. Example compressions of Text (1) are shown in Figure 4 together with their respective log-probabilities. In order to choose the best compression at any possible length, we cannot rely onl</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalized models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL97), pages 1623, Madrid, Spain, July 7-12.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>Proceedings of the First Document Understanding Conference (DUC-2001),</booktitle>
<location>New Orleans, LA,</location>
<marker></marker>
<rawString>Proceedings of the First Document Understanding Conference (DUC-2001), New Orleans, LA, September.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>Proceedings of the Second Document Understanding Conference (DUC-2002),</booktitle>
<location>Philadelphia, PA,</location>
<marker></marker>
<rawString>Proceedings of the Second Document Understanding Conference (DUC-2002), Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Producing intelligent telegraphic text reduction to provide an audio scanning service for the blind.</title>
<date>1998</date>
<booktitle>In Working Notes of the AAAI Spring Symposium on Intelligent Text Summarization,</booktitle>
<pages>111118</pages>
<location>Stanford University, CA,</location>
<contexts>
<context position="1726" citStr="Grefenstette, 1998" startWordPosition="257" endWordPosition="258">rizers simply select and present to the user the most important sentences in a text see (Mani and Maybury, 1999; Marcu, 2000; Mani, 2001) for comprehensive overviews of the methods and algorithms used to accomplish this. Headline generators are noisy-channel probabilistic systems that are trained on large corpora of \x02Headline, Text\x03 pairs (Banko et al., 2000; Berger and Mittal, 2000). These systems produce short sequences of words that are indicative of the content of the text given as input. Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases. Extraction-based summarizers often produce outputs that contain non-important sentence fragments. For example, the hypothetical extractive summary of Text (1), which is shown in Table 1, can be compacted further by deleting the clause which is already almost enough to win. Headline-based summaries, such as that shown in Table 1, are usually indicative of a texts content but not informative, grammatical, or coherent. By repeatedly applying a sentence-simplification algorith</context>
</contexts>
<marker>Grefenstette, 1998</marker>
<rawString>Gregory Grefenstette. 1998. Producing intelligent telegraphic text reduction to provide an audio scanning service for the blind. In Working Notes of the AAAI Spring Symposium on Intelligent Text Summarization, pages 111118, Stanford University, CA, March 23-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>M Light</author>
<author>E Breck</author>
<author>J Burger</author>
</authors>
<title>Deep read: A reading comprehension system.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="21988" citStr="Hirschman et al., 1999" startWordPosition="3445" endWordPosition="3448">ion chosen by the length selector is italicized and was the shortest one3.) 5 Results For testing, we began with two sets of data. The first set is drawn from the Wall Street Journal (WSJ) portion of the Penn Treebank and consists of \x13 documents, each containing between and words. The second set is drawn from a collection of stu3 This tends to be the case for very short documents, as the compressions never get sufficiently long for the length normalization to have an effect. dent compositions and consists of documents, each containing between . and words. We call this set the MITRE corpus (Hirschman et al., 1999). We would liked to have run evaluations on longer documents. Unfortunately, the forests generated even for relatively small documents are huge. Because there are an exponential number of summaries that can be generated for any given text4 , the decoder runs out of memory for longer documents; therefore, we selected shorter subtexts from the original documents. We used both the WSJ and Mitre data for evaluation because we wanted to see whether the performance of our system varies with text genre. The Mitre data consists mostly of short sentences (average document length from Mitre is sentences</context>
</contexts>
<marker>Hirschman, Light, Breck, Burger, 1999</marker>
<rawString>L. Hirschman, M. Light, E. Breck, and J. Burger. 1999. Deep read: A reading comprehension system. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics NAACL-2000,</booktitle>
<pages>310315</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="1738" citStr="Jing, 2000" startWordPosition="259" endWordPosition="260"> and present to the user the most important sentences in a text see (Mani and Maybury, 1999; Marcu, 2000; Mani, 2001) for comprehensive overviews of the methods and algorithms used to accomplish this. Headline generators are noisy-channel probabilistic systems that are trained on large corpora of \x02Headline, Text\x03 pairs (Banko et al., 2000; Berger and Mittal, 2000). These systems produce short sequences of words that are indicative of the content of the text given as input. Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases. Extraction-based summarizers often produce outputs that contain non-important sentence fragments. For example, the hypothetical extractive summary of Text (1), which is shown in Table 1, can be compacted further by deleting the clause which is already almost enough to win. Headline-based summaries, such as that shown in Table 1, are usually indicative of a texts content but not informative, grammatical, or coherent. By repeatedly applying a sentence-simplification algorithm one senten</context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>H. Jing. 2000. Sentence reduction for automatic text summarization. In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics NAACL-2000, pages 310315, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistics-based summarization step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In The 17th National Conference on Artificial Intelligence (AAAI2000),</booktitle>
<pages>703710</pages>
<location>Austin, TX,</location>
<contexts>
<context position="1763" citStr="Knight and Marcu, 2000" startWordPosition="261" endWordPosition="264"> to the user the most important sentences in a text see (Mani and Maybury, 1999; Marcu, 2000; Mani, 2001) for comprehensive overviews of the methods and algorithms used to accomplish this. Headline generators are noisy-channel probabilistic systems that are trained on large corpora of \x02Headline, Text\x03 pairs (Banko et al., 2000; Berger and Mittal, 2000). These systems produce short sequences of words that are indicative of the content of the text given as input. Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases. Extraction-based summarizers often produce outputs that contain non-important sentence fragments. For example, the hypothetical extractive summary of Text (1), which is shown in Table 1, can be compacted further by deleting the clause which is already almost enough to win. Headline-based summaries, such as that shown in Table 1, are usually indicative of a texts content but not informative, grammatical, or coherent. By repeatedly applying a sentence-simplification algorithm one sentence at a time, one can com</context>
<context position="12755" citStr="Knight and Marcu (2000)" startWordPosition="1981" endWordPosition="1984">rner, he is still on shaky ground. SPLJHTIQ HIGJUVIWPIGXFQ HIGJLFQRXGXFQ SPLJHTIQ SPLJYFQGOIZG SPLJYFQGOIZG SPLJHTIQ * * Figure 1: The discourse (full)/syntax (partial) tree for Text (1). expand operation could insert a syntactic constituent, such as this year anywhere in the syntactic tree of \x1b . A constituent-expand operation could also add single words: for instance the word now could be added between is and looking, yielding \x05[\x06 The mayor is now looking for re-election. The probability of inserting this word is based on the syntactic structure of the node into which its inserted. Knight and Marcu (2000) describe in detail a noisy-channel model that explains how short sentences can be expanded into longer ones by inserting and expanding syntactic constituents (and words). Since our constituent-expand stochastic operation simply reimplements Knight and Marcus model, we do not focus on them here. We refer the reader to (Knight and Marcu, 2000) for the details. In addition to adding syntactic constituents, our system is also able to add discourse units. Consider the summary \x1b\\\x06 John Doe has already secured the vote of most democrats in his consituency. Through a sequence of discourse expa</context>
<context position="5793" citStr="Knight &amp; Marcu (2000)" startWordPosition="884" endWordPosition="887">he document compression task is conceptually simple. Given a document \x05\x07\x06\x08\x02 \t\x0c\x0b \t\x0f\x0e\x11\x10\x12\x10\x13\x10\x14\t\x16\x15\x17\x03 , our goal is to produce a new document \x05\x19\x18 by dropping words \t\x16\x1a from \x05 . In order to achieve this goal, we 1 A number of other systems use the outputs of extractive summarizers and repair them to improve coherence (DUC, 2001; DUC, 2002). Unfortunately, none of these seems flexible enough to produce in one shot good summaries that are simultaneously coherent and grammatical. extent the noisy-channel model proposed by Knight &amp; Marcu (2000). Their system compressed sentences by dropping syntactic constituents, but could be applied to entire documents only on a sentenceby-sentence basis. As discussed in Section 1, this is not adequate because the resulting summary may contain many compressed sentences that are irrelevant. In order to extend Knight &amp; Marcus approach beyond the sentence level, we need to glue sentences together in a tree structure similar to that used at the sentence level. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) provides us this glue. The tree in Figure 1 depicts the RST structure of Text (1). </context>
<context position="8763" citStr="Knight &amp; Marcu (2000)" startWordPosition="1366" endWordPosition="1369">ith a summary \x1b and add noise to it, yielding a longer document \x05 . The noise added in our model consists of words, phrases and discourse units. For instance, given the document John Doe has secured the vote of most democrats. we could add words to it (namely the word already) to generate John Doe has already secured the vote of most democrats. We could also choose to add an entire syntactic constituent, for instance a prepositional phrase, to generate John Doe has secured the vote of most democrats in his constituency. These are both examples of sentence expansion as used previously by Knight &amp; Marcu (2000). Our system, however, also has the ability to expand on a core message by adding discourse constituents. For instance, it could decide to add another discourse constituent to the original summary John Doe has secured the vote of most democrats by CONTRASTing the information in the summary with the uncertainty regarding the support of the governor, thus yielding the text: John Doe has secured the vote of most democrats. But without the support of the governor, he is still on shaky ground. As in any noisy-channel application, there are three parts that we have to account for if we are to build </context>
<context position="18680" citStr="Knight &amp; Marcu (2000)" startWordPosition="2901" endWordPosition="2904">y dropping the first discourse constituent. Finally, we can compress the ROOT deriving into SAT=BACKGROUND NUC=SPAN by dropping the SAT=BACKGROUND constituent. We keep counts of each of these examples and, once collected, we normalize them to get the discourse expansion probabilities. 3.3 Decoder The goal of the decoder is to combine \x1c\x19\x1d)\x1b&amp;&amp;quot; with \x1c\x19\x1d\x1e\x05$\x1f%\x1b&amp;&amp;quot; to get \x1c\x19\x1d\x1e\x1b\x0c\x1f%\x05,&amp;quot; . There are a vast number of potential compressions of a large DS-tree, but we can efficiently pack them into a shared-forest structure, as described in detail by Knight &amp; Marcu (2000). Each entry in the shared-forest structure has three associated probabilities, one from the source syntax PCFG, one from the source discourse PCFG and one from the expansion-template probabilities described in Section 3.2. Once we have generated a forest representing all possible compressions of the original document, we want to extract the best (or the -best) trees, taking into account both the expansion probabilities of the channel model and the bigram and syntax and discourse PCFG probabilities of the source model. Thankfully, such a generic extractor has already been built (Langkilde, 200</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statistics-based summarization step one: Sentence compression. In The 17th National Conference on Artificial Intelligence (AAAI2000), pages 703710, Austin, TX, July 30th August 3rd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
</authors>
<title>Forest-based statistical sentence generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<volume>30</volume>
<location>Seattle, Washington,</location>
<contexts>
<context position="19282" citStr="Langkilde, 2000" startWordPosition="2996" endWordPosition="2997">&amp; Marcu (2000). Each entry in the shared-forest structure has three associated probabilities, one from the source syntax PCFG, one from the source discourse PCFG and one from the expansion-template probabilities described in Section 3.2. Once we have generated a forest representing all possible compressions of the original document, we want to extract the best (or the -best) trees, taking into account both the expansion probabilities of the channel model and the bigram and syntax and discourse PCFG probabilities of the source model. Thankfully, such a generic extractor has already been built (Langkilde, 2000). For our purposes, the extractor selects the trees with the best combination of LM and expansion scores after performing an exhaustive search over all possible summaries. It returns a list of such trees, one for each possible length. 4 System The system developed works in a pipelined fashion as shown in Figure 3. The first step along the pipeline is to generate the discourse structure. To do this, we use the decision-based discourse parser described by Marcu (2000)2. Once we have the discourse structure, we send each EDU off to a syn2 The discourse parser achieves an f-score of \x12 for EDU i</context>
<context position="20514" citStr="Langkilde, 2000" startWordPosition="3200" endWordPosition="3201">x12 for identifying hierarchical spans, \x12 for nuclearity identification and \x12 for relation tagging. \x0cParser Discourse Syntax Parser Forest Generator Decoder Chooser Length Output Summary Input Document Figure 3: The pipeline of system components. tactic parser (Collins, 1997). The syntax trees of the EDUs are then merged with the discourse tree in the forest generator to create a DS-tree similar to that shown in Figure 1. From this DS-tree we generate a forest that subsumes all possible compressions. This forest is then passed on to the forest ranking system which is used as decoder (Langkilde, 2000). The decoder gives us a list of possible compressions, for each possible length. Example compressions of Text (1) are shown in Figure 4 together with their respective log-probabilities. In order to choose the best compression at any possible length, we cannot rely only on the log-probabilities, lest the system always choose the shortest possible compression. In order to compensate for this, we normalize by length. However, in practice, simply dividing the log-probability by the length of the compression is insufficient for longer documents. Experimentally, we found a reasonable metric was to,</context>
</contexts>
<marker>Langkilde, 2000</marker>
<rawString>Irene Langkilde. 2000. Forest-based statistical sentence generation. In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics, Seattle, Washington, April 30May 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kavi Mahesh</author>
</authors>
<title>Hypertext summary extraction for fast document browsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Natural Language Processing for the World Wide Web,</booktitle>
<pages>95103</pages>
<contexts>
<context position="1684" citStr="Mahesh, 1997" startWordPosition="251" endWordPosition="252">wing three classes: Extractive summarizers simply select and present to the user the most important sentences in a text see (Mani and Maybury, 1999; Marcu, 2000; Mani, 2001) for comprehensive overviews of the methods and algorithms used to accomplish this. Headline generators are noisy-channel probabilistic systems that are trained on large corpora of \x02Headline, Text\x03 pairs (Banko et al., 2000; Berger and Mittal, 2000). These systems produce short sequences of words that are indicative of the content of the text given as input. Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases. Extraction-based summarizers often produce outputs that contain non-important sentence fragments. For example, the hypothetical extractive summary of Text (1), which is shown in Table 1, can be compacted further by deleting the clause which is already almost enough to win. Headline-based summaries, such as that shown in Table 1, are usually indicative of a texts content but not informative, grammatical, or coherent. By repeatedly a</context>
</contexts>
<marker>Mahesh, 1997</marker>
<rawString>Kavi Mahesh. 1997. Hypertext summary extraction for fast document browsing. In Proceedings of the AAAI Spring Symposium on Natural Language Processing for the World Wide Web, pages 95103.</rawString>
</citation>
<citation valid="true">
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<editor>Inderjeet Mani and Mark Maybury, editors.</editor>
<publisher>The MIT Press.</publisher>
<marker>1999</marker>
<rawString>Inderjeet Mani and Mark Maybury, editors. 1999. Advances in Automatic Text Summarization. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Automatic summarization.</title>
<date>2001</date>
<contexts>
<context position="1245" citStr="Mani, 2001" startWordPosition="184" endWordPosition="185">ents so as to generate coherent, grammatical document compressions of arbitrary length. The system outperforms both a baseline and a sentence-based compression system that operates by simplifying sequentially all sentences in a text. Our results support the claim that discourse knowledge plays an important role in document summarization. 1 Introduction Single document summarization systems proposed to date fall within one of the following three classes: Extractive summarizers simply select and present to the user the most important sentences in a text see (Mani and Maybury, 1999; Marcu, 2000; Mani, 2001) for comprehensive overviews of the methods and algorithms used to accomplish this. Headline generators are noisy-channel probabilistic systems that are trained on large corpora of \x02Headline, Text\x03 pairs (Banko et al., 2000; Berger and Mittal, 2000). These systems produce short sequences of words that are indicative of the content of the text given as input. Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phras</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Inderjeet Mani. 2001. Automatic summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243281.</tech>
<contexts>
<context position="6309" citStr="Mann and Thompson, 1988" startWordPosition="967" endWordPosition="970">re simultaneously coherent and grammatical. extent the noisy-channel model proposed by Knight &amp; Marcu (2000). Their system compressed sentences by dropping syntactic constituents, but could be applied to entire documents only on a sentenceby-sentence basis. As discussed in Section 1, this is not adequate because the resulting summary may contain many compressed sentences that are irrelevant. In order to extend Knight &amp; Marcus approach beyond the sentence level, we need to glue sentences together in a tree structure similar to that used at the sentence level. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) provides us this glue. The tree in Figure 1 depicts the RST structure of Text (1). In RST, discourse structures are nonbinary trees whose leaves correspond to elementary discourse units (EDUs), and whose internal nodes correspond to contiguous text spans. Each internal node in an RST tree is characterized by a rhetorical relation. For example, the first sentence in Text (1) provides BACKGROUND information for interpreting the information in sentences 2 and 3, which are in a CONTRAST relation (see Figure 1). Each relation holds between two adjacent non-overlapping text spans called NUCLEUS and</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts. \x0c&apos;</location>
<contexts>
<context position="1232" citStr="Marcu, 2000" startWordPosition="182" endWordPosition="183">urse constituents so as to generate coherent, grammatical document compressions of arbitrary length. The system outperforms both a baseline and a sentence-based compression system that operates by simplifying sequentially all sentences in a text. Our results support the claim that discourse knowledge plays an important role in document summarization. 1 Introduction Single document summarization systems proposed to date fall within one of the following three classes: Extractive summarizers simply select and present to the user the most important sentences in a text see (Mani and Maybury, 1999; Marcu, 2000; Mani, 2001) for comprehensive overviews of the methods and algorithms used to accomplish this. Headline generators are noisy-channel probabilistic systems that are trained on large corpora of \x02Headline, Text\x03 pairs (Banko et al., 2000; Berger and Mittal, 2000). These systems produce short sequences of words that are indicative of the content of the text given as input. Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant wo</context>
<context position="5793" citStr="Marcu (2000)" startWordPosition="886" endWordPosition="887">nt compression task is conceptually simple. Given a document \x05\x07\x06\x08\x02 \t\x0c\x0b \t\x0f\x0e\x11\x10\x12\x10\x13\x10\x14\t\x16\x15\x17\x03 , our goal is to produce a new document \x05\x19\x18 by dropping words \t\x16\x1a from \x05 . In order to achieve this goal, we 1 A number of other systems use the outputs of extractive summarizers and repair them to improve coherence (DUC, 2001; DUC, 2002). Unfortunately, none of these seems flexible enough to produce in one shot good summaries that are simultaneously coherent and grammatical. extent the noisy-channel model proposed by Knight &amp; Marcu (2000). Their system compressed sentences by dropping syntactic constituents, but could be applied to entire documents only on a sentenceby-sentence basis. As discussed in Section 1, this is not adequate because the resulting summary may contain many compressed sentences that are irrelevant. In order to extend Knight &amp; Marcus approach beyond the sentence level, we need to glue sentences together in a tree structure similar to that used at the sentence level. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) provides us this glue. The tree in Figure 1 depicts the RST structure of Text (1). </context>
<context position="8763" citStr="Marcu (2000)" startWordPosition="1368" endWordPosition="1369">mary \x1b and add noise to it, yielding a longer document \x05 . The noise added in our model consists of words, phrases and discourse units. For instance, given the document John Doe has secured the vote of most democrats. we could add words to it (namely the word already) to generate John Doe has already secured the vote of most democrats. We could also choose to add an entire syntactic constituent, for instance a prepositional phrase, to generate John Doe has secured the vote of most democrats in his constituency. These are both examples of sentence expansion as used previously by Knight &amp; Marcu (2000). Our system, however, also has the ability to expand on a core message by adding discourse constituents. For instance, it could decide to add another discourse constituent to the original summary John Doe has secured the vote of most democrats by CONTRASTing the information in the summary with the uncertainty regarding the support of the governor, thus yielding the text: John Doe has secured the vote of most democrats. But without the support of the governor, he is still on shaky ground. As in any noisy-channel application, there are three parts that we have to account for if we are to build </context>
<context position="12755" citStr="Marcu (2000)" startWordPosition="1983" endWordPosition="1984"> still on shaky ground. SPLJHTIQ HIGJUVIWPIGXFQ HIGJLFQRXGXFQ SPLJHTIQ SPLJYFQGOIZG SPLJYFQGOIZG SPLJHTIQ * * Figure 1: The discourse (full)/syntax (partial) tree for Text (1). expand operation could insert a syntactic constituent, such as this year anywhere in the syntactic tree of \x1b . A constituent-expand operation could also add single words: for instance the word now could be added between is and looking, yielding \x05[\x06 The mayor is now looking for re-election. The probability of inserting this word is based on the syntactic structure of the node into which its inserted. Knight and Marcu (2000) describe in detail a noisy-channel model that explains how short sentences can be expanded into longer ones by inserting and expanding syntactic constituents (and words). Since our constituent-expand stochastic operation simply reimplements Knight and Marcus model, we do not focus on them here. We refer the reader to (Knight and Marcu, 2000) for the details. In addition to adding syntactic constituents, our system is also able to add discourse units. Consider the summary \x1b\\\x06 John Doe has already secured the vote of most democrats in his consituency. Through a sequence of discourse expa</context>
<context position="18680" citStr="Marcu (2000)" startWordPosition="2903" endWordPosition="2904">g the first discourse constituent. Finally, we can compress the ROOT deriving into SAT=BACKGROUND NUC=SPAN by dropping the SAT=BACKGROUND constituent. We keep counts of each of these examples and, once collected, we normalize them to get the discourse expansion probabilities. 3.3 Decoder The goal of the decoder is to combine \x1c\x19\x1d)\x1b&amp;&amp;quot; with \x1c\x19\x1d\x1e\x05$\x1f%\x1b&amp;&amp;quot; to get \x1c\x19\x1d\x1e\x1b\x0c\x1f%\x05,&amp;quot; . There are a vast number of potential compressions of a large DS-tree, but we can efficiently pack them into a shared-forest structure, as described in detail by Knight &amp; Marcu (2000). Each entry in the shared-forest structure has three associated probabilities, one from the source syntax PCFG, one from the source discourse PCFG and one from the expansion-template probabilities described in Section 3.2. Once we have generated a forest representing all possible compressions of the original document, we want to extract the best (or the -best) trees, taking into account both the expansion probabilities of the channel model and the bigram and syntax and discourse PCFG probabilities of the source model. Thankfully, such a generic extractor has already been built (Langkilde, 200</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. The MIT Press, Cambridge, Massachusetts. \x0c&apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>