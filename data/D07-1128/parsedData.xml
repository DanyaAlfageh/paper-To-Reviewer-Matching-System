<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<figure confidence="0.234529875">
b&amp;apos;Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 11611165,
Prague, June 2007. c
2007 Association for Computational Linguistics
Pro3Gres Parser in the CoNLL Domain Adaptation Shared Task
Gerold Schneider and Kaarel Kaljurand and Fabio Rinaldi and Tobias Kuhn
Institute of Computational Linguistics, University of Zurich
Binzmuhlestrasse 14
CH - 8050 Zurich, Switzerland
</figure>
<email confidence="0.900485">
{gschneid,kalju,rinaldi,tkuhn}@ifi.uzh.ch
</email>
<sectionHeader confidence="0.978064" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993636">
We present Pro3Gres, a deep-syntactic, fast
dependency parser that combines a hand-
written competence grammar with proba-
bilistic performance disambiguation and that
has been used in the biomedical domain. We
discuss its performance in the domain adap-
tation open submission. We achieve aver-
age results, which is partly due to difficulties
in mapping to the dependency representation
used for the shared task.
</bodyText>
<sectionHeader confidence="0.998331" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996142083333334">
The Pro3Gres parser is a dependency parser that
combines a hand-written grammar with probabilis-
tic disambiguation. It is described in detail in
(Schneider, 2007). It uses tagger and chunker
pre-processors parsing proper happens only be-
tween heads of chunks and a post-processor graph
converter to capture long-distance dependencies.
Pro3Gres is embedded in a flexible XML pipeline.
It has been applied to many tasks, such as parsing
biomedical literature (Rinaldi et al., 2006; Rinaldi
et al., 2007) and the whole British National Cor-
pus, and has been evaluated in several ways. We
have achieved average results in the CoNLL do-
main adaptation track open submission (Marcus et
al., 1993; Johansson and Nugues, 2007; Kulick et
al., 2004; MacWhinney, 2000; Brown, 1973). The
performance of the parser is seriously affected by
mapping problems to the particular dependency rep-
resentation used in the shared task.
The paper is structured as follows. We give a brief
overview of the parser and its design policy in sec-
tion 2, we describe the domain adaptations that we
have used in section 3, comment on the results ob-
tained in section 4 and conclude in section 5.
</bodyText>
<sectionHeader confidence="0.740054" genericHeader="introduction">
2 Pro3Gres and its Design Policy
</sectionHeader>
<bodyText confidence="0.999556482758621">
There has been growing interest in exploring the
space between Treebank-trained probabilistic gram-
mars (e.g. (Collins, 1999; Nivre, 2006)) and formal
grammar-based parsers integrating statistics (e.g.
(Miyao et al., 2005; Riezler et al., 2002)). We
have developed a parsing system that explores this
space, in the vein of systems like (Kaplan et al.,
2004), using a linguistic competence grammar and
a probabilistic performance disambiguation allow-
ing us to explore interactions between lexicon and
grammar (Sinclair, 1996). The parser has been ex-
plicitly designed to be deep-syntactic like a formal
grammar-based parser, by using a dependency rep-
resentation that is close to LFG f-structure, but at
the same time mostly context-free and integrating
shallow approaches and aggressive pruning in or-
der to keep search-spaces small, without permitting
compromise on performance or linguistic adequacy.
(Abney, 1995) establishes the chunks and dependen-
cies model as a well-motivated linguistic theory. The
non-local linguistic constraints that a hand-written
grammar allows us to formulate, e.g. expressing
X-bar principles or barring very marked construc-
tions, further reduce parsing time by at least an order
of magnitude. Since the grammar is on Penn tags
(except for few closed classed words, e.g. allow-
ing including to function as preposition) the effort
for writing it manually is manageable. It has been
developed from scratch in about a person month,
</bodyText>
<page confidence="0.995646">
1161
</page>
<figureCaption confidence="0.943484">
\x0cFigure 1: Pro3Gres parser flowchart
</figureCaption>
<bodyText confidence="0.919757615384615">
using traditional grammar engineering development
cycles. It contains about 1000 rules, the number is
largely so high due to tag combinatorics: for ex-
ample, the various subject attachment rules combin-
ing a subject ( NN, NNS, NNP, NNPS) and a verb
( VBZ, VBP, VBG, VBN, VBD) are all very simi-
lar.
The parser is fast enough for large-scale appli-
cation to unrestricted texts, and it delivers depen-
dency relations which are a suitable base for a
range of applications. We have used it to parse the
entire 100 million words British National Corpus
(http://www.natcorp.ox.ac.uk) and similar amounts
of biomedical texts. Its parsing speed is about
500,000 words per hour. The flowchart of the parser
can be seen in figure 1.
Pro3Gres (PRObabilistic PROlog-implemented
RObust Grammatical Role Extraction System) uses
a dependency representation that is close to LFG
f-structure, in order to give it an established lin-
guistic background. It uses post-processing graph
structure conversions and mild context-sensitivity to
capture long-distance dependencies. We have ar-
gued in (Schneider, 2005) that LFG f-structures can
be parsed for in a completely context-free fashion,
except for embedded WH-questions, where a de-
vice such as functional uncertainty (Kaplan and Za-
enen, 1989) or the equivalent Tree-Adjoining Gram-
mar Adjoining operation (Joshi and Vijay-Shanker,
1989) is used. In Dependency Grammar, this device
is also known as lifting (Kahane et al., 1998; Nivre
and Nilsson, 2005).
We use a hand-written competence grammar,
combined with performance-driven disambiguation
obtained from the Penn Treebank (Marcus et
al., 1993). The Maximum-Likelihood Estimation
(MLE) probability of generating a dependency re-
lation R given lexical heads (a and b) at distance (in
chunks) is calculated as follows.
</bodyText>
<equation confidence="0.983339857142857">
p(R, |a, b)
= p(R|a, b) p(|R) =
#(R, a, b)
Pn
i=1 #(Ri, a, b)
#(R, )
#R
</equation>
<bodyText confidence="0.999691166666667">
The counts are backed off (Collins, 1999; Merlo
and Esteve Ferrer, 2006). The backoff levels include
semantic classes from WordNet (Fellbaum, 1998):
we back off to the lexicographer file ID of the most
frequent word sense. An example output of the
parser is shown in figure 2.
</bodyText>
<sectionHeader confidence="0.991437" genericHeader="method">
3 Domain Adaptation
</sectionHeader>
<bodyText confidence="0.995788147058824">
Based on our experience with parsing texts form the
biomedical domain, we have used the following two
adaptations to the domain of chemistry.
(Hindle and Rooth, 1993) exploit the fact that in
sentence-initial NP PP sequences the PP unambigu-
ously attaches to the noun. We have observed that in
sentence-initial NP PP PP sequences, also the sec-
ond PP frequently attaches to the noun, the noun
itself often being a relational noun. We have thus
used such sequences to learn relational nouns from
the unlabelled domain texts. Relational nouns are
allowed to attach several argument PPs in the gram-
mar, all other nouns are not.
Multi-word terms, adjective-preposition construc-
tions and frequent PP-arguments have strong collo-
cational force. We have thus used the collocation
extraction tool XTRACT (Smadja, 2003) to discover
collocations from large domain corpora. The prob-
ability of generating a dependency relation is aug-
mented for collocations above a certain threshold.
Since the tagging quality of the Chemistry testset
is high, the impact of multi-word term recognition
was lower than the biomedical domain when using a
standard tagger, as we have shown in (Rinaldi et al.,
2007).
For the CHILDES domain, we have not used any
adaptation. The hand-written grammar fares quite
well on most types of questions, which are very fre-
quent in this domain. In the spirit of the shared
task, we have not attempted to correct tagging errors,
which were frequent in the CHILDES domain. We
have restricted the use of external resources to the
hand-written, domain-independent grammar, and to
WordNet. Due to serious problems in mapping our
</bodyText>
<page confidence="0.975853">
1162
</page>
<figureCaption confidence="0.835746">
\x0cFigure 2: Example of original parser output
</figureCaption>
<bodyText confidence="0.964363333333333">
LFG f-structure based dependencies to the CoNLL
representation, much less time than expected was
available for the domain adaptation.
</bodyText>
<sectionHeader confidence="0.987595" genericHeader="method">
4 Our Results
</sectionHeader>
<bodyText confidence="0.998854545454546">
We have achieved average results: Labeled attach-
ment score: 3151 / 5001 * 100 = 63.01, unlabeled at-
tachment score: 3327 / 5001 * 100 = 66.53, label ac-
curacy score: 3832 / 5001 * 100 = 76.62. These re-
sults are about 10 % below what we typically obtain
when using our own dependency representation or
GREVAL (Carroll et al., 2003), a deep-syntactic an-
notation scheme that is close to ours. Detailed eval-
uations are reported in (Schneider, 2007). Our map-
ping was quite poor, especially when conjunctions
are involved. Also punctuation is attached poorly.
</bodyText>
<subsectionHeader confidence="0.61227">
5.7 % of all dependencies remained unmapped (un-
</subsectionHeader>
<bodyText confidence="0.976628636363636">
known in the figure). We give an overview of the the
relation-dependent results in figures 1 and 2.
Mapping problems include the following exam-
ples. First, headedness is handled very differently:
while we assume auxiliaries, prepositions and co-
ordinations to be dependents, the CoNNL repre-
sentation assumes the opposite, which leads to in-
correct mapping under complex interactions. Sec-
ond, the semantics of parentheticals (PRN) partly
remains unclear. In Quinidine elimination was
capacity limited with apparent Michaelis constant
(appKM) of 2.6 microM (about 1.2 mg/L) the gold
standard annotates the second parenthesis as paren-
thetical, but the first as nominal modification, al-
though both may be said to have appositional char-
acter. Third, we seem to have misinterpreted the
roles of ADV and AMOD, as they are often mutu-
ally exchanged. Fourth, the logical subject (LGS)
is sometimes marked on the by-PP (... are strongly
inhibited by-LGS carbon monoxide) and sometimes
on the participle (... are increased-LGS by pre-
deprel gold correct system recall (%) prec. (%)
</bodyText>
<table confidence="0.999510631578947">
ADV 366 212 302 57.92 70.20
AMOD 87 8 87 9.20 9.20
CC 11 0 0 0.00 NaN
COORD 402 233 342 57.96 68.13
DEP 9 0 0 0.00 NaN
EXP 2 0 0 0.00 NaN
GAP 14 0 0 0.00 NaN
IOBJ 3 0 0 0.00 NaN
LGS 37 0 0 0.00 NaN
NMOD 1813 1576 1763 86.93 89.39
OBJ 185 146 208 78.92 70.19
P 587 524 525 89.27 99.81
PMOD 681 533 648 78.27 82.25
PRN 34 13 68 38.24 19.12
ROOT 195 138 190 70.77 72.63
SBJ 279 217 296 77.78 73.31
VC 129 116 136 89.92 85.29
VMOD 167 116 149 69.46 77.85
unknown 0 0 287 NaN 0.00
</table>
<tableCaption confidence="0.997113">
Table 1: Prec.&amp;recall of DEPREL
</tableCaption>
<bodyText confidence="0.986677045454545">
treatment) in the gold standard. Relations between
heads of chunks, which are central for predicate-
argument structures which Pro3Gres aims to re-
cover, such as SBJ, NMOD, ROOT, perform better
than those for which Pro3Gres was not originally
designed, particularly ADV, AMOD, PRN, P. Perfor-
mance on COORD was particularly disappointing.
Generally, mapping problems between different rep-
resentations would be smaller if one used a depen-
dency representation that maximally abstracts away
from form to function, for example (Carroll et al.,
2003).
We have obtained results slightly above average
on the CHILDES domain, although we did not adapt
the parser to this domain in any way (unlabeled at-
tachment score: 3013 / 4999 * 100 = 60.27 %).
The hand-written grammar, which includes rules for
most types of questions, fares relatively well on this
domain since questions are rare in the Penn Tree-
bank (see (Hermjakob, 2001)). Pro3Gres has been
employed for question parsing at a TREC confer-
ence (Burger and Bayer, 2005).
</bodyText>
<page confidence="0.868221">
1163
</page>
<table confidence="0.99747215">
\x0cdeprel gold correct system recall (%) prec. (%)
ADV 366 161 302 43.99 53.31
AMOD 87 5 87 5.75 5.75
CC 11 0 0 0.00 NaN
COORD 402 170 342 42.29 49.71
DEP 9 0 0 0.00 NaN
EXP 2 0 0 0.00 NaN
GAP 14 0 0 0.00 NaN
IOBJ 3 0 0 0.00 NaN
LGS 37 0 0 0.00 NaN
NMOD 1813 1392 1763 76.78 78.96
OBJ 185 140 208 75.68 67.31
P 587 221 525 37.65 42.10
PMOD 681 521 648 76.51 80.40
PRN 34 12 68 35.29 17.65
ROOT 195 138 190 70.77 72.63
SBJ 279 190 296 68.10 64.19
VC 129 116 136 89.92 85.29
VMOD 167 85 149 50.90 57.05
unknown 0 0 287 NaN 0.00
</table>
<tableCaption confidence="0.992502">
Table 2: Prec.&amp;recall of DEPREL+ATTACHMENT
</tableCaption>
<sectionHeader confidence="0.992091" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999304909090909">
We have described the Pro3Gres parser. We have
achieved average results in the shared task with rel-
atively little adaptation. Mapping to different repre-
sentations is an often underestimated task. Our per-
formance on the CHILDES task, where we did not
adapt the parser, indicates that hand-written, care-
fully engineered competence grammars may be rel-
atively domain-independent while performance dis-
ambiguation is more domain-dependent. We will
adapt the parser to further domains and include more
unsupervised learning methods.
</bodyText>
<sectionHeader confidence="0.802556" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.848233259259259">
Steven Abney. 1995. Chunks and dependencies: Bring-
ing processing evidence to bear on syntax. In Jennifer
Cole, Georgia Green, and Jerry Morgan, editors, Com-
putational Linguistics and the Foundations of Linguis-
tic Theory, pages 145164. CSLI.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
John D. Burger and Sam Bayer. 2005. MITREs Qanda
at TREC-14. In E. M. Voorhees and Lori P. Buck-
land, editors, The Fourteenth Text REtrieval Confer-
ence (TREC 2005) Notebook.
John Carroll, Guido Minnen, and Edward Briscoe. 2003.
Parser evaluation: using a grammatical relation anno-
tation scheme. In Anne Abeille, editor, Treebanks:
Building and Using Parsed Corpora, pages 299316.
Kluwer, Dordrecht.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, PA.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Ulf Hermjakob. 2001. Parsing and question classifica-
tion for question answering. In Proceedings of the
ACL 2001 Workshop on Open-Domain Question An-
swering, Toulouse, France.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
</bodyText>
<reference confidence="0.943852463414634">
19:103120.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Aravind K. Joshi and K. Vijay-Shanker. 1989. Treat-
ment of long-distance dependencies in LFG and TAG:
Functional uncertainty in LFG is a corollary in TAG.
In Proceedings of ACL 89.
Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In Proceedings of
COLINGACL, volume 1, pages 646652, Montreal.
Ronald Kaplan and Annie Zaenen. 1989. Long-distance
dependencies, constituent structure, and functional un-
certainty. In Mark Baltin and Anthony Kroch, editors,
Alternative Concepts of Phrase Structrue, pages 17
42. Chicago University Press.
Ron Kaplan, Stefan Riezler, Tracy H. King, John
T. Maxwell III, Alex Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Proceedings of HLT/NAACL
2004, Boston, MA.
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313330.
Paola Merlo and Eva Esteve Ferrer. 2006. The notion of
argument in PP attachment. Computational Linguis-
tics, 32(2):341 378.
Yusuke Miyao, Takashi Ninomiya, and Junichi Tsujii.
2005. Corpus-oriented grammar development for ac-
quiring a Head-driven Phrase Structure Grammar from
</reference>
<page confidence="0.720934">
1164
</page>
<reference confidence="0.999379125">
\x0cthe Penn Treebank. In Keh-Yih Su, Junichi Tsujii,
Jong-Hyeok Lee, and Oi Yee Kwong, editors, Natural
Language Processing - IJCNLP 2004, pages 684693.
Springer.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL05), pages 99106, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Joakim Nivre. 2006. Constraints on non-projective de-
pendency parsing. In Proceedings of the European
Chapter of the Association of Computational Linguis-
tics (EACL) 2006, pages 73 80, Trento, Italy. Asso-
ciation for Computational Linguistics.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL02), Philadephia, PA.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand,
Michael Hess, and Martin Romacker. 2006. . an en-
vironment for relation mining over richly annotated
corpora: the case of GENIA. BMC Bioinformatics,
7(Suppl 3):S3.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand,
Michael Hess, Christos Andronis, Ourania Konstanti,
and Andreas Persidis. 2007. Mining of functional
relations between genes and proteins over biomedical
scientific literature using a deep-linguistic approach.
Journal of Artificial Intelligence in Medicine, 39:127
136.
Gerold Schneider. 2005. A broad-coverage, representa-
tionally minimal LFG parser: chunks and F-structures
are sufficient. In Mriram Butt and Traci Holloway
King, editors, The 10th international LFG Conference
(LFG 2005), Bergen, Norway. CSLI.
Gerold Schneider. 2007. Hybrid Long-Distance Func-
tional Dependency Parsing. Doctoral Thesis, Institute
of Computational Linguistics, University of Zurich.
accepted for publication.
John Sinclair. 1996. The empty lexicon. International
Journal of Corpus Linguistics, 1, 1996.
Frank Smadja. 2003. Retrieving collocations from text:
Xtract. Computational Linguistics, 19:1, Special issue
on using large corpora:143177.
</reference>
<page confidence="0.700875">
1165
</page>
<figure confidence="0.334681">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.575221">
<note confidence="0.930782333333333">b&amp;apos;Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 11611165, Prague, June 2007. c 2007 Association for Computational Linguistics</note>
<title confidence="0.734703">Pro3Gres Parser in the CoNLL Domain Adaptation Shared Task</title>
<author confidence="0.990638">Gerold Schneider</author>
<author confidence="0.990638">Kaarel Kaljurand</author>
<author confidence="0.990638">Fabio Rinaldi</author>
<author confidence="0.990638">Tobias Kuhn</author>
<affiliation confidence="0.999851">Institute of Computational Linguistics, University of Zurich</affiliation>
<address confidence="0.976947">Binzmuhlestrasse 14 CH - 8050 Zurich, Switzerland</address>
<email confidence="0.979036">gschneid@ifi.uzh.ch</email>
<email confidence="0.979036">kalju@ifi.uzh.ch</email>
<email confidence="0.979036">rinaldi@ifi.uzh.ch</email>
<email confidence="0.979036">tkuhn@ifi.uzh.ch</email>
<abstract confidence="0.998504818181818">We present Pro3Gres, a deep-syntactic, fast dependency parser that combines a handwritten competence grammar with probabilistic performance disambiguation and that has been used in the biomedical domain. We discuss its performance in the domain adaptation open submission. We achieve average results, which is partly due to difficulties in mapping to the dependency representation used for the shared task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proc. of the 16th Nordic Conference on Computational Linguistics (NODALIDA).</booktitle>
<contexts>
<context position="1578" citStr="Johansson and Nugues, 2007" startWordPosition="229" endWordPosition="232">obabilistic disambiguation. It is described in detail in (Schneider, 2007). It uses tagger and chunker pre-processors parsing proper happens only between heads of chunks and a post-processor graph converter to capture long-distance dependencies. Pro3Gres is embedded in a flexible XML pipeline. It has been applied to many tasks, such as parsing biomedical literature (Rinaldi et al., 2006; Rinaldi et al., 2007) and the whole British National Corpus, and has been evaluated in several ways. We have achieved average results in the CoNLL domain adaptation track open submission (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004; MacWhinney, 2000; Brown, 1973). The performance of the parser is seriously affected by mapping problems to the particular dependency representation used in the shared task. The paper is structured as follows. We give a brief overview of the parser and its design policy in section 2, we describe the domain adaptations that we have used in section 3, comment on the results obtained in section 4 and conclude in section 5. 2 Pro3Gres and its Design Policy There has been growing interest in exploring the space between Treebank-trained probabilistic grammars (e.g. (Collins, 19</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>19:103120. R. Johansson and P. Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proc. of the 16th Nordic Conference on Computational Linguistics (NODALIDA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Treatment of long-distance dependencies in LFG and TAG: Functional uncertainty in LFG is a corollary in TAG.</title>
<date>1989</date>
<booktitle>In Proceedings of ACL 89.</booktitle>
<contexts>
<context position="4918" citStr="Joshi and Vijay-Shanker, 1989" startWordPosition="748" endWordPosition="751">babilistic PROlog-implemented RObust Grammatical Role Extraction System) uses a dependency representation that is close to LFG f-structure, in order to give it an established linguistic background. It uses post-processing graph structure conversions and mild context-sensitivity to capture long-distance dependencies. We have argued in (Schneider, 2005) that LFG f-structures can be parsed for in a completely context-free fashion, except for embedded WH-questions, where a device such as functional uncertainty (Kaplan and Zaenen, 1989) or the equivalent Tree-Adjoining Grammar Adjoining operation (Joshi and Vijay-Shanker, 1989) is used. In Dependency Grammar, this device is also known as lifting (Kahane et al., 1998; Nivre and Nilsson, 2005). We use a hand-written competence grammar, combined with performance-driven disambiguation obtained from the Penn Treebank (Marcus et al., 1993). The Maximum-Likelihood Estimation (MLE) probability of generating a dependency relation R given lexical heads (a and b) at distance (in chunks) is calculated as follows. p(R, |a, b) = p(R|a, b) p(|R) = #(R, a, b) Pn i=1 #(Ri, a, b) #(R, ) #R The counts are backed off (Collins, 1999; Merlo and Esteve Ferrer, 2006). The backoff levels in</context>
</contexts>
<marker>Joshi, Vijay-Shanker, 1989</marker>
<rawString>Aravind K. Joshi and K. Vijay-Shanker. 1989. Treatment of long-distance dependencies in LFG and TAG: Functional uncertainty in LFG is a corollary in TAG. In Proceedings of ACL 89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Kahane</author>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
</authors>
<title>Pseudo-projectivity: A polynomially parsable nonprojective dependency grammar.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL,</booktitle>
<volume>1</volume>
<pages>646652</pages>
<location>Montreal.</location>
<contexts>
<context position="5008" citStr="Kahane et al., 1998" startWordPosition="764" endWordPosition="767">tion that is close to LFG f-structure, in order to give it an established linguistic background. It uses post-processing graph structure conversions and mild context-sensitivity to capture long-distance dependencies. We have argued in (Schneider, 2005) that LFG f-structures can be parsed for in a completely context-free fashion, except for embedded WH-questions, where a device such as functional uncertainty (Kaplan and Zaenen, 1989) or the equivalent Tree-Adjoining Grammar Adjoining operation (Joshi and Vijay-Shanker, 1989) is used. In Dependency Grammar, this device is also known as lifting (Kahane et al., 1998; Nivre and Nilsson, 2005). We use a hand-written competence grammar, combined with performance-driven disambiguation obtained from the Penn Treebank (Marcus et al., 1993). The Maximum-Likelihood Estimation (MLE) probability of generating a dependency relation R given lexical heads (a and b) at distance (in chunks) is calculated as follows. p(R, |a, b) = p(R|a, b) p(|R) = #(R, a, b) Pn i=1 #(Ri, a, b) #(R, ) #R The counts are backed off (Collins, 1999; Merlo and Esteve Ferrer, 2006). The backoff levels include semantic classes from WordNet (Fellbaum, 1998): we back off to the lexicographer fil</context>
</contexts>
<marker>Kahane, Nasr, Rambow, 1998</marker>
<rawString>Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998. Pseudo-projectivity: A polynomially parsable nonprojective dependency grammar. In Proceedings of COLINGACL, volume 1, pages 646652, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Kaplan</author>
<author>Annie Zaenen</author>
</authors>
<title>Long-distance dependencies, constituent structure, and functional uncertainty.</title>
<date>1989</date>
<booktitle>In Mark Baltin and Anthony Kroch, editors, Alternative Concepts of Phrase Structrue,</booktitle>
<pages>17--42</pages>
<publisher>Chicago University Press.</publisher>
<contexts>
<context position="4825" citStr="Kaplan and Zaenen, 1989" startWordPosition="735" endWordPosition="739">,000 words per hour. The flowchart of the parser can be seen in figure 1. Pro3Gres (PRObabilistic PROlog-implemented RObust Grammatical Role Extraction System) uses a dependency representation that is close to LFG f-structure, in order to give it an established linguistic background. It uses post-processing graph structure conversions and mild context-sensitivity to capture long-distance dependencies. We have argued in (Schneider, 2005) that LFG f-structures can be parsed for in a completely context-free fashion, except for embedded WH-questions, where a device such as functional uncertainty (Kaplan and Zaenen, 1989) or the equivalent Tree-Adjoining Grammar Adjoining operation (Joshi and Vijay-Shanker, 1989) is used. In Dependency Grammar, this device is also known as lifting (Kahane et al., 1998; Nivre and Nilsson, 2005). We use a hand-written competence grammar, combined with performance-driven disambiguation obtained from the Penn Treebank (Marcus et al., 1993). The Maximum-Likelihood Estimation (MLE) probability of generating a dependency relation R given lexical heads (a and b) at distance (in chunks) is calculated as follows. p(R, |a, b) = p(R|a, b) p(|R) = #(R, a, b) Pn i=1 #(Ri, a, b) #(R, ) #R Th</context>
</contexts>
<marker>Kaplan, Zaenen, 1989</marker>
<rawString>Ronald Kaplan and Annie Zaenen. 1989. Long-distance dependencies, constituent structure, and functional uncertainty. In Mark Baltin and Anthony Kroch, editors, Alternative Concepts of Phrase Structrue, pages 17 42. Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kaplan</author>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>John T Maxwell Alex Vasserman</author>
<author>Richard Crouch</author>
</authors>
<title>Speed and accuracy in shallow and deep stochastic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL 2004,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="2413" citStr="Kaplan et al., 2004" startWordPosition="367" endWordPosition="370">ured as follows. We give a brief overview of the parser and its design policy in section 2, we describe the domain adaptations that we have used in section 3, comment on the results obtained in section 4 and conclude in section 5. 2 Pro3Gres and its Design Policy There has been growing interest in exploring the space between Treebank-trained probabilistic grammars (e.g. (Collins, 1999; Nivre, 2006)) and formal grammar-based parsers integrating statistics (e.g. (Miyao et al., 2005; Riezler et al., 2002)). We have developed a parsing system that explores this space, in the vein of systems like (Kaplan et al., 2004), using a linguistic competence grammar and a probabilistic performance disambiguation allowing us to explore interactions between lexicon and grammar (Sinclair, 1996). The parser has been explicitly designed to be deep-syntactic like a formal grammar-based parser, by using a dependency representation that is close to LFG f-structure, but at the same time mostly context-free and integrating shallow approaches and aggressive pruning in order to keep search-spaces small, without permitting compromise on performance or linguistic adequacy. (Abney, 1995) establishes the chunks and dependencies mod</context>
</contexts>
<marker>Kaplan, Riezler, King, Vasserman, Crouch, 2004</marker>
<rawString>Ron Kaplan, Stefan Riezler, Tracy H. King, John T. Maxwell III, Alex Vasserman, and Richard Crouch. 2004. Speed and accuracy in shallow and deep stochastic parsing. In Proceedings of HLT/NAACL 2004, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kulick</author>
<author>A Bies</author>
<author>M Liberman</author>
<author>M Mandel</author>
<author>R McDonald</author>
<author>M Palmer</author>
<author>A Schein</author>
<author>L Ungar</author>
</authors>
<title>Integrated annotation for biomedical information extraction.</title>
<date>2004</date>
<booktitle>In Proc. of the Human Language Technology Conference and the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL).</booktitle>
<contexts>
<context position="1599" citStr="Kulick et al., 2004" startWordPosition="233" endWordPosition="236">It is described in detail in (Schneider, 2007). It uses tagger and chunker pre-processors parsing proper happens only between heads of chunks and a post-processor graph converter to capture long-distance dependencies. Pro3Gres is embedded in a flexible XML pipeline. It has been applied to many tasks, such as parsing biomedical literature (Rinaldi et al., 2006; Rinaldi et al., 2007) and the whole British National Corpus, and has been evaluated in several ways. We have achieved average results in the CoNLL domain adaptation track open submission (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004; MacWhinney, 2000; Brown, 1973). The performance of the parser is seriously affected by mapping problems to the particular dependency representation used in the shared task. The paper is structured as follows. We give a brief overview of the parser and its design policy in section 2, we describe the domain adaptations that we have used in section 3, comment on the results obtained in section 4 and conclude in section 5. 2 Pro3Gres and its Design Policy There has been growing interest in exploring the space between Treebank-trained probabilistic grammars (e.g. (Collins, 1999; Nivre, 2006)) and</context>
</contexts>
<marker>Kulick, Bies, Liberman, Mandel, McDonald, Palmer, Schein, Ungar, 2004</marker>
<rawString>S. Kulick, A. Bies, M. Liberman, M. Mandel, R. McDonald, M. Palmer, A. Schein, and L. Ungar. 2004. Integrated annotation for biomedical information extraction. In Proc. of the Human Language Technology Conference and the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for Analyzing Talk. Lawrence Erlbaum.</title>
<date>2000</date>
<contexts>
<context position="1617" citStr="MacWhinney, 2000" startWordPosition="237" endWordPosition="238">tail in (Schneider, 2007). It uses tagger and chunker pre-processors parsing proper happens only between heads of chunks and a post-processor graph converter to capture long-distance dependencies. Pro3Gres is embedded in a flexible XML pipeline. It has been applied to many tasks, such as parsing biomedical literature (Rinaldi et al., 2006; Rinaldi et al., 2007) and the whole British National Corpus, and has been evaluated in several ways. We have achieved average results in the CoNLL domain adaptation track open submission (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004; MacWhinney, 2000; Brown, 1973). The performance of the parser is seriously affected by mapping problems to the particular dependency representation used in the shared task. The paper is structured as follows. We give a brief overview of the parser and its design policy in section 2, we describe the domain adaptations that we have used in section 3, comment on the results obtained in section 4 and conclude in section 5. 2 Pro3Gres and its Design Policy There has been growing interest in exploring the space between Treebank-trained probabilistic grammars (e.g. (Collins, 1999; Nivre, 2006)) and formal grammar-ba</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>B. MacWhinney. 2000. The CHILDES Project: Tools for Analyzing Talk. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1550" citStr="Marcus et al., 1993" startWordPosition="225" endWordPosition="228">itten grammar with probabilistic disambiguation. It is described in detail in (Schneider, 2007). It uses tagger and chunker pre-processors parsing proper happens only between heads of chunks and a post-processor graph converter to capture long-distance dependencies. Pro3Gres is embedded in a flexible XML pipeline. It has been applied to many tasks, such as parsing biomedical literature (Rinaldi et al., 2006; Rinaldi et al., 2007) and the whole British National Corpus, and has been evaluated in several ways. We have achieved average results in the CoNLL domain adaptation track open submission (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004; MacWhinney, 2000; Brown, 1973). The performance of the parser is seriously affected by mapping problems to the particular dependency representation used in the shared task. The paper is structured as follows. We give a brief overview of the parser and its design policy in section 2, we describe the domain adaptations that we have used in section 3, comment on the results obtained in section 4 and conclude in section 5. 2 Pro3Gres and its Design Policy There has been growing interest in exploring the space between Treebank-trained probabilistic</context>
<context position="5179" citStr="Marcus et al., 1993" startWordPosition="787" endWordPosition="790">itivity to capture long-distance dependencies. We have argued in (Schneider, 2005) that LFG f-structures can be parsed for in a completely context-free fashion, except for embedded WH-questions, where a device such as functional uncertainty (Kaplan and Zaenen, 1989) or the equivalent Tree-Adjoining Grammar Adjoining operation (Joshi and Vijay-Shanker, 1989) is used. In Dependency Grammar, this device is also known as lifting (Kahane et al., 1998; Nivre and Nilsson, 2005). We use a hand-written competence grammar, combined with performance-driven disambiguation obtained from the Penn Treebank (Marcus et al., 1993). The Maximum-Likelihood Estimation (MLE) probability of generating a dependency relation R given lexical heads (a and b) at distance (in chunks) is calculated as follows. p(R, |a, b) = p(R|a, b) p(|R) = #(R, a, b) Pn i=1 #(Ri, a, b) #(R, ) #R The counts are backed off (Collins, 1999; Merlo and Esteve Ferrer, 2006). The backoff levels include semantic classes from WordNet (Fellbaum, 1998): we back off to the lexicographer file ID of the most frequent word sense. An example output of the parser is shown in figure 2. 3 Domain Adaptation Based on our experience with parsing texts form the biomedi</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Eva Esteve Ferrer</author>
</authors>
<title>The notion of argument in PP attachment.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>2</issue>
<pages>378</pages>
<marker>Merlo, Ferrer, 2006</marker>
<rawString>Paola Merlo and Eva Esteve Ferrer. 2006. The notion of argument in PP attachment. Computational Linguistics, 32(2):341 378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Junichi Tsujii</author>
</authors>
<title>Corpus-oriented grammar development for acquiring a Head-driven Phrase Structure Grammar from \x0cthe Penn Treebank.</title>
<date>2005</date>
<booktitle>Natural Language Processing - IJCNLP 2004,</booktitle>
<pages>684693</pages>
<editor>In Keh-Yih Su, Junichi Tsujii, Jong-Hyeok Lee, and Oi Yee Kwong, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="2277" citStr="Miyao et al., 2005" startWordPosition="343" endWordPosition="346">ser is seriously affected by mapping problems to the particular dependency representation used in the shared task. The paper is structured as follows. We give a brief overview of the parser and its design policy in section 2, we describe the domain adaptations that we have used in section 3, comment on the results obtained in section 4 and conclude in section 5. 2 Pro3Gres and its Design Policy There has been growing interest in exploring the space between Treebank-trained probabilistic grammars (e.g. (Collins, 1999; Nivre, 2006)) and formal grammar-based parsers integrating statistics (e.g. (Miyao et al., 2005; Riezler et al., 2002)). We have developed a parsing system that explores this space, in the vein of systems like (Kaplan et al., 2004), using a linguistic competence grammar and a probabilistic performance disambiguation allowing us to explore interactions between lexicon and grammar (Sinclair, 1996). The parser has been explicitly designed to be deep-syntactic like a formal grammar-based parser, by using a dependency representation that is close to LFG f-structure, but at the same time mostly context-free and integrating shallow approaches and aggressive pruning in order to keep search-spac</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2005</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Junichi Tsujii. 2005. Corpus-oriented grammar development for acquiring a Head-driven Phrase Structure Grammar from \x0cthe Penn Treebank. In Keh-Yih Su, Junichi Tsujii, Jong-Hyeok Lee, and Oi Yee Kwong, editors, Natural Language Processing - IJCNLP 2004, pages 684693. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudo-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05),</booktitle>
<pages>99106</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="5034" citStr="Nivre and Nilsson, 2005" startWordPosition="768" endWordPosition="771"> LFG f-structure, in order to give it an established linguistic background. It uses post-processing graph structure conversions and mild context-sensitivity to capture long-distance dependencies. We have argued in (Schneider, 2005) that LFG f-structures can be parsed for in a completely context-free fashion, except for embedded WH-questions, where a device such as functional uncertainty (Kaplan and Zaenen, 1989) or the equivalent Tree-Adjoining Grammar Adjoining operation (Joshi and Vijay-Shanker, 1989) is used. In Dependency Grammar, this device is also known as lifting (Kahane et al., 1998; Nivre and Nilsson, 2005). We use a hand-written competence grammar, combined with performance-driven disambiguation obtained from the Penn Treebank (Marcus et al., 1993). The Maximum-Likelihood Estimation (MLE) probability of generating a dependency relation R given lexical heads (a and b) at distance (in chunks) is calculated as follows. p(R, |a, b) = p(R|a, b) p(|R) = #(R, a, b) Pn i=1 #(Ri, a, b) #(R, ) #R The counts are backed off (Collins, 1999; Merlo and Esteve Ferrer, 2006). The backoff levels include semantic classes from WordNet (Fellbaum, 1998): we back off to the lexicographer file ID of the most frequent </context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective dependency parsing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05), pages 99106, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Constraints on non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the European Chapter of the Association of Computational Linguistics (EACL)</booktitle>
<pages>73--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Trento, Italy.</location>
<contexts>
<context position="2194" citStr="Nivre, 2006" startWordPosition="334" endWordPosition="335">lick et al., 2004; MacWhinney, 2000; Brown, 1973). The performance of the parser is seriously affected by mapping problems to the particular dependency representation used in the shared task. The paper is structured as follows. We give a brief overview of the parser and its design policy in section 2, we describe the domain adaptations that we have used in section 3, comment on the results obtained in section 4 and conclude in section 5. 2 Pro3Gres and its Design Policy There has been growing interest in exploring the space between Treebank-trained probabilistic grammars (e.g. (Collins, 1999; Nivre, 2006)) and formal grammar-based parsers integrating statistics (e.g. (Miyao et al., 2005; Riezler et al., 2002)). We have developed a parsing system that explores this space, in the vein of systems like (Kaplan et al., 2004), using a linguistic competence grammar and a probabilistic performance disambiguation allowing us to explore interactions between lexicon and grammar (Sinclair, 1996). The parser has been explicitly designed to be deep-syntactic like a formal grammar-based parser, by using a dependency representation that is close to LFG f-structure, but at the same time mostly context-free and</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Joakim Nivre. 2006. Constraints on non-projective dependency parsing. In Proceedings of the European Chapter of the Association of Computational Linguistics (EACL) 2006, pages 73 80, Trento, Italy. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL02),</booktitle>
<location>Philadephia, PA.</location>
<contexts>
<context position="2300" citStr="Riezler et al., 2002" startWordPosition="347" endWordPosition="350">ected by mapping problems to the particular dependency representation used in the shared task. The paper is structured as follows. We give a brief overview of the parser and its design policy in section 2, we describe the domain adaptations that we have used in section 3, comment on the results obtained in section 4 and conclude in section 5. 2 Pro3Gres and its Design Policy There has been growing interest in exploring the space between Treebank-trained probabilistic grammars (e.g. (Collins, 1999; Nivre, 2006)) and formal grammar-based parsers integrating statistics (e.g. (Miyao et al., 2005; Riezler et al., 2002)). We have developed a parsing system that explores this space, in the vein of systems like (Kaplan et al., 2004), using a linguistic competence grammar and a probabilistic performance disambiguation allowing us to explore interactions between lexicon and grammar (Sinclair, 1996). The parser has been explicitly designed to be deep-syntactic like a formal grammar-based parser, by using a dependency representation that is close to LFG f-structure, but at the same time mostly context-free and integrating shallow approaches and aggressive pruning in order to keep search-spaces small, without permi</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL02), Philadephia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Rinaldi</author>
<author>Gerold Schneider</author>
<author>Kaarel Kaljurand</author>
<author>Michael Hess</author>
<author>Martin Romacker</author>
</authors>
<title>an environment for relation mining over richly annotated corpora: the case of GENIA.</title>
<date>2006</date>
<journal>BMC Bioinformatics,</journal>
<volume>7</volume>
<pages>3--3</pages>
<contexts>
<context position="1341" citStr="Rinaldi et al., 2006" startWordPosition="189" endWordPosition="192">hieve average results, which is partly due to difficulties in mapping to the dependency representation used for the shared task. 1 Introduction The Pro3Gres parser is a dependency parser that combines a hand-written grammar with probabilistic disambiguation. It is described in detail in (Schneider, 2007). It uses tagger and chunker pre-processors parsing proper happens only between heads of chunks and a post-processor graph converter to capture long-distance dependencies. Pro3Gres is embedded in a flexible XML pipeline. It has been applied to many tasks, such as parsing biomedical literature (Rinaldi et al., 2006; Rinaldi et al., 2007) and the whole British National Corpus, and has been evaluated in several ways. We have achieved average results in the CoNLL domain adaptation track open submission (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004; MacWhinney, 2000; Brown, 1973). The performance of the parser is seriously affected by mapping problems to the particular dependency representation used in the shared task. The paper is structured as follows. We give a brief overview of the parser and its design policy in section 2, we describe the domain adaptations that we have used in </context>
</contexts>
<marker>Rinaldi, Schneider, Kaljurand, Hess, Romacker, 2006</marker>
<rawString>Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand, Michael Hess, and Martin Romacker. 2006. . an environment for relation mining over richly annotated corpora: the case of GENIA. BMC Bioinformatics, 7(Suppl 3):S3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Rinaldi</author>
<author>Gerold Schneider</author>
<author>Kaarel Kaljurand</author>
<author>Michael Hess</author>
<author>Christos Andronis</author>
<author>Ourania Konstanti</author>
<author>Andreas Persidis</author>
</authors>
<title>Mining of functional relations between genes and proteins over biomedical scientific literature using a deep-linguistic approach.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence in Medicine,</journal>
<volume>39</volume>
<pages>136</pages>
<contexts>
<context position="1364" citStr="Rinaldi et al., 2007" startWordPosition="193" endWordPosition="196"> which is partly due to difficulties in mapping to the dependency representation used for the shared task. 1 Introduction The Pro3Gres parser is a dependency parser that combines a hand-written grammar with probabilistic disambiguation. It is described in detail in (Schneider, 2007). It uses tagger and chunker pre-processors parsing proper happens only between heads of chunks and a post-processor graph converter to capture long-distance dependencies. Pro3Gres is embedded in a flexible XML pipeline. It has been applied to many tasks, such as parsing biomedical literature (Rinaldi et al., 2006; Rinaldi et al., 2007) and the whole British National Corpus, and has been evaluated in several ways. We have achieved average results in the CoNLL domain adaptation track open submission (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004; MacWhinney, 2000; Brown, 1973). The performance of the parser is seriously affected by mapping problems to the particular dependency representation used in the shared task. The paper is structured as follows. We give a brief overview of the parser and its design policy in section 2, we describe the domain adaptations that we have used in section 3, comment on t</context>
<context position="6900" citStr="Rinaldi et al., 2007" startWordPosition="1071" endWordPosition="1074">h several argument PPs in the grammar, all other nouns are not. Multi-word terms, adjective-preposition constructions and frequent PP-arguments have strong collocational force. We have thus used the collocation extraction tool XTRACT (Smadja, 2003) to discover collocations from large domain corpora. The probability of generating a dependency relation is augmented for collocations above a certain threshold. Since the tagging quality of the Chemistry testset is high, the impact of multi-word term recognition was lower than the biomedical domain when using a standard tagger, as we have shown in (Rinaldi et al., 2007). For the CHILDES domain, we have not used any adaptation. The hand-written grammar fares quite well on most types of questions, which are very frequent in this domain. In the spirit of the shared task, we have not attempted to correct tagging errors, which were frequent in the CHILDES domain. We have restricted the use of external resources to the hand-written, domain-independent grammar, and to WordNet. Due to serious problems in mapping our 1162 \x0cFigure 2: Example of original parser output LFG f-structure based dependencies to the CoNLL representation, much less time than expected was av</context>
</contexts>
<marker>Rinaldi, Schneider, Kaljurand, Hess, Andronis, Konstanti, Persidis, 2007</marker>
<rawString>Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand, Michael Hess, Christos Andronis, Ourania Konstanti, and Andreas Persidis. 2007. Mining of functional relations between genes and proteins over biomedical scientific literature using a deep-linguistic approach. Journal of Artificial Intelligence in Medicine, 39:127 136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerold Schneider</author>
</authors>
<title>A broad-coverage, representationally minimal LFG parser: chunks and F-structures are sufficient.</title>
<date>2005</date>
<booktitle>In Mriram Butt and Traci Holloway King, editors, The 10th international LFG Conference (LFG 2005),</booktitle>
<publisher>CSLI.</publisher>
<location>Bergen, Norway.</location>
<contexts>
<context position="4641" citStr="Schneider, 2005" startWordPosition="709" endWordPosition="710">have used it to parse the entire 100 million words British National Corpus (http://www.natcorp.ox.ac.uk) and similar amounts of biomedical texts. Its parsing speed is about 500,000 words per hour. The flowchart of the parser can be seen in figure 1. Pro3Gres (PRObabilistic PROlog-implemented RObust Grammatical Role Extraction System) uses a dependency representation that is close to LFG f-structure, in order to give it an established linguistic background. It uses post-processing graph structure conversions and mild context-sensitivity to capture long-distance dependencies. We have argued in (Schneider, 2005) that LFG f-structures can be parsed for in a completely context-free fashion, except for embedded WH-questions, where a device such as functional uncertainty (Kaplan and Zaenen, 1989) or the equivalent Tree-Adjoining Grammar Adjoining operation (Joshi and Vijay-Shanker, 1989) is used. In Dependency Grammar, this device is also known as lifting (Kahane et al., 1998; Nivre and Nilsson, 2005). We use a hand-written competence grammar, combined with performance-driven disambiguation obtained from the Penn Treebank (Marcus et al., 1993). The Maximum-Likelihood Estimation (MLE) probability of gener</context>
</contexts>
<marker>Schneider, 2005</marker>
<rawString>Gerold Schneider. 2005. A broad-coverage, representationally minimal LFG parser: chunks and F-structures are sufficient. In Mriram Butt and Traci Holloway King, editors, The 10th international LFG Conference (LFG 2005), Bergen, Norway. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerold Schneider</author>
</authors>
<title>Hybrid Long-Distance Functional Dependency Parsing. Doctoral Thesis,</title>
<date>2007</date>
<institution>Institute of Computational Linguistics, University of Zurich.</institution>
<note>accepted for publication.</note>
<contexts>
<context position="1026" citStr="Schneider, 2007" startWordPosition="143" endWordPosition="144">naldi,tkuhn}@ifi.uzh.ch Abstract We present Pro3Gres, a deep-syntactic, fast dependency parser that combines a handwritten competence grammar with probabilistic performance disambiguation and that has been used in the biomedical domain. We discuss its performance in the domain adaptation open submission. We achieve average results, which is partly due to difficulties in mapping to the dependency representation used for the shared task. 1 Introduction The Pro3Gres parser is a dependency parser that combines a hand-written grammar with probabilistic disambiguation. It is described in detail in (Schneider, 2007). It uses tagger and chunker pre-processors parsing proper happens only between heads of chunks and a post-processor graph converter to capture long-distance dependencies. Pro3Gres is embedded in a flexible XML pipeline. It has been applied to many tasks, such as parsing biomedical literature (Rinaldi et al., 2006; Rinaldi et al., 2007) and the whole British National Corpus, and has been evaluated in several ways. We have achieved average results in the CoNLL domain adaptation track open submission (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004; MacWhinney, 2000; Brown, </context>
<context position="7991" citStr="Schneider, 2007" startWordPosition="1257" endWordPosition="1258">of original parser output LFG f-structure based dependencies to the CoNLL representation, much less time than expected was available for the domain adaptation. 4 Our Results We have achieved average results: Labeled attachment score: 3151 / 5001 * 100 = 63.01, unlabeled attachment score: 3327 / 5001 * 100 = 66.53, label accuracy score: 3832 / 5001 * 100 = 76.62. These results are about 10 % below what we typically obtain when using our own dependency representation or GREVAL (Carroll et al., 2003), a deep-syntactic annotation scheme that is close to ours. Detailed evaluations are reported in (Schneider, 2007). Our mapping was quite poor, especially when conjunctions are involved. Also punctuation is attached poorly. 5.7 % of all dependencies remained unmapped (unknown in the figure). We give an overview of the the relation-dependent results in figures 1 and 2. Mapping problems include the following examples. First, headedness is handled very differently: while we assume auxiliaries, prepositions and coordinations to be dependents, the CoNNL representation assumes the opposite, which leads to incorrect mapping under complex interactions. Second, the semantics of parentheticals (PRN) partly remains </context>
</contexts>
<marker>Schneider, 2007</marker>
<rawString>Gerold Schneider. 2007. Hybrid Long-Distance Functional Dependency Parsing. Doctoral Thesis, Institute of Computational Linguistics, University of Zurich. accepted for publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sinclair</author>
</authors>
<title>The empty lexicon.</title>
<date>1996</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>1</volume>
<contexts>
<context position="2580" citStr="Sinclair, 1996" startWordPosition="391" endWordPosition="392">e results obtained in section 4 and conclude in section 5. 2 Pro3Gres and its Design Policy There has been growing interest in exploring the space between Treebank-trained probabilistic grammars (e.g. (Collins, 1999; Nivre, 2006)) and formal grammar-based parsers integrating statistics (e.g. (Miyao et al., 2005; Riezler et al., 2002)). We have developed a parsing system that explores this space, in the vein of systems like (Kaplan et al., 2004), using a linguistic competence grammar and a probabilistic performance disambiguation allowing us to explore interactions between lexicon and grammar (Sinclair, 1996). The parser has been explicitly designed to be deep-syntactic like a formal grammar-based parser, by using a dependency representation that is close to LFG f-structure, but at the same time mostly context-free and integrating shallow approaches and aggressive pruning in order to keep search-spaces small, without permitting compromise on performance or linguistic adequacy. (Abney, 1995) establishes the chunks and dependencies model as a well-motivated linguistic theory. The non-local linguistic constraints that a hand-written grammar allows us to formulate, e.g. expressing X-bar principles or </context>
</contexts>
<marker>Sinclair, 1996</marker>
<rawString>John Sinclair. 1996. The empty lexicon. International Journal of Corpus Linguistics, 1, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
</authors>
<title>Retrieving collocations from text: Xtract. Computational Linguistics, 19:1, Special issue on using large corpora:143177.</title>
<date>2003</date>
<contexts>
<context position="6527" citStr="Smadja, 2003" startWordPosition="1013" endWordPosition="1014">ntence-initial NP PP sequences the PP unambiguously attaches to the noun. We have observed that in sentence-initial NP PP PP sequences, also the second PP frequently attaches to the noun, the noun itself often being a relational noun. We have thus used such sequences to learn relational nouns from the unlabelled domain texts. Relational nouns are allowed to attach several argument PPs in the grammar, all other nouns are not. Multi-word terms, adjective-preposition constructions and frequent PP-arguments have strong collocational force. We have thus used the collocation extraction tool XTRACT (Smadja, 2003) to discover collocations from large domain corpora. The probability of generating a dependency relation is augmented for collocations above a certain threshold. Since the tagging quality of the Chemistry testset is high, the impact of multi-word term recognition was lower than the biomedical domain when using a standard tagger, as we have shown in (Rinaldi et al., 2007). For the CHILDES domain, we have not used any adaptation. The hand-written grammar fares quite well on most types of questions, which are very frequent in this domain. In the spirit of the shared task, we have not attempted to</context>
</contexts>
<marker>Smadja, 2003</marker>
<rawString>Frank Smadja. 2003. Retrieving collocations from text: Xtract. Computational Linguistics, 19:1, Special issue on using large corpora:143177.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>