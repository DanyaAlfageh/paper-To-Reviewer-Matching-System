<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.497143">
b&apos;Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 11641172,
</bodyText>
<figure confidence="0.565628">
Beijing, August 2010
Probabilistic Tree-Edit Models with Structured Latent Variables for
Textual Entailment and Question Answering
Mengqiu Wang
</figure>
<affiliation confidence="0.8106835">
Computer Science Department
Stanford University
</affiliation>
<email confidence="0.98093">
mengqiu@cs.stanford.edu
</email>
<author confidence="0.991014">
Christopher D. Manning
</author>
<affiliation confidence="0.9814175">
Computer Science Department
Stanford University
</affiliation>
<email confidence="0.99598">
manning@cs.stanford.edu
</email>
<sectionHeader confidence="0.9905" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998784608695652">
A range of Natural Language Process-
ing tasks involve making judgments about
the semantic relatedness of a pair of sen-
tences, such as Recognizing Textual En-
tailment (RTE) and answer selection for
Question Answering (QA). A key chal-
lenge that these tasks face in common
is the lack of explicit alignment annota-
tion between a sentence pair. We capture
the alignment by using a novel probabilis-
tic model that models tree-edit operations
on dependency parse trees. Unlike previ-
ous tree-edit models which require a sep-
arate alignment-finding phase and resort
to ad-hoc distance metrics, our method
treats alignments as structured latent vari-
ables, and offers a principled framework
for incorporating complex linguistic fea-
tures. We demonstrate the robustness of
our model by conducting experiments for
RTE and QA, and show that our model
performs competitively on both tasks with
the same set of general features.
</bodyText>
<sectionHeader confidence="0.998305" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996719456521739">
Many complex Natural Language Processing
(NLP) applications can be broken down to a sub-
task of evaluating the semantic relationship of
pairs of sentences (e.g., in Question Answering,
answer selection involve comparing each answer
candidate against the question). This means that
research aiming at analyzing pairs of semanti-
cally related natural language sentences is promis-
ing because of its reusability: it is not tied to
a particular internal representation of meanings,
but it nevertheless serves as a first step towards
full meaning understanding, which is applicable
to a number of applications. At the same time,
this paradigm clearly defines the input and output
space, facilitating system comparison and stan-
dard evaluation. Tasks of this paradigm have
drawn much of the focus in recent NLP research,
including Recognizing Textual Entailment (RTE),
answer selection for Question Answering (QA),
Paraphrase Identification (PI), Machine Transla-
tion Evaluation (MTE), and many more.
In each of these tasks, inputs to the systems are
pairs of sentences that may or may not convey the
desired semantic property (e.g., in RTE, whether
the hypothesis sentence can be entailed from the
premise sentence; in QA, whether the answer can-
didate sentence correctly answers the question),
and the output of the system is a binary classifi-
cation decision (or a regression score,as in MTE).
Earlier studies in these domains have concluded
that simple word overlap measures (e.g., bag of
words, n-grams) have a surprising degree of util-
ity (Papineni et al., 2002; Jijkoun and de Ri-
jke, 2005b), but are nevertheless not sufficient for
these tasks (Jijkoun and de Rijke, 2005a). A com-
mon problem identified in these earlier systems is
the lack of understanding of the semantic relation
between words and phrases. Later systems that
include more linguistic features extracted from re-
sources such as WordNet have enjoyed more suc-
cess (MacCartney et al., 2006). Studies have also
shown that certain prominent syntactic features
are often found beneficial (Snow et al., 2006).
More recent studies gained further leverage from
systematic exploration of the syntactic feature
space through analysis of parse trees (Wang et al.,
</bodyText>
<page confidence="0.972667">
1164
</page>
<bodyText confidence="0.990356015625">
\x0c2007; Das and Smith, 2009).
There are two key challenges imposed by these
tasks. The first challenge has to do with the hidden
alignment structures embedded in the sentence
pairs. It is straightforward to see that in order
to extract word-matching and/or syntax-matching
features, inevitably one has to consider the align-
ment between words and/or syntactic parts. These
alignments are not given as inputs, and it is a
non-trivial task to decide what the correct align-
ment is. Alignment-based approach have been
proven effective by many RTE, QA and MTE sys-
tems (Haghighi et al., 2005; Wang et al., 2007;
MacCartney et al., 2008; Das and Smith, 2009,
inter alia). Although alignment is a commonly
used approach, it is not the only one. Other stud-
ies have successfully applied theorem proving and
logical induction techniques, translating both sen-
tences to knowledge representations and then do-
ing inference on these representations (Moldovan
et al., 2003; Raina et al., 2005; de Salvo Braz
et al., 2005; MacCartney and Manning, 2007, in-
ter alia).
A second challenge arises when a system needs
to combine various sources of evidence (i.e., sur-
face text features, semantic features, and syntactic
features) to make a global classification decision.
Quite often these features are heavily overlapping
and sometimes contradicting, and thus a robust
learning scheme that knows when to activate what
feature is desired. Traditional approaches employ
a two-stage or multi-stage model where tasks are
broken down into alignment finding, feature ex-
traction, and feature learning subtasks (Haghighi
et al., 2005; MacCartney et al., 2008). The align-
ment finding task is typically done by commit-
ting to a one best alignment, and subsequent fea-
tures are extracted only according to this align-
ment. A large body of literature in joint learning
has demonstrated that such an approach can suffer
from cascaded errors at testing, and does not ben-
efit from the potential for joint learning (Finkel et
al., 2006).
In this paper, we present a novel undirected
graphical model to address these challenges. A
promising approach to these challenges is model-
ing the alignment as an edit operation sequence
over parse tree representation, an approach pio-
neered by (Punyakanok et al., 2004; Kouylekov
and Magnini, 2006; Harmeling, 2007; Mehdad,
2009). We improve upon this earlier work by
showing how alignment structures can be inher-
ently learned as structured latent variables in our
model. Tree edits are represented internally as
state transitions in a Finite-State Machine (FSM),
and our model is parameterized as a Condi-
tional Random Field (CRF) (Lafferty et al., 2001),
which allows us to incorporate a diverse set of ar-
bitrarily overlapping features.
In comparison to previous work that exploits
various ad-hoc or heuristic ways of incorporating
tree-edit operations, our model provides an ele-
gant and much more principled way of describing
tree-edit operations in a probabilistic setting.
</bodyText>
<sectionHeader confidence="0.897909" genericHeader="introduction">
2 Tree-edit CRF for Classification
</sectionHeader>
<bodyText confidence="0.999478433333333">
A training instance consists of a pair of sentences
and an associated binary judgment. In RTE, for
example, the input sentence pairs is made up of
a text sentence (e.g., Gabriel Garcia Marquez is
a novelist and winner of the Nobel prize for lit-
erature.) and a hypothesis sentence (e.g., Gabriel
Garcia Marquez won the Nobel for Literature.).
The pair is judged to be true if the hypothesis can
be entailed from the text (e.g., the answer is true
for the example sentence pair).
Formally, we denote the text sentence as txt and
the hypothesis sentence as hyp, and denote their
labeled dependency parse trees as t and h, re-
spectively. We use the binary variable z {0,1}
to denote the judgment.
The generative story behind our model is a
parse tree transformation process. t is trans-
formed into h through a sequence of tree ed-
its. Examples of tree edits are delete child, in-
sert parent, and substitute current. An edit se-
quence e = e1 ...em is valid if t can be success-
fully turned into h according to e. An example of
a trivial valid edit sequence is one that first deletes
all nodes in t then inserts all nodes in h.
Delete, insert and substitute form the three ba-
sic edit operations. Each step in an edit sequence
is also linked with current edit positions in both
trees, denoted as e.p = e1.p...em.p. We index
the tree nodes using a level-order tree traversal
scheme (i.e., root is visited first and assigned in-
</bodyText>
<page confidence="0.972912">
1165
</page>
<bodyText confidence="0.999469216216216">
\x0cdex 0, then each one of the first level children
of the root is visited in turn, and assigned an in-
dex number incremented by 1). It is worth noting
that every valid edit sequence has a correspond-
ing alignment mapping. Nodes that are inserted
or deleted are aligned to null, and nodes that are
substituted are aligned. One can find many edit
sequence for the same alignment, by altering the
order of edit operations.
We extend these basic edit operations into more
elaborate edit operations based on the linguistic
and syntactic properties of the current tree nodes
that they fire on. For example, the following are
all possible edit operations: delete a noun that is
SUB of the root, delete a named-entity of type
PERSON, substitute roots of the tree. In our
experiments, we designed a set of 45 edit op-
erations (12 delete, 12 insert and 21 substitute).
More details of the edit operations are described
in 4. Depending on the specific application do-
main, more sophisticated and verbose tree edit op-
erations can be designed and easily incorporated
into our model. In particular, tree edit opera-
tions involving deleting, inserting or substituting
entire treelets seem interesting and promising, re-
quiring merely a simple extension to the forward-
backward dynamic programming.
Next, we design a Finite-State Machine (FSM)
in which each edit operation is mapped to a unique
state, and an edit sequence is mapped into a tran-
sition sequence among states (denoted as e.a =
e1.a...em.a). In brief, an edit sequence is as-
sociated with a sequence of edit positions in the
trees (e.p = e1.p...em.p), as well as a transition
sequence among states (e.a = e1.a...em.a).
The probability of an edit sequence e given the
parse trees is defined as:
</bodyText>
<equation confidence="0.9882805">
P(e  |t,h) =
1
Z
|e|
i=1
exp f(ei1,ei,t,h) (1)
</equation>
<bodyText confidence="0.999897810810811">
where f are feature functions, are associated fea-
ture weights, and Z is the partition function to be
defined next.
Recall that our training data is composed of not
only positive examples but also negative exam-
ples. In order to take advantage of this label in-
formation, we adopt an interesting discriminative
learning framework first introduced by McCallum
et al. (2005). We call the FSM state set described
above the positive state set (S1), and duplicate the
exact same set of states, and call the new set nega-
tive state set (S0). We then add a starting state(Ss),
and add non-deterministic transitions from Ss to
every state in S1. We then add the same transi-
tions for S0. We now arrive at a new FSM struc-
ture where upon arriving at the starting state, one
makes a non-deterministic decision to enter either
the positive set or the negative set and stay in that
set until reaching the end of the edit sequence,
since no transitions are allowed across the positive
and negative set. Each edit operation sequence
can now be associated with a sequence of posi-
tive states as well as a sequence of negative states.
The intuitive idea is that during training, we want
to maximize the weights of the positive examples
in the positive state set and minimize their weights
in the negative state set, and vice versa. In other
words, we want the positive state set to attract
positive examples but push away negative exam-
ples. Figure 1 illustrates two example valid edit
sequences in the FSM, one in the positive state set
and one in the negative state set.
Formally, the partition function Z in (1) is de-
fined as the sum of weights of all valid edit se-
quences in both the positive set and negative set.
Features extracted from positive states are disjoint
from features extracted from negative states.
</bodyText>
<equation confidence="0.960808714285714">
Z =
e:e.aSs+{S0
S
S1}
|e|
i=1
exp f(ei1,ei,t,h)
</equation>
<bodyText confidence="0.956438">
Recall z {0,1} is the binary judgment indi-
cator variable. The conditional probability of z is
obtained by marginalizing over all edit sequences
that have state transitions in the state set corre-
sponding to z:
</bodyText>
<equation confidence="0.9999345">
P(z  |t,h) =
e:e.aSs+S
z
P(e  |t,h) (2)
</equation>
<bodyText confidence="0.958486">
The L2-norm penalized log-likelihood over n
training examples (L) is our training objective
function:
</bodyText>
<equation confidence="0.946026166666667">
L =
n
j=1
log(P(z(j)
|
(j)
t ,
(j)
h ))
kk2
22
(3)
</equation>
<bodyText confidence="0.9857755">
At test time, the z with higher probability is taken
as our prediction outcome.
</bodyText>
<page confidence="0.992619">
1166
</page>
<figureCaption confidence="0.891464">
\x0cFigure 1: This diagram illustrates the FSM architecture. There is a single start state, and we can transit into either the positive
</figureCaption>
<bodyText confidence="0.96214625">
state set (nodes that are not shaded), or the negative state set (shaded nodes). Here we show two examples of valid edit
sequences. They result in the same alignment structure as show in the bottom half of the diagram (dotted lines across the two
sentences are alignment links). Numbers over the arcs in the state diagram denote the edit sequence index, and numbers under
each word in the parse tree diagram denote each nodes level-order index number.
</bodyText>
<sectionHeader confidence="0.983105" genericHeader="method">
3 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.997201571428571">
We used Expectation Maximization method since
the objective function given in (3) is non-convex.
In the M-step, finding the optimal parameters un-
der the current model expectation involves com-
puting forward-backward style dynamic program-
ming (DP) in a three-dimensional table (two for
inputs and one for states) and optimization using
L-BFGS method. In practice the resulting DP ta-
ble can be quite large (for a sentence pair of length
100, and 2 sets of 45 states, we obtain 900,000 en-
tries). We improved efficiency by pruning out par-
tial sequences that do not lead to a complete valid
sequence and pre-compute the state-transition ta-
ble and features.
</bodyText>
<sectionHeader confidence="0.992687" genericHeader="method">
4 Edit Operations
</sectionHeader>
<bodyText confidence="0.991817823529412">
Table 1 lists the groups of edit operations we de-
signed and their descriptions. Not shown in the
table are three default edits ( insert, delete and
substitute), which fire when none of the more spe-
cific edit operations match. Edit operations listed
in the the top-left section capture basic match-
ing, deletion and insertion of surface text, part-of-
speech tags and named-entity tags. The top-right
section capture alignments of semantically related
words, based on relational information extracted
from various linguistic resources, such as Word-
Net and NomBank. And the bottom section cap-
ture syntactic edits. Note that multiple edit opera-
tions can fire at the same edit position if conditions
are matched (e.g., we can choose to delete if there
are more words to edit in txt, or to insert if there
are more words to edit in hyp).
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.999850941176471">
One of the most distinctive advantages of our
model compared to previous tree-edit based mod-
els is the ability to include a wide range of non-
independent, rich linguistic features. The features
we employed can be broken down into two cat-
egories. The first category is zero-order features
that model the current edit step. They consist of
a conditioning property of the current edit, and
the current state in the FSM. The second cate-
gory is first-order features that capture state tran-
sitions, by concatenating the current FSM state
with the previous FSM state. One simple form of
zero-order feature is the current FSM state itself.
The FSM states already carry a lot of information
about the current edits. Conditioning properties
are used to further describe the current edit. They
are often more fine-grained and complex (e.g.,
</bodyText>
<page confidence="0.646535">
1167
</page>
<figure confidence="0.939267346153846">
\x0cSurface edits Semantic edits
{I,D,S}-{POS}
insert/delete/substitute words of a POS type, S-SYNONYM substitute two words that are synonyms
where POS is noun, verb or proper noun S-HYPERNYM substitute two words that are hypernyms
{I,D,S}-NE insert/delete/substitute named-entity words S-ANTONYM substitute two words that are antonyms
{I,D,S}-LIKE
insert/delete/substitute words that expresses likeli-
hood, e.g., maybe, possibly
S-ACRONYM
substitute two words in which one is an acronym of
the other
{I,D,S}-MODAL
insert/delete/substitute modal verbs, e.g., can,
could, may
S-NOMBANK
substitute two words that are related according to
NomBank
S-{SAME/DIFF}
the words being substituted are the same or differ-
ent
S-NUM-0,1
substitute two words that are both numerical val-
ues, and 1 if they match, 0 if they mismatch
Syntactic edits
{I,D,S}-ROOT insert/delete/substitute root of the trees
{I,D,S}-{REL} insert/delete/substitute a tree node of grammatical relation type, where REL is either SUB, OBJ, VC or PRD
</figure>
<tableCaption confidence="0.964494">
Table 1: List of edit operations. I for INSERT, D for DELETE, and S for SUBSTITUTE.
</tableCaption>
<bodyText confidence="0.976921027027027">
syntactic-matching conditions listed below). To
give an example, in Figure 1, the second edit oper-
ation in the example sequence is S-NE. A match-
ing condition feature that fires with this state could
be substitute NE type PERSON, which tells us
exactly what type of named-entity is being sub-
stituted.
It is notable that in designing edit operations
and features, there is a continuum of choice in
terms of how much information to be encoded as
features versus edit operations. To better illustrate
the trade-off, consider the two extreme cases of
this continuum. At one extreme, we can design a
system where there are only three basic edit op-
erations, and all extra information in our current
set of edit operations can be encoded as features.
For example, in this case edit operation S-NE
would become S with feature substitute NE. The
other extreme is to encode every zero-order fea-
ture as a separate edit operation. The amount
of information encoded in the zero-order features
and edit operations is the same in both cases, but
the difference lies in first-order features and ef-
ficiency. When encoding more information as
edit operations (and thus more states in FSM),
first-order features become much more expres-
sive; whereas when encoding more information
as features, computation becomes cheaper as the
number of possible state transition sequences is
reduced. In our experiments, we aim to keep a
minimal set of edit operations that are meaning-
ful but not overly verbose, and encode additional
information as features. Each feature is a binary
feature initialized with weight 0.
Due to space limitation, we list the most im-
portant zero-order features. Many of these fea-
tures are inspired by MacCartney et al. (2006)
</bodyText>
<listItem confidence="0.95578425">
and Snow et al. (2006), but not as sophisticated.
Word matching features. These features detect
if a text word and a hypothesis word match the
following conditions:
1. have the same lemma
2. one is a phrase and contains the other word
3. are multi-word phrases and parts match
4. have the same/different named-entity type(s) + the
</listItem>
<equation confidence="0.414548">
named-entity type(s)
</equation>
<bodyText confidence="0.880937">
Tree structure features. These features try to
capture syntactic matching/mismatching informa-
tion from the labeled dependency parse trees. 1.
whether the roots of the two trees are aligned
</bodyText>
<listItem confidence="0.9732875">
2. parent-child pair match
3. (2.) and labels also match
4. (2.) and labels mismatch
5. (4.) and detailing the mismatching labels
6. parent+label match, child mismatch
7. child and label match, parents are {hyper/syno/anto}nym
8. looking for specific SUB/OBJ/PRD construct as in Snow
et al. (2006).
</listItem>
<sectionHeader confidence="0.666449" genericHeader="method">
6 Preprocessing
</sectionHeader>
<bodyText confidence="0.988052777777778">
In all of our experiments, each input pair of
text and hypothesis sentence is preprocessed as
following: Sentences were first tokenized by
the standard Penn TreeBank tokenization script,
and then we used MXPOST tagger (Ratnaparkhi,
1996) for part-of-speech (POS) tagging. POS
tagged sentences were then parsed by MST-
Parser (McDonald et al., 2005) to produce labeled
dependency parse trees. The parser was trained
</bodyText>
<page confidence="0.943643">
1168
</page>
<bodyText confidence="0.809714666666667">
\x0con the entire Penn TreeBank. The last step in the
pipeline is named-entity tagging using Stanford
NER Tagger (Finkel et al., 2005).
</bodyText>
<sectionHeader confidence="0.98167" genericHeader="method">
7 RTE Experiments
</sectionHeader>
<bodyText confidence="0.996587073170732">
Given an input text sentence and a hypothesis
sentence, the task of RTE is to make predictions
about whether or not the hypothesis can be en-
tailed from the text sentence. We use standard
evaluation datasets RTE1-3 from the Pascal RTE
Challenges (Dagan et al., 2006). For each RTE
dataset, we train a tree-edit CRF model on the
training portion and evaluate on the testing por-
tion. We report accuracy of classification results,
and precision and recall for the true entailment
class. There is a balanced positive-negative sam-
ple distribution in each dataset, so a random base-
line gives 50% classification accuracy. We used
RTE1 for feature selection and tuning in the L2
regularizer ( = 5 was used). RTE2 and RTE3
were reserved for testing.
Our system is compared with four systems
on RTE2 and three other systems on the RTE3
dataset.1 We chose these systems for compari-
son because they make use of syntactic depen-
dencies and lexical semantic information. No-
tably other systems that give state-of-the-art per-
formance on RTE use non-comparable techniques
such as theorem-proving and logical induction,
and often involve significant manual engineering
specifically for RTE, thus do not make meaningful
comparison to our model.
For RTE2, Kouylekov and Magnini (2006) ex-
perimented with various TED cost functions and
found a combination scheme to work the best for
RTE. Vanderwende et al. (2006) used syntactic
heuristic matching rules with a lexical-similarity
back-off model. Nielsen et al. (2006) extracted
features from dependency path, and combined
them with word-alignment features in a mixture of
experts classifier. Zanzotto et al. (2006) proposed
a syntactic cross-pair similarity measure for RTE.
For RTE3, Harmeling (2007) took a similar
classification-based approach with transformation
sequence features. Marsi et al. (2007) described
a system using dependency-based paraphrasing
</bodyText>
<footnote confidence="0.555431">
1Different systems are used for comparison because none
</footnote>
<table confidence="0.957616416666667">
of these systems reported performance on both datasets.
RTE2 Acc.% Prec.% Rec.%
Vanderwende et al., 2006 60.2 59.0 67.0
K&amp;M, 2006 60.5 58.9 70.0
Nielsen et al., 2006 61.1 59.0 73.3
Zanzotto et al., 2006 63.9 60.8 78.0
Tree-edit CRF 63.0 61.7 68.5
RTE3 Acc.% Prec.% Rec.%
Marsi et al., 2007 59.1 - -
Harmeling, 2007 59.5 - -
de Marneffe et al., 2006 60.5 61.8 60.2
Tree-edit CRF 61.1 61.3 65.3
</table>
<tableCaption confidence="0.994425">
Table 2: Results on RTE2 and RTE3 dataset. Results for de
</tableCaption>
<bodyText confidence="0.99430024">
Marneffe et al. (2006) were reported by MacCartney and
Manning (2008).
techniques for RTE. de Marneffe et al. (2006) de-
scribed a system where best alignments between
the sentence pairs were first found, then classifi-
cation decisions were made based on these align-
ments.
Table 2 presents RTE results. Our model per-
forms competitively on both datasets. On RTE2,
our model gives second best performance among
the methods we compare against, and the differ-
ence in accuracy from the best system is quite
small (7 out of 800 examples). We observe a
larger gap in recall, suggesting our method tends
to give higher precision, which is also commonly
found in other syntax-based systems (Snow et al.,
2006). It is worth noting that Zanzotto et al.
(2006) achieved second place in the official RTE2
evaluation. On RTE3, our model outperforms the
other syntax-based systems compared. In partic-
ular, out system gives the same precision level as
the second best system (de Marneffe et al., 2006)
without sacrificing as much recall, which is the
most common drawback found in syntax-based
systems.
</bodyText>
<sectionHeader confidence="0.987489" genericHeader="method">
8 QA Experiments
</sectionHeader>
<bodyText confidence="0.9989059">
A second Tree-edit CRF model was trained for
the task of answer selection for Question Answer-
ing. In this task, the input pair consists of a short
factoid question (e.g., Who beat Floyd Patterson
to take the title away?) and an answer candidate
sentence (e.g., He saw Ingemar Johansson knock
down Floyd Patterson seven times there in win-
ning the heavyweight title.). The pair is judged
positive if the answer candidate sentence correctly
answers the question and provides sufficient con-
</bodyText>
<page confidence="0.983284">
1169
</page>
<table confidence="0.998620333333333">
\x0cSystem MAP MRR
Punyakanok et al., 2004 0.4189 0.4939
Cui et al., 2005 0.4350 0.5569
Wang et al., 2007 0.6029 0.6852
H&amp;S, 2010 0.6091 0.6917
Tree-edit CRF 0.5951 0.6951
</table>
<tableCaption confidence="0.999482">
Table 3: Results on QA task reported in Mean Average Pre-
</tableCaption>
<bodyText confidence="0.979682461538462">
cision (MAP) and Mean Reciprocal Rank (MRR).
textual support (i.e., does not merely contain the
answer key, for example, Ingemar Johansson
was a world heavyweight champion would not
be a correct answer). We followed the same ex-
perimental setup as Wang et al. (2007) and Heil-
man and Smith (2010). The training portion of
the dataset consists of 5919 manually judged Q/A
pairs from previous QA tracks at Text REtrieval
Conference (TREC 812). There are also 1374
Q/A pairs for development and 1866 Q/A pairs
for testing, both from the TREC 3 evaluation. The
task is framed as a sentence retrieval task, and thus
Mean Average Precision (MAP) and Mean Recip-
rocal Rank (MRR) are reported for the ranked list
of most probable answer candidates. We com-
pare out model with four other systems. Wang et
al. (2007) proposed a Quasi-synchronous Gram-
mar formulation of the problem which also mod-
els alignment as structured latent variables, but in
a generative probabilistic model. Their method
gives the current state-of-the-art performance on
this task. Heilman and Smith (2010) presented
a classification-based approach with tree-edit fea-
tures extracted from a tree kernel. Cui et al.
(2005) developed a dependency-tree based in-
formation discrepancy measure. Punyakanok et
al. (2004) used a generalized Tree-edit Distance
method to score mappings between dependency
parse trees. All systems were evaluated against
the same dataset as the one we used. Results of
replicated systems for the last two were reported
by Wang et al. (2007), with lexical-semantic aug-
mentation from WordNet.
Results in Table 3 show that our model gives the
same level of performance as Wang et al. (2007),
with no statistically significant difference (p &amp;gt; 5
in sign test). Both systems out-perform the other
two earlier systems significantly.
</bodyText>
<sectionHeader confidence="0.977818" genericHeader="method">
9 Discussion
</sectionHeader>
<bodyText confidence="0.999416166666667">
Our experiments on RTE and QA applications
demonstrated that Tree-edit CRF models provide
results competitive with previous syntax-based
methods. Even though the improvements were
quite moderate in some cases, the important point
is that our model provides a novel principled
framework. It works across different problem do-
mains with minimal domain knowledge and fea-
ture engineering, whereas previous methods are
only engineered for a particular task and are hard
to generalize to new problems.
While the current Tree-edit CRF model can
model a large set of linguistic phenomenon and
tree-transformations, it has some clear limitations.
One of the biggest drawbacks is the lack of sup-
port for modeling phrasal re-ordering, which is a
very common and important linguistic phenom-
ena. It is not straightforward to implement re-
ordering in the current model because it breaks
the word-order constraint which admits tractable
forward-backward style dynamic programming.
However, this shortcoming can be addressed par-
tially by extending the model to deal with con-
strained re-ordering per Zhang (1996).
</bodyText>
<sectionHeader confidence="0.994545" genericHeader="related work">
10 Related Work
</sectionHeader>
<bodyText confidence="0.98777965">
Tree Edit Distance (TED) have been studied
extensively in theoretical and algorithmic re-
search (Klein, 1989; Zhang and Shasha, 1989;
Bille, 2005). In recent years we have seen many
work on applying TED based methods for NLP-
related tasks (Punyakanok et al., 2004; Kouylekov
and Magnini, 2006; Harmeling, 2007; Mehdad,
2009). Mehdad (2009) proposed a method based
on particle swarm optimization technique to au-
tomatically learn the TED cost function. Another
work that also developed an interesting approach
to stochastic tree edit distance is Bernard et al.
(2008), but unfortunately experiments in the pa-
per were limited to digit recognition and tasks on
small artificial datasets.
Many different approaches to modeling
sentence alignment have been proposed be-
fore (Haghighi et al., 2005; MacCartney et al.,
2008). Haghighi et al. (2005) treated alignment
finding in RTE as a graph matching problem
</bodyText>
<page confidence="0.840832">
1170
</page>
<bodyText confidence="0.996093586206896">
\x0cbetween sentence parse trees. MacCartney et
al. (2008) described a phrase-based alignment
model for MT, trained by the Perceptron learning
algorithm. A line of work that offers similar
treatment of alignment to our model is the
Quasi-synchronous Grammar (QG) (Smith and
Eisner, 2006; Wang et al., 2007; Das and Smith,
2009). QG models alignments between two parse
trees as structured latent variables. The generative
story of QG describes one that builds the parse
tree of one sentence, loosely conditioned on the
parse tree of the other sentence. This formalism
prefers but is not confined to tree isomorphism,
therefore possesses more model flexibility than
synchronous grammars.
The work of McCallum et al. (2005) inspired
the discriminative training framework that we
used in our experiments. They presented a String
Edit Distance model that also learns alignments as
hidden structures for simple tasks such as restau-
rant name matching.
Our work is also closely related to other re-
cent work on learning probabilistic models involv-
ing structural latent variables (Clark and Curran,
2004; Petrov et al., 2007; Blunsom et al., 2008;
Chang et al., 2010). The Tree-edit CRF model we
present here is a new addition to this family of in-
teresting models for discriminative learning with
structural latent variables.
</bodyText>
<sectionHeader confidence="0.941033" genericHeader="conclusions">
11 Conclusion
</sectionHeader>
<bodyText confidence="0.951503181818182">
We described a Tree-edit CRF model for predict-
ing semantic relatedness of pairs of sentences.
Our approach generalizes TED in a principled
probabilistic model that embeds alignments as
structured latent variables. We demonstrate a
wide-range of lexical-semantic and syntactic fea-
tures can be easily incorporated into the model.
Discriminatively trained, the Tree-edit CRF led to
competitive performance on the task of Recogniz-
ing Textual Entailment and answer selection for
Question Answering.
</bodyText>
<sectionHeader confidence="0.948556" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991986277777778">
Bernard, M., L. Boyer, A. Habrard, and M. Sebban.
2008. Learning probabilistic models of tree edit dis-
tance. Pattern Recognition, 41(8):26112629.
Bille, P. 2005. A survey on tree edit distance and
related problems. Theoretical Computer Science,
337(1-3):217239.
Blunsom, P., T. Cohn, and M. Osborne. 2008. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proceedings of ACL-HLT.
Chang, Ming-Wei, Dan Goldwasser, Dan Roth, and
Vivek Srikumar. 2010. Discriminative learning
over constrained latent representations. In Proceed-
ings of NAACL-HLT.
Clark, S. and J. R. Curran. 2004. Parsing the wsj using
ccg and log-linear models. In Proceedings of ACL.
Cui, Hang, Renxu Sun, Keya Li, Min-Yen Kan, and
Tat-Seng Chua. 2005. Question answering passage
retrieval using dependency relations. In Proceed-
ings of SIGIR.
Dagan, I., O. Glickman, and B. Magnini. 2006. The
pascal recognising textual entailment challenge.
Machine Learning Challenges, LNCS, 3944:177
190.
Das, Dipanjan and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of ACL-IJCNLP.
de Marneffe, M.-C., B. MacCartney, T. Grenager,
D. Cer, A. Rafferty, and C. D. Manning. 2006.
Learning to distinguish valid textual entailments.
In Proceedings of the second PASCAL Challenges
Workshop on RTE.
de Salvo Braz, R., R. Girju, V. Punyakanok, D. Roth,
and M. Sammons. 2005. An inference model for
semantic entailment and question-answering. In
Proceedings of AAAI.
Finkel, J. R., T. Grenager, and C. D. Manning. 2005.
</reference>
<bodyText confidence="0.559350733333333">
Incorporating non-local information into informa-
tion extraction systems by gibbs sampling. In Pro-
ceedings of ACL.
Finkel, J. R., C. D. Manning, and A. Y. Ng. 2006.
Solving the problem of cascading errors: Approx-
imate bayesian inference for linguistic annotation
pipelines. In Proceedings of EMNLP.
Haghighi, A., A. Y. Ng, and C. D. Manning. 2005. Ro-
bust textual inference via graph matching. In Pro-
ceedings of EMNLP.
Harmeling, S. 2007. An extensible probabilistic
transformation-based approach to the third recog-
nizing textual entailment challenge. In Proceedings
of ACL PASCAL Workshop on Textual Entailment
and Paraphrasing.
</bodyText>
<page confidence="0.691496">
1171
</page>
<reference confidence="0.996202397959184">
\x0cHeilman, M. and N. A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings
of NAACL-HLT.
Jijkoun, V. and M. de Rijke. 2005a. Recognizing tex-
tual entailment: Is word similarity enough?. In Ma-
chine Learning Challenge Workshop, volume 3944
of LNCS, pages 449460. Springer.
Jijkoun, V. and M. de Rijke. 2005b. Recognizing tex-
tual entailment using lexical similarity. In Proceed-
ings of the PASCAL Challenges Workshop on RTE.
Klein, P. N. 1989. Computing the edit-distance be-
tween unrooted ordered trees. In Proceedings of
European Symposium on Algorithms.
Kouylekov, M. and B. Magnini. 2006. Tree edit dis-
tance for recognizing textual entailment: Estimating
the cost of insertion. In Proceedings of the second
PASCAL Challenges Workshop on RTE.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
MacCartney, Bill and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proceedings
of Workshop on Textual Entailment and Paraphras-
ing at ACL 2007.
MacCartney, B. and C. D. Manning. 2008. Model-
ing semantic containment and exclusion in natural
language inference. In Proceedings of COLING.
MacCartney, B., T. Grenager, M.-C. de Marneffe,
D. Cer, and C. D. Manning. 2006. Learning to
recognize features of valid textual entailments. In
Proceedings of HLT-NAACL.
MacCartney, B., M. Galley, and C. D. Manning. 2008.
A phrase-based alignment model for natural lan-
guage inference. In Proceedings of EMNLP.
Marsi, E., E. Krahmer, and W. Bosma. 2007.
Dependency-based paraphrasing for recognizing
textual entailment. In Proceedings of ACL PASCAL
Workshop on Textual Entailment and Paraphrasing.
McCallum, A., K. Bellare, and F. Pereira. 2005.
A conditional random field for discriminatively-
trained finite-state string edit distance. In Proceed-
ings of UAI.
McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
Mehdad, Yashar. 2009. Automatic cost estimation for
tree edit distance using particle swarm optimization.
In Proceedings of ACL.
Moldovan, D., C. Clark, S. Harabagiu, and S. Maio-
rano. 2003. Cogex: A logic prover for question
answering. In Proceedings of HLT-NAACL.
Nielsen, R. D., W. Ward, and J. H. Martin. 2006. To-
ward dependency path based entailment. In Pro-
ceedings of the second PASCAL Challenges Work-
shop on RTE.
Papineni, K., S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL.
Petrov, S., A. Pauls, and D. Klein. 2007. Discrimi-
native log-linear grammars with latent variables. In
Proceedings of NIPS.
Punyakanok, V., D. Roth, and W. Yih. 2004. Map-
ping dependencies trees: An application to question
answering. In Proceedings of AI-Math.
Raina, R., A. Y. Ng, , and C. Manning. 2005. Robust
textual inference via learning and abductive reason-
ing. In Proceedings of AAAI.
Ratnaparkhi, Adwait. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of EMNLP.
Smith, D. A. and J. Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syn-
tactic dependencies. In Proceedings of the HLT-
NAACL Workshop on Statistical Machine Transla-
tion.
Snow, R., L. Vanderwende, and A. Menezes. 2006.
Effectively using syntax for recognizing false entail-
ment. In Proceedings of HLT-NAACL.
Vanderwende, L., A. Menezes, and R. Snow. 2006.
Microsoft research at rte-2: Syntactic contributions
in the entailment task: an implementation. In Pro-
ceedings of the second PASCAL Challenges Work-
shop on RTE.
Wang, M., N. A. Smith, and T. Mitamura. 2007.
What is the jeopardy model? a quasi-synchronous
grammar for question answering. In Proceedings of
EMNLP-CoNLL.
Zanzotto, F. M., A. Moschitti, M. Pennacchiotti, and
M.T. Pazienza. 2006. Learning textual entailment
from examples. In Proceedings of the second PAS-
CAL Challenges Workshop on RTE.
Zhang, K. and D. Shasha. 1989. Simple fast algo-
rithms for the editing distance between trees and re-
lated problems. SIAM Journal of Computing, 18.
Zhang, K. 1996. A constrained edit distance between
unordered labeled trees. Algorithmica, 15(3):205
222.
</reference>
<page confidence="0.74643">
1172
</page>
<figure confidence="0.31552">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.772369">
<note confidence="0.880758">b&apos;Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 11641172, Beijing, August 2010</note>
<title confidence="0.992227">Probabilistic Tree-Edit Models with Structured Latent Variables for Textual Entailment and Question Answering</title>
<author confidence="0.998276">Mengqiu Wang</author>
<affiliation confidence="0.9998">Computer Science Department Stanford University</affiliation>
<email confidence="0.998585">mengqiu@cs.stanford.edu</email>
<author confidence="0.999816">Christopher D Manning</author>
<affiliation confidence="0.99987">Computer Science Department Stanford University</affiliation>
<email confidence="0.99906">manning@cs.stanford.edu</email>
<abstract confidence="0.999207125">A range of Natural Language Processing tasks involve making judgments about the semantic relatedness of a pair of sentences, such as Recognizing Textual Entailment (RTE) and answer selection for Question Answering (QA). A key challenge that these tasks face in common is the lack of explicit alignment annotation between a sentence pair. We capture the alignment by using a novel probabilistic model that models tree-edit operations on dependency parse trees. Unlike previous tree-edit models which require a separate alignment-finding phase and resort to ad-hoc distance metrics, our method treats alignments as structured latent variables, and offers a principled framework for incorporating complex linguistic features. We demonstrate the robustness of our model by conducting experiments for RTE and QA, and show that our model performs competitively on both tasks with the same set of general features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Bernard</author>
<author>L Boyer</author>
<author>A Habrard</author>
<author>M Sebban</author>
</authors>
<title>Learning probabilistic models of tree edit distance.</title>
<date>2008</date>
<journal>Pattern Recognition,</journal>
<volume>41</volume>
<issue>8</issue>
<contexts>
<context position="27011" citStr="Bernard et al. (2008)" startWordPosition="4382" endWordPosition="4385">constrained re-ordering per Zhang (1996). 10 Related Work Tree Edit Distance (TED) have been studied extensively in theoretical and algorithmic research (Klein, 1989; Zhang and Shasha, 1989; Bille, 2005). In recent years we have seen many work on applying TED based methods for NLPrelated tasks (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). Mehdad (2009) proposed a method based on particle swarm optimization technique to automatically learn the TED cost function. Another work that also developed an interesting approach to stochastic tree edit distance is Bernard et al. (2008), but unfortunately experiments in the paper were limited to digit recognition and tasks on small artificial datasets. Many different approaches to modeling sentence alignment have been proposed before (Haghighi et al., 2005; MacCartney et al., 2008). Haghighi et al. (2005) treated alignment finding in RTE as a graph matching problem 1170 \x0cbetween sentence parse trees. MacCartney et al. (2008) described a phrase-based alignment model for MT, trained by the Perceptron learning algorithm. A line of work that offers similar treatment of alignment to our model is the Quasi-synchronous Grammar (</context>
</contexts>
<marker>Bernard, Boyer, Habrard, Sebban, 2008</marker>
<rawString>Bernard, M., L. Boyer, A. Habrard, and M. Sebban. 2008. Learning probabilistic models of tree edit distance. Pattern Recognition, 41(8):26112629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bille</author>
</authors>
<title>A survey on tree edit distance and related problems.</title>
<date>2005</date>
<journal>Theoretical Computer Science,</journal>
<pages>337--1</pages>
<contexts>
<context position="26593" citStr="Bille, 2005" startWordPosition="4318" endWordPosition="4319">the biggest drawbacks is the lack of support for modeling phrasal re-ordering, which is a very common and important linguistic phenomena. It is not straightforward to implement reordering in the current model because it breaks the word-order constraint which admits tractable forward-backward style dynamic programming. However, this shortcoming can be addressed partially by extending the model to deal with constrained re-ordering per Zhang (1996). 10 Related Work Tree Edit Distance (TED) have been studied extensively in theoretical and algorithmic research (Klein, 1989; Zhang and Shasha, 1989; Bille, 2005). In recent years we have seen many work on applying TED based methods for NLPrelated tasks (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). Mehdad (2009) proposed a method based on particle swarm optimization technique to automatically learn the TED cost function. Another work that also developed an interesting approach to stochastic tree edit distance is Bernard et al. (2008), but unfortunately experiments in the paper were limited to digit recognition and tasks on small artificial datasets. Many different approaches to modeling sentence alignment have b</context>
</contexts>
<marker>Bille, 2005</marker>
<rawString>Bille, P. 2005. A survey on tree edit distance and related problems. Theoretical Computer Science, 337(1-3):217239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="28488" citStr="Blunsom et al., 2008" startWordPosition="4613" endWordPosition="4616">e parse tree of the other sentence. This formalism prefers but is not confined to tree isomorphism, therefore possesses more model flexibility than synchronous grammars. The work of McCallum et al. (2005) inspired the discriminative training framework that we used in our experiments. They presented a String Edit Distance model that also learns alignments as hidden structures for simple tasks such as restaurant name matching. Our work is also closely related to other recent work on learning probabilistic models involving structural latent variables (Clark and Curran, 2004; Petrov et al., 2007; Blunsom et al., 2008; Chang et al., 2010). The Tree-edit CRF model we present here is a new addition to this family of interesting models for discriminative learning with structural latent variables. 11 Conclusion We described a Tree-edit CRF model for predicting semantic relatedness of pairs of sentences. Our approach generalizes TED in a principled probabilistic model that embeds alignments as structured latent variables. We demonstrate a wide-range of lexical-semantic and syntactic features can be easily incorporated into the model. Discriminatively trained, the Tree-edit CRF led to competitive performance on </context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Blunsom, P., T. Cohn, and M. Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Dan Goldwasser</author>
<author>Dan Roth</author>
<author>Vivek Srikumar</author>
</authors>
<title>Discriminative learning over constrained latent representations.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="28509" citStr="Chang et al., 2010" startWordPosition="4617" endWordPosition="4620">her sentence. This formalism prefers but is not confined to tree isomorphism, therefore possesses more model flexibility than synchronous grammars. The work of McCallum et al. (2005) inspired the discriminative training framework that we used in our experiments. They presented a String Edit Distance model that also learns alignments as hidden structures for simple tasks such as restaurant name matching. Our work is also closely related to other recent work on learning probabilistic models involving structural latent variables (Clark and Curran, 2004; Petrov et al., 2007; Blunsom et al., 2008; Chang et al., 2010). The Tree-edit CRF model we present here is a new addition to this family of interesting models for discriminative learning with structural latent variables. 11 Conclusion We described a Tree-edit CRF model for predicting semantic relatedness of pairs of sentences. Our approach generalizes TED in a principled probabilistic model that embeds alignments as structured latent variables. We demonstrate a wide-range of lexical-semantic and syntactic features can be easily incorporated into the model. Discriminatively trained, the Tree-edit CRF led to competitive performance on the task of Recognizi</context>
</contexts>
<marker>Chang, Goldwasser, Roth, Srikumar, 2010</marker>
<rawString>Chang, Ming-Wei, Dan Goldwasser, Dan Roth, and Vivek Srikumar. 2010. Discriminative learning over constrained latent representations. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>Parsing the wsj using ccg and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="28445" citStr="Clark and Curran, 2004" startWordPosition="4605" endWordPosition="4608">ee of one sentence, loosely conditioned on the parse tree of the other sentence. This formalism prefers but is not confined to tree isomorphism, therefore possesses more model flexibility than synchronous grammars. The work of McCallum et al. (2005) inspired the discriminative training framework that we used in our experiments. They presented a String Edit Distance model that also learns alignments as hidden structures for simple tasks such as restaurant name matching. Our work is also closely related to other recent work on learning probabilistic models involving structural latent variables (Clark and Curran, 2004; Petrov et al., 2007; Blunsom et al., 2008; Chang et al., 2010). The Tree-edit CRF model we present here is a new addition to this family of interesting models for discriminative learning with structural latent variables. 11 Conclusion We described a Tree-edit CRF model for predicting semantic relatedness of pairs of sentences. Our approach generalizes TED in a principled probabilistic model that embeds alignments as structured latent variables. We demonstrate a wide-range of lexical-semantic and syntactic features can be easily incorporated into the model. Discriminatively trained, the Tree-</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Clark, S. and J. R. Curran. 2004. Parsing the wsj using ccg and log-linear models. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Renxu Sun</author>
<author>Keya Li</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Question answering passage retrieval using dependency relations.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="23361" citStr="Cui et al., 2005" startWordPosition="3805" endWordPosition="3808">ack found in syntax-based systems. 8 QA Experiments A second Tree-edit CRF model was trained for the task of answer selection for Question Answering. In this task, the input pair consists of a short factoid question (e.g., Who beat Floyd Patterson to take the title away?) and an answer candidate sentence (e.g., He saw Ingemar Johansson knock down Floyd Patterson seven times there in winning the heavyweight title.). The pair is judged positive if the answer candidate sentence correctly answers the question and provides sufficient con1169 \x0cSystem MAP MRR Punyakanok et al., 2004 0.4189 0.4939 Cui et al., 2005 0.4350 0.5569 Wang et al., 2007 0.6029 0.6852 H&amp;S, 2010 0.6091 0.6917 Tree-edit CRF 0.5951 0.6951 Table 3: Results on QA task reported in Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). textual support (i.e., does not merely contain the answer key, for example, Ingemar Johansson was a world heavyweight champion would not be a correct answer). We followed the same experimental setup as Wang et al. (2007) and Heilman and Smith (2010). The training portion of the dataset consists of 5919 manually judged Q/A pairs from previous QA tracks at Text REtrieval Conference (TREC 812). There</context>
<context position="24696" citStr="Cui et al. (2005)" startWordPosition="4025" endWordPosition="4028">framed as a sentence retrieval task, and thus Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) are reported for the ranked list of most probable answer candidates. We compare out model with four other systems. Wang et al. (2007) proposed a Quasi-synchronous Grammar formulation of the problem which also models alignment as structured latent variables, but in a generative probabilistic model. Their method gives the current state-of-the-art performance on this task. Heilman and Smith (2010) presented a classification-based approach with tree-edit features extracted from a tree kernel. Cui et al. (2005) developed a dependency-tree based information discrepancy measure. Punyakanok et al. (2004) used a generalized Tree-edit Distance method to score mappings between dependency parse trees. All systems were evaluated against the same dataset as the one we used. Results of replicated systems for the last two were reported by Wang et al. (2007), with lexical-semantic augmentation from WordNet. Results in Table 3 show that our model gives the same level of performance as Wang et al. (2007), with no statistically significant difference (p &amp;gt; 5 in sign test). Both systems out-perform the other two ear</context>
</contexts>
<marker>Cui, Sun, Li, Kan, Chua, 2005</marker>
<rawString>Cui, Hang, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng Chua. 2005. Question answering passage retrieval using dependency relations. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>Machine Learning Challenges, LNCS,</booktitle>
<pages>3944--177</pages>
<contexts>
<context position="19566" citStr="Dagan et al., 2006" startWordPosition="3190" endWordPosition="3193">(Ratnaparkhi, 1996) for part-of-speech (POS) tagging. POS tagged sentences were then parsed by MSTParser (McDonald et al., 2005) to produce labeled dependency parse trees. The parser was trained 1168 \x0con the entire Penn TreeBank. The last step in the pipeline is named-entity tagging using Stanford NER Tagger (Finkel et al., 2005). 7 RTE Experiments Given an input text sentence and a hypothesis sentence, the task of RTE is to make predictions about whether or not the hypothesis can be entailed from the text sentence. We use standard evaluation datasets RTE1-3 from the Pascal RTE Challenges (Dagan et al., 2006). For each RTE dataset, we train a tree-edit CRF model on the training portion and evaluate on the testing portion. We report accuracy of classification results, and precision and recall for the true entailment class. There is a balanced positive-negative sample distribution in each dataset, so a random baseline gives 50% classification accuracy. We used RTE1 for feature selection and tuning in the L2 regularizer ( = 5 was used). RTE2 and RTE3 were reserved for testing. Our system is compared with four systems on RTE2 and three other systems on the RTE3 dataset.1 We chose these systems for com</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Dagan, I., O. Glickman, and B. Magnini. 2006. The pascal recognising textual entailment challenge. Machine Learning Challenges, LNCS, 3944:177 190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="3601" citStr="Das and Smith, 2009" startWordPosition="541" endWordPosition="544">hese tasks (Jijkoun and de Rijke, 2005a). A common problem identified in these earlier systems is the lack of understanding of the semantic relation between words and phrases. Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al., 2006). Studies have also shown that certain prominent syntactic features are often found beneficial (Snow et al., 2006). More recent studies gained further leverage from systematic exploration of the syntactic feature space through analysis of parse trees (Wang et al., 1164 \x0c2007; Das and Smith, 2009). There are two key challenges imposed by these tasks. The first challenge has to do with the hidden alignment structures embedded in the sentence pairs. It is straightforward to see that in order to extract word-matching and/or syntax-matching features, inevitably one has to consider the alignment between words and/or syntactic parts. These alignments are not given as inputs, and it is a non-trivial task to decide what the correct alignment is. Alignment-based approach have been proven effective by many RTE, QA and MTE systems (Haghighi et al., 2005; Wang et al., 2007; MacCartney et al., 2008</context>
<context position="27679" citStr="Das and Smith, 2009" startWordPosition="4486" endWordPosition="4489">e limited to digit recognition and tasks on small artificial datasets. Many different approaches to modeling sentence alignment have been proposed before (Haghighi et al., 2005; MacCartney et al., 2008). Haghighi et al. (2005) treated alignment finding in RTE as a graph matching problem 1170 \x0cbetween sentence parse trees. MacCartney et al. (2008) described a phrase-based alignment model for MT, trained by the Perceptron learning algorithm. A line of work that offers similar treatment of alignment to our model is the Quasi-synchronous Grammar (QG) (Smith and Eisner, 2006; Wang et al., 2007; Das and Smith, 2009). QG models alignments between two parse trees as structured latent variables. The generative story of QG describes one that builds the parse tree of one sentence, loosely conditioned on the parse tree of the other sentence. This formalism prefers but is not confined to tree isomorphism, therefore possesses more model flexibility than synchronous grammars. The work of McCallum et al. (2005) inspired the discriminative training framework that we used in our experiments. They presented a String Edit Distance model that also learns alignments as hidden structures for simple tasks such as restaura</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Das, Dipanjan and Noah A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-C de Marneffe</author>
<author>B MacCartney</author>
<author>T Grenager</author>
<author>D Cer</author>
<author>A Rafferty</author>
<author>C D Manning</author>
</authors>
<title>Learning to distinguish valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of the second PASCAL Challenges Workshop on RTE.</booktitle>
<marker>de Marneffe, MacCartney, Grenager, Cer, Rafferty, Manning, 2006</marker>
<rawString>de Marneffe, M.-C., B. MacCartney, T. Grenager, D. Cer, A. Rafferty, and C. D. Manning. 2006. Learning to distinguish valid textual entailments. In Proceedings of the second PASCAL Challenges Workshop on RTE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>de Salvo Braz</author>
<author>R Girju R</author>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>M Sammons</author>
</authors>
<title>An inference model for semantic entailment and question-answering.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="4575" citStr="Braz et al., 2005" startWordPosition="700" endWordPosition="703">e not given as inputs, and it is a non-trivial task to decide what the correct alignment is. Alignment-based approach have been proven effective by many RTE, QA and MTE systems (Haghighi et al., 2005; Wang et al., 2007; MacCartney et al., 2008; Das and Smith, 2009, inter alia). Although alignment is a commonly used approach, it is not the only one. Other studies have successfully applied theorem proving and logical induction techniques, translating both sentences to knowledge representations and then doing inference on these representations (Moldovan et al., 2003; Raina et al., 2005; de Salvo Braz et al., 2005; MacCartney and Manning, 2007, inter alia). A second challenge arises when a system needs to combine various sources of evidence (i.e., surface text features, semantic features, and syntactic features) to make a global classification decision. Quite often these features are heavily overlapping and sometimes contradicting, and thus a robust learning scheme that knows when to activate what feature is desired. Traditional approaches employ a two-stage or multi-stage model where tasks are broken down into alignment finding, feature extraction, and feature learning subtasks (Haghighi et al., 2005;</context>
</contexts>
<marker>Braz, R, Punyakanok, Roth, Sammons, 2005</marker>
<rawString>de Salvo Braz, R., R. Girju, V. Punyakanok, D. Roth, and M. Sammons. 2005. An inference model for semantic entailment and question-answering. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C D Manning</author>
</authors>
<date>2005</date>
<contexts>
<context position="19281" citStr="Finkel et al., 2005" startWordPosition="3141" endWordPosition="3144">fic SUB/OBJ/PRD construct as in Snow et al. (2006). 6 Preprocessing In all of our experiments, each input pair of text and hypothesis sentence is preprocessed as following: Sentences were first tokenized by the standard Penn TreeBank tokenization script, and then we used MXPOST tagger (Ratnaparkhi, 1996) for part-of-speech (POS) tagging. POS tagged sentences were then parsed by MSTParser (McDonald et al., 2005) to produce labeled dependency parse trees. The parser was trained 1168 \x0con the entire Penn TreeBank. The last step in the pipeline is named-entity tagging using Stanford NER Tagger (Finkel et al., 2005). 7 RTE Experiments Given an input text sentence and a hypothesis sentence, the task of RTE is to make predictions about whether or not the hypothesis can be entailed from the text sentence. We use standard evaluation datasets RTE1-3 from the Pascal RTE Challenges (Dagan et al., 2006). For each RTE dataset, we train a tree-edit CRF model on the training portion and evaluate on the testing portion. We report accuracy of classification results, and precision and recall for the true entailment class. There is a balanced positive-negative sample distribution in each dataset, so a random baseline g</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Finkel, J. R., T. Grenager, and C. D. Manning. 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M \x0cHeilman</author>
<author>N A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<marker>\x0cHeilman, Smith, 2010</marker>
<rawString>\x0cHeilman, M. and N. A. Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Jijkoun</author>
<author>M de Rijke</author>
</authors>
<title>Recognizing textual entailment: Is word similarity enough?.</title>
<date>2005</date>
<booktitle>In Machine Learning Challenge Workshop,</booktitle>
<volume>3944</volume>
<pages>449460</pages>
<publisher>Springer.</publisher>
<marker>Jijkoun, de Rijke, 2005</marker>
<rawString>Jijkoun, V. and M. de Rijke. 2005a. Recognizing textual entailment: Is word similarity enough?. In Machine Learning Challenge Workshop, volume 3944 of LNCS, pages 449460. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Jijkoun</author>
<author>M de Rijke</author>
</authors>
<title>Recognizing textual entailment using lexical similarity.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on RTE.</booktitle>
<marker>Jijkoun, de Rijke, 2005</marker>
<rawString>Jijkoun, V. and M. de Rijke. 2005b. Recognizing textual entailment using lexical similarity. In Proceedings of the PASCAL Challenges Workshop on RTE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P N Klein</author>
</authors>
<title>Computing the edit-distance between unrooted ordered trees.</title>
<date>1989</date>
<booktitle>In Proceedings of European Symposium on Algorithms.</booktitle>
<contexts>
<context position="26555" citStr="Klein, 1989" startWordPosition="4312" endWordPosition="4313">t has some clear limitations. One of the biggest drawbacks is the lack of support for modeling phrasal re-ordering, which is a very common and important linguistic phenomena. It is not straightforward to implement reordering in the current model because it breaks the word-order constraint which admits tractable forward-backward style dynamic programming. However, this shortcoming can be addressed partially by extending the model to deal with constrained re-ordering per Zhang (1996). 10 Related Work Tree Edit Distance (TED) have been studied extensively in theoretical and algorithmic research (Klein, 1989; Zhang and Shasha, 1989; Bille, 2005). In recent years we have seen many work on applying TED based methods for NLPrelated tasks (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). Mehdad (2009) proposed a method based on particle swarm optimization technique to automatically learn the TED cost function. Another work that also developed an interesting approach to stochastic tree edit distance is Bernard et al. (2008), but unfortunately experiments in the paper were limited to digit recognition and tasks on small artificial datasets. Many different approaches</context>
</contexts>
<marker>Klein, 1989</marker>
<rawString>Klein, P. N. 1989. Computing the edit-distance between unrooted ordered trees. In Proceedings of European Symposium on Algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kouylekov</author>
<author>B Magnini</author>
</authors>
<title>Tree edit distance for recognizing textual entailment: Estimating the cost of insertion.</title>
<date>2006</date>
<booktitle>In Proceedings of the second PASCAL Challenges Workshop on RTE.</booktitle>
<contexts>
<context position="5865" citStr="Kouylekov and Magnini, 2006" startWordPosition="904" endWordPosition="907">lly done by committing to a one best alignment, and subsequent features are extracted only according to this alignment. A large body of literature in joint learning has demonstrated that such an approach can suffer from cascaded errors at testing, and does not benefit from the potential for joint learning (Finkel et al., 2006). In this paper, we present a novel undirected graphical model to address these challenges. A promising approach to these challenges is modeling the alignment as an edit operation sequence over parse tree representation, an approach pioneered by (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). We improve upon this earlier work by showing how alignment structures can be inherently learned as structured latent variables in our model. Tree edits are represented internally as state transitions in a Finite-State Machine (FSM), and our model is parameterized as a Conditional Random Field (CRF) (Lafferty et al., 2001), which allows us to incorporate a diverse set of arbitrarily overlapping features. In comparison to previous work that exploits various ad-hoc or heuristic ways of incorporating tree-edit operations, our model provides an elegant and much mor</context>
<context position="20563" citStr="Kouylekov and Magnini (2006)" startWordPosition="3350" endWordPosition="3353">ture selection and tuning in the L2 regularizer ( = 5 was used). RTE2 and RTE3 were reserved for testing. Our system is compared with four systems on RTE2 and three other systems on the RTE3 dataset.1 We chose these systems for comparison because they make use of syntactic dependencies and lexical semantic information. Notably other systems that give state-of-the-art performance on RTE use non-comparable techniques such as theorem-proving and logical induction, and often involve significant manual engineering specifically for RTE, thus do not make meaningful comparison to our model. For RTE2, Kouylekov and Magnini (2006) experimented with various TED cost functions and found a combination scheme to work the best for RTE. Vanderwende et al. (2006) used syntactic heuristic matching rules with a lexical-similarity back-off model. Nielsen et al. (2006) extracted features from dependency path, and combined them with word-alignment features in a mixture of experts classifier. Zanzotto et al. (2006) proposed a syntactic cross-pair similarity measure for RTE. For RTE3, Harmeling (2007) took a similar classification-based approach with transformation sequence features. Marsi et al. (2007) described a system using depe</context>
<context position="26738" citStr="Kouylekov and Magnini, 2006" startWordPosition="4341" endWordPosition="4344">nomena. It is not straightforward to implement reordering in the current model because it breaks the word-order constraint which admits tractable forward-backward style dynamic programming. However, this shortcoming can be addressed partially by extending the model to deal with constrained re-ordering per Zhang (1996). 10 Related Work Tree Edit Distance (TED) have been studied extensively in theoretical and algorithmic research (Klein, 1989; Zhang and Shasha, 1989; Bille, 2005). In recent years we have seen many work on applying TED based methods for NLPrelated tasks (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). Mehdad (2009) proposed a method based on particle swarm optimization technique to automatically learn the TED cost function. Another work that also developed an interesting approach to stochastic tree edit distance is Bernard et al. (2008), but unfortunately experiments in the paper were limited to digit recognition and tasks on small artificial datasets. Many different approaches to modeling sentence alignment have been proposed before (Haghighi et al., 2005; MacCartney et al., 2008). Haghighi et al. (2005) treated alignment finding in RTE as a graph matching</context>
</contexts>
<marker>Kouylekov, Magnini, 2006</marker>
<rawString>Kouylekov, M. and B. Magnini. 2006. Tree edit distance for recognizing textual entailment: Estimating the cost of insertion. In Proceedings of the second PASCAL Challenges Workshop on RTE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="6222" citStr="Lafferty et al., 2001" startWordPosition="960" endWordPosition="963">l undirected graphical model to address these challenges. A promising approach to these challenges is modeling the alignment as an edit operation sequence over parse tree representation, an approach pioneered by (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). We improve upon this earlier work by showing how alignment structures can be inherently learned as structured latent variables in our model. Tree edits are represented internally as state transitions in a Finite-State Machine (FSM), and our model is parameterized as a Conditional Random Field (CRF) (Lafferty et al., 2001), which allows us to incorporate a diverse set of arbitrarily overlapping features. In comparison to previous work that exploits various ad-hoc or heuristic ways of incorporating tree-edit operations, our model provides an elegant and much more principled way of describing tree-edit operations in a probabilistic setting. 2 Tree-edit CRF for Classification A training instance consists of a pair of sentences and an associated binary judgment. In RTE, for example, the input sentence pairs is made up of a text sentence (e.g., Gabriel Garcia Marquez is a novelist and winner of the Nobel prize for l</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, J., A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<date>2007</date>
<contexts>
<context position="4605" citStr="MacCartney and Manning, 2007" startWordPosition="704" endWordPosition="707">ts, and it is a non-trivial task to decide what the correct alignment is. Alignment-based approach have been proven effective by many RTE, QA and MTE systems (Haghighi et al., 2005; Wang et al., 2007; MacCartney et al., 2008; Das and Smith, 2009, inter alia). Although alignment is a commonly used approach, it is not the only one. Other studies have successfully applied theorem proving and logical induction techniques, translating both sentences to knowledge representations and then doing inference on these representations (Moldovan et al., 2003; Raina et al., 2005; de Salvo Braz et al., 2005; MacCartney and Manning, 2007, inter alia). A second challenge arises when a system needs to combine various sources of evidence (i.e., surface text features, semantic features, and syntactic features) to make a global classification decision. Quite often these features are heavily overlapping and sometimes contradicting, and thus a robust learning scheme that knows when to activate what feature is desired. Traditional approaches employ a two-stage or multi-stage model where tasks are broken down into alignment finding, feature extraction, and feature learning subtasks (Haghighi et al., 2005; MacCartney et al., 2008). The</context>
</contexts>
<marker>MacCartney, Manning, 2007</marker>
<rawString>MacCartney, Bill and Christopher D. Manning. 2007.</rawString>
</citation>
<citation valid="true">
<title>Natural logic for textual inference.</title>
<date>2007</date>
<booktitle>In Proceedings of Workshop on Textual Entailment and Paraphrasing at ACL</booktitle>
<contexts>
<context position="21029" citStr="(2007)" startWordPosition="3422" endWordPosition="3422">ificant manual engineering specifically for RTE, thus do not make meaningful comparison to our model. For RTE2, Kouylekov and Magnini (2006) experimented with various TED cost functions and found a combination scheme to work the best for RTE. Vanderwende et al. (2006) used syntactic heuristic matching rules with a lexical-similarity back-off model. Nielsen et al. (2006) extracted features from dependency path, and combined them with word-alignment features in a mixture of experts classifier. Zanzotto et al. (2006) proposed a syntactic cross-pair similarity measure for RTE. For RTE3, Harmeling (2007) took a similar classification-based approach with transformation sequence features. Marsi et al. (2007) described a system using dependency-based paraphrasing 1Different systems are used for comparison because none of these systems reported performance on both datasets. RTE2 Acc.% Prec.% Rec.% Vanderwende et al., 2006 60.2 59.0 67.0 K&amp;M, 2006 60.5 58.9 70.0 Nielsen et al., 2006 61.1 59.0 73.3 Zanzotto et al., 2006 63.9 60.8 78.0 Tree-edit CRF 63.0 61.7 68.5 RTE3 Acc.% Prec.% Rec.% Marsi et al., 2007 59.1 - - Harmeling, 2007 59.5 - - de Marneffe et al., 2006 60.5 61.8 60.2 Tree-edit CRF 61.1 6</context>
<context position="23781" citStr="(2007)" startWordPosition="3879" endWordPosition="3879"> judged positive if the answer candidate sentence correctly answers the question and provides sufficient con1169 \x0cSystem MAP MRR Punyakanok et al., 2004 0.4189 0.4939 Cui et al., 2005 0.4350 0.5569 Wang et al., 2007 0.6029 0.6852 H&amp;S, 2010 0.6091 0.6917 Tree-edit CRF 0.5951 0.6951 Table 3: Results on QA task reported in Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). textual support (i.e., does not merely contain the answer key, for example, Ingemar Johansson was a world heavyweight champion would not be a correct answer). We followed the same experimental setup as Wang et al. (2007) and Heilman and Smith (2010). The training portion of the dataset consists of 5919 manually judged Q/A pairs from previous QA tracks at Text REtrieval Conference (TREC 812). There are also 1374 Q/A pairs for development and 1866 Q/A pairs for testing, both from the TREC 3 evaluation. The task is framed as a sentence retrieval task, and thus Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) are reported for the ranked list of most probable answer candidates. We compare out model with four other systems. Wang et al. (2007) proposed a Quasi-synchronous Grammar formulation of the proble</context>
<context position="25038" citStr="(2007)" startWordPosition="4081" endWordPosition="4081">variables, but in a generative probabilistic model. Their method gives the current state-of-the-art performance on this task. Heilman and Smith (2010) presented a classification-based approach with tree-edit features extracted from a tree kernel. Cui et al. (2005) developed a dependency-tree based information discrepancy measure. Punyakanok et al. (2004) used a generalized Tree-edit Distance method to score mappings between dependency parse trees. All systems were evaluated against the same dataset as the one we used. Results of replicated systems for the last two were reported by Wang et al. (2007), with lexical-semantic augmentation from WordNet. Results in Table 3 show that our model gives the same level of performance as Wang et al. (2007), with no statistically significant difference (p &amp;gt; 5 in sign test). Both systems out-perform the other two earlier systems significantly. 9 Discussion Our experiments on RTE and QA applications demonstrated that Tree-edit CRF models provide results competitive with previous syntax-based methods. Even though the improvements were quite moderate in some cases, the important point is that our model provides a novel principled framework. It works acros</context>
</contexts>
<marker>2007</marker>
<rawString>Natural logic for textual inference. In Proceedings of Workshop on Textual Entailment and Paraphrasing at ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Modeling semantic containment and exclusion in natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="21765" citStr="MacCartney and Manning (2008)" startWordPosition="3540" endWordPosition="3543">scribed a system using dependency-based paraphrasing 1Different systems are used for comparison because none of these systems reported performance on both datasets. RTE2 Acc.% Prec.% Rec.% Vanderwende et al., 2006 60.2 59.0 67.0 K&amp;M, 2006 60.5 58.9 70.0 Nielsen et al., 2006 61.1 59.0 73.3 Zanzotto et al., 2006 63.9 60.8 78.0 Tree-edit CRF 63.0 61.7 68.5 RTE3 Acc.% Prec.% Rec.% Marsi et al., 2007 59.1 - - Harmeling, 2007 59.5 - - de Marneffe et al., 2006 60.5 61.8 60.2 Tree-edit CRF 61.1 61.3 65.3 Table 2: Results on RTE2 and RTE3 dataset. Results for de Marneffe et al. (2006) were reported by MacCartney and Manning (2008). techniques for RTE. de Marneffe et al. (2006) described a system where best alignments between the sentence pairs were first found, then classification decisions were made based on these alignments. Table 2 presents RTE results. Our model performs competitively on both datasets. On RTE2, our model gives second best performance among the methods we compare against, and the difference in accuracy from the best system is quite small (7 out of 800 examples). We observe a larger gap in recall, suggesting our method tends to give higher precision, which is also commonly found in other syntax-based</context>
</contexts>
<marker>MacCartney, Manning, 2008</marker>
<rawString>MacCartney, B. and C. D. Manning. 2008. Modeling semantic containment and exclusion in natural language inference. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacCartney</author>
<author>T Grenager</author>
<author>M-C de Marneffe</author>
<author>D Cer</author>
<author>C D Manning</author>
</authors>
<title>Learning to recognize features of valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<marker>MacCartney, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>MacCartney, B., T. Grenager, M.-C. de Marneffe, D. Cer, and C. D. Manning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacCartney</author>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>A phrase-based alignment model for natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4201" citStr="MacCartney et al., 2008" startWordPosition="640" endWordPosition="643">7; Das and Smith, 2009). There are two key challenges imposed by these tasks. The first challenge has to do with the hidden alignment structures embedded in the sentence pairs. It is straightforward to see that in order to extract word-matching and/or syntax-matching features, inevitably one has to consider the alignment between words and/or syntactic parts. These alignments are not given as inputs, and it is a non-trivial task to decide what the correct alignment is. Alignment-based approach have been proven effective by many RTE, QA and MTE systems (Haghighi et al., 2005; Wang et al., 2007; MacCartney et al., 2008; Das and Smith, 2009, inter alia). Although alignment is a commonly used approach, it is not the only one. Other studies have successfully applied theorem proving and logical induction techniques, translating both sentences to knowledge representations and then doing inference on these representations (Moldovan et al., 2003; Raina et al., 2005; de Salvo Braz et al., 2005; MacCartney and Manning, 2007, inter alia). A second challenge arises when a system needs to combine various sources of evidence (i.e., surface text features, semantic features, and syntactic features) to make a global classi</context>
<context position="27261" citStr="MacCartney et al., 2008" startWordPosition="4420" endWordPosition="4423">applying TED based methods for NLPrelated tasks (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). Mehdad (2009) proposed a method based on particle swarm optimization technique to automatically learn the TED cost function. Another work that also developed an interesting approach to stochastic tree edit distance is Bernard et al. (2008), but unfortunately experiments in the paper were limited to digit recognition and tasks on small artificial datasets. Many different approaches to modeling sentence alignment have been proposed before (Haghighi et al., 2005; MacCartney et al., 2008). Haghighi et al. (2005) treated alignment finding in RTE as a graph matching problem 1170 \x0cbetween sentence parse trees. MacCartney et al. (2008) described a phrase-based alignment model for MT, trained by the Perceptron learning algorithm. A line of work that offers similar treatment of alignment to our model is the Quasi-synchronous Grammar (QG) (Smith and Eisner, 2006; Wang et al., 2007; Das and Smith, 2009). QG models alignments between two parse trees as structured latent variables. The generative story of QG describes one that builds the parse tree of one sentence, loosely conditione</context>
</contexts>
<marker>MacCartney, Galley, Manning, 2008</marker>
<rawString>MacCartney, B., M. Galley, and C. D. Manning. 2008. A phrase-based alignment model for natural language inference. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Marsi</author>
<author>E Krahmer</author>
<author>W Bosma</author>
</authors>
<title>Dependency-based paraphrasing for recognizing textual entailment.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL PASCAL Workshop on Textual Entailment and Paraphrasing.</booktitle>
<contexts>
<context position="21133" citStr="Marsi et al. (2007)" startWordPosition="3432" endWordPosition="3435">our model. For RTE2, Kouylekov and Magnini (2006) experimented with various TED cost functions and found a combination scheme to work the best for RTE. Vanderwende et al. (2006) used syntactic heuristic matching rules with a lexical-similarity back-off model. Nielsen et al. (2006) extracted features from dependency path, and combined them with word-alignment features in a mixture of experts classifier. Zanzotto et al. (2006) proposed a syntactic cross-pair similarity measure for RTE. For RTE3, Harmeling (2007) took a similar classification-based approach with transformation sequence features. Marsi et al. (2007) described a system using dependency-based paraphrasing 1Different systems are used for comparison because none of these systems reported performance on both datasets. RTE2 Acc.% Prec.% Rec.% Vanderwende et al., 2006 60.2 59.0 67.0 K&amp;M, 2006 60.5 58.9 70.0 Nielsen et al., 2006 61.1 59.0 73.3 Zanzotto et al., 2006 63.9 60.8 78.0 Tree-edit CRF 63.0 61.7 68.5 RTE3 Acc.% Prec.% Rec.% Marsi et al., 2007 59.1 - - Harmeling, 2007 59.5 - - de Marneffe et al., 2006 60.5 61.8 60.2 Tree-edit CRF 61.1 61.3 65.3 Table 2: Results on RTE2 and RTE3 dataset. Results for de Marneffe et al. (2006) were reported </context>
</contexts>
<marker>Marsi, Krahmer, Bosma, 2007</marker>
<rawString>Marsi, E., E. Krahmer, and W. Bosma. 2007. Dependency-based paraphrasing for recognizing textual entailment. In Proceedings of ACL PASCAL Workshop on Textual Entailment and Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Bellare</author>
<author>F Pereira</author>
</authors>
<title>A conditional random field for discriminativelytrained finite-state string edit distance.</title>
<date>2005</date>
<booktitle>In Proceedings of UAI.</booktitle>
<contexts>
<context position="10147" citStr="McCallum et al. (2005)" startWordPosition="1635" endWordPosition="1638">h a sequence of edit positions in the trees (e.p = e1.p...em.p), as well as a transition sequence among states (e.a = e1.a...em.a). The probability of an edit sequence e given the parse trees is defined as: P(e |t,h) = 1 Z |e| i=1 exp f(ei1,ei,t,h) (1) where f are feature functions, are associated feature weights, and Z is the partition function to be defined next. Recall that our training data is composed of not only positive examples but also negative examples. In order to take advantage of this label information, we adopt an interesting discriminative learning framework first introduced by McCallum et al. (2005). We call the FSM state set described above the positive state set (S1), and duplicate the exact same set of states, and call the new set negative state set (S0). We then add a starting state(Ss), and add non-deterministic transitions from Ss to every state in S1. We then add the same transitions for S0. We now arrive at a new FSM structure where upon arriving at the starting state, one makes a non-deterministic decision to enter either the positive set or the negative set and stay in that set until reaching the end of the edit sequence, since no transitions are allowed across the positive and</context>
<context position="28072" citStr="McCallum et al. (2005)" startWordPosition="4547" endWordPosition="4550">l for MT, trained by the Perceptron learning algorithm. A line of work that offers similar treatment of alignment to our model is the Quasi-synchronous Grammar (QG) (Smith and Eisner, 2006; Wang et al., 2007; Das and Smith, 2009). QG models alignments between two parse trees as structured latent variables. The generative story of QG describes one that builds the parse tree of one sentence, loosely conditioned on the parse tree of the other sentence. This formalism prefers but is not confined to tree isomorphism, therefore possesses more model flexibility than synchronous grammars. The work of McCallum et al. (2005) inspired the discriminative training framework that we used in our experiments. They presented a String Edit Distance model that also learns alignments as hidden structures for simple tasks such as restaurant name matching. Our work is also closely related to other recent work on learning probabilistic models involving structural latent variables (Clark and Curran, 2004; Petrov et al., 2007; Blunsom et al., 2008; Chang et al., 2010). The Tree-edit CRF model we present here is a new addition to this family of interesting models for discriminative learning with structural latent variables. 11 C</context>
</contexts>
<marker>McCallum, Bellare, Pereira, 2005</marker>
<rawString>McCallum, A., K. Bellare, and F. Pereira. 2005. A conditional random field for discriminativelytrained finite-state string edit distance. In Proceedings of UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="19075" citStr="McDonald et al., 2005" startWordPosition="3108" endWordPosition="3111">labels also match 4. (2.) and labels mismatch 5. (4.) and detailing the mismatching labels 6. parent+label match, child mismatch 7. child and label match, parents are {hyper/syno/anto}nym 8. looking for specific SUB/OBJ/PRD construct as in Snow et al. (2006). 6 Preprocessing In all of our experiments, each input pair of text and hypothesis sentence is preprocessed as following: Sentences were first tokenized by the standard Penn TreeBank tokenization script, and then we used MXPOST tagger (Ratnaparkhi, 1996) for part-of-speech (POS) tagging. POS tagged sentences were then parsed by MSTParser (McDonald et al., 2005) to produce labeled dependency parse trees. The parser was trained 1168 \x0con the entire Penn TreeBank. The last step in the pipeline is named-entity tagging using Stanford NER Tagger (Finkel et al., 2005). 7 RTE Experiments Given an input text sentence and a hypothesis sentence, the task of RTE is to make predictions about whether or not the hypothesis can be entailed from the text sentence. We use standard evaluation datasets RTE1-3 from the Pascal RTE Challenges (Dagan et al., 2006). For each RTE dataset, we train a tree-edit CRF model on the training portion and evaluate on the testing po</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>McDonald, R., K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
</authors>
<title>Automatic cost estimation for tree edit distance using particle swarm optimization.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5897" citStr="Mehdad, 2009" startWordPosition="910" endWordPosition="911">, and subsequent features are extracted only according to this alignment. A large body of literature in joint learning has demonstrated that such an approach can suffer from cascaded errors at testing, and does not benefit from the potential for joint learning (Finkel et al., 2006). In this paper, we present a novel undirected graphical model to address these challenges. A promising approach to these challenges is modeling the alignment as an edit operation sequence over parse tree representation, an approach pioneered by (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). We improve upon this earlier work by showing how alignment structures can be inherently learned as structured latent variables in our model. Tree edits are represented internally as state transitions in a Finite-State Machine (FSM), and our model is parameterized as a Conditional Random Field (CRF) (Lafferty et al., 2001), which allows us to incorporate a diverse set of arbitrarily overlapping features. In comparison to previous work that exploits various ad-hoc or heuristic ways of incorporating tree-edit operations, our model provides an elegant and much more principled way of describing t</context>
<context position="26770" citStr="Mehdad, 2009" startWordPosition="4347" endWordPosition="4348"> reordering in the current model because it breaks the word-order constraint which admits tractable forward-backward style dynamic programming. However, this shortcoming can be addressed partially by extending the model to deal with constrained re-ordering per Zhang (1996). 10 Related Work Tree Edit Distance (TED) have been studied extensively in theoretical and algorithmic research (Klein, 1989; Zhang and Shasha, 1989; Bille, 2005). In recent years we have seen many work on applying TED based methods for NLPrelated tasks (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). Mehdad (2009) proposed a method based on particle swarm optimization technique to automatically learn the TED cost function. Another work that also developed an interesting approach to stochastic tree edit distance is Bernard et al. (2008), but unfortunately experiments in the paper were limited to digit recognition and tasks on small artificial datasets. Many different approaches to modeling sentence alignment have been proposed before (Haghighi et al., 2005; MacCartney et al., 2008). Haghighi et al. (2005) treated alignment finding in RTE as a graph matching problem 1170 \x0cbetween senten</context>
</contexts>
<marker>Mehdad, 2009</marker>
<rawString>Mehdad, Yashar. 2009. Automatic cost estimation for tree edit distance using particle swarm optimization. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>C Clark</author>
<author>S Harabagiu</author>
<author>S Maiorano</author>
</authors>
<title>Cogex: A logic prover for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="4527" citStr="Moldovan et al., 2003" startWordPosition="690" endWordPosition="693">en words and/or syntactic parts. These alignments are not given as inputs, and it is a non-trivial task to decide what the correct alignment is. Alignment-based approach have been proven effective by many RTE, QA and MTE systems (Haghighi et al., 2005; Wang et al., 2007; MacCartney et al., 2008; Das and Smith, 2009, inter alia). Although alignment is a commonly used approach, it is not the only one. Other studies have successfully applied theorem proving and logical induction techniques, translating both sentences to knowledge representations and then doing inference on these representations (Moldovan et al., 2003; Raina et al., 2005; de Salvo Braz et al., 2005; MacCartney and Manning, 2007, inter alia). A second challenge arises when a system needs to combine various sources of evidence (i.e., surface text features, semantic features, and syntactic features) to make a global classification decision. Quite often these features are heavily overlapping and sometimes contradicting, and thus a robust learning scheme that knows when to activate what feature is desired. Traditional approaches employ a two-stage or multi-stage model where tasks are broken down into alignment finding, feature extraction, and f</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, Maiorano, 2003</marker>
<rawString>Moldovan, D., C. Clark, S. Harabagiu, and S. Maiorano. 2003. Cogex: A logic prover for question answering. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Nielsen</author>
<author>W Ward</author>
<author>J H Martin</author>
</authors>
<title>Toward dependency path based entailment.</title>
<date>2006</date>
<booktitle>In Proceedings of the second PASCAL Challenges Workshop on RTE.</booktitle>
<contexts>
<context position="20795" citStr="Nielsen et al. (2006)" startWordPosition="3386" endWordPosition="3389">because they make use of syntactic dependencies and lexical semantic information. Notably other systems that give state-of-the-art performance on RTE use non-comparable techniques such as theorem-proving and logical induction, and often involve significant manual engineering specifically for RTE, thus do not make meaningful comparison to our model. For RTE2, Kouylekov and Magnini (2006) experimented with various TED cost functions and found a combination scheme to work the best for RTE. Vanderwende et al. (2006) used syntactic heuristic matching rules with a lexical-similarity back-off model. Nielsen et al. (2006) extracted features from dependency path, and combined them with word-alignment features in a mixture of experts classifier. Zanzotto et al. (2006) proposed a syntactic cross-pair similarity measure for RTE. For RTE3, Harmeling (2007) took a similar classification-based approach with transformation sequence features. Marsi et al. (2007) described a system using dependency-based paraphrasing 1Different systems are used for comparison because none of these systems reported performance on both datasets. RTE2 Acc.% Prec.% Rec.% Vanderwende et al., 2006 60.2 59.0 67.0 K&amp;M, 2006 60.5 58.9 70.0 Niels</context>
</contexts>
<marker>Nielsen, Ward, Martin, 2006</marker>
<rawString>Nielsen, R. D., W. Ward, and J. H. Martin. 2006. Toward dependency path based entailment. In Proceedings of the second PASCAL Challenges Workshop on RTE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2908" citStr="Papineni et al., 2002" startWordPosition="432" endWordPosition="435">I), Machine Translation Evaluation (MTE), and many more. In each of these tasks, inputs to the systems are pairs of sentences that may or may not convey the desired semantic property (e.g., in RTE, whether the hypothesis sentence can be entailed from the premise sentence; in QA, whether the answer candidate sentence correctly answers the question), and the output of the system is a binary classification decision (or a regression score,as in MTE). Earlier studies in these domains have concluded that simple word overlap measures (e.g., bag of words, n-grams) have a surprising degree of utility (Papineni et al., 2002; Jijkoun and de Rijke, 2005b), but are nevertheless not sufficient for these tasks (Jijkoun and de Rijke, 2005a). A common problem identified in these earlier systems is the lack of understanding of the semantic relation between words and phrases. Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al., 2006). Studies have also shown that certain prominent syntactic features are often found beneficial (Snow et al., 2006). More recent studies gained further leverage from systematic exploration of the syntactic fe</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>A Pauls</author>
<author>D Klein</author>
</authors>
<title>Discriminative log-linear grammars with latent variables.</title>
<date>2007</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="28466" citStr="Petrov et al., 2007" startWordPosition="4609" endWordPosition="4612">ely conditioned on the parse tree of the other sentence. This formalism prefers but is not confined to tree isomorphism, therefore possesses more model flexibility than synchronous grammars. The work of McCallum et al. (2005) inspired the discriminative training framework that we used in our experiments. They presented a String Edit Distance model that also learns alignments as hidden structures for simple tasks such as restaurant name matching. Our work is also closely related to other recent work on learning probabilistic models involving structural latent variables (Clark and Curran, 2004; Petrov et al., 2007; Blunsom et al., 2008; Chang et al., 2010). The Tree-edit CRF model we present here is a new addition to this family of interesting models for discriminative learning with structural latent variables. 11 Conclusion We described a Tree-edit CRF model for predicting semantic relatedness of pairs of sentences. Our approach generalizes TED in a principled probabilistic model that embeds alignments as structured latent variables. We demonstrate a wide-range of lexical-semantic and syntactic features can be easily incorporated into the model. Discriminatively trained, the Tree-edit CRF led to compe</context>
</contexts>
<marker>Petrov, Pauls, Klein, 2007</marker>
<rawString>Petrov, S., A. Pauls, and D. Klein. 2007. Discriminative log-linear grammars with latent variables. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Mapping dependencies trees: An application to question answering.</title>
<date>2004</date>
<booktitle>In Proceedings of AI-Math.</booktitle>
<contexts>
<context position="5836" citStr="Punyakanok et al., 2004" startWordPosition="900" endWordPosition="903">nt finding task is typically done by committing to a one best alignment, and subsequent features are extracted only according to this alignment. A large body of literature in joint learning has demonstrated that such an approach can suffer from cascaded errors at testing, and does not benefit from the potential for joint learning (Finkel et al., 2006). In this paper, we present a novel undirected graphical model to address these challenges. A promising approach to these challenges is modeling the alignment as an edit operation sequence over parse tree representation, an approach pioneered by (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). We improve upon this earlier work by showing how alignment structures can be inherently learned as structured latent variables in our model. Tree edits are represented internally as state transitions in a Finite-State Machine (FSM), and our model is parameterized as a Conditional Random Field (CRF) (Lafferty et al., 2001), which allows us to incorporate a diverse set of arbitrarily overlapping features. In comparison to previous work that exploits various ad-hoc or heuristic ways of incorporating tree-edit operations, our model pro</context>
<context position="23330" citStr="Punyakanok et al., 2004" startWordPosition="3799" endWordPosition="3802">recall, which is the most common drawback found in syntax-based systems. 8 QA Experiments A second Tree-edit CRF model was trained for the task of answer selection for Question Answering. In this task, the input pair consists of a short factoid question (e.g., Who beat Floyd Patterson to take the title away?) and an answer candidate sentence (e.g., He saw Ingemar Johansson knock down Floyd Patterson seven times there in winning the heavyweight title.). The pair is judged positive if the answer candidate sentence correctly answers the question and provides sufficient con1169 \x0cSystem MAP MRR Punyakanok et al., 2004 0.4189 0.4939 Cui et al., 2005 0.4350 0.5569 Wang et al., 2007 0.6029 0.6852 H&amp;S, 2010 0.6091 0.6917 Tree-edit CRF 0.5951 0.6951 Table 3: Results on QA task reported in Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). textual support (i.e., does not merely contain the answer key, for example, Ingemar Johansson was a world heavyweight champion would not be a correct answer). We followed the same experimental setup as Wang et al. (2007) and Heilman and Smith (2010). The training portion of the dataset consists of 5919 manually judged Q/A pairs from previous QA tracks at Text REtriev</context>
<context position="24788" citStr="Punyakanok et al. (2004)" startWordPosition="4037" endWordPosition="4040">eciprocal Rank (MRR) are reported for the ranked list of most probable answer candidates. We compare out model with four other systems. Wang et al. (2007) proposed a Quasi-synchronous Grammar formulation of the problem which also models alignment as structured latent variables, but in a generative probabilistic model. Their method gives the current state-of-the-art performance on this task. Heilman and Smith (2010) presented a classification-based approach with tree-edit features extracted from a tree kernel. Cui et al. (2005) developed a dependency-tree based information discrepancy measure. Punyakanok et al. (2004) used a generalized Tree-edit Distance method to score mappings between dependency parse trees. All systems were evaluated against the same dataset as the one we used. Results of replicated systems for the last two were reported by Wang et al. (2007), with lexical-semantic augmentation from WordNet. Results in Table 3 show that our model gives the same level of performance as Wang et al. (2007), with no statistically significant difference (p &amp;gt; 5 in sign test). Both systems out-perform the other two earlier systems significantly. 9 Discussion Our experiments on RTE and QA applications demonstr</context>
<context position="26709" citStr="Punyakanok et al., 2004" startWordPosition="4337" endWordPosition="4340"> important linguistic phenomena. It is not straightforward to implement reordering in the current model because it breaks the word-order constraint which admits tractable forward-backward style dynamic programming. However, this shortcoming can be addressed partially by extending the model to deal with constrained re-ordering per Zhang (1996). 10 Related Work Tree Edit Distance (TED) have been studied extensively in theoretical and algorithmic research (Klein, 1989; Zhang and Shasha, 1989; Bille, 2005). In recent years we have seen many work on applying TED based methods for NLPrelated tasks (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). Mehdad (2009) proposed a method based on particle swarm optimization technique to automatically learn the TED cost function. Another work that also developed an interesting approach to stochastic tree edit distance is Bernard et al. (2008), but unfortunately experiments in the paper were limited to digit recognition and tasks on small artificial datasets. Many different approaches to modeling sentence alignment have been proposed before (Haghighi et al., 2005; MacCartney et al., 2008). Haghighi et al. (2005) treated alignment findi</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2004</marker>
<rawString>Punyakanok, V., D. Roth, and W. Yih. 2004. Mapping dependencies trees: An application to question answering. In Proceedings of AI-Math.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Raina</author>
<author>A Y Ng</author>
</authors>
<title>Robust textual inference via learning and abductive reasoning.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<marker>Raina, Ng, 2005</marker>
<rawString>Raina, R., A. Y. Ng, , and C. Manning. 2005. Robust textual inference via learning and abductive reasoning. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-of-speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="18966" citStr="Ratnaparkhi, 1996" startWordPosition="3093" endWordPosition="3094">cy parse trees. 1. whether the roots of the two trees are aligned 2. parent-child pair match 3. (2.) and labels also match 4. (2.) and labels mismatch 5. (4.) and detailing the mismatching labels 6. parent+label match, child mismatch 7. child and label match, parents are {hyper/syno/anto}nym 8. looking for specific SUB/OBJ/PRD construct as in Snow et al. (2006). 6 Preprocessing In all of our experiments, each input pair of text and hypothesis sentence is preprocessed as following: Sentences were first tokenized by the standard Penn TreeBank tokenization script, and then we used MXPOST tagger (Ratnaparkhi, 1996) for part-of-speech (POS) tagging. POS tagged sentences were then parsed by MSTParser (McDonald et al., 2005) to produce labeled dependency parse trees. The parser was trained 1168 \x0con the entire Penn TreeBank. The last step in the pipeline is named-entity tagging using Stanford NER Tagger (Finkel et al., 2005). 7 RTE Experiments Given an input text sentence and a hypothesis sentence, the task of RTE is to make predictions about whether or not the hypothesis can be entailed from the text sentence. We use standard evaluation datasets RTE1-3 from the Pascal RTE Challenges (Dagan et al., 2006)</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi, Adwait. 1996. A maximum entropy part-of-speech tagger. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLTNAACL Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="27638" citStr="Smith and Eisner, 2006" startWordPosition="4478" endWordPosition="4481"> unfortunately experiments in the paper were limited to digit recognition and tasks on small artificial datasets. Many different approaches to modeling sentence alignment have been proposed before (Haghighi et al., 2005; MacCartney et al., 2008). Haghighi et al. (2005) treated alignment finding in RTE as a graph matching problem 1170 \x0cbetween sentence parse trees. MacCartney et al. (2008) described a phrase-based alignment model for MT, trained by the Perceptron learning algorithm. A line of work that offers similar treatment of alignment to our model is the Quasi-synchronous Grammar (QG) (Smith and Eisner, 2006; Wang et al., 2007; Das and Smith, 2009). QG models alignments between two parse trees as structured latent variables. The generative story of QG describes one that builds the parse tree of one sentence, loosely conditioned on the parse tree of the other sentence. This formalism prefers but is not confined to tree isomorphism, therefore possesses more model flexibility than synchronous grammars. The work of McCallum et al. (2005) inspired the discriminative training framework that we used in our experiments. They presented a String Edit Distance model that also learns alignments as hidden str</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>Smith, D. A. and J. Eisner. 2006. Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies. In Proceedings of the HLTNAACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>L Vanderwende</author>
<author>A Menezes</author>
</authors>
<title>Effectively using syntax for recognizing false entailment.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="3415" citStr="Snow et al., 2006" startWordPosition="513" endWordPosition="516"> word overlap measures (e.g., bag of words, n-grams) have a surprising degree of utility (Papineni et al., 2002; Jijkoun and de Rijke, 2005b), but are nevertheless not sufficient for these tasks (Jijkoun and de Rijke, 2005a). A common problem identified in these earlier systems is the lack of understanding of the semantic relation between words and phrases. Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al., 2006). Studies have also shown that certain prominent syntactic features are often found beneficial (Snow et al., 2006). More recent studies gained further leverage from systematic exploration of the syntactic feature space through analysis of parse trees (Wang et al., 1164 \x0c2007; Das and Smith, 2009). There are two key challenges imposed by these tasks. The first challenge has to do with the hidden alignment structures embedded in the sentence pairs. It is straightforward to see that in order to extract word-matching and/or syntax-matching features, inevitably one has to consider the alignment between words and/or syntactic parts. These alignments are not given as inputs, and it is a non-trivial task to de</context>
<context position="17895" citStr="Snow et al. (2006)" startWordPosition="2924" endWordPosition="2927">it operations (and thus more states in FSM), first-order features become much more expressive; whereas when encoding more information as features, computation becomes cheaper as the number of possible state transition sequences is reduced. In our experiments, we aim to keep a minimal set of edit operations that are meaningful but not overly verbose, and encode additional information as features. Each feature is a binary feature initialized with weight 0. Due to space limitation, we list the most important zero-order features. Many of these features are inspired by MacCartney et al. (2006) and Snow et al. (2006), but not as sophisticated. Word matching features. These features detect if a text word and a hypothesis word match the following conditions: 1. have the same lemma 2. one is a phrase and contains the other word 3. are multi-word phrases and parts match 4. have the same/different named-entity type(s) + the named-entity type(s) Tree structure features. These features try to capture syntactic matching/mismatching information from the labeled dependency parse trees. 1. whether the roots of the two trees are aligned 2. parent-child pair match 3. (2.) and labels also match 4. (2.) and labels misma</context>
<context position="22393" citStr="Snow et al., 2006" startWordPosition="3646" endWordPosition="3649">es for RTE. de Marneffe et al. (2006) described a system where best alignments between the sentence pairs were first found, then classification decisions were made based on these alignments. Table 2 presents RTE results. Our model performs competitively on both datasets. On RTE2, our model gives second best performance among the methods we compare against, and the difference in accuracy from the best system is quite small (7 out of 800 examples). We observe a larger gap in recall, suggesting our method tends to give higher precision, which is also commonly found in other syntax-based systems (Snow et al., 2006). It is worth noting that Zanzotto et al. (2006) achieved second place in the official RTE2 evaluation. On RTE3, our model outperforms the other syntax-based systems compared. In particular, out system gives the same precision level as the second best system (de Marneffe et al., 2006) without sacrificing as much recall, which is the most common drawback found in syntax-based systems. 8 QA Experiments A second Tree-edit CRF model was trained for the task of answer selection for Question Answering. In this task, the input pair consists of a short factoid question (e.g., Who beat Floyd Patterson </context>
</contexts>
<marker>Snow, Vanderwende, Menezes, 2006</marker>
<rawString>Snow, R., L. Vanderwende, and A. Menezes. 2006. Effectively using syntax for recognizing false entailment. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Vanderwende</author>
<author>A Menezes</author>
<author>R Snow</author>
</authors>
<title>Microsoft research at rte-2: Syntactic contributions in the entailment task: an implementation.</title>
<date>2006</date>
<booktitle>In Proceedings of the second PASCAL Challenges Workshop on RTE.</booktitle>
<contexts>
<context position="20691" citStr="Vanderwende et al. (2006)" startWordPosition="3372" endWordPosition="3375">th four systems on RTE2 and three other systems on the RTE3 dataset.1 We chose these systems for comparison because they make use of syntactic dependencies and lexical semantic information. Notably other systems that give state-of-the-art performance on RTE use non-comparable techniques such as theorem-proving and logical induction, and often involve significant manual engineering specifically for RTE, thus do not make meaningful comparison to our model. For RTE2, Kouylekov and Magnini (2006) experimented with various TED cost functions and found a combination scheme to work the best for RTE. Vanderwende et al. (2006) used syntactic heuristic matching rules with a lexical-similarity back-off model. Nielsen et al. (2006) extracted features from dependency path, and combined them with word-alignment features in a mixture of experts classifier. Zanzotto et al. (2006) proposed a syntactic cross-pair similarity measure for RTE. For RTE3, Harmeling (2007) took a similar classification-based approach with transformation sequence features. Marsi et al. (2007) described a system using dependency-based paraphrasing 1Different systems are used for comparison because none of these systems reported performance on both </context>
</contexts>
<marker>Vanderwende, Menezes, Snow, 2006</marker>
<rawString>Vanderwende, L., A. Menezes, and R. Snow. 2006. Microsoft research at rte-2: Syntactic contributions in the entailment task: an implementation. In Proceedings of the second PASCAL Challenges Workshop on RTE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>N A Smith</author>
<author>T Mitamura</author>
</authors>
<title>What is the jeopardy model? a quasi-synchronous grammar for question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="4176" citStr="Wang et al., 2007" startWordPosition="636" endWordPosition="639">t al., 1164 \x0c2007; Das and Smith, 2009). There are two key challenges imposed by these tasks. The first challenge has to do with the hidden alignment structures embedded in the sentence pairs. It is straightforward to see that in order to extract word-matching and/or syntax-matching features, inevitably one has to consider the alignment between words and/or syntactic parts. These alignments are not given as inputs, and it is a non-trivial task to decide what the correct alignment is. Alignment-based approach have been proven effective by many RTE, QA and MTE systems (Haghighi et al., 2005; Wang et al., 2007; MacCartney et al., 2008; Das and Smith, 2009, inter alia). Although alignment is a commonly used approach, it is not the only one. Other studies have successfully applied theorem proving and logical induction techniques, translating both sentences to knowledge representations and then doing inference on these representations (Moldovan et al., 2003; Raina et al., 2005; de Salvo Braz et al., 2005; MacCartney and Manning, 2007, inter alia). A second challenge arises when a system needs to combine various sources of evidence (i.e., surface text features, semantic features, and syntactic features</context>
<context position="23393" citStr="Wang et al., 2007" startWordPosition="3811" endWordPosition="3814">ms. 8 QA Experiments A second Tree-edit CRF model was trained for the task of answer selection for Question Answering. In this task, the input pair consists of a short factoid question (e.g., Who beat Floyd Patterson to take the title away?) and an answer candidate sentence (e.g., He saw Ingemar Johansson knock down Floyd Patterson seven times there in winning the heavyweight title.). The pair is judged positive if the answer candidate sentence correctly answers the question and provides sufficient con1169 \x0cSystem MAP MRR Punyakanok et al., 2004 0.4189 0.4939 Cui et al., 2005 0.4350 0.5569 Wang et al., 2007 0.6029 0.6852 H&amp;S, 2010 0.6091 0.6917 Tree-edit CRF 0.5951 0.6951 Table 3: Results on QA task reported in Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). textual support (i.e., does not merely contain the answer key, for example, Ingemar Johansson was a world heavyweight champion would not be a correct answer). We followed the same experimental setup as Wang et al. (2007) and Heilman and Smith (2010). The training portion of the dataset consists of 5919 manually judged Q/A pairs from previous QA tracks at Text REtrieval Conference (TREC 812). There are also 1374 Q/A pairs for dev</context>
<context position="25038" citStr="Wang et al. (2007)" startWordPosition="4078" endWordPosition="4081">ured latent variables, but in a generative probabilistic model. Their method gives the current state-of-the-art performance on this task. Heilman and Smith (2010) presented a classification-based approach with tree-edit features extracted from a tree kernel. Cui et al. (2005) developed a dependency-tree based information discrepancy measure. Punyakanok et al. (2004) used a generalized Tree-edit Distance method to score mappings between dependency parse trees. All systems were evaluated against the same dataset as the one we used. Results of replicated systems for the last two were reported by Wang et al. (2007), with lexical-semantic augmentation from WordNet. Results in Table 3 show that our model gives the same level of performance as Wang et al. (2007), with no statistically significant difference (p &amp;gt; 5 in sign test). Both systems out-perform the other two earlier systems significantly. 9 Discussion Our experiments on RTE and QA applications demonstrated that Tree-edit CRF models provide results competitive with previous syntax-based methods. Even though the improvements were quite moderate in some cases, the important point is that our model provides a novel principled framework. It works acros</context>
<context position="27657" citStr="Wang et al., 2007" startWordPosition="4482" endWordPosition="4485">ts in the paper were limited to digit recognition and tasks on small artificial datasets. Many different approaches to modeling sentence alignment have been proposed before (Haghighi et al., 2005; MacCartney et al., 2008). Haghighi et al. (2005) treated alignment finding in RTE as a graph matching problem 1170 \x0cbetween sentence parse trees. MacCartney et al. (2008) described a phrase-based alignment model for MT, trained by the Perceptron learning algorithm. A line of work that offers similar treatment of alignment to our model is the Quasi-synchronous Grammar (QG) (Smith and Eisner, 2006; Wang et al., 2007; Das and Smith, 2009). QG models alignments between two parse trees as structured latent variables. The generative story of QG describes one that builds the parse tree of one sentence, loosely conditioned on the parse tree of the other sentence. This formalism prefers but is not confined to tree isomorphism, therefore possesses more model flexibility than synchronous grammars. The work of McCallum et al. (2005) inspired the discriminative training framework that we used in our experiments. They presented a String Edit Distance model that also learns alignments as hidden structures for simple </context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>Wang, M., N. A. Smith, and T. Mitamura. 2007. What is the jeopardy model? a quasi-synchronous grammar for question answering. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>A Moschitti</author>
<author>M Pennacchiotti</author>
<author>M T Pazienza</author>
</authors>
<title>Learning textual entailment from examples.</title>
<date>2006</date>
<booktitle>In Proceedings of the second PASCAL Challenges Workshop on RTE.</booktitle>
<contexts>
<context position="20942" citStr="Zanzotto et al. (2006)" startWordPosition="3407" endWordPosition="3410">RTE use non-comparable techniques such as theorem-proving and logical induction, and often involve significant manual engineering specifically for RTE, thus do not make meaningful comparison to our model. For RTE2, Kouylekov and Magnini (2006) experimented with various TED cost functions and found a combination scheme to work the best for RTE. Vanderwende et al. (2006) used syntactic heuristic matching rules with a lexical-similarity back-off model. Nielsen et al. (2006) extracted features from dependency path, and combined them with word-alignment features in a mixture of experts classifier. Zanzotto et al. (2006) proposed a syntactic cross-pair similarity measure for RTE. For RTE3, Harmeling (2007) took a similar classification-based approach with transformation sequence features. Marsi et al. (2007) described a system using dependency-based paraphrasing 1Different systems are used for comparison because none of these systems reported performance on both datasets. RTE2 Acc.% Prec.% Rec.% Vanderwende et al., 2006 60.2 59.0 67.0 K&amp;M, 2006 60.5 58.9 70.0 Nielsen et al., 2006 61.1 59.0 73.3 Zanzotto et al., 2006 63.9 60.8 78.0 Tree-edit CRF 63.0 61.7 68.5 RTE3 Acc.% Prec.% Rec.% Marsi et al., 2007 59.1 - </context>
<context position="22441" citStr="Zanzotto et al. (2006)" startWordPosition="3655" endWordPosition="3658">bed a system where best alignments between the sentence pairs were first found, then classification decisions were made based on these alignments. Table 2 presents RTE results. Our model performs competitively on both datasets. On RTE2, our model gives second best performance among the methods we compare against, and the difference in accuracy from the best system is quite small (7 out of 800 examples). We observe a larger gap in recall, suggesting our method tends to give higher precision, which is also commonly found in other syntax-based systems (Snow et al., 2006). It is worth noting that Zanzotto et al. (2006) achieved second place in the official RTE2 evaluation. On RTE3, our model outperforms the other syntax-based systems compared. In particular, out system gives the same precision level as the second best system (de Marneffe et al., 2006) without sacrificing as much recall, which is the most common drawback found in syntax-based systems. 8 QA Experiments A second Tree-edit CRF model was trained for the task of answer selection for Question Answering. In this task, the input pair consists of a short factoid question (e.g., Who beat Floyd Patterson to take the title away?) and an answer candidate</context>
</contexts>
<marker>Zanzotto, Moschitti, Pennacchiotti, Pazienza, 2006</marker>
<rawString>Zanzotto, F. M., A. Moschitti, M. Pennacchiotti, and M.T. Pazienza. 2006. Learning textual entailment from examples. In Proceedings of the second PASCAL Challenges Workshop on RTE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zhang</author>
<author>D Shasha</author>
</authors>
<title>Simple fast algorithms for the editing distance between trees and related problems.</title>
<date>1989</date>
<journal>SIAM Journal of Computing,</journal>
<volume>18</volume>
<contexts>
<context position="26579" citStr="Zhang and Shasha, 1989" startWordPosition="4314" endWordPosition="4317">ear limitations. One of the biggest drawbacks is the lack of support for modeling phrasal re-ordering, which is a very common and important linguistic phenomena. It is not straightforward to implement reordering in the current model because it breaks the word-order constraint which admits tractable forward-backward style dynamic programming. However, this shortcoming can be addressed partially by extending the model to deal with constrained re-ordering per Zhang (1996). 10 Related Work Tree Edit Distance (TED) have been studied extensively in theoretical and algorithmic research (Klein, 1989; Zhang and Shasha, 1989; Bille, 2005). In recent years we have seen many work on applying TED based methods for NLPrelated tasks (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). Mehdad (2009) proposed a method based on particle swarm optimization technique to automatically learn the TED cost function. Another work that also developed an interesting approach to stochastic tree edit distance is Bernard et al. (2008), but unfortunately experiments in the paper were limited to digit recognition and tasks on small artificial datasets. Many different approaches to modeling sentence al</context>
</contexts>
<marker>Zhang, Shasha, 1989</marker>
<rawString>Zhang, K. and D. Shasha. 1989. Simple fast algorithms for the editing distance between trees and related problems. SIAM Journal of Computing, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zhang</author>
</authors>
<title>A constrained edit distance between unordered labeled trees.</title>
<date>1996</date>
<journal>Algorithmica,</journal>
<volume>15</volume>
<issue>3</issue>
<pages>222</pages>
<contexts>
<context position="26430" citStr="Zhang (1996)" startWordPosition="4293" endWordPosition="4294">new problems. While the current Tree-edit CRF model can model a large set of linguistic phenomenon and tree-transformations, it has some clear limitations. One of the biggest drawbacks is the lack of support for modeling phrasal re-ordering, which is a very common and important linguistic phenomena. It is not straightforward to implement reordering in the current model because it breaks the word-order constraint which admits tractable forward-backward style dynamic programming. However, this shortcoming can be addressed partially by extending the model to deal with constrained re-ordering per Zhang (1996). 10 Related Work Tree Edit Distance (TED) have been studied extensively in theoretical and algorithmic research (Klein, 1989; Zhang and Shasha, 1989; Bille, 2005). In recent years we have seen many work on applying TED based methods for NLPrelated tasks (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). Mehdad (2009) proposed a method based on particle swarm optimization technique to automatically learn the TED cost function. Another work that also developed an interesting approach to stochastic tree edit distance is Bernard et al. (2008), but unfortunately</context>
</contexts>
<marker>Zhang, 1996</marker>
<rawString>Zhang, K. 1996. A constrained edit distance between unordered labeled trees. Algorithmica, 15(3):205 222.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>