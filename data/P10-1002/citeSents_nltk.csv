The classification result C(i, j) can be a boolean value: C(i, j) = p p {0, 1} (1) as produced by a support vector machine (SVM) classifier CITATION.,,
C(i, j) can also be a realvalued probability: C(i, j) = p 0 p 1 (2) as produced by an maximum entropy (ME) classifier CITATION.,,
Follow the edge based factorization method CITATION, w,,
We choose the 2nd-ordered MST model CITATION as the baseline.,,
The relative weight is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training CITATION.,,
5 Related Works 5.1 Dependency Parsing Both the graph-based (CITATIONa; CITATION; CITATION) and the transition-based (CITATION; CITATION) parsing algorithms are related to our word-pair classification model.,,
For example, they can be integrated deeply at each decoding step (CITATION; CITATION; CITATION), or can be integrated shallowly in a reranking manner (CITATION; CITATION).,,
We choose the 2nd-ordered MST model CITATION as the baseline.,,
For example, they can be integrated deeply at each decoding step (CITATION; CITATION; CITATION), or can be integrated shallowly in a reranking manner (CITATION; CITATION).,,
We choose the 2nd-ordered MST model CITATION as the baseline.,,
Previous graph-based dependency models usually use the index distance of word i and word j 1 We exclude the in between features of CITATIONa) since preliminary experiments show that these features bring no improvement to the word-pair classification model.,,
However, in order to utilize some syntax information between the pair of words, we adopt the syntactic distance representation of CITATION, named Collins distance for convenience.,,
For example, they can be integrated deeply at each decoding step (CITATION; CITATION; CITATION), or can be integrated shallowly in a reranking manner (CITATION; CITATION).,,
We choose the 2nd-ordered MST model CITATION as the baseline.,,
6.1 Word-Pair Classification Model We experiment on two popular treebanks, the Wall Street Journal (WSJ) portion of the Penn English Treebank CITATION, and the Penn Chinese Treebank (CTB) 5.0 CITATION.,,
The constituent trees in the two treebanks are transformed to dependency trees according to the headfinding rules of CITATION.,,
For English, we use the automatically-assigned POS tags produced by an implementation of the POS tagger of CITATION.,,
Both English and Chinese sentences are tagged by the implementations of the POS tagger of CITATION, which trained on WSJ and CTB 5.0 respectively.,,
The English sentences are then parsed by an implementation of 2nd-ordered MST model of CITATION, which is trained on dependency trees extracted from WSJ.,,
The alignment matrixes for sentence pairs are generated according to CITATION.,,
First, we implement a chart-based dynamic programming parser for the 2nd-ordered MST model, and develop a training procedure based on the perceptron algorithm with averaged parameters CITATION.,,
On the WSJ corpus, this parser achieves the same performance as that of CITATION.,,
The weight parameter is tuned by a minimum error-rate training algorithm CITATION.,,
Follow the edge based factorization method CITATION, we factorize the score of a dependency tree s(x, y) into its dependency edges, and design a dynamic programming algorithm to search for the candidate parse with maximum score.,,
For example, the unsupervised dependency parsing CITATION which is totally based on unannotated data, and the semisupervised dependency parsing CITATION which is based on both annotated and unannotated data.,,
Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (CITATION; CITATION; CITATION; CITATION; CITATION).,,
For dependency projection, the relationship between words in the parsed sentences can be simply projected across the word alignment to words in the unparsed sentences, according to the DCA assumption CITATION.,,
To tackle this problem, CITATION use some filtering rules to reduce noise, and some hand-designed rules to,,
Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (CITATION; CITATION).,,
CITATION aims to obtain Chinese bracketing knowledge via ITG CITATION alignment.,,
CITATION and CITATION induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity.,,
CITATION perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar features.,,
CITATION refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees.,,
All previous works for dependency projection (CITATION; CITATION; CITATION; Jiang and Liu, 2009) need complete projected trees to train the,,
erivations of buf 9: Output: the best derivation of V[1, |x|] 10: function DERIV(p, c) 11: d p c {(p root, c root)} new derivation 12: d evl EVAL(d) evaluation function 13: return d used in CITATIONb), is also applicable here.,,
Here, V[i, j] contains the candidate parsing segments of the span [i, j], and the function EVAL(d) accumulates the scores of all the edges in dependency segment d. In practice, the cube-pruning strategy CITATION is used to speed up the enumeration of derivations (loops started by line 4 and 5).,,
In order to alleviate the effect of word alignment errors, we base the projection on the alignment matrix, a compact representation of multiple GIZA++ CITATION results, rather than a single word alignment in previous dependency projection works.,,
For example, they can be integrated deeply at each decoding step (CITATION; CITATION; CITATION), or can be integrated shallowly in a reranking manner (CITATION; CITATION).,,
We choose the 2nd-ordered MST model CITATION as the baseline.,,
For example, the unsupervised dependency parsing CITATION which is totally based on unannotated data, and the semisupervised dependency parsing CITATION which is based on both annotated and unannotated data.,,
Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (CITATION; CITATION; CITATION; CITATION; CITATION).,,
For dependency projection, the relationship between words in the parsed sentences can be simply projected across the word alignment to words in the unparsed sentences, according to the DCA assumption CITATION.,,
To tackle this problem, CITATION use some filtering rules to reduc,,
For example, the unsupervised dependency parsing CITATION which is totally based on unannotated data, and the semisupervised dependency parsing CITATION which is based on both annotated and unannotated data.,,
Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (CITATION; CITATION; CITATION; CITATION; CITATION).,,
For dependency projection, the relationship between words in the parsed sentences can be simply projected across the word alignment to words in the unparsed sentences, according to the DCA assumption CITATION.,,
To tackle this problem, CITATION use some filtering rules to reduce noise, and some,,
Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (CITATION; CITATION).,,
CITATION aims to obtain Chinese bracketing knowledge via ITG CITATION alignment.,,
CITATION and CITATION induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity.,,
CITATION perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar features.,,
CITATION refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees.,,
All previous works for dependency projection (CITATION; CITATION; CITATION; Jiang and Liu, 2009) need complete pr,,
The alignment matrixes for sentence pairs are generated according to CITATION.,,
We extract a series of classification instance sets Corpus System P % CTB 2.0 CITATION 53.9 our model 56.9 CTB 5.0 CITATION 53.28 our model 58.59 Table 4: The performance of the projected classifier on the test sets of CTB 2.0 and CTB 5.0, compared with the performance of previous works on the corresponding test sets.,,
across the word alignment to words in the unparsed sentences, according to the DCA assumption CITATION.,,
To tackle this problem, CITATION use some filtering rules to reduce noise, and some hand-designed rules to handle language heterogeneity.,,
CITATION perform dependency projection and annotation adaptation with quasi-synchronous grammar features.,,
CITATION resort to a dynamic programming procedure to search for a completed projected tree.,,
CITATION aims to obtain Chinese bracketing knowledge via ITG CITATION alignment.,,
CITATION and CITATION induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity.,,
CITATION perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar features.,,
CITATION refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees.,,
All previous works for dependency projection (CITATION; CITATION; CITATION; Jiang and Liu, 2009) need complete projected trees to train the projected parsers.,,
This strategy makes better use of the projected parser while with faster decoding, compared with the cascaded approach of CITATION.,,
pairs are generated according to CITATION.,,
We extract a series of classification instance sets Corpus System P % CTB 2.0 CITATION 53.9 our model 56.9 CTB 5.0 CITATION 53.28 our model 58.59 Table 4: The performance of the projected classifier on the test sets of CTB 2.0 and CTB 5.0, compared with the performance of previous works on the corresponding test sets.,,
the unsupervised dependency parsing CITATION which is totally based on unannotated data, and the semisupervised dependency parsing CITATION which is based on both annotated and unannotated data.,,
Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (CITATION; CITATION; CITATION; CITATION; CITATION).,,
For dependency projection, the relationship between words in the parsed sentences can be simply projected across the word alignment to words in the unparsed sentences, according to the DCA assumption CITATION.,,
To tackle this problem, CITATION use some filtering rules to reduce noise, and some hand-designed rules to handle language heterogeneity.,,
1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (CITATIONa; CITATION; CITATION).,,
For example, the unsupervised dependency parsing CITATION which is totally based on unannotated data, and the semisupervised dependency parsing CITATION which is based on both annotated and unannotated data.,,
Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (CITATION; CITATION; CITATION; CITATION; CITATION).,,
1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (CITATIONa; CITATION; CITATION).,,
For example, the unsupervised dependency parsing CITATION which is totally based on unannotated data, and the semisupervised dependency parsing CITATION which is based on both annotated and unannotated data.,,
Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (CITATION; CITATION; CITATION; CITATION; CITATION).,,
Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (CITATION; CITATION).,,
CITATION aims to obtain Chinese bracketing knowledge via ITG CITATION alignment.,,
CITATION and CITATION induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity.,,
CITATION perform dependency projection and annotation adaptation with Quasi-Synchron,,
Both English and Chinese sentences are tagged by the implementations of the POS tagger of CITATION, which trained on WSJ and CTB 5.0 respectively.,,
The English sentences are then parsed by an implementation of 2nd-ordered MST model of CITATION, which is trained on dependency trees extracted from WSJ.,,
The alignment matrixes for sentence pairs are generated according to CITATION.,,
We extract a series of classification instance sets Corpus System P % CTB 2.0 CITATION 53.9 our model 56.9 CTB 5.0 CITATION 53.28 our model 58.59 Table 4:,,
Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (CITATION; CITATION).,,
CITATION aims to obtain Chinese bracketing knowledge via ITG CITATION alignment.,,
CITATION and CITATION induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity.,,
CITATION perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar features.,,
CITATION refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees.,,
6.1 Word-Pair Classification Model We experiment on two popular treebanks, the Wall Street Journal (WSJ) portion of the Penn English Treebank CITATION, and the Penn Chinese Treebank (CTB) 5.0 CITATION.,,
The constituent trees in the two treebanks are transformed to dependency trees according to the headfinding rules of CITATION.,,
For English, we use the automatically-assigned POS tags produced by an implementation of the POS tagger of CITATION.,,
1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (CITATIONa; CITATION; CITATION).,,
For example, the unsupervised dependency parsing CITATION which is totally based on unannotated data, and the semisupervised dependency parsing CITATION which is based on both annotated and unannotated data.,,
More importantly, when this classifier is integrated into a 2nd-ordered maximum spanning tree (MST) dependency parser CITATION in a weighted average manner, significant improvement is obtained over the MST baselines.,,
an be integrated deeply at each decoding step (CITATION; CITATION; CITATION), or can be integrated shallowly in a reranking manner (CITATION; CITATION).,,
We choose the 2nd-ordered MST model CITATION as the baseline.,,
The relative weight is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training CITATION.,,
5 Related Works 5.1 Dependency Parsing Both the graph-based (CITATIONa; CITATION; CITATION) and the transition-based (CITATION; CITATION) parsing algorithms are related to our word-pair classification model.,,
Both English and Chinese sentences are tagged by the implementations of the POS tagger of CITATION, which trained on WSJ and CTB 5.0 respectively.,,
The English sentences are then parsed by an implementation of 2nd-ordered MST model of CITATION, which is trained on dependency trees extracted from WSJ.,,
The alignment matrixes for sentence pairs are generated according to CITATION.,,
First, we implement a chart-based dynamic programming parser for the 2nd-ordered MST model, and develop a training procedure based on the perceptron algorithm with averaged parameters CITATION.,,
On the WSJ corpus, this parser achieves the same performance as that of CITATION.,,
The weight parameter is tuned by a minimum error-rate training algorithm CITATION.,,
1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (CITATIONa; CITATION; CITATION).,,
For example, the unsupervised dependency parsing CITATION which is totally based on unannotated data, and the semisupervised dependency parsing CITATION which is based on both annotated and unannotated data.,,
2 Word-Pair Classification Model 2.1 Model Definition Following (CITATIONa), x is used to denote the sentence to be parsed, and xi to denote the i-th word in the sentence.,,
The classification result C(i, j) can be a boolean value: C(i, j) = p p {0, 1} (1) as produced by a support vector machine (SVM) classifier CITATION.,,
The dependency probability can then be defined as: C(i, j) = exp(w f(i, j, +)) P r exp(w f(i, j, r)) = exp( P k wk fk(i, j, +)) P r exp( P k wk fk(i, j, r)) (5) 2.2 Features for Classification The feature templates for the classifier are similar to those of 1st-ordered MST model (CITATIONa).,,
Previous graph-based dependency models usually use the index distance of word i and word j 1 We exclude the in between features of CITATIONa) since preliminary experiments show that these features bring no improvement to the word-pair classification model.,,
1: Input: sentence x to be parsed 2: for hi, ji h1, |x|i in topological order do 3: buf 4: for k i..j 1 do all partitions 5: for l V[i, k] and r V[k + 1, j] do 6: insert DERIV(l, r) into buf 7: insert DERIV(r, l) into buf 8: V[i, j] top K derivations of buf 9: Output: the best derivation of V[1, |x|] 10: function DERIV(p, c) 11: d p c {(p root, c root)} new derivation 12: d evl EVAL(d) evaluation function 13: return d used in CITATIONb), is also applicable here.,,
Here, V[i, j] contains the candidate parsing segments of the span [i, j], and the function EVAL(d) accumulates the scores of all the edges in dependency segment d. In practice, the cube-pruning strategy CITATION is used to speed up the enumeration of derivations (loops started by line 4 and 5).,,
We choose the 2nd-ordered MST model CITATION as the baseline.,,
The relative weight is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training CITATION.,,
5 Related Works 5.1 Dependency Parsing Both the graph-based (CITATIONa; CITATION; CITATION) and the transition-based (CITATION; CITATION) parsing algorithms are related to our word-pair classification model.,,
1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (CITATIONa; CITATION; CITATION).,,
For example, the unsupervised dependency parsing CITATION which is totally based on unannotated data, and the semisupervised dependency parsing CITATION which is based on both annotated and unannotated data.,,
2 Word-Pair Classification Model 2.1 Model Definition Following (CITATIONa), x is used to denote the sentence to be parsed, and xi to denote the i-th word in the sentence.,,
The classification result C(i, j) can be a boolean value: C(i, j) = p p {0, 1} (1) as produced by a support vector machine (SVM) classifier CITATION.,,
The dependency probability can then be defined as: C(i, j) = exp(w f(i, j, +)) P r exp(w f(i, j, r)) = exp( P k wk fk(i, j, +)) P r exp( P k wk fk(i, j, r)) (5) 2.2 Features for Classification The feature templates for the classifier are similar to those of 1st-ordered MST model (CITATIONa).,,
Previous graph-based dependency models usually use the index distance of word i and word j 1 We exclude the in between features of CITATIONa) since preliminary experiments show that these features bring no improvement to the word-pair classification model.,,
1: Input: sentence x to be parsed 2: for hi, ji h1, |x|i in topological order do 3: buf 4: for k i..j 1 do all partitions 5: for l V[i, k] and r V[k + 1, j] do 6: insert DERIV(l, r) into buf 7: insert DERIV(r, l) into buf 8: V[i, j] top K derivations of buf 9: Output: the best derivation of V[1, |x|] 10: function DERIV(p, c) 11: d p c {(p root, c root)} new derivation 12: d evl EVAL(d) evaluation function 13: return d used in CITATIONb), is also applicable here.,,
Here, V[i, j] contains the candidate parsing segments of the span [i, j], and the function EVAL(d) accumulates the scores of all the edges in dependency segment d. In practice, the cube-pruning strategy CITATION is used to speed up the enumeration of derivations (loops started by line 4 and 5).,,
We choose the 2nd-ordered MST model CITATION as the baseline.,,
The relative weight is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training CITATION.,,
5 Related Works 5.1 Dependency Parsing Both the graph-based (CITATIONa; CITATION; CITATION) and the transition-based (CITATION; CITATION) parsing algorithms are related to our word-pair classification model.,,
84 84.5 85 85.5 86 86.5 87 1 1.5 2 2.5 3 Dependency Precision (%) Ratio r (#negative/#positive) WSJ CTB 5.0 Figure 3: Performance curves of the word-pair classification model on the development sets of WSJ and CTB 5.0, with respect to a series of ratio r. Corpus System P % WSJ CITATION 90.3 CITATION 87.3 1st-ordered MST 90.7 2nd-ordered MST 91.5 our model 86.8 CTB 5.0 1st-ordered MST 86.53 2nd-ordered MST 87.15 our model 82.06 Table 3: Performance of the word-pair classification model on WSJ and CTB 5.0, compared with the current state-of-the-art models.,,
1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (CITATIONa; CITATION; CITATION).,,
For example, the unsupervised dependency parsing CITATION which is totally based on unannotated data, and the semisupervised dependency parsing CITATION which is based on both annotated and unannotated data.,,
We choose the 2nd-ordered MST model CITATION as the baseline.,,
The relative weight is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training CITATION.,,
5 Related Works 5.1 Dependency Parsing Both the graph-based (CITATIONa; CITATION; CITATION) and the transition-based (CITATION; CITATION) parsing algorithms are related to our word-pair classification model.,,
andidate parsing segments of the span [i, j], and the function EVAL(d) accumulates the scores of all the edges in dependency segment d. In practice, the cube-pruning strategy CITATION is used to speed up the enumeration of derivations (loops started by line 4 and 5).,,
In order to alleviate the effect of word alignment errors, we base the projection on the alignment matrix, a compact representation of multiple GIZA++ CITATION results, rather than a single word alignment in previous dependency projection works.,,
We choose the 2nd-ordered MST model CITATION as the baseline.,,
The relative weight is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training CITATION.,,
5 Related Works 5.1 Dependency Parsing Both the graph-based (CITATIONa; CITATION; CITATION) and the transition-based (CITATION; CITATION) parsing algorithms are related to our word-pair classification model.,,
rser for the 2nd-ordered MST model, and develop a training procedure based on the perceptron algorithm with averaged parameters CITATION.,,
On the WSJ corpus, this parser achieves the same performance as that of CITATION.,,
The weight parameter is tuned by a minimum error-rate training algorithm CITATION.,,
For example, the unsupervised dependency parsing CITATION which is totally based on unannotated data, and the semisupervised dependency parsing CITATION which is based on both annotated and unannotated data.,,
Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (CITATION; CITATION; CITATION; CITATION; CITATION).,,
For dependency projection, the relationship between words in the parsed sentences can be simply projected across the word alignment to words in the unparsed sentences, according to the DCA assumption CITATION.,,
To tackle this problem, CITATION use some filtering rules to reduce noise, and some hand-designed rules to handle language heterog,,
has also been used in Japanese dependency parsing (CITATION; CITATION).,,
CITATION aims to obtain Chinese bracketing knowledge via ITG CITATION alignment.,,
CITATION and CITATION induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity.,,
CITATION perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar features.,,
CITATION refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees.,,
All previous works for dependency projection (CITATION; CITATION; CITATION; Jiang and Liu, 2009) need complete projected trees to train the projected parsers.,,
Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (CITATION; CITATION).,,
CITATION aims to obtain Chinese bracketing knowledge via ITG CITATION alignment.,,
CITATION and CITATION induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity.,,
CITATION perform dependency projection and annotation ada,,
odel 2.1 Model Definition Following (CITATIONa), x is used to denote the sentence to be parsed, and xi to denote the i-th word in the sentence.,,
The classification result C(i, j) can be a boolean value: C(i, j) = p p {0, 1} (1) as produced by a support vector machine (SVM) classifier CITATION.,,
C(i, j) can also be a realvalued probability: C(i, j) = p 0 p 1 (2) as produced by an maximum entropy (ME) classifier CITATION.,,
Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (CITATION; CITATION).,,
CITATION aims to obtain Chinese bracketing knowledge via ITG CITATION alignment.,,
CITATION and CITATION induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity.,,
CITATION perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar features.,,
CITATION refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees.,,
All previous works for dependency projection (CITATION; CITATION; CITATION; Jiang an,,
6.1 Word-Pair Classification Model We experiment on two popular treebanks, the Wall Street Journal (WSJ) portion of the Penn English Treebank CITATION, and the Penn Chinese Treebank (CTB) 5.0 CITATION.,,
The constituent trees in the two treebanks are transformed to dependency trees according to the headfinding rules of CITATION.,,
For English, we use the automatically-assigned POS tags produced by an implementation of the POS tagger of CITATION.,,
We choose the 2nd-ordered MST model CITATION as the baseline.,,
The relative weight is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training CITATION.,,
5 Related Works 5.1 Dependency Parsing Both the graph-based (CITATIONa; CITATION; CITATION) and the transition-based (CITATION; CITATION) parsing algorithms are related to our word-pair classification model.,,
6.1 Word-Pair Classification Model We experiment on two popular treebanks, the Wall Street Journal (WSJ) portion of the Penn English Treebank CITATION, and the Penn Chinese Treebank (CTB) 5.0 CITATION.,,
The constituent trees in the two treebanks are transformed to dependency trees according to the headfinding rules of CITATION.,,
For English, we use the automatically-assigned POS tags produced by an implementation of the POS tagger of CITATION.,,
For example, they can be integrated deeply at each decoding step (CITATION; CITATION; CITATION), or can be integrated shallowly in a reranking manner (CITATION; CITATION).,,
We choose the 2nd-ordered MST model CITATION as the baseline.,,
