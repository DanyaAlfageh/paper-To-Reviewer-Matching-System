CITATION uses this technique to merge different levels of predictors for word segmentation,,
This type of constraint may come from human input solicited in interactive inference procedure CITATION,,
We use the same Viterbi decoder as implemented for English POS tagging and use a noncommercial ILP solver included in GNU Linear Pro1060 \x0cprecision recall F-measure Viterbi 0.971 0.966 0.968 ILP 0.970 0.977 0.974 (CITATIONa), POS- 0.971 (CITATIONa), POS+ 0.973 Table 10: F-measure on Chinese word segmentation,,
Following (CITATIONa), we divide this corpus into training set (chapters 1-260), development set (chapters 2,,
Character-based feature templates We adopt the non-lexical-target feature templates in (CITATIONa),,
When no further data is used other than training data, the bidirectional tagger described in CITATION achives an accuracy of 97.33%, using a much richer feature set (E) than feature set ,,
The baseline for speed in all cases is the unconstrained tagger using CITATIONs feature and conducting a beam (=5) search,,
For example, CITATIONa) combine different levels of knowledge in an outside linear model of a twolayer cascaded model; CITATIONb) uses the forest re-ranking technique CITATION; and in CITATION, only known words in vocabulary are included in the hybrid lattice consisting of both character- and word-level nodes,,
When no further data is used other than training data, the bidirectional tagger described in CITATION achives an accuracy of 97.33%, using a much richer feature set (E) than feature set B, the one we compare with here,,
CITATION uses the stacked learning technique to merge different levels of predictors, obtaining a combined system that beats individual ones,,
CITATION and CITATION, we divide this corpus into training set (sections 0-18), development set (sections 19-21) and the final test set (sections 22-24),,
In early work, rule-based models find words one by one based on heuristics such as forward maximum match CITATION,,
This replicates the feature set B used in CITATION,,
1059 \x0cCITATIONs feature Beam=1 Beam=5 raw 96.46%/3 97.16/1 constrained 96.80%/14 97.20/10 Feature B in CITATION CITATION 97.15% (Beam=3) constrained 97.03%/11 97.20/8 Table 9: POS tagging accuracy and speed,,
F-measure 0.978 by the cascaded model in (CITATIONa),,
CITATION, (CITATIONa), and CITATION,,
We adopt the basic feature set used in CITATION and CITATION,,
As introduced in Section 2.2, we adopt a very compact feature set used in CITATION1,,
Based on experiments carried out 1 Our implementation of this feature set is basically the same as the version used in CITATION,,
parameter estimation of , we use the averaged perceptron as described in CITATION,,
Exact search is possible with a Viterbi-style algorithm, but beamsearch decoding is more popular as used in CITATION and (CITATIONa),,
1059 \x0cCITATIONs feature Beam=1 Beam=5 raw 96.46%/3 97.16/1 constrained 96.80%/14 97.20/,,
1054 \x0c2 English POS tagging 2.1 Explore deterministic constraints Suppose that, following CITATION, we distinguish major lexical categories (Noun, Verb, Adjective and Preposition) by two binary features: + |N and + |V,,
3 Chinese Word Segmentation (CWS) 3.1 Word segmentation as character tagging Considering the ambiguity problem that a Chinese character may appear in any relative position in a word and the out-of-vocabulary (OOV) problem that it is impossible to observe all words in training data, CWS is widely formulated as a character tagging problem CITATION,,
Following (CITATIONa), we divide this corpus into training set (chapters 1-260), development set (chapters 271-300) and the final test set (chapters 301-325),,
When beam-width is set to 5, tagging accuracy is not improved by the use of Feature B in CITATION; and because the size of the feature model grows, efficiency is hurt,,
As discussed in CITATION, categorical information of neighbouring words on both sides of w0 help resolve POS ambiguity of w0,,
The word count number is further processed following CITATION, wc(w) = floor(log(count(w)) 5)/5,,
1 introduction In recent work, interesting results are reported for applications of integer linear programming (ILP) such as semantic role labeling (SRL) CITATION, dependency parsing CITATION and so on,,
CITATION,,
These results are compared to the core perceptron trained without POS in (CITATIONa),,
As compared to the constrained greedy tagger using CITATIONs feature set, with the additional use of three locally lookahead feature templates, tagging accuracy is increased from 96.80% to 97.02%,,
On the other hand, consider the annotation guideline of English Treebank CITATION instead,,
In CITATION, lookahead features may be available for use during decoding since searching is bidirectional instead of left-to-right as in Viterbi decoding,,
Our baseline model, which uses CITATIONs feature set and conducts a beam (=5) search in the unconstrained,,
