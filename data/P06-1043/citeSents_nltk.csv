The BROWN parsing model is naturally better than the WSJ model for this task, but combining the two training corpora results in a better model (as in CITATION),,
Recent work, CITATION, has shown that adding many millions of words of machine parsed and reranked LA Times articles does, in fact, improve performance of the parser on the closely related WSJ data,,
But the lack of corpora has led to a situation where much of the current work on parsing is performed on a single domain using training data from that domain the Wall Street Journal (WSJ) section of the Penn Treebank CITATION,,
To use the data from NANC, we use self-training CITATION,,
4 Experiments We use the CITATION reranking parser in our experiments,,
The trends are the same as in CITATION: Adding NANC data improves parsing performance on BROWN development considerably, improving the f-score from 83.9% to 86.4%,,
3.1 Brown The BROWN corpus CITATION consists of many different genres of text, intended to approximate a balanced corpus,,
However, the work which is most directly comparable to ours is that of (CITATION; CITATION; CITATION; CITATION),,
3.2 Wall Street Journal Our out-of-domain data is the Wall Street Journal (WSJ) portion of the Penn Treebank CITATION which consists of about 40,000 sentences (one million words) annotated with syntactic information,,
While CITATION showed that this technique was effective when testing on WSJ, the true distribution was closer to WSJ so it made sense to emphasize it,,
5.3 Statistical Analysis We conducted randomization tests for the significance of the difference in corpus f-score, based on the randomization version of the paired sample ttest described by CITATION,,
We concentrate particularly on the work of (CITATION; CITATION) as they provide results which are directly comparable to those presented in this paper,,
The need to parse biomedical literature inspires (CITATION; CITATION),,
The first of these, parse-reranking (CITATION; CITATION) starts with a standard generative parser, but uses it to generate the n-best parses rather than a single parse,,
both the parser and reranker are built from WSJ training data) and parse the sentences from NANC with the 50-best CITATION parser,,
CITATION observes that for parsing a specific domain, data from that domain is most beneficial, followed by data from the same class, data from a different class, and data from a different domain,,
The unlabeled data is the North American News Corpus, NANC CITATION, which is approximately 24 million unlabeled sentences from various news sources,,
We use the same divisions as CITATION, who base their divisions on CITATION,,
CITATION applies self-training to parser adaptation to utilize unlabeled in-domain data,,
In particular, we show that the reranking parser described in CITATION improves performance of the parser on Brown to 85.2%,,
Thus, the WSJ+NANC model has better oracle rates than the WSJ model CITATION for both the WSJ and BROWN domains,,
CITATION apply cotraining to parser adaptation and find that cotraining can work across domains,,
CITATION apply cotraining to parser ad,,
Furthermore, use of the self-training techniques described in CITATION raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data,,
We have already seen the results CITATION and CITATION achieve in Table 1,,
