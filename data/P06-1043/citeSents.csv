 However, the work which is most directly comparable to ours is that of (CITATION; CITATION; CITATION; CITATION),,
 We concentrate particularly on the work of (CITATION; CITATION) as they provide results which are directly comparable to those presented in this paper,,
1 Brown The BROWN corpus CITATION consists of many different genres of text, intended to approximate a balanced corpus,,
 We use the same divisions as CITATION, who base their divisions on CITATION,,
2 Wall Street Journal Our out-of-domain data is the Wall Street Journal (WSJ) portion of the Penn Treebank CITATION which consists of about 40,000 sentences (one million words) annotated with syntactic information,,
 CITATION applies self-training to parser adaptation to utilize unlabeled in-domain data,,
 We have already seen the results CITATION and CITATION achieve in Table 1,,
 In particular, we show that the reranking parser described in CITATION improves performance of the parser on Brown to 85,,
 Furthermore, use of the self-training techniques described in CITATION raise this to 87,,
 The first of these, parse-reranking (CITATION; CITATION) starts with a standard generative parser, but uses it to generate the n-best parses rather than a single parse,,
 To use the data from NANC, we use self-training CITATION,,
 both the parser and reranker are built from WSJ training data) and parse the sentences from NANC with the 50-best CITATION parser,,
 4 Experiments We use the CITATION reranking parser in our experiments,,
 CITATION apply cotraining to parser adaptation and find that cotraining can work across domains,,
 The need to parse biomedical literature inspires (CITATION; CITATION),,
3 Statistical Analysis We conducted randomization tests for the significance of the difference in corpus f-score, based on the randomization version of the paired sample ttest described by CITATION,,
 The first of these, parse-reranking (CITATION; CITATION) starts with a standard generative parser, but uses it to generate the n-best parses rather than a single parse,,
1 Brown The BROWN corpus CITATION consists of many different genres of text, intended to approximate a balanced corpus,,
 We use the same divisions as CITATION, who base their divisions on CITATION,,
 However, the work which is most directly comparable to ours is that of (CITATION; CITATION; CITATION; CITATION),,
 We concentrate particularly on the work of (CITATION; CITATION) as they provide results which are directly comparable to those presented in this paper,,
1 Brown The BROWN corpus CITATION consists of many different genres of text, intended to approximate a balanced corpus,,
 We use the same divisions as CITATION, who base their divisions on CITATION,,
2 Wall Street Journal Our out-of-domain data is the Wall Street Journal (WSJ) portion of the Penn Treebank CITATION which consists of about 40,000 sentences (one million words) annotated with syntactic information,,
 We have already seen the results CITATION and CITATION achieve in Table 1,,
 The BROWN parsing model is naturally better than the WSJ model for this task, but combining the two training corpora results in a better model (as in CITATION),,
2 Wall Street Journal Our out-of-domain data is the Wall Street Journal (WSJ) portion of the Penn Treebank CITATION which consists of about 40,000 sentences (one million words) annotated with syntactic information,,
 The unlabeled data is the North American News Corpus, NANC CITATION, which is approximately 24 million unlabeled sentences from various news sources,,
 To use the data from NANC, we use self-training CITATION,,
 However, the work which is most directly comparable to ours is that of (CITATION; CITATION; CITATION; CITATION),,
 We concentrate particularly on the work of (CITATION; CITATION) as they provide results which are directly comparable to those presented in this paper,,
 CITATION apply cotraining to parser adaptation and find that cotraining can work across domains,,
 The need to parse biomedical literature inspires (CITATION; CITATION),,
 But the lack of corpora has led to a situation where much of the current work on parsing is performed on a single domain using training data from that domain the Wall Street Journal (WSJ) section of the Penn Treebank CITATION,,
 We use the same divisions as CITATION, who base their divisions on CITATION,,
2 Wall Street Journal Our out-of-domain data is the Wall Street Journal (WSJ) portion of the Penn Treebank CITATION which consists of about 40,000 sentences (one million words) annotated with syntactic information,,
 The unlabeled data is the North American News Corpus, NANC CITATION, which is approximately 24 million unlabeled sentences from various news sources,,
 In particular, we show that the reranking parser described in CITATION improves performance of the parser on Brown to 85,,
 Furthermore, use of the self-training techniques described in CITATION raise this to 87,,
 Recent work, CITATION, has shown that adding many millions of words of machine parsed and reranked LA Times articles does, in fact, improve performance of the parser on the closely related WSJ data,,
 The unlabeled data is the North American News Corpus, NANC CITATION, which is approximately 24 million unlabeled sentences from various news sources,,
 To use the data from NANC, we use self-training CITATION,,
 both the parser and reranker are built from WSJ training data) and parse the sentences from NANC with the 50-best CITATION parser,,
 4 Experiments We use the CITATION reranking parser in our experiments,,
 The trends are the same as in CITATION: Adding NANC data improves parsing performance on BROWN development considerably, improving the f-score from 83,,
 While CITATION showed that this technique was effective when testing on WSJ, the true distribution was closer to WSJ so it made sense to emphasize it,,
 Thus, the WSJ+NANC model has better oracle rates than the WSJ model CITATION for both the WSJ and BROWN domains,,
 However, the work which is most directly comparable to ours is that of (CITATION; CITATION; CITATION; CITATION),,
 We concentrate particularly on the work of (CITATION; CITATION) as they provide results which are directly comparable to those presented in this paper,,
 CITATION observes that for parsing a specific domain, data from that domain is most beneficial, followed by data from the same class, data from a different class, and data from a different domain,,
 CITATION apply cotraining to parser ad,,
 CITATION observes that for parsing a specific domain, data from that domain is most beneficial, followed by data from the same class, data from a different class, and data from a different domain,,
 CITATION apply cotraining to parser adaptation and find that cotraining can work across domains,,
 The need to parse biomedical literature inspires (CITATION; CITATION),,
