Moreover, CITATION show that a naive co-training process that does not explicitly seek to maximize agreement on unlabelled data can lead to similar performance, at a much lower computational cost,,
However, as theoretically shown \x0cin CITATION, and then empirically in CITATION, co-training still works under a weaker indep,,
CITATION define self-training as a single-view weakly supervised algorithm, build by training a committee of classifiers using bagging, combined with majority voting for final label selection,,
In the original definition of co-training, CITATION state conditional independence of the views as a required criterion for co-training to work,,
In recent work, CITATION shows that the independence assumption can be relaxed, and co-training is still effective under a wea,,
CITATION provide a different definition: self-training is performed using a tagger that is retrained on its own labeled cache on each round,,
A comparative analysis of words that benefit from basic/smoothed co-training with global parameter settings, versus words with little or no improvement obtained through bootstrapping reveals several observations: (1) Words with accurate basic classifiers cannot be improved through co-training, which agrees with previous observations CITATION,,
As previously noticed CITATION, it is hard to identify conditionally independent views for real-data problems,,
In natural language learning, co-training was applied to statistical parsing CITATION, reference resolution CITATION, part of speech tagging CITATION, and others, and was generally found to bring improvement over the case when no additional unlabeled data are used,,
ver, CITATION show that a naive co-training process that does not explicitly seek to maximize agreement on unlabelled data can lead to similar performance, at a much lower computational cost,,
CITATION give a comprehensive examination of learning methods and their combination,,
2.1 Co-training Starting with a set of labeled data, co-training algorithms CITATION attempt to increase the amount of annotated data using some (large) amounts of unlabeled data,,
This is fundamentally different from the approach proposed in CITATION, where they also apply majority voting in a bootstrapping framework, but in a different setting,,
abeled data, co-training algorithms CITATION attempt to increase the amount of annotated data using some (large) amounts of unlabeled data,,
 Co-training Starting with a set of labeled data, co-training algorithms CITATION attempt to increase the amount of annotated data using some (large) amounts of unlabeled data,,
However, as theoretically shown \x0cin CITATION, and then empirically in CITATION, co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations,,
The fact that no improvement was obtained agrees with previous observations that classifiers that are too accurate cannot be improved with bootstrapping CITATION,,
In natural language learning, co-training was applied to statistical parsing CITATION, reference resolution CITATION, part of speech tagging (Clark et al., 2,,
Using the domains attached to word senses, as introduced in CITATION, we observed that words that have a large subset of their senses not belonging to a specific domain (e.g,,
In recent work, CITATION shows that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption,,
Table 1 lists commonly used features in word sense disambiguation (list drawn from a larger set of features compiled by CITATION),,
We adopt this second definition, which also agrees with the definition given in CITATION,,
As previously noticed CITATION, there is no principled method for selecting optimal values for these parameters, which is an important disadvantage of these algorithms,,
We use Naive Bayes, since it was previously shown that in combination with the features we consider, can lead to a state-of-the-art disambiguation system CITATION,,
