 To score the output of a coreference model, we employ three scoring programs: MUC CITATION, B3 CITATION, and 3-CEAF CITATION,,
, unmapped) mentions CITATION,,
, CITATION) and learning-based techniques such as rule learning (e,,
, CITATION), kernels (e,,
, CITATION), and distributional methods (e,,
, CITATION),,
, CITATION) and unsupervised methods (e,,
, CITATION),,
, CITATION) and modeled generatively (e,,
, CITATION) and discriminatively (e,,
, CITATION),,
, CITATION, CITATIONb), CITATION), and are computed automatically,,
, CITATION) and learning-based techniques such as rule learning (e,,
, CITATION), kernels (e,,
, CITATION), and distributional methods (e,,
, CITATION),,
, CITATION) and unsupervised methods (e,,
, CITATION),,
, CITATION) and modeled generatively (e,,
, CITATION) and discriminatively (e,,
, CITATION),,
, CITATION) and learning-based techniques such as rule learning (e,,
, CITATION), kernels (e,,
, CITATION), and distributional methods (e,,
, CITATION),,
, CITATION) and unsupervised methods (e,,
, CITATION),,
, CITATION) and modeled generatively (e,,
, CITATION) and discriminatively (e,,
, CITATION),,
, CITATION),,
 Motivated by previous work (CITATION; CITATION; CITATION), we create cluster-level features from mention-pair features using four predicates: NONE, MOST-FALSE, MOST-TRUE, and ALL,,
, CITATION) and modeled generatively (e,,
, CITATION) and discriminatively (e,,
, CITATION),,
, CITATION),,
, CITATION),,
 The notion of ranking candidate antecedents can be traced back to centering algorithms, many of which use grammatical roles to rank forward-looking centers (see CITATION, CITATION, and CITATION),,
 As mentioned before, CITATION train a mention-ranking model,,
, CITATION) and learning-based techniques such as rule learning (e,,
, CITATION), kernels (e,,
 Given an active mention mk, we follow CITATION and use an independently-trained classifier to determine whether mk is discourse-new,,
 The discourse-new classifier used in the resolution step is trained with 26 of the 37 features2 described in CITATIONa) that are deemed useful for distinguishing between anaphoric and non-anaphoric mentions,,
, CITATION) and learning-based techniques such as rule learning (e,,
, CITATION), kernels (e,,
, CITATION), and distributional methods (e,,
, CITATION),,
, CITATION) and unsupervised methods (e,,
, CITATION),,
, CITATION) and modeled generatively (e,,
, CITATION) and discriminatively (e,,
, CITATION),,
, CITATION),,
 (1999) and CITATION, as described below,,
 Grammatical (1): The part-of-speech (POS) tag of wi obtained using the Stanford log-linear POS tagger CITATION,,
 Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer CITATION,,
 Following CITATION, we recast mention extraction as a sequence labeling task, where we assign to each token in a test text a label that indicates whether it begins a mention, is inside a mention, or is outside a mention,,
 (1999) and CITATION, as described below,,
 More recently, CITATION have proposed another entity-mention model trained by inductive logic programming,,
 The notion of ranking candidate antecedents can be traced back to centering algorithms, many of which use grammatical roles to rank forward-looking centers (see CITATION, CITATION, and CITATION),,
 As mentioned before, CITATION train a mention-ranking model,,
, CITATION), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors,,
 As a result, their cluster-ranking model employs only factors that capture the salience of a cluster, and can therefore be viewed as a simple model of attentional state (see CITATION) realized by coreference clusters,,
 CITATION represent one of the earliest attempts to investigate learning-based entity-mention models,,
 Following CITATION, we create (1) a positive instance for each discourse-old mention mk and its closest antecedent mj; and (2) a negative instance for mk paired with each of the intervening mentions, mj+1, mj+2, ,,
 To train a mention-pair classifier, we use the SVM learning algorithm from the SVMlight package CITATION, converting all multi-valued features into an equivalent set of binary-valued features,,
 We train our first baseline, the mention-pair coreference classifier, using the SVM learning algorithm as implemented in the SVMlight package CITATION,,
, CITATION, CITATIONb), CITATION, CITATION),,
 As mentioned before, CITATION train a mention-ranking model,,
, CITATION) and learning-based techniques such as rule learning (e,,
, CITATION), kernels (e,,
, CITATION), and distributional methods (e,,
, CITATION),,
, CITATION) and unsupervised methods (e,,
, CITATION),,
, CITATION) and modeled generatively (e,,
, CITATION) and discriminatively (e,,
, CITATION),,
 As mentioned previously, the work most related to ours is CITATION, whose goal is to perform pronoun resolution by assigning an anaphoric pronoun to the highest-scored preceding cluster,,
, CITATION, CITATION),,
 While entity-mention models have previously been shown to be worse or at best marginally better than their mention-pair counterparts (CITATION; CITATION), our cluster-ranking models, which are a natural extension of entity-mention models, significantly outperformed all competing approaches,,
ors that capture the salience of a cluster, and can therefore be viewed as a simple model of attentional state (see CITATION) realized by coreference clusters,,
 CITATION represent one of the earliest attempts to investigate learning-based entity-mention models,,
 Motivated by previous work (CITATION; CITATION; CITATION), we create cluster-level features from mention-pair features using four predicates: NONE, MOST-FALSE, MOST-TRUE, and ALL,,
 While the insignificant performance difference is somewhat surprising given the improved expressiveness of entitymention models over mention-pair models, similar trends have been reported by CITATION,,
 To score the output of a coreference model, we employ three scoring programs: MUC CITATION, B3 CITATION, and 3-CEAF CITATION,,
, unmapped) mentions CITATION,,
 Equally importantly, cluster rankers are conceptually simple and easy to implement and do not rely on sophisticated training and inference procedures to make coreference decisions in dependent relation to each other, unlike relational coreference models (see CITATION),,
 More recently, CITATION have proposed another entity-mention model trained by inductive logic programming,,
 The notion of ranking candidate antecedents can be traced back to centering algorithms, many of which use grammatical roles to rank forward-looking centers (see CITATION, CITATION, and CITATION),,
 As mentioned before, CITATION train a mention-ranking model,,
, CITATION), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors,,
 As a result, their cluster-ranking model employs only factors that capture the salience of a cluster, and can therefore be viewed as a simple model of attentional state (see CITATION) realized by coreference clusters,,
e, CITATION train a mention-ranking model,,
, CITATION) and learning-based techniques such as rule learning (e,,
, CITATION), kernels (e,,
, CITATION), and distributional methods (e,,
, CITATION),,
, CITATION) and unsupervised methods (e,,
, CITATION),,
, CITATION) and modeled generatively (e,,
, CITATION) and discriminatively (e,,
, CITATION),,
, CITATION, CITATIONb), CITATION, CITATION),,
 It is worth noting that researchers typically adopt a pipeline coreference architecture, performing discourse-new detection prior to coreference resolution and using the resulting information to prevent a coreference system from resolving mentions that are determined to be discourse-new (see CITATION for an overview),,
 As a result, errors in discourse-new detection could be propagated to the resolver, possibly leading to a deterioration of coreference performance (see CITATIONa)),,
, CITATION, CITATIONb), CITATION), and are computed automatically,,
 Given an active mention mk, we follow CITATION and use an independently-trained classifier to determine whether mk is discourse-new,,
 The discourse-new classifier used in the resolution step is trained with 26 of the 37 features2 described in CITATIONa) that are deemed useful for distinguishing between anaphoric and non-anaphoric mentions,,
, CITATION, CITATIONb), CITATION, CITATION),,
 It is worth noting that researchers typically adopt a pipeline coreference architecture, performing discourse-new detection prior to coreference resolution and using the resulting information to prevent a coreference system from resolving mentions that are determined to be discourse-new (see CITATION for an overview),,
 As a result, errors in discourse-new detection could be propagated to the resolver, possibly leading to a deterioration of coreference performance (see CITATIONa)),,
, CITATION, CITATIONb), CITATION), and are computed automatically,,
 Given an active mention mk, we follow CITATION and use an independently-trained classifier to determine whether mk is discourse-new,,
 The discourse-new classifier used in the resolution step is trained with 26 of the 37 features2 described in CITATIONa) that are deemed useful for distinguishing between anaphoric and non-anaphoric mentions,,
 For both types of mentions, the improvements over the corresponding results for the entity-mention baseline 7 We use Approximate Randomization CITATION for testing statistical significance, with p set to 0,,
 It is worth noting that researchers typically adopt a pipeline coreference architecture, performing discourse-new detection prior to coreference resolution and using the resulting information to prevent a coreference system from resolving mentions that are determined to be discourse-new (see CITATION for an overview),,
 As a result, errors in discourse-new detection could be propagated to the resolver, possibly leading to a deterioration of coreference performance (see CITATIONa)),,
, CITATION, CITATIONb), CITATION, CITATION),,
, CITATION, CITATIONb), CITATION, CITATION),,
, CITATION, CITATIONb), CITATION), and are computed automatically,,
oring programs: MUC CITATION, B3 CITATION, and 3-CEAF CITATION,,
, unmapped) mentions CITATION,,
 (1999) and CITATION, as described below,,
 Grammatical (1): The part-of-speech (POS) tag of wi obtained using the Stanford log-linear POS tagger CITATION,,
 Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer CITATION,,
, CITATION), kernels (e,,
, CITATION), and distributional methods (e,,
, CITATION),,
, CITATION) and unsupervised methods (e,,
, CITATION),,
, CITATION) and modeled generatively (e,,
, CITATION) and discriminatively (e,,
, CITATION),,
, CITATION),,
, CITATION) and learning-based techniques such as rule learning (e,,
, CITATION), kernels (e,,
, CITATION), and distributional methods (e,,
, CITATION),,
, CITATION) and unsupervised methods (e,,
, CITATION),,
, CITATION) and modeled generatively (e,,
, CITATION) and discriminatively (e,,
, CITATION),,
, CITATION) and learning-based techniques such as rule learning (e,,
, CITATION), kernels (e,,
, CITATION), and distributional methods (e,,
, CITATION),,
, CITATION) and unsupervised methods (e,,
, CITATION),,
, CITATION) and modeled generatively (e,,
, CITATION) and discriminatively (e,,
, CITATION),,
 To score the output of a coreference model, we employ three scoring programs: MUC CITATION, B3 CITATION, and 3-CEAF CITATION,,
, unmapped) mentions CITATION,,
 More recently, CITATION have proposed another entity-mention model trained by inductive logic programming,,
 The notion of ranking candidate antecedents can be traced back to centering algorithms, many of which use grammatical roles to rank forward-looking centers (see CITATION, CITATION, and CITATION),,
 As mentioned before, CITATION train a mention-ranking model,,
, CITATION, CITATION),,
 While entity-mention models have previously been shown to be worse or at best marginally better than their mention-pair counterparts (CITATION; CITATION), our cluster-ranking models, which are a natural extension of entity-mention models, significantly outperformed all competing approaches,,
 More recently, CITATION have proposed another entity-mention model trained by inductive logic programming,,
 The notion of ranking candidate antecedents can be traced back to centering algorithms, many of which use grammatical roles to rank forward-looking centers (see CITATION, CITATION, and CITATION),,
 Motivated by previous work (CITATION; CITATION; CITATION), we create cluster-level features from mention-pair features using four predicates: NONE, MOST-FALSE, MOST-TRUE, and ALL,,
