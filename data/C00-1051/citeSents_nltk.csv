2.2 Estimation of DPs Some of the state-of-the-art probabilistic language models such as the bottomup models P(Rjs) proposed by CITATION and Fujio et al.,,
CITATION directly estimate DPs for a given input, whereas other models such as PCFGbased topdown generation models P(R;s) do not (CITATION; CITATION; CITATION).,,
Note that, while we restrict our discussion to analysis of Japanese sentences in this paper, what we present below should also be straightforwardly applicable to more wideranged tasks such as English dependency analysis just like the problem setting considered by CITATION.,,
2.2 Estimation of DPs Some of the state-of-the-art probabilistic language models such as the bottomup models P(Rjs) proposed by CITATION and Fujio et al.,,
CITATION directly estimate DPs for a given input, whereas othe,,
ments 4.1 Settings We conducted experiments using the following \x0cve statistical parsers: Table 1: The total / 11-point accuracy achieved by each individual model total 11-point A 0.8974 0.9607 B 0.8551 0.9281 C 0.8586 0.9291 D 0.8470 0.9266 E 0.7885 0.8567 \x0f KANA CITATION: a bottom-up model based on maximum entropy estimation.,,
\x0f CHAGAKE (Fujio et al., 1998): an extension of the bottom-up model proposed by Collins CITATION.,,
\x0f Kanayama\&apos;s parser CITATION: a bottom-up model coupled with an HPSG.,,
\x0f Shirai\&apos;s parser CITATION: a topdown model incorporating lexical collocation statistics.,,
\x0f Peach Pie Parser CITATION: a bottom-up model based on maximum entropy estimation.,,
2.2 Estimation of DPs Some of the state-of-the-art probabilistic language models such as the bottomup models P(Rjs) proposed by CITATION and Fujio et al.,,
CITATION directly estimate DPs for a given input, whereas other models such as PCFGbased topdown generation models P(R;s) do not (CITATION; CITATION; CITATION).,,
4 Experiments 4.1 Settings We conducted experiments using the following \x0cve statistical parsers: Table 1: The total / 11-point accuracy achieved by each individual model total 11-point A 0.8974 0.9607 B 0.8551 0.9281 C 0.8586 0.9291 D 0.8470 0.9266 E 0.7885 0.8567 \x0f KANA CITATION: a bottom-up model based on maximum entropy estimation.,,
\x0f CHAGAKE (Fujio et al., 1998): an extension of the bottom-up model proposed by Collins CITATION.,,
\x0f Kanayama\&apos;s parser CITATION: a bottom-up model coupled with an HPSG.,,
\x0f Shirai\&apos;s parser CITATION: a topdown model incorporating lexical collocation statistics.,,
Recently, there have been various attempts to apply committee-based techniques to NLP tasks such as POS tagging (Halteren et al., 1998; Brill et al., 1998), parsing (Henderson and Brill, 1999), word sense disambiguation CITATION, machine translation CITATION, and speech recognition (Fiscus, 1997).,,
s just like the problem setting considered by CITATION.,,
2.2 Estimation of DPs Some of the state-of-the-art probabilistic language models such as the bottomup models P(Rjs) proposed by CITATION and Fujio et al.,,
CITATION directly estimate DPs for a given input, whereas other models such as PCFGbased topdown generation models P(R;s) do not (CITATION; CITATION; CITATION).,,
CITATION does not accepts multiple voting.,,
CITATION) where a parser selects as its output only a part of the parse tree that are probabilistically highly reliable.,,
ts using the following \x0cve statistical parsers: Table 1: The total / 11-point accuracy achieved by each individual model total 11-point A 0.8974 0.9607 B 0.8551 0.9281 C 0.8586 0.9291 D 0.8470 0.9266 E 0.7885 0.8567 \x0f KANA CITATION: a bottom-up model based on maximum entropy estimation.,,
\x0f CHAGAKE (Fujio et al., 1998): an extension of the bottom-up model proposed by Collins CITATION.,,
\x0f Kanayama\&apos;s parser CITATION: a bottom-up model coupled with an HPSG.,,
\x0f Shirai\&apos;s parser CITATION: a topdown model incorporating lexical collocation statistics.,,
\x0f Peach Pie Parser CITATION: a bottom-up model based on maximum entropy estimation.,,
The approximation error \x0f is given by \x0f \x14 PR0PRH PR , where PR is the probability mass of all the dependency structure candidates for s (see CITATION for the proof).,,
Recently, there have been various attempts to apply committee-based techniques to NLP tasks such as POS tagging (Halteren et al., 1998; Brill et al., 1998), parsing (Henderson and Brill, 1999), word sense disambiguation CITATION, machine translation CITATION, and speech recognition (Fiscus, 1997).,,
2.2 Estimation of DPs Some of the state-of-the-art probabilistic language models such as the bottomup models P(Rjs) proposed by CITATION and Fujio et al.,,
CITATION directly estimate DPs for a given input, whereas other models such as PCFGbased topdown generation models P(R;s) do not (CITATION; CITATION; CITATION).,,
cy achieved by each individual model total 11-point A 0.8974 0.9607 B 0.8551 0.9281 C 0.8586 0.9291 D 0.8470 0.9266 E 0.7885 0.8567 \x0f KANA CITATION: a bottom-up model based on maximum entropy estimation.,,
\x0f CHAGAKE (Fujio et al., 1998): an extension of the bottom-up model proposed by Collins CITATION.,,
\x0f Kanayama\&apos;s parser CITATION: a bottom-up model coupled with an HPSG.,,
\x0f Shirai\&apos;s parser CITATION: a topdown model incorporating lexical collocation statistics.,,
\x0f Peach Pie Parser CITATION: a bottom-up model based on maximum entropy estimation.,,
\x0f CHAGAKE (Fujio et al., 1998): an extension of the bottom-up model proposed by Collins CITATION.,,
\x0f Kanayama\&apos;s parser CITATION: a bottom-up model coupled with an HPSG.,,
\x0f Shirai\&apos;s parser CITATION: a topdown model incorporating lexical collocation statistics.,,
\x0f Peach Pie Parser CITATION: a bottom-up model based on maximum entropy estimation.,,
