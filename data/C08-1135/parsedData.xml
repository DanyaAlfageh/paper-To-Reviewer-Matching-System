<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.555112">
b&quot;Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 10731080
</bodyText>
<address confidence="0.684919">
Manchester, August 2008
</address>
<title confidence="0.458324">
Automatic Seed Word Selection for Unsupervised Sentiment
Classification of Chinese Text
</title>
<author confidence="0.726246">
Taras Zagibalov John Carroll
</author>
<affiliation confidence="0.992855">
University of Sussex
Department of Informatics
</affiliation>
<address confidence="0.992783">
Brighton BN1 9QH, UK
</address>
<email confidence="0.998198">
{T.Zagibalov,J.A.Carroll}@sussex.ac.uk
</email>
<sectionHeader confidence="0.990843" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.980640466666667">
We describe and evaluate a new method
of automatic seed word selection for un-
supervised sentiment classification of
product reviews in Chinese. The whole
method is unsupervised and does not re-
quire any annotated training data; it only
requires information about commonly oc-
curring negations and adverbials. Unsu-
pervised techniques are promising for
this task since they avoid problems of do-
main-dependency typically associated
with supervised methods. The results ob-
tained are close to those of supervised
classifiers and sometimes better, up to an
F1 of 92%.
</bodyText>
<sectionHeader confidence="0.996639" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.984762878048781">
Automatic classification of document sentiment
(and more generally extraction of opinion from
text) has recently attracted a lot of interest. One
of the main reasons for this is the importance of
such information to companies, other
organizations, and individuals. Applications
include marketing research tools that help a
company see market or media reaction towards
their brands, products or services, or search
engines that help potential purchasers make an
informed choice of a product they want to buy.
Sentiment classification research has drawn on
and contributed to research in text classification,
unsupervised machine learning, and cross-
domain adaptation.
This paper presents a new, automatic approach
to automatic seed word selection as part of senti-
ment classification of product reviews written in
Chinese, which addresses the problem of do-
2008. Licensed under the Creative Commons Attribu-
tion-Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
main-dependency of sentiment classification that
has been observed in previous work. It may also
facilitate building sentiment classification sys-
tems in other languages since the approach as-
sumes a very small amount of linguistic knowl-
edge: the only language-specific information re-
quired is a basic description of the most frequent
negated adverbial constructions in the language.
The paper is structured as follows. Section 2
surveys related work in sentiment classification,
unsupervised machine learning and Chinese
language processing. Section 3 motivates our
approach, which is described in detail in
Section 4. The data used for experiments and
baselines, as well as the results of experiments
are covered in Section 5. Section 6 discusses the
lessons learned and proposes directions for future
work.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.894869">
2.1 Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.99915845">
Most work on sentiment classification has used
approaches based on supervised machine learn-
ing. For example, Pang et al. (2002) collected
movie reviews that had been annotated with re-
spect to sentiment by the authors of the reviews,
and used this data to train supervised classifiers.
A number of studies have investigated the impact
on classification accuracy of different factors, in-
cluding choice of feature set, machine learning
algorithm, and pre-selection of the segments of
text to be classified. For example, Dave et al.
(2003) experiment with the use of linguistic, sta-
tistical and n-gram features and measures for fea-
ture selection and weighting. Pang and Lee
(2004) use a graph-based technique to identify
and analyze only subjective parts of texts. Yu
and Hatzivassiloglou (2003) use semantically-
oriented words for identification of polarity at
the sentence level. Most of this work assumes bi-
nary classification (positive and negative), some-
</bodyText>
<page confidence="0.956135">
1073
</page>
<bodyText confidence="0.999081264705882">
\x0ctimes with the addition of a neutral class (in
terms of polarity, representing lack of sentiment).
While supervised systems generally achieve
reasonably high accuracy, they do so only on test
data that is similar to the training data. To move
to another domain one would have to collect an-
notated data in the new domain and retrain the
classifier. Engstrom (2004) reports decreased ac-
curacy in cross-domain classification since senti-
ment in different domains is often expressed in
different ways. However, it is impossible in prac-
tice to have annotated data for all possible do-
mains of interest. Aue and Gamon (2005) at-
tempt to solve the problem of the absence of
large amounts of labeled data by customizing
sentiment classifiers to new domains using train-
ing data from other domains. Blitzer et al. (2007)
investigate domain adaptation for sentiment clas-
sifiers using structural correspondence learning.
Read (2005) also observed significant differ-
ences between the accuracy of classification of
reviews in the same domain but published in dif-
ferent time periods.
Recently, there has been a shift of interest to-
wards more fine-grained approaches to process-
ing of sentiment, in which opinion is extracted at
the sentence level, sometimes including informa-
tion about different features of a product that are
commented on and/or the opinion holder (Hu and
Liu, 2004; Ku et al., 2006). But even in such ap-
proaches, McDonald et al. (2007) note that infor-
mation about the overall sentiment orientation of
a document facilitates more accurate extraction
of more specific information from the text.
</bodyText>
<subsectionHeader confidence="0.999706">
2.2 Unsupervised Approach
</subsectionHeader>
<bodyText confidence="0.999527363636363">
One way of tackling the problem of domain de-
pendency could be to use an approach that does
not rely on annotated data. Turney (2002) de-
scribes a method of sentiment classification us-
ing two human-selected seed words (the words
poor and excellent) in conjunction with a very
large text corpus; the semantic orientation of
phrases is computed as their association with the
seed words (as measured by pointwise mutual in-
formation). The sentiment of a document is cal-
culated as the average semantic orientation of all
such phrases.
Yarowsky (1995) describes a &amp;apos;semi-unsuper-
vised&amp;apos; approach to the problem of sense disam-
biguation of words, also using a set of initial
seeds, in this case a few high quality sense anno-
tations. These annotations are used to start an it-
erative process of learning information about the
contexts in which senses of words appear, in
each iteration labeling senses of previously unla-
beled word tokens using information from the
previous iteration.
</bodyText>
<subsectionHeader confidence="0.998617">
2.3 Chinese Language Processing
</subsectionHeader>
<bodyText confidence="0.989690923076923">
A major issue in processing Chinese text is the
fact that words are not delimited in the written
language. In many cases, NLP researchers work-
ing with Chinese use an initial segmentation
module that is intended to break a text into
words. Although this can facilitate the use of
subsequent computational techniques, there is no
a clear definition of what a &amp;apos;word&amp;apos; is in the mod-
ern Chinese language, so the use of such seg-
menters is of dubious theoretical status; indeed,
good results have been reported from systems
which do not assume such pre-processing (Foo
and Li, 2004; Xu et al., 2004).
</bodyText>
<subsectionHeader confidence="0.985207">
2.4 Seed Word Selection
</subsectionHeader>
<bodyText confidence="0.997980375">
We are not aware of any sentiment analysis sys-
tem that uses unsupervised seed word selection.
However, Pang et al. (2002) showed that it is dif-
ficult to get good coverage of a target domain
from manually selected words, and even simple
corpus frequency counts may produce a better
list of features for supervised classification: hu-
man-created lists resulted in 64% accuracy on a
movie review corpus, while a list of frequent
words scored 69%. Pang et al. also observed that
some words without any significant emotional
orientation were quite good features: for exam-
ple, the word still turned out to be a good indi-
cator of positive reviews as it was often used in
sentences such as Still, though, it was worth
seeing&amp;apos;&amp;apos;.
</bodyText>
<sectionHeader confidence="0.974468" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.9977376875">
Our main goal is to overcome the problem of do-
main-dependency in sentiment classification.
Unsupervised approaches seem promising in this
regard, since they do not require annotated train-
ing data, just access to sufficient raw text in each
domain. We base our approach on a previously
described, &amp;apos;almost-unsupervised&amp;apos; system that
starts with only a single, human-selected seed
(good) and uses an iterative method to extract a
training sub-corpus (Zagibalov &amp; Carroll, 2008).
The approach does not use a word segmentation
module; in this paper we use the term &amp;apos;lexical
item&amp;apos; to denote any sequence of Chinese charac-
ters that is treated by the system as a unit, what-
ever it is linguistically a morpheme, a word or
a phrase.
</bodyText>
<page confidence="0.985754">
1074
</page>
<bodyText confidence="0.953569826086956">
\x0cOur initial aim was to investigate ways of im-
proving the classifier by automatically finding a
better seed, because Zagibalov &amp; Carroll indicate
that in different domains they could, by manual
trial and error, find a seed other than (good)
which produced better results.
To find such a seed automatically, we make
two assumptions:
1. Attitude is often expressed through the
negation of vocabulary items with the op-
posite meaning; for example in Chinese it
is more common to say not good than
bad (Tan, 2002). Zagibalov &amp; Carroll&amp;apos;s
system uses this observation to find nega-
tive lexical items while nevertheless start-
ing only from a positive seed. This leads
us to believe that it is possible to find
candidate seeds themselves by looking
for sequences of characters which are
used with negation.
2. The polarity of a candidate seed needs to
be determined. To do this we assume we
can use the lexical item (good) as a
gold standard for positive lexical items
and compare the pattern of contexts a
candidate seed occurs in to the pattern ex-
hibited by the gold standard.
Looking at product review corpora, we observed
that good is always more often used without
negation in positive texts, while in negative texts
it is more often used with negation (e.g. not
good). Also, good occurs more often in positive
texts than negative, and more frequently without
negation than with it. We use the latter observa-
tion as the basis for identifying seed lexical
items, finding those which occur with negation
but more frequently occur without it.
As well as detecting negation1
we also use ad-
verbials2
to avoid hypothesizing non-contentful
seeds: the characters following the sequence of a
negation and an adverbial are in general content-
ful units, as opposed to parts of words, function
words, etc. In what follows we refer to such con-
structions as negated adverbial constructions.
</bodyText>
<page confidence="0.892277">
1
</page>
<bodyText confidence="0.9763018">
We use only six frequently occurring negations: (bu),
(buhui), (meiyou), (baituo), (mianqu),
and (bimian). We are trying to be as language-inde-
pendent as possible so we take a simplistic approach to de-
tecting negation.
</bodyText>
<page confidence="0.973276">
2
</page>
<bodyText confidence="0.999921333333333">
We use five frequently occurring adverbials: (hen),
(feichang), (tai), (zui), and (bijiao). Similarly to
negation, we deliberately take a simplistic approach.
</bodyText>
<sectionHeader confidence="0.996048" genericHeader="method">
4 Method
</sectionHeader>
<bodyText confidence="0.997533928571429">
We use a similar sentiment classifier and itera-
tive retraining technique to the almost-unsuper-
vised system of Zagibalov &amp; Carroll (2008),
summarized below in Sections 4.2 and 4.3. The
main new contributions of this paper are tech-
niques for automatically finding the seeds from
raw text in a particular domain (Section 4.1), and
for detecting when the process should stop (Sec-
tion 4.4). This new system therefore differs from
that of Zagibalov &amp; Carroll (2008) in being com-
pletely unsupervised and not depending on arbi-
trary iteration limits. (The evaluation also differs
since we focus in this paper on the effects of do-
main on sentiment classification accuracy).
</bodyText>
<subsectionHeader confidence="0.999534">
4.1 Seed Lexical Item Identification
</subsectionHeader>
<bodyText confidence="0.9993725">
The first step is to identify suitable positive seeds
for the given corpus. The intuition behind the
way this is done is outlined above in Section 3.
The algorithm is as follows:
</bodyText>
<listItem confidence="0.9962015">
1. find all sequences of characters between
non-character symbols (i.e. punctuation
marks, digits and so on) that contain
negation and an adverbial, split the se-
quence at the negation, and store the char-
acter sequence that follows the negated
adverbial construction;
2. count the number of occurrences of each
distinct sequence that follows a negated
adverbial construction (X);
3. count the number of occurrences of each
distinct sequence without the construction
(Y);
4. find all sequences with Y X &gt; 0.
</listItem>
<subsectionHeader confidence="0.985657">
4.2 Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.9996358">
This approach to Chinese language processing
does not use pre-segmentation (in the sense dis-
cussed in Section 2.3) or grammatical analysis:
the basic unit of processing is the &amp;apos;lexical item&amp;apos;,
each of which is a sequence of one or more Chi-
nese characters excluding punctuation marks (so
a lexical item may actually form part of a word, a
whole word or a sequence of words), and &amp;apos;zones&amp;apos;,
each of which is a sequence of characters delim-
ited by punctuation marks.
Each zone is classified as either positive or
negative based whether positive or negative vo-
cabulary items predominate. As there are two
parts of the vocabulary (positive and negative),
we correspondingly calculate two scores (Si ,
</bodyText>
<page confidence="0.952917">
1075
</page>
<bodyText confidence="0.952345125">
\x0cwhere i is either positive or negative) using
Equation (1), where Ld is the length in characters
of a matching lexical item (raised to the power of
two to increase the significance of longer items
which capture more context), Lphrase is the length
of the current zone in characters, Sd is the current
sentiment score of the matching lexical item (ini-
tially 1.0), and Nd is a negation check coefficient.
</bodyText>
<figure confidence="0.938026666666667">
Si=
Ld
2
Lphrase
Sd N d
(1)
</figure>
<bodyText confidence="0.992609666666667">
The negation check is a regular expression which
determines if the lexical item is preceded by a
negation within its enclosing zone. If a negation
is found then Nd is set to 1.
The sentiment score of a zone is the sum of
sentiment of all the items found in it.
To determine the sentiment orientation of the
whole document, the classifier computes the dif-
ference between the number of positive and neg-
ative zones. If the result is greater than zero the
document is classified as positive, and vice ver-
sa.
</bodyText>
<subsectionHeader confidence="0.993931">
4.3 Iterative Retraining
</subsectionHeader>
<bodyText confidence="0.999472681818182">
Iterative retraining is used to enlarge the initial
seed vocabulary into a comprehensive vocabu-
lary list of sentiment-bearing lexical items. In
each iteration, the current version of the classifier
is run on the input corpus to classify each docu-
ment, resulting in a training subcorpus of posi-
tive and a negative documents. The subcorpus is
used to adjust the scores of existing positive and
negative vocabulary items and to find new items
to be included in the vocabulary.
Each lexical item that occurs at least twice in
the corpus is a candidate for inclusion in the vo-
cabulary list. After candidate items are found, the
system calculates their relative frequencies in
both the positive and negative parts of the current
training subcorpus. The system also checks for
negation while counting occurrences: if a lexical
item is preceded by a negation, its count is re-
duced by one.
For all candidate items we compare their rela-
tive frequencies in the positive and negative doc-
uments in the subcorpus using Equation (2).
</bodyText>
<equation confidence="0.966742">
difference =
F p Fn
F pFn/2
</equation>
<bodyText confidence="0.926434833333333">
(2)
If difference &lt; 1, then the frequencies are similar
and the item does not have enough distinguishing
power, so it is not included in the vocabulary.
Otherwise the sentiment score of the item is
(re-)calculated according to Equation (3) for
positive items, and analogously for negative
items.
F pFn (3)
Finally, the adjusted vocabulary list with the new
scores is ready for the next iteration3
.
</bodyText>
<subsectionHeader confidence="0.993946">
4.4 Iteration Control
</subsectionHeader>
<bodyText confidence="0.995429833333333">
To maximize the number of productive iterations
while avoiding unnecessary processing and arbi-
trary iteration limits, iterative retraining is
stopped when there is no change to the classifica-
tion of any document over the previous two itera-
tions.
</bodyText>
<sectionHeader confidence="0.998753" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.602343">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.998800043478261">
As our approach is unsupervised, we do not use
an annotated training corpus, but run our iterative
procedure on the raw data extracted from an an-
notated test corpus, and evaluate the final accura-
cy of the system with respect to the annotations
in that corpus.
Our test corpus is derived from product re-
views harvested from the website IT1684
. All the
reviews were tagged by their authors as either
positive or negative overall. Most reviews con-
sist of two or three distinct parts: positive opin-
ions, negative opinions, and comments (&amp;apos;other&amp;apos;)
although some reviews have only one part. We
removed duplicate reviews automatically using
approximate matching, giving a corpus of 29531
reviews of which 23122 are positive (78%) and
6409 are negative (22%). The total number of
different products in the corpus is 10631, the
number of product categories is 255, and most of
the reviewed products are either software prod-
ucts or consumer electronics. Unfortunately, it
appears that some users misuse the sentiment
</bodyText>
<page confidence="0.98796">
3
</page>
<bodyText confidence="0.997953888888889">
An alternative approach might be to use point-wise mutual
information instead of relative frequencies of newly found
features in a subcorpus produced in the previous iteration.
However, in preliminary experiments, SO-PMI did not pro-
duce good corpora from the first iteration. Also, it is not
clear how to manage subsequent iterations since PMI would
have to be calculated between thousands of new vocabulary
items and every newly found sequence of characters, which
would be computationally intractable.
</bodyText>
<page confidence="0.908081">
4
</page>
<footnote confidence="0.933413">
http://product.it168.com
</footnote>
<page confidence="0.965602">
1076
</page>
<bodyText confidence="0.994181125">
\x0ctagging facility on the website so quite a lot of
reviews have incorrect tags. However, the parts
of the reviews are much more reliably identified
as being positive or negative so we used these as
the items of the test corpus. In the experiments
described below we use 10 subcorpora contain-
ing a total of 7982 reviews, distributed between
product types as shown in Table 1.
</bodyText>
<figure confidence="0.448331142857143">
Corpus/product type Reviews
Monitors 683
Mobile phones 2317
Digital cameras 1705
MP3 players 779
Computer parts (CD-drives, mother-
boards)
</figure>
<page confidence="0.753139">
308
</page>
<table confidence="0.8004255">
Video cameras and lenses 361
Networking (routers, network cards) 350
Office equipment (copiers,
multifunction devices, scanners)
611
Printers (laser, inkjet) 569
Computer peripherals (mice, keyboards,
speakers)
</table>
<page confidence="0.985806">
457
</page>
<tableCaption confidence="0.995552">
Table 1. Product types and sizes of the test
</tableCaption>
<bodyText confidence="0.950719916666667">
corpora.
We constructed five of the corpora by combin-
ing smaller ones of 100250 reviews each (as in-
dicated in parentheses in Table 1) in order to
have reasonable amounts of data.
Each corpus has equal numbers of positive and
negative reviews so we can derive upper bounds
from the corpora (Section 5.2) by applying su-
pervised classifiers. We balance the corpora
since (at least on this data) these classifiers per-
form less well with skewed class distributions5
.
</bodyText>
<subsectionHeader confidence="0.992693">
5.2 Baseline and Upper Bound
</subsectionHeader>
<bodyText confidence="0.9034496">
Since the corpora are balanced with respect to
sentiment orientation the naive (unsupervised)
baseline is 50%. We also produced an upper
bound using Naive Bayes multinomial (NBm)
and Support Vector Machine (SVM)6
classifiers
with the NTU Sentiment Dictionary (Ku et al.,
2006) vocabulary items as the feature set. The
dictionary contains 2809 items in the &amp;apos;positive&amp;apos;
part and 8273 items in the &amp;apos;negative&amp;apos;. We ran
</bodyText>
<page confidence="0.954732">
5
</page>
<bodyText confidence="0.885325">
We have made this corpus publicly available at http://
</bodyText>
<equation confidence="0.425602">
www.informatics.sussex.ac.uk/users/tz21/coling08.zip
6
We used WEKA 3.4.11 (http://www.cs.waikato.ac.nz/ ml/
weka )
</equation>
<tableCaption confidence="0.693529666666667">
both classifiers in 10-fold stratified cross-valida-
tion mode, resulting in the accuracies shown in
Table 2. The macroaveraged accuracies across all
</tableCaption>
<figure confidence="0.975618666666667">
10 corpora are 82.78% (NBm) and 80.89%
(SVM).
Corpus Nbm
(%)
SVM
(%)
</figure>
<table confidence="0.9891308">
Monitors 86.21 83.87
Mobile phones 86.52 84.49
Digital cameras 82.27 82.04
MP3 players 82.64 79.43
Computer parts 81.10 79.47
Video cameras and lenses 83.05 84.16
Networking 77.65 75.35
Office equipment 82.13 80.00
Printers 81.33 79.57
Computer peripherals 84.86 80.48
</table>
<tableCaption confidence="0.994665">
Table 2. Upper bound accuracies.
</tableCaption>
<bodyText confidence="0.997667882352941">
We also tried adding the negations and adver-
bials specified in Section 3 to the feature set, and
this resulted in slightly improved accuracies, of
83.90% (Nbm) and 82.49% (SVM).
An alternative approach would have been to
automatically segment the reviews and then de-
rive a feature set of a manageable size by setting
a threshold on word frequencies; however the ex-
tra processing means that this is a less valid up-
per bound.
Another possible comparison could be with a
version of Turney&amp;apos;s (2002) sentiment classifica-
tion method applied to Chinese. However, the re-
sults would not be comparable since Turney&amp;apos;s
method would require the additional use of very
large text corpus and the manual selection of
positive and negative seed words.
</bodyText>
<subsectionHeader confidence="0.976058">
5.3 Experiment 1
</subsectionHeader>
<bodyText confidence="0.999727545454545">
To be able to compare to the accuracy of the al-
most-unsupervised approach of Zagibalov &amp;
Carroll (2008), we ran our system using the seed
(good) for each corpus. The results are shown
in Table 3. We compute precision, recall and F1
measure rather than just accuracy, since our clas-
sifier can omit some reviews whereas the super-
vised classifiers attempt to classify all reviews.
The macroaveraged F1 measure is 80.55, which
beats the naive baseline by over 30 percentage
points, and approaches the two upper bounds.
</bodyText>
<page confidence="0.990787">
1077
</page>
<table confidence="0.9997045">
\x0cCorpus Iter P R F1
Monitors 12 86.62 86.24 86.43
Mobile phones 11 90.15 89.68 89.91
Digital cameras 13 81.33 80.23 80.78
MP3 players 13 86.10 85.10 85.60
Computer parts 10 69.10 67.53 68.31
Video cameras and
lenses
10 82.81 81.44 82.12
Networking 11 69.28 68.29 68.78
Office equipment 12 81.83 80.36 81.09
Printers 12 81.04 79.61 80.32
Computer peripherals 10 82.20 81.84 82.02
Macroaverage 81.05 80.03 80.54
</table>
<tableCaption confidence="0.997725">
Table 3. Results with the single, manually
</tableCaption>
<bodyText confidence="0.783092">
chosen seed (good) for each corpus.
</bodyText>
<subsectionHeader confidence="0.813205">
5.4 Experiment 2
</subsectionHeader>
<bodyText confidence="0.9817332">
We then ran our full system, including the seed
identifier. Appendix A shows that for most of the
corpora the algorithm found different (highly do-
main-salient) seeds. Table 4 shows the results
achieved.
</bodyText>
<table confidence="0.999122857142857">
Corpus Iter P R F1
Monitors 11 85.57 85.07 85.32
Mobile phones 10 92.63 92.19 92.41
Digital cameras 13 84.92 83.58 84.24
MP3 players 13 88.69 87.55 88.11
Computer parts 12 77.78 77.27 77.52
Video cameras and
lenses
11 83.62 81.99 82.8
Networking 13 72.83 72.00 72.41
Office equipment 10 82.42 81.34 81.88
Printers 12 81.04 79.61 80.32
Computer peripherals 10 82.24 82.06 82.15
Macroaverage 83.17 82.27 82.72
</table>
<tableCaption confidence="0.998473">
Table 4. Results with the seeds automatically
</tableCaption>
<bodyText confidence="0.940425833333333">
identified for each corpus.
Across all 10 subcorpora, the improvement us-
ing automatically identified seed words com-
pared with just using the seed good is significant
(paired t-test, P&lt;0.0001), and the F1 measure lies
between the two upper bounds.
</bodyText>
<sectionHeader confidence="0.990336" genericHeader="method">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999893">
The unsupervised approach to seed words selec-
tion for sentiment classification presented in this
paper produces results which in most cases are
close to the results of supervised classifiers and
to the previous almost-unsupervised approach:
eight out of ten results showed improvement
over the human selected seed word and three re-
sults outperformed the supervised approach,
while three other results were less than 1% infe-
rior to the supervised ones.
How does it happen that the chosen seed is
usually (in our dataset always) positive? We
think that this happens due to the socially accept-
ed norm of behaviour: as a rule one needs to be
friendly to communicate with others. This in turn
defines linguistic means of expressing ideas
they will be at least slightly positive overall. The
higher prevalence of positive reviews has been
observed previously: for example, in our corpus
before we balanced it almost 80% of reviews
were positive; Pang et al. (2002) constructed
their move review corpus from an original
dataset of 1301 positive and 752 negative re-
views (63% positive). Ghose et al. (2007) quote
typical examples of highly positive language
used in the online marketplace. We can make a
preliminary conclusion that a relatively high fre-
quency of positive words is determined by the
usage of language that reflects the social beha-
viour of people.
In future work we intend to explore these is-
sues of positivity of language use. We will also
apply our approach to other genres containing
some quantity of evaluative language (for exam-
ple newspaper articles), and see if it works equal-
ly well for languages other than Chinese. It is
also likely we can use a smaller set of negation
words and adverbials to produce the seed lists.
</bodyText>
<sectionHeader confidence="0.943803" genericHeader="method">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.972617">
The first author is supported by the Ford Founda-
</bodyText>
<sectionHeader confidence="0.7696905" genericHeader="conclusions">
tion International Fellowships Program.
References
</sectionHeader>
<reference confidence="0.9892024">
Aue, Anthony, and Michael Gamon. 2005. Customiz-
ing Sentiment Classifiers to New Domains: a Case
Study. In Proceedings of the International Confer-
ence RANLP-2005 Recent Advances in Natural
Language Processing.
</reference>
<page confidence="0.757064">
1078
</page>
<reference confidence="0.996167192307692">
\x0cBlitzer, John, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, Boom-boxes and
Blenders: Domain Adaptation for Sentiment Clas-
sification. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguis-
tics. 440447.
Dave, Kushal, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the Peanut Gallery: Opinion
Extraction and Semantic Classification of Product
Reviews. In Proceedings of the Twelfth Interna-
tional World Wide Web Conference. 519528.
Engstrom, Charlotte. 2004. Topic Dependence in Sen-
timent Classification. Unpublished MPhil Disserta-
tion. University of Cambridge.
Foo, Schubert, and Hui Li. 2004. Chinese Word Seg-
mentation and Its Effects on Information Retrieval.
Information Processing and Management, 40(1).
161190.
Ghose, Anindya, Panagiotis Ipeirotis, and Arun Sun-
dararajan. 2007. Opinion Mining using Economet-
rics: A Case Study on Reputation Systems. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics. 416423.
Hu, Minqing, and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of the
10th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. 168177.
Ku, Lun-Wei, Yu-Ting Liang, and Hsin-Hsi Chen.
2006. Opinion Extraction, Summarization and
Tracking in News and Blog Corpora. In Proceed-
ings of the AAAI-2006 Spring Symposium on Com-
putational Approaches to Analyzing Weblogs.
AAAI Technical Report.
McDonald, Ryan, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured Models
for Fine-to-Coarse Sentiment Analysis. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics. 432439.
Pang, Bo, and Lillian Lee. 2004. A Sentimental Edu-
cation: Sentiment Analysis Using Subjectivity
Summarization Based on Minimum Cuts. In Pro-
ceedings of the 42nd Annual Meeting of the Associ-
ation for Computational Linguistics. 271278.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up? Sentiment Clas-
sification using Machine Learning Techniques. In
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing. 7986.
Read, Jonathon. 2005. Using Emoticons to Reduce
Dependency in Machine Learning Techniques for
Sentiment Classification. In Proceedings of the
ACL Student Research Workshop at ACL-05. 43
</reference>
<page confidence="0.570784">
48.
</page>
<bodyText confidence="0.961187">
Tan, Aoshuang. 2002. Problemy skrytoj grammatiki.
Sintaksis, semantika i pragmatika jazyka izoliru-
juscego stroja na primere kitajskogo jazyka [Prob-
lems of a hidden grammar. Syntax, semantics and
pragmatics of a language of the isolating type, tak-
ing the Chinese language as an example]. Jazyki
</bodyText>
<reference confidence="0.980267115384615">
Slavjanskoj Kultury.
Turney, Peter D. 2002. Thumbs Up or Thumbs
Down? Semantic Orientation Applied to Unsuper-
vised Classification of Reviews. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics. 417424.
Xu, Jia, Richard Zens, and Hermann Ney. 2004. Do
We Need Chinese Word Segmentation for Statisti-
cal Machine Translation? In Proceedings of the
Third SIGHAN Workshop on Chinese Language
Learning. 122128.
Yarowsky, David. 1995. Unsupervised Word Sense
Disambiguation Rivaling Supervised Methods. In
Proceedings of the 33rd Annual Meeting of the As-
sociation for Computational Linguistics. 189196.
Yu, Hong, and Vasileios Hatzivassiloglou. 2003. To-
wards Answering Opinion Questions: Separating
Facts from Opinions and Identifying the Polarity of
Opinion Sentences. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing. 129136.
Zagibalov, Taras, and John Carroll. 2008. Unsuper-
vised Classification of Sentiment and Objectivity
in Chinese Text. In Proceedings of the Third Inter-
national Joint Conference on Natural Language
Processing. 304311.
</reference>
<page confidence="0.677796">
1079
</page>
<reference confidence="0.7423235">
\x0cAppendix A. Seeds Automatically Identified for each Corpus.
Corpus Seed Corpus Seed
</reference>
<figure confidence="0.997825363636364">
Monitors (good)
(convenient; cheap)
(clear)
(straight)
(comfortable)
(fill, fulfill)
(sharp)
(comfortable)
(cool)
Video
cameras
and lenses
(clear of sound or image)
(comfortable)
(practical)
(perfect)
(cool)
Mobile
phones
(good)
(support)
(convenient; cheap)
(comfortable)
(clear of sound or image)
(sufficient)
(easy to use)
(comfortable)
(user friendly)
(smooth and easy)
(distinct)
(cool)
(has become better)
(durable)
(comfortable)
(satisfied)
(fit, suit)
(has become comfortable)
(applicable)
(handy)
(science, scientific)
Digital
cameras
(good)
(convenient; cheap)
(comfortable)
(clearof sound or image)
(special)
(cool)
(satisfied)
(durable)
(comfortable)
(perfect)
(straight)
(stable)
(has become comfortable)
(polite)
(detailed)
Networking (stable) Printers (good)
MP3 players (good)
(convenient; cheap)
(comfortable)
(practical)
(sensitive)
(comfortable)
(cool)
(has become comfortable)
Computer
peripherals
(good)
(convenient;cheap)
(comfortable)
(precise)
(comfortable)
(habitual)
(smooth and easy)
(stable)
Computer
parts
(good)
(stable)
Office
equipment
(good)
(comfortable)
(stable)
(practical)
1080
\x0c&quot;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.794329">
<note confidence="0.9112815">b&quot;Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 10731080 Manchester, August 2008</note>
<title confidence="0.9969335">Automatic Seed Word Selection for Unsupervised Sentiment Classification of Chinese Text</title>
<author confidence="0.998455">Taras Zagibalov John Carroll</author>
<affiliation confidence="0.999701">University of Sussex Department of Informatics</affiliation>
<address confidence="0.999386">Brighton BN1 9QH, UK</address>
<email confidence="0.997717">T.Zagibalov@sussex.ac.uk</email>
<email confidence="0.997717">J.A.Carroll@sussex.ac.uk</email>
<abstract confidence="0.995732875">We describe and evaluate a new method of automatic seed word selection for unsupervised sentiment classification of product reviews in Chinese. The whole method is unsupervised and does not require any annotated training data; it only requires information about commonly occurring negations and adverbials. Unsupervised techniques are promising for this task since they avoid problems of domain-dependency typically associated with supervised methods. The results obtained are close to those of supervised classifiers and sometimes better, up to an F1 of 92%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anthony Aue</author>
<author>Michael Gamon</author>
</authors>
<title>Customizing Sentiment Classifiers to New Domains: a Case Study.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference RANLP-2005 Recent Advances in Natural Language Processing.</booktitle>
<contexts>
<context position="4402" citStr="Aue and Gamon (2005)" startWordPosition="650" endWordPosition="653">\x0ctimes with the addition of a neutral class (in terms of polarity, representing lack of sentiment). While supervised systems generally achieve reasonably high accuracy, they do so only on test data that is similar to the training data. To move to another domain one would have to collect annotated data in the new domain and retrain the classifier. Engstrom (2004) reports decreased accuracy in cross-domain classification since sentiment in different domains is often expressed in different ways. However, it is impossible in practice to have annotated data for all possible domains of interest. Aue and Gamon (2005) attempt to solve the problem of the absence of large amounts of labeled data by customizing sentiment classifiers to new domains using training data from other domains. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers using structural correspondence learning. Read (2005) also observed significant differences between the accuracy of classification of reviews in the same domain but published in different time periods. Recently, there has been a shift of interest towards more fine-grained approaches to processing of sentiment, in which opinion is extracted at the sen</context>
</contexts>
<marker>Aue, Gamon, 2005</marker>
<rawString>Aue, Anthony, and Michael Gamon. 2005. Customizing Sentiment Classifiers to New Domains: a Case Study. In Proceedings of the International Conference RANLP-2005 Recent Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John \x0cBlitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<pages>440447</pages>
<marker>\x0cBlitzer, Dredze, Pereira, 2007</marker>
<rawString>\x0cBlitzer, John, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. 440447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the Peanut Gallery: Opinion Extraction and Semantic Classification of Product Reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twelfth International World Wide Web Conference.</booktitle>
<pages>519528</pages>
<contexts>
<context position="3358" citStr="Dave et al. (2003)" startWordPosition="483" endWordPosition="486">d proposes directions for future work. 2 Related Work 2.1 Sentiment Classification Most work on sentiment classification has used approaches based on supervised machine learning. For example, Pang et al. (2002) collected movie reviews that had been annotated with respect to sentiment by the authors of the reviews, and used this data to train supervised classifiers. A number of studies have investigated the impact on classification accuracy of different factors, including choice of feature set, machine learning algorithm, and pre-selection of the segments of text to be classified. For example, Dave et al. (2003) experiment with the use of linguistic, statistical and n-gram features and measures for feature selection and weighting. Pang and Lee (2004) use a graph-based technique to identify and analyze only subjective parts of texts. Yu and Hatzivassiloglou (2003) use semanticallyoriented words for identification of polarity at the sentence level. Most of this work assumes binary classification (positive and negative), some1073 \x0ctimes with the addition of a neutral class (in terms of polarity, representing lack of sentiment). While supervised systems generally achieve reasonably high accuracy, they</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Dave, Kushal, Steve Lawrence, and David M. Pennock. 2003. Mining the Peanut Gallery: Opinion Extraction and Semantic Classification of Product Reviews. In Proceedings of the Twelfth International World Wide Web Conference. 519528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charlotte Engstrom</author>
</authors>
<title>Topic Dependence in Sentiment Classification. Unpublished MPhil Dissertation.</title>
<date>2004</date>
<institution>University of Cambridge.</institution>
<contexts>
<context position="4149" citStr="Engstrom (2004)" startWordPosition="611" endWordPosition="612">ntify and analyze only subjective parts of texts. Yu and Hatzivassiloglou (2003) use semanticallyoriented words for identification of polarity at the sentence level. Most of this work assumes binary classification (positive and negative), some1073 \x0ctimes with the addition of a neutral class (in terms of polarity, representing lack of sentiment). While supervised systems generally achieve reasonably high accuracy, they do so only on test data that is similar to the training data. To move to another domain one would have to collect annotated data in the new domain and retrain the classifier. Engstrom (2004) reports decreased accuracy in cross-domain classification since sentiment in different domains is often expressed in different ways. However, it is impossible in practice to have annotated data for all possible domains of interest. Aue and Gamon (2005) attempt to solve the problem of the absence of large amounts of labeled data by customizing sentiment classifiers to new domains using training data from other domains. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers using structural correspondence learning. Read (2005) also observed significant differences between</context>
</contexts>
<marker>Engstrom, 2004</marker>
<rawString>Engstrom, Charlotte. 2004. Topic Dependence in Sentiment Classification. Unpublished MPhil Dissertation. University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Schubert Foo</author>
<author>Hui Li</author>
</authors>
<date>2004</date>
<booktitle>Chinese Word Segmentation and Its Effects on Information Retrieval. Information Processing and Management,</booktitle>
<volume>40</volume>
<issue>1</issue>
<pages>161190</pages>
<contexts>
<context position="6980" citStr="Foo and Li, 2004" startWordPosition="1072" endWordPosition="1075">iteration. 2.3 Chinese Language Processing A major issue in processing Chinese text is the fact that words are not delimited in the written language. In many cases, NLP researchers working with Chinese use an initial segmentation module that is intended to break a text into words. Although this can facilitate the use of subsequent computational techniques, there is no a clear definition of what a &amp;apos;word&amp;apos; is in the modern Chinese language, so the use of such segmenters is of dubious theoretical status; indeed, good results have been reported from systems which do not assume such pre-processing (Foo and Li, 2004; Xu et al., 2004). 2.4 Seed Word Selection We are not aware of any sentiment analysis system that uses unsupervised seed word selection. However, Pang et al. (2002) showed that it is difficult to get good coverage of a target domain from manually selected words, and even simple corpus frequency counts may produce a better list of features for supervised classification: human-created lists resulted in 64% accuracy on a movie review corpus, while a list of frequent words scored 69%. Pang et al. also observed that some words without any significant emotional orientation were quite good features:</context>
</contexts>
<marker>Foo, Li, 2004</marker>
<rawString>Foo, Schubert, and Hui Li. 2004. Chinese Word Segmentation and Its Effects on Information Retrieval. Information Processing and Management, 40(1). 161190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anindya Ghose</author>
<author>Panagiotis Ipeirotis</author>
<author>Arun Sundararajan</author>
</authors>
<title>Opinion Mining using Econometrics: A Case Study on Reputation Systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<pages>416423</pages>
<contexts>
<context position="23297" citStr="Ghose et al. (2007)" startWordPosition="3754" endWordPosition="3757"> chosen seed is usually (in our dataset always) positive? We think that this happens due to the socially accepted norm of behaviour: as a rule one needs to be friendly to communicate with others. This in turn defines linguistic means of expressing ideas they will be at least slightly positive overall. The higher prevalence of positive reviews has been observed previously: for example, in our corpus before we balanced it almost 80% of reviews were positive; Pang et al. (2002) constructed their move review corpus from an original dataset of 1301 positive and 752 negative reviews (63% positive). Ghose et al. (2007) quote typical examples of highly positive language used in the online marketplace. We can make a preliminary conclusion that a relatively high frequency of positive words is determined by the usage of language that reflects the social behaviour of people. In future work we intend to explore these issues of positivity of language use. We will also apply our approach to other genres containing some quantity of evaluative language (for example newspaper articles), and see if it works equally well for languages other than Chinese. It is also likely we can use a smaller set of negation words and a</context>
</contexts>
<marker>Ghose, Ipeirotis, Sundararajan, 2007</marker>
<rawString>Ghose, Anindya, Panagiotis Ipeirotis, and Arun Sundararajan. 2007. Opinion Mining using Econometrics: A Case Study on Reputation Systems. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. 416423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and Summarizing Customer Reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</booktitle>
<pages>168177</pages>
<contexts>
<context position="5150" citStr="Hu and Liu, 2004" startWordPosition="768" endWordPosition="771">ing training data from other domains. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers using structural correspondence learning. Read (2005) also observed significant differences between the accuracy of classification of reviews in the same domain but published in different time periods. Recently, there has been a shift of interest towards more fine-grained approaches to processing of sentiment, in which opinion is extracted at the sentence level, sometimes including information about different features of a product that are commented on and/or the opinion holder (Hu and Liu, 2004; Ku et al., 2006). But even in such approaches, McDonald et al. (2007) note that information about the overall sentiment orientation of a document facilitates more accurate extraction of more specific information from the text. 2.2 Unsupervised Approach One way of tackling the problem of domain dependency could be to use an approach that does not rely on annotated data. Turney (2002) describes a method of sentiment classification using two human-selected seed words (the words poor and excellent) in conjunction with a very large text corpus; the semantic orientation of phrases is computed as t</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Hu, Minqing, and Bing Liu. 2004. Mining and Summarizing Customer Reviews. In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 168177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lun-Wei Ku</author>
<author>Yu-Ting Liang</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>Opinion Extraction, Summarization and Tracking in News and Blog Corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI-2006 Spring Symposium on Computational Approaches to</booktitle>
<tech>Technical Report.</tech>
<contexts>
<context position="5168" citStr="Ku et al., 2006" startWordPosition="772" endWordPosition="775">from other domains. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers using structural correspondence learning. Read (2005) also observed significant differences between the accuracy of classification of reviews in the same domain but published in different time periods. Recently, there has been a shift of interest towards more fine-grained approaches to processing of sentiment, in which opinion is extracted at the sentence level, sometimes including information about different features of a product that are commented on and/or the opinion holder (Hu and Liu, 2004; Ku et al., 2006). But even in such approaches, McDonald et al. (2007) note that information about the overall sentiment orientation of a document facilitates more accurate extraction of more specific information from the text. 2.2 Unsupervised Approach One way of tackling the problem of domain dependency could be to use an approach that does not rely on annotated data. Turney (2002) describes a method of sentiment classification using two human-selected seed words (the words poor and excellent) in conjunction with a very large text corpus; the semantic orientation of phrases is computed as their association w</context>
<context position="18655" citStr="Ku et al., 2006" startWordPosition="3003" endWordPosition="3006">easonable amounts of data. Each corpus has equal numbers of positive and negative reviews so we can derive upper bounds from the corpora (Section 5.2) by applying supervised classifiers. We balance the corpora since (at least on this data) these classifiers perform less well with skewed class distributions5 . 5.2 Baseline and Upper Bound Since the corpora are balanced with respect to sentiment orientation the naive (unsupervised) baseline is 50%. We also produced an upper bound using Naive Bayes multinomial (NBm) and Support Vector Machine (SVM)6 classifiers with the NTU Sentiment Dictionary (Ku et al., 2006) vocabulary items as the feature set. The dictionary contains 2809 items in the &amp;apos;positive&amp;apos; part and 8273 items in the &amp;apos;negative&amp;apos;. We ran 5 We have made this corpus publicly available at http:// www.informatics.sussex.ac.uk/users/tz21/coling08.zip 6 We used WEKA 3.4.11 (http://www.cs.waikato.ac.nz/ ml/ weka ) both classifiers in 10-fold stratified cross-validation mode, resulting in the accuracies shown in Table 2. The macroaveraged accuracies across all 10 corpora are 82.78% (NBm) and 80.89% (SVM). Corpus Nbm (%) SVM (%) Monitors 86.21 83.87 Mobile phones 86.52 84.49 Digital cameras 82.27 82.0</context>
</contexts>
<marker>Ku, Liang, Chen, 2006</marker>
<rawString>Ku, Lun-Wei, Yu-Ting Liang, and Hsin-Hsi Chen. 2006. Opinion Extraction, Summarization and Tracking in News and Blog Corpora. In Proceedings of the AAAI-2006 Spring Symposium on Computational Approaches to Analyzing Weblogs. AAAI Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kerry Hannan</author>
<author>Tyler Neylon</author>
<author>Mike Wells</author>
<author>Jeff Reynar</author>
</authors>
<title>Structured Models for Fine-to-Coarse Sentiment Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<pages>432439</pages>
<contexts>
<context position="5221" citStr="McDonald et al. (2007)" startWordPosition="782" endWordPosition="785">tigate domain adaptation for sentiment classifiers using structural correspondence learning. Read (2005) also observed significant differences between the accuracy of classification of reviews in the same domain but published in different time periods. Recently, there has been a shift of interest towards more fine-grained approaches to processing of sentiment, in which opinion is extracted at the sentence level, sometimes including information about different features of a product that are commented on and/or the opinion holder (Hu and Liu, 2004; Ku et al., 2006). But even in such approaches, McDonald et al. (2007) note that information about the overall sentiment orientation of a document facilitates more accurate extraction of more specific information from the text. 2.2 Unsupervised Approach One way of tackling the problem of domain dependency could be to use an approach that does not rely on annotated data. Turney (2002) describes a method of sentiment classification using two human-selected seed words (the words poor and excellent) in conjunction with a very large text corpus; the semantic orientation of phrases is computed as their association with the seed words (as measured by pointwise mutual i</context>
</contexts>
<marker>McDonald, Hannan, Neylon, Wells, Reynar, 2007</marker>
<rawString>McDonald, Ryan, Kerry Hannan, Tyler Neylon, Mike Wells, and Jeff Reynar. 2007. Structured Models for Fine-to-Coarse Sentiment Analysis. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. 432439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>271278</pages>
<contexts>
<context position="3499" citStr="Pang and Lee (2004)" startWordPosition="506" endWordPosition="509"> based on supervised machine learning. For example, Pang et al. (2002) collected movie reviews that had been annotated with respect to sentiment by the authors of the reviews, and used this data to train supervised classifiers. A number of studies have investigated the impact on classification accuracy of different factors, including choice of feature set, machine learning algorithm, and pre-selection of the segments of text to be classified. For example, Dave et al. (2003) experiment with the use of linguistic, statistical and n-gram features and measures for feature selection and weighting. Pang and Lee (2004) use a graph-based technique to identify and analyze only subjective parts of texts. Yu and Hatzivassiloglou (2003) use semanticallyoriented words for identification of polarity at the sentence level. Most of this work assumes binary classification (positive and negative), some1073 \x0ctimes with the addition of a neutral class (in terms of polarity, representing lack of sentiment). While supervised systems generally achieve reasonably high accuracy, they do so only on test data that is similar to the training data. To move to another domain one would have to collect annotated data in the new </context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Pang, Bo, and Lillian Lee. 2004. A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics. 271278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>7986</pages>
<contexts>
<context position="2950" citStr="Pang et al. (2002)" startWordPosition="418" endWordPosition="421">ons in the language. The paper is structured as follows. Section 2 surveys related work in sentiment classification, unsupervised machine learning and Chinese language processing. Section 3 motivates our approach, which is described in detail in Section 4. The data used for experiments and baselines, as well as the results of experiments are covered in Section 5. Section 6 discusses the lessons learned and proposes directions for future work. 2 Related Work 2.1 Sentiment Classification Most work on sentiment classification has used approaches based on supervised machine learning. For example, Pang et al. (2002) collected movie reviews that had been annotated with respect to sentiment by the authors of the reviews, and used this data to train supervised classifiers. A number of studies have investigated the impact on classification accuracy of different factors, including choice of feature set, machine learning algorithm, and pre-selection of the segments of text to be classified. For example, Dave et al. (2003) experiment with the use of linguistic, statistical and n-gram features and measures for feature selection and weighting. Pang and Lee (2004) use a graph-based technique to identify and analyz</context>
<context position="7145" citStr="Pang et al. (2002)" startWordPosition="1101" endWordPosition="1104">, NLP researchers working with Chinese use an initial segmentation module that is intended to break a text into words. Although this can facilitate the use of subsequent computational techniques, there is no a clear definition of what a &amp;apos;word&amp;apos; is in the modern Chinese language, so the use of such segmenters is of dubious theoretical status; indeed, good results have been reported from systems which do not assume such pre-processing (Foo and Li, 2004; Xu et al., 2004). 2.4 Seed Word Selection We are not aware of any sentiment analysis system that uses unsupervised seed word selection. However, Pang et al. (2002) showed that it is difficult to get good coverage of a target domain from manually selected words, and even simple corpus frequency counts may produce a better list of features for supervised classification: human-created lists resulted in 64% accuracy on a movie review corpus, while a list of frequent words scored 69%. Pang et al. also observed that some words without any significant emotional orientation were quite good features: for example, the word still turned out to be a good indicator of positive reviews as it was often used in sentences such as Still, though, it was worth seeing&amp;apos;&amp;apos;. 3 </context>
<context position="23157" citStr="Pang et al. (2002)" startWordPosition="3731" endWordPosition="3734">performed the supervised approach, while three other results were less than 1% inferior to the supervised ones. How does it happen that the chosen seed is usually (in our dataset always) positive? We think that this happens due to the socially accepted norm of behaviour: as a rule one needs to be friendly to communicate with others. This in turn defines linguistic means of expressing ideas they will be at least slightly positive overall. The higher prevalence of positive reviews has been observed previously: for example, in our corpus before we balanced it almost 80% of reviews were positive; Pang et al. (2002) constructed their move review corpus from an original dataset of 1301 positive and 752 negative reviews (63% positive). Ghose et al. (2007) quote typical examples of highly positive language used in the online marketplace. We can make a preliminary conclusion that a relatively high frequency of positive words is determined by the usage of language that reflects the social behaviour of people. In future work we intend to explore these issues of positivity of language use. We will also apply our approach to other genres containing some quantity of evaluative language (for example newspaper arti</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing. 7986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathon Read</author>
</authors>
<title>Using Emoticons to Reduce Dependency in Machine Learning Techniques for Sentiment Classification.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Student Research Workshop at ACL-05. 43 Slavjanskoj Kultury.</booktitle>
<contexts>
<context position="4703" citStr="Read (2005)" startWordPosition="698" endWordPosition="699">he new domain and retrain the classifier. Engstrom (2004) reports decreased accuracy in cross-domain classification since sentiment in different domains is often expressed in different ways. However, it is impossible in practice to have annotated data for all possible domains of interest. Aue and Gamon (2005) attempt to solve the problem of the absence of large amounts of labeled data by customizing sentiment classifiers to new domains using training data from other domains. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers using structural correspondence learning. Read (2005) also observed significant differences between the accuracy of classification of reviews in the same domain but published in different time periods. Recently, there has been a shift of interest towards more fine-grained approaches to processing of sentiment, in which opinion is extracted at the sentence level, sometimes including information about different features of a product that are commented on and/or the opinion holder (Hu and Liu, 2004; Ku et al., 2006). But even in such approaches, McDonald et al. (2007) note that information about the overall sentiment orientation of a document facil</context>
</contexts>
<marker>Read, 2005</marker>
<rawString>Read, Jonathon. 2005. Using Emoticons to Reduce Dependency in Machine Learning Techniques for Sentiment Classification. In Proceedings of the ACL Student Research Workshop at ACL-05. 43 Slavjanskoj Kultury.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>417424</pages>
<contexts>
<context position="5537" citStr="Turney (2002)" startWordPosition="835" endWordPosition="836">pproaches to processing of sentiment, in which opinion is extracted at the sentence level, sometimes including information about different features of a product that are commented on and/or the opinion holder (Hu and Liu, 2004; Ku et al., 2006). But even in such approaches, McDonald et al. (2007) note that information about the overall sentiment orientation of a document facilitates more accurate extraction of more specific information from the text. 2.2 Unsupervised Approach One way of tackling the problem of domain dependency could be to use an approach that does not rely on annotated data. Turney (2002) describes a method of sentiment classification using two human-selected seed words (the words poor and excellent) in conjunction with a very large text corpus; the semantic orientation of phrases is computed as their association with the seed words (as measured by pointwise mutual information). The sentiment of a document is calculated as the average semantic orientation of all such phrases. Yarowsky (1995) describes a &amp;apos;semi-unsupervised&amp;apos; approach to the problem of sense disambiguation of words, also using a set of initial seeds, in this case a few high quality sense annotations. These annota</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney, Peter D. 2002. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. 417424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Do We Need Chinese Word Segmentation for Statistical Machine Translation?</title>
<date>2004</date>
<booktitle>In Proceedings of the Third SIGHAN Workshop on Chinese Language Learning.</booktitle>
<pages>122128</pages>
<contexts>
<context position="6998" citStr="Xu et al., 2004" startWordPosition="1076" endWordPosition="1079">nese Language Processing A major issue in processing Chinese text is the fact that words are not delimited in the written language. In many cases, NLP researchers working with Chinese use an initial segmentation module that is intended to break a text into words. Although this can facilitate the use of subsequent computational techniques, there is no a clear definition of what a &amp;apos;word&amp;apos; is in the modern Chinese language, so the use of such segmenters is of dubious theoretical status; indeed, good results have been reported from systems which do not assume such pre-processing (Foo and Li, 2004; Xu et al., 2004). 2.4 Seed Word Selection We are not aware of any sentiment analysis system that uses unsupervised seed word selection. However, Pang et al. (2002) showed that it is difficult to get good coverage of a target domain from manually selected words, and even simple corpus frequency counts may produce a better list of features for supervised classification: human-created lists resulted in 64% accuracy on a movie review corpus, while a list of frequent words scored 69%. Pang et al. also observed that some words without any significant emotional orientation were quite good features: for example, the </context>
</contexts>
<marker>Xu, Zens, Ney, 2004</marker>
<rawString>Xu, Jia, Richard Zens, and Hermann Ney. 2004. Do We Need Chinese Word Segmentation for Statistical Machine Translation? In Proceedings of the Third SIGHAN Workshop on Chinese Language Learning. 122128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>189196</pages>
<contexts>
<context position="5948" citStr="Yarowsky (1995)" startWordPosition="901" endWordPosition="902">n of more specific information from the text. 2.2 Unsupervised Approach One way of tackling the problem of domain dependency could be to use an approach that does not rely on annotated data. Turney (2002) describes a method of sentiment classification using two human-selected seed words (the words poor and excellent) in conjunction with a very large text corpus; the semantic orientation of phrases is computed as their association with the seed words (as measured by pointwise mutual information). The sentiment of a document is calculated as the average semantic orientation of all such phrases. Yarowsky (1995) describes a &amp;apos;semi-unsupervised&amp;apos; approach to the problem of sense disambiguation of words, also using a set of initial seeds, in this case a few high quality sense annotations. These annotations are used to start an iterative process of learning information about the contexts in which senses of words appear, in each iteration labeling senses of previously unlabeled word tokens using information from the previous iteration. 2.3 Chinese Language Processing A major issue in processing Chinese text is the fact that words are not delimited in the written language. In many cases, NLP researchers wor</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, David. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics. 189196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>129136</pages>
<contexts>
<context position="3614" citStr="Yu and Hatzivassiloglou (2003)" startWordPosition="523" endWordPosition="526">been annotated with respect to sentiment by the authors of the reviews, and used this data to train supervised classifiers. A number of studies have investigated the impact on classification accuracy of different factors, including choice of feature set, machine learning algorithm, and pre-selection of the segments of text to be classified. For example, Dave et al. (2003) experiment with the use of linguistic, statistical and n-gram features and measures for feature selection and weighting. Pang and Lee (2004) use a graph-based technique to identify and analyze only subjective parts of texts. Yu and Hatzivassiloglou (2003) use semanticallyoriented words for identification of polarity at the sentence level. Most of this work assumes binary classification (positive and negative), some1073 \x0ctimes with the addition of a neutral class (in terms of polarity, representing lack of sentiment). While supervised systems generally achieve reasonably high accuracy, they do so only on test data that is similar to the training data. To move to another domain one would have to collect annotated data in the new domain and retrain the classifier. Engstrom (2004) reports decreased accuracy in cross-domain classification since </context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Yu, Hong, and Vasileios Hatzivassiloglou. 2003. Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing. 129136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taras Zagibalov</author>
<author>John Carroll</author>
</authors>
<title>Unsupervised Classification of Sentiment and Objectivity in Chinese Text.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing. 304311. \x0cAppendix</booktitle>
<contexts>
<context position="8231" citStr="Zagibalov &amp; Carroll, 2008" startWordPosition="1279" endWordPosition="1282">ill turned out to be a good indicator of positive reviews as it was often used in sentences such as Still, though, it was worth seeing&amp;apos;&amp;apos;. 3 Our Approach Our main goal is to overcome the problem of domain-dependency in sentiment classification. Unsupervised approaches seem promising in this regard, since they do not require annotated training data, just access to sufficient raw text in each domain. We base our approach on a previously described, &amp;apos;almost-unsupervised&amp;apos; system that starts with only a single, human-selected seed (good) and uses an iterative method to extract a training sub-corpus (Zagibalov &amp; Carroll, 2008). The approach does not use a word segmentation module; in this paper we use the term &amp;apos;lexical item&amp;apos; to denote any sequence of Chinese characters that is treated by the system as a unit, whatever it is linguistically a morpheme, a word or a phrase. 1074 \x0cOur initial aim was to investigate ways of improving the classifier by automatically finding a better seed, because Zagibalov &amp; Carroll indicate that in different domains they could, by manual trial and error, find a seed other than (good) which produced better results. To find such a seed automatically, we make two assumptions: 1. Attitude</context>
<context position="10885" citStr="Zagibalov &amp; Carroll (2008)" startWordPosition="1722" endWordPosition="1725">ion words, etc. In what follows we refer to such constructions as negated adverbial constructions. 1 We use only six frequently occurring negations: (bu), (buhui), (meiyou), (baituo), (mianqu), and (bimian). We are trying to be as language-independent as possible so we take a simplistic approach to detecting negation. 2 We use five frequently occurring adverbials: (hen), (feichang), (tai), (zui), and (bijiao). Similarly to negation, we deliberately take a simplistic approach. 4 Method We use a similar sentiment classifier and iterative retraining technique to the almost-unsupervised system of Zagibalov &amp; Carroll (2008), summarized below in Sections 4.2 and 4.3. The main new contributions of this paper are techniques for automatically finding the seeds from raw text in a particular domain (Section 4.1), and for detecting when the process should stop (Section 4.4). This new system therefore differs from that of Zagibalov &amp; Carroll (2008) in being completely unsupervised and not depending on arbitrary iteration limits. (The evaluation also differs since we focus in this paper on the effects of domain on sentiment classification accuracy). 4.1 Seed Lexical Item Identification The first step is to identify suita</context>
<context position="20339" citStr="Zagibalov &amp; Carroll (2008)" startWordPosition="3268" endWordPosition="3271">cally segment the reviews and then derive a feature set of a manageable size by setting a threshold on word frequencies; however the extra processing means that this is a less valid upper bound. Another possible comparison could be with a version of Turney&amp;apos;s (2002) sentiment classification method applied to Chinese. However, the results would not be comparable since Turney&amp;apos;s method would require the additional use of very large text corpus and the manual selection of positive and negative seed words. 5.3 Experiment 1 To be able to compare to the accuracy of the almost-unsupervised approach of Zagibalov &amp; Carroll (2008), we ran our system using the seed (good) for each corpus. The results are shown in Table 3. We compute precision, recall and F1 measure rather than just accuracy, since our classifier can omit some reviews whereas the supervised classifiers attempt to classify all reviews. The macroaveraged F1 measure is 80.55, which beats the naive baseline by over 30 percentage points, and approaches the two upper bounds. 1077 \x0cCorpus Iter P R F1 Monitors 12 86.62 86.24 86.43 Mobile phones 11 90.15 89.68 89.91 Digital cameras 13 81.33 80.23 80.78 MP3 players 13 86.10 85.10 85.60 Computer parts 10 69.10 6</context>
</contexts>
<marker>Zagibalov, Carroll, 2008</marker>
<rawString>Zagibalov, Taras, and John Carroll. 2008. Unsupervised Classification of Sentiment and Objectivity in Chinese Text. In Proceedings of the Third International Joint Conference on Natural Language Processing. 304311. \x0cAppendix A. Seeds Automatically Identified for each Corpus. Corpus Seed Corpus Seed</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>