<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.5051635">
b&apos;The Automated Acquisition of Topic Signatures for Text
Summarization
</title>
<author confidence="0.722182">
Chin-Yew Lin and Eduard Hovy
</author>
<affiliation confidence="0.913595">
Information Sciences Institute
University of Southern California
</affiliation>
<address confidence="0.661647">
Marina del Rey, CA 90292, USA
</address>
<email confidence="0.994803">
fcyl,hovyg@isi.edu
</email>
<sectionHeader confidence="0.990668" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996305">
In order to produce a good summary, one has
to identify the most relevant portions of a given
text. We describe in this paper a method for au-
tomatically training topic signatures{sets of related
words, with associated weights, organized around
head topics{and illustrate with signatures we cre-
ated with 6,194 TREC collection texts over 4 se-
lected topics. We describe the possible integration
of topic signatures with ontologies and its evaluaton
on an automated text summarization system.
</bodyText>
<sectionHeader confidence="0.998301" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99959996">
This paper describes the automated creation of what
we call topic signatures, constructs that can play a
central role in automated text summarization and
information retrieval. Topic signatures can be used
to identify the presence of a complex concept|a
concept that consists of several related components
in \x0cxed relationships. Restaurant-visit, for example,
involves at least the concepts menu, eat, pay, and
possibly waiter, and Dragon Boat Festival (in Tai-
wan) involves the concepts calamus (a talisman to
ward o\x0b evil), moxa (something with the power of
preventing pestilence and strengthening health), pic-
tures of Chung Kuei (a nemesis of evil spirits), eggs
standing on end, etc. Only when the concepts co-
occur is one licensed to infer the complex concept;
eat or moxa alone, for example, are not sucient. At
this time, we do not consider the interrelationships
among the concepts.
Since many texts may describe all the compo-
nents of a complex concept without ever explic-
itly mentioning the underlying complex concept|a
topic|itself, systems that have to identify topic(s),
for summarization or information retrieval, require
a method of inferring complex concepts from their
component words in the text.
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.973949533333333">
In late 1970\&apos;s, DeJong (DeJong, 1982) developed a
system called FRUMP (Fast Reading Understand-
ing and Memory Program) to skim newspaper sto-
ries and extract the main details. FRUMP uses
a data structure called sketchy script to organize
its world knowledge. Each sketchy script is what
FRUMP knows about what can occur in particu-
lar situations such as demonstrations, earthquakes,
labor strikes, and so on. FRUMP selects a partic-
ular sketchy script based on clues to styled events
in news articles. In other words, FRUMP selects an
empty template1
whose slots will be \x0clled on the
y
as FRUMP reads a news article. A summary is gen-
erated based on what has been captured or \x0clled in
the template.
The recent success of information extraction re-
search has encouraged the FRUMP approach. The
SUMMONS (SUMMarizing Online NewS articles)
system (McKeown and Radev, 1999) takes tem-
plate outputs of information extraction systems de-
veloped for MUC conference and generating sum-
maries of multiple news articles. FRUMP and SUM-
MONS both rely on prior knowledge of their do-
mains. However, to acquire such prior knowledge
is labor-intensive and time-consuming. For exam-
ple, the University of Massachusetts CIRCUS sys-
tem used in the MUC-3 (SAIC, 1998) terrorism do-
main required about 1500 person-hours to de\x0cne ex-
traction patterns2
(Rilo\x0b, 1996). In order to make
them practical, we need to reduce the knowledge en-
gineering bottleneck and improve the portability of
FRUMP or SUMMONS-like systems.
Since the world contains thousands, or perhaps
millions, of complex concepts, it is important to be
able to learn sketchy scripts or extraction patterns
automatically from corpora|no existing knowledge
base contains nearly enough information. (Rilo\x0b and
Lorenzen, 1999) present a system AutoSlog-TS that
generates extraction patterns and learns lexical con-
straints automatically from preclassi\x0ced text to al-
leviate the knowledge engineering bottleneck men-
tioned above. Although Rilo\x0b applied AutoSlog-TS
</bodyText>
<footnote confidence="0.682398">
1We viewed sketchy scripts and templates as equivalent
constructs in the sense that they specify high level entities
and relationships for speci\x0cc topics.
2An extraction pattern is essentially a case frame contains
</footnote>
<bodyText confidence="0.986825814814815">
its trigger word, enabling conditions, variable slots, and slot
constraints. CIRCUS uses a database of extraction patterns
to parse texts (Rilo\x0b, 1996).
\x0cto text categorization and information extraction,
the concept of relevancy signatures introduced by
her is very similar to the topic signatures we pro-
posed in this paper. Relevancy signatures and topic
signatures are both trained on preclassi\x0ced docu-
ments of speci\x0cc topics and used to identify the
presence of the learned topics in previously unseen
documents. The main di\x0berences to our approach
are: relevancy signatures require a parser. They are
sentence-based and applied to text categorization.
On the contrary, topic signatures only rely on cor-
pus statistics, are document-based3
and used in text
summarization.
In the next section, we describe the automated
text summarization system SUMMARIST that we
used in the experiments to provide the context of
discussion. We then de\x0cne topic signatures and de-
tail the procedures for automatically constructing
topic signatures. In Section 5, we give an overview
of the corpus used in the evaluation. In Section 6 we
present the experimental results and the possibility
of enriching topic signatures using an existing ontol-
ogy. Finally, we end this paper with a conclusion.
</bodyText>
<sectionHeader confidence="0.7691945" genericHeader="method">
3 SUMMARIST
SUMMARIST (Hovy and Lin, 1999) is a system
</sectionHeader>
<bodyText confidence="0.999388727272727">
designed to generate summaries of multilingual in-
put texts. At this time, SUMMARIST can process
English, Arabic, Bahasa Indonesia, Japanese, Ko-
rean, and Spanish texts. It combines robust natural
language processing methods (morphological trans-
formation and part-of-speech tagging), symbolic
world knowledge, and information retrieval tech-
niques (term distribution and frequency) to achieve
high robustness and better concept-level generaliza-
tion.
The core of SUMMARIST is based on the follow-
</bodyText>
<equation confidence="0.494675">
ing `equation\&apos;:
summarization = topic identi\x0ccation +
topic interpretation + generation.
</equation>
<bodyText confidence="0.915082769230769">
These three stages are:
Topic Identi\x0ccation: Identify the most important
(central) topics of the texts. SUMMARIST
uses positional importance, topic signature, and
term frequency. Importance based on discourse
structure will be added later. This is the most
developed stage in SUMMARIST.
Topic Interpretation: To fuse concepts such as
waiter, menu, and food into one generalized
concept restaurant, we need more than the sim-
ple word aggregation used in traditional infor-
mation retrieval. We have investigated concept
3We would like to use only the relevant parts of documents
to generate topic signatures in the future. Text segmentation
algorithms such as TextTiling (Hearst, 1997) can be used to
\x0cnd subtopic segments in text.
ABCNEWS.com : Delay in Handing Flight 990 Probe to FBI
NTSB Chairman James Hall says Egyptian officials want to review results
of the investigation into the crash of EgyptAir Flight 990 before the case
is turned over to the FBI.
Nov. 16 - U.S. investigators appear to be leaning more than ever toward
the possibility that one of the co-pilots of EgyptAir Flight 990 may have
deliberately crashed the plane last month, killing all 217 people on board.
However, U.S. officials say the National Transportation Safety Board will
delay transferring the investigation of the Oct. 31 crash to the FBI - the
agency that would lead a criminal probe - for at least a few days, to allow
Egyptian experts to review evidence in the case.
Suspicions of foul play were raised after investigators listening to a tape
from the cockpit voice recorder isolated a religious prayer or statement
made by the co-pilot just before the plane\&apos;s autopilot was turned off
and the plane began its initial plunge into the Atlantic Ocean off Mas-
sachusetts\&apos; Nantucket Island.
Over the past week, after much effort, the NTSB and the Navy succeeded
in locating the plane\&apos;s two &amp;quot;black boxes,&amp;quot; the cockpit voice recorder and
the flight data recorder.
The tape indicates that shortly after the plane leveled off at its cruising
altitude of 33,000 feet, the chief pilot of the aircraft left the plane\&apos;s
cockpit, leaving one of the two co-pilots alone there as the aircraft began
its descent.
</bodyText>
<figureCaption confidence="0.986524">
Figure 1: A Nov. 16 1999 ABC News page summary
</figureCaption>
<bodyText confidence="0.98735365">
generated by SUMMARIST.
counting and topic signatures to tackle the fu-
sion problem.
Summary Generation: SUMMARIST can pro-
duce keyword and extract type summaries.
Figure 1 shows an ABC News page summary
about EgyptAir Flight 990 by SUMMARIST. SUM-
MARIST employs several di\x0berent heuristics in the
topic identi\x0ccation stage to score terms and sen-
tences. The score of a sentence is simply the sum
of all the scores of content-bearing terms in the sen-
tence. These heuristics are implemented in separate
modules using inputs from preprocessing modules
such as tokenizer, part-of-speech tagger, morpholog-
ical analyzer, term frequency and t\x0cdf weights cal-
culator, sentence length calculator, and sentence lo-
cation identi\x0cer. We only activate the position mod-
ule, the t\x0cdf module, and the topic signature module
for comparison. We discuss the e\x0bectiveness of these
modules in Section 6.
</bodyText>
<sectionHeader confidence="0.992962" genericHeader="method">
4 Topic Signatures
</sectionHeader>
<bodyText confidence="0.973404941176471">
Before addressing the problem of world knowledge
acquisition head-on, we decided to investigate what
type of knowledge would be useful for summariza-
tion. After all, one can spend a lifetime acquir-
ing knowledge in just a small domain. But what
is the minimum amount of knowledge we need to
enable e\x0bective topic identi\x0ccation as illustrated by
the restaurant-visit example? Our idea is simple.
We would collect a set of terms4
that were typi-
cally highly correlated with a target concept from a
preclassi\x0ced corpus such as TREC collections, and
then, during summarization, group the occurrence of
the related terms by the target concept. For exam-
ple, we would replace joint instances of table, menu,
waiter, order, eat, pay, tip, and so on, by the single
phrase restaurant-visit, in producing an indicative
</bodyText>
<footnote confidence="0.743224666666667">
4Terms can be stemmed words, bigrams, or trigrams.
\x0csummary. We thus de\x0cned a topic signature as a
family of related terms, as follows:
</footnote>
<equation confidence="0.9838165">
TS = ftopic;signatureg
= ftopic;&lt; (t1;w1);:::;(tn;wn) &gt;g (1)
</equation>
<bodyText confidence="0.998975714285714">
where topic is the target concept and signature is
a vector of related terms. Each ti is an term highly
correlated to topic with association weight wi. The
number of related terms n can be set empirically
according to a cuto\x0b associated weight. We describe
how to acquire related terms and their associated
weights in the next section.
</bodyText>
<subsectionHeader confidence="0.8859765">
4.1 Signature Term Extraction and Weight
Estimation
</subsectionHeader>
<bodyText confidence="0.930695846153846">
On the assumption that semantically related terms
tend to co-occur, one can construct topic signa-
tures from preclassi\x0ced text using the \x1f2
test, mu-
tual information, or other standard statistic tests
and information-theoretic measures. Instead of \x1f2
,
we use likelihood ratio (Dunning, 1993) \x15, since \x15
is more appropriate for sparse data than \x1f2
test
and the quantity 2log\x15 is asymptotically \x1f2
dis-
tributed5
. Therefore, we can determine the con\x0c-
dence level for a speci\x0cc 2log\x15 value by looking up
\x1f2
distribution table and use the value to select an
appropriate cuto\x0b associated weight.
We have documents preclassi\x0ced into a set R of
relevant texts and a set ~
R of nonrelevant texts for a
given topic. Assuming the following two hypotheses:
Hypothesis 1 (H1): P(Rjti) = p = P(Rj~
ti), i.e.
the relevancy of a document is independent of
ti.
</bodyText>
<equation confidence="0.692816">
Hypothesis 2 (H2): P(Rjti) = p1 6= p2 =
P(Rj~
</equation>
<bodyText confidence="0.999095">
ti), i.e. the presence of ti indicates strong
relevancy assuming p1 \x1d p2.
and the following 2-by-2 contingency table:
</bodyText>
<equation confidence="0.9972816">
R ~
R
ti O11 O12
~
ti O21 O22
</equation>
<bodyText confidence="0.984587333333333">
where O11 is the frequency of term ti occurring in
the relevant set, O12 is the frequency of term ti oc-
curring in the nonrelevant set, O21 is the frequency
</bodyText>
<construct confidence="0.514152166666667">
of term ~
ti 6= ti occurring in the relevant set, O22 is
the frequency of term ~
ti 6= ti occurring in the non-
relevant set.
Assuming a binomial distribution:
</construct>
<equation confidence="0.918825">
b(k;n;x) =
\x12n
k
\x13
xk(1 x)(n k)
</equation>
<footnote confidence="0.7150915">
(2)
5This assumes that the ratio is between the maximum like-
</footnote>
<bodyText confidence="0.863388">
lihood estimate over a subpart of the parameter space and the
maximum likelihood estimate over the entire parameter space.
See (Manning and Sch\x7f
utze, 1999) pages 172 to 175 for details.
then the likelihood for H1 is:
</bodyText>
<equation confidence="0.984191714285714">
L(H1) = b(O11;O11 + O12;p)b(O21;O21 + O22;p)
and for H2 is:
L(H2) = b(O11;O11 + O12;p1)b(O21;O21 + O22;p2)
The 2log\x15 value is then computed as follows:
= 2log
L(H1)
L(H2)
= 2log
b(O11; O11 + O12;p)b(O21; O21 + O22; p)
b(O11; O11 + O12;p1)b(O21; O21 + O22; p2)
= 2((O11 + O21)log p + (O12 + O22)log (1 p) (3)
(O11log p1 + O12log (1 p1) + O21log p2 + O22log (1 p2)))
= 2N \x02 (H(R) H(RjT)) (4)
= 2N \x02 I(R; T) (5)
</equation>
<bodyText confidence="0.9889705">
where N = O11 +O12 +O21 +O22 is the total num-
ber of term occurrence in the corpus, H(R) is the
entropy of terms over relevant and nonrelevant sets
of documents, H(RjT ) is the entropy of a given term
over relevant and nonrelevant sets of documents, and
I(R;T ) is the mutual information between docu-
ment relevancy and a given term. Equation 5 indi-
cates that mutual information6
is an equivalent mea-
sure to likelihood ratio when we assume a binomial
distribution and a 2-by-2 contingency table.
To create topic signature for a given topic, we:
</bodyText>
<listItem confidence="0.996855833333333">
1. classify documents as relevant or nonrelevant
according to the given topic
2. compute the 2log\x15 value using Equation 3 for
each term in the document collection
3. rank terms according to their 2log\x15 value
4. select a con\x0cdence level from the \x1f2
</listItem>
<bodyText confidence="0.893302333333333">
distribution
table; determine the cuto\x0b associated weight
and the number of terms to be included in the
</bodyText>
<sectionHeader confidence="0.688762" genericHeader="method">
signatures
5 The Corpus
</sectionHeader>
<bodyText confidence="0.989284">
The training data derives from the Question and
Answering summary evaluation data provided by
TIPSTER-SUMMAC (Mani et al., 1998) that is a
subset of the TREC collections. The TREC data is a
collection of texts, classi\x0ced into various topics, used
for formal evaluations of information retrieval sys-
tems in a series of annual comparisons. This data set
contains essential text fragments (phrases, clauses,
and sentences) which must be included in summaries
to answer some TREC topics. These fragments are
each judged by a human judge. As described in Sec-
tion 3, SUMMARIST employs several independent
modules to assign a score to each sentence, and then
combines the scores to decide which sentences to ex-
tract from the input text. One can gauge the ecacy
</bodyText>
<listItem confidence="0.571102857142857">
6The mutual information is de\x0cned according to chapter 2
of (Cover and Thomas, 1991) and is not the pairwise mutual
information used in (Church and Hanks, 1990).
\x0cTREC Topic Description
hnumi Number: 151
htitlei Topic: Coping with overcrowded prisons
hdesci Description:
</listItem>
<bodyText confidence="0.989674888888889">
The document will provide information on jail and prison overcrowding
and how inmates are forced to cope with those conditions; or it will
reveal plans to relieve the overcrowded condition.
hnarri Narrative:
A relevant document will describe scenes of overcrowding that have
become all too common in jails and prisons around the country. The
document will identify how inmates are forced to cope with those over-
crowded conditions, and/or what the Correctional System is doing, or
planning to do, to alleviate the crowded condition.
</bodyText>
<table confidence="0.657951">
h/topi
Test Questions
Q1 What are name and/or location of the correction facilities
where the reported overcrowding exists?
Q2 What negative experiences have there been at the overcrowded
facilities (whether or not they are thought to have been caused
by the overcrowding)?
Q3 What measures have been taken/planned/recommended (etc.)
</table>
<bodyText confidence="0.897939888888889">
to accommodate more inmates at penal facilities, e.g., doubling
up, new construction?
Q4 What measures have been taken/planned/recommended (etc.)
to reduce the number of new inmates, e.g., moratoriums
on admission, alternative penalties, programs to reduce
crime/recidivism?
Q5 What measures have been taken/planned/recommended (etc.)
to reduce the number of existing inmates at an overcrowded
facility, e.g., granting early release, transfering to uncrowded
</bodyText>
<figure confidence="0.7806358">
facilities?
Sample Answer Keys
hDOCNOi AP891027-0063 h/DOCNOi
hFILEIDiAP-NR-10-27-89 0615EDTh/FILEIDi
h1ST LINEir a PM-ChainedInmates 10-27 0335h/1ST LINEi
h2ND LINEiPM-Chained Inmates,0344h/2ND LINEi
hHEADiInmates Chained to Walls in Baltimore Police
Stationsh/HEADi
hDATELINEiBALTIMORE (AP) h/DATELINEi
hTEXTi
</figure>
<bodyText confidence="0.9554682">
hQ3iPrisoners are kept chained to the walls of local police lockups for
as long as three days at a time because of overcrowding in regular jail
cells, police said.h/Q3i
Overcrowding at the hQ1iBaltimore County Detention Centerh/Q1i
has forced police to ...
</bodyText>
<equation confidence="0.380932">
h/TEXTi
</equation>
<tableCaption confidence="0.960068">
Table 1: TREC topic description for topic 151, test
</tableCaption>
<bodyText confidence="0.868021421052632">
questions expected to be answered by relevant doc-
uments, and a sample document with answer keys.
of each module by comparing, for di\x0berent amounts
of extraction, how many `good\&apos; sentences the module
selects by itself. We rate a sentence as good simply
if it also occurs in the ideal human-made extract,
and measure it using combined recall and precision
(F-score). We used four topics7
of total 6,194 doc-
uments from the TREC collection. 138 of them are
relevant documents with TIPSTER-SUMMAC pro-
vided answer keys for the question and answering
evaluation. Model extracts are created automati-
cally from sentences containing answer keys. Table
1 shows TREC topic description for topic 151, test
questions expected to be answered by relevant doc-
uments8
, and a sample relevant document with an-
swer keys markup.
</bodyText>
<footnote confidence="0.729263">
7These four topics are:
</footnote>
<figureCaption confidence="0.881401">
topic 151: Overcrowded Prisons, 1211 texts, 85 relevant;
topic 257: Cigarette Consumption, 1727 texts, 126 relevant;
topic 258: Computer Security, 1701 texts, 49 relevant;
topic 271: Solar Power, 1555 texts, 59 relevant.
8A relevant document only needs to answer at least one of
the \x0cve questions.
</figureCaption>
<sectionHeader confidence="0.983138" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.988618181818182">
In order to assess the utility of topic signatures in
text summarization, we follow the procedure de-
scribed at the end of Section 4.1 to create topic
signature for each selected TREC topic. Docu-
ments are separated into relevant and nonrelevant
sets according to their TREC relevancy judgments
for each topic. We then run each document through
a part-of-speech tagger and convert each word into
its root form based on the WordNet lexical database.
We also collect individual root word (unigram) fre-
quency, two consecutive non-stopword9
(bigram) fre-
quency, and three consecutive non-stopwords (tri-
gram) frequency to facilitate the computation of the
2log\x15 value for each term. We expect high rank-
ing bigram and trigram signature terms to be very
informative. We set the cuto\x0b associated weight at
10.83 with con\x0cdence level \x0b = 0:001 by looking up
a \x1f2
statistical table.
Table 2 shows the top 10 unigram, bigram, and tri-
gram topic signature terms for each topic 10
. Several
conclusions can be drawn directly. Terms with high
2log\x15 are indeed good indicators for their corre-
sponding topics. The 2log\x15 values decrease as the
number of words in a term increases. This is rea-
sonable, since longer terms usually occur less often
than their constituents. However, bigram terms are
more informative than unigram terms as we can ob-
serve: jail/prison overcrowding of topic 151, tobacco
industry of topic 257, computer security of topic 258,
and solar energy/power of topic 271. These auto-
matically generated signature terms closely resemble
or equal the given short TREC topic descriptions.
Although trigram terms shown in the table, such
as federal court order, philip morris rjr, jet propul-
sion laboratory, and mobile telephone system are also
meaningful, they do not demonstrate the closer term
relationship among other terms in their respective
topics that is seen in the bigram cases. We expect
that more training data can improve the situation.
We notice that the 2log\x15 values for topic 258
are higher than those of the other three topics. As
indicated by (Mani et al., 1998) the majority of rel-
evant documents for topic 258 have the query topic
as their main theme; while the others mostly have
the query topics as their subsidiary themes. This
implies that it is too liberal to assume all the terms
in relevant documents of the other three topics are
relevant. We plan to apply text segmentation algo-
rithms such as TextTiling (Hearst, 1997) to segment
documents into subtopic units. We will then per-
form the topic signature creation procedure only on
the relevant units to prevent inclusion of noise terms.
</bodyText>
<footnote confidence="0.963619666666667">
9We use the stopword list supplied with the SMART re-
trieval system.
10The 2log\x15 values are not comparable across ngram cat-
</footnote>
<bodyText confidence="0.943834979591837">
egories, since each ngram category has its own sample space.
\x0cTopic 10 Signature Terms of Topic 151  |Overcrowded Prisons
Unigram 2log\x15 Bigram 2log\x15 Trigram 2log\x15
jail 461.044 county jail 160.273 federal court order 45.960
county 408.821 early release 85.361 comply consent decree 35.121
overcrowding 342.349 state prison 74.372 dekalb county sheriff 35.121
inmate 234.765 state prisoner 67.666 gov. jo frank 35.121
sheriff 154.440 day fine 61.465 joe frank harris 35.121
state 151.940 jail overcrowding 61.329 prisoner county jail 35.121
prisoner 148.178 court order 60.090 state prison county 28.043
prison 145.306 local jail 56.440 \&apos;t put prison 26.341
city 133.477 prison overcrowding 55.373 county jail state 26.341
overcrowded 128.008 central facility 52.909 hold local jail 26.341
Topic 10 Signature Terms of Topic 257  |Cigarette Consumption
Unigram 2log\x15 Bigram 2log\x15 Trigram 2log\x15
cigarette 476.038 tobacco industry 80.768 philip morris rjr 28.061
tobacco 313.017 bn cigarette 67.429 rothmans benson hedge 26.969
smoking 284.198 philip morris 54.073 lung cancer death 22.214
smoke 159.134 cigarette year 48.045 qtr firm chg 21.418
rothmans 156.675 rothmans international 44.434 qtr qtr firm 21.418
osha 148.372 tobacco smoke 44.269 bn bn bn 20.226
seita 126.421 sir patrick 40.455 consumption bn cigarette 20.226
ban 113.849 cigarette company 39.399 great american smokeout 20.226
smoker 104.110 cent market 36.223 lung cancer heart 20.226
bat 79.903 tax increase 36.223 malaysian singapore company 20.226
Topic 10 Signature Terms of Topic 258  |Computer Security
Unigram 2log\x15 Bigram 2log\x15 Trigram 2log\x15
computer 1159.351 computer security 213.331 jet propulsion laboratory 98.854
virus 927.674 graduate student 178.588 robert t. mo 98.854
hacker 887.377 computer system 146.328 cornell university graduate 79.081
morris 666.392 research center 132.413 lawrence berkeley laboratory 79.081
cornell 385.684 computer virus 126.033 nasa jet propulsion 79.081
university 305.958 cornell university 108.741 university graduate student 79.081
system 290.347 nuclear weapon 107.283 lawrence livermore national 69.195
laboratory 287.521 military computer 106.522 livermore national laboratory 69.195
lab 225.516 virus program 106.522 computer security expert 66.196
mcclary 128.515 west german 82.210 security center bethesda 49.423
Topic 10 Signature Terms of Topic 271  |Solar Power
Unigram 2log\x15 Bigram 2log\x15 Trigram 2log\x15
solar 484.315 solar energy 268.521 division multiple access 31.347
mazda 308.015 solar power 94.210 mobile telephone service 31.347
leo 276.932 christian aid 86.211 british technology group 23.510
iridium 258.705 leo system 70.535 earth height mile 23.510
pavilion 203.811 mobile telephone 70.535 financial backing iridium 23.510
pound 128.121 iridium project 62.697 global mobile satellite 23.510
tower 126.353 real goods 61.901 handheld mobile telephone 23.510
lookout 125.406 science park 54.859 mobile satellite system 23.510
inmarsat 109.728 solar concentrator 54.859 motorola iridium project 23.510
boydston 78.373 bp solar 31.347 active solar system 15.673
</bodyText>
<tableCaption confidence="0.99432">
Table 2: Top 10 signature terms of unigram, bigram, and trigram for four TREC topics.
</tableCaption>
<subsectionHeader confidence="0.985615">
6.1 Comparing Summary Extraction
E\x0bectiveness Using Topic Signatures,
</subsectionHeader>
<bodyText confidence="0.97525175">
TFIDF, and Baseline Algorithms
In order to evaluate the e\x0bectiveness of topic signa-
tures used in summary extraction, we compare the
summary sentences extracted by the topic signature
module, baseline module, and t\x0cdf modules with hu-
man annotated model summaries. We measure the
performance using a combined measure of recall (R)
and precision (P), F. F-score is de\x0cned by:
</bodyText>
<equation confidence="0.947772545454545">
F = (1+\x0c2
)PR
\x0c2P +R
; where
R = Nme
Nm
(6)
P = Nme
Ne
(7)
Nme : # of sentences extrated that also
</equation>
<bodyText confidence="0.98197148">
appear in the model summary
Nm : # of sentences in the model summary
Ne : # of sentences extracted by the system
\x0c : relative importance of R and P
We assume equal importance of recall and preci-
sion and set \x0c to 1 in our experiments. The baseline
(position) module scores each sentence by its posi-
tion in the text. The \x0crst sentence gets the high-
est score, the last sentence the lowest. The baseline
method is expected to be e\x0bective for news genre.
The t\x0cdf module assigns a score to a term ti accord-
ing to the product of its frequency within a doc-
ument j (tfij) and its inverse document frequency
(idfj = log N
dfj
). N is the total number of documents
in the corpus and dfj is the number of documents
containing term ti.
The topic signature module scans each sentence,
assigning to each word that occurs in a topic signa-
ture the weight of that keyword in the topic signa-
ture. Each sentence then receives a topic signature
score equal to the total of all signature word scores it
contains, normalized by the highest sentence score.
This score indicates the relevance of the sentence to
the signature topic.
SUMMARIST produced extracts of the same
texts separately for each module, for a series of ex-
tracts ranging from 0% to 100% of the original text.
Although many relevant documents are available
for each topic, only some of them have answer key
\x0cmarkups. The number of documents with answer
keys are listed in the row labeled: \\# of Relevant
Docs Used in Training&amp;quot;. To ensure we utilize all
the available data and conduct a sound evaluation,
we perform a three-fold cross validation. We re-
serve one-third of documents as test set, use the rest
as training set, and repeat three times with non-
overlapping test set. Furthermore, we use only uni-
gram topic signatures for evaluation.
The result is shown in Figure 2 and Table 3. We
\x0cnd that the topic signature method outperforms
the other two methods and the t\x0cdf method performs
poorly. Among 40 possible test points for four topics
with 10% summary length increment (0% means se-
lect at least one sentence) as shown in Table 3, the
topic signature method beats the baseline method
34 times. This result is really encouraging and in-
dicates that the topic signature method is a worthy
addition to a variety of text summarization methods.
</bodyText>
<subsectionHeader confidence="0.9888175">
6.2 Enriching Topic Signatures Using
Existing Ontologies
</subsectionHeader>
<bodyText confidence="0.999755666666667">
We have shown in the previous sections that topic
signatures can be used to approximate topic iden-
ti\x0ccation at the lexical level. Although the au-
tomatically acquired signature terms for a speci\x0cc
topic seem to be bound by unknown relationships as
shown in Table 2, it is hard to image how we can
enrich the inherent
at structure of topic signatures
as de\x0cned in Equation 1 to a construct as complex
as a MUC template or script.
As discussed in (Agirre et al., 2000), we propose
using an existing ontology such as SENSUS (Knight
and Luk, 1994) to identify signature term relations.
The external hierarchical framework can be used to
generalize topic signatures and suggest richer rep-
resentations for topic signatures. Automated entity
recognizers can be used to classify unknown enti-
ties into their appropriate SENSUS concept nodes.
We are also investigating other approaches to auto-
matically learn signature term relations. The idea
mentioned in this paper is just a starting point.
</bodyText>
<sectionHeader confidence="0.996732" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9981019375">
In this paper we presented a procedure to automati-
cally acquire topic signatures and valuated the e\x0bec-
tiveness of applying topic signatures to extract topic
relevant sentences against two other methods. The
topic signature method outperforms the baseline and
the t\x0cdf methods for all test topics. Topic signatures
can not only recognize related terms (topic identi\x0c-
cation), but group related terms together under one
target concept (topic interpretation). Topic identi-
\x0ccation and interpretation are two essential steps in
a typical automated text summarization system as
we present in Section 3.
Topic signatures can also been viewed as an in-
verse process of query expansion. Query expansion
intends to alleviate the word mismatch problem in
information retrieval, since documents are normally
written in di\x0berent vocabulary. How to automati-
cally identify highly correlated terms and use them
to improve information retrieval performance has
been a main research issue since late 1960\&apos;s. Re-
cent advances in the query expansion (Xu and Croft,
1996) can also shed some light on the creation of
topic signatures. Although we focus the use of topic
signatures to aid text summarization in this paper,
we plan to explore the possibility of applying topic
signatures to perform query expansion in the future.
The results reported are encouraging enough to
allow us to continue with topic signatures as the ve-
hicle for a \x0crst approximation to world knowledge.
We are now busy creating a large number of signa-
tures to overcome the world knowledge acquisition
problem and use them in topic interpretation.
</bodyText>
<sectionHeader confidence="0.997887" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.877621333333333">
We thank the anonymous reviewers for very use-
ful suggestions. This work is supported in part by
DARPA contract N66001-97-9538.
</bodyText>
<sectionHeader confidence="0.704308" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.974223151515151">
Eneko Agirre, Olatz Ansa, Eduard Hovy, and David
Mart\x13
inez. 2000. Enriching very large ontologies
using the www. In Proceedings of the Workshop
on Ontology Construction of the European Con-
ference of AI (ECAI).
Kenneth Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information and lexicog-
raphy. In Proceedings of the 28th Annual Meeting
of the Association for Computational Linguistics
(ACL-90), pages 76{83.
Thomas Cover and Joy A. Thomas. 1991. Elements
of Information Theory. John Wiley &amp; Sons.
Gerald DeJong. 1982. An overview of the FRUMP
system. In Wendy G. Lehnert and Martin H.
Ringle, editors, Strategies for natural language
processing, pages 149{76. Lawrence Erlbaum As-
sociates.
Ted Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Computa-
tional Linguistics, 19:61{74.
Marti Hearst. 1997. TextTiling: Segmenting text
into multi-paragraph subtopic passages. Compu-
tational Linguistics, 23:33{64.
Eduard Hovy and Chin-Yew Lin. 1999. Automated
text summarization in SUMMARIST. In Inder-
jeet Mani and Mark T. Maybury, editors, Ad-
vances in Automatic Text Summarization, chap-
ter 8, pages 81{94. MIT Press.
Kevin Knight and Steve K. Luk. 1994. Building a
large knowledge base for machine translation. In
Proceedings of the Eleventh National Conference
on Arti\x0ccial Intelligence (AAAI-94).
</reference>
<figure confidence="0.983985782608696">
\x0c0.00000
0.10000
0.20000
0.30000
0.40000
0.50000
0.60000
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
Summary Length
F-Measure
151_baseline
151_tfidf
151_topic_signature
257_baseline
257_tfidf
257_topic_signature
258_baseline
258_tfidf
258_topic_signature
271_baseline
271_tfidf
271_topic_signature
258
</figure>
<page confidence="0.771199333333333">
151
271
257
</page>
<figureCaption confidence="0.996717">
Figure 2: F-measure vs. summary length for all four topics. Topic signature clearly outperform t\x0cdf and
</figureCaption>
<bodyText confidence="0.915997">
baseline except for the case of topic 258 where performance for the three methods are roughly equal.
</bodyText>
<table confidence="0.985545307692308">
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
151 baseline 0.308 0.349 0.400 0.434 0.423 0.402 0.385 0.373 0.376 0.370 0.355
151 tfidf -39.93 -35.82 -15.26 -7.22 -6.35 -3.50 2.55 3.76 3.83 -0.75 0.00
151 topic sig -2.76 -2.19 +6.02 +4.58 +7.48 +13.77 +15.63 +14.17 +8.66 +3.59 0.00
257 baseline 0.098 0.155 0.191 0.184 0.199 0.193 0.189 0.181 0.181 0.185 0.190
257 tfidf -55.11 -38.56 -20.59 +6.54 +6.06 +16.44 +18.34 +21.68 +14.49 +7.09 0.00
257 topic sig +45.53 +64.06 +31.86 +34.91 +16.07 +20.40 +20.60 +18.01 +12.48 +4.24 0.00
258 baseline 0.141 0.270 0.428 0.463 0.462 0.471 0.470 0.502 0.512 0.528 0.527
258 tfidf -18.84 -27.88 -16.57 -5.21 +6.61 +16.13 +16.90 +9.56 +4.74 +1.92 0.00
258 topic sig +1.55 -21.82 -17.19 -6.56 +8.96 +14.16 +20.40 +11.44 +7.74 +3.43 0.00
271 baseline 0.167 0.316 0.368 0.338 0.335 0.351 0.331 0.310 0.321 0.333 0.332
271 tfidf -56.75 -51.35 -35.25 -13.20 -5.99 -4.83 +5.96 +10.97 +5.76 -1.22 0.00
271 topic sig +17.97 -4.40 +0.09 +13.67 +14.10 +4.63 +10.26 +15.70 +9.65 +2.29 0.00
</table>
<tableCaption confidence="0.985787">
Table 3: F-measure performance di\x0berence compared to baseline method in percentage. Columns indicate
</tableCaption>
<reference confidence="0.986949216216217">
at di\x0berent summary lengths related to full length documents. Values in the baseline rows are F-measure
scores. Values in the t\x0cdf and topic signature rows are performance increase or decrease divided by their
corresponding baseline scores and shown in percentage.
Inderjeet Mani, David House, Gary Klein, Lynette
Hirschman, Leo Obrst, Th\x13
er\x12
ese Firmin, Michael
Chrzanowski, and Beth Sundheim. 1998.
The TIPSTER SUMMAC text summariza-
tion evaluation \x0cnal report. Technical Report
MTR98W0000138, The MITRE Corporation.
Christopher Manning and Hinrich Sch\x7f
utze. 1999.
Foudations of Statistical Natural Language Pro-
cessing. MIT Press.
Kathleen McKeown and Dragomir R. Radev. 1999.
Generating summaries of multiple news articles.
In Inderjeet Mani and Mark T. Maybury, edi-
tors, Advances in Automatic Text Summarization,
chapter 24, pages 381{389. MIT Press.
Ellen Rilo\x0b and Je\x0brey Lorenzen. 1999. Extraction-
based text categorization: Generating domain-
speci\x0cc role relationships automatically. In
Tomek Strzalkowski, editor, Natural Language In-
formation Retrieval. Kluwer Academic Publishers.
Ellen Rilo\x0b. 1996. An empirical study of automated
dictionary construction for information extraction
in three domains. Arti\x0ccial Intelligence Journal,
85, August.
SAIC. 1998. Introduction to information extraction.
http://www.muc.saic.com.
Jinxi Xu and W. Bruce Croft. 1996. Query ex-
pansion using local and global document analysis.
In Proceedings of the 17th Annual International
ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, pages 4{11.
\x0c&apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.992083">
<title confidence="0.9991825">b&apos;The Automated Acquisition of Topic Signatures for Text Summarization</title>
<author confidence="0.999058">Chin-Yew Lin</author>
<author confidence="0.999058">Eduard Hovy</author>
<affiliation confidence="0.999564">Information Sciences Institute University of Southern California</affiliation>
<address confidence="0.998719">Marina del Rey, CA 90292, USA</address>
<email confidence="0.999923">fcyl,hovyg@isi.edu</email>
<abstract confidence="0.999699181818182">In order to produce a good summary, one has to identify the most relevant portions of a given text. We describe in this paper a method for automatically training topic signatures{sets of related words, with associated weights, organized around head topics{and illustrate with signatures we created with 6,194 TREC collection texts over 4 selected topics. We describe the possible integration of topic signatures with ontologies and its evaluaton on an automated text summarization system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Olatz Ansa</author>
<author>Eduard Hovy</author>
<author>David Mart\x13 inez</author>
</authors>
<title>Enriching very large ontologies using the www.</title>
<date>2000</date>
<booktitle>In Proceedings of the Workshop on Ontology Construction of the European Conference of AI (ECAI).</booktitle>
<contexts>
<context position="27271" citStr="Agirre et al., 2000" startWordPosition="4323" endWordPosition="4326"> topic signature method is a worthy addition to a variety of text summarization methods. 6.2 Enriching Topic Signatures Using Existing Ontologies We have shown in the previous sections that topic signatures can be used to approximate topic identi\x0ccation at the lexical level. Although the automatically acquired signature terms for a speci\x0cc topic seem to be bound by unknown relationships as shown in Table 2, it is hard to image how we can enrich the inherent at structure of topic signatures as de\x0cned in Equation 1 to a construct as complex as a MUC template or script. As discussed in (Agirre et al., 2000), we propose using an existing ontology such as SENSUS (Knight and Luk, 1994) to identify signature term relations. The external hierarchical framework can be used to generalize topic signatures and suggest richer representations for topic signatures. Automated entity recognizers can be used to classify unknown entities into their appropriate SENSUS concept nodes. We are also investigating other approaches to automatically learn signature term relations. The idea mentioned in this paper is just a starting point. 7 Conclusion In this paper we presented a procedure to automatically acquire topic</context>
</contexts>
<marker>Agirre, Ansa, Hovy, inez, 2000</marker>
<rawString>Eneko Agirre, Olatz Ansa, Eduard Hovy, and David Mart\x13 inez. 2000. Enriching very large ontologies using the www. In Proceedings of the Workshop on Ontology Construction of the European Conference of AI (ECAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics (ACL-90),</booktitle>
<pages>76--83</pages>
<contexts>
<context position="14668" citStr="Church and Hanks, 1990" startWordPosition="2355" endWordPosition="2358"> systems in a series of annual comparisons. This data set contains essential text fragments (phrases, clauses, and sentences) which must be included in summaries to answer some TREC topics. These fragments are each judged by a human judge. As described in Section 3, SUMMARIST employs several independent modules to assign a score to each sentence, and then combines the scores to decide which sentences to extract from the input text. One can gauge the ecacy 6The mutual information is de\x0cned according to chapter 2 of (Cover and Thomas, 1991) and is not the pairwise mutual information used in (Church and Hanks, 1990). \x0cTREC Topic Description hnumi Number: 151 htitlei Topic: Coping with overcrowded prisons hdesci Description: The document will provide information on jail and prison overcrowding and how inmates are forced to cope with those conditions; or it will reveal plans to relieve the overcrowded condition. hnarri Narrative: A relevant document will describe scenes of overcrowding that have become all too common in jails and prisons around the country. The document will identify how inmates are forced to cope with those overcrowded conditions, and/or what the Correctional System is doing, or planni</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Church and Patrick Hanks. 1990. Word association norms, mutual information and lexicography. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics (ACL-90), pages 76{83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="14592" citStr="Cover and Thomas, 1991" startWordPosition="2342" endWordPosition="2345">ed into various topics, used for formal evaluations of information retrieval systems in a series of annual comparisons. This data set contains essential text fragments (phrases, clauses, and sentences) which must be included in summaries to answer some TREC topics. These fragments are each judged by a human judge. As described in Section 3, SUMMARIST employs several independent modules to assign a score to each sentence, and then combines the scores to decide which sentences to extract from the input text. One can gauge the ecacy 6The mutual information is de\x0cned according to chapter 2 of (Cover and Thomas, 1991) and is not the pairwise mutual information used in (Church and Hanks, 1990). \x0cTREC Topic Description hnumi Number: 151 htitlei Topic: Coping with overcrowded prisons hdesci Description: The document will provide information on jail and prison overcrowding and how inmates are forced to cope with those conditions; or it will reveal plans to relieve the overcrowded condition. hnarri Narrative: A relevant document will describe scenes of overcrowding that have become all too common in jails and prisons around the country. The document will identify how inmates are forced to cope with those ove</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Thomas Cover and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald DeJong</author>
</authors>
<title>An overview of the FRUMP system.</title>
<date>1982</date>
<booktitle>Strategies for natural language processing,</booktitle>
<pages>149--76</pages>
<editor>In Wendy G. Lehnert and Martin H. Ringle, editors,</editor>
<contexts>
<context position="1998" citStr="DeJong, 1982" startWordPosition="304" endWordPosition="305">irits), eggs standing on end, etc. Only when the concepts cooccur is one licensed to infer the complex concept; eat or moxa alone, for example, are not sucient. At this time, we do not consider the interrelationships among the concepts. Since many texts may describe all the components of a complex concept without ever explicitly mentioning the underlying complex concept|a topic|itself, systems that have to identify topic(s), for summarization or information retrieval, require a method of inferring complex concepts from their component words in the text. 2 Related Work In late 1970\&apos;s, DeJong (DeJong, 1982) developed a system called FRUMP (Fast Reading Understanding and Memory Program) to skim newspaper stories and extract the main details. FRUMP uses a data structure called sketchy script to organize its world knowledge. Each sketchy script is what FRUMP knows about what can occur in particular situations such as demonstrations, earthquakes, labor strikes, and so on. FRUMP selects a particular sketchy script based on clues to styled events in news articles. In other words, FRUMP selects an empty template1 whose slots will be \x0clled on the y as FRUMP reads a news article. A summary is generate</context>
</contexts>
<marker>DeJong, 1982</marker>
<rawString>Gerald DeJong. 1982. An overview of the FRUMP system. In Wendy G. Lehnert and Martin H. Ringle, editors, Strategies for natural language processing, pages 149{76. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--61</pages>
<contexts>
<context position="10982" citStr="Dunning, 1993" startWordPosition="1705" endWordPosition="1706">. Each ti is an term highly correlated to topic with association weight wi. The number of related terms n can be set empirically according to a cuto\x0b associated weight. We describe how to acquire related terms and their associated weights in the next section. 4.1 Signature Term Extraction and Weight Estimation On the assumption that semantically related terms tend to co-occur, one can construct topic signatures from preclassi\x0ced text using the \x1f2 test, mutual information, or other standard statistic tests and information-theoretic measures. Instead of \x1f2 , we use likelihood ratio (Dunning, 1993) \x15, since \x15 is more appropriate for sparse data than \x1f2 test and the quantity 2log\x15 is asymptotically \x1f2 distributed5 . Therefore, we can determine the con\x0cdence level for a speci\x0cc 2log\x15 value by looking up \x1f2 distribution table and use the value to select an appropriate cuto\x0b associated weight. We have documents preclassi\x0ced into a set R of relevant texts and a set ~ R of nonrelevant texts for a given topic. Assuming the following two hypotheses: Hypothesis 1 (H1): P(Rjti) = p = P(Rj~ ti), i.e. the relevancy of a document is independent of ti. Hypothesis 2 (H</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19:61{74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--33</pages>
<contexts>
<context position="6805" citStr="Hearst, 1997" startWordPosition="1028" endWordPosition="1029">tant (central) topics of the texts. SUMMARIST uses positional importance, topic signature, and term frequency. Importance based on discourse structure will be added later. This is the most developed stage in SUMMARIST. Topic Interpretation: To fuse concepts such as waiter, menu, and food into one generalized concept restaurant, we need more than the simple word aggregation used in traditional information retrieval. We have investigated concept 3We would like to use only the relevant parts of documents to generate topic signatures in the future. Text segmentation algorithms such as TextTiling (Hearst, 1997) can be used to \x0cnd subtopic segments in text. ABCNEWS.com : Delay in Handing Flight 990 Probe to FBI NTSB Chairman James Hall says Egyptian officials want to review results of the investigation into the crash of EgyptAir Flight 990 before the case is turned over to the FBI. Nov. 16 - U.S. investigators appear to be leaning more than ever toward the possibility that one of the co-pilots of EgyptAir Flight 990 may have deliberately crashed the plane last month, killing all 217 people on board. However, U.S. officials say the National Transportation Safety Board will delay transferring the in</context>
<context position="20359" citStr="Hearst, 1997" startWordPosition="3238" endWordPosition="3239">respective topics that is seen in the bigram cases. We expect that more training data can improve the situation. We notice that the 2log\x15 values for topic 258 are higher than those of the other three topics. As indicated by (Mani et al., 1998) the majority of relevant documents for topic 258 have the query topic as their main theme; while the others mostly have the query topics as their subsidiary themes. This implies that it is too liberal to assume all the terms in relevant documents of the other three topics are relevant. We plan to apply text segmentation algorithms such as TextTiling (Hearst, 1997) to segment documents into subtopic units. We will then perform the topic signature creation procedure only on the relevant units to prevent inclusion of noise terms. 9We use the stopword list supplied with the SMART retrieval system. 10The 2log\x15 values are not comparable across ngram categories, since each ngram category has its own sample space. \x0cTopic 10 Signature Terms of Topic 151 |Overcrowded Prisons Unigram 2log\x15 Bigram 2log\x15 Trigram 2log\x15 jail 461.044 county jail 160.273 federal court order 45.960 county 408.821 early release 85.361 comply consent decree 35.121 overcrowd</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti Hearst. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23:33{64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Automated text summarization</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization, chapter 8,</booktitle>
<pages>81--94</pages>
<editor>in SUMMARIST. In Inderjeet Mani and Mark T. Maybury, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5524" citStr="Hovy and Lin, 1999" startWordPosition="842" endWordPosition="845">orpus statistics, are document-based3 and used in text summarization. In the next section, we describe the automated text summarization system SUMMARIST that we used in the experiments to provide the context of discussion. We then de\x0cne topic signatures and detail the procedures for automatically constructing topic signatures. In Section 5, we give an overview of the corpus used in the evaluation. In Section 6 we present the experimental results and the possibility of enriching topic signatures using an existing ontology. Finally, we end this paper with a conclusion. 3 SUMMARIST SUMMARIST (Hovy and Lin, 1999) is a system designed to generate summaries of multilingual input texts. At this time, SUMMARIST can process English, Arabic, Bahasa Indonesia, Japanese, Korean, and Spanish texts. It combines robust natural language processing methods (morphological transformation and part-of-speech tagging), symbolic world knowledge, and information retrieval techniques (term distribution and frequency) to achieve high robustness and better concept-level generalization. The core of SUMMARIST is based on the following `equation\&apos;: summarization = topic identi\x0ccation + topic interpretation + generation. The</context>
</contexts>
<marker>Hovy, Lin, 1999</marker>
<rawString>Eduard Hovy and Chin-Yew Lin. 1999. Automated text summarization in SUMMARIST. In Inderjeet Mani and Mark T. Maybury, editors, Advances in Automatic Text Summarization, chapter 8, pages 81{94. MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Knight</author>
<author>Steve K Luk</author>
</authors>
<title>Building a large knowledge base for machine translation.</title>
<date>1994</date>
<booktitle>In Proceedings of the Eleventh National Conference on Arti\x0ccial Intelligence (AAAI-94).</booktitle>
<contexts>
<context position="27348" citStr="Knight and Luk, 1994" startWordPosition="4336" endWordPosition="4339">ion methods. 6.2 Enriching Topic Signatures Using Existing Ontologies We have shown in the previous sections that topic signatures can be used to approximate topic identi\x0ccation at the lexical level. Although the automatically acquired signature terms for a speci\x0cc topic seem to be bound by unknown relationships as shown in Table 2, it is hard to image how we can enrich the inherent at structure of topic signatures as de\x0cned in Equation 1 to a construct as complex as a MUC template or script. As discussed in (Agirre et al., 2000), we propose using an existing ontology such as SENSUS (Knight and Luk, 1994) to identify signature term relations. The external hierarchical framework can be used to generalize topic signatures and suggest richer representations for topic signatures. Automated entity recognizers can be used to classify unknown entities into their appropriate SENSUS concept nodes. We are also investigating other approaches to automatically learn signature term relations. The idea mentioned in this paper is just a starting point. 7 Conclusion In this paper we presented a procedure to automatically acquire topic signatures and valuated the e\x0bectiveness of applying topic signatures to </context>
</contexts>
<marker>Knight, Luk, 1994</marker>
<rawString>Kevin Knight and Steve K. Luk. 1994. Building a large knowledge base for machine translation. In Proceedings of the Eleventh National Conference on Arti\x0ccial Intelligence (AAAI-94). at di\x0berent summary lengths related to full length documents. Values in the baseline rows are F-measure scores. Values in the t\x0cdf and topic signature rows are performance increase or decrease divided by their corresponding baseline scores and shown in percentage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>David House</author>
<author>Gary Klein</author>
<author>Lynette Hirschman</author>
<author>Leo Obrst</author>
</authors>
<title>The TIPSTER SUMMAC text summarization evaluation \x0cnal report.</title>
<date>1998</date>
<booktitle>Th\x13 er\x12 ese</booktitle>
<tech>Technical Report MTR98W0000138,</tech>
<institution>The MITRE Corporation.</institution>
<contexts>
<context position="13876" citStr="Mani et al., 1998" startWordPosition="2223" endWordPosition="2226">mial distribution and a 2-by-2 contingency table. To create topic signature for a given topic, we: 1. classify documents as relevant or nonrelevant according to the given topic 2. compute the 2log\x15 value using Equation 3 for each term in the document collection 3. rank terms according to their 2log\x15 value 4. select a con\x0cdence level from the \x1f2 distribution table; determine the cuto\x0b associated weight and the number of terms to be included in the signatures 5 The Corpus The training data derives from the Question and Answering summary evaluation data provided by TIPSTER-SUMMAC (Mani et al., 1998) that is a subset of the TREC collections. The TREC data is a collection of texts, classi\x0ced into various topics, used for formal evaluations of information retrieval systems in a series of annual comparisons. This data set contains essential text fragments (phrases, clauses, and sentences) which must be included in summaries to answer some TREC topics. These fragments are each judged by a human judge. As described in Section 3, SUMMARIST employs several independent modules to assign a score to each sentence, and then combines the scores to decide which sentences to extract from the input t</context>
<context position="19992" citStr="Mani et al., 1998" startWordPosition="3172" endWordPosition="3175"> topic 271. These automatically generated signature terms closely resemble or equal the given short TREC topic descriptions. Although trigram terms shown in the table, such as federal court order, philip morris rjr, jet propulsion laboratory, and mobile telephone system are also meaningful, they do not demonstrate the closer term relationship among other terms in their respective topics that is seen in the bigram cases. We expect that more training data can improve the situation. We notice that the 2log\x15 values for topic 258 are higher than those of the other three topics. As indicated by (Mani et al., 1998) the majority of relevant documents for topic 258 have the query topic as their main theme; while the others mostly have the query topics as their subsidiary themes. This implies that it is too liberal to assume all the terms in relevant documents of the other three topics are relevant. We plan to apply text segmentation algorithms such as TextTiling (Hearst, 1997) to segment documents into subtopic units. We will then perform the topic signature creation procedure only on the relevant units to prevent inclusion of noise terms. 9We use the stopword list supplied with the SMART retrieval system</context>
</contexts>
<marker>Mani, House, Klein, Hirschman, Obrst, 1998</marker>
<rawString>Inderjeet Mani, David House, Gary Klein, Lynette Hirschman, Leo Obrst, Th\x13 er\x12 ese Firmin, Michael Chrzanowski, and Beth Sundheim. 1998. The TIPSTER SUMMAC text summarization evaluation \x0cnal report. Technical Report MTR98W0000138, The MITRE Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
</authors>
<title>and Hinrich Sch\x7f utze.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Manning, 1999</marker>
<rawString>Christopher Manning and Hinrich Sch\x7f utze. 1999. Foudations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>Dragomir R Radev</author>
</authors>
<title>Generating summaries of multiple news articles.</title>
<date>1999</date>
<booktitle>In Inderjeet Mani and</booktitle>
<pages>381--389</pages>
<editor>Mark T. Maybury, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2829" citStr="McKeown and Radev, 1999" startWordPosition="439" endWordPosition="442">d knowledge. Each sketchy script is what FRUMP knows about what can occur in particular situations such as demonstrations, earthquakes, labor strikes, and so on. FRUMP selects a particular sketchy script based on clues to styled events in news articles. In other words, FRUMP selects an empty template1 whose slots will be \x0clled on the y as FRUMP reads a news article. A summary is generated based on what has been captured or \x0clled in the template. The recent success of information extraction research has encouraged the FRUMP approach. The SUMMONS (SUMMarizing Online NewS articles) system (McKeown and Radev, 1999) takes template outputs of information extraction systems developed for MUC conference and generating summaries of multiple news articles. FRUMP and SUMMONS both rely on prior knowledge of their domains. However, to acquire such prior knowledge is labor-intensive and time-consuming. For example, the University of Massachusetts CIRCUS system used in the MUC-3 (SAIC, 1998) terrorism domain required about 1500 person-hours to de\x0cne extraction patterns2 (Rilo\x0b, 1996). In order to make them practical, we need to reduce the knowledge engineering bottleneck and improve the portability of FRUMP </context>
</contexts>
<marker>McKeown, Radev, 1999</marker>
<rawString>Kathleen McKeown and Dragomir R. Radev. 1999. Generating summaries of multiple news articles. In Inderjeet Mani and Mark T. Maybury, editors, Advances in Automatic Text Summarization, chapter 24, pages 381{389. MIT Press.</rawString>
</citation>
<citation valid="true">
<title>Extractionbased text categorization: Generating domainspeci\x0cc role relationships automatically.</title>
<date>1999</date>
<booktitle>Ellen Rilo\x0b and Je\x0brey Lorenzen.</booktitle>
<editor>In Tomek Strzalkowski, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>1999</marker>
<rawString>Ellen Rilo\x0b and Je\x0brey Lorenzen. 1999. Extractionbased text categorization: Generating domainspeci\x0cc role relationships automatically. In Tomek Strzalkowski, editor, Natural Language Information Retrieval. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Rilo\x0b</author>
</authors>
<title>An empirical study of automated dictionary construction for information extraction in three domains.</title>
<date>1996</date>
<journal>Arti\x0ccial Intelligence Journal,</journal>
<volume>85</volume>
<marker>Rilo\x0b, 1996</marker>
<rawString>Ellen Rilo\x0b. 1996. An empirical study of automated dictionary construction for information extraction in three domains. Arti\x0ccial Intelligence Journal, 85, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SAIC</author>
</authors>
<title>Introduction to information extraction.</title>
<date>1998</date>
<note>http://www.muc.saic.com.</note>
<contexts>
<context position="3202" citStr="SAIC, 1998" startWordPosition="501" endWordPosition="502">enerated based on what has been captured or \x0clled in the template. The recent success of information extraction research has encouraged the FRUMP approach. The SUMMONS (SUMMarizing Online NewS articles) system (McKeown and Radev, 1999) takes template outputs of information extraction systems developed for MUC conference and generating summaries of multiple news articles. FRUMP and SUMMONS both rely on prior knowledge of their domains. However, to acquire such prior knowledge is labor-intensive and time-consuming. For example, the University of Massachusetts CIRCUS system used in the MUC-3 (SAIC, 1998) terrorism domain required about 1500 person-hours to de\x0cne extraction patterns2 (Rilo\x0b, 1996). In order to make them practical, we need to reduce the knowledge engineering bottleneck and improve the portability of FRUMP or SUMMONS-like systems. Since the world contains thousands, or perhaps millions, of complex concepts, it is important to be able to learn sketchy scripts or extraction patterns automatically from corpora|no existing knowledge base contains nearly enough information. (Rilo\x0b and Lorenzen, 1999) present a system AutoSlog-TS that generates extraction patterns and learns </context>
</contexts>
<marker>SAIC, 1998</marker>
<rawString>SAIC. 1998. Introduction to information extraction. http://www.muc.saic.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>W Bruce Croft</author>
</authors>
<title>Query expansion using local and global document analysis.</title>
<date>1996</date>
<booktitle>In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>4--11</pages>
<contexts>
<context position="28865" citStr="Xu and Croft, 1996" startWordPosition="4563" endWordPosition="4566">pic interpretation). Topic identi\x0ccation and interpretation are two essential steps in a typical automated text summarization system as we present in Section 3. Topic signatures can also been viewed as an inverse process of query expansion. Query expansion intends to alleviate the word mismatch problem in information retrieval, since documents are normally written in di\x0berent vocabulary. How to automatically identify highly correlated terms and use them to improve information retrieval performance has been a main research issue since late 1960\&apos;s. Recent advances in the query expansion (Xu and Croft, 1996) can also shed some light on the creation of topic signatures. Although we focus the use of topic signatures to aid text summarization in this paper, we plan to explore the possibility of applying topic signatures to perform query expansion in the future. The results reported are encouraging enough to allow us to continue with topic signatures as the vehicle for a \x0crst approximation to world knowledge. We are now busy creating a large number of signatures to overcome the world knowledge acquisition problem and use them in topic interpretation. 8 Acknowledgements We thank the anonymous revie</context>
</contexts>
<marker>Xu, Croft, 1996</marker>
<rawString>Jinxi Xu and W. Bruce Croft. 1996. Query expansion using local and global document analysis. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 4{11. \x0c&apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>