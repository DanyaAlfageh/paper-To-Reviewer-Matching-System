<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.471832">
b&amp;apos;Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 779787,
</bodyText>
<table confidence="0.294407166666667">
Beijing, August 2010
Evaluating Dependency Representation for Event Extraction
Makoto Miwa1
Sampo Pyysalo1
Tadayoshi Hara1
Junichi Tsujii1,2,3
</table>
<page confidence="0.293272">
1
</page>
<affiliation confidence="0.55611">
Department of Computer Science, the University of Tokyo
</affiliation>
<page confidence="0.690064">
2
</page>
<affiliation confidence="0.682624">
School of Computer Science, University of Manchester
</affiliation>
<page confidence="0.909096">
3
</page>
<affiliation confidence="0.740598">
National Center for Text Mining
</affiliation>
<email confidence="0.974858">
{mmiwa,smp,harasan,tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.989084" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997914541666667">
The detailed analyses of sentence struc-
ture provided by parsers have been applied
to address several information extraction
tasks. In a recent bio-molecular event ex-
traction task, state-of-the-art performance
was achieved by systems building specif-
ically on dependency representations of
parser output. While intrinsic evalua-
tions have shown significant advances in
both general and domain-specific pars-
ing, the question of how these translate
into practical advantage is seldom con-
sidered. In this paper, we analyze how
event extraction performance is affected
by parser and dependency representation,
further considering the relation between
intrinsic evaluation and performance at
the extraction task. We find that good
intrinsic evaluation results do not always
imply good extraction performance, and
that the types and structures of differ-
ent dependency representations have spe-
cific advantages and disadvantages for the
event extraction task.
</bodyText>
<sectionHeader confidence="0.997063" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99793505">
Advanced syntactic parsing methods have been
shown to effective for many information extrac-
tion tasks. The BioNLP 2009 Shared Task, a re-
cent bio-molecular event extraction task, is one
such task: analysis showed that the application of
a parser correlated with high rank in the task (Kim
et al., 2009). The automatic extraction of bio-
molecular events from text is important for a num-
ber of advanced domain applications such as path-
way construction, and event extraction thus a key
task in Biomedical Natural Language Processing
(BioNLP).
Methods building feature representations and
extraction rules around dependency representa-
tions of sentence syntax have been successfully
applied to a number of tasks in BioNLP. Several
parsers and representations have been applied in
high-performing methods both in domain studies
in general and in the BioNLP09 shared task in
particular, but no direct comparison of parsers or
representations has been performed. Likewise,
a number of evaluation of parser outputs against
gold standard corpora have been performed in the
domain, but the broader implications of the results
of such intrinsic evaluations are rarely considered.
The BioNLP09 shared task involved documents
contained also in the GENIA treebank (Tateisi et
al., 2005), creating an opportunity for direct study
of intrinsic and task-oriented evaluation results.
As the treebank can be converted into various de-
pendency formats using existing format conver-
sion methods, evaluation can further be extended
to cover the effects of different representations.
In this this paper, we consider three types of de-
pendency representation and six parsers, evaluat-
ing their performance from two different aspects:
dependency-based intrinsic evaluation, and effec-
tiveness for bio-molecular event extraction with a
state-of-the-art event extraction system. Compar-
ison of intrinsic and task-oriented evaluation re-
</bodyText>
<page confidence="0.9889">
779
</page>
<figure confidence="0.966275923076923">
\x0cphosphorylation
TRAF2
binding
inhibits
TRAF2 CD40
Theme
Theme
Theme
Cause Theme
... In this study we hypothesized that the phosphorylation of TRAF2 inhibits
binding to the CD40 cytoplasmic domain. ...
Negative_regulation
Phospholylation Binding
</figure>
<figureCaption confidence="0.999898">
Figure 1: Event Extraction.
</figureCaption>
<bodyText confidence="0.924808571428571">
sults shows that performance against gold stan-
dard annotations is not always correlated with
event extraction performance. We further find
that the dependency types and overall structures
employed by the different dependency representa-
tions have specific advantages and disadvantages
for the event extraction task.
</bodyText>
<sectionHeader confidence="0.981335" genericHeader="method">
2 Bio-molecular Event Extraction
</sectionHeader>
<bodyText confidence="0.999436451612903">
In this study, we adopt the event extraction task
defined in the BioNLP 2009 Shared Task (Kim et
al., 2009) as a model information extraction task.
Figure 1 shows an example illustrating the task
of event extraction from a sentence. The shared
task provided common and consistent task defi-
nitions, data sets for training and evaluation, and
evaluation criteria. The shared task defined five
simple events (Gene expression, Transcription,
Protein catabolism, Phosphorylation, and Local-
ization) that take one core argument, a multi-
participant binding event (Binding), and three reg-
ulation events (Regulation, Positive regulation,
and Negative regulation) used to capture both bi-
ological regulation and general causation. The
participants of simple and Binding events were
specified to be of the general Protein type, while
regulation-type events could also take other events
as arguments, creating complex event structures.
We consider two subtasks, Task 1 and Task 2,
out of the three defined in the shared task. Task 1
focuses on core event extraction, and Task 2
involves augmenting extracted events with sec-
ondary arguments (Kim et al., 2009). Events are
represented with a textual trigger, type, and ar-
guments, where the trigger is a span of text that
states the event in text. In Task 1 the event argu-
ments that need to be extracted are restricted to the
core Theme and Cause roles, with secondary ar-
guments corresponding to locations and sites con-
sidered in Task 2.
</bodyText>
<subsectionHeader confidence="0.996673">
2.1 Event Extraction System
</subsectionHeader>
<bodyText confidence="0.9994669">
For evaluation, we apply the system of Miwa et al.
(2010b). The system was originally developed for
finding core events (Task 1) using the native out-
put of the Enju and GDep parsers. The system
consists of three supervised classification-based
modules: a trigger detector, an event edge detec-
tor, and a complex event detector. The trigger
detector classifies each word into the appropriate
event types, the event edge detector classifies each
edge between an event and a candidate participant
into an argument type, and the complex event de-
tector classifies event candidates constructed by
all edge combinations, deciding between event
and non-event. The system uses one-vs-all sup-
port vector machines (SVMs) for classification.
The system operates on one sentence at a time,
building features for classification based on the
syntactic analyses for the sentence provided by
the two parsers as well as the sequence of the
words in the sentence, including the target candi-
date. The features include the constituents/words
around entities (triggers and proteins), the depen-
dencies, and the shortest paths among the enti-
ties. The feature generation is format-independent
regarding the shared properties of different for-
mats, but makes use also of format-specific infor-
mation when available for extracting features, in-
cluding the dependency tags, word-related infor-
mation (e.g. a lexical entry in Enju format), and
the constituents and their head information.
We apply here a variant of the base system in-
corporating a number of modifications. The ap-
plied system performs feature selection removing
two classes of features that were found not to be
beneficial for extraction performance, and applies
a refinement of the trigger expressions of events.
The system is further extended to find also sec-
ondary arguments (Task 2). For a detailed descrip-
tion of these improvements, we refer to Miwa et
al. (2010a).
</bodyText>
<sectionHeader confidence="0.963571" genericHeader="method">
3 Parsers and Representations
</sectionHeader>
<bodyText confidence="0.964975">
Six publicly available parsers and three depen-
dency formats are considered in this paper. The
</bodyText>
<page confidence="0.864849">
780
</page>
<figure confidence="0.948523142857143">
\x0cP2
NFAT/AP-1 complex formed only with P and
nn
nsubj
prep
dep conj
pobj cc
</figure>
<figureCaption confidence="0.964462">
Figure 2: Stanford basic dependency tree
</figureCaption>
<figure confidence="0.764063625">
P2
NFAT/AP-1 complex formed only with P and
NMOD VMOD
VMOD
PMOD CONJ
PMOD COORD
root
ROOT
</figure>
<figureCaption confidence="0.989585">
Figure 3: CoNLL-X dependency tree
</figureCaption>
<figure confidence="0.9957028125">
noun_arg1
arg1
prep_arg12
arg1
prep_arg12
arg2
P2
NFAT/AP-1 complex formed only with P and
verb_arg1
arg1
adj_arg1
arg1
coord_arg12
arg1
coord_arg12
arg2
</figure>
<figureCaption confidence="0.999854">
Figure 4: Predicate Argument Structure
</figureCaption>
<bodyText confidence="0.937552625">
parsers are GDep (Sagae and Tsujii, 2007), the
Bikel parser (Bikel) (Bikel, 2004), the Stanford
parser with two probabilistic context-free gram-
mar (PCFG) models1 (Wall Street Journal (WSJ)
model (Stanford WSJ) and augmented English
model (Stanford eng)) (Klein and Manning,
2003), the Charniak-Johnson reranking parser,
using David McCloskys self-trained biomedi-
cal parsing model (MC) (McClosky, 2009), the
C&amp;C CCG parser, adapted to biomedical text
(C&amp;C) (Rimell and Clark, 2009), and the Enju
parser with the GENIA model (Miyao et al.,
2009). The formats are Stanford Dependen-
cies (SD) (Figure 2), the CoNLL-X dependency
format (CoNLL) (Figure 3) and the predicate-
argument structure (PAS) format used by Enju
(Figure 4). With the exception of Stanford and
Enju, the analyses of these parsers were provided
by the BioNLP 2009 Shared Task organizers.
The six parsers operate in a number of different
frameworks, reflected in their analyses. GDep is
a native dependency parser that produces CoNLL
dependency trees, with dependency types similar
to those of CoNLL 2007. Bikel, Stanford, and MC
</bodyText>
<figure confidence="0.900409333333334">
1
Experiments showed no benefit from using the lexical-
ized models with the Stanford parser.
Enju
C&amp;C
GDep
McClosky-
Charniak
Bikel
PAS
PTB
CCG
SD
Conll-X
Stanford
</figure>
<figureCaption confidence="0.999548">
Figure 5: Format conversion dependencies in six
</figureCaption>
<bodyText confidence="0.981989970588235">
parsers. Formats adopted for the evaluation are
shown in solid boxes. SD: Stanford Dependency
format, CCG: Combinatory Categorial Grammar
output format, PTB: Penn Treebank format, and
PAS: Predicate Argument Structure in Enju for-
mat.
are phrase-structure parsers trained on Penn Tree-
bank format (PTB) style treebanks, and they pro-
duce PTB trees. C&amp;C is a deep parser based on
Combinatory Categorial Grammar (CCG), and its
native output is in a CCG-specific format. The
output of C&amp;C can be converted into SD by a
rule-based conversion script (Rimell and Clark,
2009). Enju is deep parser based on Head-driven
Phrase Structure Grammar (HPSG) and produces
a format containing predicate argument structures
along with a phrase structure tree in Enju format,
which can be converted into PTB format (Miyao
et al., 2009).
For direct comparison and for the study of con-
tribution of the formats in which the six parsers
output their analyses to task performance, we ap-
ply a number of conversions between the out-
puts, shown in Figure 5. The Enju PAS output is
converted into PTB using the method introduced
by (Miyao et al., 2009). SD is generated from
PTB by the Stanford tools (de Marneffe et al.,
2006), and CoNLL generated from PTB by us-
ing Treebank Converter (Johansson and Nugues,
2007). With the exception of GDep, all CoNLL
outputs are generated by the conversion and thus
share dependency types. We note that all of these
conversions can introduce some errors in the con-
version process.
</bodyText>
<page confidence="0.987099">
781
</page>
<subsectionHeader confidence="0.7971705">
\x0c4 Evaluation Setting
4.1 Event Extraction Evaluation
</subsectionHeader>
<bodyText confidence="0.984929043478261">
Event extraction performance is evaluated using
the evaluation script provided by the BioNLP09
shared task organizers for the development data
set, and the online evaluation system of the task
for the test data set2 . Results are reported under
the official evaluation criterion of the task, i.e. the
Approximate Span Matching/Approximate Re-
cursive Matching criterion.
The event extraction system described in Sec-
tion 2.1 is used with the default settings given in
(Miwa et al., 2010b). The C-values of SVMs are
set to 1.0, but the positive and negative examples
are balanced by placing more weight on the posi-
tive examples. The examples predicted with con-
fidence greater than 0.5, as well as the examples
with the most confident labels, are extracted. Task
1 and Task 2 are solved at once for the evaluation.
Some of the parse results do not include word
base forms or part-of-speech (POS) tags, which
are required by the event extraction system. To
apply these parsers, the GENIA Tagger (Tsuruoka
et al., 2005) output is adopted to add this informa-
tion to the results.
</bodyText>
<subsectionHeader confidence="0.995257">
4.2 Dependency Representation Evaluation
</subsectionHeader>
<bodyText confidence="0.996257111111111">
The parsers are evaluated with precision, recall,
and F-score for each dependency type. We note
that the parsers may produce more fine-grained
word segmentations than that of the gold standard:
for example, two words p70(S6)-kinase activa-
tion in the gold standard tree (Figure 6 (a)) is
segmented into five words by Enju (Figure 6 (b)).
In the evaluation the word segmentations in the
gold tree are used, and dependency transfer and
word-based normalization are performed to match
parser outputs to these. Dependencies related to
the segmentations are transferred to the enclosing
word as follows. If one word is segmented into
several segments by a parser, all the dependencies
between the segments are removed (Figure 6 (c))
and the dependency between another word and
the segments is converted into the dependency be-
tween the two words (Figure 6 (d)).
</bodyText>
<equation confidence="0.731380333333333">
2
http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/SharedTask/
</equation>
<bodyText confidence="0.991499266666667">
The parser outputs in SD and CoNLL can be
assumed to be trees, so each node in the tree have
only one parent node. However, in the converted
tree nodes can have more than one parent. We
cannot simply apply accuracy, or (un)labeled at-
tachment score3. Word-based normalization is
performed to avoid negative impact by the word
segmentations by parsers. When (a) and (d) in
Figure 6 are compared, the counts of correct re-
lations will be 1.0 (0.5 for upper NMOD and 0.5
for lower NMOD in Figure 6 (d)) for the parser
(precision), and the counts of correct relations will
be 1.0 (for NMOD in Figure 6 (a)) for the gold
(recall). This F-score is a good approximation of
accuracy.
</bodyText>
<subsectionHeader confidence="0.911609">
4.3 GENIA treebank processing
</subsectionHeader>
<bodyText confidence="0.9996586">
For comparison and evaluation, the texts in the
GENIA treebank (Tateisi et al., 2005) are con-
verted to the various formats as follows. To create
PAS, the treebank is converted with Enju, and for
trees that fail conversion, parse results are used in-
stead. The GENIA treebank is also converted into
PTB4, and then converted into SD and CoNLL as
described in Section 3. While based on manually
annotated gold data, the converted treebanks are
not always correct due to conversion errors.
</bodyText>
<sectionHeader confidence="0.996574" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.997854">
This section presents evaluation results. Intrinsic
evaluation is first performed in Section 5.1. Sec-
tion 5.2 considers the effect of different SD vari-
ants. Section 5.3 presents the results of experi-
ments with different parsers. Section 5.4 shows
the performance of different parsers. Finally, the
performance of the event extraction system is dis-
cussed in context of other proposed methods for
the task in Section 5.5.
</bodyText>
<subsectionHeader confidence="0.935628">
5.1 Intrinsic Evaluation
</subsectionHeader>
<bodyText confidence="0.9982402">
We initially briefly consider the results of an in-
trinsic evaluation comparing parser outputs to ref-
erence data automatically derived from the gold
standard treebank. Table 1 shows results for the
parsers whose outputs could be converted into the
</bodyText>
<figure confidence="0.978978903225806">
3
http://nextens.uvt.nl/conll/
4
http://categorizer.tmit.bme.hu/
illes/genia ptb/
782
\x0cactivation
p70(S6)-kinase
NMOD
(a) Gold Word Segmen-
tations
-kinase
p70 ( S6 ) activation
NMOD
NMOD
PRN P
P
(b) Parser Word Seg-
mentations
-kinase
p70 ( S6 ) activation
NMOD
NMOD
(c) Inner Dependency
Removal
activation
p70(S6)-kinase
NMOD
NMOD
(d) Dependency Trans-
fer
</figure>
<figureCaption confidence="0.999933">
Figure 6: Example of Word Segmentations of the words by gold and Enju and Dependency Transfer.
</figureCaption>
<table confidence="0.992171555555556">
Typed Untyped
SD CoNLL SD CoNLL
P R F P R F P R F P R F
Bikel 70.31 70.37 70.34 77.81 77.56 77.69 80.54 80.60 80.57 82.43 82.18 82.31
SP WSJ 74.11 73.94 74.03 81.41 81.47 81.44 81.36 81.16 81.26 84.05 84.05 84.05
SP eng 79.08 78.89 78.98 84.92 84.82 84.87 84.16 83.96 84.06 86.54 86.47 86.51
C&amp;C 80.31 78.04 79.16 - 84.91 82.28 83.57 -
MC 79.56 79.63 79.60 88.13 87.87 88.00 87.43 87.50 87.47 89.81 89.42 89.62
Enju 85.59 85.62 85.60 88.59 89.51 89.05 88.28 88.30 88.29 90.24 90.77 90.50
</table>
<tableCaption confidence="0.999834">
Table 1: Comparison of precision, recall, and F-score results with five parsers (two models for Stanford)
</tableCaption>
<bodyText confidence="0.999108041666667">
in two different formats on the development data set (SP abbreviates for Stanford Parser). Results
shown separately for evaluation including dependency types and one eliminating them. Parser/model
combinations above the line do not use in-domain data, others do.
SD and CoNLL dependency representations us-
ing the Stanford tools and Treebank Converter, re-
spectively. For Stanford, both the Penn Treebank
WSJ section and augmented English (eng) mod-
els were tested; the latter includes biomedical do-
main data. The Enju results for PAS are 91.48
with types and 93.39 without in F-score. GDep
not shown as its output is not compatible with that
of Treebank Converter.
Despite numerical differences, the two repre-
sentations and two criteria (typed/untyped) all
produce largely the same ranking of the parsers.5
The evaluations also largely agree on the magni-
tude of the reduction in error afforded through the
use of in-domain training data for the Stanford
parser, with all estimates falling in the 15-19%
range. Similarly, all show substantial differences
between the parsers, indicating e.g. that the error
rate of Enju is 50% or less of that of Bikel.
These results serve as a reference point for ex-
trinsic evaluation results. However, it should be
</bodyText>
<page confidence="0.969067">
5
</page>
<bodyText confidence="0.9724115">
One larger divergence is between typed and untyped SD
results for MC. Analysis suggest one cause is frequent errors
in tagging hyphenated noun-modifiers such as NF-kappaB as
adjectives.
</bodyText>
<table confidence="0.971357666666667">
BD CD CDP CTD
Task 1 55.60 54.35 54.59 54.42
Task 2 53.94 52.65 52.88 52.76
</table>
<tableCaption confidence="0.999586">
Table 2: Comparison of the F-score results with
</tableCaption>
<bodyText confidence="0.961551142857143">
different SD variants on the development data set
with the MC parser. The best score in each task is
shown in bold.
noted that as the parsers make use of annotated
domain training data to different extents, this eval-
uation does not provide a sound basis for direct
comparison of the parsers themselves.
</bodyText>
<subsectionHeader confidence="0.999714">
5.2 Stanford Dependency Setting
</subsectionHeader>
<bodyText confidence="0.998860818181818">
SD have four different variants: basic depen-
dencies (BD), collapsed dependencies (CD), col-
lapsed dependencies with propagation of conjunct
dependencies (CDP), and collapsed tree depen-
dencies (CTD) (de Marneffe and Manning, 2008).
Except for BD, these variants do not necessarily
connect all the words in the sentence, and CD and
CDP do not necessarily form a tree structure. Ta-
ble 2 shows the comparison results with the MC
parser. Dependencies are generalized by remov-
ing expressions after of the dependencies (e.g.
</bodyText>
<page confidence="0.990981">
783
</page>
<bodyText confidence="0.992592727272727">
\x0c with in prep with) for better performance. We
find that basic dependencies give the best perfor-
mance to event extraction, with little difference
between the other variants. This result is surpris-
ing, as variants other than basic have features such
as the resolution of conjunctions that are specif-
ically designed for practical applications. How-
ever, basic dependencies were found to consis-
tently provide best performance also for the other
parsers6. Thus, in the following evaluation, the
basic dependencies are adopted for all SD results.
</bodyText>
<subsectionHeader confidence="0.996065">
5.3 Parser Comparison on Event Extraction
</subsectionHeader>
<bodyText confidence="0.99382171875">
Results with different parsers and different for-
mats on the development data set are summarized
in Table 3. Baseline results are produced by re-
moving dependency information from the parse
results. The baseline results differ between the
representations as the word base forms and POS
tags produced by the GENIA tagger for use with
SD and CoNLL are different from PAS, and be-
cause head word information in the Enju format is
used. The evaluation finds best results for both
tasks with Enju, using its native output format.
However, as discussed in Section 2.1, the treat-
ment of PAS and the other two formats are slightly
different, this result does not necessarily indicate
that PAS is the best alternative for event extrac-
tion.
The Bikel and Stanford WSJ parsers, lacking
models adapted to the biomedical domain, per-
forms mostly worse than the other parsers. The
other parsers, even though trained on the treebank,
do not provide performance as high as that for
using the GENIA treebank, but, with the excep-
tion of Stanford eng with CoNLL, results with the
parsers are only slightly worse than results with
the treebank. The results with the data derived
from the GENIA treebank can be considered as
upper bounds for the parsers and formats at the
task, although conversion errors are expected to
lower these bounds to some extent. The results
suggest that there is relative little remaining ben-
efit to be gained from improving parser perfor-
mance.
</bodyText>
<page confidence="0.990821">
6
</page>
<bodyText confidence="0.996386">
Collapsed tree dependencies are not evaluated on the
C&amp;C parser since the conversion is not provided.
</bodyText>
<subsectionHeader confidence="0.998955">
5.4 Effects of Dependency Representation
</subsectionHeader>
<bodyText confidence="0.9975595">
Intrinsic evaluation results (Section 5.1) cannot
be used directly for comparing the parsers, since
some of the parsers contain models trained on the
GENIA treebank. To investigate the effects of the
evaluation results to the event extraction, we per-
formed event extraction with eliminating the de-
pendency types. Table 4 summarizes the results
with the dependency structures (without the de-
pendency types) on the development data set. In-
terestingly, we find the performance increases in
Bikel and Stanford by eliminating the dependency
types. This implies that the inaccurate depen-
dency types shown in Table 1 confused the event
extraction system. SD and PAS drops more than
CoNLL, and Enju with CoNLL structures perform
best in total when the dependency types are re-
moved. This result shows that the formats have
their own strengths in finding events, and CoNLL
structure with SD or PAS types can be a good rep-
resentation for the event extraction.
By comparing Table 3, Table 1, and Table 4,
we found that the better dependency performance
does not always produce better event extraction
performance especially when the difference of the
dependency performance is small. MC and Enju
results show that performance in dependency is
important for event extraction. SD can be better
than CoNLL for the event extraction (shown with
the gold treebank data in Table 3), but the types
and relations of CoNLL were well predicted, and
MC and Enju performed better for CoNLL than
for SD in total.
</bodyText>
<subsectionHeader confidence="0.993821">
5.5 Performance of Event Extraction System
</subsectionHeader>
<bodyText confidence="0.999888153846154">
Several systems are compared by the extraction
performance on the shared task test data in Ta-
ble 5. GDep and Enju with PAS are used for the
evaluation, which is the same evaluation setting
with the original system by Miwa et al. (2010b).
The performance of the best systems in the orig-
inal shared task is shown for reference ((Bjorne
et al., 2009) in Task 1 and (Riedel et al., 2009)
in Task 2). The event extraction system performs
significantly better than the best systems in the
shared task, further outperforming the original
system. This shows that the comparison of the
parsers is performed with a state-of-the-art sys-
</bodyText>
<page confidence="0.998973">
784
</page>
<table confidence="0.998878181818182">
\x0cTask 1 Task 2
SD CoNLL PAS SD CoNLL PAS
Baseline 51.05 - 50.42 49.17 - 48.88
Bikel 53.29 53.22 - 51.40 51.27 -
Stanford WSJ 53.51 54.38 - 52.02 52.04 -
Stanford eng 55.02 53.66 - 53.41 52.74 -
GDep - 55.70 - - 54.37 -
MC 55.60 56.01 - 53.94 54.51 -
C&amp;C 56.09 - - 54.27 - -
Enju 55.48 55.74 56.57 54.06 54.37 55.31
GENIA 56.34 56.09 57.94 55.04 54.57 56.40
</table>
<tableCaption confidence="0.99984">
Table 3: Comparison of F-score results with six parsers in three different formats on the development
</tableCaption>
<bodyText confidence="0.48262">
data set. Results without dependency information are shown as baselines. The results with the GENIA
treebank (converted into PTB and PAS) are shown for comparison. The best score in each task is shown
in bold, and the best score in each task and format is underlined.
</bodyText>
<table confidence="0.9904176">
Task 1 Task 2
SD CoNLL PAS SD CoNLL PAS
Bikel 53.41 (+0.12) 53.92 (+0.70) - 51.59 (+0.19) 52.21 (+0.94) -
Stanford WSJ 53.03 (-0.48) 54.52 (+0.14) - 51.43 (-0.59) 52.60 (-0.14) -
Stanford eng 54.48 (-0.54) 54.02 (+0.36) - 52.88 (-0.53) 52.28 (+0.24) -
GDep - 54.97 (-0.73) - - 53.71 (-0.66) -
MC 54.22 (-1.38) 55.24 (-0.77) - 52.73 (-1.21) 53.42 (-1.09) -
C&amp;C 54.64 (-1.45) - - 52.98 (-1.29) - -
Enju 53.74 (-1.74) 55.66 (-0.08) 55.23 (-1.34) 52.29 (-1.77) 53.97 (-0.40) 53.69 (-1.62)
GENIA 55.79 (-0.55) 55.64 (-0.45) 56.42 (-1.52) 54.17 (-0.87) 53.83 (-0.74) 55.34 (-1.06)
</table>
<tableCaption confidence="0.99841">
Table 4: Comparison of F-score results with six parsers in three different dependency structures (with-
</tableCaption>
<table confidence="0.955365916666667">
out the dependency types) on the development data set. The changes from Table 3 are shown.
Simple Binding Regulation All
Task 1
Ours 66.84 / 78.22 / 72.08 48.70 / 52.65 / 50.60 38.48 / 55.06 / 45.30 50.13 / 64.16 / 56.28
Miwa 65.31 / 76.44 / 70.44 52.16 / 53.08 / 52.62 35.93 / 46.66 / 40.60 48.62 / 58.96 / 53.29
Bjorne 64.21 / 77.45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95
Riedel N/A 23.05 / 48.19 / 31.19 26.32 / 41.81 / 32.30 36.90 / 55.59 / 44.35
Baseline 62.94 / 68.38 / 65.55 48.41 / 34.50 / 40.29 29.40 / 40.00 / 33.89 43.93 / 50.11 / 46.82
Task 2
Ours 65.43 / 75.56 / 70.13 46.42 / 50.31 / 48.29 38.18 / 54.45 / 44.89 49.20 / 62.57 / 55.09
Riedel N/A 22.35 / 46.99 / 30.29 25.75 / 40.75 / 31.56 35.86 / 54.08 / 43.12
Baseline 60.88 / 63.78 / 62.30 44.99 / 31.78 / 37.25 29.07 / 39.52 / 33.50 42.62 / 47.84 / 45.08
</table>
<tableCaption confidence="0.999495">
Table 5: Comparison of Recall / Precision / F-score results on the test data set. Results on simple,
</tableCaption>
<bodyText confidence="0.9801054">
binding, regulation, and all events are shown. GDep and Enju with PAS are used. Results by Miwa et
al. (2010b), Bjorne et al. (2009), Riedel et al. (2009), and Baseline for Task 1 and Task 2 are shown for
comparison. Baseline results are produced by removing dependency information from the parse results
of GDep and Enju. The best score in each result is shown in bold.
tem.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999144583333333">
Many approaches for parser comparison have
been proposed, and most comparisons have used
gold treebanks with intermediate formats (Clegg
and Shepherd, 2007; Pyysalo et al., 2007). Parser
comparison has also been proposed on specific
tasks such as unbounded dependencies (Rimell
et al., 2009) and textual entailment (Onder Eker,
2009)7. Among them, application-oriented parser
comparison across several formats was first intro-
duced by Miyao et al. (2009), who compared eight
parsers and five formats for the protein-protein in-
teraction (PPI) extraction task. PPI extraction, the
</bodyText>
<page confidence="0.963192">
7
</page>
<footnote confidence="0.240426">
http://pete.yuret.com/
</footnote>
<page confidence="0.990302">
785
</page>
<bodyText confidence="0.9987273125">
\x0crecognition of binary relations of between pro-
teins, is one of the most basic information ex-
traction tasks in the BioNLP field. Our findings
do not conflict with those of Miyao et al. Event
extraction can be viewed as an additional extrin-
sic evaluation task for syntactic parsers, providing
more reliable and evaluation and a broader per-
spective into parser performance. An additional
advantage of application-oriented evaluation on
BioNLP shared task data is the availability of a
manually annotated gold standard treebank, the
GENIA treebank, that covers the same set of ab-
stracts as the task data. This allows the gold tree-
bank to be considered as an evaluation standard,
in addition to comparison of performance in the
primary task.
</bodyText>
<sectionHeader confidence="0.997223" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998724895833333">
We compared six parsers and three formats on a
bio-molecular event extraction task with a state-
of-the-art event extraction system from two dif-
ferent aspects: dependency-based intrinsic eval-
uation and task-based extrinsic evaluation. The
specific task considered was the BioNLP shared
task, allowing the use of the GENIA treebank as
a gold standard parse reference. Five of the six
considered parsers were applied using biomedi-
cal models trained on the GENIA treebank, and
they were found to produce similar performance.
The comparison of the parsers from two aspects
showed slightly different results, and and the
dependency representations have advantages and
disadvantages for the event extraction task.
The contributions of this paper are 1) the com-
parison of intrinsic and extrinsic evaluation on
several commonly used parsers with a state-of-
the-art system, and 2) demonstration of the lim-
itation and possibility of the parser and system
improvement on the task. One limitation of this
study is that the comparison between the parsers
is not perfect, as the parsers are used with the pro-
vided models, the format conversions miss some
information from the original formats, and results
with different formats depend on the ability of
the event extraction system to take advantage of
their strengths. To maximize comparability, the
system was designed to extract features identi-
cally from similar parts of the dependency-based
formats, further adding information provided by
other formats, such as the lexical entries of the
Enju format, from external resources. The results
of this paper are expected to be useful as a guide
not only for parser selection for biomedical infor-
mation extraction but also for the development of
event extraction systems.
The comparison in the present evaluation is
limited to the dependency representation. As fu-
ture work, it would be informative to extend the
comparison to other syntactic representation, such
as the PTB format. Finally, the evaluation showed
that the system fails to recover approximately
40% of events even when provided with manually
annotated treebank data, showing that other meth-
ods and resources need to be adopted to further
improve bio-molecular event extraction systems.
Such improvement is left as future work.
</bodyText>
<sectionHeader confidence="0.928901" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.5013986">
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT,
Japan), Genome Network Project (MEXT, Japan),
and Scientific Research (C) (General) (MEXT,
Japan).
</reference>
<page confidence="0.970223">
786
</page>
<reference confidence="0.99275718367347">
\x0cReferences
Bikel, Daniel M. 2004. A distributional analysis of a
lexicalized statistical parsing model. In In EMNLP,
pages 182189.
Bjorne, Jari, Juho Heimonen, Filip Ginter, Antti
Airola, Tapio Pahikkala, and Tapio Salakoski. 2009.
Extracting complex biological events with rich
graph-based feature sets. In Proceedings of the
BioNLP09 Shared Task on Event Extraction, pages
1018.
Clegg, Andrew B. and Adrian J. Shepherd. 2007.
Benchmarking natural-language parsers for biolog-
ical applications using dependency graphs. BMC
Bioinformatics, 8.
de Marneffe, Marie-Catherine and Christopher D.
Manning. 2008. Stanford typed dependencies man-
ual. Technical report, September.
de Marneffe, Marie-Catherine, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the IEEE / ACL 2006 Workshop on
Spoken Language Technology.
Johansson, Richard and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proceedings of NODALIDA 2007, Tartu,
Estonia, May 25-26.
Kim, Jin-Dong, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Junichi Tsujii. 2009. Overview
of bionlp09 shared task on event extraction. In
BioNLP 09: Proceedings of the Workshop on
BioNLP, pages 19.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In ACL 03: Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics, pages 423430, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
McClosky, David. 2009. Any Domain Parsing: Au-
tomatic Domain Adaptation for Natural Language
Parsing. Ph.D. thesis, Department of Computer Sci-
ence, Brown University.
Miwa, Makoto, Sampo Pyysalo, Tadayoshi Hara, and
Junichi Tsujii. 2010a. A comparative study of syn-
tactic parsers for event extraction. In BioNLP2010:
Proceedings of the Workshop on BioNLP, Uppsala,
Sweden, July.
Miwa, Makoto, Rune Stre, Jin-Dong Kim, and
Junichi Tsujii. 2010b. Event extraction with com-
plex event classification using rich features. Jour-
nal of Bioinformatics and Computational Biology
(JBCB), 8(1):131146, February.
Miyao, Yusuke, Kenji Sagae, Rune Stre, Takuya
Matsuzaki, and Jun ichi Tsujii. 2009. Evalu-
ating contributions of natural language parsers to
protein-protein interaction extraction. Bioinformat-
ics, 25(3):394400.
Onder Eker. 2009. Parser evaluation using textual
entailments. Masters thesis, Bogazici Universitesi,
August.
Pyysalo, Sampo, Filip Ginter, Veronika Laippala, Ka-
tri Haverinen, Juho Heimonen, and Tapio Salakoski.
2007. On the unification of syntactic annotations
under the stanford dependency scheme: A case
study on bioinfer and genia. In Biological, transla-
tional, and clinical language processing, pages 25
32, Prague, Czech Republic, June. Association for
Computational Linguistics.
Riedel, Sebastian, Hong-Woo Chun, Toshihisa Takagi,
and Junichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In BioNLP 09:
Proceedings of the Workshop on BioNLP, pages 41
49, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Rimell, Laura and Stephen Clark. 2009. Porting a
lexicalized-grammar parser to the biomedical do-
main. J. of Biomedical Informatics, 42(5):852865.
Rimell, Laura, Stephen Clark, and Mark Steedman.
2009. Unbounded dependency recovery for parser
evaluation. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 813821, Singapore, August. Asso-
ciation for Computational Linguistics.
Sagae, Kenji and Junichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In EMNLP-CoNLL 2007.
Tateisi, Yuka, Akane Yakushiji, Tomoko Ohta, and
Junfichi Tsujii. 2005. Syntax annotation for the ge-
nia corpus. In Proceedings of the IJCNLP 2005,
Companion volume, pages 222227, Jeju Island,
Korea, October.
Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Junichi Tsujii. 2005. Developing a robust
part-of-speech tagger for biomedical text. In Boza-
nis, Panayiotis and Elias N. Houstis, editors, Pan-
hellenic Conference on Informatics, volume 3746 of
Lecture Notes in Computer Science, pages 382392.
Springer.
</reference>
<page confidence="0.926129">
787
</page>
<figure confidence="0.287379">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.007842">
<note confidence="0.879627">b&amp;apos;Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 779787, Beijing, August 2010</note>
<title confidence="0.43047075">Evaluating Dependency Representation for Event Extraction Makoto Miwa1 Sampo Pyysalo1 Tadayoshi Hara1</title>
<author confidence="0.399157">Junichi Tsujii</author>
<email confidence="0.64701">1</email>
<affiliation confidence="0.811644666666667">Department of Computer Science, the University of Tokyo 2 School of Computer Science, University of Manchester</affiliation>
<address confidence="0.365345">3 National Center for Text Mining</address>
<email confidence="0.844761">mmiwa@is.s.u-tokyo.ac.jp</email>
<email confidence="0.844761">smp@is.s.u-tokyo.ac.jp</email>
<email confidence="0.844761">harasan@is.s.u-tokyo.ac.jp</email>
<email confidence="0.844761">tsujii@is.s.u-tokyo.ac.jp</email>
<abstract confidence="0.9998072">The detailed analyses of sentence structure provided by parsers have been applied to address several information extraction tasks. In a recent bio-molecular event extraction task, state-of-the-art performance was achieved by systems building specifically on dependency representations of parser output. While intrinsic evaluations have shown significant advances in both general and domain-specific parsing, the question of how these translate into practical advantage is seldom considered. In this paper, we analyze how event extraction performance is affected by parser and dependency representation, further considering the relation between intrinsic evaluation and performance at the extraction task. We find that good intrinsic evaluation results do not always imply good extraction performance, and that the types and structures of different dependency representations have specific advantages and disadvantages for the event extraction task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences Bikel</author>
<author>M Daniel</author>
</authors>
<title>A distributional analysis of a lexicalized statistical parsing model.</title>
<date>2004</date>
<booktitle>In In EMNLP,</booktitle>
<pages>182189</pages>
<marker>Bikel, Daniel, 2004</marker>
<rawString>\x0cReferences Bikel, Daniel M. 2004. A distributional analysis of a lexicalized statistical parsing model. In In EMNLP, pages 182189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jari Bjorne</author>
</authors>
<title>Juho Heimonen, Filip Ginter, Antti Airola, Tapio Pahikkala, and Tapio Salakoski.</title>
<date>2009</date>
<marker>Bjorne, 2009</marker>
<rawString>Bjorne, Jari, Juho Heimonen, Filip Ginter, Antti Airola, Tapio Pahikkala, and Tapio Salakoski. 2009.</rawString>
</citation>
<citation valid="false">
<title>Extracting complex biological events with rich graph-based feature sets.</title>
<booktitle>In Proceedings of the BioNLP09 Shared Task on Event Extraction,</booktitle>
<pages>1018</pages>
<marker></marker>
<rawString>Extracting complex biological events with rich graph-based feature sets. In Proceedings of the BioNLP09 Shared Task on Event Extraction, pages 1018.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew B Clegg</author>
<author>Adrian J Shepherd</author>
</authors>
<title>Benchmarking natural-language parsers for biological applications using dependency graphs.</title>
<date>2007</date>
<journal>BMC Bioinformatics,</journal>
<volume>8</volume>
<pages>de</pages>
<contexts>
<context position="25391" citStr="Clegg and Shepherd, 2007" startWordPosition="4115" endWordPosition="4118">Recall / Precision / F-score results on the test data set. Results on simple, binding, regulation, and all events are shown. GDep and Enju with PAS are used. Results by Miwa et al. (2010b), Bjorne et al. (2009), Riedel et al. (2009), and Baseline for Task 1 and Task 2 are shown for comparison. Baseline results are produced by removing dependency information from the parse results of GDep and Enju. The best score in each result is shown in bold. tem. 6 Related Work Many approaches for parser comparison have been proposed, and most comparisons have used gold treebanks with intermediate formats (Clegg and Shepherd, 2007; Pyysalo et al., 2007). Parser comparison has also been proposed on specific tasks such as unbounded dependencies (Rimell et al., 2009) and textual entailment (Onder Eker, 2009)7. Among them, application-oriented parser comparison across several formats was first introduced by Miyao et al. (2009), who compared eight parsers and five formats for the protein-protein interaction (PPI) extraction task. PPI extraction, the 7 http://pete.yuret.com/ 785 \x0crecognition of binary relations of between proteins, is one of the most basic information extraction tasks in the BioNLP field. Our findings do </context>
</contexts>
<marker>Clegg, Shepherd, 2007</marker>
<rawString>Clegg, Andrew B. and Adrian J. Shepherd. 2007. Benchmarking natural-language parsers for biological applications using dependency graphs. BMC Bioinformatics, 8. de Marneffe, Marie-Catherine and Christopher D.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manning</author>
</authors>
<title>Stanford typed dependencies manual.</title>
<date>2008</date>
<tech>Technical report,</tech>
<contexts>
<context position="17848" citStr="Manning, 2008" startWordPosition="2805" endWordPosition="2806">76 Table 2: Comparison of the F-score results with different SD variants on the development data set with the MC parser. The best score in each task is shown in bold. noted that as the parsers make use of annotated domain training data to different extents, this evaluation does not provide a sound basis for direct comparison of the parsers themselves. 5.2 Stanford Dependency Setting SD have four different variants: basic dependencies (BD), collapsed dependencies (CD), collapsed dependencies with propagation of conjunct dependencies (CDP), and collapsed tree dependencies (CTD) (de Marneffe and Manning, 2008). Except for BD, these variants do not necessarily connect all the words in the sentence, and CD and CDP do not necessarily form a tree structure. Table 2 shows the comparison results with the MC parser. Dependencies are generalized by removing expressions after of the dependencies (e.g. 783 \x0c with in prep with) for better performance. We find that basic dependencies give the best performance to event extraction, with little difference between the other variants. This result is surprising, as variants other than basic have features such as the resolution of conjunctions that are specificall</context>
</contexts>
<marker>Manning, 2008</marker>
<rawString>Manning. 2008. Stanford typed dependencies manual. Technical report, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the IEEE / ACL 2006 Workshop on Spoken Language Technology.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>de Marneffe, Marie-Catherine, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the IEEE / ACL 2006 Workshop on Spoken Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA 2007,</booktitle>
<pages>25--26</pages>
<location>Tartu, Estonia,</location>
<contexts>
<context position="10497" citStr="Johansson and Nugues, 2007" startWordPosition="1602" endWordPosition="1605">format containing predicate argument structures along with a phrase structure tree in Enju format, which can be converted into PTB format (Miyao et al., 2009). For direct comparison and for the study of contribution of the formats in which the six parsers output their analyses to task performance, we apply a number of conversions between the outputs, shown in Figure 5. The Enju PAS output is converted into PTB using the method introduced by (Miyao et al., 2009). SD is generated from PTB by the Stanford tools (de Marneffe et al., 2006), and CoNLL generated from PTB by using Treebank Converter (Johansson and Nugues, 2007). With the exception of GDep, all CoNLL outputs are generated by the conversion and thus share dependency types. We note that all of these conversions can introduce some errors in the conversion process. 781 \x0c4 Evaluation Setting 4.1 Event Extraction Evaluation Event extraction performance is evaluated using the evaluation script provided by the BioNLP09 shared task organizers for the development data set, and the online evaluation system of the task for the test data set2 . Results are reported under the official evaluation criterion of the task, i.e. the Approximate Span Matching/Approxim</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Johansson, Richard and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proceedings of NODALIDA 2007, Tartu, Estonia, May 25-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Junichi Tsujii</author>
</authors>
<title>Overview of bionlp09 shared task on event extraction.</title>
<date>2009</date>
<booktitle>In BioNLP 09: Proceedings of the Workshop on BioNLP,</booktitle>
<pages>19</pages>
<contexts>
<context position="1719" citStr="Kim et al., 2009" startWordPosition="238" endWordPosition="241">insic evaluation and performance at the extraction task. We find that good intrinsic evaluation results do not always imply good extraction performance, and that the types and structures of different dependency representations have specific advantages and disadvantages for the event extraction task. 1 Introduction Advanced syntactic parsing methods have been shown to effective for many information extraction tasks. The BioNLP 2009 Shared Task, a recent bio-molecular event extraction task, is one such task: analysis showed that the application of a parser correlated with high rank in the task (Kim et al., 2009). The automatic extraction of biomolecular events from text is important for a number of advanced domain applications such as pathway construction, and event extraction thus a key task in Biomedical Natural Language Processing (BioNLP). Methods building feature representations and extraction rules around dependency representations of sentence syntax have been successfully applied to a number of tasks in BioNLP. Several parsers and representations have been applied in high-performing methods both in domain studies in general and in the BioNLP09 shared task in particular, but no direct compariso</context>
<context position="4053" citStr="Kim et al., 2009" startWordPosition="577" endWordPosition="580">ed that the phosphorylation of TRAF2 inhibits binding to the CD40 cytoplasmic domain. ... Negative_regulation Phospholylation Binding Figure 1: Event Extraction. sults shows that performance against gold standard annotations is not always correlated with event extraction performance. We further find that the dependency types and overall structures employed by the different dependency representations have specific advantages and disadvantages for the event extraction task. 2 Bio-molecular Event Extraction In this study, we adopt the event extraction task defined in the BioNLP 2009 Shared Task (Kim et al., 2009) as a model information extraction task. Figure 1 shows an example illustrating the task of event extraction from a sentence. The shared task provided common and consistent task definitions, data sets for training and evaluation, and evaluation criteria. The shared task defined five simple events (Gene expression, Transcription, Protein catabolism, Phosphorylation, and Localization) that take one core argument, a multiparticipant binding event (Binding), and three regulation events (Regulation, Positive regulation, and Negative regulation) used to capture both biological regulation and general</context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Kim, Jin-Dong, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Junichi Tsujii. 2009. Overview of bionlp09 shared task on event extraction. In BioNLP 09: Proceedings of the Workshop on BioNLP, pages 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL 03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>423430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8192" citStr="Klein and Manning, 2003" startWordPosition="1228" endWordPosition="1231">nford basic dependency tree P2 NFAT/AP-1 complex formed only with P and NMOD VMOD VMOD PMOD CONJ PMOD COORD root ROOT Figure 3: CoNLL-X dependency tree noun_arg1 arg1 prep_arg12 arg1 prep_arg12 arg2 P2 NFAT/AP-1 complex formed only with P and verb_arg1 arg1 adj_arg1 arg1 coord_arg12 arg1 coord_arg12 arg2 Figure 4: Predicate Argument Structure parsers are GDep (Sagae and Tsujii, 2007), the Bikel parser (Bikel) (Bikel, 2004), the Stanford parser with two probabilistic context-free grammar (PCFG) models1 (Wall Street Journal (WSJ) model (Stanford WSJ) and augmented English model (Stanford eng)) (Klein and Manning, 2003), the Charniak-Johnson reranking parser, using David McCloskys self-trained biomedical parsing model (MC) (McClosky, 2009), the C&amp;C CCG parser, adapted to biomedical text (C&amp;C) (Rimell and Clark, 2009), and the Enju parser with the GENIA model (Miyao et al., 2009). The formats are Stanford Dependencies (SD) (Figure 2), the CoNLL-X dependency format (CoNLL) (Figure 3) and the predicateargument structure (PAS) format used by Enju (Figure 4). With the exception of Stanford and Enju, the analyses of these parsers were provided by the BioNLP 2009 Shared Task organizers. The six parsers operate in a</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, Dan and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In ACL 03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 423430, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
</authors>
<title>Any Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Brown University.</institution>
<contexts>
<context position="8314" citStr="McClosky, 2009" startWordPosition="1245" endWordPosition="1246">LL-X dependency tree noun_arg1 arg1 prep_arg12 arg1 prep_arg12 arg2 P2 NFAT/AP-1 complex formed only with P and verb_arg1 arg1 adj_arg1 arg1 coord_arg12 arg1 coord_arg12 arg2 Figure 4: Predicate Argument Structure parsers are GDep (Sagae and Tsujii, 2007), the Bikel parser (Bikel) (Bikel, 2004), the Stanford parser with two probabilistic context-free grammar (PCFG) models1 (Wall Street Journal (WSJ) model (Stanford WSJ) and augmented English model (Stanford eng)) (Klein and Manning, 2003), the Charniak-Johnson reranking parser, using David McCloskys self-trained biomedical parsing model (MC) (McClosky, 2009), the C&amp;C CCG parser, adapted to biomedical text (C&amp;C) (Rimell and Clark, 2009), and the Enju parser with the GENIA model (Miyao et al., 2009). The formats are Stanford Dependencies (SD) (Figure 2), the CoNLL-X dependency format (CoNLL) (Figure 3) and the predicateargument structure (PAS) format used by Enju (Figure 4). With the exception of Stanford and Enju, the analyses of these parsers were provided by the BioNLP 2009 Shared Task organizers. The six parsers operate in a number of different frameworks, reflected in their analyses. GDep is a native dependency parser that produces CoNLL depen</context>
</contexts>
<marker>McClosky, 2009</marker>
<rawString>McClosky, David. 2009. Any Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing. Ph.D. thesis, Department of Computer Science, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Sampo Pyysalo</author>
<author>Tadayoshi Hara</author>
<author>Junichi Tsujii</author>
</authors>
<title>A comparative study of syntactic parsers for event extraction. In</title>
<date>2010</date>
<booktitle>BioNLP2010: Proceedings of the Workshop on BioNLP,</booktitle>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5501" citStr="Miwa et al. (2010" startWordPosition="809" endWordPosition="812">subtasks, Task 1 and Task 2, out of the three defined in the shared task. Task 1 focuses on core event extraction, and Task 2 involves augmenting extracted events with secondary arguments (Kim et al., 2009). Events are represented with a textual trigger, type, and arguments, where the trigger is a span of text that states the event in text. In Task 1 the event arguments that need to be extracted are restricted to the core Theme and Cause roles, with secondary arguments corresponding to locations and sites considered in Task 2. 2.1 Event Extraction System For evaluation, we apply the system of Miwa et al. (2010b). The system was originally developed for finding core events (Task 1) using the native output of the Enju and GDep parsers. The system consists of three supervised classification-based modules: a trigger detector, an event edge detector, and a complex event detector. The trigger detector classifies each word into the appropriate event types, the event edge detector classifies each edge between an event and a candidate participant into an argument type, and the complex event detector classifies event candidates constructed by all edge combinations, deciding between event and non-event. The s</context>
<context position="7344" citStr="Miwa et al. (2010" startWordPosition="1097" endWordPosition="1100">ble for extracting features, including the dependency tags, word-related information (e.g. a lexical entry in Enju format), and the constituents and their head information. We apply here a variant of the base system incorporating a number of modifications. The applied system performs feature selection removing two classes of features that were found not to be beneficial for extraction performance, and applies a refinement of the trigger expressions of events. The system is further extended to find also secondary arguments (Task 2). For a detailed description of these improvements, we refer to Miwa et al. (2010a). 3 Parsers and Representations Six publicly available parsers and three dependency formats are considered in this paper. The 780 \x0cP2 NFAT/AP-1 complex formed only with P and nn nsubj prep dep conj pobj cc Figure 2: Stanford basic dependency tree P2 NFAT/AP-1 complex formed only with P and NMOD VMOD VMOD PMOD CONJ PMOD COORD root ROOT Figure 3: CoNLL-X dependency tree noun_arg1 arg1 prep_arg12 arg1 prep_arg12 arg2 P2 NFAT/AP-1 complex formed only with P and verb_arg1 arg1 adj_arg1 arg1 coord_arg12 arg1 coord_arg12 arg2 Figure 4: Predicate Argument Structure parsers are GDep (Sagae and Tsu</context>
<context position="11245" citStr="Miwa et al., 2010" startWordPosition="1720" endWordPosition="1723">f these conversions can introduce some errors in the conversion process. 781 \x0c4 Evaluation Setting 4.1 Event Extraction Evaluation Event extraction performance is evaluated using the evaluation script provided by the BioNLP09 shared task organizers for the development data set, and the online evaluation system of the task for the test data set2 . Results are reported under the official evaluation criterion of the task, i.e. the Approximate Span Matching/Approximate Recursive Matching criterion. The event extraction system described in Section 2.1 is used with the default settings given in (Miwa et al., 2010b). The C-values of SVMs are set to 1.0, but the positive and negative examples are balanced by placing more weight on the positive examples. The examples predicted with confidence greater than 0.5, as well as the examples with the most confident labels, are extracted. Task 1 and Task 2 are solved at once for the evaluation. Some of the parse results do not include word base forms or part-of-speech (POS) tags, which are required by the event extraction system. To apply these parsers, the GENIA Tagger (Tsuruoka et al., 2005) output is adopted to add this information to the results. 4.2 Dependen</context>
<context position="22085" citStr="Miwa et al. (2010" startWordPosition="3503" endWordPosition="3506">cy performance is small. MC and Enju results show that performance in dependency is important for event extraction. SD can be better than CoNLL for the event extraction (shown with the gold treebank data in Table 3), but the types and relations of CoNLL were well predicted, and MC and Enju performed better for CoNLL than for SD in total. 5.5 Performance of Event Extraction System Several systems are compared by the extraction performance on the shared task test data in Table 5. GDep and Enju with PAS are used for the evaluation, which is the same evaluation setting with the original system by Miwa et al. (2010b). The performance of the best systems in the original shared task is shown for reference ((Bjorne et al., 2009) in Task 1 and (Riedel et al., 2009) in Task 2). The event extraction system performs significantly better than the best systems in the shared task, further outperforming the original system. This shows that the comparison of the parsers is performed with a state-of-the-art sys784 \x0cTask 1 Task 2 SD CoNLL PAS SD CoNLL PAS Baseline 51.05 - 50.42 49.17 - 48.88 Bikel 53.29 53.22 - 51.40 51.27 - Stanford WSJ 53.51 54.38 - 52.02 52.04 - Stanford eng 55.02 53.66 - 53.41 52.74 - GDep - 5</context>
<context position="24953" citStr="Miwa et al. (2010" startWordPosition="4043" endWordPosition="4046"> 41.81 / 32.30 36.90 / 55.59 / 44.35 Baseline 62.94 / 68.38 / 65.55 48.41 / 34.50 / 40.29 29.40 / 40.00 / 33.89 43.93 / 50.11 / 46.82 Task 2 Ours 65.43 / 75.56 / 70.13 46.42 / 50.31 / 48.29 38.18 / 54.45 / 44.89 49.20 / 62.57 / 55.09 Riedel N/A 22.35 / 46.99 / 30.29 25.75 / 40.75 / 31.56 35.86 / 54.08 / 43.12 Baseline 60.88 / 63.78 / 62.30 44.99 / 31.78 / 37.25 29.07 / 39.52 / 33.50 42.62 / 47.84 / 45.08 Table 5: Comparison of Recall / Precision / F-score results on the test data set. Results on simple, binding, regulation, and all events are shown. GDep and Enju with PAS are used. Results by Miwa et al. (2010b), Bjorne et al. (2009), Riedel et al. (2009), and Baseline for Task 1 and Task 2 are shown for comparison. Baseline results are produced by removing dependency information from the parse results of GDep and Enju. The best score in each result is shown in bold. tem. 6 Related Work Many approaches for parser comparison have been proposed, and most comparisons have used gold treebanks with intermediate formats (Clegg and Shepherd, 2007; Pyysalo et al., 2007). Parser comparison has also been proposed on specific tasks such as unbounded dependencies (Rimell et al., 2009) and textual entailment (O</context>
</contexts>
<marker>Miwa, Pyysalo, Hara, Tsujii, 2010</marker>
<rawString>Miwa, Makoto, Sampo Pyysalo, Tadayoshi Hara, and Junichi Tsujii. 2010a. A comparative study of syntactic parsers for event extraction. In BioNLP2010: Proceedings of the Workshop on BioNLP, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Rune Stre</author>
<author>Jin-Dong Kim</author>
<author>Junichi Tsujii</author>
</authors>
<title>Event extraction with complex event classification using rich features.</title>
<date>2010</date>
<journal>Journal of Bioinformatics and Computational Biology (JBCB),</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="5501" citStr="Miwa et al. (2010" startWordPosition="809" endWordPosition="812">subtasks, Task 1 and Task 2, out of the three defined in the shared task. Task 1 focuses on core event extraction, and Task 2 involves augmenting extracted events with secondary arguments (Kim et al., 2009). Events are represented with a textual trigger, type, and arguments, where the trigger is a span of text that states the event in text. In Task 1 the event arguments that need to be extracted are restricted to the core Theme and Cause roles, with secondary arguments corresponding to locations and sites considered in Task 2. 2.1 Event Extraction System For evaluation, we apply the system of Miwa et al. (2010b). The system was originally developed for finding core events (Task 1) using the native output of the Enju and GDep parsers. The system consists of three supervised classification-based modules: a trigger detector, an event edge detector, and a complex event detector. The trigger detector classifies each word into the appropriate event types, the event edge detector classifies each edge between an event and a candidate participant into an argument type, and the complex event detector classifies event candidates constructed by all edge combinations, deciding between event and non-event. The s</context>
<context position="7344" citStr="Miwa et al. (2010" startWordPosition="1097" endWordPosition="1100">ble for extracting features, including the dependency tags, word-related information (e.g. a lexical entry in Enju format), and the constituents and their head information. We apply here a variant of the base system incorporating a number of modifications. The applied system performs feature selection removing two classes of features that were found not to be beneficial for extraction performance, and applies a refinement of the trigger expressions of events. The system is further extended to find also secondary arguments (Task 2). For a detailed description of these improvements, we refer to Miwa et al. (2010a). 3 Parsers and Representations Six publicly available parsers and three dependency formats are considered in this paper. The 780 \x0cP2 NFAT/AP-1 complex formed only with P and nn nsubj prep dep conj pobj cc Figure 2: Stanford basic dependency tree P2 NFAT/AP-1 complex formed only with P and NMOD VMOD VMOD PMOD CONJ PMOD COORD root ROOT Figure 3: CoNLL-X dependency tree noun_arg1 arg1 prep_arg12 arg1 prep_arg12 arg2 P2 NFAT/AP-1 complex formed only with P and verb_arg1 arg1 adj_arg1 arg1 coord_arg12 arg1 coord_arg12 arg2 Figure 4: Predicate Argument Structure parsers are GDep (Sagae and Tsu</context>
<context position="11245" citStr="Miwa et al., 2010" startWordPosition="1720" endWordPosition="1723">f these conversions can introduce some errors in the conversion process. 781 \x0c4 Evaluation Setting 4.1 Event Extraction Evaluation Event extraction performance is evaluated using the evaluation script provided by the BioNLP09 shared task organizers for the development data set, and the online evaluation system of the task for the test data set2 . Results are reported under the official evaluation criterion of the task, i.e. the Approximate Span Matching/Approximate Recursive Matching criterion. The event extraction system described in Section 2.1 is used with the default settings given in (Miwa et al., 2010b). The C-values of SVMs are set to 1.0, but the positive and negative examples are balanced by placing more weight on the positive examples. The examples predicted with confidence greater than 0.5, as well as the examples with the most confident labels, are extracted. Task 1 and Task 2 are solved at once for the evaluation. Some of the parse results do not include word base forms or part-of-speech (POS) tags, which are required by the event extraction system. To apply these parsers, the GENIA Tagger (Tsuruoka et al., 2005) output is adopted to add this information to the results. 4.2 Dependen</context>
<context position="22085" citStr="Miwa et al. (2010" startWordPosition="3503" endWordPosition="3506">cy performance is small. MC and Enju results show that performance in dependency is important for event extraction. SD can be better than CoNLL for the event extraction (shown with the gold treebank data in Table 3), but the types and relations of CoNLL were well predicted, and MC and Enju performed better for CoNLL than for SD in total. 5.5 Performance of Event Extraction System Several systems are compared by the extraction performance on the shared task test data in Table 5. GDep and Enju with PAS are used for the evaluation, which is the same evaluation setting with the original system by Miwa et al. (2010b). The performance of the best systems in the original shared task is shown for reference ((Bjorne et al., 2009) in Task 1 and (Riedel et al., 2009) in Task 2). The event extraction system performs significantly better than the best systems in the shared task, further outperforming the original system. This shows that the comparison of the parsers is performed with a state-of-the-art sys784 \x0cTask 1 Task 2 SD CoNLL PAS SD CoNLL PAS Baseline 51.05 - 50.42 49.17 - 48.88 Bikel 53.29 53.22 - 51.40 51.27 - Stanford WSJ 53.51 54.38 - 52.02 52.04 - Stanford eng 55.02 53.66 - 53.41 52.74 - GDep - 5</context>
<context position="24953" citStr="Miwa et al. (2010" startWordPosition="4043" endWordPosition="4046"> 41.81 / 32.30 36.90 / 55.59 / 44.35 Baseline 62.94 / 68.38 / 65.55 48.41 / 34.50 / 40.29 29.40 / 40.00 / 33.89 43.93 / 50.11 / 46.82 Task 2 Ours 65.43 / 75.56 / 70.13 46.42 / 50.31 / 48.29 38.18 / 54.45 / 44.89 49.20 / 62.57 / 55.09 Riedel N/A 22.35 / 46.99 / 30.29 25.75 / 40.75 / 31.56 35.86 / 54.08 / 43.12 Baseline 60.88 / 63.78 / 62.30 44.99 / 31.78 / 37.25 29.07 / 39.52 / 33.50 42.62 / 47.84 / 45.08 Table 5: Comparison of Recall / Precision / F-score results on the test data set. Results on simple, binding, regulation, and all events are shown. GDep and Enju with PAS are used. Results by Miwa et al. (2010b), Bjorne et al. (2009), Riedel et al. (2009), and Baseline for Task 1 and Task 2 are shown for comparison. Baseline results are produced by removing dependency information from the parse results of GDep and Enju. The best score in each result is shown in bold. tem. 6 Related Work Many approaches for parser comparison have been proposed, and most comparisons have used gold treebanks with intermediate formats (Clegg and Shepherd, 2007; Pyysalo et al., 2007). Parser comparison has also been proposed on specific tasks such as unbounded dependencies (Rimell et al., 2009) and textual entailment (O</context>
</contexts>
<marker>Miwa, Stre, Kim, Tsujii, 2010</marker>
<rawString>Miwa, Makoto, Rune Stre, Jin-Dong Kim, and Junichi Tsujii. 2010b. Event extraction with complex event classification using rich features. Journal of Bioinformatics and Computational Biology (JBCB), 8(1):131146, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
</authors>
<title>Kenji Sagae, Rune Stre, Takuya Matsuzaki, and Jun ichi Tsujii.</title>
<date>2009</date>
<journal>Bioinformatics,</journal>
<volume>25</volume>
<issue>3</issue>
<marker>Miyao, 2009</marker>
<rawString>Miyao, Yusuke, Kenji Sagae, Rune Stre, Takuya Matsuzaki, and Jun ichi Tsujii. 2009. Evaluating contributions of natural language parsers to protein-protein interaction extraction. Bioinformatics, 25(3):394400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Onder Eker</author>
</authors>
<title>Parser evaluation using textual entailments.</title>
<date>2009</date>
<tech>Masters thesis,</tech>
<institution>Bogazici Universitesi,</institution>
<contexts>
<context position="25569" citStr="Eker, 2009" startWordPosition="4144" endWordPosition="4145">jorne et al. (2009), Riedel et al. (2009), and Baseline for Task 1 and Task 2 are shown for comparison. Baseline results are produced by removing dependency information from the parse results of GDep and Enju. The best score in each result is shown in bold. tem. 6 Related Work Many approaches for parser comparison have been proposed, and most comparisons have used gold treebanks with intermediate formats (Clegg and Shepherd, 2007; Pyysalo et al., 2007). Parser comparison has also been proposed on specific tasks such as unbounded dependencies (Rimell et al., 2009) and textual entailment (Onder Eker, 2009)7. Among them, application-oriented parser comparison across several formats was first introduced by Miyao et al. (2009), who compared eight parsers and five formats for the protein-protein interaction (PPI) extraction task. PPI extraction, the 7 http://pete.yuret.com/ 785 \x0crecognition of binary relations of between proteins, is one of the most basic information extraction tasks in the BioNLP field. Our findings do not conflict with those of Miyao et al. Event extraction can be viewed as an additional extrinsic evaluation task for syntactic parsers, providing more reliable and evaluation an</context>
</contexts>
<marker>Eker, 2009</marker>
<rawString>Onder Eker. 2009. Parser evaluation using textual entailments. Masters thesis, Bogazici Universitesi, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Filip Ginter</author>
<author>Veronika Laippala</author>
<author>Katri Haverinen</author>
<author>Juho Heimonen</author>
<author>Tapio Salakoski</author>
</authors>
<title>On the unification of syntactic annotations under the stanford dependency scheme: A case study on bioinfer and genia.</title>
<date>2007</date>
<booktitle>In Biological, translational, and clinical language processing,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="25414" citStr="Pyysalo et al., 2007" startWordPosition="4119" endWordPosition="4122">re results on the test data set. Results on simple, binding, regulation, and all events are shown. GDep and Enju with PAS are used. Results by Miwa et al. (2010b), Bjorne et al. (2009), Riedel et al. (2009), and Baseline for Task 1 and Task 2 are shown for comparison. Baseline results are produced by removing dependency information from the parse results of GDep and Enju. The best score in each result is shown in bold. tem. 6 Related Work Many approaches for parser comparison have been proposed, and most comparisons have used gold treebanks with intermediate formats (Clegg and Shepherd, 2007; Pyysalo et al., 2007). Parser comparison has also been proposed on specific tasks such as unbounded dependencies (Rimell et al., 2009) and textual entailment (Onder Eker, 2009)7. Among them, application-oriented parser comparison across several formats was first introduced by Miyao et al. (2009), who compared eight parsers and five formats for the protein-protein interaction (PPI) extraction task. PPI extraction, the 7 http://pete.yuret.com/ 785 \x0crecognition of binary relations of between proteins, is one of the most basic information extraction tasks in the BioNLP field. Our findings do not conflict with those</context>
</contexts>
<marker>Pyysalo, Ginter, Laippala, Haverinen, Heimonen, Salakoski, 2007</marker>
<rawString>Pyysalo, Sampo, Filip Ginter, Veronika Laippala, Katri Haverinen, Juho Heimonen, and Tapio Salakoski. 2007. On the unification of syntactic annotations under the stanford dependency scheme: A case study on bioinfer and genia. In Biological, translational, and clinical language processing, pages 25 32, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Hong-Woo Chun</author>
<author>Toshihisa Takagi</author>
<author>Junichi Tsujii</author>
</authors>
<title>A markov logic approach to bio-molecular event extraction.</title>
<date>2009</date>
<booktitle>In BioNLP 09: Proceedings of the Workshop on BioNLP,</booktitle>
<pages>41--49</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="22234" citStr="Riedel et al., 2009" startWordPosition="3531" endWordPosition="3534">r the event extraction (shown with the gold treebank data in Table 3), but the types and relations of CoNLL were well predicted, and MC and Enju performed better for CoNLL than for SD in total. 5.5 Performance of Event Extraction System Several systems are compared by the extraction performance on the shared task test data in Table 5. GDep and Enju with PAS are used for the evaluation, which is the same evaluation setting with the original system by Miwa et al. (2010b). The performance of the best systems in the original shared task is shown for reference ((Bjorne et al., 2009) in Task 1 and (Riedel et al., 2009) in Task 2). The event extraction system performs significantly better than the best systems in the shared task, further outperforming the original system. This shows that the comparison of the parsers is performed with a state-of-the-art sys784 \x0cTask 1 Task 2 SD CoNLL PAS SD CoNLL PAS Baseline 51.05 - 50.42 49.17 - 48.88 Bikel 53.29 53.22 - 51.40 51.27 - Stanford WSJ 53.51 54.38 - 52.02 52.04 - Stanford eng 55.02 53.66 - 53.41 52.74 - GDep - 55.70 - - 54.37 - MC 55.60 56.01 - 53.94 54.51 - C&amp;C 56.09 - - 54.27 - - Enju 55.48 55.74 56.57 54.06 54.37 55.31 GENIA 56.34 56.09 57.94 55.04 54.57 </context>
<context position="24999" citStr="Riedel et al. (2009)" startWordPosition="4051" endWordPosition="4054">ne 62.94 / 68.38 / 65.55 48.41 / 34.50 / 40.29 29.40 / 40.00 / 33.89 43.93 / 50.11 / 46.82 Task 2 Ours 65.43 / 75.56 / 70.13 46.42 / 50.31 / 48.29 38.18 / 54.45 / 44.89 49.20 / 62.57 / 55.09 Riedel N/A 22.35 / 46.99 / 30.29 25.75 / 40.75 / 31.56 35.86 / 54.08 / 43.12 Baseline 60.88 / 63.78 / 62.30 44.99 / 31.78 / 37.25 29.07 / 39.52 / 33.50 42.62 / 47.84 / 45.08 Table 5: Comparison of Recall / Precision / F-score results on the test data set. Results on simple, binding, regulation, and all events are shown. GDep and Enju with PAS are used. Results by Miwa et al. (2010b), Bjorne et al. (2009), Riedel et al. (2009), and Baseline for Task 1 and Task 2 are shown for comparison. Baseline results are produced by removing dependency information from the parse results of GDep and Enju. The best score in each result is shown in bold. tem. 6 Related Work Many approaches for parser comparison have been proposed, and most comparisons have used gold treebanks with intermediate formats (Clegg and Shepherd, 2007; Pyysalo et al., 2007). Parser comparison has also been proposed on specific tasks such as unbounded dependencies (Rimell et al., 2009) and textual entailment (Onder Eker, 2009)7. Among them, application-ori</context>
</contexts>
<marker>Riedel, Chun, Takagi, Tsujii, 2009</marker>
<rawString>Riedel, Sebastian, Hong-Woo Chun, Toshihisa Takagi, and Junichi Tsujii. 2009. A markov logic approach to bio-molecular event extraction. In BioNLP 09: Proceedings of the Workshop on BioNLP, pages 41 49, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Porting a lexicalized-grammar parser to the biomedical domain.</title>
<date>2009</date>
<journal>J. of Biomedical Informatics,</journal>
<volume>42</volume>
<issue>5</issue>
<contexts>
<context position="8393" citStr="Rimell and Clark, 2009" startWordPosition="1256" endWordPosition="1259">NFAT/AP-1 complex formed only with P and verb_arg1 arg1 adj_arg1 arg1 coord_arg12 arg1 coord_arg12 arg2 Figure 4: Predicate Argument Structure parsers are GDep (Sagae and Tsujii, 2007), the Bikel parser (Bikel) (Bikel, 2004), the Stanford parser with two probabilistic context-free grammar (PCFG) models1 (Wall Street Journal (WSJ) model (Stanford WSJ) and augmented English model (Stanford eng)) (Klein and Manning, 2003), the Charniak-Johnson reranking parser, using David McCloskys self-trained biomedical parsing model (MC) (McClosky, 2009), the C&amp;C CCG parser, adapted to biomedical text (C&amp;C) (Rimell and Clark, 2009), and the Enju parser with the GENIA model (Miyao et al., 2009). The formats are Stanford Dependencies (SD) (Figure 2), the CoNLL-X dependency format (CoNLL) (Figure 3) and the predicateargument structure (PAS) format used by Enju (Figure 4). With the exception of Stanford and Enju, the analyses of these parsers were provided by the BioNLP 2009 Shared Task organizers. The six parsers operate in a number of different frameworks, reflected in their analyses. GDep is a native dependency parser that produces CoNLL dependency trees, with dependency types similar to those of CoNLL 2007. Bikel, Stanf</context>
<context position="9780" citStr="Rimell and Clark, 2009" startWordPosition="1479" endWordPosition="1482">Stanford Figure 5: Format conversion dependencies in six parsers. Formats adopted for the evaluation are shown in solid boxes. SD: Stanford Dependency format, CCG: Combinatory Categorial Grammar output format, PTB: Penn Treebank format, and PAS: Predicate Argument Structure in Enju format. are phrase-structure parsers trained on Penn Treebank format (PTB) style treebanks, and they produce PTB trees. C&amp;C is a deep parser based on Combinatory Categorial Grammar (CCG), and its native output is in a CCG-specific format. The output of C&amp;C can be converted into SD by a rule-based conversion script (Rimell and Clark, 2009). Enju is deep parser based on Head-driven Phrase Structure Grammar (HPSG) and produces a format containing predicate argument structures along with a phrase structure tree in Enju format, which can be converted into PTB format (Miyao et al., 2009). For direct comparison and for the study of contribution of the formats in which the six parsers output their analyses to task performance, we apply a number of conversions between the outputs, shown in Figure 5. The Enju PAS output is converted into PTB using the method introduced by (Miyao et al., 2009). SD is generated from PTB by the Stanford to</context>
</contexts>
<marker>Rimell, Clark, 2009</marker>
<rawString>Rimell, Laura and Stephen Clark. 2009. Porting a lexicalized-grammar parser to the biomedical domain. J. of Biomedical Informatics, 42(5):852865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
</authors>
<title>Unbounded dependency recovery for parser evaluation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>813821</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25527" citStr="Rimell et al., 2009" startWordPosition="4136" endWordPosition="4139">ith PAS are used. Results by Miwa et al. (2010b), Bjorne et al. (2009), Riedel et al. (2009), and Baseline for Task 1 and Task 2 are shown for comparison. Baseline results are produced by removing dependency information from the parse results of GDep and Enju. The best score in each result is shown in bold. tem. 6 Related Work Many approaches for parser comparison have been proposed, and most comparisons have used gold treebanks with intermediate formats (Clegg and Shepherd, 2007; Pyysalo et al., 2007). Parser comparison has also been proposed on specific tasks such as unbounded dependencies (Rimell et al., 2009) and textual entailment (Onder Eker, 2009)7. Among them, application-oriented parser comparison across several formats was first introduced by Miyao et al. (2009), who compared eight parsers and five formats for the protein-protein interaction (PPI) extraction task. PPI extraction, the 7 http://pete.yuret.com/ 785 \x0crecognition of binary relations of between proteins, is one of the most basic information extraction tasks in the BioNLP field. Our findings do not conflict with those of Miyao et al. Event extraction can be viewed as an additional extrinsic evaluation task for syntactic parsers,</context>
</contexts>
<marker>Rimell, Clark, Steedman, 2009</marker>
<rawString>Rimell, Laura, Stephen Clark, and Mark Steedman. 2009. Unbounded dependency recovery for parser evaluation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 813821, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Junichi Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with LR models and parser ensembles. In EMNLP-CoNLL</title>
<date>2007</date>
<contexts>
<context position="7954" citStr="Sagae and Tsujii, 2007" startWordPosition="1194" endWordPosition="1197"> et al. (2010a). 3 Parsers and Representations Six publicly available parsers and three dependency formats are considered in this paper. The 780 \x0cP2 NFAT/AP-1 complex formed only with P and nn nsubj prep dep conj pobj cc Figure 2: Stanford basic dependency tree P2 NFAT/AP-1 complex formed only with P and NMOD VMOD VMOD PMOD CONJ PMOD COORD root ROOT Figure 3: CoNLL-X dependency tree noun_arg1 arg1 prep_arg12 arg1 prep_arg12 arg2 P2 NFAT/AP-1 complex formed only with P and verb_arg1 arg1 adj_arg1 arg1 coord_arg12 arg1 coord_arg12 arg2 Figure 4: Predicate Argument Structure parsers are GDep (Sagae and Tsujii, 2007), the Bikel parser (Bikel) (Bikel, 2004), the Stanford parser with two probabilistic context-free grammar (PCFG) models1 (Wall Street Journal (WSJ) model (Stanford WSJ) and augmented English model (Stanford eng)) (Klein and Manning, 2003), the Charniak-Johnson reranking parser, using David McCloskys self-trained biomedical parsing model (MC) (McClosky, 2009), the C&amp;C CCG parser, adapted to biomedical text (C&amp;C) (Rimell and Clark, 2009), and the Enju parser with the GENIA model (Miyao et al., 2009). The formats are Stanford Dependencies (SD) (Figure 2), the CoNLL-X dependency format (CoNLL) (Fi</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Sagae, Kenji and Junichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser ensembles. In EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuka Tateisi</author>
<author>Akane Yakushiji</author>
<author>Tomoko Ohta</author>
<author>Junfichi Tsujii</author>
</authors>
<title>Syntax annotation for the genia corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of the IJCNLP 2005, Companion volume,</booktitle>
<pages>222227</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="2687" citStr="Tateisi et al., 2005" startWordPosition="383" endWordPosition="386">ntax have been successfully applied to a number of tasks in BioNLP. Several parsers and representations have been applied in high-performing methods both in domain studies in general and in the BioNLP09 shared task in particular, but no direct comparison of parsers or representations has been performed. Likewise, a number of evaluation of parser outputs against gold standard corpora have been performed in the domain, but the broader implications of the results of such intrinsic evaluations are rarely considered. The BioNLP09 shared task involved documents contained also in the GENIA treebank (Tateisi et al., 2005), creating an opportunity for direct study of intrinsic and task-oriented evaluation results. As the treebank can be converted into various dependency formats using existing format conversion methods, evaluation can further be extended to cover the effects of different representations. In this this paper, we consider three types of dependency representation and six parsers, evaluating their performance from two different aspects: dependency-based intrinsic evaluation, and effectiveness for bio-molecular event extraction with a state-of-the-art event extraction system. Comparison of intrinsic a</context>
<context position="13576" citStr="Tateisi et al., 2005" startWordPosition="2108" endWordPosition="2111">an one parent. We cannot simply apply accuracy, or (un)labeled attachment score3. Word-based normalization is performed to avoid negative impact by the word segmentations by parsers. When (a) and (d) in Figure 6 are compared, the counts of correct relations will be 1.0 (0.5 for upper NMOD and 0.5 for lower NMOD in Figure 6 (d)) for the parser (precision), and the counts of correct relations will be 1.0 (for NMOD in Figure 6 (a)) for the gold (recall). This F-score is a good approximation of accuracy. 4.3 GENIA treebank processing For comparison and evaluation, the texts in the GENIA treebank (Tateisi et al., 2005) are converted to the various formats as follows. To create PAS, the treebank is converted with Enju, and for trees that fail conversion, parse results are used instead. The GENIA treebank is also converted into PTB4, and then converted into SD and CoNLL as described in Section 3. While based on manually annotated gold data, the converted treebanks are not always correct due to conversion errors. 5 Evaluation This section presents evaluation results. Intrinsic evaluation is first performed in Section 5.1. Section 5.2 considers the effect of different SD variants. Section 5.3 presents the resul</context>
</contexts>
<marker>Tateisi, Yakushiji, Ohta, Tsujii, 2005</marker>
<rawString>Tateisi, Yuka, Akane Yakushiji, Tomoko Ohta, and Junfichi Tsujii. 2005. Syntax annotation for the genia corpus. In Proceedings of the IJCNLP 2005, Companion volume, pages 222227, Jeju Island, Korea, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
</authors>
<title>Yuka Tateishi, Jin-Dong Kim, Tomoko Ohta, John McNaught, Sophia Ananiadou, and Junichi Tsujii.</title>
<date>2005</date>
<booktitle>Panhellenic Conference on Informatics,</booktitle>
<volume>3746</volume>
<pages>382392</pages>
<editor>In Bozanis, Panayiotis and Elias N. Houstis, editors,</editor>
<publisher>Springer.</publisher>
<marker>Tsuruoka, 2005</marker>
<rawString>Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim, Tomoko Ohta, John McNaught, Sophia Ananiadou, and Junichi Tsujii. 2005. Developing a robust part-of-speech tagger for biomedical text. In Bozanis, Panayiotis and Elias N. Houstis, editors, Panhellenic Conference on Informatics, volume 3746 of Lecture Notes in Computer Science, pages 382392. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>