The backed-o\x0b models are combined by linear interpolation, with the weights chosen as in CITATION,,
The parameters of a probabilistic TAG (CITATION; CITATION) are: X \x0b Pi(\x0b) = 1 X \x0b Ps(\x0b j \x11) = 1 X \x0c Pa(\x0c j \x11) + Pa(NONE j \x11) = 1 where \x0b ranges over initial trees, \x0c over auxiliary trees, over modi\x0cer trees, and \x11 over nodes,,
CITATION suggest other parameterizations worth exploring as well,,
For example, the attachment of an S depends on the presence or absence of the embedded subject CITATION; Treebank-style two-level NPs are mismodeled by PCFG (CITATION; CITATION); the generation of a node depends on the label of its grandparent (CITATION; CITATION),,
With \x0c\x14 40 words \x14 100 words LR LP CB 0 CB \x14 2 CB LR LP CB 0 CB \x14 2 CB CITATION 84.6 84.9 1.26 56.6 81.4 84.0 84.3 1.46 54.0 78.8 CITATION 85.8 86.3 1.14 59.9 83.6 85.3 85.7 1.32 57.2 80.8 present model 86.9 86.6 1.09 63.2 84.3 86.2 85.8 1.29 60.4 81.8 CITATION 88.1 88.6 0.91 66.5 86.9 87.5 88.1 1.07 63.9 84.6 CITATION 90.1 90.1 0.74 70.1 89.6 89.6 89.5 0.88 67.6 87.7 Figure 6: Parsing results,,
6 Conclusion: related and future work CITATION describes an experiment similar to ours, although the grammar he extracts only arrives at a complete parse for 10% of unseen sentences,,
We \x0cnd that this induction method is an improvement over the EM-based method of CITATION, and that the induced model yields results comparable to lexicalized PCFG,,
The approach of CITATION to language modeling is illustrative: even though the probability estimate of w appearing as the kth word can be conditioned on the entire history w1;:::;wk,1, the quantity of available training data limits the usable context to about two words|but which two? A trigram model chooses wk,1 and wk,2 and works quite well; a model which chose wk,7 and wk,11 would probably work less well,,
But CITATION chooses the lexical heads of the two previous constituents as determined by a shift-reduce parser, and works better than a trigram model,,
We \x0cnd that the automatically-extracted grammar gives an improvement over the EM-based induction method of CITATION, and that the parser performs comparably to lexicalized PCFG parsers, though certainly with room for improvement,,
(This is where the analogy with CITATION breaks down.) Thus certain possibilities which were not apparent in a PCFG framework or prohibitively complicated might become simple to implement in a PTAG framework; we conclude by o\x0bering two such possibilities,,
2 The formalism The formalism we use is a variant of lexicalized tree-insertion grammar (LTIG), which is in turn a restriction of LTAG CITATION,,
One, also suggested by CITATION, is to group elementary trees into families and relate the trees of a family by transformations,,
parser scored 84.4% compared with 82.4% for CITATION, an error reduction of 11%,,
With \x0c\x14 40 words \x14 100 words LR LP CB 0 CB \x14 2 CB LR LP CB 0 CB \x14 2 CB CITATION 84.6 84.9 1.26 56.6 81.4 84.0 84.3 1.46 54.0 78.8 CITATION 85.8 86.3 1.14 59.9 83.6 85.3 85.7 1.32 57.2 80.8 present model 86.9 86.6 1.09 63.2 84.3 86.2 85.8 1.29 60.4 81.8 CITATION 88.1 88.6 0.91 66.5 86.9 87.5 88.1 1.07 63.9 84.6 CITATION 90.1 90.1 0.74 70.1 89.6 89.6 89.5 0.88 67.6 87.7 Figure 6: Parsing results,,
4 Inducing a stochastic grammar from the Treebank 4.1 Reconstructing derivations We want to extract from the Penn Treebank an LTAG whose derivations mirror the dependency analysis implicit in the head-percolation rules of (CITATION; CITATION),,
he one describedin CITATION, with a modi\x0ccation to ensure completeness (because foot nodes are treated as empty, which CKY prohibits) and another to reduce useless substitutions,,
We use a beam search, computing the score of an item [\x11;i;j] by multiplying it by the prior probability P(\x11) CITATION; any item with score less than 10,5 times that of the best item in a cell is pruned,,
Following CITATION, words occurring fewer than four times in training were replaced with the symbol *UNKNOWN* and tagged with the output of the part-of-speech tagger described in CITATION,,
We \x0crst compared the parser with CITATION: we trained the model on sentences of length 40 or less in sections 02{09 of the Penn Treebank, down to parts of speech only, and then tested on sentences of length 40 or less in section 23, parsing from part-of-speech tag sequences to fully bracketed parses,,
With \x0c\x14 40 words \x14 100 words LR LP CB 0 CB \x14 2 CB LR LP CB 0 CB \x14 2 CB CITATION 84.6 84.9 1.26 56.6 81.4 84.0 84.3 1.46 54.0 78.8 CITATION 85.8 86.3 1.14 59.9 83.6 85.3 85.7 1.32 57.2 80.8 present model 86.9 86.6 1.09 63.2 84.3 86.2 85.8 1.29 60.4 81.8 CITATION 88.1 88.6 0.91 66.5 86.9 87.5 88.1 1.07 63.9 84.6 CITATION 90.1 90.1 0.74 70.1 89.6 89.6 89.5 0.88 67.6 87.7 Figure 6: Parsing results,,
6 Conclusion: related and future work CITATION describes an experiment similar to ours, although the grammar he extracts onl,,
For example, the attachment of an S depends on the presence or absence of the embedded subject CITATION; Treebank-style two-level NPs are mismodeled by PCFG (CITATION; CITATION); the generation of a node depends on the label of its grandparent (CITATION; CITATION),,
5.2 Parsing with the grammar We used a CKY-style parser similarto the one describedin CITATION, with a modi\x0ccation to ensure completeness (because foot nodes are treated as empty, which CKY prohibits) and another to reduce useless substitutions,,
We use a beam search, computing the score of an item [\x11;i;j] by multiplying it by the prior probability P(\x11) CITATION; any item with score less than 10,5 times that of the best item in a cell is pruned,,
Following CITATION, words occurring fewer than four times in training were replaced with the symbol *UNKNOWN* and tagged with the output of the part-of-speech tagger described in CITATION,,
We \x0crst compared the parser with CITATION: we trained the model on sentences of length 40 or less in sections 02{09 of the Penn Treebank, down to parts of speech only, and then tested on sentences of length 40 or less in section 23,,
 1: Grammar and derivation for \\John should leave tomorrow.&quot; model bilexical dependencies was noted early on by CITATION,,
We \x0cnd that the automatically-extracted grammar gives an improvement over the EM-based induction method of CITATION, and that the parser performs comparably to lexicalized PCFG parsers, though certainly with room for improvement,,
(This is where the analogy with CITATION breaks down.) Thus certain possibilities which were not apparent in a PCFG framework or prohibitively complicated might become simple to implement in a PTAG framework; we conclude by o\x0bering two such possibilities,,
One approach, taken in CITATION, is to choose some grammar general enough to parse the whole corpus and obtain a maximum-likelihoodestimate by EM,,
Another approach, taken in CITATION and others for lexicalized PCFGs and (CITATION; CITATION; Chen and VijayShanker, 2000) for LTAGs, is to use heuristics to reconstruct the derivations, and directly estimate the PTAG parameters from the reconstructed derivations,,
We use a beam search, computing the score of an item [\x11;i;j] by multiplying it by the prior probability P(\x11) CITATION; any item with score less than 10,5 times that of the best item in a cell is pruned,,
Following CITATION, words occurring fewer than four times in training were replaced with the symbol *UNKNOWN* and tagged with the output of the part-of-speech tagger described in CITATION,,
We \x0crst compared the parser with CITATION: we trained the model on sentences of length 40 or less in sections 02{09 of the Penn Treebank, down to parts of speech only, and then tested on sentences of length 40 or less in section 23, parsing from part-of-speech tag sequences to fully bracketed parses,,
Our parser scored 84.4% compared with 82.4% for CITATION, an error reduction of 11%,,
For example, the attachment of an S depends on the presence or absence of the embedded subject CITATION; Treebank-style two-level NPs are mismodeled by PCFG (CITATION; CITATION); the generation of a node depends on the label of its grandparent (CITATION; CITATION),,
But CITATION chooses the lexical heads of the two previous constituents as determined by a shift-reduce parser, and works better than a trigram model,,
But beginning with CITATION statistical parsers have used bilexical dependencies with great success,,
One approach, taken in CITATION, is to choose some grammar general enough to parse the whole corpus and obtain a maximum-likelihoodestimate by EM,,
Another approach, taken in CITATION and others for lexicalized PCFGs and (CITATION; CITATION; Chen and VijayShanker, 2000) for LTAGs, is to use heuristics to reconstruct the derivations, and directly estimate the PTAG parameters from the reconstructed derivations,,
4 Inducing a stochastic grammar from the Treebank 4.1 Reconstructing derivations We want to extract from the Penn Treebank an LTAG whose derivations mirror the dependency analysis implicit in the head-percolation rules of (CITATION; CITATION),,
Our parser scored 84.4% compared with 82.4% for CITATION, an error reduction of 11%,,
With \x0c\x14 40 words \x14 100 words LR LP CB 0 CB \x14 2 CB LR LP CB 0 CB \x14 2 CB CITATION 84.6 84.9 1.26 56.6 81.4 84.0 84.3 1.46 54.0 78.8 CITATION 85.8 86.3 1.14 59.9 83.6 85.3 85.7 1.32 57.2 80.8 present model 86.9 86.6 1.09 63.2 84.3 86.2 85.8 1.29 60.4 81.8 CITATION 88.1 88.6 0.91 66.5 86.9 87.5 88.1 1.07 63.9 84.6 CITATION 90.1 90.1 0.74 70.1 89.6 89.6 89.5 0.88 67.6 87.7 Figure 6: Parsing results,,
One approach, taken in CITATION, is to choose some grammar general enough to parse the whole corpus and obtain a maximum-likelihoodestimate by EM,,
Another approach, taken in CITATION and others for lexicalized PCFGs and (CITATION; CITATION; Chen and VijayShanker, 2000) for LTAGs, is to use heuristics to reconstruct the derivations, and directly estimate the PTAG parameters from the reconstructed derivations,,
32 57.2 80.8 present model 86.9 86.6 1.09 63.2 84.3 86.2 85.8 1.29 60.4 81.8 CITATION 88.1 88.6 0.91 66.5 86.9 87.5 88.1 1.07 63.9 84.6 CITATION 90.1 90.1 0.74 70.1 89.6 89.6 89.5 0.88 67.6 87.7 Figure 6: Parsing results,,
6 Conclusion: related and future work CITATION describes an experiment similar to ours, although the grammar he extracts only arrives at a complete parse for 10% of unseen sentences,,
CITATION describes a grammar extraction process similar to ours, and describes some techniques for automatically \x0cltering out invalid elementary trees,,
formalism we use is a variant of lexicalized tree-insertion grammar (LTIG), which is in turn a restriction of LTAG CITATION,,
Sister-adjunction is not an operation found in standard de\x0cnitions of TAG, but is borrowed from D-Tree Grammar CITATION,,
Following CITATION, multiple modi\x0cer trees can be sister-adjoined at a single site, but only one auxiliary tree may be adjoined at a single node,,
We use a beam search, computing the score of an item [\x11;i;j] by multiplying it by the prior probability P(\x11) CITATION; any item with score less than 10,5 times that of the best item in a cell is pruned,,
Following CITATION, words occurring fewer than four times in training were replaced with the symbol *UNKNOWN* and tagged with the output of the part-of-speech tagger described in CITATION,,
We \x0crst compared the parser with CITATION: we trained the model on sentences of length 40 or less in sections 02{09 of the Penn Treebank, down to parts of speech only, and then tested on sentences of length 40 or less in section 23, parsing from part-of-speech tag sequences to fully bracketed parses,,
Our parser scored 84.4% compared with 82.4% for CITATION, an error reduction of 11%,,
The ability of probabilistic LTAG to \x0cNP NNP John S NP# VP VB leave VP MD should VP\x03 NP NN tomorrow (\x0b1) (\x0b2) (\x0c) ( ) ) \x0b2 \x0b1 1 \x0c 2 2,1 S NP NNP John VP MD should VP VB leave NP NN tomorrow Figure 1: Grammar and derivation for \\John should leave tomorrow.&quot; model bilexical dependencies was noted early on by CITATION,,
We \x0cnd that the automatically-extracted grammar gives an improvement over the EM-based induction method of CITATION, and that the parser performs comparably to lexicalized PCFG parsers, though certainly with room for improvement,,
The parameters of a probabilistic TAG (CITATION; CITATION) are: X \x0b Pi(\x0b) = 1 X \x0b Ps(\x0b j \x11) = 1 X \x0c Pa(\x0c j \x11) + Pa(NONE j \x11) = 1 where \x0b ranges over initial trees, \x0c over auxiliary trees, over modi\x0cer trees, and \x11 over nodes,,
CITATION suggest other parameterizations worth exploring as well,,
Sister-adjunction is not an operation found in standard de\x0cnitions of TAG, but is borrowed from D-Tree Grammar CITATION,,
Following CITATION, multiple modi\x0cer trees can be sister-adjoined at a single site, but only one auxiliary tree may be adjoined at a single node,,
(This is where the analogy with CITATION breaks down.) Thus certain possibilities which were not apparent in a PCFG framework or prohibitively complicated might become simple to implement in a PTAG framework; we conclude by o\x0bering two such possibilities,,
2 The formalism The formalism we use is a variant of lexicalized tree-insertion grammar (LTIG), which is in turn a restriction of LTAG CITATION,,
Sister-adjunction is not an operation found in standard de\x0cnitions of TAG, but is borrowed from D-Tree Grammar CITATION,,
5.2 Parsing with the grammar We used a CKY-style parser similarto the one describedin CITATION, with a modi\x0ccation to ensure completeness (because foot nodes are treated as empty, which CKY prohibits) and another to reduce useless substitutions,,
We use a beam search, computing the score of an item [\x11;i;j] by multiplying it by the prior probability P(\x11) CITATION; any item with score less than 10,5 times that of the best item in a cell is pruned,,
Following CITATION, words occurring fewer than fo,,
The parameters of a probabilistic TAG (CITATION; CITATION) are: X \x0b Pi(\x0b) = 1 X \x0b Ps(\x0b j \x11) = 1 X \x0c Pa(\x0c j \x11) + Pa(NONE j \x11) = 1 where \x0b ranges over initial trees, \x0c over auxiliary trees, over modi\x0cer trees, and \x11 over nodes,,
CITATION suggest other parameterizations worth exploring as well,,
One approach, taken in CITATION, is to choose some grammar general enough to parse the whole corpus and obtain a maximum-likelihoodestimate by EM,,
Another approach, taken in CITATION and others for lexicalized PCFGs and (CITATION; CITATION; Chen and VijayShanker, 2000) for LTAGs, is to use heuristics to reconstruct the derivations, and directly estimate the PTAG parameters from the reconstructed derivations,,
6 Conclusion: related and future work CITATION describes an experiment similar to ours, although the grammar he extracts only arrives at a complete parse for 10% of unseen sentences,,
CITATION describes a grammar extraction process similar to ours, and describes some techniques for automatically \x0cltering out invalid elementary trees,,
