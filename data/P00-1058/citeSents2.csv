6 Conclusion: related and future work CITATION describes an experiment similar to ours, although the grammar he extracts onl,,
We use a beam search, computing the score of an item [\x11;i;j] by multiplying it by the prior probability P(\x11) CITATION; any item with score less than 10,5 times that of the best item in a cell is pruned,,
he one describedin CITATION, with a modi\x0ccation to ensure completeness (because foot nodes are treated as empty, which CKY prohibits) and another to reduce useless substitutions,,
Following CITATION, words occurring fewer than four times in training were replaced with the symbol *UNKNOWN* and tagged with the output of the part-of-speech tagger described in CITATION,,
We \x0crst compared the parser with CITATION: we trained the model on sentences of length 40 or less in sections 02{09 of the Penn Treebank, down to parts of speech only, and then tested on sentences of length 40 or less in section 23, parsing from part-of-speech tag sequences to fully bracketed parses,,
With \x0c\x14 40 words \x14 100 words LR LP CB 0 CB \x14 2 CB LR LP CB 0 CB \x14 2 CB CITATION 84.6 84.9 1.26 56.6 81.4 84.0 84.3 1.46 54.0 78.8 CITATION 85.8 86.3 1.14 59.9 83.6 85.3 85.7 1.32 57.2 80.8 present model 86.9 86.6 1.09 63.2 84.3 86.2 85.8 1.29 60.4 81.8 CITATION 88.1 88.6 0.91 66.5 86.9 87.5 88.1 1.07 63.9 84.6 CITATION 90.1 90.1 0.74 70.1 89.6 89.6 89.5 0.88 67.6 87.7 Figure 6: Parsing results,,
4 Inducing a stochastic grammar from the Treebank 4.1 Reconstructing derivations We want to extract from the Penn Treebank an LTAG whose derivations mirror the dependency analysis implicit in the head-percolation rules of (CITATION; CITATION),,
