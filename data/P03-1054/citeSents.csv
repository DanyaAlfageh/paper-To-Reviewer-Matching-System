In the early 1990s, as probabilistic methods swept NLP, parsing work revived the investigation of probabilistic context-free grammars (PCFGs) (CITATION; CITATION),,
This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (CITATION; CITATION),,
In the early 1990s, as probabilistic methods swept NLP, parsing work revived the investigation of probabilistic context-free grammars (PCFGs) (CITATION; CITATION),,
This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (CITATION; CITATION).,,
O(n4) if using the clever approach of CITATION,,
An unlexicalized PCFG parser is much simpler to build and optimize, including both standard code optimization techniques and the investigation of methods for search space pruning (CITATION; CITATION),,
O(n4) if using the clever approach of CITATION,,
An unlexicalized PCFG parser is much simpler to build and optimize, including both standard code optimization techniques and the investigation of methods for search space pruning (CITATION; CITATION),,
nted in CITATION,,
The second basic deficiency is that many rule types have been seen only once (and therefore have their probabilities overestimated), and many rules which occur in test sentences will never have been seen in training (and therefore have their probabilities underestimated see CITATION for analysis),,
Note that in parsing with the unsplit grammar, not having seen a rule doesnt mean one gets a parse failure, but rather a possibly very weird parse CITATION,,
One successful method of combating sparsity is to markovize the rules CITATION,,
In particular, we follow that work in markovizing out from the head child, despite the grammar being unlexicalized, because this seems the best way to capture the traditional linguistic insight that phrases are organized around a head CITATION,,
This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (CITATION; CITATION),,
In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (CITATION; CITATION; CITATION; CITATION; CITATION),,
CITATION showed that the performance of an unlexicalized PCFG over the Penn treebank could be improved enormously simply by annotating each node by its parent category,,
More recently, CITATION discusses how taking the bilexical probabilities,,
Specifically, we construct an unlexicalized PCFG which outperforms the lexicalized PCFGs of CITATION and CITATION (though not more recent models, such as CITATION or CITATION),,
First we split off auxiliary verbs with the SPLITAUX annotation, which appends BE to all forms of be and HAVE to all forms of have.10 More minorly, SPLIT-CC marked conjunction tags to indicate 10This is an extended uniform version of the partial auxiliary annotation of CITATION, wherein all auxiliaries are marked as AUX and a -G is added to gerund auxiliaries and gerund VPs,,
in CITATION,,
\x0cLength 40 LP LR F1 Exact CB 0 CB CITATION 84.9 84.6 1.26 56.6 CITATION 86.3 85.8 1.14 59.9 this paper 86.9 85.7 86.3 30.9 1.10 60.3 CITATION 87.4 87.5 1.00 62.1 CITATION 88.7 88.6 0.90 67.1 Length 100 LP LR F1 Exact CB 0 CB this paper 86.3 85.1 85.7 28.8 1.31 57.2 Figure 8: Results of the final model on the test set (section 23),,
This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (CITATION; CITATION),,
In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (CITATION; CITATION; CITATION; CITATION; CITATION),,
CITATION showed that the performance of an unlexicalized PCFG over the Penn treebank could be improved enormously simply by annotating each node by its parent category,,
More recently, CITATION discusses how taking the bilexical probabilities out of a good current lexicali,,
3In this paper we use the term subcategorization in the original general sense of CITATION, for where a syntactic cat\x0ccategories appearing in the Penn treebank,,
CITATION shows the value his parser gains from parentannotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and CITATION uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG, such as differentiating base NPs from noun phrases with phrasal modifiers, and distinguishing sentences with empty subjects from those where there is an overt subject NP,,
12This is part of the explanation of why CITATION finds that early generation of head tags as in CITATION is so beneficial,,
This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (CITATION; CITATION),,
In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (CITATION; CITATION; CITATION; CITATION; CITATION),,
CITATION showed that the performance of an unlexicalized PCFG over the Penn treebank could be improved enormously simply by annotating each node by its parent category,,
More recently, CITATION discusses how taking the bilexical probabilities out of a good current lexicalized PCFG parser h,,
3In this paper we use the term subcategorization in the original general sense of CITATION, for where a syntactic cat\x0ccategories appearing in the Penn treebank,,
CITATION shows the value his parser gains from parentannotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and CITATION uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG, such as differentiating base NPs from noun phrases with phrasal modifiers, and distinguishing sentences with empty subjects from those,,
Specifically, we construct an unlexicalized PCFG which outperforms the lexicalized PCFGs of CITATION and CITATION (though not more recent models, such as CITATION or CITATION),,
This is a partial explanation of the utility of verbal distance in CITATION,,
\x0cLength 40 LP LR F1 Exact CB 0 CB CITATION 84.9 84.6 1.26 56.6 CITATION 86.3 85.8 1.14 59.9 this paper 86.9 85.7 86.3 30.9 1.10 60.3 CITATION 87.4 87.5 1.00 62.1 CITATION 88.7 88.6 0.90 67.1 Length 100 LP LR F1 Exact CB 0 CB this paper 86.3 85.1 85.7 28.8 1.31 57.2 Figure 8: Results of the final model on the test set (section 23),,
This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (CITATION; CITATION),,
In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (CITATION; CITATION; CITATION; CITATION; CITATION),,
CITATION showed that the performance of an unlexicalized PCFG over the Penn treebank could be improved enormously simply by annotating each node by its parent category,,
More recently, CITATION discusses how taking the bilexical probabilities out of a good ,,
3In this paper we use the term subcategorization in the original general sense of CITATION, for where a syntactic cat\x0ccategories appearing in the Penn treebank,,
CITATION shows the value his parser gains from parentannotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and CITATION uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG, such as differentiating base NPs from noun phrases with phrasal modifiers, and distinguishing sentences with empty subjects from those where there is an overt subject NP,,
One way of capturing this kind of external context is to use parent annotation, as presented in CITATION,,
The second basic deficiency is that many rule types have been seen only once (and therefore have their probabilities overestimated), and many rules which occur in test sentences will never have been seen in training (and therefore have their probabilities underestimated see CITATION for analysis),,
Note that in parsing with the unsplit grammar, not having seen a rule doesnt mean one gets a parse failure, but rather a possibly very weird parse CITATION,,
One successful method of combating sparsity is to markovize the rules CITATION,,
In particular, we follow that work in markovizing out from the head child, despite the grammar being unlexicalized, because this seems the best way to capture the traditional linguistic insight that phrases are organized around a head CITATION,,
The raw treebank grammar corresponds to v = 1, h = (the upper right corner), while the parent annotation in CITATION corresponds to v = 2, h = , and the second-order model in CITATION, is broadly a smoothed version of v = 2, h = 2,,
Following CITATION, the annotation GAPPED-S marks S nodes which have an empty subject (i.e., raising and control constructions),,
12This is part of the explanation of why CITATION finds that early generation of head tags as in CITATION is so beneficial,,
CITATION captures this notion by introducing the notion of a base NP, in which any NP which dominates only preterminals is marked with a -B,,
\x0cLength 40 LP LR F1 Exact CB 0 CB CITATION 84.9 84.6 1.26 56.6 CITATION 86.3 85.8 1.14 59.9 this paper 86.9 85.7 86.3 30.9 1.10 60.3 CITATION 87.4 87.5 1.00 62.1 CITATION 88.7 88.6 0.90 67.1 Length 100 LP LR F1 Exact CB 0 CB this paper 86.3 85.1 85.7 28.8 1.31 57.2 Figure 8: Results of the final model on the test set (section 23),,
O(n4) if using the clever approach of CITATION,,
An unlexicalized PCFG parser is much simpler to build and optimize, including both standard code optimization techniques and the investigation of methods for search space pruning (CITATION; CITATION),,
e grammars (PCFGs) (CITATION; CITATION),,
This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (CITATION; CITATION),,
In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (CITATION; CITATION; CITATION; CITATION; CITATION),,
CITATION showed that the performance of an unlexicalized PCFG over the Penn treebank could be improved enormously simply by annotating each node by its parent category,,
arious lexicalized PCFG models (CITATION; CITATION; CITATION; CITATION; CITATION),,
CITATION showed that the performance of an unlexicalized PCFG over the Penn treebank could be improved enormously simply by annotating each node by its parent category,,
More recently, CITATION discusses how taking the bilexical probabilities out of a good current lexicalized PCFG parser hurts performance hardly at all: by at most 0.5% for test text from the same domain as the training data, and not at all for test text from a different domain.1 But it is precisely these bilexical dependencies that backed the intuition that lexicalized PCFGs should be very successful, for example in Hindle and Rooths demonstration from PP attachment,,
(CITATION; CITATION),,
This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (CITATION; CITATION),,
In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (CITATION; CITATION; CITATION; CITATION; CITATION),,
CITATION showed that the performance of an unlexicalized PCFG over the Penn treebank could be improved enormously simply by annotating each node by its parent category,,
-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (CITATION; CITATION),,
In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (CITATION; CITATION; CITATION; CITATION; CITATION),,
CITATION showed that the performance of an unlexicalized PCFG over the Penn treebank could be improved enormously simply by annotating each node by its parent category,,
More recently, CITATION discusses how taking the bilexical probabilities out of a good current lexicalized PCFG parser hurts performance hardly at all: by at most 0.5% for test text from the same domain as the training data, and not at all for,,
For each model, input trees were annotated or transformed in some way, as in CITATION,,
One way of capturing this kind of external context is to use parent annotation, as presented in CITATION,,
The second basic deficiency is that many rule types have been seen only once (and therefore have their probabilities overestimated), and many rules which occur in test sentences will never have been seen in training (and therefore have their probabilities underestimated see CITATION for analysis),,
Note that in parsing with the unsplit grammar, not having seen a rule doesnt mean one gets a parse failure, but rather a possibly very weird parse CITATION,,
The raw treebank grammar corresponds to v = 1, h = (the upper right corner), while the parent annotation in CITATION corresponds to v = 2, h = , and the second-order model in CITATION, is broadly a smoothed version of v = 2, h = 2,,
In the raw grammar, there are many unaries, and once any major category is constructed over a span, most others become constructible as well using unary chains (see CITATION for discussion),,
This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (CITATION; CITATION),,
In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (CITATION; CITATION; CITATION; CITATION; CITATION),,
CITATION showed that the performance of an unlexicalized PCFG over the Penn treebank could be improved enormously simply by annotating each node by its parent category,,
More recently, CITATION discusses how taking the bilexic,,
Specifically, we construct an unlexicalized PCFG which outperforms the lexicalized PCFGs of CITATION and CITATION (though not more recent models, such as CITATION or CITATION),,
This is a partial explanation of the utility of verbal distance in CITATION,,
\x0cLength 40 LP LR F1 Exact CB 0 CB CITATION 84.9 84.6 1.26 56.6 CITATION 86.3 85.8 1.14 59.9 this paper 86.9 85.7 86.3 30.9 1.10 60.3 CITATION 87.4 87.5 1.00 62.1 CITATION 88.7 88.6 0.90 67.1 Length 100 LP LR F1 Exact CB 0 CB this paper 86.3 85.1 85.7 28.8 1.31 57.2 Figure 8: Results of the final model on the test set (section 23),,
 been seen in training (and therefore have their probabilities underestimated see CITATION for analysis),,
Note that in parsing with the unsplit grammar, not having seen a rule doesnt mean one gets a parse failure, but rather a possibly very weird parse CITATION,,
One successful method of combating sparsity is to markovize the rules CITATION,,
In particular, we follow that work in markovizing out from the head child, despite the grammar being unlexicalized, because this seems the best way to capture the traditional linguistic insight that phrases are organized around a head CITATION,,
history models similar in intent to those described in CITATION,,
