1 Introduction There has been a great deal of progress in statistical parsing in the past decade (CITATION; CITATION; Chaniak, 2000),,
It was found in CITATION that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output,,
CITATION observed that the bi-lexical statistics accounted for only 1.49% of the bigram statistics used by the parser,,
CITATION presented an unlexicalized parser that eliminated all lexicalized parameters,,
We used the same data split as CITATION: Sections 1-270 and 400-931 as 156 \x0cthe training set, Sections 271-300 as testing and Sections 301-325 as the development set,,
We converted them to dependency trees using the same method and the head table as CITATION,,
In the DMV model CITATION, the probability of a dependency link is partly conditioned on whether or not there is a head word of the link already has a modifier,,
There have been several previous approaches to parsing Chinese with the Penn Chinese Treebank (e.g., CITATION; CITATION),,
Moreover, it was argued in CITATION that dependency based eva,,
Performance of Alternative Models 157 \x0c5 Related Work Previous parsing models (e.g., CITATION; CITATION) maximize the joint probability P(S, T) of a sentence S and its parse tree T,,
CITATION also computes a conditional probability of dependency structures,,
Performance of Alternative Models 157 \x0c5 Related Work Previous parsing models (e.g., CITATION; CITATION) maximize the joint probability P(S, T) of a sentence S and its parse tree T,,
CITATION also computes a conditional probability of dependency structures,,
1 Introduction There has been a great deal of progress in statistical parsing in the past decade (CITATION; CITATION; Chaniak, 2000),,
It was found in CITATION that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output,,
CITATION observed that the bi-lexical statistics accounted for only 1.49% of the bigram statistics used by the parser,,
1 Introduction There has been a great deal of progress in statistical parsing in the past decade (CITATION; CITATION; Chaniak, 2000),,
It was found in CITATION that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output,,
CITATION observed that the bi-lexical statistics accounted for only 1.49% of the bigram statistics used by the parser,,
Performance of Alternative Models 157 \x0c5 Related Work Previous parsing models (e.g., CITATION; CITATION) maximize the joint probability P(S, T) of a sentence S and its parse tree T,,
CITATION also computes a conditional probability of dependency structures,,
The generation process according to the canonical order is similar to the head outward generation process in CITATION, except that it is bottom-up whereas Collins models are top-down,,
We can compute the probability of T as follows: ( ) ( ) ( ) = = = N i i i N G G S G P S G G G P S T P 1 1 1 2 1 ,..., , | | ,..., , | Following CITATION, we require that the creation of a dependency link from head h to modifier m be preceded by placing a left STOP and a right STOP around the modifier m and STOP between h and m,,
The probability ( ) 1 1,..., , | i i G G S G P can be computed as: ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) L v L L v L v R u R u L u L u i L L v R u L u i i C v u v u link P C v E P C u E P C u E P G G S v u link E E E P G G S G P , , | , , | 1 , | , | ,..., , | , , , , ,..., , | 1 1 1 1 = = The events R w E and L w E correspond to the STOP events in CITATION and CITATION,,
Similarity-based smoothing was used in CITATION to estimate word co-occurrence probabilities,,
In CITATION, the bigram probability P(w2|w1) is computed as the weighted average of the conditional probability of w2 given similar words of w1,,
A difference between similarity-based smoothing in CITATION and our approach is that our model only computes probability distributions of binary variables,,
Firstly, when a context contains two words, we are able to use the cross product of the similar words, whereas CITATION can only use the similar words of one of the words,,
Secondly, in CITATION, the distribution P(|w1) may itself be sparsely observed,,
The similarity-based smoothing method in CITATION uses the similar words of one of the words in a bigram,,
In many dependency parsing models such as CITATION and (MacDonald et al., 2005), the score of a dependency tree is the sum of the scores of the dependency links, which are computed independently of other links,,
In the DMV model CITATION, the probability of a dependency link is partly conditioned on whether or not there is a head word of the link already has a,,
1 Introduction There has been a great deal of progress in statistical parsing in the past decade (CITATION; CITATION; Chaniak, 2000),,
It was found in CITATION that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output,,
CITATION observed that the bi-lexical statistics accounted for only 1.49% of the bigram statistics used by the parser,,
CITATION pr,,
This is known as the Distributional Hypothesis in linguistics CITATION,,
Many methods have been proposed to compute distributional similarity between words (CITATION; CITATION; CITATION; CITATION),,
This is known as the Distributional Hypothesis in linguistics CITATION,,
Many methods have been proposed to compute distributional similarity between words (CITATION; CITATION; CITATION; CITATION),,
This is known as the Distributional Hypothesis in linguistics CITATION,,
Many methods have been proposed to compute distributional similarity between words (CITATION; CITATION; CITATION; CITATION),,
was found in CITATION that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output,,
CITATION observed that the bi-lexical statistics accounted for only 1.49% of the bigram statistics used by the parser,,
CITATION presented an unlexicalized parser that eliminated all lexicalized parameters,,
The generation process according to the canonical order is similar to the head outward generation process in CITATION, except that it is bottom-up whereas Collins models are top-down,,
We can compute the probability of T as follows: ( ) ( ) ( ) = = = N i i i N G G S G P S G G G P S T P 1 1 1 2 1 ,..., , | | ,..., , | Following CITATION, we require that the creation of a dependency link from head h to modifier m be preceded by placing a left STOP and a right STOP around the modifier m and STOP between h and m,,
The probability ( ) 1 1,..., , | i i G G S G P can be computed as: ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) L v L L v L v R u R u L u L u i L L v R u L u i i C v u v u link P C v E P C u E P C u E P G G S v u link E E E P G G S G P , , | , , | 1 , | , | ,..., , | , , , , ,..., , | 1 1 1 1 = = The events R w E and L w E correspond to the STOP events in CITATION and CITATION,,
In many dependency parsing models such as CITATION and (MacDonald et al., 2005), the score of a dependency tree is the sum of the scores of the dependency links, which are computed independently of other links,,
In the DMV model CITATION, the probability of a dependency link is partly conditioned on whether or not there is a head word of the link already has a modifier,,
There have been several previous approaches to parsing Chinese with the Penn Chinese Treebank (e.g., CITATION; CITATION),,
In the DMV model CITATION, the probability of a dependency link is partly conditioned on whether or not there is a head word of the link already has a modifier,,
There have been several previous approaches to parsing Chinese with the Penn Chinese Treebank (e.g., CITATION; CITATION),,
Moreover, it was argued in CITATION that dependency based evaluation is much more mean,,
ebank (e.g., CITATION; CITATION),,
Moreover, it was argued in CITATION that dependency based evaluation is much more meaningful for the applications that use parse trees, since the semantic relationships are generally embedded in the dependency relationships,,
This is known as the Distributional Hypothesis in linguistics CITATION,,
Many methods have been proposed to compute distributional similarity between words (CITATION; CITATION; CITATION; CITATION),,
This is known as the Distributional Hypothesis in linguistics CITATION,,
Many methods have been proposed to compute distributional similarity between words (CITATION; CITATION; CITATION; CITATION),,
