<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.7054385">
b&amp;quot;Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 152159,
Vancouver, October 2005. c
</bodyText>
<sectionHeader confidence="0.477252" genericHeader="abstract">
2005 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.877273">
Strictly Lexical Dependency Parsing
</title>
<author confidence="0.836129">
Qin Iris Wang and Dale Schuurmans Dekang Lin
</author>
<affiliation confidence="0.985985">
Department of Computing Science Google, Inc.
University of Alberta 1600 Amphitheatre Parkway
</affiliation>
<address confidence="0.928382">
Edmonton, Alberta, Canada, T6G 2E8 Mountain View, California, USA, 94043
</address>
<email confidence="0.990084">
{wqin,dale}@cs.ualberta.ca lindek@google.com
</email>
<sectionHeader confidence="0.99057" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999604954545455">
We present a strictly lexical parsing
model where all the parameters are based
on the words. This model does not rely
on part-of-speech tags or grammatical
categories. It maximizes the conditional
probability of the parse tree given the
sentence. This is in contrast with most
previous models that compute the joint
probability of the parse tree and the sen-
tence. Although the maximization of
joint and conditional probabilities are
theoretically equivalent, the conditional
model allows us to use distributional
word similarity to generalize the ob-
served frequency counts in the training
corpus. Our experiments with the Chi-
nese Treebank show that the accuracy of
the conditional model is 13.6% higher
than the joint model and that the strictly
lexicalized conditional model outper-
forms the corresponding unlexicalized
model based on part-of-speech tags.
</bodyText>
<sectionHeader confidence="0.998311" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998446688888889">
There has been a great deal of progress in statisti-
cal parsing in the past decade (Collins, 1996;
Collins, 1997; Chaniak, 2000). A common charac-
teristic of these parsers is their use of lexicalized
statistics. However, it was discovered recently that
bi-lexical statistics (parameters that involve two
words) actually played much smaller role than
previously believed. It was found in (Gildea,
2001) that the removal of bi-lexical statistics from
a state-of-the-art PCFG parser resulted very small
change in the output. Bikel (2004) observed that
the bi-lexical statistics accounted for only 1.49%
of the bigram statistics used by the parser. When
considering only bigram statistics involved in the
highest probability parse, this percentage becomes
28.8%. However, even when the bi-lexical statis-
tics do get used, they are remarkably similar to
their back-off values using part-of-speech tags.
Therefore, the utility of bi-lexical statistics be-
comes rather questionable. Klein and Manning
(2003) presented an unlexicalized parser that
eliminated all lexicalized parameters. Its perform-
ance was close to the state-of-the-art lexicalized
parsers.
We present a statistical dependency parser that
represents the other end of spectrum where all
statistical parameters are lexical and the parser
does not require part-of-speech tags or grammati-
cal categories. We call this strictly lexicalized
parsing.
A part-of-speech lexicon has always been con-
sidered to be a necessary component in any natu-
ral language parser. This is true in early rule-based
as well as modern statistical parsers and in de-
pendency parsers as well as constituency parsers.
The need for part-of-speech tags arises from the
sparseness of natural language data. They provide
generalizations of words that are critical for pars-
ers to deal with the sparseness. Words belonging
to the same part-of-speech are expected to have
the same syntactic behavior.
Instead of part-of-speech tags, we rely on dis-
tributional word similarities computed automati-
cally from a large unannotated text corpus. One of
the benefits of strictly lexicalized parsing is that
</bodyText>
<page confidence="0.998389">
152
</page>
<bodyText confidence="0.976227653846154">
\x0cfunds
investors continue to pour cash into money
Many
0 1 2 3 4 5 6 7 8 9
the parser can be trained with a treebank that only
contains the dependency relationships between
words. The annotators do not need to annotate
parts-of-speech or non-terminal symbols (they
dont even have to know about them), making the
construction of the treebank easier.
Strictly lexicalized parsing is especially benefi-
cial for languages such as Chinese, where parts-
of-speech are not as clearly defined as English. In
Chinese, clear indicators of a word&amp;apos;s part-of-
speech such as suffixes -ment, -ous or function
words such as the, are largely absent. In fact,
monolingual Chinese dictionaries that are mainly
intended for native speakers almost never contain
part-of-speech information.
In the next section, we present a method for
modeling the probabilities of dependency trees.
Section 3 applies similarity-based smoothing to
the probability model to deal with data sparseness.
We then present experimental results with the
Chinese Treebank in Section 4 and discuss related
work in Section 5.
</bodyText>
<sectionHeader confidence="0.997108" genericHeader="method">
2 A Probabilistic Dependency Model
</sectionHeader>
<bodyText confidence="0.940231521739131">
Let S be a sentence. The dependency structure T
of S is a directed tree connecting the words in S.
Each link in the tree represents a dependency rela-
tionship between two words, known as the head
and the modifier. The direction of the link is from
the head to the modifier. We add an artificial root
node () at the beginning of each sentence and a
dependency link from to the head of the sen-
tence so that the head of the sentence can be
treated in the same way as other words. Figure 1
shows an example dependency tree.
We denote a dependency link l by a triple (u, v,
d), where u and v are the indices (u &amp;lt; v) of the
words connected by l, and d specifies the direction
of the link l. The value of d is either L or R. If d =
L, v is the index of the head word; otherwise, u is
the index of the head word.
Dependency trees are typically assumed to be
projective (without crossing arcs), which means
that if there is an arc from h to m, h is an ancestor
of all the words between h and m. Let F(S) be the
set of possible directed, projective trees spanning
on S. The parsing problem is to find
</bodyText>
<equation confidence="0.9252985">
( ) ( )
S
T
P
S
F
T |
max
</equation>
<bodyText confidence="0.994290696969697">
arg
Generative parsing models are usually defined
recursively from top down, even though the de-
coders (parsers) for such models almost always
take a bottom-up approach. The model proposed
here is a bottom-up one. Like previous ap-
proaches, we decompose the generation of a parse
tree into a sequence of steps and define the prob-
ability of each step. The probability of the tree is
simply the product of the probabilities of the steps
involved in the generation process. This scheme
requires that different sequences of steps must not
lead to the same tree. We achieve this by defining
a canonical ordering of the links in a dependency
tree. Each generation step corresponds to the con-
struction of a dependency link in the canonical
order.
Given two dependency links l and l&amp;apos; with the
heads being h and h&amp;apos; and the modifiers being m
and m&amp;apos;, respectively, the order between l and l&amp;apos; are
determined as follows:
If h = h&amp;apos; and there is a directed path from one
(say h) to the other (say h), then l precedes l.
If h = h&amp;apos; and there does not exist a directed path
between h and h, the order between l and l is
determined by the order of h and h in the sen-
tence (h precedes h l precedes l).
If h = h&amp;apos; and the modifiers m and m are on dif-
ferent sides of h, the link with modifier on the
right precedes the other.
If h = h&amp;apos; and the modifiers m and m are on the
same side of the head h, the link with its modi-
fier closer to h precedes the other one.
</bodyText>
<figureCaption confidence="0.981839">
Figure 1. An Example Dependency Tree.
</figureCaption>
<page confidence="0.999625">
153
</page>
<bodyText confidence="0.996024">
\x0cFor example, the canonical order of the links in
the dependency tree in Figure 1 is: (1, 2, L), (5, 6,
R), (8, 9, L), (7, 9, R), (5, 7, R), (4, 5, R), (3, 4,
R), (2, 3, L), (0, 3, L).
The generation process according to the ca-
nonical order is similar to the head outward gen-
eration process in (Collins, 1999), except that it is
bottom-up whereas Collins models are top-down.
Suppose the dependency tree T is constructed in
steps G1, ..., GN in the canonical order of the de-
pendency links, where N is the number of words
in the sentence. We can compute the probability
of T as follows:
</bodyText>
<equation confidence="0.678869382352941">
( )
( )
( )
=
=
=
N
i i
i
N
G
G
S
G
P
S
G
G
G
P
S
T
P
1 1
1
2
1
,...,
,
|
|
,...,
,
|
</equation>
<bodyText confidence="0.9859888">
Following (Klein and Manning, 2004), we re-
quire that the creation of a dependency link from
head h to modifier m be preceded by placing a left
STOP and a right STOP around the modifier m
and STOP between h and m.
</bodyText>
<equation confidence="0.85160225">
Let L
w
E (and R
w
</equation>
<bodyText confidence="0.9782284">
E ) denote the event that there
are no more modifiers on the left (and right) of a
word w. Suppose the dependency link created in
the step i is (u, v, d). If d = L, Gi is the conjunc-
tion of the four events: R
</bodyText>
<equation confidence="0.572999333333333">
u
E , L
u
E , L
v
E
and
linkL(u, v). If d = R, Gi consists of four events:
L
v
E , R
v
E , R
u
E
</equation>
<bodyText confidence="0.8309675">
and linkR(u, v).
The event Gi is conditioned on 1
</bodyText>
<equation confidence="0.911717833333333">
1,...,
,
i
G
G
S ,
</equation>
<bodyText confidence="0.963206">
which are the words in the sentence and a forest of
trees constructed up to step i-1. Let L
</bodyText>
<equation confidence="0.9744015">
w
C (and R
w
C )
</equation>
<bodyText confidence="0.944290571428571">
be the number of modifiers of w on its left (and
right). We make the following independence as-
sumptions:
Whether there is any more modifier of w on
the d side depends only on the number of
modifiers already found on the d side of w.
That is, d
</bodyText>
<equation confidence="0.98349825">
w
E depends only on w and d
w
C .
</equation>
<bodyText confidence="0.9436688">
Whether there is a dependency link from a
word h to another word m depends only on the
words h and m and the number of modifiers of
h between m and h. That is,
o linkR(u,v) depends only on u, v, and R
</bodyText>
<equation confidence="0.794079">
u
C .
o linkL(u,v) depends only on u, v, and L
v
C .
</equation>
<bodyText confidence="0.826522">
Suppose Gi corresponds to a dependency link (u,
v, L). The probability ( )
</bodyText>
<equation confidence="0.9417695">
1
1,...,
,
|
i
i G
G
S
G
P can be
computed as:
( )
( )
( )
( ) ( )
( )
( ) ( )
( )
</equation>
<figure confidence="0.963718673913044">
L
v
L
L
v
L
v
R
u
R
u
L
u
L
u
i
L
L
v
R
u
L
u
i
i
C
v
u
v
u
link
P
C
v
E
P
C
u
E
P
C
u
E
P
G
G
S
v
u
link
E
E
E
P
G
G
S
G
P
,
,
|
,
,
|
1
,
|
,
|
,...,
,
|
,
,
,
,
,...,
,
|
1
1
1
1
=
=
The events R
w
E and L
w
E correspond to the
STOP events in (Collins, 1999) and (Klein and
</figure>
<bodyText confidence="0.980658">
Manning, 2004). They are crucial for modeling
the number of dependents. Without them, the
parse trees often contain some obvious errors,
such as determiners taking arguments, or preposi-
tions having arguments on their left (instead of
right).
Our model requires three types of parameters:
</bodyText>
<equation confidence="0.921058375">
( )
d
w
d
w C
w
E
P ,
</equation>
<bodyText confidence="0.863858333333333">
 |, where w is a word, d is a di-
rection (left or right). This is the probability of
a STOP after taking d
</bodyText>
<figure confidence="0.947037024390244">
w
C modifiers on the d
side.
( )
( )
R
u
R C
v
u
v
u
link
P ,
,
|
, is the probability of v
being the ( 1
+
R
u
C )th modifier of u on the
right.
( )
( )
L
v
L C
v
u
v
u
link
P ,
,
|
, is the probability of u
being the ( 1
+
L
v
</figure>
<bodyText confidence="0.952626">
C )th modifier of v on the
left.
The Maximum Likelihood estimations of these
parameters can be obtained from the frequency
counts in the training corpus:
C(w, c, d): the frequency count of w with c
modifiers on the d side.
C(u, v, c, d): If d = L, this is the frequency
count words u and v co-occurring in a sen-
tence and v has c modifiers between itself and
u. If d = R, this is the frequency count words u
and v co-occurring in a sentence and u has c
modifiers between itself and v.
K(u, v, c, d): similar to C(u, v, c, d) with an
additional constraint that linkd(u, v) is true.
</bodyText>
<page confidence="0.997811">
154
</page>
<figure confidence="0.877704783505155">
\x0c( ) ( )
( )
=
c
c
d
w
d
w
d
c
w
C
d
c
w
C
C
w
E
P
&amp;apos;
,
&amp;apos;
,
,
,
,
 |, where c = d
w
C ;
( )
( ) ( )
( )
R
c
v
u
C
R
c
v
u
K
C
v
u
v
u
link
P R
u
R
,
,
,
,
,
,
,
,
|
, = ,
where c = R
u
C ;
( )
( ) ( )
( )
L
c
v
u
C
L
c
v
u
K
C
v
u
v
u
link
P L
v
L
,
,
,
,
,
,
,
,
|
</figure>
<equation confidence="0.975554">
, = ,
where c = L
v
C .
</equation>
<bodyText confidence="0.9995628">
We compute the probability of the tree condi-
tioned on the words. All parameters in our model
are conditional probabilities where the left sides of
the conditioning bar are binary variables. In con-
trast, most previous approaches compute joint
probability of the tree and the words in the tree.
Many of their model parameters consist of the
probability of a word in a given context.
We use a dynamic programming algorithm
similar to chart parsing as the decoder for this
model. The algorithm builds a packed parse forest
from bottom up in the canonical order of the
parser trees. It attaches all the right children be-
fore attaching the left ones to maintain the canoni-
cal order as required by our model.
</bodyText>
<sectionHeader confidence="0.980389" genericHeader="method">
3 Similarity-based Smoothing
</sectionHeader>
<subsectionHeader confidence="0.993725">
3.1 Distributional Word Similarity
</subsectionHeader>
<bodyText confidence="0.996295346153846">
Words that tend to appear in the same contexts
tend to have similar meanings. This is known as
the Distributional Hypothesis in linguistics (Harris,
1968). For example, the words test and exam are
similar because both of them follow verbs such as
administer, cancel, cheat on, conduct, ... and both of
them can be preceded by adjectives such as aca-
demic, comprehensive, diagnostic, difficult, ...
Many methods have been proposed to compute
distributional similarity between words (Hindle,
1990; Pereira et al., 1993; Grefenstette, 1994; Lin,
1998). Almost all of the methods represent a word
by a feature vector where each feature corre-
sponds to a type of context in which the word ap-
peared. They differ in how the feature vectors are
constructed and how the similarity between two
feature vectors is computed.
We define the features of a word w to be the set
of words that occurred within a small context win-
dow of w in a large corpus. The context window
of an instance of w consists of the closest non-
stop-word on each side of w and the stop-words in
between. In our experiments, the set of stop-words
are defined as the top 100 most frequent words in
the corpus. The value of a feature w&amp;apos; is defined as
the point-wise mutual information between the w&amp;apos;
</bodyText>
<equation confidence="0.98068375">
and w:
( ) ( )
( ) ( )
=
&amp;apos;
&amp;apos;
,
log
&amp;apos;
,
w
P
w
P
w
w
P
w
w
PMI
</equation>
<bodyText confidence="0.9893352">
where P(w, w) is the probability of w and w co-
occur in a context window.
The similarity between two vectors is computed
as the cosine of the angle between the vectors.
The following are the top similar words for the
</bodyText>
<table confidence="0.865501444444445">
word keystone obtained from the English Giga-
word Corpus:
centrepiece 0.28, figment 0.27, fulcrum 0.21, culmi-
nation 0.20, albatross 0.19, bane 0.19, pariahs 0.18,
lifeblood 0.18, crux 0.18, redoubling 0.17, apotheo-
sis 0.17, cornerstones 0.17, perpetuation 0.16, fore-
runners 0.16, shirking 0.16, cornerstone 0.16,
birthright 0.15, hallmark 0.15, centerpiece 0.15, evi-
denced 0.15, germane 0.15, gist 0.14, reassessing
0.14, engrossed 0.14, Thorn 0.14, biding 0.14, nar-
rowness 0.14, linchpin 0.14, enamored 0.14, formal-
ised 0.14, tenths 0.13, testament 0.13, certainties
0.13, forerunner 0.13, re-evaluating 0.13, antithetical
0.12, extinct 0.12, rarest 0.12, imperiled 0.12, remiss
0.12, hindrance 0.12, detriment 0.12, prouder 0.12,
upshot 0.12, cosponsor 0.12, hiccups 0.12, premised
0.12, perversion 0.12, destabilisation 0.12, prefaced
0.11, ......
</table>
<subsectionHeader confidence="0.985599">
3.2 Similarity-based Smoothing
</subsectionHeader>
<bodyText confidence="0.876183666666667">
The parameters in our model consist of condi-
tional probabilities P(E|C) where E is the binary
variable linkd(u, v) or d
</bodyText>
<figure confidence="0.727886">
w
E and the context C is
either [ ]
d
w
C
w, or [ ]
d
w
C
v
u ,
, , which involves one
</figure>
<bodyText confidence="0.9992565">
or two words in the input sentence. Due to the
sparseness of natural language data, the contexts
observed in the training data only covers a tiny
fraction of the contexts whose probability distri-
bution are needed during parsing. The standard
approach is to back off the probability to word
classes (such as part-of-speech tags). We have
taken a different approach. We search in the train-
</bodyText>
<page confidence="0.995058">
155
</page>
<bodyText confidence="0.986359714285714">
\x0cing data to find a set of similar contexts to C and
estimate the probability of E based on its prob-
abilities in the similar contexts that are observed
in the training corpus.
Similarity-based smoothing was used in (Dagan
et al., 1999) to estimate word co-occurrence prob-
abilities. Their method performed almost 40%
better than the more commonly used back-off
method. Unfortunately, similarity-based smooth-
ing has not been successfully applied to statistical
parsing up to now.
In (Dagan et al., 1999), the bigram probability
P(w2|w1) is computed as the weighted average of
the conditional probability of w2 given similar
</bodyText>
<figure confidence="0.86478964">
words of w1.
( ) ( )
( )
( )
( )
=
1
1
&amp;apos;
1
2
1
1
1
1
2 &amp;apos;
|
&amp;apos;
,
|
w
S
w
MLE
SIM w
</figure>
<equation confidence="0.914573866666667">
w
P
w
norm
w
w
sim
w
w
P
where ( )
1
1 &amp;apos;
,w
w
</equation>
<bodyText confidence="0.9622095">
sim denotes the similarity (or an
increasing function of the similarity) between w1
and w1, S(w1) denote the set of words that are
most similar to w1 and norm(w1) is the normaliza-
</bodyText>
<figure confidence="0.784285111111111">
tion factor ( ) ( )
( )
=
1
1
&amp;apos;
1
1
1 &amp;apos;
,
w
S
w
w
w
sim
w
norm .
</figure>
<bodyText confidence="0.913921142857143">
The underlying assumption of this smoothing
scheme is that a word is more likely to occur after
w1 if it tends to occur after similar words of w1.
We make a similar assumption: the probability
P(E|C) of event E given the context C is computed
as the weight average of P(E|C) where C is a
similar context of C and is attested in the training
</bodyText>
<figure confidence="0.815425071428571">
corpus:
( ) ( )
( )
( )
( )
=
O
C
S
C
MLE
SIM C
E
P
C
norm
C
C
sim
C
E
P
&amp;apos;
&amp;apos;
|
&amp;apos;
,
|
</figure>
<bodyText confidence="0.981424">
where S(C) is the set of top-K most similar con-
texts of C (in the experiments reported in this pa-
per, K = 50); O is the set of contexts observed in
the training corpus, sim(C,C) is the similarity
between two contexts and norm(C) is the nor-
malization factor.
In our model, a context is either [ ]
</bodyText>
<equation confidence="0.929389681818182">
d
w
C
w, or
[ ]
d
w
C
v
u ,
, . Their similar contexts are defined as:
[ ]
( ) [ ] ( )
{ }
[ ]
( ) [ ]
{ }
)
(
&amp;apos;
),
(
</equation>
<figure confidence="0.919394285714286">
&amp;apos;
,
&amp;apos;
,
&amp;apos;
,
,
&amp;apos;
,
&amp;apos;
, &amp;apos;
v
S
v
u
S
u
C
v
u
C
v
u
S
w
S
w
C
w
C
w
S
d
w
d
w
d
w
d
w
=
=
</figure>
<bodyText confidence="0.818793428571428">
where S(w) is the set of top-K similar words of w
(K = 50).
Since all contexts used in our model contain at
least one word, we compute the similarity be-
tween two contexts, sim(C, C), as the geometric
average of the similarities between corresponding
words:
</bodyText>
<figure confidence="0.962846236363636">
[ ][ ]
( ) ( )
[ ][ ]
( ) ( ) ( )
&amp;apos;
,
&amp;apos;
,
,
&amp;apos;
,
&amp;apos;
,
,
,
&amp;apos;
,
,
&amp;apos;
,
,
&amp;apos;
&amp;apos;
v
v
sim
u
u
sim
C
v
u
C
v
u
sim
w
w
sim
C
w
C
w
sim
d
w
d
w
d
w
d
w
=
=
Similarity-smoothed probability is only neces-
</figure>
<bodyText confidence="0.987145">
sary when the frequency count of the context C in
the training corpus is low. We therefore compute
</bodyText>
<equation confidence="0.992198">
P(E  |C) = PMLE(E  |C) + (1 ) PSIM(E  |C)
</equation>
<bodyText confidence="0.909876">
where the smoothing factor
</bodyText>
<figure confidence="0.962168909090909">
5
|
|
1
|
|
+
+
=
C
C
</figure>
<figureCaption confidence="0.448952">
and |C |is
the frequency count of the context C in the train-
ing data.
</figureCaption>
<bodyText confidence="0.994802428571428">
A difference between similarity-based smooth-
ing in (Dagan et al., 1999) and our approach is
that our model only computes probability distribu-
tions of binary variables. Words only appear as
parts of contexts on the right side of the condition-
ing bar. This has two important implications.
Firstly, when a context contains two words, we
are able to use the cross product of the similar
words, whereas (Dagan et al., 1999) can only use
the similar words of one of the words. This turns
out to have significant impact on the performance
(see Section 4).
Secondly, in (Dagan et al., 1999), the distribu-
tion P(|w1) may itself be sparsely observed.
</bodyText>
<figure confidence="0.9107378">
When ( )
1
2 &amp;apos;
 |w
w
</figure>
<bodyText confidence="0.9588951">
PMLE is 0, it is often due to data
sparseness. Their smoothing scheme therefore
tends to under-estimate the probability values.
This problem is avoided in our approach. If a con-
text did not occur in the training data, we do not
include it in the average. If it did occur, the
Maximum Likelihood estimation is reasonably
accurate even if the context only occurred a few
times, since the entropy of the probability distri-
bution is upper-bounded by log 2.
</bodyText>
<sectionHeader confidence="0.997878" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999669333333333">
We experimented with our parser on the Chinese
Treebank (CTB) 3.0. We used the same data split
as (Bikel, 2004): Sections 1-270 and 400-931 as
</bodyText>
<page confidence="0.997795">
156
</page>
<bodyText confidence="0.998918952380952">
\x0cthe training set, Sections 271-300 as testing and
Sections 301-325 as the development set. The
CTB contains constituency trees. We converted
them to dependency trees using the same method
and the head table as (Bikel, 2004). Parsing Chi-
nese generally involve segmentation as a pre-
processing step. We used the gold standard seg-
mentation in the CTB.
The distributional similarities between the Chi-
nese words are computed using the Chinese Gi-
gaword corpus. We did not segment the Chinese
corpus when computing the word similarity.
We measure the quality of the parser by the un-
directed accuracy, which is defined as the number
of correct undirected dependency links divided by
the total number of dependency links in the corpus
(the treebank parse and the parser output always
have the same number of links). The results are
summarized in Table 1. It can be seen that the per-
formance of the parser is highly correlated with
the length of the sentences.
</bodyText>
<table confidence="0.9899855">
Max Sentence Length 10 15 20 40
Undirected Accuracy 90.8 85.6 84.0 79.9
</table>
<tableCaption confidence="0.999667">
Table 1. Evaluation Results on CTB 3.0
</tableCaption>
<bodyText confidence="0.99679475">
We also experimented with several alternative
models for dependency parsing. Table 2 summer-
izes the results of these models on the test corpus
with sentences up to 40 words long.
One of the characteristics of our parser is that it
uses the similar words of both the head and the
modifier for smoothing. The similarity-based
smoothing method in (Dagan et al., 1999) uses the
similar words of one of the words in a bigram. We
can change the definition of similar context as
follows so that only one word in a similar context
of C may be different from a word in C (see
</bodyText>
<figure confidence="0.872716093023256">
Model (b) in Table 2):
[ ]
( )
[ ]
{ } [ ]
{ }
)
(
&amp;apos;
,
&amp;apos;
,
)
(
&amp;apos;
,
,
&amp;apos;
,
,
v
S
v
C
v
u
u
S
u
C
v
u
C
v
u
S
d
w
d
w
d
w
=
</figure>
<bodyText confidence="0.999008333333333">
where w is either v or u depending on whether d is
L or R. This change led to a 2.2% drop in accuracy
(compared with Model (a) in Table 2), which we
attribute to the fact that many contexts do not have
similar contexts in the training corpus.
Since most previous parsing models maximize
the joint probability of the parse tree and the sen-
tence P(T, S) instead of P(T  |S), we also imple-
mented a joint model (see Model (c) in Table 2):
</bodyText>
<equation confidence="0.9826296">
( )
( ) ( )
( )
( ) ( )
=
=
N
i
d
h
i
i
d
h
i
d
h
R
m
i
R
m
L
m
i
L
m
i
i
i
i
i
i
i
i
</equation>
<figure confidence="0.966648517241379">
C
h
m
P
C
h
E
P
C
m
E
P
C
m
E
P
S
T
P
1 ,
|
,
|
1
,
|
,
|
,
</figure>
<bodyText confidence="0.9869255">
where hi and mi are the head and the modifier of
the i&amp;apos;th dependency link. The probability
</bodyText>
<equation confidence="0.991993318181818">
( )
i
i
d
h
i
i C
h
m
P ,
 |is smoothed by averaging the
probabilities ( )
i
i
d
h
i
i C
h
m
P ,
&amp;apos;
</equation>
<bodyText confidence="0.997594434782608">
 |, where hi is a similar
word of hi, as in (Dagan et al., 1999). The result
was a dramatic decrease in accuracy from the con-
ditional models 79.9%. to 66.3%.
Our use of distributional word similarity can
be viewed as assigning soft clusters to words. In
contrast, parts-of-speech can be viewed as hard
clusters of words. We can modify both the condi-
tional and joint models to use part-of-speech tags,
instead of words. Since there are only a small
number of tags, the modified models used MLE
without any smoothing except using a small con-
stant as the probability of unseen events. Without
smoothing, maximizing the conditional model is
equivalent to maximizing the joint model. The
accuracy of the unlexicalized models (see Model
(d) and Model (e) in Table 2) is 71.1% which is
considerably lower than the strictly lexicalized
conditional model, but higher than the strictly
lexicalized joint model. This demonstrated that
soft clusters obtained through distributional word
similarity perform better than the part-of-speech
tags when used appropriately.
</bodyText>
<figure confidence="0.9826448">
Models Accuracy
(a)
Strictly lexicalized conditional
model
79.9
(b)
At most one word is different in
a similar context
77.7
(c) Strictly lexicalized joint model 66.3
(d)
Unlexicalized conditional mod-
els
71.1
(e) Unlexicalized joint models 71.1
</figure>
<tableCaption confidence="0.979024">
Table 2. Performance of Alternative Models
</tableCaption>
<page confidence="0.99676">
157
</page>
<sectionHeader confidence="0.974017" genericHeader="method">
\x0c5 Related Work
</sectionHeader>
<bodyText confidence="0.999727735294118">
Previous parsing models (e.g., Collins, 1997;
Charniak, 2000) maximize the joint probability
P(S, T) of a sentence S and its parse tree T. We
maximize the conditional probability P(T  |S). Al-
though they are theoretically equivalent, the use of
conditional model allows us to take advantage of
similarity-based smoothing.
Clark et al. (2002) also computes a conditional
probability of dependency structures. While the
probability space in our model consists of all pos-
sible non-projective dependency trees, their prob-
ability space is constrained to all the dependency
structures that are allowed by a Combinatorial
Category Grammar (CCG) and a category diction-
ary (lexicon). They therefore do not need the
STOP markers in their model. Another major dif-
ference between our model and (Clark et al.,
2002) is that the parameters in our model consist
exclusively of conditional probabilities of binary
variables.
Ratnaparkhis maximum entropy model (Rat-
naparkhi, 1999) is also a conditional model. How-
ever, his model maximizes the probability of the
action during each step of the parsing process,
instead of overall quality of the parse tree.
Yamada and Matsumoto (2002) presented a de-
pendency parsing model using support vector ma-
chines. Their model is a discriminative model that
maximizes the differences between scores of the
correct parse and the scores of the top competing
incorrect parses.
In many dependency parsing models such as
(Eisner, 1996) and (MacDonald et al., 2005), the
score of a dependency tree is the sum of the scores
of the dependency links, which are computed in-
dependently of other links. An undesirable conse-
quence of this is that the parser often creates
multiple dependency links that are separately
likely but jointly improbable (or even impossible).
For example, there is nothing in such models to
prevent the parser from assigning two subjects to
a verb. In the DMV model (Klein and Manning,
2004), the probability of a dependency link is
partly conditioned on whether or not there is a
head word of the link already has a modifier. Our
model is quite similar to the DMV model, except
that we compute the conditional probability of the
parse tree given the sentence, instead of the joint
probability of the parse tree and the sentence.
There have been several previous approaches to
parsing Chinese with the Penn Chinese Treebank
(e.g., Bikel and Chiang, 2000; Levy and Manning,
2003). Both of these approaches employed phrase-
structure joint models and used part-of-speech
tags in back-off smoothing. Their results were
evaluated with the precision and recall of the
bracketings implied in the phrase structure parse
trees. In contrast, the accuracy of our model is
measured in terms of the dependency relation-
ships. A dependency tree may correspond to more
than one constituency trees. Our results are there-
fore not directly comparable with the precision
and recall values in previous research. Moreover,
it was argued in (Lin 1995) that dependency based
evaluation is much more meaningful for the appli-
cations that use parse trees, since the semantic
relationships are generally embedded in the de-
pendency relationships.
</bodyText>
<sectionHeader confidence="0.99873" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99984">
To the best of our knowledge, all previous natural
language parsers have to rely on part-of-speech
tags. We presented a strictly lexicalized model for
dependency parsing that only relies on word sta-
tistics. We compared our parser with an unlexical-
ized parser that employs the same probabilistic
model except that the parameters are estimated
using gold standard tags in the Chinese Treebank.
Our experiments show that the strictly lexicalized
parser significantly outperformed its unlexicalized
counter-part.
An important distinction between our statistical
model from previous parsing models is that all the
parameters in our model are conditional probabil-
ity of binary variables. This allows us to take ad-
vantage of similarity-based smoothing, which has
not been successfully applied to parsing before.
</bodyText>
<sectionHeader confidence="0.962611" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.990961666666667">
The authors would like to thank Mark Steedman
for suggesting the comparison with unlexicalized
parsing in Section 4 and the anonymous reviewers
for their comments. This work was supported in
part by NSERC, the Alberta Ingenuity Centre for
Machine Learning and the Canada Research
</bodyText>
<page confidence="0.998487">
158
</page>
<reference confidence="0.908466">
\x0cChairs program. Qin Iris Wang was also sup-
ported by iCORE Scholarship.
</reference>
<sectionHeader confidence="0.576665" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999780974025974">
Daniel M. Bikel. 2004. Intricacies of Collins Parsing
Model. Computational Linguistics, 30(4), pp. 479-
511.
Daniel M. Bikel and David Chiang. 2000. Two Statisti-
cal Parsing Models applied to the Chinese Treebank.
In Proceedings of the second Chinese Language
Processing Workshop, pp. 1-6.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of the Second Meeting of
North American Chapter of Association for Compu-
tational Linguistics (NAACL-2000), pp. 132-139.
Stephen Clark, Julia Hockenmaier and Mark Steedman.
2002. Building Deep Dependency Structures with a
Wide-Coverage CCG Parser. In Proceedings of the
40th Annual Meeting of the ACL, pp. 327-334.
Michael Collins. 1996. A New Statistical Parser Based
on Bigram Lexical Dependencies. In Proceedings of
the 34th Annual Meeting of the ACL, pp. 184-191.
Santa Cruz.
Michael Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the ACL (jointly with the 8th
Conference of the EACL), pp. 16-23. Madrid.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. PhD Dissertation,
University of Pennsylvania.
Ido Dagan, Lillian Lee and Fernando Pereira. 1999.
Similarity-based models of cooccurrence probabili-
ties. Machine Learning, Vol. 34(1-3) special issue
on Natural Language Learning, pp. 43-69.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of COLING-96, pp. 340-345, Copenhagen.
Daniel Gildea. 2001. Corpus Variation and Parser Per-
formance. In Proceedings of EMNLP-2001, pp. 167-
202. Pittsburgh, PA.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Press, Bos-
ton, MA.
Zelig S. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
Donald Hindle. 1990. Noun Classification from Predi-
cate-Argument Structures. In Proceedings of ACL-
90, pp. 268-275. Pittsburg, Pennsylvania.
Dan Klein and Chris Manning. 2002. Fast exact infer-
ence with a factored model for natural language
parsing. In Proceedings of Neural Information
Processing Systems.
Dan Klein and Chris Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the 41st Annual
Meeting of the ACL, pp. 423-430.
Dan Klein and Chris Manning. 2004. Corpus-Based
Induction of Syntactic Structure: Models of De-
pendency and Constituency. In Proceedings of the
42nd Annual Meeting of the ACL, pp. 479-486.
Roger Levy and Chris Manning. 2003. Is it harder to
parse Chinese, or the Chinese Treebank? In Pro-
ceedings of the 41st Annual Meeting of the ACL, pp.
439-446.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of IJCAI-95, pp.1420-1425.
Dekang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceeding of COLING-
ACL98, pp. 768-774. Montreal, Canada.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL-2005, pp.
91-98.
Fernando Pereira, Naftali Z. Tishby, and Lillian Lee.
1993. Distributional clustering of English words. In
Proceedings of ACL-1993, pp. 183-190, Columbus,
Ohio.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal Dependency Analysis with Support Vector Ma-
chines. In Proceedings of the 8th International
Workshop on Parsing Technologies, pp.195-206.
</reference>
<page confidence="0.990303">
159
</page>
<figure confidence="0.24722">
\x0c&amp;quot;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.679680">
<note confidence="0.952521333333333">b&amp;quot;Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 152159, Vancouver, October 2005. c 2005 Association for Computational Linguistics</note>
<title confidence="0.973453">Strictly Lexical Dependency Parsing</title>
<author confidence="0.9815">Qin Iris Wang</author>
<author confidence="0.9815">Dale Schuurmans Dekang Lin</author>
<affiliation confidence="0.911663">Department of Computing Science Google, Inc. University of Alberta 1600 Amphitheatre Parkway</affiliation>
<address confidence="0.999949">Edmonton, Alberta, Canada, T6G 2E8 Mountain View, California, USA, 94043</address>
<email confidence="0.99458">wqin@cs.ualberta.calindek@google.com</email>
<email confidence="0.99458">dale@cs.ualberta.calindek@google.com</email>
<abstract confidence="0.999505826086957">We present a strictly lexical parsing model where all the parameters are based on the words. This model does not rely on part-of-speech tags or grammatical categories. It maximizes the conditional probability of the parse tree given the sentence. This is in contrast with most previous models that compute the joint probability of the parse tree and the sentence. Although the maximization of joint and conditional probabilities are theoretically equivalent, the conditional model allows us to use distributional word similarity to generalize the observed frequency counts in the training corpus. Our experiments with the Chinese Treebank show that the accuracy of the conditional model is 13.6% higher than the joint model and that the strictly lexicalized conditional model outperforms the corresponding unlexicalized model based on part-of-speech tags.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins Parsing Model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>479--511</pages>
<contexts>
<context position="1874" citStr="Bikel (2004)" startWordPosition="271" endWordPosition="272"> corresponding unlexicalized model based on part-of-speech tags. 1 Introduction There has been a great deal of progress in statistical parsing in the past decade (Collins, 1996; Collins, 1997; Chaniak, 2000). A common characteristic of these parsers is their use of lexicalized statistics. However, it was discovered recently that bi-lexical statistics (parameters that involve two words) actually played much smaller role than previously believed. It was found in (Gildea, 2001) that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output. Bikel (2004) observed that the bi-lexical statistics accounted for only 1.49% of the bigram statistics used by the parser. When considering only bigram statistics involved in the highest probability parse, this percentage becomes 28.8%. However, even when the bi-lexical statistics do get used, they are remarkably similar to their back-off values using part-of-speech tags. Therefore, the utility of bi-lexical statistics becomes rather questionable. Klein and Manning (2003) presented an unlexicalized parser that eliminated all lexicalized parameters. Its performance was close to the state-of-the-art lexical</context>
<context position="18691" citStr="Bikel, 2004" startWordPosition="3710" endWordPosition="3711">ved. When ( ) 1 2 &amp;apos; |w w PMLE is 0, it is often due to data sparseness. Their smoothing scheme therefore tends to under-estimate the probability values. This problem is avoided in our approach. If a context did not occur in the training data, we do not include it in the average. If it did occur, the Maximum Likelihood estimation is reasonably accurate even if the context only occurred a few times, since the entropy of the probability distribution is upper-bounded by log 2. 4 Experimental Results We experimented with our parser on the Chinese Treebank (CTB) 3.0. We used the same data split as (Bikel, 2004): Sections 1-270 and 400-931 as 156 \x0cthe training set, Sections 271-300 as testing and Sections 301-325 as the development set. The CTB contains constituency trees. We converted them to dependency trees using the same method and the head table as (Bikel, 2004). Parsing Chinese generally involve segmentation as a preprocessing step. We used the gold standard segmentation in the CTB. The distributional similarities between the Chinese words are computed using the Chinese Gigaword corpus. We did not segment the Chinese corpus when computing the word similarity. We measure the quality of the pa</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of Collins Parsing Model. Computational Linguistics, 30(4), pp. 479-511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>David Chiang</author>
</authors>
<title>Two Statistical Parsing Models applied to the Chinese Treebank.</title>
<date>2000</date>
<booktitle>In Proceedings of the second Chinese Language Processing Workshop,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="24998" citStr="Bikel and Chiang, 2000" startWordPosition="4863" endWordPosition="4866">mpossible). For example, there is nothing in such models to prevent the parser from assigning two subjects to a verb. In the DMV model (Klein and Manning, 2004), the probability of a dependency link is partly conditioned on whether or not there is a head word of the link already has a modifier. Our model is quite similar to the DMV model, except that we compute the conditional probability of the parse tree given the sentence, instead of the joint probability of the parse tree and the sentence. There have been several previous approaches to parsing Chinese with the Penn Chinese Treebank (e.g., Bikel and Chiang, 2000; Levy and Manning, 2003). Both of these approaches employed phrasestructure joint models and used part-of-speech tags in back-off smoothing. Their results were evaluated with the precision and recall of the bracketings implied in the phrase structure parse trees. In contrast, the accuracy of our model is measured in terms of the dependency relationships. A dependency tree may correspond to more than one constituency trees. Our results are therefore not directly comparable with the precision and recall values in previous research. Moreover, it was argued in (Lin 1995) that dependency based eva</context>
</contexts>
<marker>Bikel, Chiang, 2000</marker>
<rawString>Daniel M. Bikel and David Chiang. 2000. Two Statistical Parsing Models applied to the Chinese Treebank. In Proceedings of the second Chinese Language Processing Workshop, pp. 1-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A Maximum-Entropy-Inspired Parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second Meeting of North American Chapter of Association for Computational Linguistics (NAACL-2000),</booktitle>
<pages>132--139</pages>
<contexts>
<context position="22674" citStr="Charniak, 2000" startWordPosition="4487" endWordPosition="4488">ctly lexicalized conditional model, but higher than the strictly lexicalized joint model. This demonstrated that soft clusters obtained through distributional word similarity perform better than the part-of-speech tags when used appropriately. Models Accuracy (a) Strictly lexicalized conditional model 79.9 (b) At most one word is different in a similar context 77.7 (c) Strictly lexicalized joint model 66.3 (d) Unlexicalized conditional models 71.1 (e) Unlexicalized joint models 71.1 Table 2. Performance of Alternative Models 157 \x0c5 Related Work Previous parsing models (e.g., Collins, 1997; Charniak, 2000) maximize the joint probability P(S, T) of a sentence S and its parse tree T. We maximize the conditional probability P(T |S). Although they are theoretically equivalent, the use of conditional model allows us to take advantage of similarity-based smoothing. Clark et al. (2002) also computes a conditional probability of dependency structures. While the probability space in our model consists of all possible non-projective dependency trees, their probability space is constrained to all the dependency structures that are allowed by a Combinatorial Category Grammar (CCG) and a category dictionary</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser. In Proceedings of the Second Meeting of North American Chapter of Association for Computational Linguistics (NAACL-2000), pp. 132-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Building Deep Dependency Structures with a Wide-Coverage CCG Parser.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>327--334</pages>
<contexts>
<context position="22952" citStr="Clark et al. (2002)" startWordPosition="4530" endWordPosition="4533">exicalized conditional model 79.9 (b) At most one word is different in a similar context 77.7 (c) Strictly lexicalized joint model 66.3 (d) Unlexicalized conditional models 71.1 (e) Unlexicalized joint models 71.1 Table 2. Performance of Alternative Models 157 \x0c5 Related Work Previous parsing models (e.g., Collins, 1997; Charniak, 2000) maximize the joint probability P(S, T) of a sentence S and its parse tree T. We maximize the conditional probability P(T |S). Although they are theoretically equivalent, the use of conditional model allows us to take advantage of similarity-based smoothing. Clark et al. (2002) also computes a conditional probability of dependency structures. While the probability space in our model consists of all possible non-projective dependency trees, their probability space is constrained to all the dependency structures that are allowed by a Combinatorial Category Grammar (CCG) and a category dictionary (lexicon). They therefore do not need the STOP markers in their model. Another major difference between our model and (Clark et al., 2002) is that the parameters in our model consist exclusively of conditional probabilities of binary variables. Ratnaparkhis maximum entropy mod</context>
</contexts>
<marker>Clark, Hockenmaier, Steedman, 2002</marker>
<rawString>Stephen Clark, Julia Hockenmaier and Mark Steedman. 2002. Building Deep Dependency Structures with a Wide-Coverage CCG Parser. In Proceedings of the 40th Annual Meeting of the ACL, pp. 327-334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A New Statistical Parser Based on Bigram Lexical Dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL,</booktitle>
<pages>184--191</pages>
<location>Santa Cruz.</location>
<contexts>
<context position="1438" citStr="Collins, 1996" startWordPosition="207" endWordPosition="208">sentence. Although the maximization of joint and conditional probabilities are theoretically equivalent, the conditional model allows us to use distributional word similarity to generalize the observed frequency counts in the training corpus. Our experiments with the Chinese Treebank show that the accuracy of the conditional model is 13.6% higher than the joint model and that the strictly lexicalized conditional model outperforms the corresponding unlexicalized model based on part-of-speech tags. 1 Introduction There has been a great deal of progress in statistical parsing in the past decade (Collins, 1996; Collins, 1997; Chaniak, 2000). A common characteristic of these parsers is their use of lexicalized statistics. However, it was discovered recently that bi-lexical statistics (parameters that involve two words) actually played much smaller role than previously believed. It was found in (Gildea, 2001) that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output. Bikel (2004) observed that the bi-lexical statistics accounted for only 1.49% of the bigram statistics used by the parser. When considering only bigram statistics involved in t</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A New Statistical Parser Based on Bigram Lexical Dependencies. In Proceedings of the 34th Annual Meeting of the ACL, pp. 184-191. Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three Generative, Lexicalised Models for Statistical Parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL (jointly with the 8th Conference of the EACL),</booktitle>
<pages>16--23</pages>
<location>Madrid.</location>
<contexts>
<context position="1453" citStr="Collins, 1997" startWordPosition="209" endWordPosition="210">ugh the maximization of joint and conditional probabilities are theoretically equivalent, the conditional model allows us to use distributional word similarity to generalize the observed frequency counts in the training corpus. Our experiments with the Chinese Treebank show that the accuracy of the conditional model is 13.6% higher than the joint model and that the strictly lexicalized conditional model outperforms the corresponding unlexicalized model based on part-of-speech tags. 1 Introduction There has been a great deal of progress in statistical parsing in the past decade (Collins, 1996; Collins, 1997; Chaniak, 2000). A common characteristic of these parsers is their use of lexicalized statistics. However, it was discovered recently that bi-lexical statistics (parameters that involve two words) actually played much smaller role than previously believed. It was found in (Gildea, 2001) that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output. Bikel (2004) observed that the bi-lexical statistics accounted for only 1.49% of the bigram statistics used by the parser. When considering only bigram statistics involved in the highest prob</context>
<context position="22657" citStr="Collins, 1997" startWordPosition="4485" endWordPosition="4486">r than the strictly lexicalized conditional model, but higher than the strictly lexicalized joint model. This demonstrated that soft clusters obtained through distributional word similarity perform better than the part-of-speech tags when used appropriately. Models Accuracy (a) Strictly lexicalized conditional model 79.9 (b) At most one word is different in a similar context 77.7 (c) Strictly lexicalized joint model 66.3 (d) Unlexicalized conditional models 71.1 (e) Unlexicalized joint models 71.1 Table 2. Performance of Alternative Models 157 \x0c5 Related Work Previous parsing models (e.g., Collins, 1997; Charniak, 2000) maximize the joint probability P(S, T) of a sentence S and its parse tree T. We maximize the conditional probability P(T |S). Although they are theoretically equivalent, the use of conditional model allows us to take advantage of similarity-based smoothing. Clark et al. (2002) also computes a conditional probability of dependency structures. While the probability space in our model consists of all possible non-projective dependency trees, their probability space is constrained to all the dependency structures that are allowed by a Combinatorial Category Grammar (CCG) and a ca</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three Generative, Lexicalised Models for Statistical Parsing. In Proceedings of the 35th Annual Meeting of the ACL (jointly with the 8th Conference of the EACL), pp. 16-23. Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>PhD</tech>
<institution>Dissertation, University of Pennsylvania.</institution>
<contexts>
<context position="7476" citStr="Collins, 1999" startWordPosition="1270" endWordPosition="1271">l). If h = h&amp;apos; and the modifiers m and m are on different sides of h, the link with modifier on the right precedes the other. If h = h&amp;apos; and the modifiers m and m are on the same side of the head h, the link with its modifier closer to h precedes the other one. Figure 1. An Example Dependency Tree. 153 \x0cFor example, the canonical order of the links in the dependency tree in Figure 1 is: (1, 2, L), (5, 6, R), (8, 9, L), (7, 9, R), (5, 7, R), (4, 5, R), (3, 4, R), (2, 3, L), (0, 3, L). The generation process according to the canonical order is similar to the head outward generation process in (Collins, 1999), except that it is bottom-up whereas Collins models are top-down. Suppose the dependency tree T is constructed in steps G1, ..., GN in the canonical order of the dependency links, where N is the number of words in the sentence. We can compute the probability of T as follows: ( ) ( ) ( ) = = = N i i i N G G S G P S G G G P S T P 1 1 1 2 1 ,..., , | | ,..., , | Following (Klein and Manning, 2004), we require that the creation of a dependency link from head h to modifier m be preceded by placing a left STOP and a right STOP around the modifier m and STOP between h and m. Let L w E (and R w E ) d</context>
<context position="9506" citStr="Collins, 1999" startWordPosition="1800" endWordPosition="1801"> word m depends only on the words h and m and the number of modifiers of h between m and h. That is, o linkR(u,v) depends only on u, v, and R u C . o linkL(u,v) depends only on u, v, and L v C . Suppose Gi corresponds to a dependency link (u, v, L). The probability ( ) 1 1,..., , | i i G G S G P can be computed as: ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) L v L L v L v R u R u L u L u i L L v R u L u i i C v u v u link P C v E P C u E P C u E P G G S v u link E E E P G G S G P , , | , , | 1 , | , | ,..., , | , , , , ,..., , | 1 1 1 1 = = The events R w E and L w E correspond to the STOP events in (Collins, 1999) and (Klein and Manning, 2004). They are crucial for modeling the number of dependents. Without them, the parse trees often contain some obvious errors, such as determiners taking arguments, or prepositions having arguments on their left (instead of right). Our model requires three types of parameters: ( ) d w d w C w E P , |, where w is a word, d is a direction (left or right). This is the probability of a STOP after taking d w C modifiers on the d side. ( ) ( ) R u R C v u v u link P , , | , is the probability of v being the ( 1 + R u C )th modifier of u on the right. ( ) ( ) L v L C v u v u</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. PhD Dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Similarity-based models of cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning, Vol. 34(1-3) special issue on Natural Language Learning,</booktitle>
<pages>43--69</pages>
<contexts>
<context position="15056" citStr="Dagan et al., 1999" startWordPosition="2900" endWordPosition="2903">ds in the input sentence. Due to the sparseness of natural language data, the contexts observed in the training data only covers a tiny fraction of the contexts whose probability distribution are needed during parsing. The standard approach is to back off the probability to word classes (such as part-of-speech tags). We have taken a different approach. We search in the train155 \x0cing data to find a set of similar contexts to C and estimate the probability of E based on its probabilities in the similar contexts that are observed in the training corpus. Similarity-based smoothing was used in (Dagan et al., 1999) to estimate word co-occurrence probabilities. Their method performed almost 40% better than the more commonly used back-off method. Unfortunately, similarity-based smoothing has not been successfully applied to statistical parsing up to now. In (Dagan et al., 1999), the bigram probability P(w2|w1) is computed as the weighted average of the conditional probability of w2 given similar words of w1. ( ) ( ) ( ) ( ) ( ) = 1 1 &amp;apos; 1 2 1 1 1 1 2 &amp;apos; | &amp;apos; , | w S w MLE SIM w w P w norm w w sim w w P where ( ) 1 1 &amp;apos; ,w w sim denotes the similarity (or an increasing function of the similarity) between w1 an</context>
<context position="17514" citStr="Dagan et al., 1999" startWordPosition="3499" endWordPosition="3502">en two contexts, sim(C, C), as the geometric average of the similarities between corresponding words: [ ][ ] ( ) ( ) [ ][ ] ( ) ( ) ( ) &amp;apos; , &amp;apos; , , &amp;apos; , &amp;apos; , , , &amp;apos; , , &amp;apos; , , &amp;apos; &amp;apos; v v sim u u sim C v u C v u sim w w sim C w C w sim d w d w d w d w = = Similarity-smoothed probability is only necessary when the frequency count of the context C in the training corpus is low. We therefore compute P(E |C) = PMLE(E |C) + (1 ) PSIM(E |C) where the smoothing factor 5 | | 1 | | + + = C C and |C |is the frequency count of the context C in the training data. A difference between similarity-based smoothing in (Dagan et al., 1999) and our approach is that our model only computes probability distributions of binary variables. Words only appear as parts of contexts on the right side of the conditioning bar. This has two important implications. Firstly, when a context contains two words, we are able to use the cross product of the similar words, whereas (Dagan et al., 1999) can only use the similar words of one of the words. This turns out to have significant impact on the performance (see Section 4). Secondly, in (Dagan et al., 1999), the distribution P(|w1) may itself be sparsely observed. When ( ) 1 2 &amp;apos; |w w PMLE is 0,</context>
<context position="20156" citStr="Dagan et al., 1999" startWordPosition="3956" endWordPosition="3959">The results are summarized in Table 1. It can be seen that the performance of the parser is highly correlated with the length of the sentences. Max Sentence Length 10 15 20 40 Undirected Accuracy 90.8 85.6 84.0 79.9 Table 1. Evaluation Results on CTB 3.0 We also experimented with several alternative models for dependency parsing. Table 2 summerizes the results of these models on the test corpus with sentences up to 40 words long. One of the characteristics of our parser is that it uses the similar words of both the head and the modifier for smoothing. The similarity-based smoothing method in (Dagan et al., 1999) uses the similar words of one of the words in a bigram. We can change the definition of similar context as follows so that only one word in a similar context of C may be different from a word in C (see Model (b) in Table 2): [ ] ( ) [ ] { } [ ] { } ) ( &amp;apos; , &amp;apos; , ) ( &amp;apos; , , &amp;apos; , , v S v C v u u S u C v u C v u S d w d w d w = where w is either v or u depending on whether d is L or R. This change led to a 2.2% drop in accuracy (compared with Model (a) in Table 2), which we attribute to the fact that many contexts do not have similar contexts in the training corpus. Since most previous parsing model</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Ido Dagan, Lillian Lee and Fernando Pereira. 1999. Similarity-based models of cooccurrence probabilities. Machine Learning, Vol. 34(1-3) special issue on Natural Language Learning, pp. 43-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING-96,</booktitle>
<pages>340--345</pages>
<location>Copenhagen.</location>
<contexts>
<context position="24061" citStr="Eisner, 1996" startWordPosition="4705" endWordPosition="4706">l consist exclusively of conditional probabilities of binary variables. Ratnaparkhis maximum entropy model (Ratnaparkhi, 1999) is also a conditional model. However, his model maximizes the probability of the action during each step of the parsing process, instead of overall quality of the parse tree. Yamada and Matsumoto (2002) presented a dependency parsing model using support vector machines. Their model is a discriminative model that maximizes the differences between scores of the correct parse and the scores of the top competing incorrect parses. In many dependency parsing models such as (Eisner, 1996) and (MacDonald et al., 2005), the score of a dependency tree is the sum of the scores of the dependency links, which are computed independently of other links. An undesirable consequence of this is that the parser often creates multiple dependency links that are separately likely but jointly improbable (or even impossible). For example, there is nothing in such models to prevent the parser from assigning two subjects to a verb. In the DMV model (Klein and Manning, 2004), the probability of a dependency link is partly conditioned on whether or not there is a head word of the link already has a</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of COLING-96, pp. 340-345, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus Variation and Parser Performance.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP-2001,</booktitle>
<pages>167--202</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1741" citStr="Gildea, 2001" startWordPosition="251" endWordPosition="252">racy of the conditional model is 13.6% higher than the joint model and that the strictly lexicalized conditional model outperforms the corresponding unlexicalized model based on part-of-speech tags. 1 Introduction There has been a great deal of progress in statistical parsing in the past decade (Collins, 1996; Collins, 1997; Chaniak, 2000). A common characteristic of these parsers is their use of lexicalized statistics. However, it was discovered recently that bi-lexical statistics (parameters that involve two words) actually played much smaller role than previously believed. It was found in (Gildea, 2001) that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output. Bikel (2004) observed that the bi-lexical statistics accounted for only 1.49% of the bigram statistics used by the parser. When considering only bigram statistics involved in the highest probability parse, this percentage becomes 28.8%. However, even when the bi-lexical statistics do get used, they are remarkably similar to their back-off values using part-of-speech tags. Therefore, the utility of bi-lexical statistics becomes rather questionable. Klein and Manning (2003) pr</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus Variation and Parser Performance. In Proceedings of EMNLP-2001, pp. 167-202. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Press,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="12342" citStr="Grefenstette, 1994" startWordPosition="2422" endWordPosition="2423">ired by our model. 3 Similarity-based Smoothing 3.1 Distributional Word Similarity Words that tend to appear in the same contexts tend to have similar meanings. This is known as the Distributional Hypothesis in linguistics (Harris, 1968). For example, the words test and exam are similar because both of them follow verbs such as administer, cancel, cheat on, conduct, ... and both of them can be preceded by adjectives such as academic, comprehensive, diagnostic, difficult, ... Many methods have been proposed to compute distributional similarity between words (Hindle, 1990; Pereira et al., 1993; Grefenstette, 1994; Lin, 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. We define the features of a word w to be the set of words that occurred within a small context window of w in a large corpus. The context window of an instance of w consists of the closest nonstop-word on each side of w and the stop-words in between. In our experiments, the set of stop-words are defined as the top 100 most</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Press, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelig S Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="11961" citStr="Harris, 1968" startWordPosition="2363" endWordPosition="2364">model parameters consist of the probability of a word in a given context. We use a dynamic programming algorithm similar to chart parsing as the decoder for this model. The algorithm builds a packed parse forest from bottom up in the canonical order of the parser trees. It attaches all the right children before attaching the left ones to maintain the canonical order as required by our model. 3 Similarity-based Smoothing 3.1 Distributional Word Similarity Words that tend to appear in the same contexts tend to have similar meanings. This is known as the Distributional Hypothesis in linguistics (Harris, 1968). For example, the words test and exam are similar because both of them follow verbs such as administer, cancel, cheat on, conduct, ... and both of them can be preceded by adjectives such as academic, comprehensive, diagnostic, difficult, ... Many methods have been proposed to compute distributional similarity between words (Hindle, 1990; Pereira et al., 1993; Grefenstette, 1994; Lin, 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Zelig S. Harris. 1968. Mathematical Structures of Language. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun Classification from Predicate-Argument Structures.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL90,</booktitle>
<pages>268--275</pages>
<location>Pittsburg, Pennsylvania.</location>
<contexts>
<context position="12300" citStr="Hindle, 1990" startWordPosition="2416" endWordPosition="2417">maintain the canonical order as required by our model. 3 Similarity-based Smoothing 3.1 Distributional Word Similarity Words that tend to appear in the same contexts tend to have similar meanings. This is known as the Distributional Hypothesis in linguistics (Harris, 1968). For example, the words test and exam are similar because both of them follow verbs such as administer, cancel, cheat on, conduct, ... and both of them can be preceded by adjectives such as academic, comprehensive, diagnostic, difficult, ... Many methods have been proposed to compute distributional similarity between words (Hindle, 1990; Pereira et al., 1993; Grefenstette, 1994; Lin, 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. We define the features of a word w to be the set of words that occurred within a small context window of w in a large corpus. The context window of an instance of w consists of the closest nonstop-word on each side of w and the stop-words in between. In our experiments, the set of </context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Donald Hindle. 1990. Noun Classification from Predicate-Argument Structures. In Proceedings of ACL90, pp. 268-275. Pittsburg, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2002</date>
<booktitle>In Proceedings of Neural Information Processing Systems.</booktitle>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Chris Manning. 2002. Fast exact inference with a factored model for natural language parsing. In Proceedings of Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="2338" citStr="Klein and Manning (2003)" startWordPosition="336" endWordPosition="339">was found in (Gildea, 2001) that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output. Bikel (2004) observed that the bi-lexical statistics accounted for only 1.49% of the bigram statistics used by the parser. When considering only bigram statistics involved in the highest probability parse, this percentage becomes 28.8%. However, even when the bi-lexical statistics do get used, they are remarkably similar to their back-off values using part-of-speech tags. Therefore, the utility of bi-lexical statistics becomes rather questionable. Klein and Manning (2003) presented an unlexicalized parser that eliminated all lexicalized parameters. Its performance was close to the state-of-the-art lexicalized parsers. We present a statistical dependency parser that represents the other end of spectrum where all statistical parameters are lexical and the parser does not require part-of-speech tags or grammatical categories. We call this strictly lexicalized parsing. A part-of-speech lexicon has always been considered to be a necessary component in any natural language parser. This is true in early rule-based as well as modern statistical parsers and in dependen</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Chris Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the 41st Annual Meeting of the ACL, pp. 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the ACL,</booktitle>
<pages>479--486</pages>
<contexts>
<context position="7874" citStr="Klein and Manning, 2004" startWordPosition="1362" endWordPosition="1365">s: (1, 2, L), (5, 6, R), (8, 9, L), (7, 9, R), (5, 7, R), (4, 5, R), (3, 4, R), (2, 3, L), (0, 3, L). The generation process according to the canonical order is similar to the head outward generation process in (Collins, 1999), except that it is bottom-up whereas Collins models are top-down. Suppose the dependency tree T is constructed in steps G1, ..., GN in the canonical order of the dependency links, where N is the number of words in the sentence. We can compute the probability of T as follows: ( ) ( ) ( ) = = = N i i i N G G S G P S G G G P S T P 1 1 1 2 1 ,..., , | | ,..., , | Following (Klein and Manning, 2004), we require that the creation of a dependency link from head h to modifier m be preceded by placing a left STOP and a right STOP around the modifier m and STOP between h and m. Let L w E (and R w E ) denote the event that there are no more modifiers on the left (and right) of a word w. Suppose the dependency link created in the step i is (u, v, d). If d = L, Gi is the conjunction of the four events: R u E , L u E , L v E and linkL(u, v). If d = R, Gi consists of four events: L v E , R v E , R u E and linkR(u, v). The event Gi is conditioned on 1 1,..., , i G G S , which are the words in the s</context>
<context position="9536" citStr="Klein and Manning, 2004" startWordPosition="1803" endWordPosition="1806"> on the words h and m and the number of modifiers of h between m and h. That is, o linkR(u,v) depends only on u, v, and R u C . o linkL(u,v) depends only on u, v, and L v C . Suppose Gi corresponds to a dependency link (u, v, L). The probability ( ) 1 1,..., , | i i G G S G P can be computed as: ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) L v L L v L v R u R u L u L u i L L v R u L u i i C v u v u link P C v E P C u E P C u E P G G S v u link E E E P G G S G P , , | , , | 1 , | , | ,..., , | , , , , ,..., , | 1 1 1 1 = = The events R w E and L w E correspond to the STOP events in (Collins, 1999) and (Klein and Manning, 2004). They are crucial for modeling the number of dependents. Without them, the parse trees often contain some obvious errors, such as determiners taking arguments, or prepositions having arguments on their left (instead of right). Our model requires three types of parameters: ( ) d w d w C w E P , |, where w is a word, d is a direction (left or right). This is the probability of a STOP after taking d w C modifiers on the d side. ( ) ( ) R u R C v u v u link P , , | , is the probability of v being the ( 1 + R u C )th modifier of u on the right. ( ) ( ) L v L C v u v u link P , , | , is the probabi</context>
<context position="24536" citStr="Klein and Manning, 2004" startWordPosition="4784" endWordPosition="4787">es between scores of the correct parse and the scores of the top competing incorrect parses. In many dependency parsing models such as (Eisner, 1996) and (MacDonald et al., 2005), the score of a dependency tree is the sum of the scores of the dependency links, which are computed independently of other links. An undesirable consequence of this is that the parser often creates multiple dependency links that are separately likely but jointly improbable (or even impossible). For example, there is nothing in such models to prevent the parser from assigning two subjects to a verb. In the DMV model (Klein and Manning, 2004), the probability of a dependency link is partly conditioned on whether or not there is a head word of the link already has a modifier. Our model is quite similar to the DMV model, except that we compute the conditional probability of the parse tree given the sentence, instead of the joint probability of the parse tree and the sentence. There have been several previous approaches to parsing Chinese with the Penn Chinese Treebank (e.g., Bikel and Chiang, 2000; Levy and Manning, 2003). Both of these approaches employed phrasestructure joint models and used part-of-speech tags in back-off smoothi</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Chris Manning. 2004. Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency. In Proceedings of the 42nd Annual Meeting of the ACL, pp. 479-486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Chris Manning</author>
</authors>
<title>Is it harder to parse Chinese, or the Chinese Treebank?</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<pages>439--446</pages>
<contexts>
<context position="25023" citStr="Levy and Manning, 2003" startWordPosition="4867" endWordPosition="4870"> there is nothing in such models to prevent the parser from assigning two subjects to a verb. In the DMV model (Klein and Manning, 2004), the probability of a dependency link is partly conditioned on whether or not there is a head word of the link already has a modifier. Our model is quite similar to the DMV model, except that we compute the conditional probability of the parse tree given the sentence, instead of the joint probability of the parse tree and the sentence. There have been several previous approaches to parsing Chinese with the Penn Chinese Treebank (e.g., Bikel and Chiang, 2000; Levy and Manning, 2003). Both of these approaches employed phrasestructure joint models and used part-of-speech tags in back-off smoothing. Their results were evaluated with the precision and recall of the bracketings implied in the phrase structure parse trees. In contrast, the accuracy of our model is measured in terms of the dependency relationships. A dependency tree may correspond to more than one constituency trees. Our results are therefore not directly comparable with the precision and recall values in previous research. Moreover, it was argued in (Lin 1995) that dependency based evaluation is much more mean</context>
</contexts>
<marker>Levy, Manning, 2003</marker>
<rawString>Roger Levy and Chris Manning. 2003. Is it harder to parse Chinese, or the Chinese Treebank? In Proceedings of the 41st Annual Meeting of the ACL, pp. 439-446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A dependency-based method for evaluating broad-coverage parsers.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI-95,</booktitle>
<pages>1420--1425</pages>
<contexts>
<context position="25572" citStr="Lin 1995" startWordPosition="4955" endWordPosition="4956">ebank (e.g., Bikel and Chiang, 2000; Levy and Manning, 2003). Both of these approaches employed phrasestructure joint models and used part-of-speech tags in back-off smoothing. Their results were evaluated with the precision and recall of the bracketings implied in the phrase structure parse trees. In contrast, the accuracy of our model is measured in terms of the dependency relationships. A dependency tree may correspond to more than one constituency trees. Our results are therefore not directly comparable with the precision and recall values in previous research. Moreover, it was argued in (Lin 1995) that dependency based evaluation is much more meaningful for the applications that use parse trees, since the semantic relationships are generally embedded in the dependency relationships. 6 Conclusion To the best of our knowledge, all previous natural language parsers have to rely on part-of-speech tags. We presented a strictly lexicalized model for dependency parsing that only relies on word statistics. We compared our parser with an unlexicalized parser that employs the same probabilistic model except that the parameters are estimated using gold standard tags in the Chinese Treebank. Our e</context>
</contexts>
<marker>Lin, 1995</marker>
<rawString>Dekang Lin. 1995. A dependency-based method for evaluating broad-coverage parsers. In Proceedings of IJCAI-95, pp.1420-1425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceeding of COLINGACL98,</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="12354" citStr="Lin, 1998" startWordPosition="2424" endWordPosition="2425"> Similarity-based Smoothing 3.1 Distributional Word Similarity Words that tend to appear in the same contexts tend to have similar meanings. This is known as the Distributional Hypothesis in linguistics (Harris, 1968). For example, the words test and exam are similar because both of them follow verbs such as administer, cancel, cheat on, conduct, ... and both of them can be preceded by adjectives such as academic, comprehensive, diagnostic, difficult, ... Many methods have been proposed to compute distributional similarity between words (Hindle, 1990; Pereira et al., 1993; Grefenstette, 1994; Lin, 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. We define the features of a word w to be the set of words that occurred within a small context window of w in a large corpus. The context window of an instance of w consists of the closest nonstop-word on each side of w and the stop-words in between. In our experiments, the set of stop-words are defined as the top 100 most frequent wo</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic Retrieval and Clustering of Similar Words. In Proceeding of COLINGACL98, pp. 768-774. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005,</booktitle>
<pages>91--98</pages>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of ACL-2005, pp. 91-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Z Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL-1993,</booktitle>
<pages>183--190</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="12322" citStr="Pereira et al., 1993" startWordPosition="2418" endWordPosition="2421">anonical order as required by our model. 3 Similarity-based Smoothing 3.1 Distributional Word Similarity Words that tend to appear in the same contexts tend to have similar meanings. This is known as the Distributional Hypothesis in linguistics (Harris, 1968). For example, the words test and exam are similar because both of them follow verbs such as administer, cancel, cheat on, conduct, ... and both of them can be preceded by adjectives such as academic, comprehensive, diagnostic, difficult, ... Many methods have been proposed to compute distributional similarity between words (Hindle, 1990; Pereira et al., 1993; Grefenstette, 1994; Lin, 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. We define the features of a word w to be the set of words that occurred within a small context window of w in a large corpus. The context window of an instance of w consists of the closest nonstop-word on each side of w and the stop-words in between. In our experiments, the set of stop-words are defined</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Z. Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proceedings of ACL-1993, pp. 183-190, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical Dependency Analysis with Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies,</booktitle>
<pages>195--206</pages>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical Dependency Analysis with Support Vector Machines. In Proceedings of the 8th International Workshop on Parsing Technologies, pp.195-206.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>