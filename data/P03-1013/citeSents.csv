 and trained on the Penn Treebank CITATION, which raises the question whether these models generalize to other languages, and to annotation schemes that differ from the Penn Treebank markup,,
The present paper addresses this question by proposing a probabilistic parsing model trained on Negra CITATION, a syntactically annotated corpus for German,,
While Negra has been used to build probabilistic chunkers (CITATION; CITATION), the research reported in this paper is the first attempt to develop a probabilistic full parsing model for German trained on a treebank (to our knowledge),,
Lexicalization can increase parsing performance dramatically for English (CITATION; CITATION, 2000; CITATION), and the lexicalized model proposed by CITATION has been successfully applied to Czech CITATION and Chinese CITATION,,
CITATION used Negra to train a maximum entropy-based chunker, and report LR and LP of 84.4% for NP and PP chunking,,
Using cascaded Markov models, CITATION reports an improved performance on the same task (LR 84.4%, LP 88.3%),,
CITATION train an unlexicalized PCFG on Negra to perform a different chunking task, viz., the identification of topological fields (sentence-based chunks),,
The head-lexicalized model of CITATION has been applied to German by Beil et al,,
3It is unclear what effect bi-lexical statistics have on the sister-head model; while CITATION shows bi-lexical statistics are sparse for some grammars, CITATION found they play a greater role in binarized grammars,,
roll and Rooths Head-Lexicalized Model The head-lexicalized PCFG model of CITATION is a minimal departure from the standard unlexicalized PCFG model, which makes it ideal for a direct comparison.1 A grammar rule LHS RHS can be written as P C1 ...Cn, where P is the mother category, and C1 ...Cn are daughters,,
The rule probability is then defined as (see also CITATION): P(RHS|LHS) = Prule(C1 ...Cn|P,l(P)) (3) n i=1 Pchoice(l(Ci)|Ci,P,l(P)) Here Prule(C1 ...Cn|P,l(P)) is the probability that category P with lexical head l(P) is expanded by the rule P C1 ...Cn, and Pchoice(l(C)|C,P,l(P)) is the probability that the (non-head) category C has the lexical head l(C) given that its mother is P with lexical head l(P),,
3.3 Collinss Head-Lexicalized Model In contrast to Carroll and Rooths (1998) approach, the model proposed by CITATION does not compute rule probabilities directly,,
3It is unclear what effect bi-lexical statistics have on the sister-head model; while CITATION shows bi-lexical statistics are sparse for some grammars, CITATION found they play a greater role in binarized grammars,,
CITATION report an evaluation using an NP chunking task, achieving 92% LR and LP,,
The work by CITATION and CITATION has demonstrated the applicability of the CITATION model for Czech and Chinese,,
While Negra has been used to build probabilistic chunkers (CITATION; CITATION), the research reported in this paper is the first attempt to develop a probabilistic full parsing model for German trained on a treebank (to our knowledge),,
Lexicalization can increase parsing performance dramatically for English (CITATION; CITATION, 2000; CITATION), and the lexicalized model proposed by CITATION has been successfully applied to Czech CITATION and Chinese CITATION,,
Neither CITATION nor CITATION compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages,,
Section 3 describes two standard lexicalized models (CITATION; CITATION), as well as an unlexical,,
Prominent among these properties is the semi-free \x0cLanguage Size LR LP Source English 40,000 87.4% 88.1% CITATION Chinese 3,484 69.0% 74.8% CITATION Czech 19,000 - 80.0% - CITATION Table 1: Results for the CITATION model for various languages (dependency precision for Czech) wordorder, i.e., German wordorder is fixed in some respects, but variable in others,,
CITATION report an evaluation using an NP chunking task, achieving 92% LR and LP,,
The work by CITATION and CITATION has demonstrated the applicability of the CITATION model for Czech and Chinese,,
However, the learning curve for Negra (see Figure 1) indicates that the performance of the CITATION model is stable, even for small training sets,,
CITATION and CITATION do not ,,
We used TnT CITATION, trained on the Negra training set,,
CITATION used Negra to train a maximum entropy-based chunker, and report LR and LP of 84.4% for NP and PP chunking,,
Using cascaded Markov models, CITATION reports an improved performance on the same task (LR 84.4%, LP 88.3%),,
CITATION train an unlexicalized PCFG on Negra to perform a different chunking task, viz., the identification of topological fields (sentence-based chunks),,
The head-lexicalized model of CITATION has been applied to German by Beil et al,,
3It is unclear what effect bi-lexical statistics have on the sister-head model; while CITATION shows bi-lexical statistics are sparse for some grammars, CITATION found they play a greater role in bin,,
sing model trained on Negra CITATION, a syntactically annotated corpus for German,,
While Negra has been used to build probabilistic chunkers (CITATION; CITATION), the research reported in this paper is the first attempt to develop a probabilistic full parsing model for German trained on a treebank (to our knowledge),,
Lexicalization can increase parsing performance dramatically for English (CITATION; CITATION, 2000; CITATION), and the lexicalized model proposed by CITATION has been successfully applied to Czech CITATION and Chinese CITATION,,
Neither CITATION nor CITATION compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages,,
3 Probabilistic Parsing Models 3.1 Probabilistic Context-Free Grammars Lexicalization has been shown to improve parsing performance for the Penn Treebank (e.g., CITATION; CITATION, 2000; CITATION),,
More specifically, we used a standard probabilistic context-free grammar (PCFG; see CITATION),,
We tested a variant of the CITATION model that takes this into account,,
Grammar Induction For the unlexicalized PCFG model (henceforth baseline model), we used the probabilistic left-corner parser Lopar CITATION,,
The head-lexicalized model of CITATION (henceforth C&R model) was again realized using Lopar, which in lexicalized mode implements the model in Section 3.2,,
The lexicalized model proposed by CITATION (henceforth Collins model) was re-implemented by \x0cone of the authors,,
sister head tag X Table 4: Linguistic features in the current model compared to the models of CITATION, CITATION, and CITATION Negra, based on Collinss (1997) model for nonrecursive NPs in the Penn Treebank (which are also flat),,
For non-recursive NPs, CITATION does not use the probability function in (5), but instead substitutes Pr (and, by analogy, Pl) by: Pr(Ri,t(Ri),l(Ri)|P,Ri1,t(Ri1),l(Ri1),d(i)) (8) Here the head H is substituted by the sister Ri1 (and Li1),,
CITATION used Negra to train a maximum entropy-based chunker, and report LR and LP of 84.4% for NP and PP chunking,,
Using cascaded Markov models, CITATION reports an improved performance on the same task (LR 84.4%, LP 88.3%),,
CITATION train an unlexicalized PCFG on Negra to perform a different chunking task, viz., the identification of topological fields (sentence-based chunks),,
The head-lexicalized model of CITATION has been applied to German by Beil et al,,
3It is unclear what effect bi-lexical statistics have on the sister-head model; while CITATION shows bi-lexical statistics are sparse for some grammars, CITATION found they play a greater role in binarized grammars,,
3 Probabilistic Parsing Models 3.1 Probabilistic Context-Free Grammars Lexicalization has been shown to improve parsing performance for the Penn Treebank (e.g., CITATION; CITATION, 2000; CITATION),,
More specifically, we used a standard probabilistic context-free grammar (PCFG; see CITATION),,
3.2 Carroll and Rooths Head-Lexicalized Model The head-lexicalized PCFG model of CITATION is a minimal departure from the standard unlexicalized PCFG model, which makes it ideal for a direct comparison.1 A grammar rule LHS RHS can be written as P C1 ...Cn, where P is the mother category, and ,,
ra CITATION, a syntactically annotated corpus for German,,
While Negra has been used to build probabilistic chunkers (CITATION; CITATION), the research reported in this paper is the first attempt to develop a probabilistic full parsing model for German trained on a treebank (to our knowledge),,
Lexicalization can increase parsing performance dramatically for English (CITATION; CITATION, 2000; CITATION), and the lexicalized model proposed by CITATION has been successfully applied to Czech CITATION and Chinese CITATION,,
Neither CITATION nor CITATION compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages,,
3 Probabilistic Parsing Models 3.1 Probabilistic Context-Free Grammars Lexicalization has been shown to improve parsing performance for the Penn Treebank (e.g., CITATION; CITATION, 2000; CITATION),,
More specifically, we used a standard probabilistic context-free grammar (PCFG; see CITATION),,
1 Introduction Treebank-based probabilistic parsing has been the subject of intensive research over the past few years, resulting in parsing models that achieve both broad coverage and high parsing accuracy (e.g., CITATION; CITATION),,
However, most of the existing models have been developed for English and trained on the Penn Treebank CITATION, which raises the question whether these models generalize to other languages, and to annotation schemes that differ from the Penn Treebank markup,,
The present paper addresses this question by proposing a probabilistic parsing model trained on Negra CITATION, a syntactically annotated corpus for German,,
sister head tag X Table 4: Linguistic features in the current model compared to the models of CITATION, CITATION, and CITATION Negra, based on Collinss (1997) model for nonrecursive NPs in the Penn Treebank (which are also flat),,
For non-recursive NPs, CITATION does not use the probability function in (5), but instead substitutes Pr (and, by analogy, Pl) by: Pr(Ri,t(Ri),l(Ri)|P,Ri1,t(Ri1),l(Ri1),d(i)) (8) Here the head H is substituted by the sister Ri1 (and Li1),,
es CITATION and then add non-lexical sis2This result generalizes to Ss, which are also flat in Negra (see Section 2.2),,
TnT tagging Perfect tagging LR LP LR LP PP 3.45 1.60 4.21 3.35 S 1.28 0.11 2.23 1.22 Coord 1.87 0.39 1.54 0.80 VP 0.72 0.18 0.58 0.30 AP 0.57 0.10 0.08 0.07 AVP 0.32 0.44 0.10 0.11 NP 0.06 0.78 0.15 0.02 Table 6: Change in performance when reverting to head-head statistics for individual categories ter information CITATION, as illustrated in Table 4,,
1 Introduction Treebank-based probabilistic parsing has been the subject of intensive research over the past few years, resulting in parsing models that achieve both broad coverage and high parsing accuracy (e.g., CITATION; CITATION),,
However, most of the existing models have been developed for English and trained on the Penn Treebank CITATION, which raises the question whether these models generalize to other languages, and to annotation schemes that differ from the Penn Treebank markup,,
The present paper addresses this question by proposing a probabilistic parsing model trained on Negra CITATION, a syntactically annotated corpus for German,,
9) and Chinese CITATION,,
Neither CITATION nor CITATION compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages,,
Section 3 describes two standard lexicalized models (CITATION; CITATION), as well as an unlexicalized baseline model,,
Prominent among these properties is the semi-free \x0cLanguage Size LR LP Source English 40,000 87.4% 88.1% CITATION Chinese 3,484 69.0% 74.8% CITATION Czech 19,000 - 80.0% - CITATION Table 1: Results for the CITATION model for various languages (dependency precision for Czech) wordorder, i.e., German wordorder is fixed in some respects, but variable in others,,
3 Probabilistic Parsing Models 3.1 Probabilistic Context-Free Grammars Lexicalization has been shown to improve parsing performance for the Penn Treebank (e.g., CITATION; CITATION, 2000; CITATION),,
More specifically, we used a standard probabilistic context-free grammar (PCFG; see CITATION),,
The rule probability is then defined as (see also CITATION): P(RHS|LHS) = Prule(C1 ...Cn|P,l(P)) (3) n i=1 Pchoice(l(Ci)|Ci,P,l(P)) Here Prule(C1 ...Cn|P,l(P)) is the probability that category P with lexical head l(P) is expanded by the rule P C1 ...Cn, and Pchoice(l(C)|C,P,l(P)) is the probability that the (non-head) category C has the lexical head l(C) given that its mother is P with lexical head l(P),,
3.3 Collinss Head-Lexicalized Model In contrast to Carroll and Rooths (1998) approach, the model proposed by CITATION does not compute rule probabilities directly,,
The head-lexicalized model of CITATION (henceforth C&R model) was again realized using Lopar, which in lexicalized mode implements the model in Section 3.2,,
The lexicalized model proposed by CITATION (henceforth Collins model) was re-implemented by \x0cone of the authors,,
sister head tag X Table 4: Linguistic features in the current model compared to the models of CITATION, CITATION, and CITATION Negra, based on Collinss (1997) model for nonrecursive NPs in the Penn Treebank (which are also flat),,
For non-recursive NPs, CITATION does not use the probability function in (5), but instead substitutes Pr (and, by analogy, Pl) by: Pr(Ri,t(Ri),l(Ri)|P,Ri1,t(Ri1),l(Ri1),d(i)) (8) Here the head H is substituted by the sister Ri1 (and Li1),,
The progression in the probabilistic parsing literature has been to start with lexical head-head dependencies CITATION and then add non-lexical sis2This result generalizes to Ss, which are also flat in Negra (see Section 2.2),,
TnT tagging Perfect tagging LR LP LR LP PP 3.45 1.60 4.21 3.35 S 1.28 0.11 2.23 1.22 Coord 1.87 0.39 1.54 0.80 VP 0.72 0.18 0.58 0.30 AP 0.57 0.10 0.08 0.07 AVP 0.32 0.44 0.10 0.11 NP 0.06 0.78 0.15 0.02 Table 6: Change in performance when reverting to head-head statistics for individual categories ter information CITATION, a,,
CITATION report an evaluation using an NP chunking task, achieving 92% LR and LP,,
The work by CITATION and CITATION has demonstrated the applicability of the CITATION model for Czech and Chinese,,
However, the learning curve for Negra (see Figure 1) indicates that the performance of the CITATION model is stable, even for small training sets,,
CITATION and CITATION do not compare their models with an unlexicalized baseline; henc,,
While Negra has been used to build probabilistic chunkers (CITATION; CITATION), the research reported in this paper is the first attempt to develop a probabilistic full parsing model for German trained on a treebank (to our knowledge),,
Lexicalization can increase parsing performance dramatically for English (CITATION; CITATION, 2000; CITATION), and the lexicalized model proposed by CITATION has been successfully applied to Czech CITATION and Chinese CITATION,,
Neither CITATION nor CITATION compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages,,
Section 3 describes two standard lexicalized models (CITATION; Co,,
Prominent among these properties is the semi-free \x0cLanguage Size LR LP Source English 40,000 87.4% 88.1% CITATION Chinese 3,484 69.0% 74.8% CITATION Czech 19,000 - 80.0% - CITATION Table 1: Results for the CITATION model for various languages (dependency precision for Czech) wordorder, i.e., German wordorder is fixed in some respects, but variable in others,,
CITATION report an evaluation using an NP chunking task, achieving 92% LR and LP,,
The work by CITATION and CITATION has demonstrated the applicability of the CITATION model for Czech and Chinese,,
However, the learning curve for Negra (see Figure 1) indicates that the performance of the CITATION model is stable, even for small training sets,,
CITATION and Bik,,
Using cascaded Markov models, CITATION reports an improved performance on the same task (LR 84.4%, LP 88.3%),,
CITATION train an unlexicalized PCFG on Negra to perform a different chunking task, viz., the identification of topological fields (sentence-based chunks),,
The head-lexicalized model of CITATION has been applied to German by Beil et al,,
3It is unclear what effect bi-lexical statistics have on the sister-head model; while CITATION shows bi-lexical statistics are sparse for some grammars, CITATION found they play a greater role in binarized grammars,,
CITATION report an evaluation using an NP chunking task, achieving 92% LR and LP,,
Using cascaded Markov models, CITATION reports an improved performance on the same task (LR 84.4%, LP 88.3%),,
CITATION train an unlexicalized PCFG on Negra to perform a different chunking task, viz., the identification of topological fields (sentence-based chunks),,
The head-lexicalized model of CITATION has been applied to German by Beil et al,,
3It is unclear what effect bi-lexical statistics have on the sister-head model; while CITATION shows bi-lexical statistics are sparse for some grammars, CITATION found they play a greater role in binarized grammars,,
CITATION report an evaluation using an NP chunking task, achieving 92% LR and LP,,
1 Introduction Treebank-based probabilistic parsing has been the subject of intensive research over the past few years, resulting in parsing models that achieve both broad coverage and high parsing accuracy (e.g., CITATION; CITATION),,
However, most of the existing models have been developed for English and trained on the Penn Treebank CITATION, which raises the question whether these models generalize to other languages, and to annotation schemes that differ from the Penn Treebank markup,,
The present paper addresses this question by proposing a probabilistic parsing model trained on Negra CITATION, a syntactically annotated corpus for German,,
While Negra has been used to build probabilistic chunkers (CITATION; CITATION), the res,,
The annotation scheme CITATION is modeled to a certain extent on that of the Penn Treebank CITATION, with crucial differences,,
Grammar Induction For the unlexicalized PCFG model (henceforth baseline model), we used the probabilistic left-corner parser Lopar CITATION,,
The head-lexicalized model of CITATION (henceforth C&R model) was again realized using Lopar, which in lexicalized mode implements the model in Section 3.2,,
The reader is referred to CITATION and CITATION for details,,
 Treebank CITATION, which raises the question whether these models generalize to other languages, and to annotation schemes that differ from the Penn Treebank markup,,
The present paper addresses this question by proposing a probabilistic parsing model trained on Negra CITATION, a syntactically annotated corpus for German,,
While Negra has been used to build probabilistic chunkers (CITATION; CITATION), the research reported in this paper is the first attempt to develop a probabilistic full parsing model for German trained on a treebank (to our knowledge),,
Lexicalization can increase parsing performance dramatically for English (CITATION; CITATION, 2000; CITATION), and the lexicalized model proposed by CITATION has been successfully applied to Czech CITATION and Chinese CITATION,,
CITATION used Negra to train a maximum entropy-based chunker, and report LR and LP of 84.4% for NP and PP chunking,,
Using cascaded Markov models, CITATION reports an improved performance on the same task (LR 84.4%, LP 88.3%),,
CITATION train an unlexicalized PCFG on Negra to perform a different chunking task, viz., the identification of topological fields (sentence-based chunks),,
The head-lexicalized model of CITATION has been applied to German by Beil et al,,
bank-based probabilistic parsing has been the subject of intensive research over the past few years, resulting in parsing models that achieve both broad coverage and high parsing accuracy (e.g., CITATION; CITATION),,
However, most of the existing models have been developed for English and trained on the Penn Treebank CITATION, which raises the question whether these models generalize to other languages, and to annotation schemes that differ from the Penn Treebank markup,,
The present paper addresses this question by proposing a probabilistic parsing model trained on Negra CITATION, a syntactically annotated corpus for German,,
While Negra has been used to build probabilistic chunkers (CITATION; CITATION), the research reported in this paper is the first attempt to develop a probabilistic full parsing model for German trained on a treebank (to our knowledge),,
Lexicalization can increase parsing performance dramatically for English (CITATION; CITATION, 2000; ,,
The annotation scheme CITATION is modeled to a certain extent on that of the Penn Treebank CITATION, with crucial differences,,
While verb order is fixed in German, the order of complements and adjuncts is variable, and influenced by a variety of syntactic and non-syntactic factors, including pronominalization, information structure, definiteness, and animacy (e.g., CITATION),,
