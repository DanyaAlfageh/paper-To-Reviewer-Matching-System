Prominent among these properties is the semi-free \x0cLanguage Size LR LP Source English 40,000 87.4% 88.1% CITATION Chinese 3,484 69.0% 74.8% CITATION Czech 19,000 - 80.0% - CITATION Table 1: Results for the CITATION model for various languages (dependency precision for Czech) wordorder, i.e., German wordorder is fixed in some respects, but variable in others,,
The progression in the probabilistic parsing literature has been to start with lexical head-head dependencies CITATION and then add non-lexical sis2This result generalizes to Ss, which are also flat in Negra (see Section 2.2),,
sister head tag X Table 4: Linguistic features in the current model compared to the models of CITATION, CITATION, and CITATION Negra, based on Collinss (1997) model for nonrecursive NPs in the Penn Treebank (which are also flat),,
CITATION report an evaluation using an NP chunking task, achieving 92% LR and LP,,
Neither CITATION nor CITATION compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages,,
9) and Chinese CITATION,,
The rule probability is then defined as (see also CITATION): P(RHS|LHS) = Prule(C1 ...Cn|P,l(P)) (3) n i=1 Pchoice(l(Ci)|Ci,P,l(P)) Here Prule(C1 ...Cn|P,l(P)) is the probability that category P with lexical head l(P) is expanded by the rule P C1 ...Cn, and Pchoice(l(C)|C,P,l(P)) is the probability that the (non-head) category C has the lexical head l(C) given that its mother is P with lexical head l(P),,
TnT tagging Perfect tagging LR LP LR LP PP 3.45 1.60 4.21 3.35 S 1.28 0.11 2.23 1.22 Coord 1.87 0.39 1.54 0.80 VP 0.72 0.18 0.58 0.30 AP 0.57 0.10 0.08 0.07 AVP 0.32 0.44 0.10 0.11 NP 0.06 0.78 0.15 0.02 Table 6: Change in performance when reverting to head-head statistics for individual categories ter information CITATION, a,,
CITATION and CITATION do not compare their models with an unlexicalized baseline; henc,,
Section 3 describes two standard lexicalized models (CITATION; CITATION), as well as an unlexicalized baseline model,,
However, most of the existing models have been developed for English and trained on the Penn Treebank CITATION, which raises the question whether these models generalize to other languages, and to annotation schemes that differ from the Penn Treebank markup,,
1 Introduction Treebank-based probabilistic parsing has been the subject of intensive research over the past few years, resulting in parsing models that achieve both broad coverage and high parsing accuracy (e.g., CITATION; CITATION),,
The present paper addresses this question by proposing a probabilistic parsing model trained on Negra CITATION, a syntactically annotated corpus for German,,
3 Probabilistic Parsing Models 3.1 Probabilistic Context-Free Grammars Lexicalization has been shown to improve parsing performance for the Penn Treebank (e.g., CITATION; CITATION, 2000; CITATION),,
3.3 Collinss Head-Lexicalized Model In contrast to Carroll and Rooths (1998) approach, the model proposed by CITATION does not compute rule probabilities directly,,
The work by CITATION and CITATION has demonstrated the applicability of the CITATION model for Czech and Chinese,,
For non-recursive NPs, CITATION does not use the probability function in (5), but instead substitutes Pr (and, by analogy, Pl) by: Pr(Ri,t(Ri),l(Ri)|P,Ri1,t(Ri1),l(Ri1),d(i)) (8) Here the head H is substituted by the sister Ri1 (and Li1),,
The lexicalized model proposed by CITATION (henceforth Collins model) was re-implemented by \x0cone of the authors,,
More specifically, we used a standard probabilistic context-free grammar (PCFG; see CITATION),,
The head-lexicalized model of CITATION (henceforth C&R model) was again realized using Lopar, which in lexicalized mode implements the model in Section 3.2,,
However, the learning curve for Negra (see Figure 1) indicates that the performance of the CITATION model is stable, even for small training sets,,
