<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<bodyText confidence="0.386549">
b&amp;apos;Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 163171,
Portland, Oregon, USA, 2324 June 2011. c
</bodyText>
<figure confidence="0.47341875">
2011 Association for Computational Linguistics
Automatically Building Training Examples for Entity Extraction
Marco Pennacchiotti
Yahoo! Labs
Sunnyvale, CA, USA
pennac@yahoo-inc.com
Patrick Pantel
Microsoft Research
</figure>
<address confidence="0.974988">
Redmond, WA, USA
</address>
<email confidence="0.991491">
ppantel@microsoft.com
</email>
<sectionHeader confidence="0.988007" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9904935">
In this paper we present methods for automat-
ically acquiring training examples for the task
of entity extraction. Experimental evidence
show that: (1) our methods compete with a
current heavily supervised state-of-the-art sys-
tem, within 0.04 absolute mean average pre-
cision; and (2) our model significantly out-
performs other supervised and unsupervised
baselines by between 0.15 and 0.30 in abso-
lute mean average precision.
</bodyText>
<sectionHeader confidence="0.997999" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999441618181818">
Entity extraction is a fundamental task in NLP and
related applications. It is broadly defined as the task
of extracting entities of a given semantic class from
texts (e.g., lists of actors, musicians, cities). Search
engines such as Bing, Yahoo, and Google collect
large sets of entities to better interpret queries (Tan
and Peng, 2006), to improve query suggestions (Cao
et al., 2008) and to understand query intents (Hu et
al., 2009). In response, automated techniques for
entity extraction have been proposed (Pasca, 2007;
Wang and Cohen, 2008; Chaudhuri et al., 2009; Pan-
tel et al., 2009).
There is mounting evidence that combining
knowledge sources and information extraction sys-
tems yield significant improvements over applying
each in isolation (Pasca et al., 2006; Mirkin et al.,
2006). This intuition is explored by the Ensem-
ble Semantics (ES) framework proposed by Pennac-
chiotti and Pantel (2009), which outperforms pre-
vious state-of-the-art systems. A severe limitation
of this type of extraction system is its reliance on
editorial judgments for building large training sets
for each semantic class to be extracted. This is
particularly troublesome for applications such as
web search that require large numbers of semantic
classes in order to have a sufficient coverage of facts
and objects (Tan and Peng, 2006). Hand-crafting
training sets across international markets is often in-
feasible. In an exploratory study we estimated that
a pool of editors would need roughly 300 working
days to complete a basic set of 100 English classes
using the ES framework. Critically needed are meth-
ods for automatically building training sets that pre-
serve the extraction quality.
In this paper, we propose simple and intuitively
appealing solutions to automatically build training
sets. Positive and negative training sets for a tar-
get semantic class are acquired by leveraging: i)
trusted sources such as structured databases (e.g.,
IMDB or Wikipedia for acquiring a list of Actors);
ii) automatically constructed semantic lexicons; and
iii) instances of semantic classes other than the tar-
get class. Our models focus on extracting training
sets that are large, balanced, and representative of
the unlabeled data. These models can be used in any
extraction setting, where trusted sources of knowl-
edge are available: Today, the popularity of struc-
tured and semi-structured sources such as Wikipedia
and internet databases, makes this approach widely
applicable. As an example, in this paper we show
that our methods can be successfully adapted and
used in the ES framework. This gives us the possi-
bility to test the methods on a large-scale entity ex-
traction task. We replace the manually built training
data in the the ES model with the training data built
</bodyText>
<page confidence="0.998016">
163
</page>
<bodyText confidence="0.998935684210526">
\x0cby our algorithms. We show by means of a large em-
pirical study that our algorithms perform nearly as
good as the fully supervised ES model, within 4% in
absolute mean average precision. Further, we com-
pare the performance of our method against both
Pasca et al. (2006) and Mirkin et al. (2006), show-
ing 17% and 15% improvements in absolute mean
average precision, respectively.
The main contributions of this paper are:
We propose several general methods for
automatically acquiring labeled training data;
we show that they can be used in a large-scale
extraction framework, namely ES; and
We show empirical evidence on a large-scale
entity extraction task that our system using
automatically labeled training data performs
nearly as well as the fully-supervised ES
model, and that it significantly outperforms
state-of-the-art systems.
</bodyText>
<sectionHeader confidence="0.938493" genericHeader="method">
2 Automatic Acquisition of Training Data
</sectionHeader>
<bodyText confidence="0.994472333333333">
Supervised machine learning algorithms require
training data that is: (1) balanced and large enough
to correctly model the problem at hand (Kubat and
Matwin, 1997; Japkowicz and Stephen, 2002); and
(2) representative of the unlabeled data to decode,
i.e., training and unlabeled instances should be ide-
ally drawn from the same distribution (Blumer et al.,
1989; Blum and Langley, 1997). If these two prop-
erties are not met, various learning problems, such
as overfitting, can drastically impair predictive ac-
curacy. To address the above properties, a common
approach is to select a subset of the unlabeled data
(i.e., the instances to be decoded), and manually la-
bel them to build the training set.
In this section we propose methods to automate
this task by leveraging the multitude of structured
knowledge bases available on the Web.
Formally, given a target class c, our goal is
to implement methods to automatically build a
training set T(c), composed of both positive and
negative examples, respectively P(c) and N(c);
and to apply T(c) to classify (or rank) a set
of unlabeled data U(c), by using a learning
algorithm. For example, in entity extraction,
given the class Actors, we might have P(c) =
{Brad Pitt, Robert De Niro} and N(c) =
{Serena Williams, Rome, Robert Demiro}.
Below, we define the components of a typical
knowledge acquisition system as in the ES frame-
work, where our methods can be applied :
Sources. Textual repositories of information, ei-
ther structured (e.g., Freebase), semi-structured
(e.g., HTML tables) or unstructured (e.g., a we-
bcrawl). Information sources serve as inputs to the
extraction system, either for the Knowledge Extrac-
tors to generate candidate instances, or for the Fea-
ture Generators to generate features (see below).
Knowledge Extractors (KE). Algorithms re-
sponsible for extracting candidate instances such as
entities or facts. Extractors fall into two categories:
trusted and untrusted. Trusted extractors execute on
structured sources where the contents are deemed to
be highly accurate. Untrusted extractors execute on
unstructured or semi-structured sources and gener-
ally generate high coverage but noisy knowledge.
Feature Generators. Methods that extract evi-
dence (features) of knowledge in order to decide
which extracted candidate instances are correct.
Ranker. A module for ranking the extracted in-
stances using the features generated by the feature
generators. In supervised ML-based rankers, labeled
training instances are required to train the model.
Our goal here is to automatically label training in-
stances thus avoiding the editorial costs.
</bodyText>
<subsectionHeader confidence="0.998689">
2.1 Acquiring Positive Examples
</subsectionHeader>
<bodyText confidence="0.999579">
Trusted positives: Candidate instances for a class
c that are extracted by a trusted Knowledge Extrac-
tor (e.g., a wrapper induction system over IMDB),
tend to be mostly positive examples. A basic ap-
proach to acquiring a set of positive examples is then
to sample from the unlabeled set U(c) as follows:
</bodyText>
<equation confidence="0.998872">
P(c) = {i U(c) : ( KEi|KEi is trusted} (1)
</equation>
<bodyText confidence="0.995022777777778">
where KEi is a knowledge extractor that extracted
instance i.
The main advantage of this method is that P(c) is
guaranteed to be highly accurate, i.e., most instances
are true positives. On the downside, instances in
P(c) are not necessarily representative of the un-
trusted KEs. This can highly impact the perfor-
mance of the learning algorithm, which could over-
fit the training data on properties that are specific to
</bodyText>
<page confidence="0.999199">
164
</page>
<bodyText confidence="0.996186666666667">
\x0cthe trusted KEs, but that are not representative of the
true population to be decoded (which is largely com-
ing from untrusted KEs).
We therefore enforce that the instances in P(c)
are extracted not only from a trusted KE, but also
from any of the untrusted extractors:
</bodyText>
<equation confidence="0.9755015">
P(c) = {i U(c) : KEi|KEi is trusted
KEj|KEj is untrusted}
</equation>
<bodyText confidence="0.9620024">
(2)
External positives: This method selects the set of
positive examples P(c) from an external repository,
such as an ontology, a database, or an automati-
cally harvested source. The main advantage of this
method is that such resources are widely available
for many knowledge extraction tasks. Yet, the risk
is that P(c) is not representative of the unlabeled in-
stances U(c), as they are drawn from different pop-
ulations.
</bodyText>
<subsubsectionHeader confidence="0.822321">
2.1.1 Acquiring Negative Examples
</subsubsectionHeader>
<bodyText confidence="0.9932135">
Acquiring negative training examples is a much
more daunting task (Fagni and Sebastiani, 2007).
The main challenge is to select a set which is a good
representative of the unlabeled negatives in U(c).
Various strategies can be adopted, ranging from
selecting near-miss examples to acquiring generic
ones, each having its own pros and cons. Below
we propose our methods, some building on previous
work described in Section 5.
Near-class negatives: This method selects N(c)
from the population U(C) of the set of classes C
which are semantically similar to c. For example, in
entity extraction, the classes Athletes, Directors
and Musicians are semantically similar to the class
Actors, while Manufacturers and Products are
dissimilar. Similar classes allow us to select negative
examples that are semantic near-misses for the class
c. The hypothesis is the following:
A positive instance extracted for a class similar
to the target class c, is likely to be a near-miss
incorrect instance for c.
To model this hypothesis, we acquire N(c) from
the set of instances having the following two restric-
tions:
</bodyText>
<listItem confidence="0.8001355">
1. The instance is most likely correct for C
2. The instance is most likely incorrect for c
</listItem>
<bodyText confidence="0.990808416666667">
Note that restriction (1) alone is not sufficient, as an
instance of C can be at the same time also instance
of c. For example, given the target class Actors, the
instance Woody Allen Directors, is not a good
negative example for Actors, since Woody Allen is
both a director and an actor.
In order to enforce restriction (1), we select only
instances that have been extracted by a trusted KE
of C, i.e., the confidence of them being positive is
very high. To enforce (2), we select instances that
have never been extracted by any KE of c. More
formally, we define N(c) as follows:
</bodyText>
<equation confidence="0.99988125">
N(c) =
[
ciC
P(ci) \\ U(c) (3)
</equation>
<bodyText confidence="0.999217483870968">
The main advantage of this method is that it acquires
negatives that are semantic near-misses of the tar-
get class, thus allowing the learning algorithm to fo-
cus on these borderline cases (Fagni and Sebastiani,
2007). This is a very important property, as most
incorrect instances extracted by unsupervised KEs
are indeed semantic near-misses. On the downside,
the extracted examples are not representative of the
negative examples of the target class c, since they
are drawn from two different distributions.
Generic negatives: This method selects N(c)
from the population U(C) of all classes C different
from the target class c, i.e., both classes semantically
similar and dissimilar to c. The method is very sim-
ilar to the one above, apart from the selection of C,
which now includes any class different from c. The
underlying hypothesis is the following:
A positive instance extracted for a class different
from the target class c, is likely to be an incorrect
instance for c.
This method acquires negatives that are both seman-
tic near-misses and far-misses of the target class.
The learning algorithm is then able to focus both on
borderline cases and on clear-cut incorrect cases, i.e.
the hypothesis space is potentially larger than for the
near-class method. On the downside, the distribu-
tion of c and C are very different. By enlarging the
potential hypothesis space, the risk is then again to
capture hypotheses that overfit the training data on
properties which are not representative of the true
population to be decoded.
</bodyText>
<page confidence="0.996752">
165
</page>
<bodyText confidence="0.992924">
\x0cSame-class negatives: This method selects the
set of negative examples N(c) from the population
U(c). The driving hypothesis is the following:
If a candidate instance for a class c has been ex-
tracted by only one KE and this KE is untrusted,
then the instance is likely to be incorrect, i.e., a
negative example for c.
The above hypothesis stems from an intuitive obser-
vation common to many ensemble-based paradigms
(e.g., ensemble learning in Machine Learning): the
more evidence you have of a given fact, the higher is
the probability of it being actually true. In our case,
the fact that an instance has been extracted by only
one untrusted KE, provides weak evidence that the
instance is correct. N(c) is defined as follows:
</bodyText>
<equation confidence="0.9837685">
N(c) = {i U(c) : ! KEi KEi is untrusted}
(4)
</equation>
<bodyText confidence="0.99793635">
The main advantage of this method is that the ac-
quired instances in N(c) are good representatives of
the negatives that will have to be decoded, i.e., they
are drawn from the same distribution U(c). This al-
lows the learning algorithm to focus on the typical
properties of the incorrect examples extracted by the
pool of KEs.
A drawback of this method is that instances in
N(c) are not guaranteed to be true negatives. It fol-
lows that the final training set may be noisy. Two
main strategies can be applied to mitigate this prob-
lem: (1) Use a learning algorithm which is robust to
noise in the training data; and (2) Adopt techniques
to automatically reduce or eliminate noise. We here
adopt the first solution, and leave the second as a
possible avenue for future work, as described in Sec-
tion 6. In Section 4 we demonstrate the amount of
noise in our training data, and show that its impact
is not detrimental for the overall performance of the
system.
</bodyText>
<sectionHeader confidence="0.914539" genericHeader="method">
3 A Use Case: Entity Extraction
</sectionHeader>
<bodyText confidence="0.990334342857143">
Entity extraction is a fundamental task in NLP
(Cimiano and Staab, 2004; McCarthy and Lehnert,
2005) and web search (Chaudhuri et al., 2009; Hu
et al., 2009; Tan and Peng, 2006), responsible for
extracting instances of semantic classes (e.g., Brad
Pitt and Tom Hanks are instances of the class Ac-
tors). In this section we apply our methods for auto-
matically acquiring training data to the ES entity ex-
traction system described in Pennacchiotti and Pan-
tel (2009).1
The system relies on the following three knowl-
edge extractors. KEtrs: a trusted database
wrapper extractor acquiring entities from sources
such as Yahoo! Movies, Yahoo! Music and Yahoo!
Sports, for extracting respectively Actors, Musicians
and Athletes. KEpat: an untrusted pattern-based
extractor reimplementing Pasca et al.s (2006) state-
of-the-art web-scale fact extractor. KEdis: an un-
trusted distributional extractor implementing a vari-
ant of Pantel et al.s (2009).
The system includes four feature generators,
which compute a total of 402 features of various
types extracted from the following sources: (1) a
body of 600 million documents crawled from the
Web at Yahoo! in 2008; (2) one year of web search
queries issued to Yahoo! Search; (3) all HTML inner
tables extracted from the above web crawl; (4) an
official Wikipedia dump from February 2008, con-
sisting of about 2 million articles.
The system adopts as a ranker a supervised
Gradient Boosted Decision Tree regression model
(GBDT) (Friedman, 2001). GBDT is generally con-
sidered robust to noisy training data, and hence is a
good choice given the errors introduced by our auto-
matic training set construction algorithms.
</bodyText>
<subsectionHeader confidence="0.999957">
3.1 Training Data Acquisition
</subsectionHeader>
<bodyText confidence="0.997439714285714">
The positive and negative components of the training
set for GBDT are built using the methods presented
in Section 2, as follows:
Trusted positives (Ptrs and Pcls): According to
Eq. 2, we acquire a set of positive instances Pcls
as a random sample of the instances extracted by
both KEtrs and either: KEdis, KEpat or both of
them. As a basic variant, we also experiment with
the simpler definition in Eq. 1, i.e. we acquire a set
of positive instances Ptrs as a random sample of the
instances extracted by the trusted extractor KEtrs,
irrespective of KEdis and KEpat.
External positives (Pcbc): Any external repository
of positive examples would serve here. In our spe-
</bodyText>
<page confidence="0.768287">
1
</page>
<bodyText confidence="0.9997015">
We here give a summary description of our implementation
of that system. Refer to the original paper for more details.
</bodyText>
<page confidence="0.995812">
166
</page>
<bodyText confidence="0.996593791666667">
\x0ccific implementation, we select a set of positive ex-
amples from the CBC repository (Pantel and Lin,
2002). CBC is a word clustering algorithm that
groups instances appearing in similar textual con-
texts. By manually analyzing the cluster members
in the repository created by CBC, it is easy to pick-
up the cluster(s) representing a target class.
Same-class negatives (Ncls): We select a set of
negative instances as a random sample of the in-
stances extracted by only one extractor, which can
be either of the two untrusted ones, KEdis or
KEpat.
Near-class negatives (Noth): We select a set of
negative instances, as a random sample of the in-
stances extracted by any of our three extractors for a
class different than the one at hand. We also enforce
the condition that instances in Noth must not have
been extracted for the class at hand.
Generic negatives (Ncbc): We automatically se-
lect as generic negatives a random sample of in-
stances appearing in any CBC cluster, except those
containing at least one member of the class at hand
(i.e., containing at least one instance extracted by
one of our KEs for the given class).
</bodyText>
<sectionHeader confidence="0.996189" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.9985474">
In this section, we report experiments comparing
the ranking performance of our different methods
for acquiring training data presented in Section 3,
to three different baselines and a fully supervised
upper-bound.
</bodyText>
<subsectionHeader confidence="0.994057">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.997294636363636">
We evaluate over three semantic classes: Actors
(movie, tv and stage actors); Athletes (profes-
sional and amateur); Musicians (singers, musicians,
composers, bands, and orchestras), so to compare
with (Pennacchiotti and Pantel, 2009). Ranking per-
formance is tested over the test set described in the
above paper, composed of 500 instances, randomly
selected from the instances extracted by KEpat and
KEdis for each of the classes2.
We experiment with various instantiations of the
ES system, each trained on a different training set
</bodyText>
<page confidence="0.930975">
2
</page>
<bodyText confidence="0.986623142857143">
We do not test over instances extracted by KEtrs, as they
do not go though the decoding phase
obtained from our methods. The different system in-
stantiations (i.e., different training sets) are reported
in Table 1 (Columns 1-3). Each training set consists
of 500 positive examples, and 500 negative exam-
ples.
As an upper bound, we use the ES system, where
the training consists of 500 manually annotated in-
stances (Pman and Nman), randomly selected from
those extracted by the KEs. This allows us to di-
rectly check if our automatically acquired training
sets can compete to the human upper-bound. We
also compare to the following baselines.
</bodyText>
<figureCaption confidence="0.8893327">
Baseline 1: An unsupervised rule-based ES sys-
tem, assigning the lowest score to instances ex-
tracted by only one KE, when the KE is untrusted;
and assigning the highest score to any other instance.
Baseline 2: An unsupervised rule-based ES sys-
tem, adopting as KEs the two untrusted extractors
KEpat and KEdis, and a rule-based Ranker that as-
signs scores to instances according to the sum of
their normalized confidence scores.
Baseline 3: An instantiation of our ES system,
</figureCaption>
<bodyText confidence="0.956447125">
trained on Pman and Nman. The only differ-
ence with the upper-bound is that it uses only two
features, namely the confidence score returned by
KEdis and KEpat. This instantiation implements
the system presented in (Mirkin et al., 2006).
For evaluation, we use average precision (AP), a
standard information retrieval measure for evaluat-
ing ranking algorithms:
</bodyText>
<equation confidence="0.996973333333333">
AP(L) =
P|L|
i=1 P(i) corr(i)
P|L|
i=1 corr(i)
(5)
</equation>
<bodyText confidence="0.994878">
where L is a ranked list produced by a system, P(i)
is the precision of L at rank i, and corr(i) is 1 if the
instance at rank i is correct, and 0 otherwise.
In order to accurately compute statistical signifi-
cance, we divide the test set in 10-folds, and com-
pute the AP mean and variance obtained over the
10-folds. For each configuration, we perform the
random sampling of the training set five times, re-
building the model each time, to estimate the vari-
ance when varying the training sampling.
</bodyText>
<subsectionHeader confidence="0.980683">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.985226">
Table 1 reports average precision (AP) results for
different ES instantiations, separately on the three
</bodyText>
<page confidence="0.99242">
167
</page>
<table confidence="0.840101166666667">
\x0cSystem Training Set AP MAP
Positives Negatives Actors Athletes Musicians
Baseline1 (unsup.) - - 0.562 0.535 0.437 0.511
Baseline2 (unsup.) - - 0.676 0.664 0.576 0.639
Baseline3 (sup.) Pman Nman 0.715 0.697 0.576 0.664
Upper-bound (full-sup.) Pman Nman 0.860
</table>
<figure confidence="0.989612965517241">
0.901
0.786
0.849
S1. Pcls Noth 0.751
0.880
0.642 0.758
S2. Pcls Ncbc 0.734
0.854
0.644 0.744
S3. Pcls Ncls 0.842
0.806
0.770
0.806
S4. Pcls Noth + Ncbc 0.756
0.853
0.693
0.767
S5. Pcls Ncls + Noth 0.835
0.807
0.763
0.802
S6. Pcls Ncls + Ncbc 0.838
0.822
0.768
0.809
S7. Pcls Ncls + Noth + Ncbc 0.838
0.818
0.764
0.807
</figure>
<tableCaption confidence="0.97055">
Table 1: Average precision (AP) results of systems using different training sets, compared to two usupervised Base-
</tableCaption>
<bodyText confidence="0.989062903225806">
lines, a supervised Baseline, and a fully supervised upper-bound system. indicates statistical significance at the 0.95
level wrt all Baselines. indicates statistical significance at the 0.95 level wrt Baseline1 and Baseline 2. indicates
statistical significance at the 0.95 level wrt Baseline1.
classes; and the mean average precision (MAP)
computed across the classes. We report results us-
ing Pcls as positive training, and varying the neg-
ative training composition3. Systems S1-S3 use a
single method to build the negatives. Systems S4-
S6 combine two methods (250 examples from one
method, 250 from the other), and S7 combines all
three methods. Table 3 reports additional basic re-
sults when varying the positive training set compo-
sition, and fixing the best performing negative set
(namely Ncls).
Table 1 shows that all systems outperform the
baselines in MAP, with 0.95 statistical significance,
but S2 which is not significant wrt Baseline 3. S6 is
the best performing system, achieving 0.809 MAP,
only 4% below the supervised upper-bound (statis-
tically insignificant at the 0.95 level). These results
indicate that our methods for automatically acquir-
ing training data are highly effective and competitive
with manually crafted training sets.
A class-by-class analysis reveals similar behav-
ior for Actors and Musicians. For these two classes,
the best negative set is Ncls (system S3), achieving
alone the best AP (respectively 0.842 and 0.770 for
Actors and Musicians, 2.1% and 1.6% points below
the upper-bound). Noth and Ncbc show a lower ac-
curacy, more than 10% below Ncls. This suggest
that the most promising strategy for automatically
</bodyText>
<page confidence="0.985505">
3
</page>
<bodyText confidence="0.656634">
For space limitation we cannot report exhaustively all com-
binations.
</bodyText>
<table confidence="0.9924062">
Negative set False Negatives
Actors Athletes Musicians
Ncls 5% 45% 30%
Noth 0% 10% 10%
Ncbc 0% 0% 15%
</table>
<tableCaption confidence="0.997722">
Table 2: Percentage of false negatives in different types of
</tableCaption>
<bodyText confidence="0.992057666666667">
negative sets, across the three experimented classes (esti-
mations over a random sample of 20 examples per class).
acquiring negative training data is to collect exam-
ples from the target class, as they guarantee to be
drawn from the same distribution as the instances to
be decoded. The use of near- and far-misses is still
valuable (AP results are still better than the base-
lines), but less effective.
Results for Athletes give different evidence: the
best performing negative set is Noth, performing
significantly better than Ncls. To investigate this
contrasting result, we manually picked 20 exam-
ples from Ncls, Noth and Ncbc for each class, and
checked their degree of noise, i.e., how many false
negatives they contain. Table 2 reports the results:
these numbers indicate that the Ncls is very noisy
for the Athletes class, while it is more clean for the
other two classes. This suggests that the learning
algorithm, while being robust enough to cope with
the small noise in Ncls for Actors and Musicians, it
starts to diverge when too many false negatives are
presented for training, as it happens for Athletes.
False negatives in Ncls are correct instances ex-
tracted by one untrusted KE alone. The results in
</bodyText>
<page confidence="0.993763">
168
</page>
<bodyText confidence="0.98986906122449">
\x0cTable 2 indicates that our untrusted KEs are more
accurate in extracting instances for Athletes than for
the other classes: accurate enough to make our train-
ing set too noisy, thus decreasing the performance
of S3 wrt S1 and S2. This indicates that the effec-
tiveness of Ncls decreases when the accuracy of the
untrusted KEs is higher.
A good strategy to avoid the above problem is to
pair Ncls with another negative set, either Ncbc or
Noth, as in S5 and S6, respectively. Then, when
the above problem is presented, the learning algo-
rithm can rely on the other negative set to com-
pensate some for the noise. Indeed, when adding
Ncbc to Ncls (system S6) the accuracy over Athletes
improves, while the overall performance across all
classes (MAP) is kept constant wrt the system using
Ncls (S3).
It is interesting that in Table 2, Ncbc and Noth also
have a few false negatives. An intrinsic analysis re-
veals that these are either: (1) Incorrect instances
of the other classes that are actual instances of the
target class; (2) Correct instances of other classes
that are also instances of the target class. Case (1) is
caused by errors of KEs for the other classes (e.g.,
erroneously extracting Matthew Flynt as a Musi-
cian). Case (2) covers cases in which instances are
ambiguous across classes, for example Kerry Tay-
lor is both an Actor and a Musician. This observa-
tion is still surprising, since Eq. 3 explicitly removes
from Ncbc and Noth any correct instance of the tar-
get class extracted by the KEs. The presence of false
negatives is then due to the low coverage of the KEs
for the target class, e.g. the KEs were not able to ex-
tract Matthew Flynt and Kerry Taylor as actors.
Correlations. We computed the Spearman corre-
lation coefficient r among the rankings produced
by the different system instantiations, to verify
how complementary the information enclosed in the
training sets are for building the learning model.
Among the basic systems S1 S3, the highest cor-
relation is between S1 and S2 (r = 0.66 in aver-
age across all classes), which is expected, since they
both apply the principle of acquiring negative ex-
amples from classes other than the target one. S3
exhibits lower correlation with both S1 and S2, re-
spectively r = 0.57 and r = 0.53, suggesting that it
is complementary to them. Also, the best system S6
System Training Set AP MAP
Pos. Neg. Act. Ath. Mus.
</bodyText>
<table confidence="0.705809">
S3. Pcls Ncls 0.842 0.806 0.770 0.806
S8. Ptrs Ncls 0.556 0.779 0.557 0.631
S9. Pcbc Ncls 0.633 0.521 0.561 0.571
</table>
<tableCaption confidence="0.989045">
Table 3: Comparative average precision (AP) results for
</tableCaption>
<figureCaption confidence="0.814796">
systems using different positive sets as training data.
Figure 1: Average precision of system S6 with different
training sizes.
</figureCaption>
<bodyText confidence="0.998246">
has higher correlation with S3 (r = 0.94) than with
S2 (r = 0.62), indicating that in the combination of
Ncls and Ncbc, most of the model is built on Ncls.
Varying the positive training. Table 3 reports re-
sults when fixing the negative set to the best per-
forming Ncls, and exploring the use of other posi-
tive sets. As expected Pcls largely outperforms Ptrs,
confirming that removing the constraint in Eq. 2 and
using the simpler Eq. 1 makes the training set unrep-
resentative of the unlabeled population. A similar
observation stands for Pcbc. These results indicate
that having a good trusted KE, or even an external
resource of positives, is effective only when select-
ing from the training set examples that are also ex-
tracted by the untrusted KEs.
Varying the training size. In Figure 1 we report
an analysis of the AP achieved by the best perform-
ing System (S6), when varying the training size, i.e.,
changing the cardinality of Pcls and Ncls + Ncbc.
The results show that a relatively small-sized train-
ing set offers good performance, the plateau being
reached already with 500 training examples. This
is an encouraging result, showing that our methods
can potentially be applied also in cases where few
examples are available, e.g., for rare or not well-
represented classes.
</bodyText>
<page confidence="0.996901">
169
</page>
<sectionHeader confidence="0.977574" genericHeader="related work">
\x0c5 Related Work
</sectionHeader>
<bodyText confidence="0.99978262745098">
Most relevant are efforts in semi-supervised learn-
ing. Semi-supervised systems use both labeled and
unlabeled data to train a machine learning system.
Most common techniques are based on co-training
and self-training. Co-training uses a small set of la-
beled examples to train two classifiers at the same
time. The classifiers use independent views (i.e.
conditionally independent feature sets) to repre-
sent the labeled examples. After the learning phase,
the most confident predictions of each classifier
on the unlabeled data are used to increase the la-
beled set of the other. These two phases are re-
peated until a stop condition is met. Co-training
has been successfully applied to various applica-
tions, such as statistical parsing (Sarkar, 2001) and
web pages classification (Yarowsky, 1998). Self-
training techniques (or bootsrapping) (Yarowsky,
1995) start with a small set of labeled data, and it-
eratively classify unlabeled data, selecting the most
confident predictions as additional training. Self-
training has been applied in many NLP tasks, such
as word sense disambiguation (Yarowsky, 1995) and
relation extraction (Hearst, 1992). Unlike typical
semi-supervised approaches, our approach reduces
the needed amount of labeled data not by acting on
the learning algorithm itself (any algorithm can be
used in our approach), but on the method to acquire
the labeled training data.
Our work also relates to the automatic acquisi-
tion of labeled negative training data. Yangarber et
al. (2002) propose a pattern-based bootstrapping ap-
proach for harvesting generalized names (e.g., dis-
eases, locations), where labeled negative examples
for a given class are taken from positive seed exam-
ples of competing classes (e.g. examples of dis-
eases are used as negatives for locations). The ap-
proach is semi-supervised, in that it requires some
manually annotated seeds. The study shows that
using competing categories improves the accuracy
of the system, by avoiding sematic drift, which is
a common cause of divergence in boostrapping ap-
proaches. Similar approaches are used among others
in (Thelen and Riloff, 2002) for learning semantic
lexicons, in (Collins and Singer, 1999) for named-
entity recognition, and in (Fagni and Sebastiani,
2007) for hierarchical text categorization. Some of
our methods rely on the same intuition described
above, i.e., using instances of other classes as nega-
tive training examples. Yet, the ES framework al-
lows us to add further restrictions to improve the
quality of the data.
</bodyText>
<sectionHeader confidence="0.997847" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9994623125">
We presented simple and general techniques for au-
tomatically acquiring training data, and then tested
them in the context of the Ensemble Semantics
framework. Experimental results show that our
methods can compete with supervised systems us-
ing manually crafted training data. It is our hope that
these simple and easy-to-implement methods can al-
leviate some of the cost of building machine learn-
ing architectures for supporting open-domain infor-
mation extraction, where the potentially very large
number of classes to be extracted makes infeasible
the use of manually labeled training data.
There are many avenues for future work. Al-
though our reliance on high-quality knowledge
sources is not an issue for many head classes, it
poses a challenge for tail classes such as wine con-
noisseurs, where finding alternative sources of high
precision samples is important. We also plan to ex-
plore techniques to automatically identify and elim-
inate mislabeled examples in the training data as
in (Rebbapragada and Brodley, 2007), and relax the
boolean assumption of trusted/untrusted extractors
into a graded one. Another important issue regards
the discovery of near-classes for collecting near-
classes negatives: we plan to automate this step by
adapting existing techniques as in (McIntosh, 2010).
Finally, we plan to experiment on a larger set of
classes, to show the generalizability of the approach.
Our current work focuses on leveraging auto-
learning to create an extensive taxonomy of classes,
which will constitute the foundation of a very large
knowledge-base for supporting web search.
</bodyText>
<sectionHeader confidence="0.977487" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.961986">
Avrim L. Blum and Pat Langley. 1997. Selection of rel-
evant features and examples in machine learning. Ar-
tificial Intelligence, 97:245271.
A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. War-
muth. 1989. Proceedings of ltc-07. Journal of ACM,
36:929965.
</reference>
<page confidence="0.900924">
170
</page>
<reference confidence="0.999535525773195">
\x0cHuanhuan Cao, Daxin Jiang, Jian Pei, Qi He, Zhen Liao,
Enhong Chen, and Hang Li. 2008. Context-aware
query suggestion by mining click-through and session
data. In Proceedings of KDD-08, pages 875883.
Surajit Chaudhuri, Venkatesh Ganti, and Dong Xin.
2009. Exploiting web search to generate synonyms for
entities. In Proceedings of WWW-09, pages 151160.
Philipp Cimiano and Steffen Staab. 2004. Learning by
googling. SIGKDD Explorations, 6(2):2434.
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classification. In Proceedings of
WVLC/EMNLP-99, pages 100110.
Tiziano Fagni and Fabrizio Sebastiani. 2007. On the se-
lection of negative examples for hierarchical text cate-
gorization. In Proceedings of LTC-07, pages 2428.
Jerome H. Friedman. 2001. Greedy function approxima-
tion: A gradient boosting machine. Annals of Statis-
tics, 29(5):11891232.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING-92, pages 539545.
Jian Hu, Gang Wang, Fred Lochovsky, Jian tao Sun, and
Zheng Chen. 2009. Understanding users query intent
with Wikipedia. In Proceedings of WWW-09, pages
471480.
N. Japkowicz and S. Stephen. 2002. The class imbalance
problem: A systematic study. Intelligent Data Analy-
sis, 6(5).
M. Kubat and S. Matwin. 1997. Addressing the curse
of inbalanced data sets: One-side sampleing. In Pro-
ceedings of the ICML-1997, pages 179186. Morgan
Kaufmann.
Joseph F. McCarthy and Wendy G Lehnert. 2005. Using
decision trees for coreference resolution. In Proceed-
ings of IJCAI-1995, pages 10501055.
Tara McIntosh. 2010. Unsupervised discovery of nega-
tive categories in lexicon bootstrapping. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 356365, Mas-
sachusetts, USA. Association for Computational Lin-
guistics.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similarity
methods for lexical entailment acquisition. In Pro-
ceedings of ACL/COLING-06, pages 579586.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Organizing and search-
ing the world wide web of facts - step one: The one-
million fact extraction challenge. In Proceedings of
AAAI-06, pages 14001405.
Marius Pasca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of CIKM-07, pages 683690, New York, NY,
USA.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of KDD-02, pages
613619.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP-09.
Marco Pennacchiotti and Patrick Pantel. 2009. Entity
extraction via ensemble semantics. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 238247, Singapore.
Association for Computational Linguistics.
Umaa Rebbapragada and Carla E. Brodley. 2007. Class
noise mitigation through instance weighting. In Pro-
ceedings of the 18th European Conference on Machine
Learning.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In NAACL-2001.
Bin Tan and Fuchun Peng. 2006. Unsupervised query
segmentation using generative language models and
wikipedia. In Proceedings of WWW-06, pages 1400
1405.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 214221, Philadelphia, PA, USA. As-
sociation for Computational Linguistics.
Richard C. Wang and William W. Cohen. 2008. Itera-
tive set expansion of named entities using the web. In
ICDM 08: Proceedings of the 2008 Eighth IEEE In-
ternational Conference on Data Mining, pages 1091
1096, Washington, DC, USA. IEEE Computer Society.
Roman Yangarber, Winston Lin, and Ralph Grishman.
2002. Unsupervised learning of generalized names.
In COLING-2002.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of ACL-1996, pages 189196.
David Yarowsky. 1998. Combining labeled and unla-
beled data with co-training. In Proceedings of the
Workshop on Computational Learning Theory, pages
92100.
</reference>
<page confidence="0.969948">
171
</page>
<figure confidence="0.258657">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.704058">
<note confidence="0.984406">b&amp;apos;Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 163171, Portland, Oregon, USA, 2324 June 2011. c 2011 Association for Computational Linguistics</note>
<title confidence="0.984514">Automatically Building Training Examples for Entity Extraction</title>
<author confidence="0.999571">Marco Pennacchiotti</author>
<affiliation confidence="0.766989">Yahoo! Labs</affiliation>
<address confidence="0.983614">Sunnyvale, CA, USA</address>
<email confidence="0.99973">pennac@yahoo-inc.com</email>
<author confidence="0.998955">Patrick Pantel</author>
<affiliation confidence="0.997745">Microsoft Research</affiliation>
<address confidence="0.998424">Redmond, WA, USA</address>
<email confidence="0.999799">ppantel@microsoft.com</email>
<abstract confidence="0.998562181818182">In this paper we present methods for automatically acquiring training examples for the task of entity extraction. Experimental evidence show that: (1) our methods compete with a current heavily supervised state-of-the-art system, within 0.04 absolute mean average precision; and (2) our model significantly outperforms other supervised and unsupervised baselines by between 0.15 and 0.30 in absolute mean average precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Avrim L Blum</author>
<author>Pat Langley</author>
</authors>
<title>Selection of relevant features and examples in machine learning.</title>
<date>1997</date>
<journal>Artificial Intelligence,</journal>
<pages>97--245271</pages>
<contexts>
<context position="4874" citStr="Blum and Langley, 1997" startWordPosition="747" endWordPosition="750"> extraction task that our system using automatically labeled training data performs nearly as well as the fully-supervised ES model, and that it significantly outperforms state-of-the-art systems. 2 Automatic Acquisition of Training Data Supervised machine learning algorithms require training data that is: (1) balanced and large enough to correctly model the problem at hand (Kubat and Matwin, 1997; Japkowicz and Stephen, 2002); and (2) representative of the unlabeled data to decode, i.e., training and unlabeled instances should be ideally drawn from the same distribution (Blumer et al., 1989; Blum and Langley, 1997). If these two properties are not met, various learning problems, such as overfitting, can drastically impair predictive accuracy. To address the above properties, a common approach is to select a subset of the unlabeled data (i.e., the instances to be decoded), and manually label them to build the training set. In this section we propose methods to automate this task by leveraging the multitude of structured knowledge bases available on the Web. Formally, given a target class c, our goal is to implement methods to automatically build a training set T(c), composed of both positive and negative</context>
</contexts>
<marker>Blum, Langley, 1997</marker>
<rawString>Avrim L. Blum and Pat Langley. 1997. Selection of relevant features and examples in machine learning. Artificial Intelligence, 97:245271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blumer</author>
<author>A Ehrenfeucht</author>
<author>D Haussler</author>
<author>M K Warmuth</author>
</authors>
<date>1989</date>
<booktitle>Proceedings of ltc-07. Journal of ACM,</booktitle>
<pages>36--929965</pages>
<contexts>
<context position="4849" citStr="Blumer et al., 1989" startWordPosition="743" endWordPosition="746"> a large-scale entity extraction task that our system using automatically labeled training data performs nearly as well as the fully-supervised ES model, and that it significantly outperforms state-of-the-art systems. 2 Automatic Acquisition of Training Data Supervised machine learning algorithms require training data that is: (1) balanced and large enough to correctly model the problem at hand (Kubat and Matwin, 1997; Japkowicz and Stephen, 2002); and (2) representative of the unlabeled data to decode, i.e., training and unlabeled instances should be ideally drawn from the same distribution (Blumer et al., 1989; Blum and Langley, 1997). If these two properties are not met, various learning problems, such as overfitting, can drastically impair predictive accuracy. To address the above properties, a common approach is to select a subset of the unlabeled data (i.e., the instances to be decoded), and manually label them to build the training set. In this section we propose methods to automate this task by leveraging the multitude of structured knowledge bases available on the Web. Formally, given a target class c, our goal is to implement methods to automatically build a training set T(c), composed of b</context>
</contexts>
<marker>Blumer, Ehrenfeucht, Haussler, Warmuth, 1989</marker>
<rawString>A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. Warmuth. 1989. Proceedings of ltc-07. Journal of ACM, 36:929965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cHuanhuan Cao</author>
<author>Daxin Jiang</author>
<author>Jian Pei</author>
<author>Qi He</author>
<author>Zhen Liao</author>
<author>Enhong Chen</author>
<author>Hang Li</author>
</authors>
<title>Context-aware query suggestion by mining click-through and session data.</title>
<date>2008</date>
<booktitle>In Proceedings of KDD-08,</booktitle>
<pages>875883</pages>
<contexts>
<context position="1230" citStr="Cao et al., 2008" startWordPosition="173" endWordPosition="176">te-of-the-art system, within 0.04 absolute mean average precision; and (2) our model significantly outperforms other supervised and unsupervised baselines by between 0.15 and 0.30 in absolute mean average precision. 1 Introduction Entity extraction is a fundamental task in NLP and related applications. It is broadly defined as the task of extracting entities of a given semantic class from texts (e.g., lists of actors, musicians, cities). Search engines such as Bing, Yahoo, and Google collect large sets of entities to better interpret queries (Tan and Peng, 2006), to improve query suggestions (Cao et al., 2008) and to understand query intents (Hu et al., 2009). In response, automated techniques for entity extraction have been proposed (Pasca, 2007; Wang and Cohen, 2008; Chaudhuri et al., 2009; Pantel et al., 2009). There is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (Pasca et al., 2006; Mirkin et al., 2006). This intuition is explored by the Ensemble Semantics (ES) framework proposed by Pennacchiotti and Pantel (2009), which outperforms previous state-of-the-art systems. A severe limitation of t</context>
</contexts>
<marker>Cao, Jiang, Pei, He, Liao, Chen, Li, 2008</marker>
<rawString>\x0cHuanhuan Cao, Daxin Jiang, Jian Pei, Qi He, Zhen Liao, Enhong Chen, and Hang Li. 2008. Context-aware query suggestion by mining click-through and session data. In Proceedings of KDD-08, pages 875883.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Surajit Chaudhuri</author>
<author>Venkatesh Ganti</author>
<author>Dong Xin</author>
</authors>
<title>Exploiting web search to generate synonyms for entities.</title>
<date>2009</date>
<booktitle>In Proceedings of WWW-09,</booktitle>
<pages>151160</pages>
<contexts>
<context position="1415" citStr="Chaudhuri et al., 2009" startWordPosition="202" endWordPosition="205">in absolute mean average precision. 1 Introduction Entity extraction is a fundamental task in NLP and related applications. It is broadly defined as the task of extracting entities of a given semantic class from texts (e.g., lists of actors, musicians, cities). Search engines such as Bing, Yahoo, and Google collect large sets of entities to better interpret queries (Tan and Peng, 2006), to improve query suggestions (Cao et al., 2008) and to understand query intents (Hu et al., 2009). In response, automated techniques for entity extraction have been proposed (Pasca, 2007; Wang and Cohen, 2008; Chaudhuri et al., 2009; Pantel et al., 2009). There is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (Pasca et al., 2006; Mirkin et al., 2006). This intuition is explored by the Ensemble Semantics (ES) framework proposed by Pennacchiotti and Pantel (2009), which outperforms previous state-of-the-art systems. A severe limitation of this type of extraction system is its reliance on editorial judgments for building large training sets for each semantic class to be extracted. This is particularly troublesome for appli</context>
<context position="13926" citStr="Chaudhuri et al., 2009" startWordPosition="2255" endWordPosition="2258">ed to mitigate this problem: (1) Use a learning algorithm which is robust to noise in the training data; and (2) Adopt techniques to automatically reduce or eliminate noise. We here adopt the first solution, and leave the second as a possible avenue for future work, as described in Section 6. In Section 4 we demonstrate the amount of noise in our training data, and show that its impact is not detrimental for the overall performance of the system. 3 A Use Case: Entity Extraction Entity extraction is a fundamental task in NLP (Cimiano and Staab, 2004; McCarthy and Lehnert, 2005) and web search (Chaudhuri et al., 2009; Hu et al., 2009; Tan and Peng, 2006), responsible for extracting instances of semantic classes (e.g., Brad Pitt and Tom Hanks are instances of the class Actors). In this section we apply our methods for automatically acquiring training data to the ES entity extraction system described in Pennacchiotti and Pantel (2009).1 The system relies on the following three knowledge extractors. KEtrs: a trusted database wrapper extractor acquiring entities from sources such as Yahoo! Movies, Yahoo! Music and Yahoo! Sports, for extracting respectively Actors, Musicians and Athletes. KEpat: an untrusted p</context>
</contexts>
<marker>Chaudhuri, Ganti, Xin, 2009</marker>
<rawString>Surajit Chaudhuri, Venkatesh Ganti, and Dong Xin. 2009. Exploiting web search to generate synonyms for entities. In Proceedings of WWW-09, pages 151160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Steffen Staab</author>
</authors>
<title>Learning by googling.</title>
<date>2004</date>
<journal>SIGKDD Explorations,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="13858" citStr="Cimiano and Staab, 2004" startWordPosition="2244" endWordPosition="2247">the final training set may be noisy. Two main strategies can be applied to mitigate this problem: (1) Use a learning algorithm which is robust to noise in the training data; and (2) Adopt techniques to automatically reduce or eliminate noise. We here adopt the first solution, and leave the second as a possible avenue for future work, as described in Section 6. In Section 4 we demonstrate the amount of noise in our training data, and show that its impact is not detrimental for the overall performance of the system. 3 A Use Case: Entity Extraction Entity extraction is a fundamental task in NLP (Cimiano and Staab, 2004; McCarthy and Lehnert, 2005) and web search (Chaudhuri et al., 2009; Hu et al., 2009; Tan and Peng, 2006), responsible for extracting instances of semantic classes (e.g., Brad Pitt and Tom Hanks are instances of the class Actors). In this section we apply our methods for automatically acquiring training data to the ES entity extraction system described in Pennacchiotti and Pantel (2009).1 The system relies on the following three knowledge extractors. KEtrs: a trusted database wrapper extractor acquiring entities from sources such as Yahoo! Movies, Yahoo! Music and Yahoo! Sports, for extractin</context>
</contexts>
<marker>Cimiano, Staab, 2004</marker>
<rawString>Philipp Cimiano and Steffen Staab. 2004. Learning by googling. SIGKDD Explorations, 6(2):2434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of WVLC/EMNLP-99,</booktitle>
<pages>100110</pages>
<contexts>
<context position="30285" citStr="Collins and Singer, 1999" startWordPosition="4952" endWordPosition="4955">harvesting generalized names (e.g., diseases, locations), where labeled negative examples for a given class are taken from positive seed examples of competing classes (e.g. examples of diseases are used as negatives for locations). The approach is semi-supervised, in that it requires some manually annotated seeds. The study shows that using competing categories improves the accuracy of the system, by avoiding sematic drift, which is a common cause of divergence in boostrapping approaches. Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for namedentity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. Some of our methods rely on the same intuition described above, i.e., using instances of other classes as negative training examples. Yet, the ES framework allows us to add further restrictions to improve the quality of the data. 6 Conclusion We presented simple and general techniques for automatically acquiring training data, and then tested them in the context of the Ensemble Semantics framework. Experimental results show that our methods can compete with supervised systems using manually </context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proceedings of WVLC/EMNLP-99, pages 100110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiziano Fagni</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>On the selection of negative examples for hierarchical text categorization.</title>
<date>2007</date>
<booktitle>In Proceedings of LTC-07,</booktitle>
<pages>2428</pages>
<contexts>
<context position="8776" citStr="Fagni and Sebastiani, 2007" startWordPosition="1374" endWordPosition="1377">trusted extractors: P(c) = {i U(c) : KEi|KEi is trusted KEj|KEj is untrusted} (2) External positives: This method selects the set of positive examples P(c) from an external repository, such as an ontology, a database, or an automatically harvested source. The main advantage of this method is that such resources are widely available for many knowledge extraction tasks. Yet, the risk is that P(c) is not representative of the unlabeled instances U(c), as they are drawn from different populations. 2.1.1 Acquiring Negative Examples Acquiring negative training examples is a much more daunting task (Fagni and Sebastiani, 2007). The main challenge is to select a set which is a good representative of the unlabeled negatives in U(c). Various strategies can be adopted, ranging from selecting near-miss examples to acquiring generic ones, each having its own pros and cons. Below we propose our methods, some building on previous work described in Section 5. Near-class negatives: This method selects N(c) from the population U(C) of the set of classes C which are semantically similar to c. For example, in entity extraction, the classes Athletes, Directors and Musicians are semantically similar to the class Actors, while Man</context>
<context position="10702" citStr="Fagni and Sebastiani, 2007" startWordPosition="1704" endWordPosition="1707">t a good negative example for Actors, since Woody Allen is both a director and an actor. In order to enforce restriction (1), we select only instances that have been extracted by a trusted KE of C, i.e., the confidence of them being positive is very high. To enforce (2), we select instances that have never been extracted by any KE of c. More formally, we define N(c) as follows: N(c) = [ ciC P(ci) \\ U(c) (3) The main advantage of this method is that it acquires negatives that are semantic near-misses of the target class, thus allowing the learning algorithm to focus on these borderline cases (Fagni and Sebastiani, 2007). This is a very important property, as most incorrect instances extracted by unsupervised KEs are indeed semantic near-misses. On the downside, the extracted examples are not representative of the negative examples of the target class c, since they are drawn from two different distributions. Generic negatives: This method selects N(c) from the population U(C) of all classes C different from the target class c, i.e., both classes semantically similar and dissimilar to c. The method is very similar to the one above, apart from the selection of C, which now includes any class different from c. T</context>
<context position="30350" citStr="Fagni and Sebastiani, 2007" startWordPosition="4962" endWordPosition="4965"> labeled negative examples for a given class are taken from positive seed examples of competing classes (e.g. examples of diseases are used as negatives for locations). The approach is semi-supervised, in that it requires some manually annotated seeds. The study shows that using competing categories improves the accuracy of the system, by avoiding sematic drift, which is a common cause of divergence in boostrapping approaches. Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for namedentity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. Some of our methods rely on the same intuition described above, i.e., using instances of other classes as negative training examples. Yet, the ES framework allows us to add further restrictions to improve the quality of the data. 6 Conclusion We presented simple and general techniques for automatically acquiring training data, and then tested them in the context of the Ensemble Semantics framework. Experimental results show that our methods can compete with supervised systems using manually crafted training data. It is our hope that these simple and easy-</context>
</contexts>
<marker>Fagni, Sebastiani, 2007</marker>
<rawString>Tiziano Fagni and Fabrizio Sebastiani. 2007. On the selection of negative examples for hierarchical text categorization. In Proceedings of LTC-07, pages 2428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome H Friedman</author>
</authors>
<title>Greedy function approximation: A gradient boosting machine.</title>
<date>2001</date>
<journal>Annals of Statistics,</journal>
<volume>29</volume>
<issue>5</issue>
<contexts>
<context position="15262" citStr="Friedman, 2001" startWordPosition="2468" endWordPosition="2469">distributional extractor implementing a variant of Pantel et al.s (2009). The system includes four feature generators, which compute a total of 402 features of various types extracted from the following sources: (1) a body of 600 million documents crawled from the Web at Yahoo! in 2008; (2) one year of web search queries issued to Yahoo! Search; (3) all HTML inner tables extracted from the above web crawl; (4) an official Wikipedia dump from February 2008, consisting of about 2 million articles. The system adopts as a ranker a supervised Gradient Boosted Decision Tree regression model (GBDT) (Friedman, 2001). GBDT is generally considered robust to noisy training data, and hence is a good choice given the errors introduced by our automatic training set construction algorithms. 3.1 Training Data Acquisition The positive and negative components of the training set for GBDT are built using the methods presented in Section 2, as follows: Trusted positives (Ptrs and Pcls): According to Eq. 2, we acquire a set of positive instances Pcls as a random sample of the instances extracted by both KEtrs and either: KEdis, KEpat or both of them. As a basic variant, we also experiment with the simpler definition </context>
</contexts>
<marker>Friedman, 2001</marker>
<rawString>Jerome H. Friedman. 2001. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5):11891232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>539545</pages>
<contexts>
<context position="29251" citStr="Hearst, 1992" startWordPosition="4793" endWordPosition="4794"> are used to increase the labeled set of the other. These two phases are repeated until a stop condition is met. Co-training has been successfully applied to various applications, such as statistical parsing (Sarkar, 2001) and web pages classification (Yarowsky, 1998). Selftraining techniques (or bootsrapping) (Yarowsky, 1995) start with a small set of labeled data, and iteratively classify unlabeled data, selecting the most confident predictions as additional training. Selftraining has been applied in many NLP tasks, such as word sense disambiguation (Yarowsky, 1995) and relation extraction (Hearst, 1992). Unlike typical semi-supervised approaches, our approach reduces the needed amount of labeled data not by acting on the learning algorithm itself (any algorithm can be used in our approach), but on the method to acquire the labeled training data. Our work also relates to the automatic acquisition of labeled negative training data. Yangarber et al. (2002) propose a pattern-based bootstrapping approach for harvesting generalized names (e.g., diseases, locations), where labeled negative examples for a given class are taken from positive seed examples of competing classes (e.g. examples of diseas</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of COLING-92, pages 539545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Hu</author>
<author>Gang Wang</author>
<author>Fred Lochovsky</author>
<author>Jian tao Sun</author>
<author>Zheng Chen</author>
</authors>
<title>Understanding users query intent with Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of WWW-09,</booktitle>
<pages>471480</pages>
<contexts>
<context position="1280" citStr="Hu et al., 2009" startWordPosition="182" endWordPosition="185">rage precision; and (2) our model significantly outperforms other supervised and unsupervised baselines by between 0.15 and 0.30 in absolute mean average precision. 1 Introduction Entity extraction is a fundamental task in NLP and related applications. It is broadly defined as the task of extracting entities of a given semantic class from texts (e.g., lists of actors, musicians, cities). Search engines such as Bing, Yahoo, and Google collect large sets of entities to better interpret queries (Tan and Peng, 2006), to improve query suggestions (Cao et al., 2008) and to understand query intents (Hu et al., 2009). In response, automated techniques for entity extraction have been proposed (Pasca, 2007; Wang and Cohen, 2008; Chaudhuri et al., 2009; Pantel et al., 2009). There is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (Pasca et al., 2006; Mirkin et al., 2006). This intuition is explored by the Ensemble Semantics (ES) framework proposed by Pennacchiotti and Pantel (2009), which outperforms previous state-of-the-art systems. A severe limitation of this type of extraction system is its reliance on e</context>
<context position="13943" citStr="Hu et al., 2009" startWordPosition="2259" endWordPosition="2262">lem: (1) Use a learning algorithm which is robust to noise in the training data; and (2) Adopt techniques to automatically reduce or eliminate noise. We here adopt the first solution, and leave the second as a possible avenue for future work, as described in Section 6. In Section 4 we demonstrate the amount of noise in our training data, and show that its impact is not detrimental for the overall performance of the system. 3 A Use Case: Entity Extraction Entity extraction is a fundamental task in NLP (Cimiano and Staab, 2004; McCarthy and Lehnert, 2005) and web search (Chaudhuri et al., 2009; Hu et al., 2009; Tan and Peng, 2006), responsible for extracting instances of semantic classes (e.g., Brad Pitt and Tom Hanks are instances of the class Actors). In this section we apply our methods for automatically acquiring training data to the ES entity extraction system described in Pennacchiotti and Pantel (2009).1 The system relies on the following three knowledge extractors. KEtrs: a trusted database wrapper extractor acquiring entities from sources such as Yahoo! Movies, Yahoo! Music and Yahoo! Sports, for extracting respectively Actors, Musicians and Athletes. KEpat: an untrusted pattern-based extr</context>
</contexts>
<marker>Hu, Wang, Lochovsky, Sun, Chen, 2009</marker>
<rawString>Jian Hu, Gang Wang, Fred Lochovsky, Jian tao Sun, and Zheng Chen. 2009. Understanding users query intent with Wikipedia. In Proceedings of WWW-09, pages 471480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Japkowicz</author>
<author>S Stephen</author>
</authors>
<title>The class imbalance problem: A systematic study.</title>
<date>2002</date>
<journal>Intelligent Data Analysis,</journal>
<volume>6</volume>
<issue>5</issue>
<contexts>
<context position="4681" citStr="Japkowicz and Stephen, 2002" startWordPosition="716" endWordPosition="719">eral methods for automatically acquiring labeled training data; we show that they can be used in a large-scale extraction framework, namely ES; and We show empirical evidence on a large-scale entity extraction task that our system using automatically labeled training data performs nearly as well as the fully-supervised ES model, and that it significantly outperforms state-of-the-art systems. 2 Automatic Acquisition of Training Data Supervised machine learning algorithms require training data that is: (1) balanced and large enough to correctly model the problem at hand (Kubat and Matwin, 1997; Japkowicz and Stephen, 2002); and (2) representative of the unlabeled data to decode, i.e., training and unlabeled instances should be ideally drawn from the same distribution (Blumer et al., 1989; Blum and Langley, 1997). If these two properties are not met, various learning problems, such as overfitting, can drastically impair predictive accuracy. To address the above properties, a common approach is to select a subset of the unlabeled data (i.e., the instances to be decoded), and manually label them to build the training set. In this section we propose methods to automate this task by leveraging the multitude of struc</context>
</contexts>
<marker>Japkowicz, Stephen, 2002</marker>
<rawString>N. Japkowicz and S. Stephen. 2002. The class imbalance problem: A systematic study. Intelligent Data Analysis, 6(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kubat</author>
<author>S Matwin</author>
</authors>
<title>Addressing the curse of inbalanced data sets: One-side sampleing.</title>
<date>1997</date>
<booktitle>In Proceedings of the ICML-1997,</booktitle>
<pages>179186</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="4651" citStr="Kubat and Matwin, 1997" startWordPosition="712" endWordPosition="715">: We propose several general methods for automatically acquiring labeled training data; we show that they can be used in a large-scale extraction framework, namely ES; and We show empirical evidence on a large-scale entity extraction task that our system using automatically labeled training data performs nearly as well as the fully-supervised ES model, and that it significantly outperforms state-of-the-art systems. 2 Automatic Acquisition of Training Data Supervised machine learning algorithms require training data that is: (1) balanced and large enough to correctly model the problem at hand (Kubat and Matwin, 1997; Japkowicz and Stephen, 2002); and (2) representative of the unlabeled data to decode, i.e., training and unlabeled instances should be ideally drawn from the same distribution (Blumer et al., 1989; Blum and Langley, 1997). If these two properties are not met, various learning problems, such as overfitting, can drastically impair predictive accuracy. To address the above properties, a common approach is to select a subset of the unlabeled data (i.e., the instances to be decoded), and manually label them to build the training set. In this section we propose methods to automate this task by lev</context>
</contexts>
<marker>Kubat, Matwin, 1997</marker>
<rawString>M. Kubat and S. Matwin. 1997. Addressing the curse of inbalanced data sets: One-side sampleing. In Proceedings of the ICML-1997, pages 179186. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph F McCarthy</author>
<author>Wendy G Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI-1995,</booktitle>
<pages>10501055</pages>
<contexts>
<context position="13887" citStr="McCarthy and Lehnert, 2005" startWordPosition="2248" endWordPosition="2251">y be noisy. Two main strategies can be applied to mitigate this problem: (1) Use a learning algorithm which is robust to noise in the training data; and (2) Adopt techniques to automatically reduce or eliminate noise. We here adopt the first solution, and leave the second as a possible avenue for future work, as described in Section 6. In Section 4 we demonstrate the amount of noise in our training data, and show that its impact is not detrimental for the overall performance of the system. 3 A Use Case: Entity Extraction Entity extraction is a fundamental task in NLP (Cimiano and Staab, 2004; McCarthy and Lehnert, 2005) and web search (Chaudhuri et al., 2009; Hu et al., 2009; Tan and Peng, 2006), responsible for extracting instances of semantic classes (e.g., Brad Pitt and Tom Hanks are instances of the class Actors). In this section we apply our methods for automatically acquiring training data to the ES entity extraction system described in Pennacchiotti and Pantel (2009).1 The system relies on the following three knowledge extractors. KEtrs: a trusted database wrapper extractor acquiring entities from sources such as Yahoo! Movies, Yahoo! Music and Yahoo! Sports, for extracting respectively Actors, Musici</context>
</contexts>
<marker>McCarthy, Lehnert, 2005</marker>
<rawString>Joseph F. McCarthy and Wendy G Lehnert. 2005. Using decision trees for coreference resolution. In Proceedings of IJCAI-1995, pages 10501055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tara McIntosh</author>
</authors>
<title>Unsupervised discovery of negative categories in lexicon bootstrapping.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>356365</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Massachusetts, USA.</location>
<marker>McIntosh, 2010</marker>
<rawString>Tara McIntosh. 2010. Unsupervised discovery of negative categories in lexicon bootstrapping. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 356365, Massachusetts, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Ido Dagan</author>
<author>Maayan Geffet</author>
</authors>
<title>Integrating pattern-based and distributional similarity methods for lexical entailment acquisition.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL/COLING-06,</booktitle>
<pages>579586</pages>
<contexts>
<context position="1638" citStr="Mirkin et al., 2006" startWordPosition="236" endWordPosition="239">lists of actors, musicians, cities). Search engines such as Bing, Yahoo, and Google collect large sets of entities to better interpret queries (Tan and Peng, 2006), to improve query suggestions (Cao et al., 2008) and to understand query intents (Hu et al., 2009). In response, automated techniques for entity extraction have been proposed (Pasca, 2007; Wang and Cohen, 2008; Chaudhuri et al., 2009; Pantel et al., 2009). There is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (Pasca et al., 2006; Mirkin et al., 2006). This intuition is explored by the Ensemble Semantics (ES) framework proposed by Pennacchiotti and Pantel (2009), which outperforms previous state-of-the-art systems. A severe limitation of this type of extraction system is its reliance on editorial judgments for building large training sets for each semantic class to be extracted. This is particularly troublesome for applications such as web search that require large numbers of semantic classes in order to have a sufficient coverage of facts and objects (Tan and Peng, 2006). Hand-crafting training sets across international markets is often i</context>
<context position="3904" citStr="Mirkin et al. (2006)" startWordPosition="602" endWordPosition="605">ach widely applicable. As an example, in this paper we show that our methods can be successfully adapted and used in the ES framework. This gives us the possibility to test the methods on a large-scale entity extraction task. We replace the manually built training data in the the ES model with the training data built 163 \x0cby our algorithms. We show by means of a large empirical study that our algorithms perform nearly as good as the fully supervised ES model, within 4% in absolute mean average precision. Further, we compare the performance of our method against both Pasca et al. (2006) and Mirkin et al. (2006), showing 17% and 15% improvements in absolute mean average precision, respectively. The main contributions of this paper are: We propose several general methods for automatically acquiring labeled training data; we show that they can be used in a large-scale extraction framework, namely ES; and We show empirical evidence on a large-scale entity extraction task that our system using automatically labeled training data performs nearly as well as the fully-supervised ES model, and that it significantly outperforms state-of-the-art systems. 2 Automatic Acquisition of Training Data Supervised mach</context>
<context position="19528" citStr="Mirkin et al., 2006" startWordPosition="3172" endWordPosition="3175">es extracted by only one KE, when the KE is untrusted; and assigning the highest score to any other instance. Baseline 2: An unsupervised rule-based ES system, adopting as KEs the two untrusted extractors KEpat and KEdis, and a rule-based Ranker that assigns scores to instances according to the sum of their normalized confidence scores. Baseline 3: An instantiation of our ES system, trained on Pman and Nman. The only difference with the upper-bound is that it uses only two features, namely the confidence score returned by KEdis and KEpat. This instantiation implements the system presented in (Mirkin et al., 2006). For evaluation, we use average precision (AP), a standard information retrieval measure for evaluating ranking algorithms: AP(L) = P|L| i=1 P(i) corr(i) P|L| i=1 corr(i) (5) where L is a ranked list produced by a system, P(i) is the precision of L at rank i, and corr(i) is 1 if the instance at rank i is correct, and 0 otherwise. In order to accurately compute statistical significance, we divide the test set in 10-folds, and compute the AP mean and variance obtained over the 10-folds. For each configuration, we perform the random sampling of the training set five times, rebuilding the model e</context>
</contexts>
<marker>Mirkin, Dagan, Geffet, 2006</marker>
<rawString>Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006. Integrating pattern-based and distributional similarity methods for lexical entailment acquisition. In Proceedings of ACL/COLING-06, pages 579586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
<author>Dekang Lin</author>
<author>Jeffrey Bigham</author>
<author>Andrei Lifchits</author>
<author>Alpa Jain</author>
</authors>
<title>Organizing and searching the world wide web of facts - step one: The onemillion fact extraction challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI-06,</booktitle>
<pages>14001405</pages>
<contexts>
<context position="1616" citStr="Pasca et al., 2006" startWordPosition="232" endWordPosition="235">s from texts (e.g., lists of actors, musicians, cities). Search engines such as Bing, Yahoo, and Google collect large sets of entities to better interpret queries (Tan and Peng, 2006), to improve query suggestions (Cao et al., 2008) and to understand query intents (Hu et al., 2009). In response, automated techniques for entity extraction have been proposed (Pasca, 2007; Wang and Cohen, 2008; Chaudhuri et al., 2009; Pantel et al., 2009). There is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (Pasca et al., 2006; Mirkin et al., 2006). This intuition is explored by the Ensemble Semantics (ES) framework proposed by Pennacchiotti and Pantel (2009), which outperforms previous state-of-the-art systems. A severe limitation of this type of extraction system is its reliance on editorial judgments for building large training sets for each semantic class to be extracted. This is particularly troublesome for applications such as web search that require large numbers of semantic classes in order to have a sufficient coverage of facts and objects (Tan and Peng, 2006). Hand-crafting training sets across internatio</context>
<context position="3879" citStr="Pasca et al. (2006)" startWordPosition="597" endWordPosition="600">abases, makes this approach widely applicable. As an example, in this paper we show that our methods can be successfully adapted and used in the ES framework. This gives us the possibility to test the methods on a large-scale entity extraction task. We replace the manually built training data in the the ES model with the training data built 163 \x0cby our algorithms. We show by means of a large empirical study that our algorithms perform nearly as good as the fully supervised ES model, within 4% in absolute mean average precision. Further, we compare the performance of our method against both Pasca et al. (2006) and Mirkin et al. (2006), showing 17% and 15% improvements in absolute mean average precision, respectively. The main contributions of this paper are: We propose several general methods for automatically acquiring labeled training data; we show that they can be used in a large-scale extraction framework, namely ES; and We show empirical evidence on a large-scale entity extraction task that our system using automatically labeled training data performs nearly as well as the fully-supervised ES model, and that it significantly outperforms state-of-the-art systems. 2 Automatic Acquisition of Trai</context>
</contexts>
<marker>Pasca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lifchits, and Alpa Jain. 2006. Organizing and searching the world wide web of facts - step one: The onemillion fact extraction challenge. In Proceedings of AAAI-06, pages 14001405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
</authors>
<title>Weakly-supervised discovery of named entities using web search queries.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM-07,</booktitle>
<pages>683690</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="1369" citStr="Pasca, 2007" startWordPosition="196" endWordPosition="197">baselines by between 0.15 and 0.30 in absolute mean average precision. 1 Introduction Entity extraction is a fundamental task in NLP and related applications. It is broadly defined as the task of extracting entities of a given semantic class from texts (e.g., lists of actors, musicians, cities). Search engines such as Bing, Yahoo, and Google collect large sets of entities to better interpret queries (Tan and Peng, 2006), to improve query suggestions (Cao et al., 2008) and to understand query intents (Hu et al., 2009). In response, automated techniques for entity extraction have been proposed (Pasca, 2007; Wang and Cohen, 2008; Chaudhuri et al., 2009; Pantel et al., 2009). There is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (Pasca et al., 2006; Mirkin et al., 2006). This intuition is explored by the Ensemble Semantics (ES) framework proposed by Pennacchiotti and Pantel (2009), which outperforms previous state-of-the-art systems. A severe limitation of this type of extraction system is its reliance on editorial judgments for building large training sets for each semantic class to be extract</context>
</contexts>
<marker>Pasca, 2007</marker>
<rawString>Marius Pasca. 2007. Weakly-supervised discovery of named entities using web search queries. In Proceedings of CIKM-07, pages 683690, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of KDD-02,</booktitle>
<pages>613619</pages>
<contexts>
<context position="16366" citStr="Pantel and Lin, 2002" startWordPosition="2652" endWordPosition="2655">h KEtrs and either: KEdis, KEpat or both of them. As a basic variant, we also experiment with the simpler definition in Eq. 1, i.e. we acquire a set of positive instances Ptrs as a random sample of the instances extracted by the trusted extractor KEtrs, irrespective of KEdis and KEpat. External positives (Pcbc): Any external repository of positive examples would serve here. In our spe1 We here give a summary description of our implementation of that system. Refer to the original paper for more details. 166 \x0ccific implementation, we select a set of positive examples from the CBC repository (Pantel and Lin, 2002). CBC is a word clustering algorithm that groups instances appearing in similar textual contexts. By manually analyzing the cluster members in the repository created by CBC, it is easy to pickup the cluster(s) representing a target class. Same-class negatives (Ncls): We select a set of negative instances as a random sample of the instances extracted by only one extractor, which can be either of the two untrusted ones, KEdis or KEpat. Near-class negatives (Noth): We select a set of negative instances, as a random sample of the instances extracted by any of our three extractors for a class diffe</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of KDD-02, pages 613619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP-09.</booktitle>
<contexts>
<context position="1751" citStr="Pantel (2009)" startWordPosition="256" endWordPosition="257">tter interpret queries (Tan and Peng, 2006), to improve query suggestions (Cao et al., 2008) and to understand query intents (Hu et al., 2009). In response, automated techniques for entity extraction have been proposed (Pasca, 2007; Wang and Cohen, 2008; Chaudhuri et al., 2009; Pantel et al., 2009). There is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (Pasca et al., 2006; Mirkin et al., 2006). This intuition is explored by the Ensemble Semantics (ES) framework proposed by Pennacchiotti and Pantel (2009), which outperforms previous state-of-the-art systems. A severe limitation of this type of extraction system is its reliance on editorial judgments for building large training sets for each semantic class to be extracted. This is particularly troublesome for applications such as web search that require large numbers of semantic classes in order to have a sufficient coverage of facts and objects (Tan and Peng, 2006). Hand-crafting training sets across international markets is often infeasible. In an exploratory study we estimated that a pool of editors would need roughly 300 working days to com</context>
<context position="14248" citStr="Pantel (2009)" startWordPosition="2311" endWordPosition="2313">t of noise in our training data, and show that its impact is not detrimental for the overall performance of the system. 3 A Use Case: Entity Extraction Entity extraction is a fundamental task in NLP (Cimiano and Staab, 2004; McCarthy and Lehnert, 2005) and web search (Chaudhuri et al., 2009; Hu et al., 2009; Tan and Peng, 2006), responsible for extracting instances of semantic classes (e.g., Brad Pitt and Tom Hanks are instances of the class Actors). In this section we apply our methods for automatically acquiring training data to the ES entity extraction system described in Pennacchiotti and Pantel (2009).1 The system relies on the following three knowledge extractors. KEtrs: a trusted database wrapper extractor acquiring entities from sources such as Yahoo! Movies, Yahoo! Music and Yahoo! Sports, for extracting respectively Actors, Musicians and Athletes. KEpat: an untrusted pattern-based extractor reimplementing Pasca et al.s (2006) stateof-the-art web-scale fact extractor. KEdis: an untrusted distributional extractor implementing a variant of Pantel et al.s (2009). The system includes four feature generators, which compute a total of 402 features of various types extracted from the followin</context>
<context position="17879" citStr="Pantel, 2009" startWordPosition="2899" endWordPosition="2900">ember of the class at hand (i.e., containing at least one instance extracted by one of our KEs for the given class). 4 Experimental Evaluation In this section, we report experiments comparing the ranking performance of our different methods for acquiring training data presented in Section 3, to three different baselines and a fully supervised upper-bound. 4.1 Experimental Setup We evaluate over three semantic classes: Actors (movie, tv and stage actors); Athletes (professional and amateur); Musicians (singers, musicians, composers, bands, and orchestras), so to compare with (Pennacchiotti and Pantel, 2009). Ranking performance is tested over the test set described in the above paper, composed of 500 instances, randomly selected from the instances extracted by KEpat and KEdis for each of the classes2. We experiment with various instantiations of the ES system, each trained on a different training set 2 We do not test over instances extracted by KEtrs, as they do not go though the decoding phase obtained from our methods. The different system instantiations (i.e., different training sets) are reported in Table 1 (Columns 1-3). Each training set consists of 500 positive examples, and 500 negative </context>
</contexts>
<marker>Pantel, 2009</marker>
<rawString>Patrick Pantel, Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas. 2009. Web-scale distributional similarity and entity set expansion. In Proceedings of EMNLP-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Patrick Pantel</author>
</authors>
<title>Entity extraction via ensemble semantics.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>238247</pages>
<institution>Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="1751" citStr="Pennacchiotti and Pantel (2009)" startWordPosition="253" endWordPosition="257"> of entities to better interpret queries (Tan and Peng, 2006), to improve query suggestions (Cao et al., 2008) and to understand query intents (Hu et al., 2009). In response, automated techniques for entity extraction have been proposed (Pasca, 2007; Wang and Cohen, 2008; Chaudhuri et al., 2009; Pantel et al., 2009). There is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (Pasca et al., 2006; Mirkin et al., 2006). This intuition is explored by the Ensemble Semantics (ES) framework proposed by Pennacchiotti and Pantel (2009), which outperforms previous state-of-the-art systems. A severe limitation of this type of extraction system is its reliance on editorial judgments for building large training sets for each semantic class to be extracted. This is particularly troublesome for applications such as web search that require large numbers of semantic classes in order to have a sufficient coverage of facts and objects (Tan and Peng, 2006). Hand-crafting training sets across international markets is often infeasible. In an exploratory study we estimated that a pool of editors would need roughly 300 working days to com</context>
<context position="14248" citStr="Pennacchiotti and Pantel (2009)" startWordPosition="2309" endWordPosition="2313">onstrate the amount of noise in our training data, and show that its impact is not detrimental for the overall performance of the system. 3 A Use Case: Entity Extraction Entity extraction is a fundamental task in NLP (Cimiano and Staab, 2004; McCarthy and Lehnert, 2005) and web search (Chaudhuri et al., 2009; Hu et al., 2009; Tan and Peng, 2006), responsible for extracting instances of semantic classes (e.g., Brad Pitt and Tom Hanks are instances of the class Actors). In this section we apply our methods for automatically acquiring training data to the ES entity extraction system described in Pennacchiotti and Pantel (2009).1 The system relies on the following three knowledge extractors. KEtrs: a trusted database wrapper extractor acquiring entities from sources such as Yahoo! Movies, Yahoo! Music and Yahoo! Sports, for extracting respectively Actors, Musicians and Athletes. KEpat: an untrusted pattern-based extractor reimplementing Pasca et al.s (2006) stateof-the-art web-scale fact extractor. KEdis: an untrusted distributional extractor implementing a variant of Pantel et al.s (2009). The system includes four feature generators, which compute a total of 402 features of various types extracted from the followin</context>
<context position="17879" citStr="Pennacchiotti and Pantel, 2009" startWordPosition="2897" endWordPosition="2900">ing at least one member of the class at hand (i.e., containing at least one instance extracted by one of our KEs for the given class). 4 Experimental Evaluation In this section, we report experiments comparing the ranking performance of our different methods for acquiring training data presented in Section 3, to three different baselines and a fully supervised upper-bound. 4.1 Experimental Setup We evaluate over three semantic classes: Actors (movie, tv and stage actors); Athletes (professional and amateur); Musicians (singers, musicians, composers, bands, and orchestras), so to compare with (Pennacchiotti and Pantel, 2009). Ranking performance is tested over the test set described in the above paper, composed of 500 instances, randomly selected from the instances extracted by KEpat and KEdis for each of the classes2. We experiment with various instantiations of the ES system, each trained on a different training set 2 We do not test over instances extracted by KEtrs, as they do not go though the decoding phase obtained from our methods. The different system instantiations (i.e., different training sets) are reported in Table 1 (Columns 1-3). Each training set consists of 500 positive examples, and 500 negative </context>
</contexts>
<marker>Pennacchiotti, Pantel, 2009</marker>
<rawString>Marco Pennacchiotti and Patrick Pantel. 2009. Entity extraction via ensemble semantics. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 238247, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umaa Rebbapragada</author>
<author>Carla E Brodley</author>
</authors>
<title>Class noise mitigation through instance weighting.</title>
<date>2007</date>
<booktitle>In Proceedings of the 18th European Conference on Machine Learning.</booktitle>
<marker>Rebbapragada, Brodley, 2007</marker>
<rawString>Umaa Rebbapragada and Carla E. Brodley. 2007. Class noise mitigation through instance weighting. In Proceedings of the 18th European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In NAACL-2001.</booktitle>
<contexts>
<context position="28860" citStr="Sarkar, 2001" startWordPosition="4737" endWordPosition="4738">earning system. Most common techniques are based on co-training and self-training. Co-training uses a small set of labeled examples to train two classifiers at the same time. The classifiers use independent views (i.e. conditionally independent feature sets) to represent the labeled examples. After the learning phase, the most confident predictions of each classifier on the unlabeled data are used to increase the labeled set of the other. These two phases are repeated until a stop condition is met. Co-training has been successfully applied to various applications, such as statistical parsing (Sarkar, 2001) and web pages classification (Yarowsky, 1998). Selftraining techniques (or bootsrapping) (Yarowsky, 1995) start with a small set of labeled data, and iteratively classify unlabeled data, selecting the most confident predictions as additional training. Selftraining has been applied in many NLP tasks, such as word sense disambiguation (Yarowsky, 1995) and relation extraction (Hearst, 1992). Unlike typical semi-supervised approaches, our approach reduces the needed amount of labeled data not by acting on the learning algorithm itself (any algorithm can be used in our approach), but on the method</context>
</contexts>
<marker>Sarkar, 2001</marker>
<rawString>Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In NAACL-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Tan</author>
<author>Fuchun Peng</author>
</authors>
<title>Unsupervised query segmentation using generative language models and wikipedia.</title>
<date>2006</date>
<booktitle>In Proceedings of WWW-06,</booktitle>
<pages>1400--1405</pages>
<contexts>
<context position="1181" citStr="Tan and Peng, 2006" startWordPosition="165" endWordPosition="168">thods compete with a current heavily supervised state-of-the-art system, within 0.04 absolute mean average precision; and (2) our model significantly outperforms other supervised and unsupervised baselines by between 0.15 and 0.30 in absolute mean average precision. 1 Introduction Entity extraction is a fundamental task in NLP and related applications. It is broadly defined as the task of extracting entities of a given semantic class from texts (e.g., lists of actors, musicians, cities). Search engines such as Bing, Yahoo, and Google collect large sets of entities to better interpret queries (Tan and Peng, 2006), to improve query suggestions (Cao et al., 2008) and to understand query intents (Hu et al., 2009). In response, automated techniques for entity extraction have been proposed (Pasca, 2007; Wang and Cohen, 2008; Chaudhuri et al., 2009; Pantel et al., 2009). There is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (Pasca et al., 2006; Mirkin et al., 2006). This intuition is explored by the Ensemble Semantics (ES) framework proposed by Pennacchiotti and Pantel (2009), which outperforms previous s</context>
<context position="13964" citStr="Tan and Peng, 2006" startWordPosition="2263" endWordPosition="2266">arning algorithm which is robust to noise in the training data; and (2) Adopt techniques to automatically reduce or eliminate noise. We here adopt the first solution, and leave the second as a possible avenue for future work, as described in Section 6. In Section 4 we demonstrate the amount of noise in our training data, and show that its impact is not detrimental for the overall performance of the system. 3 A Use Case: Entity Extraction Entity extraction is a fundamental task in NLP (Cimiano and Staab, 2004; McCarthy and Lehnert, 2005) and web search (Chaudhuri et al., 2009; Hu et al., 2009; Tan and Peng, 2006), responsible for extracting instances of semantic classes (e.g., Brad Pitt and Tom Hanks are instances of the class Actors). In this section we apply our methods for automatically acquiring training data to the ES entity extraction system described in Pennacchiotti and Pantel (2009).1 The system relies on the following three knowledge extractors. KEtrs: a trusted database wrapper extractor acquiring entities from sources such as Yahoo! Movies, Yahoo! Music and Yahoo! Sports, for extracting respectively Actors, Musicians and Athletes. KEpat: an untrusted pattern-based extractor reimplementing </context>
</contexts>
<marker>Tan, Peng, 2006</marker>
<rawString>Bin Tan and Fuchun Peng. 2006. Unsupervised query segmentation using generative language models and wikipedia. In Proceedings of WWW-06, pages 1400 1405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Thelen</author>
<author>Ellen Riloff</author>
</authors>
<title>A bootstrapping method for learning semantic lexicons using extraction pattern contexts.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>214221</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="30223" citStr="Thelen and Riloff, 2002" startWordPosition="4943" endWordPosition="4946">l. (2002) propose a pattern-based bootstrapping approach for harvesting generalized names (e.g., diseases, locations), where labeled negative examples for a given class are taken from positive seed examples of competing classes (e.g. examples of diseases are used as negatives for locations). The approach is semi-supervised, in that it requires some manually annotated seeds. The study shows that using competing categories improves the accuracy of the system, by avoiding sematic drift, which is a common cause of divergence in boostrapping approaches. Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for namedentity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. Some of our methods rely on the same intuition described above, i.e., using instances of other classes as negative training examples. Yet, the ES framework allows us to add further restrictions to improve the quality of the data. 6 Conclusion We presented simple and general techniques for automatically acquiring training data, and then tested them in the context of the Ensemble Semantics framework. Experimental results show that o</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>Michael Thelen and Ellen Riloff. 2002. A bootstrapping method for learning semantic lexicons using extraction pattern contexts. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 214221, Philadelphia, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard C Wang</author>
<author>William W Cohen</author>
</authors>
<title>Iterative set expansion of named entities using the web. In</title>
<date>2008</date>
<booktitle>ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining,</booktitle>
<pages>1091--1096</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="1391" citStr="Wang and Cohen, 2008" startWordPosition="198" endWordPosition="201">between 0.15 and 0.30 in absolute mean average precision. 1 Introduction Entity extraction is a fundamental task in NLP and related applications. It is broadly defined as the task of extracting entities of a given semantic class from texts (e.g., lists of actors, musicians, cities). Search engines such as Bing, Yahoo, and Google collect large sets of entities to better interpret queries (Tan and Peng, 2006), to improve query suggestions (Cao et al., 2008) and to understand query intents (Hu et al., 2009). In response, automated techniques for entity extraction have been proposed (Pasca, 2007; Wang and Cohen, 2008; Chaudhuri et al., 2009; Pantel et al., 2009). There is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (Pasca et al., 2006; Mirkin et al., 2006). This intuition is explored by the Ensemble Semantics (ES) framework proposed by Pennacchiotti and Pantel (2009), which outperforms previous state-of-the-art systems. A severe limitation of this type of extraction system is its reliance on editorial judgments for building large training sets for each semantic class to be extracted. This is particular</context>
</contexts>
<marker>Wang, Cohen, 2008</marker>
<rawString>Richard C. Wang and William W. Cohen. 2008. Iterative set expansion of named entities using the web. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 1091 1096, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Winston Lin</author>
<author>Ralph Grishman</author>
</authors>
<title>Unsupervised learning of generalized names.</title>
<date>2002</date>
<contexts>
<context position="29608" citStr="Yangarber et al. (2002)" startWordPosition="4848" endWordPosition="4851"> set of labeled data, and iteratively classify unlabeled data, selecting the most confident predictions as additional training. Selftraining has been applied in many NLP tasks, such as word sense disambiguation (Yarowsky, 1995) and relation extraction (Hearst, 1992). Unlike typical semi-supervised approaches, our approach reduces the needed amount of labeled data not by acting on the learning algorithm itself (any algorithm can be used in our approach), but on the method to acquire the labeled training data. Our work also relates to the automatic acquisition of labeled negative training data. Yangarber et al. (2002) propose a pattern-based bootstrapping approach for harvesting generalized names (e.g., diseases, locations), where labeled negative examples for a given class are taken from positive seed examples of competing classes (e.g. examples of diseases are used as negatives for locations). The approach is semi-supervised, in that it requires some manually annotated seeds. The study shows that using competing categories improves the accuracy of the system, by avoiding sematic drift, which is a common cause of divergence in boostrapping approaches. Similar approaches are used among others in (Thelen an</context>
</contexts>
<marker>Yangarber, Lin, Grishman, 2002</marker>
<rawString>Roman Yangarber, Winston Lin, and Ralph Grishman. 2002. Unsupervised learning of generalized names.</rawString>
</citation>
<citation valid="false">
<booktitle>In COLING-2002.</booktitle>
<marker></marker>
<rawString>In COLING-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL-1996,</booktitle>
<pages>189196</pages>
<contexts>
<context position="28966" citStr="Yarowsky, 1995" startWordPosition="4750" endWordPosition="4751">all set of labeled examples to train two classifiers at the same time. The classifiers use independent views (i.e. conditionally independent feature sets) to represent the labeled examples. After the learning phase, the most confident predictions of each classifier on the unlabeled data are used to increase the labeled set of the other. These two phases are repeated until a stop condition is met. Co-training has been successfully applied to various applications, such as statistical parsing (Sarkar, 2001) and web pages classification (Yarowsky, 1998). Selftraining techniques (or bootsrapping) (Yarowsky, 1995) start with a small set of labeled data, and iteratively classify unlabeled data, selecting the most confident predictions as additional training. Selftraining has been applied in many NLP tasks, such as word sense disambiguation (Yarowsky, 1995) and relation extraction (Hearst, 1992). Unlike typical semi-supervised approaches, our approach reduces the needed amount of labeled data not by acting on the learning algorithm itself (any algorithm can be used in our approach), but on the method to acquire the labeled training data. Our work also relates to the automatic acquisition of labeled negat</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of ACL-1996, pages 189196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on Computational Learning Theory,</booktitle>
<pages>92100</pages>
<contexts>
<context position="28906" citStr="Yarowsky, 1998" startWordPosition="4743" endWordPosition="4744">ased on co-training and self-training. Co-training uses a small set of labeled examples to train two classifiers at the same time. The classifiers use independent views (i.e. conditionally independent feature sets) to represent the labeled examples. After the learning phase, the most confident predictions of each classifier on the unlabeled data are used to increase the labeled set of the other. These two phases are repeated until a stop condition is met. Co-training has been successfully applied to various applications, such as statistical parsing (Sarkar, 2001) and web pages classification (Yarowsky, 1998). Selftraining techniques (or bootsrapping) (Yarowsky, 1995) start with a small set of labeled data, and iteratively classify unlabeled data, selecting the most confident predictions as additional training. Selftraining has been applied in many NLP tasks, such as word sense disambiguation (Yarowsky, 1995) and relation extraction (Hearst, 1992). Unlike typical semi-supervised approaches, our approach reduces the needed amount of labeled data not by acting on the learning algorithm itself (any algorithm can be used in our approach), but on the method to acquire the labeled training data. Our wor</context>
</contexts>
<marker>Yarowsky, 1998</marker>
<rawString>David Yarowsky. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the Workshop on Computational Learning Theory, pages 92100.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>