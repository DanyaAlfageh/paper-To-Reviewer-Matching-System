<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.688041">
b&amp;apos;Applying System Combination to Base Noun Phrase Identi\x0ccation
</title>
<figure confidence="0.758329692307692">
Erik F. Tjong Kim Sang\x0b, Walter Daelemans\x0b, Herv\x13
e D\x13
ejean\x1c ,
Rob Koeling
, Yuval Krymolowski\x0c, Vasin Punyakanok\x13, Dan Roth\x13
\x0bUniversity of Antwerp \x1cUniversit\x7f
at T\x7f
ubingen
SRI Cambridge
Universiteitsplein 1 Kleine Wilhelmstra\x19e 113 23 Millers Yard,Mill Lane
B-2610 Wilrijk, Belgium D-72074 T\x7f
ubingen, Germany Cambridge, CB2 1RQ, UK
ferikt,daelemg@uia.ua.ac.be dejean@sfs.nphil.uni-tuebingen.de koeling@cam.sri.com
</figure>
<affiliation confidence="0.612017">
\x0cBar-Ilan University \x13University of Illinois
</affiliation>
<address confidence="0.581307">
Ramat Gan, 52900, Israel 1304 W. Spring\x0celd Ave.
</address>
<email confidence="0.6459555">
yuvalk@macs.biu.ac.il Urbana, IL 61801, USA
fpunyakan,danrg@cs.uiuc.edu
</email>
<sectionHeader confidence="0.989454" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999571888888889">
We use seven machine learning algorithms for
one task: identifying base noun phrases. The
results have been processed by di\x0berent system
combination methods and all of these outper-
formed the best individual result. We have ap-
plied the seven learners with the best combina-
tor, a majority vote of the top \x0cve systems, to a
standard data set and managed to improve the
best published result for this data set.
</bodyText>
<sectionHeader confidence="0.996441" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989397111111111">
Van Halteren et al. (1998) and Brill and Wu
(1998) show that part-of-speech tagger perfor-
mance can be improved by combining di\x0berent
taggers. By using techniques such as majority
voting, errors made by the minority of the tag-
gers can be removed. Van Halteren et al. (1998)
report that the results of such a combined ap-
proach can improve upon the accuracy error of
the best individualsystem with as much as 19%.
The positive e\x0bect of system combination for
non-language processing tasks has been shown
in a large body of machine learning work.
In this paper we will use system combination
for identifying base noun phrases (baseNPs).
We will apply seven machine learning algo-
rithms to the same baseNP task. At two points
we will apply combination methods. We will
start with making the systems process \x0cve out-
put representations and combine the results by
choosing the majority of the output features.
Three of the seven systems use this approach.
After this we will make an overall combination
of the results of the seven systems. There we
will evaluate several system combination meth-
ods. The best performing method will be ap-
plied to a standard data set for baseNP identi-
\x0ccation.
</bodyText>
<sectionHeader confidence="0.939287" genericHeader="introduction">
2 Methods and experiments
</sectionHeader>
<bodyText confidence="0.999429142857143">
In this section we willdescribeour learningtask:
recognizing base noun phrases. After this we
will describe the data representations we used
and the machine learning algorithms that we
will apply to the task. We will conclude with
an overview of the combination methods that
we will test.
</bodyText>
<subsectionHeader confidence="0.977456">
2.1 Task description
</subsectionHeader>
<bodyText confidence="0.991144722222222">
Base noun phrases (baseNPs) are noun phrases
which do not contain another noun phrase. For
example, the sentence
In [ early trading ] in [ Hong Kong ]
[ Monday ] , [ gold ] was quoted at
[ $ 366.50 ] [ an ounce ] .
contains six baseNPs (marked as phrases be-
tween square brackets). The phrase $ 366.50
an ounce is a noun phrase as well. However, it
is not a baseNP since it contains two other noun
phrases. Two baseNP data sets have been put
forward by Ramshaw and Marcus (1995). The
main data set consist of four sections of the Wall
Street Journal (WSJ) part of the Penn Tree-
bank (Marcus et al., 1993) as training mate-
rial (sections 15-18, 211727 tokens) and one sec-
tion as test material (section 20, 47377 tokens)1.
The data contains words, their part-of-speech
</bodyText>
<page confidence="0.915501">
1
</page>
<bodyText confidence="0.995338785714286">
This Ramshaw and Marcus (1995) baseNP data set
is available via ftp://ftp.cis.upenn.edu/pub/chunker/
\x0c(POS) tags as computed by the Brill tagger and
their baseNP segmentation as derived from the
Treebank (with some modi\x0ccations).
In the baseNP identi\x0ccation task, perfor-
mance is measured with three rates. First,
with the percentage of detected noun phrases
that are correct (precision). Second, with the
percentage of noun phrases in the data that
were found by the classi\x0cer (recall). And third,
with the F\x0c=1 rate which is equal to (2*preci-
sion*recall)/(precision+recall). The latter rate
has been used as the target for optimization.
</bodyText>
<subsectionHeader confidence="0.997198">
2.2 Data representation
</subsectionHeader>
<bodyText confidence="0.998587928571428">
In our example sentence in section 2.1, noun
phrases are represented by bracket structures.
It has been shown by Mu~
noz et al. (1999)
that for baseNP recognition, the representa-
tion with brackets outperforms other data rep-
resentations. One classi\x0cer can be trained to
recognize open brackets (O) and another can
handle close brackets (C). Their results can be
combined by making pairs of open and close
brackets with large probability scores. We have
used this bracket representation (O+C) as well.
However, we have not used the combination
strategy from Mu~
noz et al. (1999) but in-
stead used the strategy outlined in Tjong Kim
Sang (2000): regard only the shortest possi-
ble phrases between candidate open and close
brackets as base noun phrases.
An alternative representation for baseNPs
has been put forward by Ramshaw and Mar-
cus (1995). They have de\x0cned baseNP recog-
nition as a tagging task: words can be inside a
baseNP (I) or outside a baseNP (O). In the case
that one baseNP immediately follows another
baseNP, the \x0crst word in the second baseNP
receives tag B. Example:
InO earlyI tradingI inO HongI KongI
</bodyText>
<equation confidence="0.538589">
MondayB ,O goldI wasO quotedO atO
$I 366.50I anB ounceI .O
</equation>
<bodyText confidence="0.9986281875">
This set of three tags is su\x0ecient for encod-
ing baseNP structures since these structures are
nonrecursive and nonoverlapping.
Tjong Kim Sang (2000) outlines alternative
versions of this tagging representation. First,
the B tag can be used for the \x0crst word of ev-
ery baseNP (IOB2 representation). Second, in-
stead of the B tag an E tag can be used to
mark the last word of a baseNP immediately
before another baseNP (IOE1). And third, the
E tag can be used for every noun phrase \x0cnal
word (IOE2). He used the Ramshaw and Mar-
cus (1995) representation as well (IOB1). We
will use these four tagging representations and
the O+C representation for the system-internal
combination experiments.
</bodyText>
<subsectionHeader confidence="0.985294">
2.3 Machine learning algorithms
</subsectionHeader>
<bodyText confidence="0.9963048">
This section contains a brief description of the
seven machine learning algorithms that we will
apply to the baseNP identi\x0ccation task: AL-
LiS, c5.0, IGTree, MaxEnt, MBL, MBSL and
SNoW.
</bodyText>
<sectionHeader confidence="0.526879" genericHeader="method">
ALLiS2 (Architecture for Learning Linguistic
</sectionHeader>
<bodyText confidence="0.995196333333333">
Structures) is a learning system which uses the-
ory re\x0cnement in order to learn non-recursive
NP and VP structures (D\x13
ejean, 2000). ALLiS
generates a regular expression grammar which
describes the phrase structure (NP or VP). This
grammar is then used by the CASS parser (Ab-
ney, 1996). Following the principle of theory re-
\x0cnement, the learning task is composed of two
steps. The \x0crst step is the generation of an
initial grammar. The generation of this gram-
mar uses the notion of default values and some
background knowledge which provides general
expectations concerning the inner structure of
NPs and VPs. This initial grammar provides
an incomplete and/or incorrect analysis of the
data. The second step is the re\x0cnement of this
grammar. During this step, the validity of the
rules of the initial grammar is checked and the
rules are improved (re\x0cned) if necessary. This
re\x0cnement relies on the use of two operations:
the contextualization (in which contexts such a
tag always belongs to the phrase) and lexical-
ization (use of information about the words and
not only about POS).
c5.03, a commercial version of c4.5 (Quin-
lan, 1993), performs top-down induction of de-
cision trees (tdidt). On the basis of an in-
stance base of examples, c5.0 constructs a deci-
sion tree which compresses the classi\x0ccation in-
formation in the instance base by exploiting dif-
ferences in relative importance of di\x0berent fea-
tures. Instances are stored in the tree as paths
</bodyText>
<page confidence="0.921415">
2
</page>
<bodyText confidence="0.865429">
A demo of the NP and VP chunker is available at
</bodyText>
<footnote confidence="0.6710096">
http://www.sfb441.unituebingen.de/~dejean/chunker.h
tml
3
Available from http://www.rulequest.com
\x0cof connected nodes ending in leaves which con-
</footnote>
<bodyText confidence="0.992772362745098">
tain classi\x0ccation information. Nodes are con-
nected via arcs denoting feature values. Feature
information gain (mutual information between
features and class) is used to determine the or-
der in which features are employed as tests at all
levels of the tree (Quinlan, 1993). With the full
input representation (words and POS tags), we
were not able to run complete experiments. We
therefore experimented only with the POS tags
(with a context of two left and right). We have
used the default parameter setting with decision
trees combined with value grouping.
We have used a nearest neighbor algorithm
(ib1-ig, here listed as MBL) and a decision tree
algorithm (IGTree) from the TiMBL learning
package (Daelemans et al., 1999b). Both algo-
rithms store the training data and classify new
items by choosing the most frequent classi\x0cca-
tion among training items which are closest to
this new item. Data items are represented as
sets of feature-value pairs. Each feature receives
a weight which is based on the amount of in-
formation which it provides for computing the
classi\x0ccation of the items in the training data.
ib1-ig uses these weights for computing the dis-
tance between a pair of data items and IGTree
uses them for deciding which feature-value de-
cisions should be made in the top nodes of the
decision tree (Daelemans et al., 1999b). We
will use their default parameters except for the
ib1-ig parameter for the number of examined
nearest neighbors (k) which we have set to 3
(Daelemans et al., 1999a). The classi\x0cers use a
left and right context of four words and part-
of-speech tags. For the four IO representations
we have used a second processing stage which
used a smaller context but which included in-
formation about the IO tags predicted by the
\x0crst processing phase (Tjong Kim Sang, 2000).
When building a classi\x0cer, one must gather
evidence for predicting the correct class of an
item from its context. The Maximum Entropy
(MaxEnt) framework is especially suited for
integrating evidence from various information
sources. Frequencies of evidence/class combi-
nations (called features) are extracted from a
sample corpus and considered to be properties
of the classi\x0ccation process. Attention is con-
strained to models with these properties. The
MaxEnt principle now demands that among all
the probability distributions that obey these
constraints, the most uniform is chosen. Dur-
ing training, features are assigned weights in
such a way that, given the MaxEnt principle,
the training data is matched as well as possible.
During evaluation it is tested which features are
active (i.e. a feature is active when the context
meets the requirements given by the feature).
For every class the weights of the active fea-
tures are combined and the best scoring class
is chosen (Berger et al., 1996). For the classi-
\x0cer built here the surrounding words, their POS
tags and baseNP tags predicted for the previous
words are used as evidence. A mixture of simple
features (consisting of one of the mentioned in-
formation sources) and complex features (com-
binations thereof) were used. The left context
never exceeded 3 words, the right context was
maximally 2 words. The model was calculated
using existing software (Dehaspe, 1997).
MBSL (Argamon et al., 1999) uses POS data
in order to identify baseNPs. Inference re-
lies on a memory which contains all the oc-
currences of POS sequences which appear in
the beginning, or the end, of a baseNP (in-
cluding complete phrases). These sequences
may include a few context tags, up to a pre-
speci\x0ced max context. During inference, MBSL
tries to \&amp;apos;tile\&amp;apos; each POS string with parts of
noun-phrases from the memory. If the string
could be fully covered by the tiles, it becomes
part of a candidate list, ambiguities between
candidates are resolved by a constraint propa-
gation algorithm. Adding a context extends the
possibilities for tiling, thereby giving more op-
portunities to better candidates. The approach
of MBSL to the problem of identifying baseNPs
is sequence-based rather than word-based, that
is, decisions are taken per POS sequence, or per
candidate, but not for a single word. In addi-
tion, the tiling process gives no preference to
any direction in the sentence. The tiles may be
of any length, up to the maximal length of a
phrase in the training data, which gives MBSL
a generalization power that compensates for the
setup of using only POS tags. The results pre-
sented here were obtained by optimizing MBSL
parameters based on 5-fold CV on the training
data.
SNoW uses the Open/Close model, described
in Mu~
noz et al. (1999). As is shown there, this
</bodyText>
<table confidence="0.997743875">
\x0cMBL MaxEnt IGTree
section 21 O C F\x0c=1 O C F\x0c=1 O C F\x0c=1
IOB1 97.81% 97.97% 91.68 97.90% 98.11% 92.43 96.62% 96.89% 87.88
IOB2 97.63% 97.96% 91.79 97.81% 98.14% 92.14 97.27% 97.30% 90.03
IOE1 97.80% 97.92% 91.54 97.88% 98.12% 92.37 95.88% 96.01% 82.80
IOE2 97.72% 97.94% 92.06 97.84% 98.12% 92.13 97.19% 97.62% 89.98
O+C 97.72% 98.04% 92.03 97.82% 98.15% 92.26 96.89% 97.49% 89.37
Majority 98.04% 98.20% 92.82 97.94% 98.24% 92.60 97.70% 97.99% 91.92
</table>
<tableCaption confidence="0.999701">
Table 1: The e\x0bects of system-internal combination by using di\x0berent output representations. A
</tableCaption>
<bodyText confidence="0.987093818181818">
straight-forward majority vote of the output yields better bracket accuracies and F\x0c=1 rates than
any included individual classi\x0cer. The bracket accuracies in the columns O and C show what
percentage of words was correctly classi\x0ced as baseNP start, baseNP end or neither.
model produced better results than the other
paradigm evaluated there, the Inside/Outside
paradigm. The Open/Close model consists of
two SNoW predictors, one of which predicts the
beginning of baseNPs (Open predictor), and the
other predicts the end of the phrase (Close pre-
dictor). The Open predictor is learned using
SNoW (Carlson et al., 1999; Roth, 1998) as a
function of features that utilize words and POS
tags in the sentence and, given a new sentence,
will predict for each word whether it is the \x0crst
word in the phrase or not. For each Open, the
Close predictoris learnedusingSNoW as a func-
tion of features that utilize the words in the sen-
tence, the POS tags and the open prediction. It
will predict, for each word, whether it can be
the end of the phrase, given the previously pre-
dicted Open. Each pair of predicted Open and
Close forms a candidate of a baseNP. These can-
didates may con
ict due to overlapping; at this
stage, a graph-based constraint satisfaction al-
gorithm that uses the con\x0cdence values SNoW
associates with its predictionsis employed. This
algorithm (&amp;quot;the combinator&amp;quot;) produces the list
of the \x0cnal baseNPs for each sentence. Details
of SNoW, its application in shallow parsing and
the combinator\&amp;apos;s algorithm are in Mu~
noz et al.
(1999).
</bodyText>
<subsectionHeader confidence="0.997803">
2.4 Combination techniques
</subsectionHeader>
<bodyText confidence="0.995961232142857">
At two points in our noun phrase recognition
process we willuse system combination. We will
start with system-internal combination: apply
the same learning algorithm to variants of the
task and combine the results. The approach
we have chosen here is the same as in Tjong
Kim Sang (2000): generate di\x0berent variants
of the task by using di\x0berent representations
of the output (IOB1, IOB2, IOE1, IOE2 and
O+C). The \x0cve outputs will converted to the
open bracket representation (O) and the close
bracket representation (C) and after this, the
most frequent of the \x0cve analyses of each word
will chosen (majority voting, see below). We
expect the systems which use this combination
phase to perform better than their individual
members (Tjong Kim Sang, 2000).
Our seven learners willgenerate di\x0berent clas-
si\x0ccations of the training data and we need to
\x0cnd out which combination techniques are most
appropriate. For the system-external combi-
nation experiment, we have evaluated di\x0berent
voting mechanisms, e\x0bectively the voting meth-
ods as described in Van Halteren et al. (1998).
In the \x0crst method each classi\x0ccation receives
the same weight and the most frequent classi\x0c-
cation is chosen (Majority). The second method
regards as the weight of each individual clas-
si\x0ccation algorithm its accuracy on some part
of the data, the tuning data (TotPrecision).
The third voting method computes the preci-
sion of each assigned tag per classi\x0cer and uses
this value as a weight for the classi\x0cer in those
cases that it chooses the tag (TagPrecision).
The fourth method uses both the precision of
each assigned tag and the recall of the com-
peting tags (Precision-Recall). Finally, the \x0cfth
method uses not only a weight for the current
classi\x0ccation but it also computes weights for
other possible classi\x0ccations. The other classi-
\x0ccations are determined by examining the tun-
\x0cing data and registering the correct values for
every pair of classi\x0cer results (pair-wise voting,
see Van Halteren et al. (1998) for an elaborate
explanation).
Apart from these \x0cve voting methods we have
also processed the output streams with two clas-
si\x0cers: MBL and IGTree. This approach is
called classi\x0cer stacking. Like Van Halteren et
al. (1998), we have used di\x0berent input ver-
sions: one containing only the classi\x0cer output
and another containing both classi\x0cer output
and a compressed representation of the data
item under consideration. For the latter pur-
pose we have used the part-of-speech tag of the
current word.
</bodyText>
<sectionHeader confidence="0.862823" genericHeader="method">
3 Results4
</sectionHeader>
<bodyText confidence="0.98645924137931">
We want to \x0cnd out whether system combi-
nation could improve performance of baseNP
recognition and, if this is the fact, we want to
select the best combination technique. For this
purpose we have performed an experiment with
sections 15-18 of the WSJ part of the Penn Tree-
bank as training data (211727 tokens) and sec-
tion 21 as test data (40039 tokens). Like the
data used by Ramshaw and Marcus (1995), this
data was retagged by the Brill tagger in order
to obtain realistic part-of-speech (POS) tags5.
The data was segmented into baseNP parts and
non-baseNP parts in a similar fashion as the
data used by Ramshaw and Marcus (1995). Of
the training data, only 90% was used for train-
ing. The remaining 10% was used as tuning
data for determining the weights of the combi-
nation techniques.
For three classi\x0cers (MBL, MaxEnt and
IGTree) we have used system-internal combi-
nation. These learning algorithms have pro-
cessed \x0cve di\x0berent representations of the out-
put (IOB1, IOB2, IOE1, IOE2 and O+C) and
the results have been combined with majority
voting. The test data results can be found in
Table 1. In all cases, the combined results were
better than that of the best included system.
The resultsof ALLiS,c5.0, MBSL andSNoW
have been converted to the O and the C repre-
</bodyText>
<page confidence="0.852409">
4
</page>
<footnote confidence="0.3921615">
Detailed results of our experiments are available on
http://lcg-www.uia.ac.be/~erikt/npcombi/
</footnote>
<page confidence="0.958937">
5
</page>
<bodyText confidence="0.98454325">
The retagging was necessary to assure that the per-
formance rates obtained here would be similar to rates
obtained for texts for which no Treebank POS tags are
available.
</bodyText>
<table confidence="0.997467863636364">
section 21 O C F\x0c=1
Classi\x0cer
ALLiS 97.87% 98.08% 92.15
c5.0 97.05% 97.76% 89.97
IGTree 97.70% 97.99% 91.92
MaxEnt 97.94% 98.24% 92.60
MBL 98.04% 98.20% 92.82
MBSL 97.27% 97.66% 90.71
SNoW 97.78% 97.68% 91.87
Simple Voting
Majority 98.08% 98.21% 92.95
TotPrecision 98.08% 98.21% 92.95
TagPrecision 98.08% 98.21% 92.95
Precision-Recall 98.08% 98.21% 92.95
Pairwise Voting
TagPair 98.13% 98.23% 93.07
Memory-Based
Tags 98.24% 98.35% 93.39
Tags + POS 98.14% 98.33% 93.24
Decision Trees
Tags 98.24% 98.35% 93.39
Tags + POS 98.13% 98.32% 93.21
</table>
<tableCaption confidence="0.993692">
Table 2: Bracket accuracies and F\x0c=1 scores
</tableCaption>
<bodyText confidence="0.998456083333333">
for section WSJ 21 of the Penn Treebank with
seven individual classi\x0cers and combinations of
them. Each combination performs better than
its best individual member. The stacked classi-
\x0cers without context information perform best.
sentation. Together with the bracket represen-
tations of the other three techniques, this gave
us a total of seven O results and seven C results.
These two data streams have been combined
with the combination techniques described in
section 2.4. After this, we built baseNPs from
the O and C results of each combination tech-
nique, like described in section 2.2. The bracket
accuracies and the F\x0c=1 scores for test data can
be found in Table 2.
All combinations improve the results of the
best individual classi\x0cer. The best results were
obtained with a memory-based stacked classi-
\x0cer. This is di\x0berent from the combination re-
sults presented in Van Halteren et al. (1998),
in which pairwise voting performed best. How-
ever, in their later work stacked classi\x0cers out-
perform voting methods as well (Van Halteren
et al., to appear).
</bodyText>
<table confidence="0.986499714285714">
\x0csection 20 accuracy precision recall F\x0c=1
Best-\x0cve combination O:98.32% C:98.41% 94.18% 93.55% 93.86
Tjong Kim Sang (2000) O:98.10% C:98.29% 93.63% 92.89% 93.26
Mu~
noz et al. (1999) O:98.1% C:98.2% 92.4% 93.1% 92.8
Ramshaw and Marcus (1995) IOB1:97.37% 91.80% 92.27% 92.03
Argamon et al. (1999) - 91.6% 91.6% 91.6
</table>
<tableCaption confidence="0.7655715">
Table 3: The overall performance of the majority voting combination of our best \x0cve systems
(selected on tuning data performance) applied to the standard data set put forward by Ramshaw
</tableCaption>
<bodyText confidence="0.999064882352941">
and Marcus (1995) together with an overview of earlier work. The accuracy scores indicate how
often a word was classi\x0ced correctly with the representation used (O, C or IOB1). The combined
system outperforms all earlier reported results for this data set.
Based on an earlier combination study
(Tjong Kim Sang, 2000) we had expected the
voting methods to do better. We suspect that
their performance is below that of the stacked
classi\x0cers because the di\x0berence between the
best and the worst individual system is larger
than in our earlier study. We assume that the
voting methods might perform better if they
were only applied to the classi\x0cers that per-
form well on this task. In order to test this
hypothesis, we have repeated the combination
experiments with the best n classi\x0cers, where
n took values from 3 to 6 and the classi\x0cers
were ranked based on their performance on the
tuning data. The best performances were ob-
tained with \x0cve classi\x0cers: F\x0c=1=93.44 for all
\x0cve voting methods with the best stacked classi-
\x0cer reaching 93.24. With the top \x0cve classi\x0cers,
the voting methods outperform the best combi-
nation with seven systems6. Adding extra clas-
si\x0ccation results to a good combination system
should not make overall performance worse so
it is clear that there is some room left for im-
provement of our combination algorithms.
We conclude that the best results in this
task can be obtained with the simplest voting
method, majority voting, applied to the best
\x0cve of our classi\x0cers. Our next task was to
apply the combination approach to a standard
data set so that we could compare our results
with other work. For this purpose we have used
</bodyText>
<page confidence="0.970049">
6
</page>
<bodyText confidence="0.999302821428572">
We are unaware of a good method for determining
the signi\x0ccance of F\x0c=1 di\x0berences but we assume that
this F\x0c=1 di\x0berence is not signi\x0ccant. However, we be-
lieve that the fact that more combination methods per-
form well, shows that it easier to get a good performance
out of the best \x0cve systems than with all seven.
the data put forward by Ramshaw and Marcus
(1995). Again, only 90% of the training data
was used for training while the remaining 10%
was reserved for ranking the classi\x0cers. The
seven learners were trained with the same pa-
rameters as in the previous experiment. Three
of the classi\x0cers (MBL, MaxEnt and IGTree)
used system-internal combination by processing
di\x0berent output representations.
The classi\x0cer output was converted to the
O and the C representation. Based on the
tuning data performance, the classi\x0cers ALLiS,
igtree, MaxEnt, MBL and SNoW were se-
lected for being combined with majority vot-
ing. After this, the resulting O and C repre-
sentations were combined to baseNPs by using
the method described in section 2.2. The re-
sults can be found in Table 3. Our combined
system obtains an F\x0c=1 score of 93.86 which
corresponds to an 8% error reduction compared
with the best published result for this data set
(93.26).
</bodyText>
<sectionHeader confidence="0.990072" genericHeader="conclusions">
4 Concluding remarks
</sectionHeader>
<bodyText confidence="0.99803084375">
In this paper we have examined two methods for
combining the results of machine learning algo-
rithms for identifying base noun phrases. In the
\x0crst method, the learnerprocessed di\x0berent out-
put data representations and the results were
combined by majority voting. This approach
yielded better results than the best included
classi\x0cer. In the second combination approach
we have combined the results of seven learning
systems (ALLiS, c5.0, IGTree, MaxEnt, MBL,
MBSL and SNoW). Here we have tested dif-
ferent combination methods. Each combination
\x0cmethod outperformed the best individual learn-
ing algorithm and a majority vote of the top
\x0cve systems performed best. We have applied
this approach of system-internal and system-
external combination to a standard data set for
base noun phrase identi\x0ccation and the perfor-
mance of our system was better than any other
published result for this data set.
Our study shows that the combination meth-
ods that we have tested are sensitive for the in-
clusion of classi\x0cer results of poor quality. This
leaves room for improvement of our results by
evaluating other combinators. Another interest-
ing approach which might lead to a better per-
formance is taking into account more context
information, for example by combining com-
plete phrases instead of independent brackets.
It would also be worthwhile to evaluate using
more elaborate methods for building baseNPs
out of open and close bracket candidates.
</bodyText>
<sectionHeader confidence="0.919038" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.856475833333333">
D\x13
ejean, Koeling and Tjong Kim Sang are
funded by the TMR network Learning Compu-
tational Grammars7. Punyakanok and Roth are
supported by NFS grants IIS-9801638 and SBR-
9873450.
</bodyText>
<sectionHeader confidence="0.972302" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998557619047619">
Steven Abney. 1996. Partial parsing via \x0cnite-
state cascades. In In Proceedings of the ESS-
LLI \&amp;apos;96 Robust Parsing Workshop.
Shlomo Argamon, Ido Dagan, and Yuval Kry-
molowski. 1999. A memory-based approach
to learningshallow natural language patterns.
Journal of Experimental and Theoretical AI,
11(3).
Adam L. Berger, Stephen A. DellaPietra, and
Vincent J. DellaPietra. 1996. A maximum
entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1).
Eric Brill and Jun Wu. 1998. Classi\x0cer com-
bination for improved lexical disambiguation.
In Proceedings of COLING-ACL \&amp;apos;98. Associ-
ation for Computational Linguistics.
A. Carlson, C. Cumby, J. Rosen, and
D. Roth. 1999. The SNoW learning archi-
tecture. Technical Report UIUCDCS-R-99-
2101, UIUC Computer Science Department,
May.
</reference>
<page confidence="0.950548">
7
</page>
<reference confidence="0.989881611111111">
http://lcg-www.uia.ac.be/
Walter Daelemans, Antal van den Bosch, and
Jakub Zavrel. 1999a. Forgetting exceptions
is harmful in language learning. Machine
Learning, 34(1).
Walter Daelemans, Jakub Zavrel, Ko van der
Sloot, and Antal van den Bosch. 1999b.
TiMBL: Tilburg Memory Based Learner, ver-
sion 2.0, Reference Guide. ILK Technical
Report 99-01. http://ilk.kub.nl/.
Luc Dehaspe. 1997. Maximum entropy model-
ing with clausal constraints. In Proceedings of
the 7th International Workshop on Inductive
Logic Programming.
Herv\x13
e D\x13
ejean. 2000. Theory re\x0cnement and
natural language processing. In Proceedings
of the Coling2000. Association for Computa-
tional Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of english: the penn
treebank. Computational Linguistics, 19(2).
Marcia Munoz, Vasin Punyakanok, Dan Roth,
and Dav Zimak. 1999. A learning ap-
proach to shallow parsing. In Proceedings of
EMNLP-WVLC\&amp;apos;99. Association for Compu-
tational Linguistics.
J. Ross Quinlan. 1993. c4.5: Programs for Ma-
chine Learning. Morgan Kaufmann.
Lance A. Ramshaw and Mitchell P. Marcus.
1995. Text chunking using transformation-
based learning. In Proceedings of the Third
ACL Workshop on Very Large Corpora. As-
sociation for Computational Linguistics.
D. Roth. 1998. Learning to resolve natural lan-
guage ambiguities: A uni\x0ced approach. In
AAAI-98.
Erik F. Tjong Kim Sang. 2000. Noun phrase
recognition by system combination. In Pro-
ceedings of the ANLP-NAACL-2000. Seattle,
Washington, USA. Morgan Kaufman Pub-
lishers.
Hans van Halteren, Jakub Zavrel, and Wal-
ter Daelemans. 1998. Improving data driven
wordclass tagging by system combination. In
Proceedings of COLING-ACL \&amp;apos;98. Associa-
tion for Computational Linguistics.
Hans van Halteren, Jakub Zavrel, and Walter
Daelemans. to appear. Improving accuracy
in nlp through combination of machine learn-
ing systems.
\x0c&amp;apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.056628">
<title confidence="0.981388">b&amp;apos;Applying System Combination to Base Noun Phrase Identi\x0ccation</title>
<author confidence="0.878574">Erik F Tjong Kim Sang\xb</author>
<author confidence="0.878574">Walter Daelemans\xb</author>
<author confidence="0.878574">Herv\x</author>
<affiliation confidence="0.526367">e D\x13</affiliation>
<email confidence="0.576408">ejean\x1c,</email>
<author confidence="0.944633">Rob Koeling</author>
<note confidence="0.869316666666667">Yuval Krymolowski\x0c, Vasin Punyakanok\x13, Dan Roth\x13 \x0bUniversity of Antwerp \x1cUniversit\x7f at T\x7f</note>
<email confidence="0.815391">ubingen</email>
<affiliation confidence="0.725248">SRI Cambridge Universiteitsplein 1 Kleine Wilhelmstra\x19e 113 23 Millers Yard,Mill Lane</affiliation>
<address confidence="0.926617">B-2610 Wilrijk, Belgium D-72074 T\x7f ubingen, Germany Cambridge, CB2 1RQ, UK</address>
<email confidence="0.880113">ferikt,daelemg@uia.ua.ac.bedejean@sfs.nphil.uni-tuebingen.dekoeling@cam.sri.com</email>
<affiliation confidence="0.85095">x0cBar-Ilan University \x13University of Illinois</affiliation>
<address confidence="0.813906">Ramat Gan, 52900, Israel 1304 W. Spring\x0celd Ave. yuvalk@macs.biu.ac.il Urbana, IL 61801, USA</address>
<email confidence="0.999325">fpunyakan,danrg@cs.uiuc.edu</email>
<abstract confidence="0.9988982">We use seven machine learning algorithms for one task: identifying base noun phrases. The results have been processed by di\x0berent system combination methods and all of these outperformed the best individual result. We have applied the seven learners with the best combinator, a majority vote of the top \x0cve systems, to a standard data set and managed to improve the best published result for this data set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Partial parsing via \x0cnitestate cascades. In</title>
<date>1996</date>
<booktitle>In Proceedings of the ESSLLI \&amp;apos;96 Robust Parsing Workshop.</booktitle>
<contexts>
<context position="6547" citStr="Abney, 1996" startWordPosition="1050" endWordPosition="1052">ntation for the system-internal combination experiments. 2.3 Machine learning algorithms This section contains a brief description of the seven machine learning algorithms that we will apply to the baseNP identi\x0ccation task: ALLiS, c5.0, IGTree, MaxEnt, MBL, MBSL and SNoW. ALLiS2 (Architecture for Learning Linguistic Structures) is a learning system which uses theory re\x0cnement in order to learn non-recursive NP and VP structures (D\x13 ejean, 2000). ALLiS generates a regular expression grammar which describes the phrase structure (NP or VP). This grammar is then used by the CASS parser (Abney, 1996). Following the principle of theory re\x0cnement, the learning task is composed of two steps. The \x0crst step is the generation of an initial grammar. The generation of this grammar uses the notion of default values and some background knowledge which provides general expectations concerning the inner structure of NPs and VPs. This initial grammar provides an incomplete and/or incorrect analysis of the data. The second step is the re\x0cnement of this grammar. During this step, the validity of the rules of the initial grammar is checked and the rules are improved (re\x0cned) if necessary. Thi</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Steven Abney. 1996. Partial parsing via \x0cnitestate cascades. In In Proceedings of the ESSLLI \&amp;apos;96 Robust Parsing Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
<author>Ido Dagan</author>
<author>Yuval Krymolowski</author>
</authors>
<title>A memory-based approach to learningshallow natural language patterns.</title>
<date>1999</date>
<journal>Journal of Experimental and Theoretical AI,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="11206" citStr="Argamon et al., 1999" startWordPosition="1802" endWordPosition="1805">the requirements given by the feature). For every class the weights of the active features are combined and the best scoring class is chosen (Berger et al., 1996). For the classi\x0cer built here the surrounding words, their POS tags and baseNP tags predicted for the previous words are used as evidence. A mixture of simple features (consisting of one of the mentioned information sources) and complex features (combinations thereof) were used. The left context never exceeded 3 words, the right context was maximally 2 words. The model was calculated using existing software (Dehaspe, 1997). MBSL (Argamon et al., 1999) uses POS data in order to identify baseNPs. Inference relies on a memory which contains all the occurrences of POS sequences which appear in the beginning, or the end, of a baseNP (including complete phrases). These sequences may include a few context tags, up to a prespeci\x0ced max context. During inference, MBSL tries to \&amp;apos;tile\&amp;apos; each POS string with parts of noun-phrases from the memory. If the string could be fully covered by the tiles, it becomes part of a candidate list, ambiguities between candidates are resolved by a constraint propagation algorithm. Adding a context extends the poss</context>
<context position="20806" citStr="Argamon et al. (1999)" startWordPosition="3348" endWordPosition="3351">t results were obtained with a memory-based stacked classi\x0cer. This is di\x0berent from the combination results presented in Van Halteren et al. (1998), in which pairwise voting performed best. However, in their later work stacked classi\x0cers outperform voting methods as well (Van Halteren et al., to appear). \x0csection 20 accuracy precision recall F\x0c=1 Best-\x0cve combination O:98.32% C:98.41% 94.18% 93.55% 93.86 Tjong Kim Sang (2000) O:98.10% C:98.29% 93.63% 92.89% 93.26 Mu~ noz et al. (1999) O:98.1% C:98.2% 92.4% 93.1% 92.8 Ramshaw and Marcus (1995) IOB1:97.37% 91.80% 92.27% 92.03 Argamon et al. (1999) - 91.6% 91.6% 91.6 Table 3: The overall performance of the majority voting combination of our best \x0cve systems (selected on tuning data performance) applied to the standard data set put forward by Ramshaw and Marcus (1995) together with an overview of earlier work. The accuracy scores indicate how often a word was classi\x0ced correctly with the representation used (O, C or IOB1). The combined system outperforms all earlier reported results for this data set. Based on an earlier combination study (Tjong Kim Sang, 2000) we had expected the voting methods to do better. We suspect that their </context>
</contexts>
<marker>Argamon, Dagan, Krymolowski, 1999</marker>
<rawString>Shlomo Argamon, Ido Dagan, and Yuval Krymolowski. 1999. A memory-based approach to learningshallow natural language patterns. Journal of Experimental and Theoretical AI, 11(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A DellaPietra</author>
<author>Vincent J DellaPietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="10747" citStr="Berger et al., 1996" startWordPosition="1728" endWordPosition="1731">tion process. Attention is constrained to models with these properties. The MaxEnt principle now demands that among all the probability distributions that obey these constraints, the most uniform is chosen. During training, features are assigned weights in such a way that, given the MaxEnt principle, the training data is matched as well as possible. During evaluation it is tested which features are active (i.e. a feature is active when the context meets the requirements given by the feature). For every class the weights of the active features are combined and the best scoring class is chosen (Berger et al., 1996). For the classi\x0cer built here the surrounding words, their POS tags and baseNP tags predicted for the previous words are used as evidence. A mixture of simple features (consisting of one of the mentioned information sources) and complex features (combinations thereof) were used. The left context never exceeded 3 words, the right context was maximally 2 words. The model was calculated using existing software (Dehaspe, 1997). MBSL (Argamon et al., 1999) uses POS data in order to identify baseNPs. Inference relies on a memory which contains all the occurrences of POS sequences which appear in</context>
</contexts>
<marker>Berger, DellaPietra, DellaPietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. DellaPietra, and Vincent J. DellaPietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Jun Wu</author>
</authors>
<title>Classi\x0cer combination for improved lexical disambiguation.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL \&amp;apos;98. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1190" citStr="Brill and Wu (1998)" startWordPosition="159" endWordPosition="162">llinois Ramat Gan, 52900, Israel 1304 W. Spring\x0celd Ave. yuvalk@macs.biu.ac.il Urbana, IL 61801, USA fpunyakan,danrg@cs.uiuc.edu Abstract We use seven machine learning algorithms for one task: identifying base noun phrases. The results have been processed by di\x0berent system combination methods and all of these outperformed the best individual result. We have applied the seven learners with the best combinator, a majority vote of the top \x0cve systems, to a standard data set and managed to improve the best published result for this data set. 1 Introduction Van Halteren et al. (1998) and Brill and Wu (1998) show that part-of-speech tagger performance can be improved by combining di\x0berent taggers. By using techniques such as majority voting, errors made by the minority of the taggers can be removed. Van Halteren et al. (1998) report that the results of such a combined approach can improve upon the accuracy error of the best individualsystem with as much as 19%. The positive e\x0bect of system combination for non-language processing tasks has been shown in a large body of machine learning work. In this paper we will use system combination for identifying base noun phrases (baseNPs). We will app</context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>Eric Brill and Jun Wu. 1998. Classi\x0cer combination for improved lexical disambiguation. In Proceedings of COLING-ACL \&amp;apos;98. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>C Cumby</author>
<author>J Rosen</author>
<author>D Roth</author>
</authors>
<title>The SNoW learning architecture.</title>
<date>1999</date>
<journal>http://lcg-www.uia.ac.be/ Walter Daelemans, Antal</journal>
<booktitle>Machine Learning,</booktitle>
<tech>Technical Report UIUCDCS-R-99-2101,</tech>
<volume>34</volume>
<issue>1</issue>
<institution>UIUC Computer Science Department,</institution>
<contexts>
<context position="13743" citStr="Carlson et al., 1999" startWordPosition="2216" endWordPosition="2219">t-forward majority vote of the output yields better bracket accuracies and F\x0c=1 rates than any included individual classi\x0cer. The bracket accuracies in the columns O and C show what percentage of words was correctly classi\x0ced as baseNP start, baseNP end or neither. model produced better results than the other paradigm evaluated there, the Inside/Outside paradigm. The Open/Close model consists of two SNoW predictors, one of which predicts the beginning of baseNPs (Open predictor), and the other predicts the end of the phrase (Close predictor). The Open predictor is learned using SNoW (Carlson et al., 1999; Roth, 1998) as a function of features that utilize words and POS tags in the sentence and, given a new sentence, will predict for each word whether it is the \x0crst word in the phrase or not. For each Open, the Close predictoris learnedusingSNoW as a function of features that utilize the words in the sentence, the POS tags and the open prediction. It will predict, for each word, whether it can be the end of the phrase, given the previously predicted Open. Each pair of predicted Open and Close forms a candidate of a baseNP. These candidates may con ict due to overlapping; at this stage, a gr</context>
</contexts>
<marker>Carlson, Cumby, Rosen, Roth, 1999</marker>
<rawString>A. Carlson, C. Cumby, J. Rosen, and D. Roth. 1999. The SNoW learning architecture. Technical Report UIUCDCS-R-99-2101, UIUC Computer Science Department, May. http://lcg-www.uia.ac.be/ Walter Daelemans, Antal van den Bosch, and Jakub Zavrel. 1999a. Forgetting exceptions is harmful in language learning. Machine Learning, 34(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 2.0, Reference Guide.</title>
<date>1999</date>
<tech>ILK Technical Report 99-01. http://ilk.kub.nl/.</tech>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 1999</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 1999b. TiMBL: Tilburg Memory Based Learner, version 2.0, Reference Guide. ILK Technical Report 99-01. http://ilk.kub.nl/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luc Dehaspe</author>
</authors>
<title>Maximum entropy modeling with clausal constraints.</title>
<date>1997</date>
<booktitle>In Proceedings of the 7th International Workshop on Inductive Logic Programming.</booktitle>
<contexts>
<context position="11177" citStr="Dehaspe, 1997" startWordPosition="1799" endWordPosition="1800">hen the context meets the requirements given by the feature). For every class the weights of the active features are combined and the best scoring class is chosen (Berger et al., 1996). For the classi\x0cer built here the surrounding words, their POS tags and baseNP tags predicted for the previous words are used as evidence. A mixture of simple features (consisting of one of the mentioned information sources) and complex features (combinations thereof) were used. The left context never exceeded 3 words, the right context was maximally 2 words. The model was calculated using existing software (Dehaspe, 1997). MBSL (Argamon et al., 1999) uses POS data in order to identify baseNPs. Inference relies on a memory which contains all the occurrences of POS sequences which appear in the beginning, or the end, of a baseNP (including complete phrases). These sequences may include a few context tags, up to a prespeci\x0ced max context. During inference, MBSL tries to \&amp;apos;tile\&amp;apos; each POS string with parts of noun-phrases from the memory. If the string could be fully covered by the tiles, it becomes part of a candidate list, ambiguities between candidates are resolved by a constraint propagation algorithm. Addi</context>
</contexts>
<marker>Dehaspe, 1997</marker>
<rawString>Luc Dehaspe. 1997. Maximum entropy modeling with clausal constraints. In Proceedings of the 7th International Workshop on Inductive Logic Programming.</rawString>
</citation>
<citation valid="true">
<title>Theory re\x0cnement and natural language processing.</title>
<date>2000</date>
<booktitle>Herv\x13 e D\x13 ejean.</booktitle>
<contexts>
<context position="4757" citStr="(2000)" startWordPosition="757" endWordPosition="757">, noun phrases are represented by bracket structures. It has been shown by Mu~ noz et al. (1999) that for baseNP recognition, the representation with brackets outperforms other data representations. One classi\x0cer can be trained to recognize open brackets (O) and another can handle close brackets (C). Their results can be combined by making pairs of open and close brackets with large probability scores. We have used this bracket representation (O+C) as well. However, we have not used the combination strategy from Mu~ noz et al. (1999) but instead used the strategy outlined in Tjong Kim Sang (2000): regard only the shortest possible phrases between candidate open and close brackets as base noun phrases. An alternative representation for baseNPs has been put forward by Ramshaw and Marcus (1995). They have de\x0cned baseNP recognition as a tagging task: words can be inside a baseNP (I) or outside a baseNP (O). In the case that one baseNP immediately follows another baseNP, the \x0crst word in the second baseNP receives tag B. Example: InO earlyI tradingI inO HongI KongI MondayB ,O goldI wasO quotedO atO $I 366.50I anB ounceI .O This set of three tags is su\x0ecient for encoding baseNP str</context>
<context position="14992" citStr="(2000)" startWordPosition="2431" endWordPosition="2431"> that uses the con\x0cdence values SNoW associates with its predictionsis employed. This algorithm (&amp;quot;the combinator&amp;quot;) produces the list of the \x0cnal baseNPs for each sentence. Details of SNoW, its application in shallow parsing and the combinator\&amp;apos;s algorithm are in Mu~ noz et al. (1999). 2.4 Combination techniques At two points in our noun phrase recognition process we willuse system combination. We will start with system-internal combination: apply the same learning algorithm to variants of the task and combine the results. The approach we have chosen here is the same as in Tjong Kim Sang (2000): generate di\x0berent variants of the task by using di\x0berent representations of the output (IOB1, IOB2, IOE1, IOE2 and O+C). The \x0cve outputs will converted to the open bracket representation (O) and the close bracket representation (C) and after this, the most frequent of the \x0cve analyses of each word will chosen (majority voting, see below). We expect the systems which use this combination phase to perform better than their individual members (Tjong Kim Sang, 2000). Our seven learners willgenerate di\x0berent classi\x0ccations of the training data and we need to \x0cnd out which com</context>
<context position="20633" citStr="(2000)" startWordPosition="3324" endWordPosition="3324">ket accuracies and the F\x0c=1 scores for test data can be found in Table 2. All combinations improve the results of the best individual classi\x0cer. The best results were obtained with a memory-based stacked classi\x0cer. This is di\x0berent from the combination results presented in Van Halteren et al. (1998), in which pairwise voting performed best. However, in their later work stacked classi\x0cers outperform voting methods as well (Van Halteren et al., to appear). \x0csection 20 accuracy precision recall F\x0c=1 Best-\x0cve combination O:98.32% C:98.41% 94.18% 93.55% 93.86 Tjong Kim Sang (2000) O:98.10% C:98.29% 93.63% 92.89% 93.26 Mu~ noz et al. (1999) O:98.1% C:98.2% 92.4% 93.1% 92.8 Ramshaw and Marcus (1995) IOB1:97.37% 91.80% 92.27% 92.03 Argamon et al. (1999) - 91.6% 91.6% 91.6 Table 3: The overall performance of the majority voting combination of our best \x0cve systems (selected on tuning data performance) applied to the standard data set put forward by Ramshaw and Marcus (1995) together with an overview of earlier work. The accuracy scores indicate how often a word was classi\x0ced correctly with the representation used (O, C or IOB1). The combined system outperforms all ear</context>
</contexts>
<marker>2000</marker>
<rawString>Herv\x13 e D\x13 ejean. 2000. Theory re\x0cnement and natural language processing. In Proceedings of the Coling2000. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3272" citStr="Marcus et al., 1993" startWordPosition="522" endWordPosition="525"> Task description Base noun phrases (baseNPs) are noun phrases which do not contain another noun phrase. For example, the sentence In [ early trading ] in [ Hong Kong ] [ Monday ] , [ gold ] was quoted at [ $ 366.50 ] [ an ounce ] . contains six baseNPs (marked as phrases between square brackets). The phrase $ 366.50 an ounce is a noun phrase as well. However, it is not a baseNP since it contains two other noun phrases. Two baseNP data sets have been put forward by Ramshaw and Marcus (1995). The main data set consist of four sections of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) as training material (sections 15-18, 211727 tokens) and one section as test material (section 20, 47377 tokens)1. The data contains words, their part-of-speech 1 This Ramshaw and Marcus (1995) baseNP data set is available via ftp://ftp.cis.upenn.edu/pub/chunker/ \x0c(POS) tags as computed by the Brill tagger and their baseNP segmentation as derived from the Treebank (with some modi\x0ccations). In the baseNP identi\x0ccation task, performance is measured with three rates. First, with the percentage of detected noun phrases that are correct (precision). Second, with the percentage of noun phr</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcia Munoz</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Dav Zimak</author>
</authors>
<title>A learning approach to shallow parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP-WVLC\&amp;apos;99. Association for Computational Linguistics.</booktitle>
<marker>Munoz, Punyakanok, Roth, Zimak, 1999</marker>
<rawString>Marcia Munoz, Vasin Punyakanok, Dan Roth, and Dav Zimak. 1999. A learning approach to shallow parsing. In Proceedings of EMNLP-WVLC\&amp;apos;99. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>c4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="7412" citStr="Quinlan, 1993" startWordPosition="1191" endWordPosition="1193">h provides general expectations concerning the inner structure of NPs and VPs. This initial grammar provides an incomplete and/or incorrect analysis of the data. The second step is the re\x0cnement of this grammar. During this step, the validity of the rules of the initial grammar is checked and the rules are improved (re\x0cned) if necessary. This re\x0cnement relies on the use of two operations: the contextualization (in which contexts such a tag always belongs to the phrase) and lexicalization (use of information about the words and not only about POS). c5.03, a commercial version of c4.5 (Quinlan, 1993), performs top-down induction of decision trees (tdidt). On the basis of an instance base of examples, c5.0 constructs a decision tree which compresses the classi\x0ccation information in the instance base by exploiting differences in relative importance of di\x0berent features. Instances are stored in the tree as paths 2 A demo of the NP and VP chunker is available at http://www.sfb441.unituebingen.de/~dejean/chunker.h tml 3 Available from http://www.rulequest.com \x0cof connected nodes ending in leaves which contain classi\x0ccation information. Nodes are connected via arcs denoting feature </context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Ross Quinlan. 1993. c4.5: Programs for Machine Learning. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text chunking using transformationbased learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third ACL Workshop on Very Large Corpora. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3147" citStr="Ramshaw and Marcus (1995)" startWordPosition="498" endWordPosition="501">ing algorithms that we will apply to the task. We will conclude with an overview of the combination methods that we will test. 2.1 Task description Base noun phrases (baseNPs) are noun phrases which do not contain another noun phrase. For example, the sentence In [ early trading ] in [ Hong Kong ] [ Monday ] , [ gold ] was quoted at [ $ 366.50 ] [ an ounce ] . contains six baseNPs (marked as phrases between square brackets). The phrase $ 366.50 an ounce is a noun phrase as well. However, it is not a baseNP since it contains two other noun phrases. Two baseNP data sets have been put forward by Ramshaw and Marcus (1995). The main data set consist of four sections of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) as training material (sections 15-18, 211727 tokens) and one section as test material (section 20, 47377 tokens)1. The data contains words, their part-of-speech 1 This Ramshaw and Marcus (1995) baseNP data set is available via ftp://ftp.cis.upenn.edu/pub/chunker/ \x0c(POS) tags as computed by the Brill tagger and their baseNP segmentation as derived from the Treebank (with some modi\x0ccations). In the baseNP identi\x0ccation task, performance is measured with three rat</context>
<context position="4956" citStr="Ramshaw and Marcus (1995)" startWordPosition="785" endWordPosition="789">a representations. One classi\x0cer can be trained to recognize open brackets (O) and another can handle close brackets (C). Their results can be combined by making pairs of open and close brackets with large probability scores. We have used this bracket representation (O+C) as well. However, we have not used the combination strategy from Mu~ noz et al. (1999) but instead used the strategy outlined in Tjong Kim Sang (2000): regard only the shortest possible phrases between candidate open and close brackets as base noun phrases. An alternative representation for baseNPs has been put forward by Ramshaw and Marcus (1995). They have de\x0cned baseNP recognition as a tagging task: words can be inside a baseNP (I) or outside a baseNP (O). In the case that one baseNP immediately follows another baseNP, the \x0crst word in the second baseNP receives tag B. Example: InO earlyI tradingI inO HongI KongI MondayB ,O goldI wasO quotedO atO $I 366.50I anB ounceI .O This set of three tags is su\x0ecient for encoding baseNP structures since these structures are nonrecursive and nonoverlapping. Tjong Kim Sang (2000) outlines alternative versions of this tagging representation. First, the B tag can be used for the \x0crst wo</context>
<context position="17688" citStr="Ramshaw and Marcus (1995)" startWordPosition="2855" endWordPosition="2858">x0cer output and another containing both classi\x0cer output and a compressed representation of the data item under consideration. For the latter purpose we have used the part-of-speech tag of the current word. 3 Results4 We want to \x0cnd out whether system combination could improve performance of baseNP recognition and, if this is the fact, we want to select the best combination technique. For this purpose we have performed an experiment with sections 15-18 of the WSJ part of the Penn Treebank as training data (211727 tokens) and section 21 as test data (40039 tokens). Like the data used by Ramshaw and Marcus (1995), this data was retagged by the Brill tagger in order to obtain realistic part-of-speech (POS) tags5. The data was segmented into baseNP parts and non-baseNP parts in a similar fashion as the data used by Ramshaw and Marcus (1995). Of the training data, only 90% was used for training. The remaining 10% was used as tuning data for determining the weights of the combination techniques. For three classi\x0cers (MBL, MaxEnt and IGTree) we have used system-internal combination. These learning algorithms have processed \x0cve di\x0berent representations of the output (IOB1, IOB2, IOE1, IOE2 and O+C)</context>
<context position="20752" citStr="Ramshaw and Marcus (1995)" startWordPosition="3340" endWordPosition="3343">e the results of the best individual classi\x0cer. The best results were obtained with a memory-based stacked classi\x0cer. This is di\x0berent from the combination results presented in Van Halteren et al. (1998), in which pairwise voting performed best. However, in their later work stacked classi\x0cers outperform voting methods as well (Van Halteren et al., to appear). \x0csection 20 accuracy precision recall F\x0c=1 Best-\x0cve combination O:98.32% C:98.41% 94.18% 93.55% 93.86 Tjong Kim Sang (2000) O:98.10% C:98.29% 93.63% 92.89% 93.26 Mu~ noz et al. (1999) O:98.1% C:98.2% 92.4% 93.1% 92.8 Ramshaw and Marcus (1995) IOB1:97.37% 91.80% 92.27% 92.03 Argamon et al. (1999) - 91.6% 91.6% 91.6 Table 3: The overall performance of the majority voting combination of our best \x0cve systems (selected on tuning data performance) applied to the standard data set put forward by Ramshaw and Marcus (1995) together with an overview of earlier work. The accuracy scores indicate how often a word was classi\x0ced correctly with the representation used (O, C or IOB1). The combined system outperforms all earlier reported results for this data set. Based on an earlier combination study (Tjong Kim Sang, 2000) we had expected t</context>
<context position="23102" citStr="Ramshaw and Marcus (1995)" startWordPosition="3730" endWordPosition="3733">ting method, majority voting, applied to the best \x0cve of our classi\x0cers. Our next task was to apply the combination approach to a standard data set so that we could compare our results with other work. For this purpose we have used 6 We are unaware of a good method for determining the signi\x0ccance of F\x0c=1 di\x0berences but we assume that this F\x0c=1 di\x0berence is not signi\x0ccant. However, we believe that the fact that more combination methods perform well, shows that it easier to get a good performance out of the best \x0cve systems than with all seven. the data put forward by Ramshaw and Marcus (1995). Again, only 90% of the training data was used for training while the remaining 10% was reserved for ranking the classi\x0cers. The seven learners were trained with the same parameters as in the previous experiment. Three of the classi\x0cers (MBL, MaxEnt and IGTree) used system-internal combination by processing di\x0berent output representations. The classi\x0cer output was converted to the O and the C representation. Based on the tuning data performance, the classi\x0cers ALLiS, igtree, MaxEnt, MBL and SNoW were selected for being combined with majority voting. After this, the resulting O </context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformationbased learning. In Proceedings of the Third ACL Workshop on Very Large Corpora. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to resolve natural language ambiguities: A uni\x0ced approach.</title>
<date>1998</date>
<booktitle>In AAAI-98.</booktitle>
<contexts>
<context position="13756" citStr="Roth, 1998" startWordPosition="2220" endWordPosition="2221">e of the output yields better bracket accuracies and F\x0c=1 rates than any included individual classi\x0cer. The bracket accuracies in the columns O and C show what percentage of words was correctly classi\x0ced as baseNP start, baseNP end or neither. model produced better results than the other paradigm evaluated there, the Inside/Outside paradigm. The Open/Close model consists of two SNoW predictors, one of which predicts the beginning of baseNPs (Open predictor), and the other predicts the end of the phrase (Close predictor). The Open predictor is learned using SNoW (Carlson et al., 1999; Roth, 1998) as a function of features that utilize words and POS tags in the sentence and, given a new sentence, will predict for each word whether it is the \x0crst word in the phrase or not. For each Open, the Close predictoris learnedusingSNoW as a function of features that utilize the words in the sentence, the POS tags and the open prediction. It will predict, for each word, whether it can be the end of the phrase, given the previously predicted Open. Each pair of predicted Open and Close forms a candidate of a baseNP. These candidates may con ict due to overlapping; at this stage, a graph-based con</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>D. Roth. 1998. Learning to resolve natural language ambiguities: A uni\x0ced approach. In AAAI-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Noun phrase recognition by system combination.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP-NAACL-2000.</booktitle>
<publisher>Morgan Kaufman Publishers.</publisher>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="4757" citStr="Sang (2000)" startWordPosition="756" endWordPosition="757">n 2.1, noun phrases are represented by bracket structures. It has been shown by Mu~ noz et al. (1999) that for baseNP recognition, the representation with brackets outperforms other data representations. One classi\x0cer can be trained to recognize open brackets (O) and another can handle close brackets (C). Their results can be combined by making pairs of open and close brackets with large probability scores. We have used this bracket representation (O+C) as well. However, we have not used the combination strategy from Mu~ noz et al. (1999) but instead used the strategy outlined in Tjong Kim Sang (2000): regard only the shortest possible phrases between candidate open and close brackets as base noun phrases. An alternative representation for baseNPs has been put forward by Ramshaw and Marcus (1995). They have de\x0cned baseNP recognition as a tagging task: words can be inside a baseNP (I) or outside a baseNP (O). In the case that one baseNP immediately follows another baseNP, the \x0crst word in the second baseNP receives tag B. Example: InO earlyI tradingI inO HongI KongI MondayB ,O goldI wasO quotedO atO $I 366.50I anB ounceI .O This set of three tags is su\x0ecient for encoding baseNP str</context>
<context position="9742" citStr="Sang, 2000" startWordPosition="1573" endWordPosition="1574"> and IGTree uses them for deciding which feature-value decisions should be made in the top nodes of the decision tree (Daelemans et al., 1999b). We will use their default parameters except for the ib1-ig parameter for the number of examined nearest neighbors (k) which we have set to 3 (Daelemans et al., 1999a). The classi\x0cers use a left and right context of four words and partof-speech tags. For the four IO representations we have used a second processing stage which used a smaller context but which included information about the IO tags predicted by the \x0crst processing phase (Tjong Kim Sang, 2000). When building a classi\x0cer, one must gather evidence for predicting the correct class of an item from its context. The Maximum Entropy (MaxEnt) framework is especially suited for integrating evidence from various information sources. Frequencies of evidence/class combinations (called features) are extracted from a sample corpus and considered to be properties of the classi\x0ccation process. Attention is constrained to models with these properties. The MaxEnt principle now demands that among all the probability distributions that obey these constraints, the most uniform is chosen. During t</context>
<context position="14992" citStr="Sang (2000)" startWordPosition="2430" endWordPosition="2431">rithm that uses the con\x0cdence values SNoW associates with its predictionsis employed. This algorithm (&amp;quot;the combinator&amp;quot;) produces the list of the \x0cnal baseNPs for each sentence. Details of SNoW, its application in shallow parsing and the combinator\&amp;apos;s algorithm are in Mu~ noz et al. (1999). 2.4 Combination techniques At two points in our noun phrase recognition process we willuse system combination. We will start with system-internal combination: apply the same learning algorithm to variants of the task and combine the results. The approach we have chosen here is the same as in Tjong Kim Sang (2000): generate di\x0berent variants of the task by using di\x0berent representations of the output (IOB1, IOB2, IOE1, IOE2 and O+C). The \x0cve outputs will converted to the open bracket representation (O) and the close bracket representation (C) and after this, the most frequent of the \x0cve analyses of each word will chosen (majority voting, see below). We expect the systems which use this combination phase to perform better than their individual members (Tjong Kim Sang, 2000). Our seven learners willgenerate di\x0berent classi\x0ccations of the training data and we need to \x0cnd out which com</context>
<context position="20633" citStr="Sang (2000)" startWordPosition="3323" endWordPosition="3324"> bracket accuracies and the F\x0c=1 scores for test data can be found in Table 2. All combinations improve the results of the best individual classi\x0cer. The best results were obtained with a memory-based stacked classi\x0cer. This is di\x0berent from the combination results presented in Van Halteren et al. (1998), in which pairwise voting performed best. However, in their later work stacked classi\x0cers outperform voting methods as well (Van Halteren et al., to appear). \x0csection 20 accuracy precision recall F\x0c=1 Best-\x0cve combination O:98.32% C:98.41% 94.18% 93.55% 93.86 Tjong Kim Sang (2000) O:98.10% C:98.29% 93.63% 92.89% 93.26 Mu~ noz et al. (1999) O:98.1% C:98.2% 92.4% 93.1% 92.8 Ramshaw and Marcus (1995) IOB1:97.37% 91.80% 92.27% 92.03 Argamon et al. (1999) - 91.6% 91.6% 91.6 Table 3: The overall performance of the majority voting combination of our best \x0cve systems (selected on tuning data performance) applied to the standard data set put forward by Ramshaw and Marcus (1995) together with an overview of earlier work. The accuracy scores indicate how often a word was classi\x0ced correctly with the representation used (O, C or IOB1). The combined system outperforms all ear</context>
</contexts>
<marker>Sang, 2000</marker>
<rawString>Erik F. Tjong Kim Sang. 2000. Noun phrase recognition by system combination. In Proceedings of the ANLP-NAACL-2000. Seattle, Washington, USA. Morgan Kaufman Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Improving data driven wordclass tagging by system combination.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL \&amp;apos;98. Association for Computational Linguistics. Hans</booktitle>
<marker>van Halteren, Zavrel, Daelemans, 1998</marker>
<rawString>Hans van Halteren, Jakub Zavrel, and Walter Daelemans. 1998. Improving data driven wordclass tagging by system combination. In Proceedings of COLING-ACL \&amp;apos;98. Association for Computational Linguistics. Hans van Halteren, Jakub Zavrel, and Walter Daelemans. to appear. Improving accuracy in nlp through combination of machine learning systems. \x0c&amp;apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>