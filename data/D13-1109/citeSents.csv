Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing CITATION or pivoting through related languages (CITATION; CITATION),,
We test our model using three NEW-domain corpora: (1) the EMEA medical corpus CITATION, (2) a corpus of scientific abstracts (CITATIONa), and (3) a corpus of translated movie subtitles CITATION,,
4.2 Machine translation We use the Moses MT framework CITATION to build a standard statistical phrase-based MT model using our OLD-domain training data,,
Recent work has identified NTS words in NEW-domain corpora (CITATIONb), and in future work we plan to incorporate discovered translations for such words into MT,,
We test our model using three NEW-domain corpora: (1) the EMEA medical corpus CITATION, (2) a corpus of scientific abstracts (CITATIONa), and (3) a corpus of translated movie subtitles CITATION,,
4.2 Machine translation We use the Moses MT framework CITATION to build a standard statistical phrase-based MT model using our OLD-domain training data,,
Recent work has identified NTS words in NEW-domain corpora (CITATIONb), and in future work we plan to incorporate discovered translations for such words into MT,,
Features are combined using a log-linear model optimized for BLEU, using the n-best batch MIRA algorithm CITATION,,
Finally, we form a new estimate of the joint distribution by moving pnew 1:k1 in the direction of pnew k , via: pnew 1:k = pnew 1:k1 + u h pnew k pnew 1:k1 i The learning rate u is set to 0.001.5 This incremental update of parameters is similar to the margin infused relaxed algorithm (MIRA) CITATION,,
CITATION take a fundamentally different approach and construct a graph using source language monolingual text and identify translations for source language OOV words by pivoting through paraphrases,,
Della CITATION and CITATION explore models for combining foreground and background distributions for the purpose of language modeling, and their approaches are somewhat similar to ours,,
rns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing CITATION or pivoting through related languages (CITATION; CITATION),,
CITATION take a fundamentally different approach and construct a graph using source language monolingual text and identify translations for source language OOV words by pivoting through paraphrases,,
Della CITATION and CITATION explore models for combining foreground and background distributions for the purpose of language modeling, and their approaches are somewhat similar to ours,,
A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
Considering the marginal distributions from each document pair to be a separate subproblem, we could approach the global objective of satisfying all subproblems as an instance of dual decomposition CITATION or ADMM (CITATION; CITATION),,
Considering the marginal distributions from each document pair to be a separate subproblem, we could approach the global objective of satisfying all subproblems as an instance of dual decomposition CITATION or ADMM (CITATION; CITATION),,
A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
2 Previous Work In prior work CITATION, we presented a systematic analysis of errors that occur when shift2 www.wikipedia.org ing domains in machine translation,,
A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
We test our model using three NEW-domain corpora: (1) the EMEA medical corpus CITATION, (2) a corpus of scientific abstracts (CITATIONa), and (3) a corpus of translated movie subtitles CITATION,,
4.2 Machine translation We use the Moses MT framework CITATION to build a standard statistical phrase-based MT model using our OLD-domain training data,,
As CITATION explains, a texts domain is most related to its topic, while a texts register is related to its type and purpose,,
A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
CITATION) may yield additional performance gains, but, qualitatively, the ranked Wikipedia pages seemed reasonable to the authors,,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing CITATION or pivoting through related languages (CITATION; CITATION),,
 monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing CITATION or pivoting through related languages (CITATION; CITATION),,
For the OLD-domain joint distribution, we use a simple maximum likelihood estimate based on non-null automatic word alignments (using grow-diag-final GIZA++ alignments CITATION),,
A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
The intuition that we should match marginal distributions is similar to work using no example labels but only label proportions to estimate labels, for example in CITATION,,
A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
ethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing CITATION or pivoting through related languages (CITATION; CITATION),,
Following prior work CITATION, we would like the matrix to remain as sparse as possible; that is, introduce the smallest number of new translation pairs necessary,,
CITATION take a fundamentally different approach and construct a graph using source language monolingual text and identify translations for source language OOV words by pivoting through paraphrases,,
Della CITATION and CITATION explore models for combining foreground and background distributions for the purpose of language modeling, and their approaches are somewhat similar to ours,,
We weight D(w) entries with BM25 CITATION,,
A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
CITATION, for example, mine parallel text from comparable corpora,,
Parallel sentences are informative but also rare: in the data released by CITATION, only 21% of the foreign sentences have a near-parallel counterpart in the English article.1 Furthermore, these sentences do not capture all terms,,
Considering the marginal distributions from each document pair to be a separate subproblem, we could approach the global objective of satisfying all subproblems as an instance of dual decomposition CITATION or ADMM (CITATION; CITATION),,
We test our model using three NEW-domain corpora: (1) the EMEA medical corpus CITATION, (2) a corpus of scientific abstracts (CITATIONa), and (3) a corpus of translated movie subtitles CITATION,,
4.2 Machine translation We use the Moses MT framework CITATION to build a standard statistical phrase-based MT model using our OLD-domain training data,,
