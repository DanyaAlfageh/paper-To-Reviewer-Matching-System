4.2 Machine translation We use the Moses MT framework CITATION to build a standard statistical phrase-based MT model using our OLD-domain training data,,
Following prior work CITATION, we would like the matrix to remain as sparse as possible; that is, introduce the smallest number of new translation pairs necessary,,
Finally, we form a new estimate of the joint distribution by moving pnew 1:k1 in the direction of pnew k , via: pnew 1:k = pnew 1:k1 + u h pnew k pnew 1:k1 i The learning rate u is set to 0.001.5 This incremental update of parameters is similar to the margin infused relaxed algorithm (MIRA) CITATION,,
rns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
CITATION, for example, mine parallel text from comparable corpora,,
It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing CITATION or pivoting through related languages (CITATION; CITATION),,
Features are combined using a log-linear model optimized for BLEU, using the n-best batch MIRA algorithm CITATION,,
The intuition that we should match marginal distributions is similar to work using no example labels but only label proportions to estimate labels, for example in CITATION,,
CITATION) may yield additional performance gains, but, qualitatively, the ranked Wikipedia pages seemed reasonable to the authors,,
 monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
We weight D(w) entries with BM25 CITATION,,
As CITATION explains, a texts domain is most related to its topic, while a texts register is related to its type and purpose,,
ethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Della CITATION and CITATION explore models for combining foreground and background distributions for the purpose of language modeling, and their approaches are somewhat similar to ours,,
2 Previous Work In prior work CITATION, we presented a systematic analysis of errors that occur when shift2 www.wikipedia.org ing domains in machine translation,,
Parallel sentences are informative but also rare: in the data released by CITATION, only 21% of the foreign sentences have a near-parallel counterpart in the English article.1 Furthermore, these sentences do not capture all terms,,
Considering the marginal distributions from each document pair to be a separate subproblem, we could approach the global objective of satisfying all subproblems as an instance of dual decomposition CITATION or ADMM (CITATION; CITATION),,
Recent work has identified NTS words in NEW-domain corpora (CITATIONb), and in future work we plan to incorporate discovered translations for such words into MT,,
CITATION take a fundamentally different approach and construct a graph using source language monolingual text and identify translations for source language OOV words by pivoting through paraphrases,,
A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Our approach bears some similarity to CITATION, CITATION, and CITATION; we learn a translation distribution despite a lack of parallel data,,
For the OLD-domain joint distribution, we use a simple maximum likelihood estimate based on non-null automatic word alignments (using grow-diag-final GIZA++ alignments CITATION),,
We test our model using three NEW-domain corpora: (1) the EMEA medical corpus CITATION, (2) a corpus of scientific abstracts (CITATIONa), and (3) a corpus of translated movie subtitles CITATION,,
