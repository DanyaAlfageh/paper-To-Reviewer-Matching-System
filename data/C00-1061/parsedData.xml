<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.754777">
b&apos;English-to-Korean Transliteration using Multiple Unbounded
Overlapping Phoneme Chunks
</title>
<author confidence="0.700868">
In-Ho Kang and GilChang Kim
</author>
<affiliation confidence="0.960922">
Department of Computer Science
Korea Advanced Institute of Science and Technology
</affiliation>
<sectionHeader confidence="0.974257" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.988614125">
We present in this paper the method of
English-to-Korean(E-K) transliteration and
back-transliteration. In Korean technical
documents, many English words are translit-
erated into Korean words in various forms in
diverse ways. As English words and Korean
transliterations are usually technical terms and
proper nouns, it is hard to \x0cnd a transliteration
and its variations in a dictionary. Therefore
an automatic transliteration system is needed
to \x0cnd the transliterations of English words
without manual intervention.
To explain E-K transliteration phenomena,
we use phoneme chunks that do not have a
length limit. By applying phoneme chunks,
we combine di\x0berent length information with
easy. The E-K transliteration method has
three steps. In the \x0crst, we make a phoneme
network that shows all possible transliterations
of the given word. In the second step, we apply
phoneme chunks, extracted from training data,
to calculate the reliability of each possible
transliteration. Then we obtain probable
transliterations of the given English word.
</bodyText>
<sectionHeader confidence="0.998124" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990673333333333">
In Korean technical documents, many English
words are used in their original forms. But
sometimes they are transliterated into Korean
in di\x0berent forms. Ex. 1, 2 show the examples
of various transliterations in KTSET 2.0(Park
et al., 1996).
</bodyText>
<figure confidence="0.981558555555556">
(1) data
(a) HL\x19$\x1e&amp;quot;(teyitha) [1,033]1
(b) HL\x19$\&apos;)(teyithe) [527]
1
the frequency in KTSET
(2) digital
(a) \x11$\x1a$OUe(ticithul) [254]
(b) \x11$\x1a$\x1e&amp;quot;(tichithal) [7]
(c) \x11$\x1a$*,(ticithel) [6]
</figure>
<bodyText confidence="0.986271866666667">
These various transliterations are not negligi-
ble for natural language processing, especiallyin
information retrieval. Because same words are
treated as di\x0berent ones, the calculation based
on the frequency of word would produce mis-
leading results. An experiment shows that the
e\x0bectiveness of information retrieval increases
when various forms including English words are
treated equivalently(Jeong et al., 1997).
We may use a dictionary, to \x0cnd a correct
transliteration and its variations. But it is not
feasible because transliterated words are usually
technical terms and proper nouns that have rich
productivity. Therefore an automatic translit-
eration system is needed to \x0cnd transliterations
without manual intervention.
There have been some studies on E-K
transliteration. They tried to explain translit-
eration as phoneme-per-phoneme or alphabet-
per-phoneme classi\x0ccation problem. They re-
stricted the information length to two or three
units before and behind an input unit. In fact,
many linguistic phenomena involved in the E-K
transliteration are expressed in terms of units
that exceed a phoneme and an alphabet. For
example, `a\&apos; in `ace\&apos; is transliterated into \\9L
\x19$(eyi)&amp;quot; but in `acetic\&apos;, \\\x19)(e)&amp;quot; and in `acetone\&apos;,
\\\x19&amp;quot;(a)&amp;quot;. If we restrict the information length
to two alphabets, then we cannot explain these
phenomena. Three words get the same result
</bodyText>
<listItem confidence="0.811001">
for `a\&apos;.
(3) ace 9L\x19$ah(eyisu)
(4) acetic \x19)\x17$\x1e$ (esithik)
\x0c(5) acetone \x19&amp;quot;8LOOc(aseython)
</listItem>
<bodyText confidence="0.99916725">
In this paper, we propose the E-K transliter-
ation model based on phoneme chunks that
do not have a length limit and can explain
transliteration phenomena in some degree of
reliability. Not a alphabet-per-alphabet but a
chunk-per-chunk classi\x0ccation problem.
This paper is organized as follows. In section
2, we survey an E-K transliteration. In section
3, we propose phoneme chunks based translit-
eration and back-transliteration. In Section 4,
the results of experiments are presented. Fi-
nally, the conclusion follows in section 5.
</bodyText>
<sectionHeader confidence="0.989934" genericHeader="method">
2 English-to-Korean transliteration
</sectionHeader>
<bodyText confidence="0.999231642857143">
E-K transliteration models are classi\x0ced in two
methods: the pivot method and the direct
method. In the pivot method, transliteration
is done in two steps: converting English words
into pronunciation symbols and then converting
these symbols into Korean words by using the
Korean standard conversion rule. In the direct
method, English words are directly converted to
Korean words without intermediate steps. An
experiment shows that the direct method is bet-
ter than the pivot method in \x0cnding variations
of a transliteration(Lee and Choi, 1998). Statis-
tical information, neural network and decision
tree were used to implement the direct method.
</bodyText>
<subsectionHeader confidence="0.983668">
2.1 Statistical Transliteration method
</subsectionHeader>
<bodyText confidence="0.997300818181818">
An English word is divided into phoneme se-
quence or alphabet sequence as e1;e2;::: ;en.
Then a corresponding Korean word is rep-
resented as k1;k2;::: ;kn. If a correspond-
ing Korean character (ki) does not exist, we
\x0cll the blank with `-\&apos;. For example, an En-
glish word \\dressing&amp;quot; and a Korean word \\\\h
IL\x17$E(tuleysing)&amp;quot; are represented as Fig. 1. The
upper one in Fig. 1 is divided into an English
phoneme unit and the lower one is divided into
an alphabet unit.
</bodyText>
<figure confidence="0.94201325">
G\x12\x0eU\x120\x0eH\x12K\x0e\x03VV\x12&lt;\x0eL\x12Z\x0eQJ\x12&amp;gt;
G\x12\x0eU\x120\x0eH\x12K\x0eV\x12&lt;\x0eV\x12\x10\x0eL\x12Z\x0eQ\x12&amp;gt;\x0eJ\x12\x10
G\x12\x0eU\x120\x0eH\x12K\x0e\x03VV\x12&lt;\x0eL\x12Z\x0eQJ\x12&amp;gt;
G\x12\x0eU\x120\x0eH\x12K\x0eV\x12&lt;\x0eV\x12\x10\x0eL\x12Z\x0eQ\x12&amp;gt;\x0eJ\x12\x10
</figure>
<figureCaption confidence="0.758298">
dressing : Eo
dressing : Eo
Figure 1: An E-K transliteration example
</figureCaption>
<bodyText confidence="0.98894">
The problem in statistical transliteration
method is to \x0cndout the most probable translit-
eration for a given word. Let p(K) be the prob-
ability of a Korean word K, then, for a given
English word E, the transliteration probability
of a word K can be written as p(KjE). By using
the Bayes\&apos; theorem, we can rewrite the translit-
eration problem as follows:
arg max
</bodyText>
<equation confidence="0.999539">
K p(KjE) = arg max
K p(K)p(EjK) (1)
</equation>
<bodyText confidence="0.865916">
With the Markov Independent Assumption,
we approximate p(K) and p(EjK) as follows:
</bodyText>
<equation confidence="0.997367333333333">
p(K) \x18
= p(k1)
n
Y
i=2
p(kijki,1) (2)
p(EjK) \x18
=
n
Y
i=1
p(eijki) (3)
</equation>
<bodyText confidence="0.980373733333333">
As we do not know the pronunciation of a
given word, we consider all possible phoneme
sequences. For example, `data\&apos; has following
possible phoneme sequences, `d-a-t-a, d-at-a,
da-ta, :::\&apos;.
As the history length is lengthened, we can
get more discrimination. But long history in-
formation causes a data sparseness problem. In
order to solve a sparseness problem, Maximum
Entropy Model, Back-o\x0b, and Linear interpola-
tion methods are used. They combine di\x0berent
statistical estimators. (Tae-il Kim, 2000) use up
to \x0cve phonemes in feature function(Berger et
al., 1996). Nine feature functions are combined
with Maximum Entropy Method.
</bodyText>
<subsectionHeader confidence="0.994434">
2.2 Neural Network and Decision Tree
</subsectionHeader>
<bodyText confidence="0.997903296296296">
Methods based on neural network and decision
tree deterministically decide a Korean charac-
ter for a given English input. These methods
take two or three alphabets or phonemes as
an input and generate a Korean alphabet
or phoneme as an output. (Jung-Jae Kim,
1999) proposed a neural network method that
uses two surrounding phonemes as an input.
(Kang, 1999) proposed a decision tree method
that uses six surrounding alphabets. If an
input does not cover the phenomena of proper
transliterations, we cannot get a correct answer.
\x0cEven though we use combining methods to
solve the data sparseness problem, the increase
of an information length would double the
complexity and the time cost of a problem. It
is not easy to increase the information length.
To avoid these di\x0eculties, previous studies
does not use previous outputs(ki,1). But it
loses good information of target language.
Our proposed method is based on the direct
method to extract the transliteration and its
variations. Unlike other methods that deter-
mine a certain input unit\&apos;s output with history
information, we increase the reliability of a cer-
tain transliteration, with known E-K transliter-
ation phenomena (phoneme chunks).
</bodyText>
<sectionHeader confidence="0.866721" genericHeader="method">
3 Transliteration using Multiple
</sectionHeader>
<bodyText confidence="0.991162619047619">
unbounded overlapping phoneme
chunks
For unknown data, we can estimate a Korean
transliteration from hand-written rules. We
can also predict a Korean transliteration with
experimental information. With known English
and Korean transliteration pairs, we can as-
sume possible transliterations without linguistic
knowledge. For example, `scalar\&apos; has common
part with `scale:ah&lt;L\x19$(sukheyil)\&apos;, `casino:\x1d&amp;quot;
\x1a$Zf(khacino)\&apos;, `koala:VW\x19&amp;quot;\x13&amp;quot;(khoalla)\&apos;, and
`car:\x1d&amp;quot;(kha)\&apos; (Fig. 2). We can assume possible
transliteration with these words and their
transliterations. From `scale\&apos; and its transliter-
ation ah&lt;L\x19$(sukheyil), the `sc\&apos; in `scalar\&apos; can be
transliterated as `ah;(sukh)\&apos;. From a `casino\&apos;
example, the `c\&apos; has more evidence that can be
transliterated as `;(kh)\&apos;. We assume that we
can get a correct Korean transliteration, if we
get useful experimental information and their
proper weight that represents reliability.
</bodyText>
<subsectionHeader confidence="0.994643">
3.1 The alignment of an English word
</subsectionHeader>
<bodyText confidence="0.990509769230769">
with a Korean word
We can align an English word with its translit-
eration in alphabet unit or in phoneme unit.
Korean vowels are usually aligned with English
vowels and Korean consonants are aligned with
English consonants. For example, a Korean
consonant, `2(p)\&apos; can be aligned with English
consonants `b\&apos;, `p\&apos;, and `v\&apos;. With this heuristic
we can align an English word with its translit-
eration in an alphabet unit and a phoneme unit
with the accuracy of 99.4%(Kang, 1999).
s c a l a r
s c a l e
</bodyText>
<equation confidence="0.985869625">
\x05
\x05 o
o u
a -
c a s i n o
o
o o
o i
U
k o a l a
o o
o aa
aa o
c a r
o o
o
</equation>
<figureCaption confidence="0.713604">
Figure 2: the transliteration of `scalar : ah\x1d&amp;quot;
\x13&amp;quot;(sukhalla)\&apos;
</figureCaption>
<subsectionHeader confidence="0.998939">
3.2 Extraction of Phoneme Chunks
</subsectionHeader>
<bodyText confidence="0.987161714285714">
From aligned training data, we extract phoneme
chunks. We enumerate all possible subsets of
the given English-Korean aligned pair. During
enumerating subsets, we add start and end posi-
tion information. From an aligned data \\dress-
ing&amp;quot; and \\\\hIL\x17$E(tuleysing)&amp;quot;, we can get subsets
as Table 12.
</bodyText>
<tableCaption confidence="0.992939">
Table 1: The extraction of phoneme chunks
</tableCaption>
<equation confidence="0.849033666666667">
Context Output
@d d/\\h(d)
d d/\\h(d)
@dr d/\\h(d)+r/)(r)
r r/)(r)
@dre d/\\h(d)+r/)(r)+e/D(ey)
</equation>
<bodyText confidence="0.9385215">
The context stands for a given English al-
phabets, and the output stands for its translit-
eration. We assign a proper weight to each
phoneme chunk with Equation 4.
</bodyText>
<equation confidence="0.99872">
weight(context : output) = C(output)
C(context) (4)
</equation>
<bodyText confidence="0.998784285714286">
C(x) means the frequency of x in training data.
Equation 4 shows that the ambiguous phe-
nomenon gets the less evidence. The chunk
weight is transmitted to each phoneme symbol.
To compensate for the length of phoneme, we
multiply the length of phoneme to the weight of
the phoneme chunk(Fig. 3).
</bodyText>
<page confidence="0.901697">
2
</page>
<bodyText confidence="0.564082">
@ means the start and end position of a word
</bodyText>
<equation confidence="0.9645884375">
\x0cweight(surfing: s/
weight(surfing: s/
+
+ ur
ur/
/
+
+ f/
f/
+
+ i/l +
i/l + ng
ng/
/
) =
) =
</equation>
<figure confidence="0.7346475">
2
2
2
2
</figure>
<figureCaption confidence="0.999602">
Figure 3: The weight of a chunk and a phoneme
</figureCaption>
<bodyText confidence="0.9966798125">
This chunk weight does not mean the relia-
bility of a given transliteration phenomenon.
We know real reliability, after all overlapping
phoneme chunks are applied. The chunk that
has some common part with other chunks
gives a context information to them. Therefore
a chunk is not only an input unit but also
a means to calculate the reliability of other
chunks.
We also extract the connection information.
From aligned training data, we obtain all pos-
sible combinations of Korean characters and
English characters. With this connection in-
formation, we exclude impossible connections
of Korean characters and English phoneme se-
quences. We can get the following connection
</bodyText>
<tableCaption confidence="0.741245">
information from \\dressing&amp;quot; example(Table 2).
Table 2: Connection Information
</tableCaption>
<table confidence="0.9279326">
English Korean
left right left right
@ d @ \\h(d)
d r \\h(d) )(r)
r e )(r) D(ey)
</table>
<subsectionHeader confidence="0.992205">
3.3 A Transliteration Network
</subsectionHeader>
<bodyText confidence="0.999635">
For a given word, we get all possible phonemes
and make a Korean transliteration network.
Each node in a network has an Englishphoneme
and a corresponding Korean character. Nodes
are connected with sequence order. For exam-
ple, `scalar\&apos; has the Korean transliteration net-
work as Fig. 4. In this network, we disconnect
some nodes with extracted connection informa-
tion.
After drawing the Korean transliteration net-
work, we apply all possible phoneme chunks
to the network. Each node increases its own
weight with the weight of phoneme symbol in a
phoneme chunks (Fig. 5). By overlapping the
weight, nodes in the longer chunks get more ev-
idence. Then we get the best path that has the
</bodyText>
<figure confidence="0.803560882352941">
Q
Qu
Q
QY
Q
Q
Q
Q
Q
Q
QO
Q
Q
Q
Q
U
U U
</figure>
<figureCaption confidence="0.9264102">
Figure 4: Korean Transliteration Network for
`scalar\&apos;
highest sum of weights, with the Viterbi algo-
rithm. The Tree-Trellis algorithm is used to get
the variations(Soong and Huang, 1991).
</figureCaption>
<equation confidence="0.913390745614035">
st
ar
t
st
ar
t
s/
s/
s/u
s/u
c/Y
c/Y
c/
c/
a/
a/
a/
a/
al
/
al
/
l
/
l
/
l
/
l
/
r
/-
r
/-
a/
a/
a/
a/
ar
/
ar
/
r
/
r
/
end
end
s/
s/
c/
c/ a/
a/
l
/
l
/
a/
a/
l
/
l
/
a/
a/
:
:
+
+
+
+
+
+
s/
s/
c/
c/ a/
a/
l
/
l
/
a/
a/
l
/
l
/
a/
a/
:
:
+ +
s/
s/
c/
c/ a/
a/
l
/
l
/
a/
a/
l
/
l
/
a/
a/
:
:
+ +
+
</equation>
<figureCaption confidence="0.984026">
Figure 5: Weight application example
</figureCaption>
<figure confidence="0.5509765">
4 E-K back-transliteration
E-K back transliterationis a more di\x0ecultprob-
</figure>
<bodyText confidence="0.985614533333333">
lem than E-K transliteration. During the E-K
transliteration, di\x0berent alphabets are treated
equivalently. For example, `f, p\&apos; and `v, b\&apos;
are transliterated into `=(ph)\&apos; and `2(p)\&apos; re-
spectively and the long sound and the short
sound are also treated equivalently. Therefore
the number of possible English phonemes per
a Korean character is bigger than the number
of Korean characters per an English phoneme.
The ambiguity is increased. In E-K back-
transliteration, Korean phonemes and English
phonemes switch their roles. Just switching the
position. A Korean word is aligned with an
English word in a phoneme unit or a character
unit(Fig. 6).
</bodyText>
<figure confidence="0.922314833333333">
\x12G\x0e0\x12U\x0eK\x12H\x0e&lt;\x12VV\x0eZ\x12L\x0e&amp;gt;\x12QJ
.\x12G\x0eX\x12\x10\x0e0\x12U\x0eK\x12H\x0e&lt;\x12VV\x0eZ\x12L\x0e&amp;gt;\x12QJ
\x12G\x0e0\x12U\x0eK\x12H\x0e&lt;\x12VV\x0eZ\x12L\x0e&amp;gt;\x12QJ
.\x12G\x0eX\x12\x10\x0e0\x12U\x0eK\x12H\x0e&lt;\x12VV\x0eZ\x12L\x0e&amp;gt;\x12QJ
Eo : dressing
Eo : dressing
</figure>
<figureCaption confidence="0.999167">
Figure 6: E-K back-transliteration example
</figureCaption>
<sectionHeader confidence="0.9224" genericHeader="evaluation">
\x0c5 Experiments
</sectionHeader>
<bodyText confidence="0.988868666666667">
Experiments were done in two points of view:
the accuracy test and the variation coverage
test.
</bodyText>
<subsectionHeader confidence="0.994524">
5.1 Test Sets
</subsectionHeader>
<bodyText confidence="0.997961176470588">
We use two data sets for an accuracy test. Test
Set I is consists of 1,650 English and Korean
word pairs that aligned in a phoneme unit. It
was made by (Lee and Choi, 1998) and tested by
many methods. To compare our method with
other methods, we use this data set. We use
same training data (1,500 words) and test data
(150 words). Test Set II is consists of 7,185
English and Korean word pairs. We use Test
Set II to show the relation between the size of
training data and the accuracy. We use 90%
of total size as training data and 10% as test
data. For a variation coverage test, we use Test
Set III that is extracted from KTSET 2.0. Test
Set III is consists of 2,391 English words and
their transliterations. An English word has 1.14
various transliterations in average.
</bodyText>
<subsectionHeader confidence="0.998052">
5.2 Evaluation functions
</subsectionHeader>
<bodyText confidence="0.983753333333333">
Accuracy was measured by the percentage of
the number of correct transliterations divided
by the numberof generated transliterations. We
call it as word accuracy(W.A.). We use one
more measure, called character accuracy(C.A.)
that measures the character edit distance be-
tween a correct word and a generated word.
W:A: = no: of correct words
no: of generated words (5)
</bodyText>
<equation confidence="0.979607">
C:A: = L,(i + d+ s)
L (6)
</equation>
<bodyText confidence="0.990317">
where L is the length of the original string, and
i;d, and s are the number of insertion, deletion
and substitution respectively. If the dividend is
negative (when L &lt; (i + d + s)), we consider it
as zero(Hall and Dowling, 1980).
For the real usage test, we used variation cov-
erage (V.C.) that considers various usages. We
evaluated both for the term frequency (tf) and
document frequency (df), where tf is the number
of term appearance in the documents and df is
the number of documents that contain the term.
If we set the usage tf (or df) of the translitera-
tions to 1 for each transliteration, we can calcu-
late the transliteration coverage for the unique
word types, single frequency(sf).
V:C: = ftf;df;sfg of found words
ftf;df;sfg of used words (7)
</bodyText>
<subsectionHeader confidence="0.998019">
5.3 Accuracy tests
</subsectionHeader>
<bodyText confidence="0.873742875">
We compare our result [PCa, PCp]3 with the
simple statistical information based model(Lee
and Choi, 1998) [ST], the Maximum Entropy
based model(Tae-il Kim, 2000) [MEM], the
Neural Network model(Jung-Jae Kim, 1999)
[NN] and the Decision Tree based model(Kang,
1999)[DT]. Table 3 shows the result of E-
K transliteration and back-transliteration test
</bodyText>
<tableCaption confidence="0.63948225">
with Test Set I.
Table 3: C.A. and W.A. with Test Set I
E-K trans. E-K back trans.
method C.A. W.A. C.A. W.A.
</tableCaption>
<table confidence="0.996133166666667">
ST 69.3% 40.7%4 60.5% -
MEM 72.3% 43.3% - -
NN 79.0% 35.1% - -
DT 78.1% 37.6% 77.1% 31.0%
PCp 86.5% 55.3% 81.4% 34.7%
PCa 85.3% 46.7% 79.3% 32.6%
</table>
<figureCaption confidence="0.476056">
Fig. 7, 8 show the results of our proposed
method with the size of training data, Test Set
II. We compare our result with the decision tree
based method.
</figureCaption>
<figure confidence="0.996881285714286">
^`
_`
``
a`
b`
c`
d`
\\[[[ ][[[ ^[[[ _[[[ `[[[ a[[[
nYlY o\x7f
nYlY {n
nYlY{n
YlY o\x7f
YlY {n
YlY {n
</figure>
<figureCaption confidence="0.989087">
Figure 7: E-K transliteration results with Test
</figureCaption>
<figure confidence="0.998852909090909">
Set II
3
PC stands for phoneme chunks based method and
a and b stands for aligned by an alphabet unit and a
phoneme unit respectively
4
with 20 higher rank results
\x0c][
^[
_[
`[
a[
b[
c[
d[
\\[[[ ][[[ ^[[[ _[[[ `[[[ a[[[
nYlY o\x7f
nYlY {n
nYlY {n
YlY o\x7f
YlY {n
YlY {n
</figure>
<figureCaption confidence="0.984316">
Figure 8: E-K back-transliteration results with
</figureCaption>
<table confidence="0.596393666666667">
Test Set II
With Test Set II, we can get the following
result (Table 4).
</table>
<tableCaption confidence="0.703204333333333">
Table 4: C.A. and W.A. with the Test Set II
E-K trans. E-K back trans.
method C.A. W.A. C.A. W.A.
</tableCaption>
<table confidence="0.976102">
PCp 89.5% 57.2% 84.9% 40.9%
PCa 90.6% 58.3% 84.8% 40.8%
</table>
<subsectionHeader confidence="0.768204">
5.4 Variation coverage tests
</subsectionHeader>
<bodyText confidence="0.973272">
To compare our result(PCp) with (Lee and
Choi, 1998), we trained our methods with the
training data of Test Set I. In ST, (Lee and
Choi, 1998) use 20 high rank results, but we
just use 5 results. Table 5 shows the coverage
</bodyText>
<tableCaption confidence="0.673107333333333">
of our proposed method.
Table 5: variation coverage with Test Set III
method tf df sf
</tableCaption>
<table confidence="0.9470505">
ST 76.0% 73.9% 47.1%
PCp 84.0% 84.0% 64.0%
</table>
<bodyText confidence="0.963626">
Fig. 9 shows the increase of coverage with the
number of outputs.
</bodyText>
<subsectionHeader confidence="0.857527">
5.5 Discussion
</subsectionHeader>
<bodyText confidence="0.989048714285714">
We summarize the information length and the
kind of information(Table 6). The results of
experiments and information usage show that
MEM combines various information better
than DT and NN. ST does not use a previous
input (ei,1) but use a previous output(ki,1) to
calculate the current output\&apos;s probability like
</bodyText>
<figure confidence="0.992596166666667">
_`
`[
``
a[
a`
b[
b`
c[
c`
d[
d`
\\ ] ^ _ ` a b c d \\[
</figure>
<figureCaption confidence="0.998659">
Figure 9: The V.C. result
</figureCaption>
<tableCaption confidence="0.730588">
Table 6: Information Usage
method before behind previous output
</tableCaption>
<table confidence="0.9379646">
ST 2 0 Y
MEM 2 2 N
NN 1 1 N
DT 3 3 N
PC - - Y
</table>
<bodyText confidence="0.976869533333333">
Part-of-Speech Tagging problem. But ST gets
the lowest accuracy. It means that surrounding
alphabets give more information than previous
output. In other words, E-K transliteration is
not the alphabet-per-alphabet or phoneme-per-
phoneme classi\x0ccation problem. A previous
output does not give enough information for
current unit\&apos;s disambiguation. An input unit
and an output unit should be extended. E-K
transliteration is a chunk-per-chunk classi\x0cca-
tion problem.
We restrict the length of information, to see
the in
uence of phoneme-chunk size. Fig. 10
shows the results.
</bodyText>
<figure confidence="0.989859466666667">
[
\\[
][
^[
_[
`[
a[
b[
c[
d[
\\ ] ^ _ ` a b
nYlY \x7f ~ t
nYlY \x7f ~ tt
YlY \x7f ~ t
YlY \x7f ~ tt
</figure>
<figureCaption confidence="0.70984">
Figure 10: the result of a length limit test
</figureCaption>
<bodyText confidence="0.996776294117647">
\x0cWith the same length of information, we
get the higher C.A. and W.A. than other
methods. It means previous outputs give good
information and our chunk-based method is
a good combining method. It also suggests
that we can restrict the max size of chunk in a
permissible size.
PCa gets a higher accuracy than PCp. It
is due to the number of possible phoneme se-
quences. A transliteration network that con-
sists of phoneme unit has more nodes than a
transliteration network that consists of alpha-
bet unit. With small training data, despite of
the loss due to the phoneme sequences ambi-
guity a phoneme gives more information than
an alphabet. When the information is enough,
PCa outperforms PCp.
</bodyText>
<sectionHeader confidence="0.999313" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.998967">
We propose the method of English-to-Korean
transliteration and back-transliteration with
multiple unbounded overlapping phoneme
chunks. We showed that E-K transliteration
and back-transliteration are not a phoneme-
per-phoneme and alphabet-per-alphabet
classi\x0ccation problem. So we use phoneme
chunks that do not have a length limit and
can explain E-K transliteration phenomena.
We get the reliability of a given transliter-
ation phenomenon by applying overlapping
phoneme chunks. Our method is simple and
does not need a complex combining method
for various length of information. The change
of an information length does not a\x0bect the
internal representation of the problem. Our
chunk-based method can be used to other
classi\x0ccation problems and can give a simple
combining method.
</bodyText>
<sectionHeader confidence="0.9689" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.972768711538462">
Tae-il Kim. 2000. English to Korean translit-
eration model using maximum entropy model
for cross language information retrieval. Mas-
ter\&apos;s thesis, Seogang University (in Korean).
Kil Soon Jeong, Sung Hyun Myaeng, Jae Sung
Lee, and Key-Sun Choi. 1999. Automatic
identi\x0ccation and back-transliteration of for-
eign words for information retrieval. Infor-
mation Processing and Management.
Key-Sun Choi Jung-Jae Kim, Jae Sung Lee.
1999. Pronunciation unit based automatic
English-Korean transliteration model using
neural network. In Proceedings of Korea Cog-
nitive Science Association(in Korean).
Byung-Ju Kang. 1999. Automatic Korean-
English back-transliteration. In Proceedings
of the 11th Conference on Hangul and Ko-
rean Language Information Processing(in Ko-
rean).
Jae Sung Lee and Key-Sun Choi. 1998. English
to Korean statistical transliteration for in-
formation retrieval. Computer Processing of
Oriental Languages.
K. Jeong, Y. Kwon, and S. H. Myaeng. 1997.
The e\x0bect of a proper handling of foreign and
English words in retrieving Korean text. In
Proceedings of the 2nd International Work-
shop on Information Retrieval with Asian
Languages.
K. Knight and J. Graehl. 1997. Machine
transliteration. In Proceedings of the 35th
Annual Meeting of the Association for Com-
putational Linguistics.
Adam L. Berger, Stephen A. Della Pietra, and
Vincent J. Della Pietra. 1996. A maximum
entropy approach to natural language pro-
cessing. Computational Linguistics.
Y. C. Park, K. Choi, J. Kim, and Y. Kim.
1996. Development of the data collection ver.
2.0ktset2:0 for Korean information retrieval
studies. In Arti\x0ccial Intelligence Spring Con-
ference. Korea Information Science Society
(in Korean).
Frank K. Soong and Eng-Fong Huang. 1991.
A tree-trellis based fast search for \x0cnding
the n best sentence hypotheses in continuous
speech recognition. In IEEE International
Conference on Acoustic Speech and Signal
Processing, pages 546{549.
P. Hall and G. Dowling. 1980. Approximate
string matching. Computing Surveys.
\x0c&apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.584295">
<title confidence="0.999211">b&apos;English-to-Korean Transliteration using Multiple Unbounded Overlapping Phoneme Chunks</title>
<author confidence="0.997914">In-Ho Kang</author>
<author confidence="0.997914">GilChang Kim</author>
<affiliation confidence="0.7939225">Department of Computer Science Korea Advanced Institute of Science and Technology</affiliation>
<abstract confidence="0.9994112">We present in this paper the method of English-to-Korean(E-K) transliteration and back-transliteration. In Korean technical documents, many English words are transliterated into Korean words in various forms in diverse ways. As English words and Korean transliterations are usually technical terms and proper nouns, it is hard to \x0cnd a transliteration and its variations in a dictionary. Therefore an automatic transliteration system is needed to \x0cnd the transliterations of English words without manual intervention. To explain E-K transliteration phenomena, we use phoneme chunks that do not have a length limit. By applying phoneme chunks, we combine di\x0berent length information with easy. The E-K transliteration method has three steps. In the \x0crst, we make a phoneme network that shows all possible transliterations of the given word. In the second step, we apply phoneme chunks, extracted from training data, to calculate the reliability of each possible transliteration. Then we obtain probable transliterations of the given English word.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tae-il Kim</author>
</authors>
<title>English to Korean transliteration model using maximum entropy model for cross language information retrieval.</title>
<date>2000</date>
<tech>Master\&apos;s thesis,</tech>
<institution>Seogang University (in Korean).</institution>
<contexts>
<context position="6399" citStr="Kim, 2000" startWordPosition="921" endWordPosition="922">) and p(EjK) as follows: p(K) \x18 = p(k1) n Y i=2 p(kijki,1) (2) p(EjK) \x18 = n Y i=1 p(eijki) (3) As we do not know the pronunciation of a given word, we consider all possible phoneme sequences. For example, `data\&apos; has following possible phoneme sequences, `d-a-t-a, d-at-a, da-ta, :::\&apos;. As the history length is lengthened, we can get more discrimination. But long history information causes a data sparseness problem. In order to solve a sparseness problem, Maximum Entropy Model, Back-o\x0b, and Linear interpolation methods are used. They combine di\x0berent statistical estimators. (Tae-il Kim, 2000) use up to \x0cve phonemes in feature function(Berger et al., 1996). Nine feature functions are combined with Maximum Entropy Method. 2.2 Neural Network and Decision Tree Methods based on neural network and decision tree deterministically decide a Korean character for a given English input. These methods take two or three alphabets or phonemes as an input and generate a Korean alphabet or phoneme as an output. (Jung-Jae Kim, 1999) proposed a neural network method that uses two surrounding phonemes as an input. (Kang, 1999) proposed a decision tree method that uses six surrounding alphabets. If</context>
<context position="16031" citStr="Kim, 2000" startWordPosition="2556" endWordPosition="2557">valuated both for the term frequency (tf) and document frequency (df), where tf is the number of term appearance in the documents and df is the number of documents that contain the term. If we set the usage tf (or df) of the transliterations to 1 for each transliteration, we can calculate the transliteration coverage for the unique word types, single frequency(sf). V:C: = ftf;df;sfg of found words ftf;df;sfg of used words (7) 5.3 Accuracy tests We compare our result [PCa, PCp]3 with the simple statistical information based model(Lee and Choi, 1998) [ST], the Maximum Entropy based model(Tae-il Kim, 2000) [MEM], the Neural Network model(Jung-Jae Kim, 1999) [NN] and the Decision Tree based model(Kang, 1999)[DT]. Table 3 shows the result of EK transliteration and back-transliteration test with Test Set I. Table 3: C.A. and W.A. with Test Set I E-K trans. E-K back trans. method C.A. W.A. C.A. W.A. ST 69.3% 40.7%4 60.5% - MEM 72.3% 43.3% - - NN 79.0% 35.1% - - DT 78.1% 37.6% 77.1% 31.0% PCp 86.5% 55.3% 81.4% 34.7% PCa 85.3% 46.7% 79.3% 32.6% Fig. 7, 8 show the results of our proposed method with the size of training data, Test Set II. We compare our result with the decision tree based method. ^` _</context>
</contexts>
<marker>Kim, 2000</marker>
<rawString>Tae-il Kim. 2000. English to Korean transliteration model using maximum entropy model for cross language information retrieval. Master\&apos;s thesis, Seogang University (in Korean).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kil Soon Jeong</author>
<author>Sung Hyun Myaeng</author>
<author>Jae Sung Lee</author>
<author>Key-Sun Choi</author>
</authors>
<title>Automatic identi\x0ccation and back-transliteration of foreign words for information retrieval. Information Processing and Management.</title>
<date>1999</date>
<marker>Jeong, Myaeng, Lee, Choi, 1999</marker>
<rawString>Kil Soon Jeong, Sung Hyun Myaeng, Jae Sung Lee, and Key-Sun Choi. 1999. Automatic identi\x0ccation and back-transliteration of foreign words for information retrieval. Information Processing and Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Key-Sun Choi Jung-Jae Kim</author>
<author>Jae Sung Lee</author>
</authors>
<title>Pronunciation unit based automatic English-Korean transliteration model using neural network.</title>
<date>1999</date>
<booktitle>In Proceedings of Korea Cognitive Science Association(in Korean).</booktitle>
<marker>Kim, Lee, 1999</marker>
<rawString>Key-Sun Choi Jung-Jae Kim, Jae Sung Lee. 1999. Pronunciation unit based automatic English-Korean transliteration model using neural network. In Proceedings of Korea Cognitive Science Association(in Korean).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Byung-Ju Kang</author>
</authors>
<title>Automatic KoreanEnglish back-transliteration.</title>
<date>1999</date>
<booktitle>In Proceedings of the 11th Conference on Hangul and Korean Language Information Processing(in Korean).</booktitle>
<contexts>
<context position="6927" citStr="Kang, 1999" startWordPosition="1006" endWordPosition="1007">thods are used. They combine di\x0berent statistical estimators. (Tae-il Kim, 2000) use up to \x0cve phonemes in feature function(Berger et al., 1996). Nine feature functions are combined with Maximum Entropy Method. 2.2 Neural Network and Decision Tree Methods based on neural network and decision tree deterministically decide a Korean character for a given English input. These methods take two or three alphabets or phonemes as an input and generate a Korean alphabet or phoneme as an output. (Jung-Jae Kim, 1999) proposed a neural network method that uses two surrounding phonemes as an input. (Kang, 1999) proposed a decision tree method that uses six surrounding alphabets. If an input does not cover the phenomena of proper transliterations, we cannot get a correct answer. \x0cEven though we use combining methods to solve the data sparseness problem, the increase of an information length would double the complexity and the time cost of a problem. It is not easy to increase the information length. To avoid these di\x0eculties, previous studies does not use previous outputs(ki,1). But it loses good information of target language. Our proposed method is based on the direct method to extract the tr</context>
<context position="9262" citStr="Kang, 1999" startWordPosition="1350" endWordPosition="1351">n, if we get useful experimental information and their proper weight that represents reliability. 3.1 The alignment of an English word with a Korean word We can align an English word with its transliteration in alphabet unit or in phoneme unit. Korean vowels are usually aligned with English vowels and Korean consonants are aligned with English consonants. For example, a Korean consonant, `2(p)\&apos; can be aligned with English consonants `b\&apos;, `p\&apos;, and `v\&apos;. With this heuristic we can align an English word with its transliteration in an alphabet unit and a phoneme unit with the accuracy of 99.4%(Kang, 1999). s c a l a r s c a l e \x05 \x05 o o u a - c a s i n o o o o o i U k o a l a o o o aa aa o c a r o o o Figure 2: the transliteration of `scalar : ah\x1d&amp;quot; \x13&amp;quot;(sukhalla)\&apos; 3.2 Extraction of Phoneme Chunks From aligned training data, we extract phoneme chunks. We enumerate all possible subsets of the given English-Korean aligned pair. During enumerating subsets, we add start and end position information. From an aligned data \\dressing&amp;quot; and \\\\hIL\x17$E(tuleysing)&amp;quot;, we can get subsets as Table 12. Table 1: The extraction of phoneme chunks Context Output @d d/\\h(d) d d/\\h(d) @dr d/\\h(d)+r/)</context>
<context position="16134" citStr="Kang, 1999" startWordPosition="2571" endWordPosition="2572">appearance in the documents and df is the number of documents that contain the term. If we set the usage tf (or df) of the transliterations to 1 for each transliteration, we can calculate the transliteration coverage for the unique word types, single frequency(sf). V:C: = ftf;df;sfg of found words ftf;df;sfg of used words (7) 5.3 Accuracy tests We compare our result [PCa, PCp]3 with the simple statistical information based model(Lee and Choi, 1998) [ST], the Maximum Entropy based model(Tae-il Kim, 2000) [MEM], the Neural Network model(Jung-Jae Kim, 1999) [NN] and the Decision Tree based model(Kang, 1999)[DT]. Table 3 shows the result of EK transliteration and back-transliteration test with Test Set I. Table 3: C.A. and W.A. with Test Set I E-K trans. E-K back trans. method C.A. W.A. C.A. W.A. ST 69.3% 40.7%4 60.5% - MEM 72.3% 43.3% - - NN 79.0% 35.1% - - DT 78.1% 37.6% 77.1% 31.0% PCp 86.5% 55.3% 81.4% 34.7% PCa 85.3% 46.7% 79.3% 32.6% Fig. 7, 8 show the results of our proposed method with the size of training data, Test Set II. We compare our result with the decision tree based method. ^` _` `` a` b` c` d` \\[[[ ][[[ ^[[[ _[[[ `[[[ a[[[ nYlY o\x7f nYlY {n nYlY{n YlY o\x7f YlY {n YlY {n Figur</context>
</contexts>
<marker>Kang, 1999</marker>
<rawString>Byung-Ju Kang. 1999. Automatic KoreanEnglish back-transliteration. In Proceedings of the 11th Conference on Hangul and Korean Language Information Processing(in Korean).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jae Sung Lee</author>
<author>Key-Sun Choi</author>
</authors>
<title>English to Korean statistical transliteration for information retrieval. Computer Processing of Oriental Languages.</title>
<date>1998</date>
<contexts>
<context position="4363" citStr="Lee and Choi, 1998" startWordPosition="624" endWordPosition="627">onclusion follows in section 5. 2 English-to-Korean transliteration E-K transliteration models are classi\x0ced in two methods: the pivot method and the direct method. In the pivot method, transliteration is done in two steps: converting English words into pronunciation symbols and then converting these symbols into Korean words by using the Korean standard conversion rule. In the direct method, English words are directly converted to Korean words without intermediate steps. An experiment shows that the direct method is better than the pivot method in \x0cnding variations of a transliteration(Lee and Choi, 1998). Statistical information, neural network and decision tree were used to implement the direct method. 2.1 Statistical Transliteration method An English word is divided into phoneme sequence or alphabet sequence as e1;e2;::: ;en. Then a corresponding Korean word is represented as k1;k2;::: ;kn. If a corresponding Korean character (ki) does not exist, we \x0cll the blank with `-\&apos;. For example, an English word \\dressing&amp;quot; and a Korean word \\\\h IL\x17$E(tuleysing)&amp;quot; are represented as Fig. 1. The upper one in Fig. 1 is divided into an English phoneme unit and the lower one is divided into an alp</context>
<context position="14075" citStr="Lee and Choi, 1998" startWordPosition="2214" endWordPosition="2217">12U\x0eK\x12H\x0e&lt;\x12VV\x0eZ\x12L\x0e&amp;gt;\x12QJ .\x12G\x0eX\x12\x10\x0e0\x12U\x0eK\x12H\x0e&lt;\x12VV\x0eZ\x12L\x0e&amp;gt;\x12QJ \x12G\x0e0\x12U\x0eK\x12H\x0e&lt;\x12VV\x0eZ\x12L\x0e&amp;gt;\x12QJ .\x12G\x0eX\x12\x10\x0e0\x12U\x0eK\x12H\x0e&lt;\x12VV\x0eZ\x12L\x0e&amp;gt;\x12QJ Eo : dressing Eo : dressing Figure 6: E-K back-transliteration example \x0c5 Experiments Experiments were done in two points of view: the accuracy test and the variation coverage test. 5.1 Test Sets We use two data sets for an accuracy test. Test Set I is consists of 1,650 English and Korean word pairs that aligned in a phoneme unit. It was made by (Lee and Choi, 1998) and tested by many methods. To compare our method with other methods, we use this data set. We use same training data (1,500 words) and test data (150 words). Test Set II is consists of 7,185 English and Korean word pairs. We use Test Set II to show the relation between the size of training data and the accuracy. We use 90% of total size as training data and 10% as test data. For a variation coverage test, we use Test Set III that is extracted from KTSET 2.0. Test Set III is consists of 2,391 English words and their transliterations. An English word has 1.14 various transliterations in averag</context>
<context position="15975" citStr="Lee and Choi, 1998" startWordPosition="2546" endWordPosition="2549">sed variation coverage (V.C.) that considers various usages. We evaluated both for the term frequency (tf) and document frequency (df), where tf is the number of term appearance in the documents and df is the number of documents that contain the term. If we set the usage tf (or df) of the transliterations to 1 for each transliteration, we can calculate the transliteration coverage for the unique word types, single frequency(sf). V:C: = ftf;df;sfg of found words ftf;df;sfg of used words (7) 5.3 Accuracy tests We compare our result [PCa, PCp]3 with the simple statistical information based model(Lee and Choi, 1998) [ST], the Maximum Entropy based model(Tae-il Kim, 2000) [MEM], the Neural Network model(Jung-Jae Kim, 1999) [NN] and the Decision Tree based model(Kang, 1999)[DT]. Table 3 shows the result of EK transliteration and back-transliteration test with Test Set I. Table 3: C.A. and W.A. with Test Set I E-K trans. E-K back trans. method C.A. W.A. C.A. W.A. ST 69.3% 40.7%4 60.5% - MEM 72.3% 43.3% - - NN 79.0% 35.1% - - DT 78.1% 37.6% 77.1% 31.0% PCp 86.5% 55.3% 81.4% 34.7% PCa 85.3% 46.7% 79.3% 32.6% Fig. 7, 8 show the results of our proposed method with the size of training data, Test Set II. We comp</context>
<context position="17407" citStr="Lee and Choi, 1998" startWordPosition="2816" endWordPosition="2819"> stands for phoneme chunks based method and a and b stands for aligned by an alphabet unit and a phoneme unit respectively 4 with 20 higher rank results \x0c][ ^[ _[ `[ a[ b[ c[ d[ \\[[[ ][[[ ^[[[ _[[[ `[[[ a[[[ nYlY o\x7f nYlY {n nYlY {n YlY o\x7f YlY {n YlY {n Figure 8: E-K back-transliteration results with Test Set II With Test Set II, we can get the following result (Table 4). Table 4: C.A. and W.A. with the Test Set II E-K trans. E-K back trans. method C.A. W.A. C.A. W.A. PCp 89.5% 57.2% 84.9% 40.9% PCa 90.6% 58.3% 84.8% 40.8% 5.4 Variation coverage tests To compare our result(PCp) with (Lee and Choi, 1998), we trained our methods with the training data of Test Set I. In ST, (Lee and Choi, 1998) use 20 high rank results, but we just use 5 results. Table 5 shows the coverage of our proposed method. Table 5: variation coverage with Test Set III method tf df sf ST 76.0% 73.9% 47.1% PCp 84.0% 84.0% 64.0% Fig. 9 shows the increase of coverage with the number of outputs. 5.5 Discussion We summarize the information length and the kind of information(Table 6). The results of experiments and information usage show that MEM combines various information better than DT and NN. ST does not use a previous inp</context>
</contexts>
<marker>Lee, Choi, 1998</marker>
<rawString>Jae Sung Lee and Key-Sun Choi. 1998. English to Korean statistical transliteration for information retrieval. Computer Processing of Oriental Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jeong</author>
<author>Y Kwon</author>
<author>S H Myaeng</author>
</authors>
<title>The e\x0bect of a proper handling of foreign and English words in retrieving Korean text.</title>
<date>1997</date>
<booktitle>In Proceedings of the 2nd International Workshop on Information Retrieval with Asian Languages.</booktitle>
<contexts>
<context position="2154" citStr="Jeong et al., 1997" startWordPosition="297" endWordPosition="300">) HL\x19$\x1e&amp;quot;(teyitha) [1,033]1 (b) HL\x19$\&apos;)(teyithe) [527] 1 the frequency in KTSET (2) digital (a) \x11$\x1a$OUe(ticithul) [254] (b) \x11$\x1a$\x1e&amp;quot;(tichithal) [7] (c) \x11$\x1a$*,(ticithel) [6] These various transliterations are not negligible for natural language processing, especiallyin information retrieval. Because same words are treated as di\x0berent ones, the calculation based on the frequency of word would produce misleading results. An experiment shows that the e\x0bectiveness of information retrieval increases when various forms including English words are treated equivalently(Jeong et al., 1997). We may use a dictionary, to \x0cnd a correct transliteration and its variations. But it is not feasible because transliterated words are usually technical terms and proper nouns that have rich productivity. Therefore an automatic transliteration system is needed to \x0cnd transliterations without manual intervention. There have been some studies on E-K transliteration. They tried to explain transliteration as phoneme-per-phoneme or alphabetper-phoneme classi\x0ccation problem. They restricted the information length to two or three units before and behind an input unit. In fact, many linguist</context>
</contexts>
<marker>Jeong, Kwon, Myaeng, 1997</marker>
<rawString>K. Jeong, Y. Kwon, and S. H. Myaeng. 1997. The e\x0bect of a proper handling of foreign and English words in retrieving Korean text. In Proceedings of the 2nd International Workshop on Information Retrieval with Asian Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<title>Machine transliteration.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Knight, Graehl, 1997</marker>
<rawString>K. Knight and J. Graehl. 1997. Machine transliteration. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing. Computational Linguistics.</title>
<date>1996</date>
<contexts>
<context position="6466" citStr="Berger et al., 1996" startWordPosition="930" endWordPosition="933">i,1) (2) p(EjK) \x18 = n Y i=1 p(eijki) (3) As we do not know the pronunciation of a given word, we consider all possible phoneme sequences. For example, `data\&apos; has following possible phoneme sequences, `d-a-t-a, d-at-a, da-ta, :::\&apos;. As the history length is lengthened, we can get more discrimination. But long history information causes a data sparseness problem. In order to solve a sparseness problem, Maximum Entropy Model, Back-o\x0b, and Linear interpolation methods are used. They combine di\x0berent statistical estimators. (Tae-il Kim, 2000) use up to \x0cve phonemes in feature function(Berger et al., 1996). Nine feature functions are combined with Maximum Entropy Method. 2.2 Neural Network and Decision Tree Methods based on neural network and decision tree deterministically decide a Korean character for a given English input. These methods take two or three alphabets or phonemes as an input and generate a Korean alphabet or phoneme as an output. (Jung-Jae Kim, 1999) proposed a neural network method that uses two surrounding phonemes as an input. (Kang, 1999) proposed a decision tree method that uses six surrounding alphabets. If an input does not cover the phenomena of proper transliterations, </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Park</author>
<author>K Choi</author>
<author>J Kim</author>
<author>Y Kim</author>
</authors>
<title>Development of the data collection ver. 2.0ktset2:0 for Korean information retrieval studies.</title>
<date>1996</date>
<booktitle>In Arti\x0ccial Intelligence Spring Conference. Korea Information Science Society (in Korean).</booktitle>
<contexts>
<context position="1522" citStr="Park et al., 1996" startWordPosition="216" endWordPosition="219">asy. The E-K transliteration method has three steps. In the \x0crst, we make a phoneme network that shows all possible transliterations of the given word. In the second step, we apply phoneme chunks, extracted from training data, to calculate the reliability of each possible transliteration. Then we obtain probable transliterations of the given English word. 1 Introduction In Korean technical documents, many English words are used in their original forms. But sometimes they are transliterated into Korean in di\x0berent forms. Ex. 1, 2 show the examples of various transliterations in KTSET 2.0(Park et al., 1996). (1) data (a) HL\x19$\x1e&amp;quot;(teyitha) [1,033]1 (b) HL\x19$\&apos;)(teyithe) [527] 1 the frequency in KTSET (2) digital (a) \x11$\x1a$OUe(ticithul) [254] (b) \x11$\x1a$\x1e&amp;quot;(tichithal) [7] (c) \x11$\x1a$*,(ticithel) [6] These various transliterations are not negligible for natural language processing, especiallyin information retrieval. Because same words are treated as di\x0berent ones, the calculation based on the frequency of word would produce misleading results. An experiment shows that the e\x0bectiveness of information retrieval increases when various forms including English words are treated </context>
</contexts>
<marker>Park, Choi, Kim, Kim, 1996</marker>
<rawString>Y. C. Park, K. Choi, J. Kim, and Y. Kim. 1996. Development of the data collection ver. 2.0ktset2:0 for Korean information retrieval studies. In Arti\x0ccial Intelligence Spring Conference. Korea Information Science Society (in Korean).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank K Soong</author>
<author>Eng-Fong Huang</author>
</authors>
<title>A tree-trellis based fast search for \x0cnding the n best sentence hypotheses in continuous speech recognition.</title>
<date>1991</date>
<booktitle>In IEEE International Conference on Acoustic Speech and Signal Processing,</booktitle>
<pages>546--549</pages>
<contexts>
<context position="12378" citStr="Soong and Huang, 1991" startWordPosition="1909" endWordPosition="1912">. In this network, we disconnect some nodes with extracted connection information. After drawing the Korean transliteration network, we apply all possible phoneme chunks to the network. Each node increases its own weight with the weight of phoneme symbol in a phoneme chunks (Fig. 5). By overlapping the weight, nodes in the longer chunks get more evidence. Then we get the best path that has the Q Qu Q QY Q Q Q Q Q Q QO Q Q Q Q U U U Figure 4: Korean Transliteration Network for `scalar\&apos; highest sum of weights, with the Viterbi algorithm. The Tree-Trellis algorithm is used to get the variations(Soong and Huang, 1991). st ar t st ar t s/ s/ s/u s/u c/Y c/Y c/ c/ a/ a/ a/ a/ al / al / l / l / l / l / r /- r /- a/ a/ a/ a/ ar / ar / r / r / end end s/ s/ c/ c/ a/ a/ l / l / a/ a/ l / l / a/ a/ : : + + + + + + s/ s/ c/ c/ a/ a/ l / l / a/ a/ l / l / a/ a/ : : + + s/ s/ c/ c/ a/ a/ l / l / a/ a/ l / l / a/ a/ : : + + + Figure 5: Weight application example 4 E-K back-transliteration E-K back transliterationis a more di\x0ecultproblem than E-K transliteration. During the E-K transliteration, di\x0berent alphabets are treated equivalently. For example, `f, p\&apos; and `v, b\&apos; are transliterated into `=(ph)\&apos; and `2(p</context>
</contexts>
<marker>Soong, Huang, 1991</marker>
<rawString>Frank K. Soong and Eng-Fong Huang. 1991. A tree-trellis based fast search for \x0cnding the n best sentence hypotheses in continuous speech recognition. In IEEE International Conference on Acoustic Speech and Signal Processing, pages 546{549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hall</author>
<author>G Dowling</author>
</authors>
<title>Approximate string matching. Computing Surveys. \x0c&apos;</title>
<date>1980</date>
<contexts>
<context position="15325" citStr="Hall and Dowling, 1980" startWordPosition="2436" endWordPosition="2439"> Accuracy was measured by the percentage of the number of correct transliterations divided by the numberof generated transliterations. We call it as word accuracy(W.A.). We use one more measure, called character accuracy(C.A.) that measures the character edit distance between a correct word and a generated word. W:A: = no: of correct words no: of generated words (5) C:A: = L,(i + d+ s) L (6) where L is the length of the original string, and i;d, and s are the number of insertion, deletion and substitution respectively. If the dividend is negative (when L &lt; (i + d + s)), we consider it as zero(Hall and Dowling, 1980). For the real usage test, we used variation coverage (V.C.) that considers various usages. We evaluated both for the term frequency (tf) and document frequency (df), where tf is the number of term appearance in the documents and df is the number of documents that contain the term. If we set the usage tf (or df) of the transliterations to 1 for each transliteration, we can calculate the transliteration coverage for the unique word types, single frequency(sf). V:C: = ftf;df;sfg of found words ftf;df;sfg of used words (7) 5.3 Accuracy tests We compare our result [PCa, PCp]3 with the simple stati</context>
</contexts>
<marker>Hall, Dowling, 1980</marker>
<rawString>P. Hall and G. Dowling. 1980. Approximate string matching. Computing Surveys. \x0c&apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>