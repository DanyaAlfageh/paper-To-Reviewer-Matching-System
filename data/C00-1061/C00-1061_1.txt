b'English-to-Korean Transliteration using Multiple Unbounded
Overlapping Phoneme Chunks
In-Ho Kang and GilChang Kim
Department of Computer Science
Korea Advanced Institute of Science and Technology
Abstract
We present in this paper the method of
English-to-Korean(E-K) transliteration and
back-transliteration. In Korean technical
documents, many English words are translit-
erated into Korean words in various forms in
diverse ways. As English words and Korean
transliterations are usually technical terms and
proper nouns, it is hard to \x0cnd a transliteration
and its variations in a dictionary. Therefore
an automatic transliteration system is needed
to \x0cnd the transliterations of English words
without manual intervention.
To explain E-K transliteration phenomena,
we use phoneme chunks that do not have a
length limit. By applying phoneme chunks,
we combine di\x0berent length information with
easy. The E-K transliteration method has
three steps. In the \x0crst, we make a phoneme
network that shows all possible transliterations
of the given word. In the second step, we apply
phoneme chunks, extracted from training data,
to calculate the reliability of each possible
transliteration. Then we obtain probable
transliterations of the given English word.
1 Introduction
In Korean technical documents, many English
words are used in their original forms. But
sometimes they are transliterated into Korean
in di\x0berent forms. Ex. 1, 2 show the examples
of various transliterations in KTSET 2.0(Park
et al., 1996).
(1) data
(a) HL\x19$\x1e"(teyitha) [1,033]1
(b) HL\x19$\')(teyithe) [527]
1
the frequency in KTSET
(2) digital
(a) \x11$\x1a$OUe(ticithul) [254]
(b) \x11$\x1a$\x1e"(tichithal) [7]
(c) \x11$\x1a$*,(ticithel) [6]
These various transliterations are not negligi-
ble for natural language processing, especiallyin
information retrieval. Because same words are
treated as di\x0berent ones, the calculation based
on the frequency of word would produce mis-
leading results. An experiment shows that the
e\x0bectiveness of information retrieval increases
when various forms including English words are
treated equivalently(Jeong et al., 1997).
We may use a dictionary, to \x0cnd a correct
transliteration and its variations. But it is not
feasible because transliterated words are usually
technical terms and proper nouns that have rich
productivity. Therefore an automatic translit-
eration system is needed to \x0cnd transliterations
without manual intervention.
There have been some studies on E-K
transliteration. They tried to explain translit-
eration as phoneme-per-phoneme or alphabet-
per-phoneme classi\x0ccation problem. They re-
stricted the information length to two or three
units before and behind an input unit. In fact,
many linguistic phenomena involved in the E-K
transliteration are expressed in terms of units
that exceed a phoneme and an alphabet. For
example, `a\' in `ace\' is transliterated into \\9L
\x19$(eyi)" but in `acetic\', \\\x19)(e)" and in `acetone\',
\\\x19"(a)". If we restrict the information length
to two alphabets, then we cannot explain these
phenomena. Three words get the same result
for `a\'.
(3) ace 9L\x19$ah(eyisu)
(4) acetic \x19)\x17$\x1e$ (esithik)
\x0c(5) acetone \x19"8LOOc(aseython)
In this paper, we propose the E-K transliter-
ation model based on phoneme chunks that
do not have a length limit and can explain
transliteration phenomena in some degree of
reliability. Not a alphabet-per-alphabet but a
chunk-per-chunk classi\x0ccation problem.
This paper is organized as follows. In section
2, we survey an E-K transliteration. In section
3, we propose phoneme chunks based translit-
eration and back-transliteration. In Section 4,
the results of experiments are presented. Fi-
nally, the conclusion follows in section 5.
2 English-to-Korean transliteration
E-K transliteration models are classi\x0ced in two
methods: the pivot method and the direct
method. In the pivot method, transliteration
is done in two steps: converting English words
into pronunciation symbols and then converting
these symbols into Korean words by using the
Korean standard conversion rule. In the direct
method, English words are directly converted to
Korean words without intermediate steps. An
experiment shows that the direct method is bet-
ter than the pivot method in \x0cnding variations
of a transliteration(Lee and Choi, 1998). Statis-
tical information, neural network and decision
tree were used to implement the direct method.
2.1 Statistical Transliteration method
An English word is divided into phoneme se-
quence or alphabet sequence as e1;e2;::: ;en.
Then a corresponding Korean word is rep-
resented as k1;k2;::: ;kn. If a correspond-
ing Korean character (ki) does not exist, we
\x0cll the blank with `-\'. For example, an En-
glish word \\dressing" and a Korean word \\\\h
IL\x17$E(tuleysing)" are represented as Fig. 1. The
upper one in Fig. 1 is divided into an English
phoneme unit and the lower one is divided into
an alphabet unit.
G\x12\x0eU\x120\x0eH\x12K\x0e\x03VV\x12<\x0eL\x12Z\x0eQJ\x12>
G\x12\x0eU\x120\x0eH\x12K\x0eV\x12<\x0eV\x12\x10\x0eL\x12Z\x0eQ\x12>\x0eJ\x12\x10
G\x12\x0eU\x120\x0eH\x12K\x0e\x03VV\x12<\x0eL\x12Z\x0eQJ\x12>
G\x12\x0eU\x120\x0eH\x12K\x0eV\x12<\x0eV\x12\x10\x0eL\x12Z\x0eQ\x12>\x0eJ\x12\x10
dressing : Eo
dressing : Eo
Figure 1: An E-K transliteration example
The problem in statistical transliteration
method is to \x0cndout the most probable translit-
eration for a given word. Let p(K) be the prob-
ability of a Korean word K, then, for a given
English word E, the transliteration probability
of a word K can be written as p(KjE). By using
the Bayes\' theorem, we can rewrite the translit-
eration problem as follows:
arg max
K p(KjE) = arg max
K p(K)p(EjK) (1)
With the Markov Independent Assumption,
we approximate p(K) and p(EjK) as follows:
p(K) \x18
= p(k1)
n
Y
i=2
p(kijki,1) (2)
p(EjK) \x18
=
n
Y
i=1
p(eijki) (3)
As we do not know the pronunciation of a
given word, we consider all possible phoneme
sequences. For example, `data\' has following
possible phoneme sequences, `d-a-t-a, d-at-a,
da-ta, :::\'.
As the history length is lengthened, we can
get more discrimination. But long history in-
formation causes a data sparseness problem. In
order to solve a sparseness problem, Maximum
Entropy Model, Back-o\x0b, and Linear interpola-
tion methods are used. They combine di\x0berent
statistical estimators. (Tae-il Kim, 2000) use up
to \x0cve phonemes in feature function(Berger et
al., 1996). Nine feature functions are combined
with Maximum Entropy Method.
2.2 Neural Network and Decision Tree
Methods based on neural network and decision
tree deterministically decide a Korean charac-
ter for a given English input. These methods
take two or three alphabets or phonemes as
an input and generate a Korean alphabet
or phoneme as an output. (Jung-Jae Kim,
1999) proposed a neural network method that
uses two surrounding phonemes as an input.
(Kang, 1999) proposed a decision tree method
that uses six surrounding alphabets. If an
input does not cover the phenomena of proper
transliterations, we cannot get a correct answer.
\x0cEven though we use combining methods to
solve the data sparseness problem, the increase
of an information length would double the
complexity and the time cost of a problem. It
is not easy to increase the information length.
To avoid these di\x0eculties, previous studies
does not use previous outputs(ki,1). But it
loses good information of target language.
Our proposed method is based on the direct
method to extract the transliteration and its
variations. Unlike other methods that deter-
mine a certain input unit\'s output with history
information, we increase the reliability of a cer-
tain transliteration, with known E-K transliter-
ation phenomena (phoneme chunks).
3 Transliteration using Multiple
unbounded overlapping phoneme
chunks
For unknown data, we can estimate a Korean
transliteration from hand-written rules. We
can also predict a Korean transliteration with
experimental information. With known English
and Korean transliteration pairs, we can as-
sume possible transliterations without linguistic
knowledge. For example, `scalar\' has common
part with `scale:ah<L\x19$(sukheyil)\', `casino:\x1d"
\x1a$Zf(khacino)\', `koala:VW\x19"\x13"(khoalla)\', and
`car:\x1d"(kha)\' (Fig. 2). We can assume possible
transliteration with these words and their
transliterations. From `scale\' and its transliter-
ation ah<L\x19$(sukheyil), the `sc\' in `scalar\' can be
transliterated as `ah;(sukh)\'. From a `casino\'
example, the `c\' has more evidence that can be
transliterated as `;(kh)\'. We assume that we
can get a correct Korean transliteration, if we
get useful experimental information and their
proper weight that represents reliability.
3.1 The alignment of an English word
with a Korean word
We can align an English word with its translit-
eration in alphabet unit or in phoneme unit.
Korean vowels are usually aligned with English
vowels and Korean consonants are aligned with
English consonants. For example, a Korean
consonant, `2(p)\' can be aligned with English
consonants `b\', `p\', and `v\'. With this heuristic
we can align an English word with its translit-
eration in an alphabet unit and a phoneme unit
with the accuracy of 99.4%(Kang, 1999).
s c a l a r
s c a l e
\x05
\x05 o
o u
 a -
c a s i n o
o
o o
o i 
 U 
k o a l a
o  o
o aa
aa o
c a r
o o
o
Figure 2: the transliteration of `scalar : ah\x1d"
\x13"(sukhalla)\'
3.2 Extraction of Phoneme Chunks
From aligned training data, we extract phoneme
chunks. We enumerate all possible subsets of
the given English-Korean aligned pair. During
enumerating subsets, we add start and end posi-
tion information. From an aligned data \\dress-
ing" and \\\\hIL\x17$E(tuleysing)", we can get subsets
as Table 12.
Table 1: The extraction of phoneme chunks
Context Output
@d d/\\h(d)
d d/\\h(d)
@dr d/\\h(d)+r/)(r)
r r/)(r)
@dre d/\\h(d)+r/)(r)+e/D(ey)
The context stands for a given English al-
phabets, and the output stands for its translit-
eration. We assign a proper weight to each
phoneme chunk with Equation 4.
weight(context : output) = C(output)
C(context) (4)
C(x) means the frequency of x in training data.
Equation 4 shows that the ambiguous phe-
nomenon gets the less evidence. The chunk
weight is transmitted to each phoneme symbol.
To compensate for the length of phoneme, we
multiply the length of phoneme to the weight of
the phoneme chunk(Fig. 3).
2
@ means the start and end position of a word
\x0cweight(surfing: s/
weight(surfing: s/
 +
+ ur
ur/
/
+
+ f/
f/
 +
+ i/l +
i/l + ng
ng/
/
) =
) = 


 2
2
 
 
 2
2

Figure 3: The weight of a chunk and a phoneme
This chunk weight does not mean the relia-
bility of a given transliteration phenomenon.
We know real reliability, after all overlapping
phoneme chunks are applied. The chunk that
has some common part with other chunks
gives a context information to them. Therefore
a chunk is not only an input unit but also
a means to calculate the reliability of other
chunks.
We also extract the connection information.
From aligned training data, we obtain all pos-
sible combinations of Korean characters and
English characters. With this connection in-
formation, we exclude impossible connections
of Korean characters and English phoneme se-
quences. We can get the following connection
information from \\dressing" example(Table 2).
Table 2: Connection Information
English Korean
left right left right
@ d @ \\h(d)
d r \\h(d) )(r)
r e )(r) D(ey)
3.3 A Transliteration Network
For a given word, we get all possible phonemes
and make a Korean transliteration network.
Each node in a network has an Englishphoneme
and a corresponding Korean character. Nodes
are connected with sequence order. For exam-
ple, `scalar\' has the Korean transliteration net-
work as Fig. 4. In this network, we disconnect
some nodes with extracted connection informa-
tion.
After drawing the Korean transliteration net-
work, we apply all possible phoneme chunks
to the network. Each node increases its own
weight with the weight of phoneme symbol in a
phoneme chunks (Fig. 5). By overlapping the
weight, nodes in the longer chunks get more ev-
idence. Then we get the best path that has the

Q
Qu
Q
QY
Q
Q
Q
Q
Q
Q
QO
Q
Q
Q
Q

U
U U
Figure 4: Korean Transliteration Network for
`scalar\'
highest sum of weights, with the Viterbi algo-
rithm. The Tree-Trellis algorithm is used to get
the variations(Soong and Huang, 1991).
st
ar
t
st
ar
t
s/
s/
s/u
s/u
c/Y
c/Y
c/
c/
a/
a/
a/
a/
al
/
al
/
l
/
l
/
l
/
l
/
r
/-
r
/-
a/
a/
a/
a/
ar
/
ar
/
r
/
r
/
end
end
s/
s/
c/
c/ a/
a/
l
/
l
/
a/
a/
l
/
l
/
a/
a/
:
:
+
+
+
+
+
+
s/
s/
c/
c/ a/
a/
l
/
l
/
a/
a/
l
/
l
/
a/
a/
:
:
+ +
s/
s/
c/
c/ a/
a/
l
/
l
/
a/
a/
l
/
l
/
a/
a/
:
:
+ +
+
Figure 5: Weight application example
4 E-K back-transliteration
E-K back transliterationis a more di\x0ecultprob-
lem than E-K transliteration. During the E-K
transliteration, di\x0berent alphabets are treated
equivalently. For example, `f, p\' and `v, b\'
are transliterated into `=(ph)\' and `2(p)\' re-
spectively and the long sound and the short
sound are also treated equivalently. Therefore
the number of possible English phonemes per
a Korean character is bigger than the number
of Korean characters per an English phoneme.
The ambiguity is increased. In E-K back-
transliteration, Korean phonemes and English
phonemes switch their roles. Just switching the
position. A Korean word is aligned with an
English word in a phoneme unit or a character
unit(Fig. 6).
\x12G\x0e0\x12U\x0eK\x12H\x0e<\x12VV\x0eZ\x12L\x0e>\x12QJ
.\x12G\x0eX\x12\x10\x0e0\x12U\x0eK\x12H\x0e<\x12VV\x0eZ\x12L\x0e>\x12QJ
\x12G\x0e0\x12U\x0eK\x12H\x0e<\x12VV\x0eZ\x12L\x0e>\x12QJ
.\x12G\x0eX\x12\x10\x0e0\x12U\x0eK\x12H\x0e<\x12VV\x0eZ\x12L\x0e>\x12QJ
Eo : dressing
Eo : dressing
Figure 6: E-K back-transliteration example
\x0c5 Experiments
Experiments were done in two points of view:
the accuracy test and the variation coverage
test.
5.1 Test Sets
We use two data sets for an accuracy test. Test
Set I is consists of 1,650 English and Korean
word pairs that aligned in a phoneme unit. It
was made by (Lee and Choi, 1998) and tested by
many methods. To compare our method with
other methods, we use this data set. We use
same training data (1,500 words) and test data
(150 words). Test Set II is consists of 7,185
English and Korean word pairs. We use Test
Set II to show the relation between the size of
training data and the accuracy. We use 90%
of total size as training data and 10% as test
data. For a variation coverage test, we use Test
Set III that is extracted from KTSET 2.0. Test
Set III is consists of 2,391 English words and
their transliterations. An English word has 1.14
various transliterations in average.
5.2 Evaluation functions
Accuracy was measured by the percentage of
the number of correct transliterations divided
by the numberof generated transliterations. We
call it as word accuracy(W.A.). We use one
more measure, called character accuracy(C.A.)
that measures the character edit distance be-
tween a correct word and a generated word.
W:A: = no: of correct words
no: of generated words (5)
C:A: = L,(i + d+ s)
L (6)
where L is the length of the original string, and
i;d, and s are the number of insertion, deletion
and substitution respectively. If the dividend is
negative (when L < (i + d + s)), we consider it
as zero(Hall and Dowling, 1980).
For the real usage test, we used variation cov-
erage (V.C.) that considers various usages. We
evaluated both for the term frequency (tf) and
document frequency (df), where tf is the number
of term appearance in the documents and df is
the number of documents that contain the term.
If we set the usage tf (or df) of the translitera-
tions to 1 for each transliteration, we can calcu-
late the transliteration coverage for the unique
word types, single frequency(sf).
V:C: = ftf;df;sfg of found words
ftf;df;sfg of used words (7)
5.3 Accuracy tests
We compare our result [PCa, PCp]3 with the
simple statistical information based model(Lee
and Choi, 1998) [ST], the Maximum Entropy
based model(Tae-il Kim, 2000) [MEM], the
Neural Network model(Jung-Jae Kim, 1999)
[NN] and the Decision Tree based model(Kang,
1999)[DT]. Table 3 shows the result of E-
K transliteration and back-transliteration test
with Test Set I.
Table 3: C.A. and W.A. with Test Set I
E-K trans. E-K back trans.
method C.A. W.A. C.A. W.A.
ST 69.3% 40.7%4 60.5% -
MEM 72.3% 43.3% - -
NN 79.0% 35.1% - -
DT 78.1% 37.6% 77.1% 31.0%
PCp 86.5% 55.3% 81.4% 34.7%
PCa 85.3% 46.7% 79.3% 32.6%
Fig. 7, 8 show the results of our proposed
method with the size of training data, Test Set
II. We compare our result with the decision tree
based method.
^`
_`
``
a`
b`
c`
d`
\\[[[ ][[[ ^[[[ _[[[ `[[[ a[[[
nYlY o\x7f
nYlY {n
nYlY{n
YlY o\x7f
YlY {n
YlY {n
Figure 7: E-K transliteration results with Test
Set II
3
PC stands for phoneme chunks based method and
a and b stands for aligned by an alphabet unit and a
phoneme unit respectively
4
with 20 higher rank results
\x0c][
^[
_[
`[
a[
b[
c[
d[
\\[[[ ][[[ ^[[[ _[[[ `[[[ a[[[
nYlY o\x7f
nYlY {n
nYlY {n
YlY o\x7f
YlY {n
YlY {n
Figure 8: E-K back-transliteration results with
Test Set II
With Test Set II, we can get the following
result (Table 4).
Table 4: C.A. and W.A. with the Test Set II
E-K trans. E-K back trans.
method C.A. W.A. C.A. W.A.
PCp 89.5% 57.2% 84.9% 40.9%
PCa 90.6% 58.3% 84.8% 40.8%
5.4 Variation coverage tests
To compare our result(PCp) with (Lee and
Choi, 1998), we trained our methods with the
training data of Test Set I. In ST, (Lee and
Choi, 1998) use 20 high rank results, but we
just use 5 results. Table 5 shows the coverage
of our proposed method.
Table 5: variation coverage with Test Set III
method tf df sf
ST 76.0% 73.9% 47.1%
PCp 84.0% 84.0% 64.0%
Fig. 9 shows the increase of coverage with the
number of outputs.
5.5 Discussion
We summarize the information length and the
kind of information(Table 6). The results of
experiments and information usage show that
MEM combines various information better
than DT and NN. ST does not use a previous
input (ei,1) but use a previous output(ki,1) to
calculate the current output\'s probability like
_`
`[
``
a[
a`
b[
b`
c[
c`
d[
d`
\\ ] ^ _ ` a b c d \\[



Figure 9: The V.C. result
Table 6: Information Usage
method before behind previous output
ST 2 0 Y
MEM 2 2 N
NN 1 1 N
DT 3 3 N
PC - - Y
Part-of-Speech Tagging problem. But ST gets
the lowest accuracy. It means that surrounding
alphabets give more information than previous
output. In other words, E-K transliteration is
not the alphabet-per-alphabet or phoneme-per-
phoneme classi\x0ccation problem. A previous
output does not give enough information for
current unit\'s disambiguation. An input unit
and an output unit should be extended. E-K
transliteration is a chunk-per-chunk classi\x0cca-
tion problem.
We restrict the length of information, to see
the in
uence of phoneme-chunk size. Fig. 10
shows the results.
[
\\[
][
^[
_[
`[
a[
b[
c[
d[
\\ ] ^ _ ` a b
nYlY \x7f ~ t
nYlY \x7f ~ tt
YlY \x7f ~ t
YlY \x7f ~ tt
Figure 10: the result of a length limit test
\x0cWith the same length of information, we
get the higher C.A. and W.A. than other
methods. It means previous outputs give good
information and our chunk-based method is
a good combining method. It also suggests
that we can restrict the max size of chunk in a
permissible size.
PCa gets a higher accuracy than PCp. It
is due to the number of possible phoneme se-
quences. A transliteration network that con-
sists of phoneme unit has more nodes than a
transliteration network that consists of alpha-
bet unit. With small training data, despite of
the loss due to the phoneme sequences ambi-
guity a phoneme gives more information than
an alphabet. When the information is enough,
PCa outperforms PCp.
6 Conclusions
We propose the method of English-to-Korean
transliteration and back-transliteration with
multiple unbounded overlapping phoneme
chunks. We showed that E-K transliteration
and back-transliteration are not a phoneme-
per-phoneme and alphabet-per-alphabet
classi\x0ccation problem. So we use phoneme
chunks that do not have a length limit and
can explain E-K transliteration phenomena.
We get the reliability of a given transliter-
ation phenomenon by applying overlapping
phoneme chunks. Our method is simple and
does not need a complex combining method
for various length of information. The change
of an information length does not a\x0bect the
internal representation of the problem. Our
chunk-based method can be used to other
classi\x0ccation problems and can give a simple
combining method.
References
Tae-il Kim. 2000. English to Korean translit-
eration model using maximum entropy model
for cross language information retrieval. Mas-
ter\'s thesis, Seogang University (in Korean).
Kil Soon Jeong, Sung Hyun Myaeng, Jae Sung
Lee, and Key-Sun Choi. 1999. Automatic
identi\x0ccation and back-transliteration of for-
eign words for information retrieval. Infor-
mation Processing and Management.
Key-Sun Choi Jung-Jae Kim, Jae Sung Lee.
1999. Pronunciation unit based automatic
English-Korean transliteration model using
neural network. In Proceedings of Korea Cog-
nitive Science Association(in Korean).
Byung-Ju Kang. 1999. Automatic Korean-
English back-transliteration. In Proceedings
of the 11th Conference on Hangul and Ko-
rean Language Information Processing(in Ko-
rean).
Jae Sung Lee and Key-Sun Choi. 1998. English
to Korean statistical transliteration for in-
formation retrieval. Computer Processing of
Oriental Languages.
K. Jeong, Y. Kwon, and S. H. Myaeng. 1997.
The e\x0bect of a proper handling of foreign and
English words in retrieving Korean text. In
Proceedings of the 2nd International Work-
shop on Information Retrieval with Asian
Languages.
K. Knight and J. Graehl. 1997. Machine
transliteration. In Proceedings of the 35th
Annual Meeting of the Association for Com-
putational Linguistics.
Adam L. Berger, Stephen A. Della Pietra, and
Vincent J. Della Pietra. 1996. A maximum
entropy approach to natural language pro-
cessing. Computational Linguistics.
Y. C. Park, K. Choi, J. Kim, and Y. Kim.
1996. Development of the data collection ver.
2.0ktset2:0 for Korean information retrieval
studies. In Arti\x0ccial Intelligence Spring Con-
ference. Korea Information Science Society
(in Korean).
Frank K. Soong and Eng-Fong Huang. 1991.
A tree-trellis based fast search for \x0cnding
the n best sentence hypotheses in continuous
speech recognition. In IEEE International
Conference on Acoustic Speech and Signal
Processing, pages 546{549.
P. Hall and G. Dowling. 1980. Approximate
string matching. Computing Surveys.
\x0c'