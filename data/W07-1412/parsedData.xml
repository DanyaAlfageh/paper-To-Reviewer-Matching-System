<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<figure confidence="0.650989625">
b&apos;Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 7277,
Prague, June 2007. c
2007 Association for Computational Linguistics
Shallow Semantics in Fast Textual Entailment Rule Learners
Fabio Massimo Zanzotto
DISP
University of Rome Tor Vergata
Roma, Italy
zanzotto@info.uniroma2.it
Marco Pennacchiotti
Computerlinguistik
Universitat des Saarlandes,
Saarbrucken, Germany
pennacchiotti@coli.uni-sb.de
Alessandro Moschitti
DIT
</figure>
<affiliation confidence="0.7584595">
University of Trento
Povo di Trento, Italy
</affiliation>
<email confidence="0.957324">
moschitti@dit.unitn.it
</email>
<sectionHeader confidence="0.987175" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999480142857143">
In this paper, we briefly describe two
enhancements of the cross-pair similarity
model for learning textual entailment rules:
1) the typed anchors and 2) a faster compu-
tation of the similarity. We will report and
comment on the preliminary experiments
and on the submission results.
</bodyText>
<sectionHeader confidence="0.998159" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992454604651163">
Results of the second RTE challenge (Bar Haim et
al., 2006) have suggested that both deep semantic
models and machine learning approaches can suc-
cessfully be applied to solve textual entailment. The
only problem seems to be the size of the knowledge
bases. The two best systems (Tatu et al., 2005; Hickl
et al., 2005), which are significantly above all the
others (more than +10% accuracy), use implicit or
explicit knowledge bases larger than all the other
systems. In (Tatu et al., 2005), a deep semantic
representation is paired with a large amount of gen-
eral and task specific semantic rules (explicit knowl-
edge). In (Hickl et al., 2005), the machine learning
model is trained over a large amounts of examples
(implicit knowledge).
In contrast, Zanzotto&amp;Moschitti (2006) proposed
a machine-learning based approach which reaches a
high accuracy by only using the available RTE data.
The key idea is the cross-pair similarity, i.e. a simi-
larity applied to two text and hypothesis pairs which
considers the relations between the words in the two
texts and between the words in the two hypotheses.
This is obtained by using placeholders to link the re-
lated words. Results in (Bar Haim et al., 2006) are
comparable with the best machine learning system
when this latter is trained only on the RTE exam-
ples.
Given the high potential of the cross-pair similar-
ity model, for the RTE3 challenge, we built on it by
including some features of the two best systems: 1)
we go towards a deeper semantic representation of
learning pairs including shallow semantic informa-
tion in the syntactic trees using typed placeholders;
2) we reduce the computational cost of the cross-pair
similarity computation algorithm to allow the learn-
ing over larger training sets.
The paper is organized as follows: in Sec. 2 we
review the cross-pair similarity model and its limits;
in Sec. 3, we introduce our model for typed anchors;
in Sec. 4 we describe how we limit the computa-
tional cost of the similarity; in Sec. 5 we present the
two submission experiments, and in Sec. 6 we draw
some conclusions.
</bodyText>
<sectionHeader confidence="0.509987" genericHeader="method">
2 Cross-pair similarity and its limits
</sectionHeader>
<subsectionHeader confidence="0.738552">
2.1 Learning entailment rules with syntactic
</subsectionHeader>
<bodyText confidence="0.977783727272727">
cross-pair similarity
The cross-pair similarity model (Zanzotto and
Moschitti, 2006) proposes a similarity measure
aiming at capturing rewrite rules from train-
ing examples, computing a cross-pair similarity
KS((T0, H0), (T00, H00)). The rationale is that if two
pairs are similar, it is extremely likely that they have
the same entailment value. The key point is the use
of placeholders to mark the relations between the
sentence words. A placeholder co-indexes two sub-
structures in the parse trees of text and hypothesis,
</bodyText>
<page confidence="0.996339">
72
</page>
<bodyText confidence="0.9964925">
\x0cindicating that such substructures are related. For
example, the sentence pair, All companies file an-
nual reports implies All insurance companies file
annual reports, is represented as follows:
</bodyText>
<table confidence="0.934099714285714">
T1 (S (NP: 1 (DT All) (NNS: 1 compa-
nies)) (VP: 2 (VBP: 2 file) (NP: 3 (JJ: 3
annual) (NNS: 3 reports))))
H1 (S (NP: 1 (DT All) (NNP Fortune)
(CD 50) (NNS: 1 companies)) (VP: 2
(VBP: 2 file) (NP: 3 (JJ: 3 annual)
(NNS: 3 reports))))
</table>
<equation confidence="0.425552">
(E1)
</equation>
<bodyText confidence="0.98038175">
where the placeholders 1 , 2 , and 3 indicate the rela-
tions between the structures of T and of H.
Placeholders help to determine if two pairs share
the same rewriting rule by looking at the subtrees
that they have in common. For example, suppose
we have to determine if In autumn, all leaves fall
implies In autumn, all maple leaves fall. The re-
lated co-indexed representation is:
</bodyText>
<figure confidence="0.668632461538462">
T2 (S (PP (IN In) (NP (NN: a automn)))
(, ,) (NP: b (DT all) (NNS: b leaves))
(VP: c (VBP: c fall)))
H2 (S (PP (IN In) (NP: a (NN: a automn)))
(, ,) (NP: b (DT all) (NN maple)
(NNS: a leaves)) (VP: c (VBP: c fall)))
(E2)
E1 and E2 share the following subtrees:
T3 (S (NP: x (DT all) (NNS: x )) (VP: y
(VBP: y )))
H3 (S (NP: x (DT all) (NN) (NNS: x ))
(VP: x (VBP: x )))
(R3)
</figure>
<bodyText confidence="0.994224333333333">
This is the rewrite rule they have in common. Then,
E2 can be likely classified as a valid entailment, as
it shares the rule with the valid entailment E1.
The cross-pair similarity model uses: (1) a tree
similarity measure KT (1, 2) (Collins and Duffy,
2002) that counts the subtrees that 1 and 2 have
in common; (2) a substitution function t(, c) that
changes names of the placeholders in a tree accord-
ing to a set of correspondences between placehold-
ers c. Given C as the collection of all correspon-
dences between the placeholders of (T0, H0) and
(T00, H00), the cross-pair similarity is computed as:
</bodyText>
<equation confidence="0.998479909090909">
KS((T0
, H0
), (T00
, H00
)) =
maxcC(KT (t(T0
, c), t(T00
, c)) + KT (t(H0
, c), t(H00
, c)))
(1)
</equation>
<bodyText confidence="0.99845525">
The cross-pair similarity KS, used in a kernel-based
learning model as the support vector machines, al-
lows the exploitation of implicit true and false en-
tailment rewrite rules described in the examples.
</bodyText>
<subsectionHeader confidence="0.992444">
2.2 Limits of the syntactic cross-pair similarity
</subsectionHeader>
<bodyText confidence="0.967559529411765">
Learning from examples using cross-pair similarity
is an attractive and effective approach. However,
the cross-pair strategy, as any machine learning ap-
proach, is highly sensitive on how the examples are
represented in the feature space, as this can strongly
bias the performance of the classifier.
Consider for example the following text-
hypothesis pair, which can lead to an incorrect rule,
if misused.
T4 For my younger readers, Chapman
killed John Lennon more than twenty
years ago.
H4 John Lennon died more than twenty
years ago.
(E4)
In the basic cross-pair similarity model, the learnt
rule would be the following:
</bodyText>
<figure confidence="0.9463584">
T5 (S (NP: x ) (VP: y (VBD: y ) (NP: z )
(ADVP: k )))
H5 (S (NP: z ) (VP: y (VBD: y )
(ADVP: k )))
(R5)
</figure>
<bodyText confidence="0.985666">
where the verbs kill and die are connected by the y
placeholder. This rule is useful to classify examples
</bodyText>
<listItem confidence="0.772733">
like:
T6 Cows are vegetarian but, to save
money on mass-production, farmers fed
cows animal extracts.
H6 Cows have eaten animal extracts.
(E6)
but it will clearly fail when used for:
T7 FDA warns migraine medicine makers
that they are illegally selling migraine
medicines without federal approval.
H7 Migraine medicine makers declared
that their medicines have been ap-
proved.
(E7)
</listItem>
<bodyText confidence="0.99659">
where warn and declare are connected as generically
similar verbs.
The problem of the basic cross-pair similarity
measure is that placeholders do not convey the
semantic knowledge needed in cases such as the
above, where the semantic relation between con-
nected verbs is essential.
</bodyText>
<subsectionHeader confidence="0.999087">
2.3 Computational cost of the cross-similarity
</subsectionHeader>
<bodyText confidence="0.985977857142857">
measure
Let us go back to the computational cost of KS (eq.
1). It heavily depends on the size of C. We de-
fine p0 and p00 as the placeholders of, respectively,
(T0, H0) and (T00, H00). As C is combinatorial with
respect to |p0 |and |p00|, |C |rapidly grows. Assigning
placeholders only to chunks helps controlling their
</bodyText>
<page confidence="0.996812">
73
</page>
<bodyText confidence="0.999416166666666">
\x0cnumber. For example, in the RTE data the number
of placeholders hardly goes beyond 7, as hypothe-
ses are generally short sentences. But, even in these
cases, the number of KT computations grows. As
the trees t(, c) are obtained from a single tree
(containing placeholder) applying different c C,
it is reasonable to think that they will share com-
mon subparts. Then, during the iterations of c
C, KT (t(0, c), t(00, c)) will compute the similarity
between subtrees that have already been evaluated.
The reformulation of the cross-pair similarity func-
tion we present takes advantage of this.
</bodyText>
<sectionHeader confidence="0.763684" genericHeader="method">
3 Adding semantic information to
</sectionHeader>
<bodyText confidence="0.9874443125">
cross-pair similarity
The examples in the previous section show that
the cross-pairs approach lacks the lexical-semantic
knowledge connecting the words in a placeholder.
In the examples, the missed knowledge is the type
of semantic relation between the main verbs. The
relation that links kill and die is not a generic sim-
ilarity, as a WordNet based similarity measure can
suggest, but a more specific causal relation. The
learnt rewrite rule R5 holds only for verbs in such
relation. In facts, it is correctly applied in example
E6, as feed causes eat, but it gives a wrong sugges-
tion in example E7, since warn and declare are only
related by a generic similarity relation.
We then need to encode this information in the
syntactic trees in order to learn correct rules.
</bodyText>
<subsectionHeader confidence="0.998752">
3.1 Defining anchor types
</subsectionHeader>
<bodyText confidence="0.983845666666667">
The idea of introducing anchor types should be in
principle very simple and effective. Yet, this may be
not the case: simpler attempts to introduce semantic
information in RTE systems have often failed. To
investigate the validity of our idea, we then need to
focus on a small set of relevant relation types, and to
carefully control ambiguity for each type.
A valuable source of relation types among words
is WordNet. We choose to integrate in our system
three important relation standing at the word level:
part-of, antinomy, and verb entailment. We also de-
fine two more general anchor types: similarity and
the surface matching. The first type links words
which are similar according to some WordNet simi-
larity measure. Specifically, this type is intended to
</bodyText>
<listItem confidence="0.900220333333333">
Rank Relation Type Symbol
1. antinomy
2. part-of
3. verb entailment
4. similarity
5. surface matching =
</listItem>
<tableCaption confidence="0.991292">
Table 1: Ranked anchor types
</tableCaption>
<bodyText confidence="0.9741322">
capture the semantic relations of synonymy and hy-
peronymy. The second type is activated when words
or lemmas match: then, it captures cases in which
words are semantically equivalent. The complete set
of relation types used in the experiments is given in
</bodyText>
<tableCaption confidence="0.977289">
Table 1.
</tableCaption>
<subsectionHeader confidence="0.970768">
3.2 Type anchors in the syntactic tree
</subsectionHeader>
<bodyText confidence="0.9970729375">
To learn more correct rewrite rules by using the an-
chor types defined in the previous section, we need
to add this information to syntactic trees. The best
position would be in the same nodes of the anchors.
Also, to be more effective, this information should
be inserted in as many subtrees as possible. Thus we
define the typed-anchor climbing-up rules. We then
implement in our model the following climbing up
rule:
if two typed anchors climb up to the same
node, give precedence to that with the high-
est ranking in Tab. 1.
This rule can be easily showed to be consistent with
common sense intuitions. For an example like John
is a tall boy that does not entail John is a short
boy, our strategy will produce these trees:
</bodyText>
<equation confidence="0.8513962">
(E8)
T8 H8
S 3
NP = 1
NNP = 1
</equation>
<figure confidence="0.87738808">
John
VP 2
AUX
is
NP 3
DT
a
JJ 2
tall
NN = 3
boy
S 3
NP = 1
NNP = 1
John
VP 2
AUX
is
NP 3
DT
a
JJ 2
short
NN = 3
boy
</figure>
<bodyText confidence="0.9821968">
This representation can be used to derive a correct
rewrite rule, such as:
if two fragments have the same syntactic struc-
ture S(NP1, V P(AUX, NP2)), and there is an
antonym type () on the S and NP2 , then the
</bodyText>
<page confidence="0.893608">
74
</page>
<equation confidence="0.993616877777777">
\x0cc1 = {(a, 1), (b, 2), (c, 3)} c2 = {(a, 1), (b, 2), (d, 3)}
1 t(1, c1) t(1, c2)
X1 a
A2 a
B3 a
w1
a
C4 b
w2
b
D5 d
D6 c
w3
c
C7 d
w4
d
X1 a:1
A2 a:1
B3 a:1
w1
a:1
C4 b:2
w2
b:2
D5 d
D6 c:3
w3
c:3
C7 d
w4
d
X1 a:1
A2 a:1
B3 a:1
w1
a:1
C4 b:2
w2
b:2
D5 d:3
D6 c
w3
c
C7 d:3
w4
d:3
2 t(2, c1) t(2, c2)
X1 1
A2 1
B3 1
m1
1
C4 2
m2
2
D5
D6 3
m3
3
C7
m4
X1 a:1
A2 a:1
B3 a:1
m1
a:1
C4 b:2
m2
b:2
D5
D6 c:3
m3
c:3
C7
m4
X1 a:1
A2 a:1
B3 a:1
m1
a:1
C4 b:2
m2
b:2
D5
D6 d:3
m3
d:3
C7
m4
</equation>
<figureCaption confidence="0.59589075">
Figure 1: Tree pairs with placeholders and t(T, c) transformation
entailment does not hold.
4 Reducing computational cost of the
cross-pair similarity computation
</figureCaption>
<subsectionHeader confidence="0.994625">
4.1 The original kernel function
</subsectionHeader>
<bodyText confidence="0.998074">
In this section, we describe more in detail the simi-
larity function KS (Eq. 1). To simplify, we focus on
the computation of only one KT of the kernel sum.
</bodyText>
<equation confidence="0.995787571428571">
KS(0
, 00
) = max
cC
KT (t(0
, c), t(00
, c)), (2)
</equation>
<bodyText confidence="0.991120821428571">
where the (0, 00) pair can be either (T0, T00) or
(H0, H00). We apply this simplification since we
are interested in optimizing the evaluation of the
KT with respect to different sets of correspondences
c C.
To better explain KS, we need to analyze the role
of the substitution function t(, c) and to review the
tree kernel function KT .
The aim of t(, c) is to coherently replace place-
holders in two trees 0 and 00 so that these two trees
can be compared. The substitution is carried out
according to the set of correspondences c. Let p0
and p00 be placeholders of 0 and 00, respectively,
if p00 p0 then c is a bijection between a subset
b
p0 p0 and p00. For example (Fig. 1), the trees 1
has p1 ={a , b , c , d } as placeholder set and 2 has
p2 ={1 , 2 , 3 }. In this case, a possible set of corre-
spondence is c1 = {(a, 1), (b, 2), (c, 3)}. In Fig. 1
the substitution function replaces each placeholder
a of the tree 1with the new placeholder a:1 by
t(, c) obtaining the transformed tree t(1, c1), and
each placeholder 1 of 2 with a:1 . After these sub-
stitutions, the labels of the two trees can be matched
and the similarity function KT is applicable.
KT (0, 00), as defined in (Collins and Duffy,
2002), computes the number of common subtrees
between 0 and 00.
</bodyText>
<subsectionHeader confidence="0.870817">
4.2 An observation to reduce the
</subsectionHeader>
<bodyText confidence="0.9930165">
computational cost
The above section has shown that the similarity
function KS firstly applies the transformation t(, c)
and then computes the tree kernel KT . The overall
process can be optimized by factorizing redundant
KT computations.
Indeed, two trees, t(, c0) and t(, c00), obtained
by applying two sets of correspondences c0, c00 C,
may partially overlap since c0 and c00 can share a non-
empty set of common elements. Let us consider the
subtree set S shared by t(, c0) and t(, c00) such
that they contain placeholders in c0 c00 = c, then
t(, c) = t(, c0) = t(, c00) S. Therefore if
we apply a tree kernel function KT to a pair (0, 00),
we can find a c such that subtrees of 0 and subtrees
of 00 are invariant with respect to c0 and c00. There-
fore, KT (t(0, c), t(00, c)) = KT (t(0, c0), t(00, c0))
= KT (t(0, c00), t(00, c00)). This implies that it is
possible to refine the dynamic programming algo-
rithm used to compute the matrices while com-
</bodyText>
<page confidence="0.987669">
75
</page>
<bodyText confidence="0.998773964285714">
\x0cputing the kernel KS(0, 00).
To better explain this idea let us consider
Fig. 1 that represents two trees, 1 and 2,
and the application of two different transforma-
tions c1 = {(a, 1), (b, 2), (c, 3)} and c2 =
{(a, 1), (b, 2), (d, 3)}. Nodes are generally in the
form Xi z where X is the original node label, z is
the placeholder, and i is used to index nodes of the
tree. Two nodes are equal if they have the same node
label and the same placeholder. The first column of
the figure represents the original trees 1 and 2.
The second and third columns contain respectively
the transformed trees t(, c1) and t(, c2)
Since the subtree of 1 starting from A2 a con-
tains only placeholders that are in c, in the trans-
formed trees, t(1, c1) and t(1, c2), the subtrees
rooted in A2 a:1 are identical. The same happens
for 2 with the subtree rooted in A2 1. In the trans-
formed trees, t(2, c1) and t(2, c2), subtrees rooted
in A2 a:1 are identical. The computation of KT
applied to the above subtrees gives an identical re-
sult. Then, this computation can be avoided. If cor-
rectly used in a dynamic programming algorithm,
the above observation can produce an interesting de-
crease in the time computational cost. More de-
tails on the algorithm and the decrease in computa-
tional cost may be found in (Moschitti and Zanzotto,
2007).
</bodyText>
<sectionHeader confidence="0.997819" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.583704">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.997696892857143">
We implemented the novel cross-similarity kernel
in the SVM-light-TK (Moschitti, 2006) that en-
codes the basic syntactic kernel KT in SVM-light
(Joachims, 1999).
To assess the validity of the typed anchor model
(tap), we evaluated two sets of systems: the plain
and lexical-boosted systems. The plain systems are:
-tap: our tree-kernel approach using typed place-
holders with climbing in the syntactic tree;
-tree: the cross-similarity model described in Sec.2.
Its comparison with tap indicates the effectiveness
of our approaches;
The lexical-boosted systems are:
-lex: a standard approach based on lexical over-
lap. The classifier uses as the only feature the lexi-
cal overlap similarity score described in (Corley and
Mihalcea, 2005);
-lex+tap: these configurations mix lexical overlap
and our typed anchor approaches;
-lex+tree: the comparison of this configuration with
lex+tap should further support the validity of our in-
tuition on typed anchors;
Preliminary experiments have been performed us-
ing two datasets: RTE2 (the 1600 entailment pairs
from the RTE-2 challenge) and RTE3d (the devel-
opment dataset of this challenge). We randomly
divided this latter in two halves: RTE3d0 and
RTE3d1.
</bodyText>
<subsectionHeader confidence="0.672477">
5.2 Investigatory Results Analysis and
Submission Results
</subsectionHeader>
<bodyText confidence="0.998030484848485">
Table 2 reports the results of the experiments. The
first column indicates the training set whereas the
second one specifies the used test set. The third and
the forth columns represent the accuracy of basic
models: the original tree model and the enhanced
tap model. The latter three columns report the basic
lex model and the two combined models, lex+tree
and lex+tap. The second and the third rows repre-
sent the accuracy of the models with respect to the
first randomly selected half of RTE3d whilst the
last two rows are related to the second half.
The experimental results show some interesting
facts. In the case of the plain systems (tree and tap),
we have the following observations:
- The use of the typed anchors in the model seems
to be effective. All the tap model results are higher
than the corresponding tree model results. This sug-
gests that the method used to integrate this kind of
information in the syntactic tree is effective.
- The claim that using more training material helps
seems not to be supported by these experiments. The
gap between tree and tap is higher when learn-
ing with RTE2 + RTE3d0 than when learning
with RTE30. This supports the claim. How-
ever, the result is not kept when learning with
RTE2 + RTE3d1 with respect to when learning
with RTE31. This suggests that adding not very
specific information, i.e. derived from corpora dif-
ferent from the target one (RTE3), may not help the
learning of accurate rules.
On the other hand, in the case of the lexical-
boosted systems (lex, lex+tree, and lex+tap), we
see that:
</bodyText>
<page confidence="0.957609">
76
</page>
<table confidence="0.9983386">
\x0cTrain Test tree tap lex lex+tree lex+tap
RTE3d0 RTE3d1 62.97 64.23 69.02 68.26 69.02
RTE2 + RTE3d0 RTE3d1 62.22 62.47 71.03 71.28 71.79
RTE3d1 RTE3d0 62.03 62.78 70.22 70.22 71.22
RTE2 + RTE3d0 RTE3d0 63.77 64.76 71.46 71.22 72.95
</table>
<tableCaption confidence="0.998387">
Table 2: Accuracy of the systems on two folds of RTE3 development
</tableCaption>
<bodyText confidence="0.982835782608696">
- There is an extremely high accuracy result for the
pure lex model. This result is counterintuitive. A
model like lex has been likely used by QA or IE
systems to extract examples for the RTE3d set. If
this is the case we may expect that positive and
negative examples should have similar values for
this lex distance indicator. It is then not clear why
this model results in so high accuracy.
- Given the high results of the lex model, the model
lex+tree does not increase the performances.
- On the contrary, the model lex+tap is always better
(or equal) than the lex model. This suggests that
for this particular set of examples the typed anchors
are necessary to effectively use the rewriting rules
implicitly encoded in the examples.
- When the tap model is used in combination with
the lex model, it seems that the claim the more
training examples the better is valid. The gaps
between lex and lex+tap are higher when the RTE2
is used in combination with the RTE3d related set.
Given this analysis we submitted two systems
both based on the lex+tap model. We did two differ-
ent training: one using RTE3d and the other using
</bodyText>
<figure confidence="0.7724826">
RTE2 + RTE3d. Results are reported in the Table
below:
Train Accuracy
RTE3d 66.75%
RTE2 + RTE3d 65.75%
</figure>
<bodyText confidence="0.982122375">
Such results seem too low to be statistically consis-
tent with our development outcome. This suggests
that there is a clear difference between the content
of RTE3d and the RTE3 test set. Moreover, in
contrast with what expected, the system trained with
only the RTE3d data is more accurate than the oth-
ers. Again, this suggests that the RTE corpora (from
all the challenges) are most probably very different.
</bodyText>
<sectionHeader confidence="0.995533" genericHeader="conclusions">
6 Conclusions and final remarks
</sectionHeader>
<bodyText confidence="0.998449333333333">
This paper demonstrates that it is possible to ef-
fectively include shallow semantics in syntax-based
learning approaches. Moreover, as it happened in
RTE2, it is not always true that more learning ex-
amples increase the accuracy of RTE systems. This
claim is still under investigation.
</bodyText>
<sectionHeader confidence="0.991231" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999495216216216">
Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Gi-
ampiccolo, Bernardo Magnini, and Idan Szpektor. 2006.
The II PASCAL RTE challenge. In PASCAL Challenges
Workshop, Venice, Italy.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proceedings of ACL02.
Courtney Corley and Rada Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proc. of the ACL Workshop
on Empirical Modeling of Semantic Equivalence and Entail-
ment, pages 1318, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2005. Recognizing textual en-
tailment with LCCs GROUNDHOG system. In Proceedings
of the Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
Thorsten Joachims. 1999. Making large-scale svm learning
practical. In B. Schlkopf, C. Burges, and A. Smola, editors,
Advances in Kernel Methods-Support Vector Learning. MIT
Press.
Alessandro Moschitti and Fabio Massimo Zanzotto. 2007.
Fast and effective kernels for relational learning from texts.
In Proceedings of the International Conference of Machine
Learning (ICML), Corvallis, Oregon.
Alessandro Moschitti. 2006. Making tree kernels practical
for natural language learning. In Proceedings of EACL06,
Trento, Italy.
Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi, and
Dan Moldovan. 2005. COGEX at the second recognizing
textual entailment challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Textual En-
tailment, Venice, Italy.
Fabio Massimo Zanzotto and Alessandro Moschitti. 2006. Au-
tomatic learning of textual entailments with cross-pair sim-
ilarities. In Proceedings of the 21st Coling and 44th ACL,
pages 401408, Sydney, Australia, July.
</reference>
<page confidence="0.953406">
77
</page>
<figure confidence="0.270526">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.454500">
<note confidence="0.911140666666667">b&apos;Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 7277, Prague, June 2007. c 2007 Association for Computational Linguistics</note>
<title confidence="0.987702">Shallow Semantics in Fast Textual Entailment Rule Learners</title>
<author confidence="0.999856">Fabio Massimo Zanzotto</author>
<affiliation confidence="0.988138">DISP University of Rome Tor Vergata</affiliation>
<address confidence="0.991229">Roma, Italy</address>
<email confidence="0.99511">zanzotto@info.uniroma2.it</email>
<author confidence="0.999475">Marco Pennacchiotti</author>
<affiliation confidence="0.9455925">Computerlinguistik Universitat des Saarlandes,</affiliation>
<address confidence="0.991379">Saarbrucken, Germany</address>
<email confidence="0.997604">pennacchiotti@coli.uni-sb.de</email>
<author confidence="0.999789">Alessandro Moschitti</author>
<affiliation confidence="0.888209">DIT University of Trento Povo di Trento, Italy</affiliation>
<email confidence="0.998044">moschitti@dit.unitn.it</email>
<abstract confidence="0.999217125">In this paper, we briefly describe two enhancements of the cross-pair similarity model for learning textual entailment rules: 1) the typed anchors and 2) a faster computation of the similarity. We will report and comment on the preliminary experiments and on the submission results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roy Bar Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<date>2006</date>
<booktitle>The II PASCAL RTE challenge. In PASCAL Challenges Workshop,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="880" citStr="Haim et al., 2006" startWordPosition="115" endWordPosition="118">rgata Roma, Italy zanzotto@info.uniroma2.it Marco Pennacchiotti Computerlinguistik Universitat des Saarlandes, Saarbrucken, Germany pennacchiotti@coli.uni-sb.de Alessandro Moschitti DIT University of Trento Povo di Trento, Italy moschitti@dit.unitn.it Abstract In this paper, we briefly describe two enhancements of the cross-pair similarity model for learning textual entailment rules: 1) the typed anchors and 2) a faster computation of the similarity. We will report and comment on the preliminary experiments and on the submission results. 1 Introduction Results of the second RTE challenge (Bar Haim et al., 2006) have suggested that both deep semantic models and machine learning approaches can successfully be applied to solve textual entailment. The only problem seems to be the size of the knowledge bases. The two best systems (Tatu et al., 2005; Hickl et al., 2005), which are significantly above all the others (more than +10% accuracy), use implicit or explicit knowledge bases larger than all the other systems. In (Tatu et al., 2005), a deep semantic representation is paired with a large amount of general and task specific semantic rules (explicit knowledge). In (Hickl et al., 2005), the machine lear</context>
</contexts>
<marker>Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The II PASCAL RTE challenge. In PASCAL Challenges Workshop, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL02.</booktitle>
<contexts>
<context position="4956" citStr="Collins and Duffy, 2002" startWordPosition="828" endWordPosition="831"> (IN In) (NP (NN: a automn))) (, ,) (NP: b (DT all) (NNS: b leaves)) (VP: c (VBP: c fall))) H2 (S (PP (IN In) (NP: a (NN: a automn))) (, ,) (NP: b (DT all) (NN maple) (NNS: a leaves)) (VP: c (VBP: c fall))) (E2) E1 and E2 share the following subtrees: T3 (S (NP: x (DT all) (NNS: x )) (VP: y (VBP: y ))) H3 (S (NP: x (DT all) (NN) (NNS: x )) (VP: x (VBP: x ))) (R3) This is the rewrite rule they have in common. Then, E2 can be likely classified as a valid entailment, as it shares the rule with the valid entailment E1. The cross-pair similarity model uses: (1) a tree similarity measure KT (1, 2) (Collins and Duffy, 2002) that counts the subtrees that 1 and 2 have in common; (2) a substitution function t(, c) that changes names of the placeholders in a tree according to a set of correspondences between placeholders c. Given C as the collection of all correspondences between the placeholders of (T0, H0) and (T00, H00), the cross-pair similarity is computed as: KS((T0 , H0 ), (T00 , H00 )) = maxcC(KT (t(T0 , c), t(T00 , c)) + KT (t(H0 , c), t(H00 , c))) (1) The cross-pair similarity KS, used in a kernel-based learning model as the support vector machines, allows the exploitation of implicit true and false entail</context>
<context position="13309" citStr="Collins and Duffy, 2002" startWordPosition="2393" endWordPosition="2396">tively, if p00 p0 then c is a bijection between a subset b p0 p0 and p00. For example (Fig. 1), the trees 1 has p1 ={a , b , c , d } as placeholder set and 2 has p2 ={1 , 2 , 3 }. In this case, a possible set of correspondence is c1 = {(a, 1), (b, 2), (c, 3)}. In Fig. 1 the substitution function replaces each placeholder a of the tree 1with the new placeholder a:1 by t(, c) obtaining the transformed tree t(1, c1), and each placeholder 1 of 2 with a:1 . After these substitutions, the labels of the two trees can be matched and the similarity function KT is applicable. KT (0, 00), as defined in (Collins and Duffy, 2002), computes the number of common subtrees between 0 and 00. 4.2 An observation to reduce the computational cost The above section has shown that the similarity function KS firstly applies the transformation t(, c) and then computes the tree kernel KT . The overall process can be optimized by factorizing redundant KT computations. Indeed, two trees, t(, c0) and t(, c00), obtained by applying two sets of correspondences c0, c00 C, may partially overlap since c0 and c00 can share a nonempty set of common elements. Let us consider the subtree set S shared by t(, c0) and t(, c00) such that they cont</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of ACL02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Corley</author>
<author>Rada Mihalcea</author>
</authors>
<title>Measuring the semantic similarity of texts.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>1318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="16452" citStr="Corley and Mihalcea, 2005" startWordPosition="2954" endWordPosition="2957">ic syntactic kernel KT in SVM-light (Joachims, 1999). To assess the validity of the typed anchor model (tap), we evaluated two sets of systems: the plain and lexical-boosted systems. The plain systems are: -tap: our tree-kernel approach using typed placeholders with climbing in the syntactic tree; -tree: the cross-similarity model described in Sec.2. Its comparison with tap indicates the effectiveness of our approaches; The lexical-boosted systems are: -lex: a standard approach based on lexical overlap. The classifier uses as the only feature the lexical overlap similarity score described in (Corley and Mihalcea, 2005); -lex+tap: these configurations mix lexical overlap and our typed anchor approaches; -lex+tree: the comparison of this configuration with lex+tap should further support the validity of our intuition on typed anchors; Preliminary experiments have been performed using two datasets: RTE2 (the 1600 entailment pairs from the RTE-2 challenge) and RTE3d (the development dataset of this challenge). We randomly divided this latter in two halves: RTE3d0 and RTE3d1. 5.2 Investigatory Results Analysis and Submission Results Table 2 reports the results of the experiments. The first column indicates the tr</context>
</contexts>
<marker>Corley, Mihalcea, 2005</marker>
<rawString>Courtney Corley and Rada Mihalcea. 2005. Measuring the semantic similarity of texts. In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1318, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing textual entailment with LCCs GROUNDHOG system.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="1138" citStr="Hickl et al., 2005" startWordPosition="159" endWordPosition="162">act In this paper, we briefly describe two enhancements of the cross-pair similarity model for learning textual entailment rules: 1) the typed anchors and 2) a faster computation of the similarity. We will report and comment on the preliminary experiments and on the submission results. 1 Introduction Results of the second RTE challenge (Bar Haim et al., 2006) have suggested that both deep semantic models and machine learning approaches can successfully be applied to solve textual entailment. The only problem seems to be the size of the knowledge bases. The two best systems (Tatu et al., 2005; Hickl et al., 2005), which are significantly above all the others (more than +10% accuracy), use implicit or explicit knowledge bases larger than all the other systems. In (Tatu et al., 2005), a deep semantic representation is paired with a large amount of general and task specific semantic rules (explicit knowledge). In (Hickl et al., 2005), the machine learning model is trained over a large amounts of examples (implicit knowledge). In contrast, Zanzotto&amp;Moschitti (2006) proposed a machine-learning based approach which reaches a high accuracy by only using the available RTE data. The key idea is the cross-pair </context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2005</marker>
<rawString>Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2005. Recognizing textual entailment with LCCs GROUNDHOG system. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale svm learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods-Support Vector Learning.</booktitle>
<editor>In B. Schlkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="15878" citStr="Joachims, 1999" startWordPosition="2868" endWordPosition="2869">ooted in A2 a:1 are identical. The computation of KT applied to the above subtrees gives an identical result. Then, this computation can be avoided. If correctly used in a dynamic programming algorithm, the above observation can produce an interesting decrease in the time computational cost. More details on the algorithm and the decrease in computational cost may be found in (Moschitti and Zanzotto, 2007). 5 Experimental Results 5.1 Experimental Setup We implemented the novel cross-similarity kernel in the SVM-light-TK (Moschitti, 2006) that encodes the basic syntactic kernel KT in SVM-light (Joachims, 1999). To assess the validity of the typed anchor model (tap), we evaluated two sets of systems: the plain and lexical-boosted systems. The plain systems are: -tap: our tree-kernel approach using typed placeholders with climbing in the syntactic tree; -tree: the cross-similarity model described in Sec.2. Its comparison with tap indicates the effectiveness of our approaches; The lexical-boosted systems are: -lex: a standard approach based on lexical overlap. The classifier uses as the only feature the lexical overlap similarity score described in (Corley and Mihalcea, 2005); -lex+tap: these configur</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale svm learning practical. In B. Schlkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods-Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Fast and effective kernels for relational learning from texts.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference of Machine Learning (ICML),</booktitle>
<location>Corvallis, Oregon.</location>
<contexts>
<context position="15671" citStr="Moschitti and Zanzotto, 2007" startWordPosition="2837" endWordPosition="2840">t are in c, in the transformed trees, t(1, c1) and t(1, c2), the subtrees rooted in A2 a:1 are identical. The same happens for 2 with the subtree rooted in A2 1. In the transformed trees, t(2, c1) and t(2, c2), subtrees rooted in A2 a:1 are identical. The computation of KT applied to the above subtrees gives an identical result. Then, this computation can be avoided. If correctly used in a dynamic programming algorithm, the above observation can produce an interesting decrease in the time computational cost. More details on the algorithm and the decrease in computational cost may be found in (Moschitti and Zanzotto, 2007). 5 Experimental Results 5.1 Experimental Setup We implemented the novel cross-similarity kernel in the SVM-light-TK (Moschitti, 2006) that encodes the basic syntactic kernel KT in SVM-light (Joachims, 1999). To assess the validity of the typed anchor model (tap), we evaluated two sets of systems: the plain and lexical-boosted systems. The plain systems are: -tap: our tree-kernel approach using typed placeholders with climbing in the syntactic tree; -tree: the cross-similarity model described in Sec.2. Its comparison with tap indicates the effectiveness of our approaches; The lexical-boosted s</context>
</contexts>
<marker>Moschitti, Zanzotto, 2007</marker>
<rawString>Alessandro Moschitti and Fabio Massimo Zanzotto. 2007. Fast and effective kernels for relational learning from texts. In Proceedings of the International Conference of Machine Learning (ICML), Corvallis, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making tree kernels practical for natural language learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL06,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="1595" citStr="Moschitti (2006)" startWordPosition="233" endWordPosition="234">applied to solve textual entailment. The only problem seems to be the size of the knowledge bases. The two best systems (Tatu et al., 2005; Hickl et al., 2005), which are significantly above all the others (more than +10% accuracy), use implicit or explicit knowledge bases larger than all the other systems. In (Tatu et al., 2005), a deep semantic representation is paired with a large amount of general and task specific semantic rules (explicit knowledge). In (Hickl et al., 2005), the machine learning model is trained over a large amounts of examples (implicit knowledge). In contrast, Zanzotto&amp;Moschitti (2006) proposed a machine-learning based approach which reaches a high accuracy by only using the available RTE data. The key idea is the cross-pair similarity, i.e. a similarity applied to two text and hypothesis pairs which considers the relations between the words in the two texts and between the words in the two hypotheses. This is obtained by using placeholders to link the related words. Results in (Bar Haim et al., 2006) are comparable with the best machine learning system when this latter is trained only on the RTE examples. Given the high potential of the cross-pair similarity model, for the</context>
<context position="3065" citStr="Moschitti, 2006" startWordPosition="478" endWordPosition="479">he computational cost of the cross-pair similarity computation algorithm to allow the learning over larger training sets. The paper is organized as follows: in Sec. 2 we review the cross-pair similarity model and its limits; in Sec. 3, we introduce our model for typed anchors; in Sec. 4 we describe how we limit the computational cost of the similarity; in Sec. 5 we present the two submission experiments, and in Sec. 6 we draw some conclusions. 2 Cross-pair similarity and its limits 2.1 Learning entailment rules with syntactic cross-pair similarity The cross-pair similarity model (Zanzotto and Moschitti, 2006) proposes a similarity measure aiming at capturing rewrite rules from training examples, computing a cross-pair similarity KS((T0, H0), (T00, H00)). The rationale is that if two pairs are similar, it is extremely likely that they have the same entailment value. The key point is the use of placeholders to mark the relations between the sentence words. A placeholder co-indexes two substructures in the parse trees of text and hypothesis, 72 \x0cindicating that such substructures are related. For example, the sentence pair, All companies file annual reports implies All insurance companies file ann</context>
<context position="15805" citStr="Moschitti, 2006" startWordPosition="2856" endWordPosition="2857">ooted in A2 1. In the transformed trees, t(2, c1) and t(2, c2), subtrees rooted in A2 a:1 are identical. The computation of KT applied to the above subtrees gives an identical result. Then, this computation can be avoided. If correctly used in a dynamic programming algorithm, the above observation can produce an interesting decrease in the time computational cost. More details on the algorithm and the decrease in computational cost may be found in (Moschitti and Zanzotto, 2007). 5 Experimental Results 5.1 Experimental Setup We implemented the novel cross-similarity kernel in the SVM-light-TK (Moschitti, 2006) that encodes the basic syntactic kernel KT in SVM-light (Joachims, 1999). To assess the validity of the typed anchor model (tap), we evaluated two sets of systems: the plain and lexical-boosted systems. The plain systems are: -tap: our tree-kernel approach using typed placeholders with climbing in the syntactic tree; -tree: the cross-similarity model described in Sec.2. Its comparison with tap indicates the effectiveness of our approaches; The lexical-boosted systems are: -lex: a standard approach based on lexical overlap. The classifier uses as the only feature the lexical overlap similarity</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Making tree kernels practical for natural language learning. In Proceedings of EACL06, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Tatu</author>
<author>Brandon Iles</author>
<author>John Slavick</author>
<author>Adrian Novischi</author>
<author>Dan Moldovan</author>
</authors>
<title>COGEX at the second recognizing textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="1117" citStr="Tatu et al., 2005" startWordPosition="155" endWordPosition="158">@dit.unitn.it Abstract In this paper, we briefly describe two enhancements of the cross-pair similarity model for learning textual entailment rules: 1) the typed anchors and 2) a faster computation of the similarity. We will report and comment on the preliminary experiments and on the submission results. 1 Introduction Results of the second RTE challenge (Bar Haim et al., 2006) have suggested that both deep semantic models and machine learning approaches can successfully be applied to solve textual entailment. The only problem seems to be the size of the knowledge bases. The two best systems (Tatu et al., 2005; Hickl et al., 2005), which are significantly above all the others (more than +10% accuracy), use implicit or explicit knowledge bases larger than all the other systems. In (Tatu et al., 2005), a deep semantic representation is paired with a large amount of general and task specific semantic rules (explicit knowledge). In (Hickl et al., 2005), the machine learning model is trained over a large amounts of examples (implicit knowledge). In contrast, Zanzotto&amp;Moschitti (2006) proposed a machine-learning based approach which reaches a high accuracy by only using the available RTE data. The key id</context>
</contexts>
<marker>Tatu, Iles, Slavick, Novischi, Moldovan, 2005</marker>
<rawString>Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi, and Dan Moldovan. 2005. COGEX at the second recognizing textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Automatic learning of textual entailments with cross-pair similarities.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st Coling and 44th ACL,</booktitle>
<pages>401408</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="3065" citStr="Zanzotto and Moschitti, 2006" startWordPosition="476" endWordPosition="479">) we reduce the computational cost of the cross-pair similarity computation algorithm to allow the learning over larger training sets. The paper is organized as follows: in Sec. 2 we review the cross-pair similarity model and its limits; in Sec. 3, we introduce our model for typed anchors; in Sec. 4 we describe how we limit the computational cost of the similarity; in Sec. 5 we present the two submission experiments, and in Sec. 6 we draw some conclusions. 2 Cross-pair similarity and its limits 2.1 Learning entailment rules with syntactic cross-pair similarity The cross-pair similarity model (Zanzotto and Moschitti, 2006) proposes a similarity measure aiming at capturing rewrite rules from training examples, computing a cross-pair similarity KS((T0, H0), (T00, H00)). The rationale is that if two pairs are similar, it is extremely likely that they have the same entailment value. The key point is the use of placeholders to mark the relations between the sentence words. A placeholder co-indexes two substructures in the parse trees of text and hypothesis, 72 \x0cindicating that such substructures are related. For example, the sentence pair, All companies file annual reports implies All insurance companies file ann</context>
</contexts>
<marker>Zanzotto, Moschitti, 2006</marker>
<rawString>Fabio Massimo Zanzotto and Alessandro Moschitti. 2006. Automatic learning of textual entailments with cross-pair similarities. In Proceedings of the 21st Coling and 44th ACL, pages 401408, Sydney, Australia, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>