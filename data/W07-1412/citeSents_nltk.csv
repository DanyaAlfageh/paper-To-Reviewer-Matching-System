1 Introduction Results of the second RTE challenge (Bar CITATION) have suggested that both deep semantic models and machine learning approaches can successfully be applied to solve textual entailment.,,
The two best systems (CITATION; CITATION), which are significantly above all the others (more than +10% accuracy), use implicit or explicit knowledge bases larger than all the other systems.,,
In CITATION, a deep semantic representation is paired with a large amount of general and task specific semantic rules (explicit knowledge).,,
In CITATION, the machine lear,,
The cross-pair similarity model uses: (1) a tree similarity measure KT (1, 2) CITATION that counts the subtrees that 1 and 2 have in common; (2) a substitution function t(, c) that changes names of the placeholders in a tree according to a set of correspondences between placeholders c. Given C as the collection of all correspondences between the placeholders of (T0, H0) and (T00, H00), the cross-pair similarity is computed as: KS((T0 , H0 ), (T00 , H00 )) = maxcC(KT (t(T0 , c), t(T00 , c)) + KT (t(H0 , c), t(H00 , c))) (1) The cross-pair similarity KS, used in a kernel-based learning model as the support vector machines, allows the exploitation of implicit true and false entail,,
KT (0, 00), as defined in CITATION, computes the number of common subtrees between 0 and 00.,,
ic syntactic kernel KT in SVM-light CITATION.,,
The classifier uses as the only feature the lexical overlap similarity score described in CITATION; -lex+tap: these configurations mix lexical overlap and our typed anchor approaches; -lex+tree: the comparison of this configuration with lex+tap should further support the validity of our intuition on typed anchors; Preliminary experiments have been performed using two datasets: RTE2 (the 1600 entailment pairs from the RTE-2 challenge) and RTE3d (the development dataset of this challenge).,,
1 Introduction Results of the second RTE challenge (Bar CITATION) have suggested that both deep semantic models and machine learning approaches can successfully be applied to solve textual entailment.,,
The two best systems (CITATION; CITATION), which are significantly above all the others (more than +10% accuracy), use implicit or explicit knowledge bases larger than all the other systems.,,
In CITATION, a deep semantic representation is paired with a large amount of general and task specific semantic rules (explicit knowledge).,,
In CITATION, the machine learning model is trained over a large amounts of examples (implicit knowledge).,,
In contrast, Zanzotto&CITATION proposed a machine-learning based approach which reaches a high accuracy by only using the available RTE data.,,
More details on the algorithm and the decrease in computational cost may be found in CITATION.,,
5 Experimental Results 5.1 Experimental Setup We implemented the novel cross-similarity kernel in the SVM-light-TK CITATION that encodes the basic syntactic kernel KT in SVM-light CITATION.,,
The classifier uses as the only feature the lexical overlap similarity score described in CITATION; -lex+tap: these configur,,
More details on the algorithm and the decrease in computational cost may be found in CITATION.,,
5 Experimental Results 5.1 Experimental Setup We implemented the novel cross-similarity kernel in the SVM-light-TK CITATION that encodes the basic syntactic kernel KT in SVM-light CITATION.,,
The two best systems (CITATION; CITATION), which are significantly above all the others (more than +10% accuracy), use implicit or explicit knowledge bases larger than all the other systems.,,
In CITATION, a deep semantic representation is paired with a large amount of general and task specific semantic rules (explicit knowledge).,,
In CITATION, the machine learning model is trained over a large amounts of examples (implicit knowledge).,,
In contrast, Zanzotto&CITATION proposed a machine-learning based approach which reaches a high accuracy by only using the available RTE data.,,
Results in (Bar CITATION) are comparable with the best machine learning system when this latter is trained only on the RTE examples.,,
2 Cross-pair similarity and its limits 2.1 Learning entailment rules with syntactic cross-pair similarity The cross-pair similarity model (Zanzotto and CITATION) proposes a similarity measure aiming at capturing rewrite rules from training examples, computing a cross-pair similarity KS((T0, H0), (T00, H00)).,,
More details on the algorithm and the decrease in computational cost may be found in CITATION.,,
5 Experimental Results 5.1 Experimental Setup We implemented the novel cross-similarity kernel in the SVM-light-TK CITATION that encodes the basic syntactic kernel KT in SVM-light CITATION.,,
1 Introduction Results of the second RTE challenge (Bar CITATION) have suggested that both deep semantic models and machine learning approaches can successfully be applied to solve textual entailment.,,
The two best systems (CITATION; CITATION), which are significantly above all the others (more than +10% accuracy), use implicit or explicit knowledge bases larger than all the other systems.,,
In CITATION, a deep semantic representation is paired with a large amount of general and task specific semantic rules (explicit knowledge).,,
In CITATION, the machine learning model is trained over a large amounts of examples (implicit knowledge).,,
In contrast, Zanzotto&CITATION proposed a machine-learning based approach which reaches a high accuracy by only using the available RTE data.,,
2 Cross-pair similarity and its limits 2.1 Learning entailment rules with syntactic cross-pair similarity The cross-pair similarity model (Zanzotto and CITATION) proposes a similarity measure aiming at capturing rewrite rules from training examples, computing a cross-pair similarity KS((T0, H0), (T00, H00)).,,
