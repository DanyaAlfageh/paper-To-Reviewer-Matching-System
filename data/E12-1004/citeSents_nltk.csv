rds tend to share similar contexts, DS has been very successful in tasks that require quantifying semantic similarity among words, such as synonym detection and concept clustering CITATION.,,
Although the general focus in the area is to perform algebraic operations on word semantic vectors CITATION, some researchers have also directly examined the corpus contexts of phrases.,,
For example, CITATION studied vector extraction for phrases because they were interested in the decomposability of multiword expressions.,,
CITATION and CITATION look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically.,,
We then took the Cartesian product of these 256 adjectives with the 200 concrete nouns in the BLESS data set CITATION.,,
One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (CITATION; CITATION; CITATION; CITATION).,,
The other strand attempts to reformulate FSs notion of logical inference in terms that DS can capture (CITATION; CITATION; CITATION; CITATION).,,
 synonym detection and concept clustering CITATION.,,
Although the general focus in the area is to perform algebraic operations on word semantic vectors CITATION, some researchers have also directly examined the corpus contexts of phrases.,,
For example, CITATION studied vector extraction for phrases because they were interested in the decomposability of multiword expressions.,,
CITATION and CITATION look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically.,,
Entailment systems have been compared under this new perspective in various evaluation campaigns, the best known being the Recognizing Textual Entailment (RTE) initiative CITATION.,,
Most RTE systems are based on advanced NLP components, machine learning techniques, and/or syntactic transformations (CITATION; CITATION).,,
A few systems exploit deep FS analysis (CITATION; CITATION).,,
Lexical entailment is in turn fundamental for constructing ontologies and other lexical resources CITATION.,,
Second, our QN study demonstrates that phrasal entailment can be automatically detected and thus paves the way to apply DS to advanced NLP tasks such as recognizing textual entailment CITATION.,,
2 Background 2.1 Distributional semantics above the word level DS models such as LSA CITATION and HAL CITATION approximate the meaning of a word by a vector that summarizes its distribution in a corpus, for example by counting co-occurrences of the word with other words.,,
Entailment systems have been compared under this new perspective in various evaluation campaigns, the best known being the Recognizing Textual Entailment (RTE) initiative CITATION.,,
Most RTE systems are based on advanced NLP components, machine learning techniques, and/or syntactic transformations (CITATION; CITATION).,,
A few systems exploit deep FS analysis (CITATION; CITATION).,,
SVM Support vector machines are widely used high-performance discriminative classifiers that find the hyperplane providing the best separation between negative and positive instances CITATION.,,
Our SVM classifiers are trained and tested using Weka 3 and LIBSVM 2.8 CITATION.,,
In the second step, following standard practice, the co-occurrence counts are converted into pointwise mutual information (PMI) scores CITATION.,,
SVM Support vector machines are widely used high-performance discriminative classifiers that find the hyperplane providing the best separation between negative and positive instances CITATION.,,
Our SVM classifiers are trained and tested using Weka 3 and LIBSVM 2.8 CITATION.,,
Lexical entailment is in turn fundamental for constructing ontologies and other lexical resources CITATION.,,
Second, our QN study demonstrates that phrasal entailment can be automatically detected and thus paves the way to apply DS to advanced NLP tasks such as recognizing textual entailment CITATION.,,
2 Background 2.1 Distributional semantics above the word level DS models such as LSA CITATION and HAL CITATION approximate the meaning of a word by a vector that summarizes its distribution in a corpus, for example by counting co-occurrences of the word with other words.,,
Entailment systems have been compared under this new perspective in various evaluation campaigns, the best known being the Recognizing Textual Entailment (RTE) initiative CITATION.,,
Most RTE systems are based on advanced NLP components, machine learning techniques, and/or syntactic transformations (CITATION; CITATION).,,
A few systems exploit deep FS analysis (CITATION; CITATION).,,
One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (CITATION; CITATION; CITATION; CITATION).,,
The other strand attempts to reformulate FSs notion of logical inference in terms that DS can capture (CITATION; CITATION; CITATION; CITATION).,,
Entailment in DS CITATION suggests that it may not be possible to induce lexical entailment directly from a vector space representation, but it is possible to encode the relation in this space after it has been derived through other means.,,
On the other hand, recent studies (Geffet and Dagan, 25 \x0c2005; CITATION; CITATION) have pursued the intuition that entailment is the asymmetric ability of one term to substitute for another.,,
One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (CITATION; CITATION; CITATION; CITATION).,,
The other strand attempts to reformulate FSs notion of logical inference in terms that DS can capture (CITATION; CITATION; CITATION; CITATION).,,
One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (CITATION; CITATION; CITATION; CITATION).,,
The other strand attempts to reformulate FSs notion of logical inference in terms that DS can capture (CITATION; CITATION; CITATION; CITATION).,,
One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (CITATION; CITATION; CITATION; CITATION).,,
The other strand attempts to reformulate FSs notion of logical inference in terms that DS can capture (CITATION; CITATION; CITATION; CITATION).,,
ustering CITATION.,,
Although the general focus in the area is to perform algebraic operations on word semantic vectors CITATION, some researchers have also directly examined the corpus contexts of phrases.,,
For example, CITATION studied vector extraction for phrases because they were interested in the decomposability of multiword expressions.,,
CITATION and CITATION look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically.,,
The most popular approach, first adopted by CITATION, extracts lexical relations from patterns in large corpora.,,
Several studies have refined and extended this approach (CITATION; CITATION; CITATION; CITATION).,,
The success of DS in lexical semantics has validated the hypothesis that semantically similar expressions occur in similar contexts (CITATION; CITATION; CITATION; CITATION; CITATION).,,
FS has successfully modeled quantification and captured inferential relations between phrases and between sentences (CITATION; Thomason, 1974; CITATION).,,
One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (CITATION; CITATION; CITATION; CITATION).,,
The other strand attempts to reformulate FSs notion of logical inference in terms that DS can capture (CITATION; CITATION; CITATION; CITATION).,,
Entailment in DS CITATION suggests that it may not be possible to induce lexical entailment directly from a vector space representation, but it is possible to encode the relation in this space after it has been derived through other means.,,
On the other hand, recent studies (Geffet and Dagan, 25 \x0c2005; CITATION; CITATION) have pursued the intuition that entailment is the asymmetric ability of one term to substitute for another.,,
In particular, CITATION carefully crafted the balAPinc measure (see Section 3.5 below).,,
3.5 Classification methods We consider two methods to classify candidate pairs as entailing or non-entailing, the balAPinc measure of CITATION and a standard Support Vector Machine (SVM) classifier.,,
This suggests that the vector pairs representing entailment between nouns are also in an inclusion relation, supporting the conjectures of CITATION and others.,,
Finally, the hypothesis that entailment among nouns is reflected by distributional inclusion among their semantic vectors CITATION is supported both by the successful generalization of the SVM classifier trained on AN |= N pairs and by the good performance of the balAPinc measure.,,
Entailment systems have been compared under this new perspective in various evaluation campaigns, the best known being the Recognizing Textual Entailment (RTE) initiative CITATION.,,
Most RTE systems are based on advanced NLP components, machine learning techniques, and/or syntactic transformations (CITATION; CITATION).,,
A few systems exploit deep FS analysis (CITATION; CITATION).,,
The success of DS in lexical semantics has validated the hypothesis that semantically similar expressions occur in similar contexts (CITATION; CITATION; CITATION; CITATION; CITATION).,,
FS has successfully modeled quantification and captured inferential relations between phrases and between sentences (CITATION; Thomason, 1974; CITATION).,,
Lexical entailment is in turn fundamental for constructing ontologies and other lexical resources CITATION.,,
Second, our QN study demonstrates that phrasal entailment can be automatically detected and thus paves the way to apply DS to advanced NLP tasks such as recognizing textual entailment CITATION.,,
2 Background 2.1 Distributional semantics above the word level DS models such as LSA CITATION and HAL CITATION approximate the meaning of a word by a vector that summarizes its distribution in a corpus, for example by counting co-occurrences of the word with other words.,,
Since semantically similar words tend to share similar contexts, DS has been very successful in tasks that require quantifying semantic similarity among words, such as synonym detection and concept clustering CITATION.,,
average it with LIN, the widely used symmetric measure of distributional similarity proposed by CITATION: LIN(u, v) = P fFuFv [wu(f) + wv(f)] P fFu wu(f) + P fFv wv(f) LIN essentially measures feature vector overlap.,,
The success of DS in lexical semantics has validated the hypothesis that semantically similar expressions occur in similar contexts (CITATION; CITATION; CITATION; CITATION; CITATION).,,
FS has successfully modeled quantification and captured inferential relations between phrases and between sentences (CITATION; Thomason, 1974; CITATION).,,
Lexical entailment is in turn fundamental for constructing ontologies and other lexical resources CITATION.,,
Second, our QN study demonstrates that phrasal entailment can be automatically detected and thus paves the way to apply DS to advanced NLP tasks such as recognizing textual entailment CITATION.,,
2 Background 2.1 Distributional semantics above the word level DS models such as LSA CITATION and HAL CITATION approximate the meaning of a word by a vector that summarizes its distribution in a corpus, for example by counting co-occurrences of the word with other words.,,
Since semantically similar words tend to share similar contexts, DS has been very successful in tasks that require quantifying semantic similarity among words, such as synonym detection and concept clustering CITATION.,,
An SVM with a polynomial kernel takes into account not only individual input features but also their interactions (CITATION, chapter 15).,,
One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (CITATION; CITATION; CITATION; CITATION).,,
The other strand attempts to reformulate FSs notion of logical inference in terms that DS can capture (CITATION; CITATION; CITATION; CITATION).,,
Since semantically similar words tend to share similar contexts, DS has been very successful in tasks that require quantifying semantic similarity among words, such as synonym detection and concept clustering CITATION.,,
Although the general focus in the area is to perform algebraic operations on word semantic vectors CITATION, some researchers have also directly examined the corpus contexts of phrases.,,
For example, CITATION studied vector extraction for phrases because they were interested in the decomposability of multiword expressions.,,
CITATION and CITATION look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically.,,
The success of DS in lexical semantics has validated the hypothesis that semantically similar expressions occur in similar contexts (CITATION; CITATION; CITATION; CITATION; CITATION).,,
FS has successfully modeled quantification and captured inferential relations between phrases and between sentences (CITATION; Thomason, 1974; CITATION).,,
The most popular approach, first adopted by CITATION, extracts lexical relations from patterns in large corpora.,,
Several studies have refined and extended this approach (CITATION; CITATION; CITATION; CITATION).,,
Both methods that generalize entailment from AN |= N to N1 |= N2 perform well, with 70% mais, 1997; CITATION; CITATION), we tried balAPinc on the SVD-reduced vectors as well, but results were consistently worse than with PMI vectors.,,
The success of DS in lexical semantics has validated the hypothesis that semantically similar expressions occur in similar contexts (CITATION; CITATION; CITATION; CITATION; CITATION).,,
FS has successfully modeled quantification and captured inferential relations between phrases and between sentences (CITATION; Thomason, 1974; CITATION).,,
We tokenize and POS-tag this corpus, then lemmatize it with TreeTagger CITATION to merge singular and plural instances of words and phrases (some dogs is mapped to some dog).,,
The success of DS in lexical semantics has validated the hypothesis that semantically similar expressions occur in similar contexts (CITATION; CITATION; CITATION; CITATION; CITATION).,,
FS has successfully modeled quantification and captured inferential relations between phrases and between sentences (CITATION; Thomason, 1974; CITATION).,,
Both methods that generalize entailment from AN |= N to N1 |= N2 perform well, with 70% mais, 1997; CITATION; CITATION), we tried balAPinc on the SVD-reduced vectors as well, but results were consistently worse than with PMI vectors.,,
The most popular approach, first adopted by CITATION, extracts lexical relations from patterns in large corpora.,,
Several studies have refined and extended this approach (CITATION; CITATION; CITATION; CITATION).,,
The most popular approach, first adopted by CITATION, extracts lexical relations from patterns in large corpora.,,
Several studies have refined and extended this approach (CITATION; CITATION; CITATION; CITATION).,,
The success of DS in lexical semantics has validated the hypothesis that semantically similar expressions occur in similar contexts (CITATION; CITATION; CITATION; CITATION; CITATION).,,
FS has successfully modeled quantification and captured inferential relations between phrases and between sentences (CITATION; Thomason, 1974; CITATION).,,
2 Background 2.1 Distributional semantics above the word level DS models such as LSA CITATION and HAL CITATION approximate the meaning of a word by a vector that summarizes its distribution in a corpus, for example by counting co-occurrences of the word with other words.,,
Since semantically similar words tend to share similar contexts, DS has been very successful in tasks that require quantifying semantic similarity among words, such as synonym detection and concept clustering CITATION.,,
Although the general focus in the area is to perform algebraic operations on word semantic vectors CITATION, some researchers have also directly examined the corpus contexts of phrases.,,
For example, CITATION studied vector extraction for phrases because they were interested in the decomposability of multiword expressions.,,
CITATION and CITATION look at corpus-harv,,
The most popular approach, first adopted by CITATION, extracts lexical relations from patterns in large corpora.,,
Several studies have refined and extended this approach (CITATION; CITATION; CITATION; CITATION).,,
Entailment in DS CITATION suggests that it may not be possible to induce lexical entailment directly from a vector space representation, but it is possible to encode the relation in this space after it has been derived through other means.,,
On the other hand, recent studies (Geffet and Dagan, 25 \x0c2005; CITATION; CITATION) have pursued the intuition that entailment is the asymmetric ability of one term to substitute for another.,,
In particular, CITATION carefully crafted the balAPinc measure (see Section 3.5 below).,,
Entailment systems have been compared under this new perspective in various evaluation campaigns, the best known being the Recognizing Textual Entailment (RTE) initiative CITATION.,,
Most RTE systems are based on advanced NLP components, machine learning techniques, and/or syntactic transformations (CITATION; CITATION).,,
A few systems exploit deep FS analysis (CITATION; CITATION).,,
One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (CITATION; CITATION; CITATION; CITATION).,,
The other strand attempts to reformulate FSs notion of logical inference in terms that DS can capture (CITATION; CITATION; CITATION; CITATION).,,
