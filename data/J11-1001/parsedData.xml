<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<figure confidence="0.515163285714286">
b&amp;apos;Squibs
Nouveau-ROUGE: A Novelty Metric for
Update Summarization
John M. Conroy
IDA/Center for Computing Sciences
Judith D. Schlesinger
IDA/Center for Computing Sciences
</figure>
<author confidence="0.88638">
Dianne P. OLeary
</author>
<affiliation confidence="0.982794">
University of Maryland
</affiliation>
<bodyText confidence="0.999601636363636">
An update summary should provide a fluent summarization of new information on a time-
evolving topic, assuming that the reader has already reviewed older documents or summaries.
In 2007 and 2008, an annual summarization evaluation included an update summarization
task. Several participating systems produced update summaries indistinguishable from human-
generated summaries when measured using ROUGE. However, no machine system performed
near human-level performance in manual evaluations such as pyramid and overall responsive-
ness scoring.
We present a metric called Nouveau-ROUGE that improves correlation with manual
evaluation metrics and can be used to predict both the pyramid score and overall responsiveness
for update summaries. Nouveau-ROUGE can serve as a less expensive surrogate for manual
evaluations when comparing existing systems and when developing new ones.
</bodyText>
<sectionHeader confidence="0.993979" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.933942666666667">
Update summaries focus on what is new relative to a previous body of information.
They pose new challenges both to algorithm developers and to evaluation of sum-
maries. In 2007, DUC (Document Understanding Conference) introduced an update
summarization task, repeated in 2008 for TAC (Text Analysis Conference).1
This task
consisted of producing a multi-document summary for a set of articles on a single topic,
followed by one (2008) or two (2007) multi-document summaries for sets of articles on
Institute for Defense Analyses, Center for Computing Sciences, 17100 Science Drive, Bowie, MD 20715
USA. E-mail: {judith,conroy}@super.org.
</bodyText>
<affiliation confidence="0.787297">
Computer Science Department, Institute for Advanced Computer Studies, University of Maryland,
</affiliation>
<address confidence="0.443705">
College Park, MD 20742 USA. E-mail: oleary@cs.umd.edu.
</address>
<sectionHeader confidence="0.368941" genericHeader="method">
1 DUC (http:/
</sectionHeader>
<bodyText confidence="0.4841458">
/duc.nist.gov), the summarization evaluation event, was replaced in 2008 by TAC
(http:/
/www.nist.gov/tac). Both were sponsored by NIST, the U.S. National Institute of Standards
and Technology.
Submission received: 14 January 2010; revised submission received: 15 May 2010; accepted for publication:
</bodyText>
<sectionHeader confidence="0.788296" genericHeader="method">
27 September 2010.
2011 Association for Computational Linguistics
</sectionHeader>
<bodyText confidence="0.991118866666666">
\x0cComputational Linguistics Volume 37, Number 1
the same topic published at later dates. The goal was to generate a good first summary,
along with update(s) that contained new content and minimized redundancy.
The modifier manual is used to identify evaluations, and the corresponding scores,
produced by humans. The modifier automatic is used to identify evaluations, and
the corresponding scores, produced by machines. Similarly, human-generated and
machine-generated will be used to distinguish between summaries created by humans
and those generated by machine systems, respectively.2
Because we are working with update summarization, there is a minimum of two
summaries for a set of documents. The first summary for the document set is called the
original (Task A) summary and a later summary is called an update (Task B) summary.
Several machine summarizing systems produced update summaries that were
statistically indistinguishable from human-generated summaries, as measured by the
ROUGE metrics, the standard metrics for automatic evaluation of summaries. However,
none of these machine systems performed near human levels in overall responsiveness
or pyramid evaluation, the currently used manual evaluation metrics.
We define the metric gap (or gap) as the distance between a prediction of a manual
score, based on automatic scores, and the observed manual score.
The purpose of our work is to investigate and mitigate this metric gap by in-
troducing an automatic evaluation that is a better predictor of manual evaluation.
Reducing the metric gap is important for two reasons. First, the gap severely limits the
usefulness of automatic evaluation and forces the use of much more expensive manual
evaluation for comparing existing systems. More importantly, however, this gap is a
severe handicap to research on new update summarization methods because it makes
it difficult to evaluate new ideas and compare them with existing methods.
In Section 2, we analyze the results of the TAC 2008 summarization task, demon-
strating the large gap between ROUGE automatic metrics and manual evaluation of
update summaries. In Section 3, we modify ROUGE to produce scores that correlate
significantly better with manual evaluation. We evaluate our new metric on TAC 2008
data in Section 4, demonstrating its superiority as a predictor of manual evaluations.
</bodyText>
<sectionHeader confidence="0.79227" genericHeader="method">
2. State-of-the-Art Evaluation of Update Summaries
</sectionHeader>
<bodyText confidence="0.99455">
TAC 2008 presented 48 20-document sets, with 10 documents in each of two subsets, A
and B. Subset B documents were more recent. Original summaries were generated for
the A subsets and update summaries were then produced for the B subsets.
In TAC 2008, ROUGE was used for automatic evaluation. ROUGE (Lin and Hovy
2000) compares any summary to any other (typically human-generated) summary using
a recall-oriented approach. ROUGE-1 and -2 are based on unigrams and bigrams,
respectively; ROUGE-SU4 uses bigrams with a maximum skip distance of 4 between
bigrams; ROUGE-BE (Hovy, Lin, and Zhou 2005) is an n-gram approach based on basic
elements, computed via parsing or automatic entity recognition. ROUGE-2, ROUGE-
SU4, and ROUGE-BE were the official automatic metrics for TAC 2008, used to com-
pare machine-generated summaries to human-generated summaries, and to compare
human-generated summaries to each other using a jackknife approach. In addition to
the three official metrics, we include ROUGE-1 in our study as it is often competitive
with the official metrics.
</bodyText>
<sectionHeader confidence="0.697763" genericHeader="method">
2 NIST uses model for human-generated summaries and peer for machine-generated summaries.
</sectionHeader>
<page confidence="0.82498">
2
</page>
<bodyText confidence="0.96627324">
\x0cConroy, Schlesinger, and OLeary Nouveau-ROUGE
Three manual evaluation metrics were used in TAC 2008: pyramid, overall respon-
siveness, and linguistic quality (not considered in our work). The pyramid method
(Nenkova and Passonneau 2004) is a content-based metric for which human annotators
mark content units in the human-generated summaries. The content units are collected
across a set of human-generated summaries for a topic, and a weight is computed
based on how many human-generated summaries include this content unit. TAC 2008
also used a manual overall responsiveness score. After evaluating data from 2005
2007 (Dang 2007; Conroy and Dang 2008), NIST decided that this score, which eval-
uates summary usefulness including linguistic quality, is a reliable and stable manual
evaluation.
We analyzed the three official TAC 2008 automatic evaluation scores to see how
well they predict the manual evaluation metrics of overall responsiveness and pyramid
score. Figure 1 shows scatter plots of responsiveness and pyramid scores vs. the three
official ROUGE measures for the TAC 2008 update task. Each solid data point represents
the average score for a human summarizer over 24 document sets, and a dashed line
marks the minimum; each open data point represents the average score for a machine
system. We also show robust linear least squares fits to the data as well as the Pearson
correlation coefficients between ROUGE-BE, -2, and -SU4 and the manual-evaluation
scores. Surprisingly, these correlations are higher for the update task than for the
original summarization task (data not shown); nevertheless, the gap between the lines
predictions and the scores for the human-generated summaries is larger.
Figure 1
TAC 2008: The update task (Task B) responsiveness and pyramid scores vs. ROUGE scores;
human-generated summaries (solid points) and machine-generated summaries (open points).
</bodyText>
<page confidence="0.986794">
3
</page>
<bodyText confidence="0.990093636363636">
\x0cComputational Linguistics Volume 37, Number 1
We report correlation coefficients only for the machine-generated summaries. Scores
for the human-generated summaries are distributed differently, and correlation for the
set of human-generated summaries is often not significant due to the small number of
human summarizers.
The correlation coefficients in Figure 1 show that the automatic metrics do well
in predicting responsiveness and pyramid scoring for machine-generated summaries. In
contrast, the scores for human-generated summaries far exceed the predictions, with
a large gap between predicted and actual scores. As may be expected, ROUGE is more
highly correlated with the pyramid evaluation, which is a pure content evaluation score,
whereas the responsiveness score also reflects linguistic quality.
</bodyText>
<sectionHeader confidence="0.843896" genericHeader="method">
3. Improving Automatic EvaluationNouveau-ROUGE
</sectionHeader>
<bodyText confidence="0.992354708333333">
We more formally define the metric gap to be the absolute value of the difference between
a manual evaluation score and our prediction of it based only on automatic evaluation
scores. A number of TAC 2008 machine systems performed within statistical confidence
of human performance in the automatic evaluation metrics, but no system performed
near human performance in the manual evaluations. This has also been observed in
previous summarization evaluations (Conroy and Dang 2008). Progress has been made
in closing this metric gap but it persists, especially for update summaries.
A good update summary must contain essential information but focus on new
information. When a machine-generated update summary is good, it is similar to the
human-generated update summaries. This is assessed quite well by a ROUGE score. But the
machine-generated update summary should also be different from the human-generated
original summaries, and we need an automatic metric to assess this difference, or lack of
redundancy. We suggest using a ROUGE score to measure similarity, and thus redun-
dancy, between a given original summary and an update summary: A high ROUGE
score indicates high redundancy.
To illustrate this, we used the CLASSY algorithm (Conroy, Schlesinger, and OLeary
2006; Schlesinger, OLeary, and Conroy 2008) to produce original summaries and update
summaries for the TAC 2008 data. We also produced update summaries using a variant,
projected-CLASSY, that reduces overlap by using a linear algebra projection of the
term-sentence matrix (Conroy, Schlesinger, and OLeary 2007) of candidate sentences
against the matrix for the original (Task A) summary in order to favor new infor-
mation. Table 1 gives average ROUGE-2 scores and 95% confidence intervals, com-
puted via bootstrapping (Efron and Tibshirani 1993), over the 48 document sets.
Two scores are given: R
</bodyText>
<figure confidence="0.846673125">
(BB)
2 compares each CLASSY update (Task B) summary to the
human-generated summaries, and R
(AB)
2 compares each to the original (Task A) model
summaries. Whereas the two variants score comparably using R
(BB)
2 , there is a significant
difference in the R
(AB)
2 metric, as desired.
Table 1
TAC 2008: Average ROUGE-2 scores and 95% confidence intervals for update summaries
produced by two variants of CLASSY.
Variation R(BB)
2 R(AB)
</figure>
<page confidence="0.436151">
2
</page>
<table confidence="0.953130666666667">
projected-CLASSY 0.087 (0.080, 0.094) 0.075 (0.070, 0.079)
CLASSY 0.089 (0.082, 0.096) 0.083 (0.078, 0.088)
4
\x0cConroy, Schlesinger, and OLeary Nouveau-ROUGE
Table 2
TAC 2008: Nouveau-ROUGE -parameters.
Predicting Responsiveness Predicting Pyramid Scores
i,0 i,1 i,2 i,0 i,1 i,2
R1 0.0271 7.3550 13.4227 0.2143 1.9011 3.1118
R2 0.9126 5.4536 21.1556 0.0143 1.3499 4.3778
RSU4 1.1381 2.6931 35.8555 0.0346 1.1680 7.2589
RBE 1.0602 5.0811 24.8365 0.0145 1.3156 5.0446
</table>
<bodyText confidence="0.7636315">
Given this evidence, we propose predicting manual scores for update summaries by
using two ROUGE scores, R
</bodyText>
<equation confidence="0.9912848">
(AB)
i and R
(BB)
i (i = 1, 2, SU4, ...), in a three-parameter model
called Nouveau-ROUGE:
Ni = i,0 + i,1R
(AB)
i + i,2R
(BB)
i
</equation>
<bodyText confidence="0.999421833333333">
We determine the parameters (Table 2) using robust linear regression on the TAC 2008
evaluation data so that the Nouveau-ROUGE score Ni best predicts the manual scores
of responsiveness and pyramid performance.
Nouveau-ROUGE could be used by researchers to predict how a new system would
compare with the TAC 2008 systems in overall responsiveness and pyramid scoring, a
comparison that up to now has been impossible.
</bodyText>
<sectionHeader confidence="0.994081" genericHeader="method">
4. Evaluating Nouveau-ROUGE
</sectionHeader>
<bodyText confidence="0.9971965">
We evaluate Nouveau-ROUGE using cross validation studies to demonstrate that if
manual scores are available for a subset of summaries (in this case, those from 29
machine systems, half of those that participated in TAC 2008), then Nouveau-ROUGE
can predict manual scores for the remaining (held-back) summaries.
</bodyText>
<subsectionHeader confidence="0.976962">
4.1 Improved Correlation with Manual Evaluation
</subsectionHeader>
<bodyText confidence="0.824120428571429">
Correlation scores between automatic and manual scores have traditionally been used
as a measure of the effectiveness of automatic evaluation as a surrogate for manual
evaluation. Pearson correlation coefficients, shown in Table 3, were computed for the
scores for the held-back subset of summaries. Correlation is indeed higher for the
Nouveau-ROUGE scores than for any of the ROUGE scores.
Table 3
Correlation scores for TAC 2008 human evaluations.
</bodyText>
<table confidence="0.827643576923077">
Average Responsiveness Score Average Pyramid Score
Metric i = 1 i = 2 i = SU4 i = BE i = 1 i = 2 i = SU4 i = BE
R(AB)
i 0.676 0.576 0.619 0.490 0.698 0.592 0.634 0.483
R(BB)
i 0.870 0.921 0.902 0.933 0.910 0.952 0.933 0.964
Ni 0.888 0.929 0.912 0.935 0.946 0.961 0.951 0.969
5
\x0cComputational Linguistics Volume 37, Number 1
Figure 2
TAC 2008: ROUGE and Nouveau-ROUGE responsiveness and pyramid predictions for
subtask B.
6
\x0cConroy, Schlesinger, and OLeary Nouveau-ROUGE
Table 4
TAC 2008: Median Pearson correlation coefficients for automatic vs. manual evaluations.
Average Responsiveness Score Jackknife Pyramid Score
Metric R(AB)
i R(BB)
i Ni p-value R(AB)
i R(BB)
i Ni p-value
R1 0.378 0.804 0.920 5.4e-284 0.406 0.837 0.943 3.5e-307
R2 0.149 0.889 0.925 1.6e-104 0.177 0.909 0.941 1.8e-99
RSU4 0.267 0.846 0.913 1.3e-176 0.291 0.875 0.933 8.7e-214
RBE 0.222 0.913 0.919 6.2e-09 0.243 0.924 0.933 1.7e-17
</table>
<bodyText confidence="0.88716275">
Figure 2 shows that ROUGE-BE and ROUGE-1 predictions of both responsiveness
and pyramid scores are inferior to the Nouveau-ROUGE-BE predictions. Plots for N2
and NSU4 are omitted due to space restrictions, but performance improvement relative
to ROUGE is greater than that for NBE and less than that for N1.
</bodyText>
<subsectionHeader confidence="0.99986">
4.2 Validation Using Bootstrapping Experiments
</subsectionHeader>
<bodyText confidence="0.801503222222222">
To show that our results are not due to a lucky partitioning of the data, we used
bootstrapping (Efron and Tibshirani 1993), a resampling method, which allows us to
compute our statistical confidence in the results. This model assumes that observed data
(scores for the 58 systems) characterize all data. Given this model, the proper sampling
method is to choose subsets with replacement. We chose 58 systems (with replacement)
and used half to determine the Nouveau-ROUGE parameters and half to test the model.
We repeated this process 1,000 times. Table 4 gives the correlation coefficients (for the
tested-half of the data) for all four ROUGE metrics with each of the manual evalua-
tions when comparing the machine-generated summaries with the human-generated
summaries. Data in the columns labeled p-value result from a Mann-Whitney U-
test for equal medians of R
(BB)
i and Ni of the distributions of correlations returned by
the bootstrapping procedure. Because all p-values are small, we can conclude that the
differences between the R
(BB)
i and Ni correlation scores are statistically significant for all
variants of ROUGE.
</bodyText>
<subsectionHeader confidence="0.663471">
4.3 Narrowing the Gap for Update Summaries
</subsectionHeader>
<tableCaption confidence="0.861772">
Table 5 gives the median gap on the TAC 2008 data for predicting responsiveness and
pyramid scores. Recall that the gap is the absolute value of the difference between the
Table 5
</tableCaption>
<table confidence="0.996344">
Narrowing the TAC 2008 metric gap.
Responsiveness Metric Gap Pyramid Metric Gap
Metric R Gap N Gap p-value R Gap N Gap p-value
R1 2.025 1.277 7.8e-03 0.285 0.187 7.8e-03
R2 1.655 1.518 7.8e-03 0.241 0.197 7.8e-03
RSU4 1.887 1.344 7.8e-03 0.273 0.206 7.8e-03
RBE 1.591 1.547 7.8e-03 0.229 0.206 7.8e-03
</table>
<page confidence="0.906834">
7
</page>
<bodyText confidence="0.812899555555556">
\x0cComputational Linguistics Volume 37, Number 1
manual score and the prediction of it. The median gap is always smaller for Nouveau-
ROUGE than for ROUGE; in fact, the gap is smaller on every trial.
We used the Wilcox sign test to test the significance of this observation. The null
hypothesis is that the differences in the gaps between ROUGE and Nouveau-ROUGE
has median 0. The p-values from the Wilcox test indicate that the null hypothesis is true
with probability 1
128 7.812 103
, so it can be safely rejected.
</bodyText>
<sectionHeader confidence="0.966675" genericHeader="conclusions">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.999210076923077">
Our new metric, Nouveau-ROUGE, includes a measure of novelty for an update sum-
mary. We demonstrated that it has higher correlation to manual evaluation of overall
responsiveness and to pyramid scores than does ROUGE. The most obvious deficiency
in ROUGE is the lack of a linguistic quality measurement, which we take to encompass all
language-related issues: lexical, syntactic, and semantic. Therefore, we conjecture that
most remaining prediction error in Nouveau-ROUGE is a result of omitting linguistic
quality and caution that better prediction would be achieved only for systems of
comparable linguistic quality.
We believe that responsiveness is an imperfect surrogate for task-based summary
evaluation such as that done in SUMMAC (Mani et al. 1999). We would welcome
a return to task-based evaluation, as well as research increasing the reliability and
consistency of manual evaluation metrics. Investigation could also quantify the impact
of low responsiveness and pyramid scores on the ability to perform a specific task.
</bodyText>
<sectionHeader confidence="0.938727" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.964708081967213">
Conroy, John M. and Hoa Trang Dang. 2008.
Mind the gap: Dangers of divorcing
evaluations of summary content from
linguistic quality. In Proceedings of the 22nd
International Conference on Computational
Linguistics (Coling 2008), pages 145152,
Manchester.
Conroy, John M., Judith D. Schlesinger, and
Dianne P. OLeary. 2006. Topic-focused
multi-document summarization using an
approximate oracle score. In Proceedings of
the COLING/ACL 2006 Main Conference
Poster Sessions, pages 152159, Sydney.
Conroy, John M., Judith D. Schlesinger, and
Dianne P. OLeary. 2007. CLASSY 2007 at
DUC 2007. In Proceedings of the Seventh
Document Understanding Conference (DUC),
Rochester, NY. Available at http://duc.
nist.gov/pubs.html#2007.
Dang, Hoa Trang. 2007. Overview of DUC
2007. In Proceedings of the Seventh Document
Understanding Conference (DUC), Rochester,
NY. Available at http://duc.nist.gov/
pubs.html#2005.
Efron, B. and R. J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman
&amp; Hall, New York.
Hovy, Eduard, Chin-Yew Lin, and Liang
Zhou. 2005. Evaluating DUC 2005 using
basic elements. In Proceedings of the Fifth
Document Understanding Conference (DUC),
Vancouver. Available at www-nlpir/
nist.gov/projects/duc/pubs.html.
Lin, Chin-Yew and Eduard Hovy. 2000. The
automated acquisition of topic signatures
for text summarization. In Proceedings
of the 18th Conference on Computational
Linguistics, pages 495501, Morristown, NJ.
Mani, Inderjeet, Therese Firmin, David
House, Gary Klein, Beth Sundheim, and
Lynette Hirschman. 1999. The TIPSTER
SUMMAC text summarization evaluation.
In Proceedings of EACL99: Ninth Conference
of the European Chapter of the Association for
Computational Linguistics, pages 7785,
Bergen.
Nenkova, Ani and Rebecca Passonneau.
2004. Evaluating content selection in
summarization: The pyramid method.
In Proceedings of the Human Language
Technology Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 145152,
Boston, MA.
Schlesinger, Judith D., Dianne P. OLeary,
and John M. Conroy. 2008. Arabic/
English multi-document summarization
with CLASSYThe past and the future.
In Alexander F. Gelbukh, editor, CICLing,
volume 4919 of Lecture Notes in Computer
Science. Springer, Haifa, pages 568581.
</reference>
<page confidence="0.899543">
8
</page>
<figure confidence="0.305111">
\x0c\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.662329">
<title confidence="0.988135666666667">b&amp;apos;Squibs Nouveau-ROUGE: A Novelty Metric for Update Summarization</title>
<author confidence="0.999925">John M Conroy</author>
<affiliation confidence="0.843791">IDA/Center for Computing Sciences</affiliation>
<author confidence="0.994926">Judith D Schlesinger</author>
<affiliation confidence="0.843447">IDA/Center for Computing Sciences</affiliation>
<author confidence="0.987291">Dianne P OLeary</author>
<affiliation confidence="0.998361">University of Maryland</affiliation>
<abstract confidence="0.998566727272727">An update summary should provide a fluent summarization of new information on a timeevolving topic, assuming that the reader has already reviewed older documents or summaries. In 2007 and 2008, an annual summarization evaluation included an update summarization task. Several participating systems produced update summaries indistinguishable from humangenerated summaries when measured using ROUGE. However, no machine system performed near human-level performance in manual evaluations such as pyramid and overall responsiveness scoring. We present a metric called Nouveau-ROUGE that improves correlation with manual evaluation metrics and can be used to predict both the pyramid score and overall responsiveness for update summaries. Nouveau-ROUGE can serve as a less expensive surrogate for manual evaluations when comparing existing systems and when developing new ones.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Mind the gap: Dangers of divorcing evaluations of summary content from linguistic quality.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>145152</pages>
<location>Manchester.</location>
<contexts>
<context position="6494" citStr="Conroy and Dang 2008" startWordPosition="956" endWordPosition="959">Nouveau-ROUGE Three manual evaluation metrics were used in TAC 2008: pyramid, overall responsiveness, and linguistic quality (not considered in our work). The pyramid method (Nenkova and Passonneau 2004) is a content-based metric for which human annotators mark content units in the human-generated summaries. The content units are collected across a set of human-generated summaries for a topic, and a weight is computed based on how many human-generated summaries include this content unit. TAC 2008 also used a manual overall responsiveness score. After evaluating data from 2005 2007 (Dang 2007; Conroy and Dang 2008), NIST decided that this score, which evaluates summary usefulness including linguistic quality, is a reliable and stable manual evaluation. We analyzed the three official TAC 2008 automatic evaluation scores to see how well they predict the manual evaluation metrics of overall responsiveness and pyramid score. Figure 1 shows scatter plots of responsiveness and pyramid scores vs. the three official ROUGE measures for the TAC 2008 update task. Each solid data point represents the average score for a human summarizer over 24 document sets, and a dashed line marks the minimum; each open data poin</context>
<context position="9081" citStr="Conroy and Dang 2008" startWordPosition="1337" endWordPosition="1340">content evaluation score, whereas the responsiveness score also reflects linguistic quality. 3. Improving Automatic EvaluationNouveau-ROUGE We more formally define the metric gap to be the absolute value of the difference between a manual evaluation score and our prediction of it based only on automatic evaluation scores. A number of TAC 2008 machine systems performed within statistical confidence of human performance in the automatic evaluation metrics, but no system performed near human performance in the manual evaluations. This has also been observed in previous summarization evaluations (Conroy and Dang 2008). Progress has been made in closing this metric gap but it persists, especially for update summaries. A good update summary must contain essential information but focus on new information. When a machine-generated update summary is good, it is similar to the human-generated update summaries. This is assessed quite well by a ROUGE score. But the machine-generated update summary should also be different from the human-generated original summaries, and we need an automatic metric to assess this difference, or lack of redundancy. We suggest using a ROUGE score to measure similarity, and thus redun</context>
</contexts>
<marker>Conroy, Dang, 2008</marker>
<rawString>Conroy, John M. and Hoa Trang Dang. 2008. Mind the gap: Dangers of divorcing evaluations of summary content from linguistic quality. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 145152, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>Dianne P OLeary</author>
</authors>
<title>Topic-focused multi-document summarization using an approximate oracle score.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>152159</pages>
<location>Sydney.</location>
<marker>Conroy, Schlesinger, OLeary, 2006</marker>
<rawString>Conroy, John M., Judith D. Schlesinger, and Dianne P. OLeary. 2006. Topic-focused multi-document summarization using an approximate oracle score. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 152159, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>Dianne P OLeary</author>
</authors>
<title>at DUC</title>
<date>2007</date>
<journal>CLASSY</journal>
<booktitle>In Proceedings of the Seventh Document Understanding Conference (DUC),</booktitle>
<location>Rochester, NY.</location>
<note>Available at http://duc. nist.gov/pubs.html#2007.</note>
<marker>Conroy, Schlesinger, OLeary, 2007</marker>
<rawString>Conroy, John M., Judith D. Schlesinger, and Dianne P. OLeary. 2007. CLASSY 2007 at DUC 2007. In Proceedings of the Seventh Document Understanding Conference (DUC), Rochester, NY. Available at http://duc. nist.gov/pubs.html#2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2007</date>
<booktitle>In Proceedings of the Seventh Document Understanding Conference (DUC),</booktitle>
<location>Rochester, NY.</location>
<note>Available at http://duc.nist.gov/ pubs.html#2005.</note>
<contexts>
<context position="6471" citStr="Dang 2007" startWordPosition="954" endWordPosition="955">and OLeary Nouveau-ROUGE Three manual evaluation metrics were used in TAC 2008: pyramid, overall responsiveness, and linguistic quality (not considered in our work). The pyramid method (Nenkova and Passonneau 2004) is a content-based metric for which human annotators mark content units in the human-generated summaries. The content units are collected across a set of human-generated summaries for a topic, and a weight is computed based on how many human-generated summaries include this content unit. TAC 2008 also used a manual overall responsiveness score. After evaluating data from 2005 2007 (Dang 2007; Conroy and Dang 2008), NIST decided that this score, which evaluates summary usefulness including linguistic quality, is a reliable and stable manual evaluation. We analyzed the three official TAC 2008 automatic evaluation scores to see how well they predict the manual evaluation metrics of overall responsiveness and pyramid score. Figure 1 shows scatter plots of responsiveness and pyramid scores vs. the three official ROUGE measures for the TAC 2008 update task. Each solid data point represents the average score for a human summarizer over 24 document sets, and a dashed line marks the minim</context>
</contexts>
<marker>Dang, 2007</marker>
<rawString>Dang, Hoa Trang. 2007. Overview of DUC 2007. In Proceedings of the Seventh Document Understanding Conference (DUC), Rochester, NY. Available at http://duc.nist.gov/ pubs.html#2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R J Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap.</title>
<date>1993</date>
<publisher>Chapman &amp; Hall,</publisher>
<location>New York.</location>
<contexts>
<context position="10416" citStr="Efron and Tibshirani 1993" startWordPosition="1541" endWordPosition="1544">To illustrate this, we used the CLASSY algorithm (Conroy, Schlesinger, and OLeary 2006; Schlesinger, OLeary, and Conroy 2008) to produce original summaries and update summaries for the TAC 2008 data. We also produced update summaries using a variant, projected-CLASSY, that reduces overlap by using a linear algebra projection of the term-sentence matrix (Conroy, Schlesinger, and OLeary 2007) of candidate sentences against the matrix for the original (Task A) summary in order to favor new information. Table 1 gives average ROUGE-2 scores and 95% confidence intervals, computed via bootstrapping (Efron and Tibshirani 1993), over the 48 document sets. Two scores are given: R (BB) 2 compares each CLASSY update (Task B) summary to the human-generated summaries, and R (AB) 2 compares each to the original (Task A) model summaries. Whereas the two variants score comparably using R (BB) 2 , there is a significant difference in the R (AB) 2 metric, as desired. Table 1 TAC 2008: Average ROUGE-2 scores and 95% confidence intervals for update summaries produced by two variants of CLASSY. Variation R(BB) 2 R(AB) 2 projected-CLASSY 0.087 (0.080, 0.094) 0.075 (0.070, 0.079) CLASSY 0.089 (0.082, 0.096) 0.083 (0.078, 0.088) 4 </context>
<context position="14258" citStr="Efron and Tibshirani 1993" startWordPosition="2158" endWordPosition="2161">6e-104 0.177 0.909 0.941 1.8e-99 RSU4 0.267 0.846 0.913 1.3e-176 0.291 0.875 0.933 8.7e-214 RBE 0.222 0.913 0.919 6.2e-09 0.243 0.924 0.933 1.7e-17 Figure 2 shows that ROUGE-BE and ROUGE-1 predictions of both responsiveness and pyramid scores are inferior to the Nouveau-ROUGE-BE predictions. Plots for N2 and NSU4 are omitted due to space restrictions, but performance improvement relative to ROUGE is greater than that for NBE and less than that for N1. 4.2 Validation Using Bootstrapping Experiments To show that our results are not due to a lucky partitioning of the data, we used bootstrapping (Efron and Tibshirani 1993), a resampling method, which allows us to compute our statistical confidence in the results. This model assumes that observed data (scores for the 58 systems) characterize all data. Given this model, the proper sampling method is to choose subsets with replacement. We chose 58 systems (with replacement) and used half to determine the Nouveau-ROUGE parameters and half to test the model. We repeated this process 1,000 times. Table 4 gives the correlation coefficients (for the tested-half of the data) for all four ROUGE metrics with each of the manual evaluations when comparing the machine-genera</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>Efron, B. and R. J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman &amp; Hall, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Chin-Yew Lin</author>
<author>Liang Zhou</author>
</authors>
<title>Evaluating DUC</title>
<date>2005</date>
<booktitle>In Proceedings of the Fifth Document Understanding Conference (DUC),</booktitle>
<location>Vancouver.</location>
<note>Available at www-nlpir/ nist.gov/projects/duc/pubs.html.</note>
<marker>Hovy, Lin, Zhou, 2005</marker>
<rawString>Hovy, Eduard, Chin-Yew Lin, and Liang Zhou. 2005. Evaluating DUC 2005 using basic elements. In Proceedings of the Fifth Document Understanding Conference (DUC), Vancouver. Available at www-nlpir/ nist.gov/projects/duc/pubs.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics,</booktitle>
<pages>495501</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="4997" citStr="Lin and Hovy 2000" startWordPosition="733" endWordPosition="736">ate summaries. In Section 3, we modify ROUGE to produce scores that correlate significantly better with manual evaluation. We evaluate our new metric on TAC 2008 data in Section 4, demonstrating its superiority as a predictor of manual evaluations. 2. State-of-the-Art Evaluation of Update Summaries TAC 2008 presented 48 20-document sets, with 10 documents in each of two subsets, A and B. Subset B documents were more recent. Original summaries were generated for the A subsets and update summaries were then produced for the B subsets. In TAC 2008, ROUGE was used for automatic evaluation. ROUGE (Lin and Hovy 2000) compares any summary to any other (typically human-generated) summary using a recall-oriented approach. ROUGE-1 and -2 are based on unigrams and bigrams, respectively; ROUGE-SU4 uses bigrams with a maximum skip distance of 4 between bigrams; ROUGE-BE (Hovy, Lin, and Zhou 2005) is an n-gram approach based on basic elements, computed via parsing or automatic entity recognition. ROUGE-2, ROUGESU4, and ROUGE-BE were the official automatic metrics for TAC 2008, used to compare machine-generated summaries to human-generated summaries, and to compare human-generated summaries to each other using a j</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Lin, Chin-Yew and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th Conference on Computational Linguistics, pages 495501, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Therese Firmin</author>
<author>David House</author>
<author>Gary Klein</author>
<author>Beth Sundheim</author>
<author>Lynette Hirschman</author>
</authors>
<title>The TIPSTER SUMMAC text summarization evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL99: Ninth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>7785</pages>
<location>Bergen.</location>
<marker>Mani, Firmin, House, Klein, Sundheim, Hirschman, 1999</marker>
<rawString>Mani, Inderjeet, Therese Firmin, David House, Gary Klein, Beth Sundheim, and Lynette Hirschman. 1999. The TIPSTER SUMMAC text summarization evaluation. In Proceedings of EACL99: Ninth Conference of the European Chapter of the Association for Computational Linguistics, pages 7785, Bergen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>145152</pages>
<location>Boston, MA.</location>
<contexts>
<context position="6076" citStr="Nenkova and Passonneau 2004" startWordPosition="891" endWordPosition="894"> TAC 2008, used to compare machine-generated summaries to human-generated summaries, and to compare human-generated summaries to each other using a jackknife approach. In addition to the three official metrics, we include ROUGE-1 in our study as it is often competitive with the official metrics. 2 NIST uses model for human-generated summaries and peer for machine-generated summaries. 2 \x0cConroy, Schlesinger, and OLeary Nouveau-ROUGE Three manual evaluation metrics were used in TAC 2008: pyramid, overall responsiveness, and linguistic quality (not considered in our work). The pyramid method (Nenkova and Passonneau 2004) is a content-based metric for which human annotators mark content units in the human-generated summaries. The content units are collected across a set of human-generated summaries for a topic, and a weight is computed based on how many human-generated summaries include this content unit. TAC 2008 also used a manual overall responsiveness score. After evaluating data from 2005 2007 (Dang 2007; Conroy and Dang 2008), NIST decided that this score, which evaluates summary usefulness including linguistic quality, is a reliable and stable manual evaluation. We analyzed the three official TAC 2008 a</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Nenkova, Ani and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 145152, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith D Schlesinger</author>
<author>Dianne P OLeary</author>
<author>John M Conroy</author>
</authors>
<title>Arabic/ English multi-document summarization with CLASSYThe past and the future.</title>
<date>2008</date>
<booktitle>of Lecture Notes in Computer Science.</booktitle>
<volume>4919</volume>
<pages>568581</pages>
<editor>In Alexander F. Gelbukh, editor, CICLing,</editor>
<publisher>Springer,</publisher>
<location>Haifa,</location>
<marker>Schlesinger, OLeary, Conroy, 2008</marker>
<rawString>Schlesinger, Judith D., Dianne P. OLeary, and John M. Conroy. 2008. Arabic/ English multi-document summarization with CLASSYThe past and the future. In Alexander F. Gelbukh, editor, CICLing, volume 4919 of Lecture Notes in Computer Science. Springer, Haifa, pages 568581.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>