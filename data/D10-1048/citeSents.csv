For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) CITATION,,
3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 CITATION computed over mention pairs in the same entity cluster; (b) MUC CITATION which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and CITATION) which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively,,
CITATION; CITATION) for an analysis of these metrics,,
1 Introduction Recent work on coreference resolution has shown that a rich feature space that models lexical, syntactic, semantic, and discourse phenomena is crucial to successfully address the task (CITATION; CITATION; Haghighi and Klein, 2010),,
When such a rich representation is available, even a simple deterministic model can achieve state-of-the-art performance CITATION,,
2 Related Work This work builds upon the recent observation that strong features outweigh complex models for coreference resolution, in both supervised and unsupervised learning setups (CITATION; CITATION),,
Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (CITATION; CITATION; CITATION; Stoyanov, 2010),,
Two recent works that diverge from this pattern are CITATION and CITATION,,
3.1 Corpora We used the following corpora for development and evaluation: ACE2004-ROTH-DEV2 development split of CITATION, from the corpus used in the 2004 Automatic Content Extraction (ACE) evaluation,,
2 We use the same corpus names as CITATION to facilitate comparison with previous work,,
493 \x0c ACE2004-CULOTTA-TEST partition of ACE 2004 corpus reserved for testing by several previous works (CITATION; CITATION; CITATION),,
ACE2004-NWIRE the newswire subset of the ACE 2004 corpus, utilized by CITATION and CITATION for testing.,,
For example, in ACE2004- CULOTTA-TEST our system has a B3 F1 score only .4 points lower than CITATION and it outperforms all unsupervised approaches,,
In MUC6-TEST, our sieves B3 F1 score is 1.8 points lower than CITATION +S, but it outperforms a supervised system that used gold named entity labels,,
) 83.7 74.1 78.6 88.1 74.2 80.5 80.1 51.0 62.3 This work (single pass) 82.2 72.6 77.1 86.8 72.6 79.1 76.0 47.6 58.5 CITATION S 78.3 70.5 74.2 84.0 71.0 76.9 71.3 45.4 55.5 CITATION +S 77.9 74.1 75.9 81.8 74.3 77.9 68.2 51.2 58.5 ACE2004-CULOTTA-TEST This work (sieve) 80.4 71.8 75.8 86.3 75.4 80.4 71.6 46.2 56.1 This work (single pass) 78.4 69.2 73.5 85.1 73.9 79.1 69.5 44.1 53.9 CITATION S 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3 CITATION +S 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5 CITATION 86.7 73.2 79.3 CITATION 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2 MUC6-TEST This work (sieve) 90.5 68.0 77.7 91.2 61.2 73.2 90.3 53.3 67.1 This work (single pass) 89.3 65.9 75.8 90.2 58.8 71.1 89.5 50.6 64.7 CITATION +S 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3 CITATION 83.0 75.8 79.2 63.0 57.0 60.0 CITATION +G 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 ACE2004-NWIRE This work (sieve) 83.8 73.2 78.1 87.5 71.9 78.9 79.6 46.2 58.4 This work (single pass) 82.2 71.5 76.5 86.2 70.0 77.3 76.9 41.9 54.2 CITATION +S 77.0 75.9 76.5 79.4 74.5 76.9 66.9,,
We use the following attributes for these constraints: Number we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular or plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from CITATION,,
Gender we assign gender attributes from static lexicons from (CITATION; CITATION),,
Animacy we set animacy attributes using: (a) a static list for pronouns; (b) NER labels, e.g., PERSON is animate whereas LOCATION is not; and (c) a dictionary boostrapped from the web CITATION.,,
From a high level perspective, this work falls under the theory of shaping, defined as a method of successive approximations for learning CITATION,,
This theory is known by different names in many NLP applications: CITATION used simple models as stepping stones for more complex word alignment models; Collins (1999) used cautious decision list learning for named entity classification; CITATION used baby steps for unsupervised dependency parsing, etc,,
in both supervised and unsupervised learning setups (CITATION; CITATION),,
Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (CITATION; CITATION; CITATION; Stoyanov, 2010),,
Two recent works that diverge from this pattern are CITATION and CITATION,,
3.1 Corpora We used the following corpora for development and evaluation: ACE2004-ROTH-DEV2 development split of CITATION, from the corpus used in the 2004 Automatic Content Extraction (ACE) evaluation,,
2 We use the same corpus names as CITATION to facilitate comparison with previous work,,
493 \x0c ACE2004-CULOTTA-TEST partition of ACE 2004 corpus reserved for testing by several previous works (CITATION; CITATION; CITATION),,
ACE2004-NWIRE the newswire subset of the ACE 2004 corpus, utilized by CITATION and CITATION for testing,,
Amongst the comparisons, several are supervised (CITATION; CITATION; CITATION),,
The system of CITATION +S uses a lexicon of semantically-compatible noun pairs acquired transductively, i.e., with knowledge of the mentions in the test set,,
Our system does not rely on labeled corpora for training (like supervised approaches) nor access to corpora during testing (like CITATION),,
The system that is closest to ours is CITATION S,,
 F1 ACE2004-ROTH-DEV This work (sieve) 83.7 74.1 78.6 88.1 74.2 80.5 80.1 51.0 62.3 This work (single pass) 82.2 72.6 77.1 86.8 72.6 79.1 76.0 47.6 58.5 CITATION S 78.3 70.5 74.2 84.0 71.0 76.9 71.3 45.4 55.5 CITATION +S 77.9 74.1 75.9 81.8 74.3 77.9 68.2 51.2 58.5 ACE2004-CULOTTA-TEST This work (sieve) 80.4 71.8 75.8 86.3 75.4 80.4 71.6 46.2 56.1 This work (single pass) 78.4 69.2 73.5 85.1 73.9 79.1 69.5 44.1 53.9 CITATION S 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3 CITATION +S 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5 CITATION 86.7 73.2 79.3 CITATION 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2 MUC6-TEST This work (sieve) 90.5 68.0 77.7 91.2 61.2 73.2 90.3 53.3 67.1 This work (single pass) 89.3 65.9 75.8 90.2 58.8 71.1 89.5 50.6 64.7 CITATION +S 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3 CITATION 83.0 75.8 79.2 63.0 57.0 60.0 CITATION +G 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 ACE2004-NWIRE This work (sieve) 83.8 73.2 78.1 87.5 71.9 78.9 79.6 46.2 58.4 This work (single pass) 82.2 71.5 76.5 86.2 70.0 77.3 76.9 41.9 54.2 Haghighi and Klein (200,,
4.2.3 Pass 3 - Strict Head Matching Linking a mention to an antecedent based on the naive matching of their head words generates a lot of spurious links because it completely ignores possibly incompatible modifiers CITATION,,
We parse all documents using the Stanford parser CITATION,,
For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) CITATION,,
3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 CITATION computed over mention pairs in the same entity cluster; (b) MUC CITATION which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and CITATION) which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively,,
CITATION; Fink,,
rk This work builds upon the recent observation that strong features outweigh complex models for coreference resolution, in both supervised and unsupervised learning setups (CITATION; CITATION),,
Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (CITATION; CITATION; CITATION; Stoyanov, 2010),,
Two recent works that diverge from this pattern are CITATION and CITATION,,
3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 CITATION computed over mention pairs in the same entity cluster; (b) MUC CITATION which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and CITATION) which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively,,
CITATION; CITATION) for an analysis of these metrics,,
Amongst the comparisons, several are supervised (CITATION; CITATION; CITATION),,
The system of CITATION +S uses a lexicon of semantically-compatible noun pairs acquired transductively, i.e., with knowledge of the mentions in the test set,,
Our system does not rely on labeled corpora for training (like supervised approaches) nor access to corpora during testing (like CITATION),,
The system that is closest to ours is CITATION S,,
 56.1 This work (single pass) 78.4 69.2 73.5 85.1 73.9 79.1 69.5 44.1 53.9 CITATION S 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3 CITATION +S 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5 CITATION 86.7 73.2 79.3 CITATION 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2 MUC6-TEST This work (sieve) 90.5 68.0 77.7 91.2 61.2 73.2 90.3 53.3 67.1 This work (single pass) 89.3 65.9 75.8 90.2 58.8 71.1 89.5 50.6 64.7 CITATION +S 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3 CITATION 83.0 75.8 79.2 63.0 57.0 60.0 CITATION +G 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 ACE2004-NWIRE This work (sieve) 83.8 73.2 78.1 87.5 71.9 78.9 79.6 46.2 58.4 This work (single pass) 82.2 71.5 76.5 86.2 70.0 77.3 76.9 41.9 54.2 CITATION +S 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7 CITATION 71.3 70.5 70.9 62.6 38.9 48.0 CITATION +G 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9 Table 3: Results using gold mention boundaries,,
We parse all documents using the Stanford parser CITATION,,
For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) CITATION,,
3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 CITATION computed over mention pairs in the same entity cluster; (b) MUC CITATION which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and CITATION) which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively,,
CITATION; CITATION) for an analysis of these metrics,,
1 Introduction Recent work on coreference resolution has shown that a rich feature space that models lexical, syntactic, semantic, and discourse phenomena is crucial to successfully address the task (CITATION; CITATION; Haghighi and Klein, 2010),,
When such a rich representation is available, even a simple deterministic model can achieve state-of-the-art performance CITATION,,
2 Related Work This work builds upon the recent observation that strong features outweigh complex models for coreference resolution, in both supervised and unsupervised learning setups (CITATION; CITATION),,
Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (CITATION; CITATION; CITATION; Stoyanov, 2010),,
Two recent works that diverge from this pattern are CITATION and CITATION,,
3.1 Corpora We used the following corpora for development and evaluation: ACE2004-ROTH-DEV2 development split of CITATION, from the corpus used in the 2004 Automatic Content Extraction (ACE) evaluation,,
2 We use the same corpus names as CITATION to facilitate comparison with previous work,,
493 \x0c ACE2004-CULOTTA-TEST partition of ACE 2004 corpus reserved for testing by several previous works (CITATION; CITATION; CITATION),,
ACE2004-NWIRE the newswire subset of the ACE 2004 corpus, utilized by CITATION and CITATION for testing,,
We sort candidate antecedents using syntactic information provided by the Stanford parser, as follows: Same Sentence Candidates in the same sentence are sorted using left-to-right breadth-first traversal of syntactic trees CITATION,,
The breadthfirst traversal promotes syntactic salience by ranking higher noun phrases that are closer to the top of the parse tree CITATION,,
Subjects are more probable antecedents for pronouns CITATION,,
We use the same syntactic rules to detect appositions as CITATION,,
Not i-within-i the two mentions are not in an iwithin-i construct, i.e., one cannot be a child NP in the others NP constituent CITATION,,
The CITATION numbers have two variants: with semantics (+S) and without (S),,
Amongst the comparisons, several are supervised (CITATION; CITATION; CITATION),,
The system of CITATION +S uses a lexicon of semantically-compatible noun pairs acquired transductively, i.e., with knowledge of the mentions in the test set,,
Our system does not rely on labeled corpora for training (like supervised approaches) nor access to corpora during testing (like CITATION),,
The system that is closest to ours is CITATION S,,
To understand if the difference is due to the multi-pass architecture or the richer feature set we compared CITATION S against both our multi-pass system and its single-pass variant,,
The comparison indicates that both these contributions help: our single-pass system outperforms CITATION consistently, and the multi-pass architecture further improves the performance of our single-pass system between 1 and 4 F1 points, depending on the corpus and evaluation metric,,
6.2 Semantic Head Matching Recent unsupervised coreference work from CITATION included a novel semantic component that matched related head words (e.g., AOL is a company) learned from select 498 \x0cMUC B3 Pairwise P R F1 P R F1 P R F1 ACE2004-ROTH-DEV This work (sieve) 83.7 74.1 78.6 88.1 74.2 80.5 80.1 51.0 62.3 This work (single pass) 82.2 72.6 77.1 86.8 72.6 79.1 76.0 47.6 58.5 CITATION S 78.3 70.5 74.2 84.0 71.0 76.9 71.3 45.4 55.5 CITATION +S 77.9 74.1 75.9 81.8 74.3 77.9 68.2 51.2 58.5 ACE2004-CULOTTA-TEST This work (sieve) 80.4 71.8 75.8 86.3 75.4 80.4 71.6 46.2 56.1 This work (single pass) 78.4 69.2 73.5 85.1 73.9 79.1 69.5 44,,
5 CITATION 86.7 73.2 79.3 CITATION 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2 MUC6-TEST This work (sieve) 90.5 68.0 77.7 91.2 61.2 73.2 90.3 53.3 67.1 This work (single pass) 89.3 65.9 75.8 90.2 58.8 71.1 89.5 50.6 64.7 CITATION +S 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3 CITATION 83.0 75.8 79.2 63.0 57.0 60.0 CITATION +G 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 ACE2004-NWIRE This work (sieve) 83.8 73.2 78.1 87.5 71.9 78.9 79.6 46.2 58.4 This work (single pass) 82.2 71.5 76.5 86.2 70.0 77.3 76.9 41.9 54.2 CITATION +S 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7 CITATION 71.3 70.5 70.9 62.6 38.9 48.0 CITATION +G 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9 Table 3: Results using gold mention boundaries,,
+/-S indicates if the CITATION system includes/excludes their semantic component,,
We sort candidate antecedents using syntactic information provided by the Stanford parser, as follows: Same Sentence Candidates in the same sentence are sorted using left-to-right breadth-first traversal of syntactic trees CITATION,,
The breadthfirst traversal promotes syntactic salience by ranking higher noun phrases that are closer to the top of the parse tree CITATION,,
We use the following attributes for these constraints: Number we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular or plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from CITATION,,
Gender we assign gender attributes from static lexicons from (CITATION; CITATION),,
Animacy we set animacy attributes using: (a) a static list for pronouns; (b) NER labels, e.g., PERSON is animate whereas LOCATION is not; and (c) a dictionary boostrapped from the web CITATION,,
tree CITATION,,
Subjects are more probable antecedents for pronouns CITATION,,
 (CITATION; CITATION; CITATION),,
ACE2004-NWIRE the newswire subset of the ACE 2004 corpus, utilized by CITATION and CITATION for testing,,
We parse all documents using the Stanford parser CITATION,,
For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) CITATION,,
3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 CITATION computed over mention pairs in the same entity cluster; (b) MUC CITATION which measures how many predic,,
3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 CITATION computed over mention pairs in the same entity cluster; (b) MUC CITATION which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and CITATION) which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively,,
CITATION; CITATION) for an analysis of these metrics,,
upervised learning setups (CITATION; CITATION),,
Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (CITATION; CITATION; CITATION; Stoyanov, 2010),,
Two recent works that diverge from this pattern are CITATION and CITATION,,
on: ACE2004-ROTH-DEV2 development split of CITATION, from the corpus used in the 2004 Automatic Content Extraction (ACE) evaluation,,
2 We use the same corpus names as CITATION to facilitate comparison with previous work,,
493 \x0c ACE2004-CULOTTA-TEST partition of ACE 2004 corpus reserved for testing by several previous works (CITATION; CITATION; CITATION),,
ACE2004-NWIRE the newswire subset of the ACE 2004 corpus, utilized by CITATION and CITATION for testing,,
We parse all documents using the Stanford parser CITATION,,
Predicate nominative the two mentions (nominal or pronominal) are in a copulative subject-object relation, e.g., [The New York-based College Board] is [a nonprofit organization that administers the SATs and promotes higher education] CITATION,,
This feature is inspired by CITATION, who triggered it only if the mention is labeled as a person by the NER,,
is work (sieve) 80.4 71.8 75.8 86.3 75.4 80.4 71.6 46.2 56.1 This work (single pass) 78.4 69.2 73.5 85.1 73.9 79.1 69.5 44.1 53.9 CITATION S 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3 CITATION +S 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5 CITATION 86.7 73.2 79.3 CITATION 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2 MUC6-TEST This work (sieve) 90.5 68.0 77.7 91.2 61.2 73.2 90.3 53.3 67.1 This work (single pass) 89.3 65.9 75.8 90.2 58.8 71.1 89.5 50.6 64.7 CITATION +S 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3 CITATION 83.0 75.8 79.2 63.0 57.0 60.0 CITATION +G 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 ACE2004-NWIRE This work (sieve) 83.8 73.2 78.1 87.5 71.9 78.9 79.6 46.2 58.4 This work (single pass) 82.2 71.5 76.5 86.2 70.0 77.3 76.9 41.9 54.2 CITATION +S 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7 CITATION 71.3 70.5 70.9 62.6 38.9 48.0 CITATION +G 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9 Table 3: Results using gold mention boundaries,,
From a high level perspective, this work falls under the theory of shaping, defined as a method of successive approximations for learning CITATION,,
This theory is known by different names in many NLP applications: CITATION used simple models as stepping stones for more complex word alignment models; Collins (1999) used cautious decision list learning for named entity classification; CITATION used baby steps for unsupervised dependency parsing, etc,,
From a high level perspective, this work falls under the theory of shaping, defined as a method of successive approximations for learning CITATION,,
This theory is known by different names in many NLP applications: CITATION used simple models as stepping stones for more complex word alignment models; Collins (1999) used cautious decision list learning for named entity classification; CITATION used baby steps for unsupervised dependency parsing, etc,,
using the Stanford parser CITATION,,
For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) CITATION,,
3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 CITATION computed over mention pairs in the same entity cluster; (b) MUC CITATION which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and CITATION) which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively,,
CITATION; CITATION) for an analysis of these metrics,,
