<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000048">
<bodyText confidence="0.7323085">
b&amp;apos;Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 2528,
Sydney, July 2006. c
</bodyText>
<sectionHeader confidence="0.549927" genericHeader="abstract">
2006 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.865039">
FERRET: Interactive Question-Answering for Real-World Environments
</title>
<author confidence="0.997647">
Andrew Hickl, Patrick Wang, John Lehmann, and Sanda Harabagiu
</author>
<affiliation confidence="0.965227">
Language Computer Corporation
</affiliation>
<address confidence="0.91171">
1701 North Collins Boulevard
Richardson, Texas 75080 USA
</address>
<email confidence="0.994033">
ferret@languagecomputer.com
</email>
<sectionHeader confidence="0.990694" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999474454545455">
This paper describes FERRET, an interac-
tive question-answering (Q/A) system de-
signed to address the challenges of inte-
grating automatic Q/A applications into
real-world environments. FERRET utilizes
a novel approach to Q/A known as pre-
dictive questioning which attempts to
identify the questions (and answers) that
users need by analyzing how a user inter-
acts with a system while gathering infor-
mation related to a particular scenario.
</bodyText>
<sectionHeader confidence="0.99827" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996951389830508">
As the accuracy of todays best factoid question-
answering (Q/A) systems (Harabagiu et al., 2005;
Sun et al., 2005) approaches 70%, research has be-
gun to address the challenges of integrating auto-
matic Q/A systems into real-world environments.
A new class of applications known as interactive
Q/A systems are now being developed which al-
low users to ask questions in the context of ex-
tended dialogues in order to gather information
related to any number of complex scenarios. In
this paper, we describe our interactive Q/A system
known as FERRET which uses an approach
based on predictive questioning in order to meet
the changing information needs of users over the
course of a Q/A dialogue.
Answering questions in an interactive setting
poses three new types of challenges for traditional
Q/A systems. First, since current Q/A systems are
designed to answer single questions in isolation,
interactive Q/A systems must look for ways to fos-
ter interaction with a user throughout all phases of
the research process. Unlike traditional Q/A ap-
plications, interactive Q/A systems must do more
than cooperatively answer a users single question.
Instead, in order to keep a user collaborating with
the system, interactive Q/A systems need to pro-
vide access to new types of information that are
somehow relevant to the users stated and un-
stated information needs.
Second, we have found that users of Q/A sys-
tems in real-world settings often ask questions that
are much more complex than the types of fac-
toid questions that have been evaluated in the an-
nual Text Retrieval Conference (TREC) evalua-
tions. When faced with a limited period of time
to gather information, even experienced users of
Q/A may find it difficult to translate their infor-
mation needs into the simpler types of questions
that Q/A systems can answer. In order to pro-
vide effective answers to these questions, interac-
tive question-answering systems need to include
question decomposition techniques that can break
down complex questions into the types of simpler
factoid-like questions that traditional Q/A systems
were designed to answer.
Finally, interactive Q/A systems must be sen-
sitive not only to the content of a users question
but also to the context that it is asked in. Like
other types of task-oriented dialogue systems, in-
teractive Q/A systems need to model both what a
user knows and what a user wants to know
over the course of a Q/A dialogue: systems that
fail to represent a users knowledge base run the
risk of returning redundant information, while sys-
tems that do not model a users intentions can end
up returning irrelevant information.
In the rest of this paper, we discuss how the
FERRET interactive Q/A system currently ad-
dresses the first two of these three challenges.
</bodyText>
<page confidence="0.997813">
25
</page>
<figureCaption confidence="0.805864">
\x0cFigure 1: The FERRET Interactive Q/A System
</figureCaption>
<sectionHeader confidence="0.901983" genericHeader="method">
2 The FERRET Interactive
</sectionHeader>
<subsectionHeader confidence="0.708278">
Question-Answering System
</subsectionHeader>
<bodyText confidence="0.999516277777778">
This section provides a basic overview of the func-
tionality provided by the FERRET interactive Q/A
system. 1
FERRET returns three types of information in
response to a users query. First, FERRET uti-
lizes an automatic Q/A system to find answers to
users questions in a document collection. In or-
der to provide users with the timely results that
they expect from information gathering applica-
tions (such as Internet search engines), every ef-
fort was made to reduce the time FERRET takes to
extract answers from text. (In the current version
of the system, answers are returned on average in
12.78 seconds. 2)
In addition to answers, FERRET also provides
information in the form of two different types
of predictive question-answer pairs (or QUABs).
With FERRET, users can select from QUABs that
</bodyText>
<page confidence="0.892547">
1
</page>
<bodyText confidence="0.9866388">
For more details on FERRETs question-answering ca-
pabilities, the reader is invited to consult (Harabagiu et al.,
2005a); for more information on FERRETs predictive ques-
tion generation component, please see (Harabagiu et al.,
2005b).
</bodyText>
<page confidence="0.968614">
2
</page>
<bodyText confidence="0.991868208333333">
This test was run on a machine with a Pentium 4 3.0 GHz
processor with 2 GB of RAM.
were either generated automatically from the set
of documents returned by the Q/A system or that
were selected from a large database of more than
10,000 question-answer pairs created offline by
human annotators. In the current version of FER-
RET, the top 10 automatically-generated and hand-
crafted QUABs that are most judged relevant to
the users original question are returned to the user
as potential continuations of the dialogue. Each
set of QUABs is presented in a separate pane
found to the right of the answers returned by the
Q/A system; QUABs are ranked in order of rele-
vance to the users original query.
Figure 1 provides a screen shot of FERRETs
interface. Q/A answers are presented in the cen-
ter pane of the FERRET browser, while QUAB
question-answer pairs are presented in two sep-
arate tabs found in the rightmost pane of the
browser. FERRETs leftmost pane includes a
drag-and-drop clipboard which facilitates note-
taking and annotation over the course of an inter-
active Q/A dialogue.
</bodyText>
<sectionHeader confidence="0.975577" genericHeader="method">
3 Predictive Question-Answering
</sectionHeader>
<bodyText confidence="0.9676295">
First introduced in (Harabagiu et al., 2005b),
a predictive questioning approach to automatic
</bodyText>
<page confidence="0.971531">
26
</page>
<bodyText confidence="0.998373217391304">
\x0cquestion-answering assumes that Q/A systems can
use the set of documents relevant to a users query
in order to generate sets of questions known as
predictive questions that anticipate a users in-
formation needs. Under this approach, topic repre-
sentations like those introduced in (Lin and Hovy,
2000) and (Harabagiu, 2004) are used to identify a
set of text passages that are relevant to a users do-
main of interest. Topic-relevant passages are then
semantically parsed (using a PropBank-style se-
mantic parser) and submitted to a question gener-
ation module, which uses a set of syntactic rewrite
rules in order to create natural language questions
from the original passage.
Generated questions are then assembled into
question-answer pairs known as QUABs with
the original passage serving as the questions an-
swer, and are then returned to the user. For ex-
ample, two of the predictive question-answer pairs
generated from the documents returned for ques-
tion Q0, What has been the impact of job out-
sourcing programs on Indias relationship with the
U.S.?, are presented in Table 1.
</bodyText>
<listItem confidence="0.828228842105263">
Q0 What has been the impact of job outsourcing programs on Indias
relationship with the U.S.?
PQ1 How could India respond to U.S. efforts to limit job outsourcing?
A1 U.S. officials have countered that the best way for India to
counter U.S. efforts to limit job outsourcing is to further liber-
alize its markets.
PQ2 What benefits does outsourcing provide to India?
A2 Indias prowess in outsourcing is no longer the only reason why
outsourcing to India is an attractive option. The difference lies
in the scalability of major Indian vendors, their strong focus on
quality and their experience delivering a wide range of services,
says John Blanco, senior vice president at Cablevision Systems
Corp. in Bethpage, N.Y.
PQ2 Besides India, what other countries are popular destinations for
outsourcing?
A2 A number of countries are now beginning to position themselves
as outsourcing centers including China, Russia, Malaysia, the
Philippines, South Africa and several countries in Eastern Eu-
rope.
</listItem>
<tableCaption confidence="0.974363">
Table 1: Predictive Question-Answer Pairs
</tableCaption>
<bodyText confidence="0.966053130434783">
While neither PQ1 nor PQ2 provide users with
an exact answer to the original question Q0, both
QUABs can be seen as providing users informa-
tion which is complementary to acquiring infor-
mation on the topic of job outsourcing: PQ1 pro-
vides details on how India could respond to anti-
outsourcing legislation, while PQ2 talks about
other countries that are likely targets for outsourc-
ing.
We believe that QUABs can play an impor-
tant role in fostering extended dialogue-like in-
teractions with users. We have observed that the
incorporation of predictive-question answer pairs
into an interactive question-answering system like
FERRET can promote dialogue-like interactions
between users and the system. When presented
with a set of QUAB questions, users typically se-
lected a coherent set of follow-on questions which
served to elaborate or clarify their initial question.
The dialogue fragment in Table 2 provides an ex-
ample of the kinds of dialogues that users can gen-
erate by interacting with the predictive questions
that FERRET generates.
</bodyText>
<footnote confidence="0.950339944444444">
UserQ1: What has been the impact of job outsourcing programs
on Indias relationship with the U.S.?
QUAB1: How could India respond to U.S. efforts to limit job out-
sourcing?
QUAB2: Besides India, what other countries are popular destinations
for outsourcing?
UserQ2: What industries are outsourcing jobs to India?
QUAB3: Which U.S. technology companies have opened customer
service departments in India?
QUAB4: Will Dell follow through on outsourcing technical support
jobs to India?
QUAB5: Why do U.S. companies find India an attractive destination
for outsourcing?
UserQ3: What anti-outsourcing legislation has been considered in
the U.S.?
QUAB6: Which Indiana legislator introduced a bill that would make
it illegal to outsource Indiana jobs?
QUAB7: What U.S. Senators have come out against anti-outsourcing
</footnote>
<tableCaption confidence="0.763083">
legislation?
Table 2: Dialogue Fragment
</tableCaption>
<bodyText confidence="0.984597266666667">
In experiments with human users of FERRET,
we have found that QUAB pairs enhanced the
quality of information retrieved that users were
able to retrieve during a dialogue with the sys-
tem. 3 In 100 user dialogues with FERRET, users
clicked hyperlinks associated with QUAB pairs
56.7% of the time, despite the fact the system re-
turned (on average) approximately 20 times more
answers than QUAB pairs. Users also derived
value from information contained in QUAB pairs:
reports written by users who had access to QUABs
while gathering information were judged to be sig-
nificantly (p &amp;lt; 0.05) better than those reports writ-
ten by users who only had access to FERRETs
Q/A system alone.
</bodyText>
<sectionHeader confidence="0.935595" genericHeader="method">
4 Answering Complex Questions
</sectionHeader>
<bodyText confidence="0.9911015">
As was mentioned in Section 2, FERRET uses
a special dialogue-optimized version of an auto-
matic question-answering system in order to find
high-precision answers to users questions in a
document collection.
During a Q/A dialogue, users of interactive Q/A
systems frequently ask complex questions that
must be decomposed syntactically and semanti-
cally before they can be answered using traditional
Q/A techniques. Complex questions submitted to
</bodyText>
<page confidence="0.983076">
3
</page>
<bodyText confidence="0.971925">
For details of user experiments with FERRET, please
see (Harabagiu et al., 2005b).
</bodyText>
<page confidence="0.989889">
27
</page>
<bodyText confidence="0.9639383">
\x0cFERRET are first subject to a set of syntactic de-
composition heuristics which seek to extract each
overtly-mentioned subquestion from the original
question. Under this approach, questions featuring
coordinated question stems, entities, verb phrases,
or clauses are split into their separate conjuncts;
answers to each syntactically decomposed ques-
tion are presented separately to the user. Table 3
provides an example of syntactic decomposition
performed in FERRET.
</bodyText>
<table confidence="0.708018">
CQ1 What industries have been outsourcing or offshoring jobs
to India or Malaysia?
QD1 What industries have been outsourcing jobs to India?
QD2 What industries have been offshoring jobs to India?
QD3 What industries have been outsourcing jobs to Malaysia?
QD4 What industries have been offshoring jobs to Malaysia?
</table>
<tableCaption confidence="0.993329">
Table 3: Syntactic Decomposition
</tableCaption>
<bodyText confidence="0.944166384615385">
FERRET also performs semantic decomposition
of complex questions using techniques first out-
lined in (Harabagiu et al., 2006). Under this ap-
proach, three types of semantic and pragmatic in-
formation are identified in complex questions: (1)
information associated with a complex questions
expected answer type, (2) semantic dependencies
derived from predicate-argument structures dis-
covered in the question, and (3) and topic informa-
tion derived from documents retrieved using the
keywords contained the question. Examples of the
types of automatic semantic decomposition that is
performed in FERRET is presented in Table 4.
</bodyText>
<footnote confidence="0.839632125">
CQ2 What has been the impact of job outsourcing programs
on Indias relationship with the U.S.?
QD5 What is meant by Indias relationship with the U.S.?
QD6 What outsourcing programs involve India and the U.S.?
QD7 Who has started outsourcing programs for India and the
U.S.?
QD8 What statements were made regarding outsourcing on In-
dias relationship with the U.S.?
</footnote>
<tableCaption confidence="0.986565">
Table 4: Semantic Question Decomposition
</tableCaption>
<bodyText confidence="0.9982798">
Complex questions are decomposed by a pro-
cedure that operates on a Markov chain, by fol-
lowing a random walk on a bipartite graph of
question decompositions and relations relevant to
the topic of the question. Unlike with syntactic
decomposition, FERRET combines answers from
semantically decomposed question automatically
and presents users with a single set of answers
that represents the contributions of each question.
Users are notified that semantic decomposition has
occurred, however; decomposed questions are dis-
played to the user upon request.
In addition to techniques for answering com-
plex questions, FERRETs Q/A system improves
performance for a variety of question types by em-
ploying separate question processing strategies in
order to provide answers to four different types of
questions, including factoid questions, list ques-
tions, relationship questions, and definition ques-
tions.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999347">
We created FERRET as part of a larger effort de-
signed to address the challenges of integrating
automatic question-answering systems into real-
world research environments. We have focused
on two components that have been implemented
into the latest version of FERRET: (1) predic-
tive questioning, which enables systems to provide
users with question-answer pairs that may antici-
pate their information needs, and (2) question de-
composition, which serves to break down complex
questions into sets of conceptually-simpler ques-
tions that Q/A systems can answer successfully.
</bodyText>
<sectionHeader confidence="0.999426" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.974103666666667">
This material is based upon work funded in whole
or in part by the U.S. Government and any opin-
ions, findings, conclusions, or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the views of the U.S.
Government.
</bodyText>
<sectionHeader confidence="0.984503" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99623948">
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl,
and P. Wang. 2005a. Employing Two Question Answer-
ing Systems in TREC 2005. In Proceedings of the Four-
teenth Text REtrieval Conference.
Sanda Harabagiu, Andrew Hickl, John Lehmann, and
Dan Moldovan. 2005b. Experiments with Interactive
Question-Answering. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguistics
(ACL05).
Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl. 2006.
Answering complex questions with random walk models.
In Proceedings of the 29th Annual International ACM SI-
GIR Conference on Research and Development in Infor-
mation Retrieval, Seattle, WA.
Sanda Harabagiu. 2004. Incremental Topic Representations.
In Proceedings of the 20th COLING Conference, Geneva,
Switzerland.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text summariza-
tion. In Proceedings of the 18th COLING Conference,
Saarbrucken, Germany.
R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y. Kan.
2005. Using Syntactic and Semantic Relation Analysis in
Question Answering. In Proceedings of The Fourteenth
Text REtrieval Conference (TREC 2005).
</reference>
<page confidence="0.883159">
28
</page>
<figure confidence="0.322074">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.752283">
<note confidence="0.904231">b&amp;apos;Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 2528, Sydney, July 2006. c 2006 Association for Computational Linguistics</note>
<title confidence="0.986125">FERRET: Interactive Question-Answering for Real-World Environments</title>
<author confidence="0.997798">Andrew Hickl</author>
<author confidence="0.997798">Patrick Wang</author>
<author confidence="0.997798">John Lehmann</author>
<author confidence="0.997798">Sanda Harabagiu</author>
<affiliation confidence="0.999877">Language Computer Corporation</affiliation>
<address confidence="0.998525">1701 North Collins Boulevard Richardson, Texas 75080 USA</address>
<email confidence="0.999944">ferret@languagecomputer.com</email>
<abstract confidence="0.995376833333333">This paper describes FERRET, an interactive question-answering (Q/A) system designed to address the challenges of integrating automatic Q/A applications into real-world environments. FERRET utilizes a novel approach to Q/A known as predictive questioning which attempts to identify the questions (and answers) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>C Clark</author>
<author>M Bowden</author>
<author>A Hickl</author>
<author>P Wang</author>
</authors>
<title>Employing Two Question Answering Systems in TREC</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourteenth Text REtrieval Conference.</booktitle>
<contexts>
<context position="950" citStr="Harabagiu et al., 2005" startWordPosition="127" endWordPosition="130">s Boulevard Richardson, Texas 75080 USA ferret@languagecomputer.com Abstract This paper describes FERRET, an interactive question-answering (Q/A) system designed to address the challenges of integrating automatic Q/A applications into real-world environments. FERRET utilizes a novel approach to Q/A known as predictive questioning which attempts to identify the questions (and answers) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario. 1 Introduction As the accuracy of todays best factoid questionanswering (Q/A) systems (Harabagiu et al., 2005; Sun et al., 2005) approaches 70%, research has begun to address the challenges of integrating automatic Q/A systems into real-world environments. A new class of applications known as interactive Q/A systems are now being developed which allow users to ask questions in the context of extended dialogues in order to gather information related to any number of complex scenarios. In this paper, we describe our interactive Q/A system known as FERRET which uses an approach based on predictive questioning in order to meet the changing information needs of users over the course of a Q/A dialogue. Ans</context>
<context position="4624" citStr="Harabagiu et al., 2005" startWordPosition="733" endWordPosition="736">n order to provide users with the timely results that they expect from information gathering applications (such as Internet search engines), every effort was made to reduce the time FERRET takes to extract answers from text. (In the current version of the system, answers are returned on average in 12.78 seconds. 2) In addition to answers, FERRET also provides information in the form of two different types of predictive question-answer pairs (or QUABs). With FERRET, users can select from QUABs that 1 For more details on FERRETs question-answering capabilities, the reader is invited to consult (Harabagiu et al., 2005a); for more information on FERRETs predictive question generation component, please see (Harabagiu et al., 2005b). 2 This test was run on a machine with a Pentium 4 3.0 GHz processor with 2 GB of RAM. were either generated automatically from the set of documents returned by the Q/A system or that were selected from a large database of more than 10,000 question-answer pairs created offline by human annotators. In the current version of FERRET, the top 10 automatically-generated and handcrafted QUABs that are most judged relevant to the users original question are returned to the user as potent</context>
<context position="5897" citStr="Harabagiu et al., 2005" startWordPosition="946" endWordPosition="949">s is presented in a separate pane found to the right of the answers returned by the Q/A system; QUABs are ranked in order of relevance to the users original query. Figure 1 provides a screen shot of FERRETs interface. Q/A answers are presented in the center pane of the FERRET browser, while QUAB question-answer pairs are presented in two separate tabs found in the rightmost pane of the browser. FERRETs leftmost pane includes a drag-and-drop clipboard which facilitates notetaking and annotation over the course of an interactive Q/A dialogue. 3 Predictive Question-Answering First introduced in (Harabagiu et al., 2005b), a predictive questioning approach to automatic 26 \x0cquestion-answering assumes that Q/A systems can use the set of documents relevant to a users query in order to generate sets of questions known as predictive questions that anticipate a users information needs. Under this approach, topic representations like those introduced in (Lin and Hovy, 2000) and (Harabagiu, 2004) are used to identify a set of text passages that are relevant to a users domain of interest. Topic-relevant passages are then semantically parsed (using a PropBank-style semantic parser) and submitted to a question gener</context>
<context position="11186" citStr="Harabagiu et al., 2005" startWordPosition="1779" endWordPosition="1782"> written by users who only had access to FERRETs Q/A system alone. 4 Answering Complex Questions As was mentioned in Section 2, FERRET uses a special dialogue-optimized version of an automatic question-answering system in order to find high-precision answers to users questions in a document collection. During a Q/A dialogue, users of interactive Q/A systems frequently ask complex questions that must be decomposed syntactically and semantically before they can be answered using traditional Q/A techniques. Complex questions submitted to 3 For details of user experiments with FERRET, please see (Harabagiu et al., 2005b). 27 \x0cFERRET are first subject to a set of syntactic decomposition heuristics which seek to extract each overtly-mentioned subquestion from the original question. Under this approach, questions featuring coordinated question stems, entities, verb phrases, or clauses are split into their separate conjuncts; answers to each syntactically decomposed question are presented separately to the user. Table 3 provides an example of syntactic decomposition performed in FERRET. CQ1 What industries have been outsourcing or offshoring jobs to India or Malaysia? QD1 What industries have been outsourcin</context>
</contexts>
<marker>Harabagiu, Moldovan, Clark, Bowden, Hickl, Wang, 2005</marker>
<rawString>S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang. 2005a. Employing Two Question Answering Systems in TREC 2005. In Proceedings of the Fourteenth Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Andrew Hickl</author>
<author>John Lehmann</author>
<author>Dan Moldovan</author>
</authors>
<title>Experiments with Interactive Question-Answering.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05).</booktitle>
<contexts>
<context position="950" citStr="Harabagiu et al., 2005" startWordPosition="127" endWordPosition="130">s Boulevard Richardson, Texas 75080 USA ferret@languagecomputer.com Abstract This paper describes FERRET, an interactive question-answering (Q/A) system designed to address the challenges of integrating automatic Q/A applications into real-world environments. FERRET utilizes a novel approach to Q/A known as predictive questioning which attempts to identify the questions (and answers) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario. 1 Introduction As the accuracy of todays best factoid questionanswering (Q/A) systems (Harabagiu et al., 2005; Sun et al., 2005) approaches 70%, research has begun to address the challenges of integrating automatic Q/A systems into real-world environments. A new class of applications known as interactive Q/A systems are now being developed which allow users to ask questions in the context of extended dialogues in order to gather information related to any number of complex scenarios. In this paper, we describe our interactive Q/A system known as FERRET which uses an approach based on predictive questioning in order to meet the changing information needs of users over the course of a Q/A dialogue. Ans</context>
<context position="4624" citStr="Harabagiu et al., 2005" startWordPosition="733" endWordPosition="736">n order to provide users with the timely results that they expect from information gathering applications (such as Internet search engines), every effort was made to reduce the time FERRET takes to extract answers from text. (In the current version of the system, answers are returned on average in 12.78 seconds. 2) In addition to answers, FERRET also provides information in the form of two different types of predictive question-answer pairs (or QUABs). With FERRET, users can select from QUABs that 1 For more details on FERRETs question-answering capabilities, the reader is invited to consult (Harabagiu et al., 2005a); for more information on FERRETs predictive question generation component, please see (Harabagiu et al., 2005b). 2 This test was run on a machine with a Pentium 4 3.0 GHz processor with 2 GB of RAM. were either generated automatically from the set of documents returned by the Q/A system or that were selected from a large database of more than 10,000 question-answer pairs created offline by human annotators. In the current version of FERRET, the top 10 automatically-generated and handcrafted QUABs that are most judged relevant to the users original question are returned to the user as potent</context>
<context position="5897" citStr="Harabagiu et al., 2005" startWordPosition="946" endWordPosition="949">s is presented in a separate pane found to the right of the answers returned by the Q/A system; QUABs are ranked in order of relevance to the users original query. Figure 1 provides a screen shot of FERRETs interface. Q/A answers are presented in the center pane of the FERRET browser, while QUAB question-answer pairs are presented in two separate tabs found in the rightmost pane of the browser. FERRETs leftmost pane includes a drag-and-drop clipboard which facilitates notetaking and annotation over the course of an interactive Q/A dialogue. 3 Predictive Question-Answering First introduced in (Harabagiu et al., 2005b), a predictive questioning approach to automatic 26 \x0cquestion-answering assumes that Q/A systems can use the set of documents relevant to a users query in order to generate sets of questions known as predictive questions that anticipate a users information needs. Under this approach, topic representations like those introduced in (Lin and Hovy, 2000) and (Harabagiu, 2004) are used to identify a set of text passages that are relevant to a users domain of interest. Topic-relevant passages are then semantically parsed (using a PropBank-style semantic parser) and submitted to a question gener</context>
<context position="11186" citStr="Harabagiu et al., 2005" startWordPosition="1779" endWordPosition="1782"> written by users who only had access to FERRETs Q/A system alone. 4 Answering Complex Questions As was mentioned in Section 2, FERRET uses a special dialogue-optimized version of an automatic question-answering system in order to find high-precision answers to users questions in a document collection. During a Q/A dialogue, users of interactive Q/A systems frequently ask complex questions that must be decomposed syntactically and semantically before they can be answered using traditional Q/A techniques. Complex questions submitted to 3 For details of user experiments with FERRET, please see (Harabagiu et al., 2005b). 27 \x0cFERRET are first subject to a set of syntactic decomposition heuristics which seek to extract each overtly-mentioned subquestion from the original question. Under this approach, questions featuring coordinated question stems, entities, verb phrases, or clauses are split into their separate conjuncts; answers to each syntactically decomposed question are presented separately to the user. Table 3 provides an example of syntactic decomposition performed in FERRET. CQ1 What industries have been outsourcing or offshoring jobs to India or Malaysia? QD1 What industries have been outsourcin</context>
</contexts>
<marker>Harabagiu, Hickl, Lehmann, Moldovan, 2005</marker>
<rawString>Sanda Harabagiu, Andrew Hickl, John Lehmann, and Dan Moldovan. 2005b. Experiments with Interactive Question-Answering. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Finley Lacatusu</author>
<author>Andrew Hickl</author>
</authors>
<title>Answering complex questions with random walk models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="12135" citStr="Harabagiu et al., 2006" startWordPosition="1917" endWordPosition="1920">ch syntactically decomposed question are presented separately to the user. Table 3 provides an example of syntactic decomposition performed in FERRET. CQ1 What industries have been outsourcing or offshoring jobs to India or Malaysia? QD1 What industries have been outsourcing jobs to India? QD2 What industries have been offshoring jobs to India? QD3 What industries have been outsourcing jobs to Malaysia? QD4 What industries have been offshoring jobs to Malaysia? Table 3: Syntactic Decomposition FERRET also performs semantic decomposition of complex questions using techniques first outlined in (Harabagiu et al., 2006). Under this approach, three types of semantic and pragmatic information are identified in complex questions: (1) information associated with a complex questions expected answer type, (2) semantic dependencies derived from predicate-argument structures discovered in the question, and (3) and topic information derived from documents retrieved using the keywords contained the question. Examples of the types of automatic semantic decomposition that is performed in FERRET is presented in Table 4. CQ2 What has been the impact of job outsourcing programs on Indias relationship with the U.S.? QD5 Wha</context>
</contexts>
<marker>Harabagiu, Lacatusu, Hickl, 2006</marker>
<rawString>Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl. 2006. Answering complex questions with random walk models. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
</authors>
<title>Incremental Topic Representations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th COLING Conference,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="6276" citStr="Harabagiu, 2004" startWordPosition="1006" endWordPosition="1007">the browser. FERRETs leftmost pane includes a drag-and-drop clipboard which facilitates notetaking and annotation over the course of an interactive Q/A dialogue. 3 Predictive Question-Answering First introduced in (Harabagiu et al., 2005b), a predictive questioning approach to automatic 26 \x0cquestion-answering assumes that Q/A systems can use the set of documents relevant to a users query in order to generate sets of questions known as predictive questions that anticipate a users information needs. Under this approach, topic representations like those introduced in (Lin and Hovy, 2000) and (Harabagiu, 2004) are used to identify a set of text passages that are relevant to a users domain of interest. Topic-relevant passages are then semantically parsed (using a PropBank-style semantic parser) and submitted to a question generation module, which uses a set of syntactic rewrite rules in order to create natural language questions from the original passage. Generated questions are then assembled into question-answer pairs known as QUABs with the original passage serving as the questions answer, and are then returned to the user. For example, two of the predictive question-answer pairs generated from t</context>
</contexts>
<marker>Harabagiu, 2004</marker>
<rawString>Sanda Harabagiu. 2004. Incremental Topic Representations. In Proceedings of the 20th COLING Conference, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th COLING Conference,</booktitle>
<location>Saarbrucken, Germany.</location>
<contexts>
<context position="6254" citStr="Lin and Hovy, 2000" startWordPosition="1001" endWordPosition="1004">in the rightmost pane of the browser. FERRETs leftmost pane includes a drag-and-drop clipboard which facilitates notetaking and annotation over the course of an interactive Q/A dialogue. 3 Predictive Question-Answering First introduced in (Harabagiu et al., 2005b), a predictive questioning approach to automatic 26 \x0cquestion-answering assumes that Q/A systems can use the set of documents relevant to a users query in order to generate sets of questions known as predictive questions that anticipate a users information needs. Under this approach, topic representations like those introduced in (Lin and Hovy, 2000) and (Harabagiu, 2004) are used to identify a set of text passages that are relevant to a users domain of interest. Topic-relevant passages are then semantically parsed (using a PropBank-style semantic parser) and submitted to a question generation module, which uses a set of syntactic rewrite rules in order to create natural language questions from the original passage. Generated questions are then assembled into question-answer pairs known as QUABs with the original passage serving as the questions answer, and are then returned to the user. For example, two of the predictive question-answer </context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th COLING Conference, Saarbrucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sun</author>
<author>J Jiang</author>
<author>Y F Tan</author>
<author>H Cui</author>
<author>T-S Chua</author>
<author>M-Y Kan</author>
</authors>
<title>Using Syntactic and Semantic Relation Analysis in Question Answering.</title>
<date>2005</date>
<booktitle>In Proceedings of The Fourteenth Text REtrieval Conference (TREC</booktitle>
<contexts>
<context position="969" citStr="Sun et al., 2005" startWordPosition="131" endWordPosition="134">Texas 75080 USA ferret@languagecomputer.com Abstract This paper describes FERRET, an interactive question-answering (Q/A) system designed to address the challenges of integrating automatic Q/A applications into real-world environments. FERRET utilizes a novel approach to Q/A known as predictive questioning which attempts to identify the questions (and answers) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario. 1 Introduction As the accuracy of todays best factoid questionanswering (Q/A) systems (Harabagiu et al., 2005; Sun et al., 2005) approaches 70%, research has begun to address the challenges of integrating automatic Q/A systems into real-world environments. A new class of applications known as interactive Q/A systems are now being developed which allow users to ask questions in the context of extended dialogues in order to gather information related to any number of complex scenarios. In this paper, we describe our interactive Q/A system known as FERRET which uses an approach based on predictive questioning in order to meet the changing information needs of users over the course of a Q/A dialogue. Answering questions in</context>
</contexts>
<marker>Sun, Jiang, Tan, Cui, Chua, Kan, 2005</marker>
<rawString>R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y. Kan. 2005. Using Syntactic and Semantic Relation Analysis in Question Answering. In Proceedings of The Fourteenth Text REtrieval Conference (TREC 2005).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>