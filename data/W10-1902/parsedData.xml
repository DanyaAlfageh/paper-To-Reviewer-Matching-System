<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.322304">
b&amp;apos;Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 1018,
Uppsala, Sweden, 15 July 2010. c
</note>
<title confidence="0.50364025">
2010 Association for Computational Linguistics
Recognizing Biomedical Named Entities using Skip-chain Conditional
Random Fields
Jingchen Liu Minlie Huang
</title>
<author confidence="0.893278">
Xiaoyan Zhu
</author>
<affiliation confidence="0.7479205">
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
</affiliation>
<email confidence="0.9229745">
liu-jc04@mails.tsinghua.edu.cn
{aihuang, zxy-dcs}@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.7289725" genericHeader="abstract">
Abstract
Linear-chain Conditional Random Fields
</sectionHeader>
<bodyText confidence="0.989208727272727">
(CRF) has been applied to perform the
Named Entity Recognition (NER) task in
many biomedical text mining and infor-
mation extraction systems. However, the
linear-chain CRF cannot capture long dis-
tance dependency, which is very common
in the biomedical literature. In this pa-
per, we propose a novel study of capturing
such long distance dependency by defin-
ing two principles of constructing skip-
edges for a skip-chain CRF: linking sim-
ilar words and linking words having typed
dependencies. The approach is applied to
recognize gene/protein mentions in the lit-
erature. When tested on the BioCreAtIvE
II Gene Mention dataset and GENIA cor-
pus, the approach contributes significant
improvements over the linear-chain CRF.
We also present in-depth error analysis on
inconsistent labeling and study the influ-
ence of the quality of skip edges on the la-
beling performance.
</bodyText>
<sectionHeader confidence="0.99462" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.973579090909091">
Named Entity Recognition (NER) is a key task in
most text mining and information extraction sys-
tems. The improvement in NER can benefit the
final system performance. NER is a challenging
task, particularly in the biomedical literature due
to the variety of biomedical terminologies and the
complicated syntactic structures.
Many studies have been devoted to biomedical
NER. To evaluate biomedical NER systems, sev-
eral challenge competitions had been held, such
as BioNLP/NLPBA in 20041, BioCreAtIvE I in
</bodyText>
<footnote confidence="0.82389225">
Corresponding author
1
http://research.nii.ac.jp/ collier/
workshops/JNLPBA04st.htm
</footnote>
<bodyText confidence="0.950051684210526">
2004 and BioCreAtIvE II in 20062. The overview
reports from these competitions, presenting state-
of-the-art of biomedical NER studies, show that
linear-chain Conditional Random Fields (CRF) is
one of the most commonly used models and has
the most competitive results (Yeh et al., 2005;
Smith et al., 2008). Linear-chain CRF has also
been successfully applied to other NLP tasks such
as POS-tagging (Lafferty et al., 2001) and sen-
tence chunking (Sha and Pereira, 2003). However,
in most of these applications, only linear-chain
CRF was fully exploited, assuming that only adja-
cent words are inter-dependent. The dependency
between distant words, which occurs frequently in
the biomedical literature, is yet to be captured.
In the biomedical literature, the repeated ap-
pearance of same or similar words in one sentence
is a common type of long distance dependencies.
This phenomenon is due to the complicated syn-
tactic structures and the various biomedical termi-
nologies in nature. See the following example:
Both GH deficiency and impaired
spinal growth may result in short
stature, whereas the occurrence of early
puberty in association with GH defi-
ciency reduces the time available for
GH therapy.
the mentions of GH are repeated three times. If
the entity are referred by a pronoun, the meaning
of the sentence will be confusing and unclear be-
cause of the complex sentence structure. In this
sentence:
These 8-oxoguanine DNA glycosy-
lases, hOgg1 (human) and mOgg1
(murine) , are homologous to each other
and to yeast Ogg1.
the words hOgg1, mOgg1 and Ogg1 are homolo-
gous genes belonging to different species, having
</bodyText>
<page confidence="0.894059">
2
</page>
<note confidence="0.299678">
http://www.biocreative.org/
</note>
<page confidence="0.996686">
10
</page>
<bodyText confidence="0.99904397368421">
\x0cvery similar entity names. Some other types of
long distance dependencies also occur frequently
in the biomedical literature. For example, in this
sentence
Western immunoblot analysis detected
p55gag and its cleavage products p39
and p27 in purified particles derived by
expression of gag and gag-pol, respec-
tively.
the words p55gag, p39 and p27 conjuncted by
and, have similar semantic meanings but they are
separated by several tokens. A human curator
can easily recognize such long distance dependen-
cies and annotate these words consistently. How-
ever, when applying the linear-chain CRF, incon-
sistency errors in annotating these entities could
happen due to the inability of representing long
distance dependency.
In this paper, we present an approach of cap-
turing long distance dependencies between words.
We adopte the skip-chain CRF to improve the per-
formance of gene mention recognition. We de-
fine two principles of connecting skip-edges for
skip-chain CRF to capture long distance depen-
dencies. The efficacy of the principles is inves-
tigated with extensive experiments. We test our
method on two data sets and significant improve-
ments are observed over the linear-chain CRF. We
present in-depth error analysis on inconsistent la-
beling. We also investigat whether the quality of
connected edges affect the labeling performance.
The remainder of this paper is organized as fol-
lows: We survey related studies in Section 2. We
introduce linear-chain CRF and skip-chain CRF in
Section 3. The method of connecting skip-chain
edges is described in Section 4 . In Section 5 we
present our experiments and in-depth analysis. We
summarize our work in Section 6.
</bodyText>
<sectionHeader confidence="0.999203" genericHeader="method">
2 Related work
</sectionHeader>
<bodyText confidence="0.999373967213115">
NER is a widely studied topic in text mining
research, and many new challenges are seen in
domain-specific applications, such as biomedical
NER (Zhou et al., 2004). The dictionary based
method is a common technique as biomedical the-
sauruses play a key role in understanding such
text. Most dictionary based NER systems fo-
cused on: (1) integrating and normalizing differ-
ent biomedical databases to improve the quality of
the dictionary to be used; (2) improving matching
strategies that are more suitable for biomedical ter-
minologies; and (3) making filtering rules for post-
processing to refine the matching results or to ad-
just the boundary of entities, see (Fukuda et al.,
1998; Narayanaswamy et al., 2003; Yang et al.,
2008). Many information extraction systems had
a dictionary matching module to perform prelim-
inary detection of named entities (Schuhmann et
al., 2007; Kolarik et al., 2007; Wang et al., 2010).
Applying machine learning techniques gener-
ally obtains superior performance for the biomedi-
cal NER task. The automated learning process can
induce patterns for recognizing biomedical names
and rules for pre- and post-processing. Gener-
ally speaking, there are two categories of ma-
chine learning based methods: one treats NER as
a classification task, while the other treats NER
as a sequence labeling task. For the first cate-
gory, Support Vector Machine (SVM) was a com-
monly adopted model (Kazama et al., 2002; Zhou
et al., 2004). Lee et al. (2004) proposed a two-
step framework to perform biomedical NER using
SVM: firstly detecting the boundaries of named
entities using classifiers; secondly classifying each
named entity into predefined target types. For the
second category, a sentence was treated as a se-
quence of tokens and the objective was to find the
optimal label sequence for these tokens. The label
space was often defined as {B,I,O}, where B in-
dicates the beginning token of an entity, I denotes
the continuing token and O represents the token
outside an entity. The sequence labeling task can
be approached by Hidden Markov Model (HMM),
Conditional Random Field (CRF) , or a combina-
tion of different models (Zhou et al., 2005; Tatar
and Cicekli, 2009).
Since proposed in (Lafferty et al., 2001), CRF
has been applied to many sequence labeling
tasks, including recognizing gene mentions from
biomedical text (McDonald and Pereira, 2005).
The Gene Mention Recognition task was included
in both BioCreAtIvE I and BioCreAtIvE II chal-
lenges. CRF had been used in most of top per-
forming systems in the Gene Mention Recognition
task of BioCreAtIvE II (Smith et al., 2008). Some
novel use of linear-chain CRF was proposed. For
example, in (Kuo et al., 2007) labeling was per-
formed in forward and backward directions on the
same sentence and results were combined from
the two directions. Huang et al. (2007) com-
bines a linear-chain CRF and two SVM models
</bodyText>
<page confidence="0.999448">
11
</page>
<bodyText confidence="0.976521333333333">
\x0cto enhance the recall. Finkel et al. (2005) used
Gibbs Sampling to add non-local dependencies
into linear-chain CRF model for information ex-
traction. However, the CRF models used in these
systems were all linear-chain CRFs. To the best of
our knowledge, no previous work has been done
on using non-linear-chain CRF in the biomedical
NER task.
Beyond the biomedical domain, skip-chain
CRF has been used in several studies to model
long distance dependency. In (Galley, 2006), skip
edges were linked between sentences with non-
local pragmatic dependencies to rank meetings.
In (Ding et al., 2008), skip-chain CRF was used
to detect the context and answers from online fo-
rums. The most close work to ours was in (Sut-
ton and McCallum, 2004), which used skip-chain
CRF to extract information from email messages
announcing seminars. By linking the same words
whose initial letter is capital, the method obtained
improvements on extracting speakers name. Our
work is in the spirit of this idea, but we approach
it in a different way. We found that the problem is
much more difficult in the biomedical NER task:
that is why we systematically studied the princi-
ples of linking skip edges and the quality of con-
nected edges.
</bodyText>
<sectionHeader confidence="0.409595" genericHeader="method">
3 linear-chain and skip-chain CRF
</sectionHeader>
<bodyText confidence="0.992887875">
Conditional Random Field is a probabilistic
graphic model. The model predicts the output
variables y for each input variables in x by calcu-
lating the conditional probability p(y|x) accord-
ing to the graph structure that represents the de-
pendencies between the y variables. Formally,
given a graph structure over y, the CRF model can
be written as:
</bodyText>
<equation confidence="0.946575666666667">
p(y|x) =
1
Z(x)
Y
Cp
Y
cCp
c(xc, yc; p) (1)
Z(x) is a normalization factor.
</equation>
<bodyText confidence="0.9987784">
In this definition, the graph is partitioned into a
set of cliques = {C1, C2, . . . Cp}, where each
Cp is a clique template. Each c, called a factor,
is corresponding to one edge in the clique c, and
can be parameterized as:
</bodyText>
<equation confidence="0.997495">
c(xc, yc; p) = exp
X
k=1
pkfpk(xc, yc) (2)
</equation>
<bodyText confidence="0.99949047826087">
Each feature function fpk(xc, yc) represents one
feature of x and the pk is the feature weight.
In the training phrase, the parameters is esti-
mated using an optimization algorithm such as
limited memory BFGS etc. In the testing phrase,
CRF finds the most likely label sequence for an
unseen instance by maximizing the probability de-
fined in (1).
In the NER task, one sentence is firstly tok-
enized into a sequences of tokens and each token
can be seen as one word. Each node in the graph is
usually corresponding to one word in a sentence.
Each x variable represents a set of features for one
word, and each y is the variable for the label of
one word. Note that when one edge is linked be-
tween two words, the edge is actually linked be-
tween their corresponding y variables. The y label
is one of {B,I,O}, in which B means the beginning
word of an entity, I means the inside word of an
entity, and O means outside an entity.
If we link each word with its immediate preced-
ing words to form a linear structure for one sen-
tence, we get a linear-chain CRF, defined as:
</bodyText>
<equation confidence="0.999158857142857">
p(y|x) =
1
Z(x)
T
Y
t=1
t(yt, yt1, x) (3)
</equation>
<bodyText confidence="0.9958036">
This structure contains only one clique template.
If we add an extra clique template that contains
some skip edges between nonadjacent words, the
CRF become a skip-chain CRF, formulated as fol-
lows:
</bodyText>
<equation confidence="0.9993385">
p(y|x) =
1
Z(x)
T
Y
t=1
t(yt, yt1, x)
Y
(u,v)
uv(yu, yv, x) (4)
</equation>
<bodyText confidence="0.9962345">
is the edge set of the extra clique template con-
taining skip edges. An illustration of linear-chain
and skip-chain CRF is given in Figure 1. It is
straightforward to change a linear-chain CRF to
a skip-chain CRF by simply linking some addi-
tional skip edges. However, it must be careful to
add such edges because different graph structures
require different inference algorithms. Those in-
ference algorithms may have quite different time
complexity. For example, for the linear-chain
CRF, inference can be performed efficiently and
exactly by a dynamic-programming algorithm.
However, for the non-linear structure, approxi-
mate inference algorithms must be used. Solv-
ing arbitrary CRF graph structures is NP-hard. In
other word, we must be careful to link too many
</bodyText>
<page confidence="0.995109">
12
</page>
<figureCaption confidence="0.838102">
\x0cFigure 1: The illustration of linear-chain CRF and skip-chain CRF. The blue edges represent the linear-
</figureCaption>
<bodyText confidence="0.9748425">
chain edges belonging to one clique template, while the red edges represent the skip edges belonging to
another clique template.
skip edges to avoid making the model impracti-
cal. Therefore, it is absolutely necessary to study
which kinds of edges will contribute to the perfor-
mance while avoiding over-connected edges.
</bodyText>
<subsectionHeader confidence="0.961661">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.997471615384615">
As our interest is in modifying the CRF graph
structure rather than evaluating the effectiveness
of features, we simply adopted features from the
state-of-the-art such as (McDonald and Pereira,
2005) and (Kuo et al., 2007).
Common Features: the original word, the
stemmed word, the POS-tag of a word, the
word length, is or not the beginning or end-
ing word of the sentence etc.
Regular Expression Features: a set of reg-
ular expressions to extract orthographic fea-
tures for the word.
Dictionary Features: We use several lexi-
cons. For example, a protein name dictionary
compiled from SWISS-PROT, a species dic-
tionary from NCBI Taxonomy, a drug name
dictionary from DrugBank database, and a
disease name dictionary from several Internet
web site.
N-gram Features: For each token, we ex-
tract the corresponding 2-4 grams into the
feature set.
Each word will include the adjacent words fea-
tures within {2, 1, 0, 1, 2} offsets. The features
used in the linear-chain CRF and skip-chain CRF
are all the same in our experiment.
</bodyText>
<sectionHeader confidence="0.996827" genericHeader="method">
4 Method
</sectionHeader>
<bodyText confidence="0.9998155625">
As the limitations discussed above, detecting
the necessary nodes to link should be the first
step in constructing a skip-chain CRF. In the
speaker name extraction task (Sutton and Mc-
Callum, 2004), only identical capitalized words
are linked, because there is few variations in the
speakers name. However, gene mentions often
involve words without obvious orthographic fea-
tures and such phenomena are common in the
biomedical literature such as RGC DNA sequence
and multisubunit TFIID protein. If we link all
the words like DNA, sequence and protein, the ef-
ficiency and performance will drop due to over-
connected edges. Therefore, the most important
step of detecting gene mentions is to determine
which edges should be connected.
</bodyText>
<subsectionHeader confidence="0.977621">
4.1 Detect keywords in gene mention
</subsectionHeader>
<bodyText confidence="0.999445666666667">
We found that many gene mentions have at least
one important word for the identification of gene
mentions. For example, the word, Gal4, is such a
</bodyText>
<page confidence="0.997481">
13
</page>
<bodyText confidence="0.997014275862069">
\x0ckeyword in Gal4 protein and NS1A in NS1A pro-
tein. These words can distinguish gene mentions
from other common English words and phrases,
and can distinguish different gene mentions as
well. We define such words as the keyword of
a gene mention. The skip edges are limited to
only connect these keywords. We use a rule-based
method to detect keywords. By examining the an-
notated data, we defined keywords as those con-
taining at least one capital letter or digit. And at
the same time, keywords must conform to the fol-
lowing rules:
Keywords are not stop words, single letters,
numbers, Greek letters, Roman numbers or
nucleotide sequence such as ATTCCCTGG.
Keywords are not in the form of an upper-
case initial letter followed by lowercase let-
ters, such as Comparison and Watson. These
words have capital letters only because they
are the first word in the sentences, or they are
the names of people or other objects. This
rule will miss some correct candidates, but
reduces noise.
Keywords do not include some common
words with capital letters such as DNA,
cDNA, RNA, mRNA, tRNA etc. and some fre-
quently appearing non-gene names such as
HIV and mmHg. We defined a lexicon for
such words on the training data.
</bodyText>
<subsectionHeader confidence="0.987994">
4.2 Link similar keywords
</subsectionHeader>
<bodyText confidence="0.99695325">
After keyword candidates are detected, we judge
each pair of keywords in the same sentence to find
similar word pairs. Each word pair is examined by
these rules:
They are exactly the same words.
Words only differ in digit letters, such as
CYP1 and CYP2.
Words with the same prefix, such as IgA and
IgG, or with the same suffix, such as ANF and
pANF.
The token pair will be linked by a skip edge if they
match at least one rule.
</bodyText>
<subsectionHeader confidence="0.963801">
4.3 Link typed dependencies
</subsectionHeader>
<bodyText confidence="0.995210222222222">
Some long distance dependency cannot be de-
tected simply by string similarity. To capture such
dependency, we used stanford parser3 to parse sen-
tences and extract typed dependencies from parsed
results. The typed dependencies are a set of bi-
nary relations belonging to 55 pre-defined types to
provide a description of the grammatical relation-
ships in a sentence (Marneffe and Manning, 2008).
Some examples of typed dependencies are listed in
</bodyText>
<tableCaption confidence="0.97662">
Table 1.
</tableCaption>
<subsectionHeader confidence="0.794028">
Type Description
</subsectionHeader>
<bodyText confidence="0.884261166666667">
conj conjuncted by the conjunc-
tion such as and
prep prepositional modifier
nn noun compound modifier
amod adjectival modifier
dep uncertain types
</bodyText>
<tableCaption confidence="0.693898">
Table 1: Examples for typed dependencies.
</tableCaption>
<bodyText confidence="0.998300454545455">
The output of the parser is pairs of dependent
words, along with typed dependencies between
two words in a pair. For example, in the sentence:
...and activate transcription of a set
of genes that includes G1 cyclins CLN1,
CLN2, and many DN, synthesis genes.
a typed dependency nn(G1,CLN1) is extracted by
the parser, meaning the words G1 and CLN1 has a
typed dependency of nn because they form a noun
phrase under a dependency grammar: modifica-
tion. Similarly, in the sentence
Using the same approach we have
shown that hFIRE binds the stimula-
tory proteins Sp1 and Sp3 in addition to
CBF.
the words Sp1 and Sp3 can be detected to have a
typed dependency of conj and, and the two words
have a typed denpendency of prep in addition to
with CBF, respectively. The most common type
dependencies are conj and, nn and dep. The key-
words having typed dependencies will be linked
by a skip edge.
</bodyText>
<sectionHeader confidence="0.992502" genericHeader="method">
5 Experiment
</sectionHeader>
<bodyText confidence="0.8652985">
We tested our method on two datasets: the Gene
Mention (GM) data in BioCreAtIvE II (BCIIGM)
</bodyText>
<page confidence="0.608103">
3
</page>
<equation confidence="0.3895795">
http://nlp.stanford.edu/software/
lex-parser.shtml
</equation>
<page confidence="0.98802">
14
</page>
<bodyText confidence="0.998043526315789">
\x0c4and GENIA corpus5. The BCIIGM dataset was
used in the BioCreAtIvE II Gene Mention Recog-
nition task in 2006. It was built from the GENE-
TAG corpus (Tanabe et al., 2005) with some mod-
ification of the annotation. The dataset contains
15000 sentences for training and 5000 sentences
for testing. Two gold-standard sets, GENE and
ALTGENE, were provided for evaluation and an
official evaluation procedure in Perl script was
provided. The ALTGENE set provides alternate
forms for genes in the GENE set. In the official
evaluation, each identified string will be looked up
in both GENE and ALTGENE. If the correspond-
ing gene was found in either GENE or ALTGENE,
the identified string will be counted as a correct
answer.
The GENIA corpus is a widely used dataset in
many NER and information extraction tasks due
to its high quality annotation. The GENIA corpus
contains 2000 abstracts from MEDLINE, with ap-
proximately 18500 sentences. The corpus was an-
notated by biomedical experts according to a pre-
defined GENIA ontology. In this work, we only
used the annotated entities that have a category of
protein, DNA, or RNA. These categories are re-
lated to the definition of gene mention in BioCre-
AtIvE II. We only used strict matching evaluation
(no alternate forms check) for the GENIA corpus
as no ALTGENE-like annotation is available.
The performance is measured by precision, re-
call and F score. Each identified string is counted
as a true positive (TP) if it is matched by a gold-
standard gene mention, otherwise the identified
string is a false positive (FP). Each gold standard
gene mention is counted as a false negative (FN) if
it is not identified by the approach. Then the pre-
cision, recall and their harmonic average F score
is calculated as follows:
</bodyText>
<equation confidence="0.940327714285714">
precision =
TP
TP + FP
recall =
TP
TP + FN
F =
</equation>
<figure confidence="0.9718389">
2 precision recall
precision + recall
To implement both linear-chain CRF and skip-
4
http://sourceforge.net/projects/
biocreative/files/
5
http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/home/wiki.cgi?page=Technical+Term+
Annotation
</figure>
<bodyText confidence="0.76343075">
chain CRF, we used the GRMM Java package6
which is an extended version of MALLET. The
package provides an implement of arbitrary struc-
ture CRF.
</bodyText>
<subsectionHeader confidence="0.978955">
5.1 Result Comparison
</subsectionHeader>
<bodyText confidence="0.999644857142857">
We evaluated our approach on the BCIIGM
dataset and GENIA corpus. For the BCIIGM
dataset, two evaluation criteria were used: official
- exactly the same as that used in the BioCreAtIvE
II competition, with the official evaluation proce-
dure; and strict - strict matching for each identi-
fied string without checking its alternate forms in
ALTGENE. The GENIA dataset were randomly
divided into 10 parts to perform a 10-fold cross
validation. However, we didnt do cross validation
on the BCIIGM dataset because the BioCreAtIvE
II competition annotations and evaluation proce-
dure were tailored to evaluating participating sys-
tems.
The comparative results are listed in Table 2.
We compared the two edge linking principles,
linking similar words and linking words having
typed dependencies. The F score from the skip-
chain CRF is better than that from the linear-chain
CRF. Significance tests were performed to check
whether these results have significant differences.
Paired two-tail t-tests were conducted with respect
to the F scores of linear-chain CRF vs. those of the
two skip-chain CRFs, respectively. The p-value
was 1.989107 for the skip-chain CRF linked by
similar words vs. linear-chain CRF. The p-value
was 3.971 105 for the skip-chain CRF linked
by typed dependencies vs. linear-chain CRF. This
shows that the improvement is significant.
Note that we did not compare our results on the
BCIIGM dataset to those submitted to the compe-
tition. There are two reasons for this: First, our
focus is on comparing the skip-chain CRF with
the linear-chain CRF. Second, in the competition,
most participating systems that used CRF also
applied other algorithms, or sophisticated rules
for adjusting detected boundaries or refining the
recognized results, to achieve competitive perfor-
mance. By contrast, we did not employ any post-
processing rule or algorithm to further improve the
performance. In this sense, comparing our results
to those has become unfair.
</bodyText>
<page confidence="0.812962">
6
</page>
<footnote confidence="0.267662">
http://mallet.cs.umass.edu/grmm/
index.php
</footnote>
<page confidence="0.759323">
15
</page>
<table confidence="0.979102">
\x0cData Model Precision(%) Recall(%) F score(%)
BCIIGM official
linear-chain CRF 85.16 81.50 83.29
skip-chain CRF linked by sim-words 86.68 82.75 84.67
skip-chain CRF linked by typed-dep 86.73 82.36 84.49
BCIIGM strict
linear-chain CRF 74.09 69.49 71.73
skip-chain CRF linked by sim-words 76.26 71.53 73.82
skip-chain CRF linked by typed-dep 75.99 70.49 73.14
GENIA
linear-chain CRF 76.77 74.92 75.83
skip-chain CRF linked by sim-words 78.57 77.12 77.82
skip-chain CRF linked by typed-dep 78.18 76.87 77.52
</table>
<tableCaption confidence="0.997073">
Table 2: The result comparison between the linear-chain CRF and skip-chain CRF. BCIIGM is the
</tableCaption>
<sectionHeader confidence="0.612997" genericHeader="method">
BioCreAtIvE II Gene Mention Recognition dataset. official means using the official provided evalua-
</sectionHeader>
<bodyText confidence="0.968715666666667">
tion procedure and strict means using strict matching to evaluate the results. sim-words means similar
words and typed-dep means typed dependencies. The results for GENIA are averaged over 10-fold cross
validation.
</bodyText>
<subsectionHeader confidence="0.997891">
5.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999422453125">
We provided in-depth analysis of our results on the
BCIIGM dataset. As one of our motivations for
connecting words with skip edges is to enhance
the consistency of labeling, we firstly examined
whether the proposed approach can provide con-
sistent labeling. Let us start from two typical ex-
amples. In the first sentence
The response sequences were localized
between -67 and +30 in the simian cy-
tomegalovirus IE94 promoter and up-
stream of position +9 in the HCMV IE68
promoter.
the word IE94 is missed (not labeled) while its
similar word IE68 is labeled correctly by the
linear-chain CRF. In the second sentence
It is suggested that biliary secretion of
both TBZ and FBZ and their metabolites
may contribute to this recycling.
the word TBZ is labeled as a gene mention in-
correctly (false positive) while its similar word
FBZ is not labeled at all (true negative) by the
linear-chain CRF. Both sentences are correctly la-
beled by the skip-chain CRF. Similar improve-
ments are also made by the skip-chain CRF model
linked by typed dependencies. To study label-
ing consistency, we counted the statistics of in-
consistency errors, as shown in Table 3. Two
kinds of inconsistency errors were counted: false
negatives correctable by consistency (FNCC) and
false positives correctable by consistency (FPCC).
An FNCC means that a gold-standard mention is
missed by the system while its skip edge linked
gene mention is correctly labeled, which is simi-
lar to the inconsistent miss in (Sutton and McCal-
lum, 2004), as the IE94 in the first example. An
FPCC means a non-gene mention is labeled as a
gene while its skip edge linked mention (also non-
gene mention) is not recognized, as TBZ in the sec-
ond example. These two kinds of inconsistency er-
rors lead to inconsistent false negatives (FN) and
false positives (FP). A good model should reduce
as much inconsistency errors as possible. The in-
consistency errors are reduced substantially as we
expected, showing that the reduction of inconsis-
tency errors is one reason for the performance im-
provements.
The skip-chain CRF linked by similar words
had better performance than the skip-chain CRF
linked by typed dependencies. This may infer that
the quality of skip edges has impact on the per-
formance. In order to study this issue, the qual-
ity of skip edges was examined. The statistics of
skip edges in the BCIIGM dataset for the two skip-
chain CRF models (linked by similar words and by
typed dependencies respectively) is shown in the
first two rows of Table 4. A skip edge is counted as
a correct edge if the edge links two words that are
both gene mentions in the gold-standard annota-
tion. The statistics shows that the skip-chain CRF
linked by similar words has a higher precision than
the model by typed dependencies. To make the
comparison more evident, we built another skip-
chain CRF whose skip edges were randomly con-
nected. The number of skip edges in this model
</bodyText>
<page confidence="0.9788">
16
</page>
<table confidence="0.985343818181818">
\x0cSkip edge
Model FPCC FNCC
type
sim-words
linear-chain 112 70
skip-chain 48 20
Percentage of reduction 57.14% 71.43%
typed-dep
linear-chain 32 29
skip-chain 9 5
Percentage of reduction 71.88% 82.76%
</table>
<tableCaption confidence="0.999434">
Table 3: Statistics of inconsistency errors for
</tableCaption>
<bodyText confidence="0.9739809">
the linear-chain CRF and skip-chain CRF. FPCC
is false positives correctable by consistency and
FNCC is false negatives correctable by consis-
tency in the table. The percentage is calculated
by dividing the reduction of errors by the error
number of linear-chain CRF, for example (112
48)/48 = 57.14%.
approximately equals to that in the skip-chain CRF
linked by similar words. The percentage of cor-
rect skip-edges in this model is small, as shown
in the last row of Table 4. We tested this skip-
chain CRF model on the BCIIGM dataset under
the strict matching criterion. The performance of
the randomly linked skip-chain CRF is shown in
Table 5. As can be seen from the table, the perfor-
mance of the randomly connected skip-chain CRF
droped remarkably, even worse than that of the
linear-chain CRF. This confirms that the quality
of skip edges is a key factor for the performance
improvement.
</bodyText>
<table confidence="0.736702857142857">
Model Edges
Correct
Percentage
edges
sim-words 1912 1344 70.29%
typed-dep 728 425 53.38%
random 1906 41 2.15%
</table>
<tableCaption confidence="0.998763">
Table 4: Statistics of skip edges and correct
</tableCaption>
<bodyText confidence="0.982155727272727">
skip edges for the skip-chain CRF models. sim-
words means the skip-chain CRF linked by sim-
ilar words, typed-dep means the CRF linked by
typed dependencies and random means the skip-
chain CRF has randomly connected skip edges.
The edges are counted in the BCIIGM testing data.
From the above discussion, we summarize this
section as follows: (1) the skip-chain CRF with
high quality skip edges can reduce inconsistent la-
beling errors, and (2) the quality of skip edges is
crucial to the performance improvement.
</bodyText>
<table confidence="0.9976854">
Model P (%) R(%) F(%)
linear 74.09 69.49 71.73
sim-words 76.26 71.53 73.82
typed-dep 75.99 70.49 73.14
random 73.66 69.13 71.32
</table>
<tableCaption confidence="0.997985">
Table 5: Performance comparison between the
</tableCaption>
<bodyText confidence="0.9944239">
randomly linked skip-chain CRF and other mod-
els. The result was tested on the BCIIGM dataset
under the strict matching criterion. P, R and F
denote the precision, recall and F score respec-
tively. linear denotes the linear-chain CRF. sim-
words denotes the skip-chain CRF linked by sim-
ilar words. typed-dep denotes the skip-chain CRF
linked by typed dependencies. random denotes
the skip-chain CRF having randomly linked skip
edges.
</bodyText>
<sectionHeader confidence="0.997819" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.996346047619048">
This paper proposed a method to construct a skip-
chain CRF to perform named entity recognition in
the biomedical literature. We presented two prin-
ciples to connect skip edges to address the issue
of capturing long distance dependency: linking
similar keywords and linking words having typed
dependencies. We evaluated our method on the
BioCreAtIvE II GM dataset and GENIA corpus.
Significant improvements were observed. More-
over, we presented in-depth analysis on inconsis-
tent labeling errors and the quality of skip edges.
The study shows that the quality of linked edges is
a key factor of the system performance.
The quality of linked edges plays an important
role in not only performance but also time effi-
ciency. Thus, we are planning to apply machine
learning techniques to automatically induce pat-
terns for linking high-quality skip-edges. Further-
more, to refine the recognition results, we are plan-
ning to employ post-processing algorithms or con-
struct refinement rules.
</bodyText>
<sectionHeader confidence="0.933301" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.5942505">
This work was partly supported by the Chi-
nese Natural Science Foundation under grant No.
60803075 and No.60973104, and partly carried
out with the aid of a grant from the International
Development Research Center, Ottawa, Canada
IRCI project from the International Development.
</bodyText>
<page confidence="0.76709">
17
</page>
<reference confidence="0.993805336448598">
\x0cReferences
Shilin Ding, Gao Cong, Chin-Yew Lin and Xiaoyan
Zhu. 2008. Using Conditional Random Fields to
Extract Contexts and Answers of Questions from On-
line Forums. In Proceedings of 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL08), pp 710-718.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. Proceedings of the 43rd Annual Meeting
of the ACL, pages 363C370.
K. Fukuda, A. Tamura, T. Tsunoda and T. Takagi.
1998. Toward information extraction: identifying
protein names from biological papers. Pacific Sym-
posium on Biocomputing. 1998.
Michel Galley. 2006. A Skip-Chain Conditional Ran-
dom Field for Ranking Meeting Utterances by Im-
portance. Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2006), pages 364-372.
Han-Shen Huang, Yu-Shi Lin, Kuan-Ting Lin, Cheng-
Ju Kuo, Yu-Ming Chang, Bo-Hou Yang, I-Fang
Chung and Chun-Nan Hsu. 2007. High-recall gene
mention recognition by unification of multiple back-
ward parsing models. Proceedings of the Second
BioCreative Challenge Evaluation Workshop, pages
109-111.
Junichi Kazama, Takaki Makino, Yoshihiro Ohta and
Junichi Tsujii. 2002. Tuning support vector
machines for biomedical named entity recognition.
Proceedings of the ACL-02 workshop on Natural
language processing in the biomedical domain - Vol-
ume 3.
Corinna Kolarik, Martin Hofmann-Apitius, Marc Zim-
mermann and Juliane Fluck. 2007. Identification of
new drug classification terms in textual resources.
Bioinformatics 2007 23(13):i264-i272
Cheng-Ju Kuo, Yu-Ming Chang, Han-Shen Huang,
Kuan-Ting Lin, Bo-Hou Yang, Yu-Shi Lin, Chun-
Nan Hsu and I-Fang Chung. 2007. Rich Feature
Set, Unification of Bidirectional Parsing and Dictio-
nary Filtering for High F-Score Gene Mention Tag-
ging. Proceedings of the Second BioCreative Chal-
lenge Evaluation Workshop, pages 105-107.
Ki-Joong Lee, Young-Sook Hwang, Seonho Kim and
Hae-Chang Rim. 2004. Biomedical named entity
recognition using two-phase model based on SVMs.
Journal of Biomedical Informatics, Volume 37, Issue
6, December 2004, Pages 436-447.
John Lafferty, Andrew McCallum and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. ICML-01, pages 282-289,
2001.
Ryan McDonald and Fernando Pereira. 2005. Identi-
fying gene and protein mentions in text using con-
ditional random fields. BMC Bioinformatics 2005,
6(Suppl 1):S6.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
M. Narayanaswamy, K.E. Ravikumar and K. Vijay-
Shanker. 2003. A biological named entity recog-
nizer. Pacific Symposium on Biocomputing. 2003.
Dietrich Rebholz-Schuhmann, Harald Kirsch, Miguel
Arregui, Sylvain Gaudan, Mark Riethoven and Pe-
ter Stoehr. 2007. EBIMedtext crunching to gather
facts for proteins from Medline. Bioinformatics
2007 23(2):e237-e244
Fei Sha and Fernando Pereira. 2003. Shallow Pars-
ing with Conditional Random Fields. Proceedings
of HLT-NAACL 2003, Main Papers, pp.134-141
Larry Smith, Lorraine K Tanabe, et al. 2008.
Overview of BioCreative II gene mention recogni-
tion. Genome Biology 2008, 9(Suppl 2):S2.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive Segmentation and Labeling of Distant Entities
in Information Extraction. ICML workshop on Sta-
tistical Relational Learning, 2004.
Lorraine Tanabe, Natalie Xie, Lynne H Thom, Wayne
Matten and W John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recog-
nition . BMC Bioinformatics 2005, 6(Suppl 1):S3
Serhan Tatar and Ilyas Cicekli. 2009. Two learning
approaches for protein name extraction. Journal of
Biomedical Informatics 42(2009) 1046-1055
Xinglong Wang, Junichi Tsujii and Sophia Anani-
adou. 2010. Disambiguating the species of biomed-
ical named entities using natural language parsers.
Bioinformatics 2010 26(5):661-667
Zhihao Yang, Hongfei Lin and Yanpeng Li. 2008. Ex-
ploiting the performance of dictionary-based bio-
entity name recognition in biomedical literature.
Computational Biology and Chemistry 32(2008)
287-291.
Alexander Yeh, Alexander Morgan, Marc Colosimo
and Lynette Hirschman. 2005. BioCreAtIvE Task
1A: gene mention finding evaluation. BMC Bioin-
formatics 2005, 6(Suppl 1):S2.
GuoDong Zhou, Jie Zhang, Jian Su, Dan Shen,
ChewLim Tan. 2004. Recognizing names in
biomedical texts: a machine learning approach.
Bioinformatics 2004, Vol.20(7),pp.1178C1190.
GuoDong Zhou, Dan Shen, Jie Zhang, Jian Su1 and
SoonHeng Tan. 2005. Recognition of protein/gene
names from text using an ensemble of classifiers.
BMC Bioinformatics 2005, 6(Suppl 1):S7.
</reference>
<page confidence="0.876403">
18
</page>
<figure confidence="0.330854">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.278349">
<note confidence="0.920401333333333">b&amp;apos;Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 1018, Uppsala, Sweden, 15 July 2010. c 2010 Association for Computational Linguistics</note>
<title confidence="0.8778875">Recognizing Biomedical Named Entities using Skip-chain Conditional Random Fields</title>
<author confidence="0.8343335">Jingchen Liu Minlie Huang Xiaoyan Zhu</author>
<affiliation confidence="0.999952">Department of Computer Science and Technology</affiliation>
<address confidence="0.844924">Tsinghua University, Beijing 100084, China</address>
<email confidence="0.863976">liu-jc04@mails.tsinghua.edu.cn{aihuang,zxy-dcs}@tsinghua.edu.cn</email>
<abstract confidence="0.998531416666667">Linear-chain Conditional Random Fields (CRF) has been applied to perform the Named Entity Recognition (NER) task in many biomedical text mining and information extraction systems. However, the linear-chain CRF cannot capture long distance dependency, which is very common in the biomedical literature. In this paper, we propose a novel study of capturing such long distance dependency by defining two principles of constructing skipedges for a skip-chain CRF: linking similar words and linking words having typed dependencies. The approach is applied to recognize gene/protein mentions in the literature. When tested on the BioCreAtIvE II Gene Mention dataset and GENIA corpus, the approach contributes significant improvements over the linear-chain CRF. We also present in-depth error analysis on inconsistent labeling and study the influence of the quality of skip edges on the labeling performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences Shilin Ding</author>
<author>Gao Cong</author>
<author>Chin-Yew Lin</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Using Conditional Random Fields to Extract Contexts and Answers of Questions from Online Forums.</title>
<date>2008</date>
<booktitle>In Proceedings of 46th Annual Meeting of the Association for Computational Linguistics (ACL08),</booktitle>
<pages>710--718</pages>
<contexts>
<context position="8770" citStr="Ding et al., 2008" startWordPosition="1371" endWordPosition="1374">nd two SVM models 11 \x0cto enhance the recall. Finkel et al. (2005) used Gibbs Sampling to add non-local dependencies into linear-chain CRF model for information extraction. However, the CRF models used in these systems were all linear-chain CRFs. To the best of our knowledge, no previous work has been done on using non-linear-chain CRF in the biomedical NER task. Beyond the biomedical domain, skip-chain CRF has been used in several studies to model long distance dependency. In (Galley, 2006), skip edges were linked between sentences with nonlocal pragmatic dependencies to rank meetings. In (Ding et al., 2008), skip-chain CRF was used to detect the context and answers from online forums. The most close work to ours was in (Sutton and McCallum, 2004), which used skip-chain CRF to extract information from email messages announcing seminars. By linking the same words whose initial letter is capital, the method obtained improvements on extracting speakers name. Our work is in the spirit of this idea, but we approach it in a different way. We found that the problem is much more difficult in the biomedical NER task: that is why we systematically studied the principles of linking skip edges and the qualit</context>
</contexts>
<marker>Ding, Cong, Lin, Zhu, 2008</marker>
<rawString>\x0cReferences Shilin Ding, Gao Cong, Chin-Yew Lin and Xiaoyan Zhu. 2008. Using Conditional Random Fields to Extract Contexts and Answers of Questions from Online Forums. In Proceedings of 46th Annual Meeting of the Association for Computational Linguistics (ACL08), pp 710-718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="8220" citStr="Finkel et al. (2005)" startWordPosition="1284" endWordPosition="1287">mentions from biomedical text (McDonald and Pereira, 2005). The Gene Mention Recognition task was included in both BioCreAtIvE I and BioCreAtIvE II challenges. CRF had been used in most of top performing systems in the Gene Mention Recognition task of BioCreAtIvE II (Smith et al., 2008). Some novel use of linear-chain CRF was proposed. For example, in (Kuo et al., 2007) labeling was performed in forward and backward directions on the same sentence and results were combined from the two directions. Huang et al. (2007) combines a linear-chain CRF and two SVM models 11 \x0cto enhance the recall. Finkel et al. (2005) used Gibbs Sampling to add non-local dependencies into linear-chain CRF model for information extraction. However, the CRF models used in these systems were all linear-chain CRFs. To the best of our knowledge, no previous work has been done on using non-linear-chain CRF in the biomedical NER task. Beyond the biomedical domain, skip-chain CRF has been used in several studies to model long distance dependency. In (Galley, 2006), skip edges were linked between sentences with nonlocal pragmatic dependencies to rank meetings. In (Ding et al., 2008), skip-chain CRF was used to detect the context an</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of the 43rd Annual Meeting of the ACL, pages 363C370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Fukuda</author>
<author>A Tamura</author>
<author>T Tsunoda</author>
<author>T Takagi</author>
</authors>
<title>Toward information extraction: identifying protein names from biological papers. Pacific Symposium on Biocomputing.</title>
<date>1998</date>
<contexts>
<context position="5973" citStr="Fukuda et al., 1998" startWordPosition="914" endWordPosition="917">many new challenges are seen in domain-specific applications, such as biomedical NER (Zhou et al., 2004). The dictionary based method is a common technique as biomedical thesauruses play a key role in understanding such text. Most dictionary based NER systems focused on: (1) integrating and normalizing different biomedical databases to improve the quality of the dictionary to be used; (2) improving matching strategies that are more suitable for biomedical terminologies; and (3) making filtering rules for postprocessing to refine the matching results or to adjust the boundary of entities, see (Fukuda et al., 1998; Narayanaswamy et al., 2003; Yang et al., 2008). Many information extraction systems had a dictionary matching module to perform preliminary detection of named entities (Schuhmann et al., 2007; Kolarik et al., 2007; Wang et al., 2010). Applying machine learning techniques generally obtains superior performance for the biomedical NER task. The automated learning process can induce patterns for recognizing biomedical names and rules for pre- and post-processing. Generally speaking, there are two categories of machine learning based methods: one treats NER as a classification task, while the oth</context>
</contexts>
<marker>Fukuda, Tamura, Tsunoda, Takagi, 1998</marker>
<rawString>K. Fukuda, A. Tamura, T. Tsunoda and T. Takagi. 1998. Toward information extraction: identifying protein names from biological papers. Pacific Symposium on Biocomputing. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
</authors>
<title>A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance.</title>
<date>2006</date>
<booktitle>Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>364--372</pages>
<contexts>
<context position="8650" citStr="Galley, 2006" startWordPosition="1354" endWordPosition="1355"> same sentence and results were combined from the two directions. Huang et al. (2007) combines a linear-chain CRF and two SVM models 11 \x0cto enhance the recall. Finkel et al. (2005) used Gibbs Sampling to add non-local dependencies into linear-chain CRF model for information extraction. However, the CRF models used in these systems were all linear-chain CRFs. To the best of our knowledge, no previous work has been done on using non-linear-chain CRF in the biomedical NER task. Beyond the biomedical domain, skip-chain CRF has been used in several studies to model long distance dependency. In (Galley, 2006), skip edges were linked between sentences with nonlocal pragmatic dependencies to rank meetings. In (Ding et al., 2008), skip-chain CRF was used to detect the context and answers from online forums. The most close work to ours was in (Sutton and McCallum, 2004), which used skip-chain CRF to extract information from email messages announcing seminars. By linking the same words whose initial letter is capital, the method obtained improvements on extracting speakers name. Our work is in the spirit of this idea, but we approach it in a different way. We found that the problem is much more difficu</context>
</contexts>
<marker>Galley, 2006</marker>
<rawString>Michel Galley. 2006. A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance. Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 364-372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Han-Shen Huang</author>
<author>Yu-Shi Lin</author>
</authors>
<title>Kuan-Ting Lin, ChengJu Kuo, Yu-Ming Chang, Bo-Hou Yang, I-Fang Chung and Chun-Nan Hsu.</title>
<date>2007</date>
<booktitle>Proceedings of the Second BioCreative Challenge Evaluation Workshop,</booktitle>
<pages>109--111</pages>
<marker>Huang, Lin, 2007</marker>
<rawString>Han-Shen Huang, Yu-Shi Lin, Kuan-Ting Lin, ChengJu Kuo, Yu-Ming Chang, Bo-Hou Yang, I-Fang Chung and Chun-Nan Hsu. 2007. High-recall gene mention recognition by unification of multiple backward parsing models. Proceedings of the Second BioCreative Challenge Evaluation Workshop, pages 109-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junichi Kazama</author>
<author>Takaki Makino</author>
<author>Yoshihiro Ohta</author>
<author>Junichi Tsujii</author>
</authors>
<title>Tuning support vector machines for biomedical named entity recognition.</title>
<date>2002</date>
<booktitle>Proceedings of the ACL-02 workshop on Natural language processing in the biomedical domain -</booktitle>
<volume>3</volume>
<contexts>
<context position="6718" citStr="Kazama et al., 2002" startWordPosition="1032" endWordPosition="1035">orm preliminary detection of named entities (Schuhmann et al., 2007; Kolarik et al., 2007; Wang et al., 2010). Applying machine learning techniques generally obtains superior performance for the biomedical NER task. The automated learning process can induce patterns for recognizing biomedical names and rules for pre- and post-processing. Generally speaking, there are two categories of machine learning based methods: one treats NER as a classification task, while the other treats NER as a sequence labeling task. For the first category, Support Vector Machine (SVM) was a commonly adopted model (Kazama et al., 2002; Zhou et al., 2004). Lee et al. (2004) proposed a twostep framework to perform biomedical NER using SVM: firstly detecting the boundaries of named entities using classifiers; secondly classifying each named entity into predefined target types. For the second category, a sentence was treated as a sequence of tokens and the objective was to find the optimal label sequence for these tokens. The label space was often defined as {B,I,O}, where B indicates the beginning token of an entity, I denotes the continuing token and O represents the token outside an entity. The sequence labeling task can be</context>
</contexts>
<marker>Kazama, Makino, Ohta, Tsujii, 2002</marker>
<rawString>Junichi Kazama, Takaki Makino, Yoshihiro Ohta and Junichi Tsujii. 2002. Tuning support vector machines for biomedical named entity recognition. Proceedings of the ACL-02 workshop on Natural language processing in the biomedical domain - Volume 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Kolarik</author>
<author>Martin Hofmann-Apitius</author>
<author>Marc Zimmermann</author>
<author>Juliane Fluck</author>
</authors>
<title>Identification of new drug classification terms in textual resources.</title>
<date>2007</date>
<contexts>
<context position="6188" citStr="Kolarik et al., 2007" startWordPosition="947" endWordPosition="950"> such text. Most dictionary based NER systems focused on: (1) integrating and normalizing different biomedical databases to improve the quality of the dictionary to be used; (2) improving matching strategies that are more suitable for biomedical terminologies; and (3) making filtering rules for postprocessing to refine the matching results or to adjust the boundary of entities, see (Fukuda et al., 1998; Narayanaswamy et al., 2003; Yang et al., 2008). Many information extraction systems had a dictionary matching module to perform preliminary detection of named entities (Schuhmann et al., 2007; Kolarik et al., 2007; Wang et al., 2010). Applying machine learning techniques generally obtains superior performance for the biomedical NER task. The automated learning process can induce patterns for recognizing biomedical names and rules for pre- and post-processing. Generally speaking, there are two categories of machine learning based methods: one treats NER as a classification task, while the other treats NER as a sequence labeling task. For the first category, Support Vector Machine (SVM) was a commonly adopted model (Kazama et al., 2002; Zhou et al., 2004). Lee et al. (2004) proposed a twostep framework t</context>
</contexts>
<marker>Kolarik, Hofmann-Apitius, Zimmermann, Fluck, 2007</marker>
<rawString>Corinna Kolarik, Martin Hofmann-Apitius, Marc Zimmermann and Juliane Fluck. 2007. Identification of new drug classification terms in textual resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bioinformatics</author>
</authors>
<title>23(13):i264-i272 Cheng-Ju Kuo, Yu-Ming Chang, Han-Shen Huang, Kuan-Ting Lin, Bo-Hou Yang, Yu-Shi Lin, ChunNan Hsu and I-Fang Chung.</title>
<date>2007</date>
<booktitle>Proceedings of the Second BioCreative Challenge Evaluation Workshop,</booktitle>
<pages>105--107</pages>
<marker>Bioinformatics, 2007</marker>
<rawString>Bioinformatics 2007 23(13):i264-i272 Cheng-Ju Kuo, Yu-Ming Chang, Han-Shen Huang, Kuan-Ting Lin, Bo-Hou Yang, Yu-Shi Lin, ChunNan Hsu and I-Fang Chung. 2007. Rich Feature Set, Unification of Bidirectional Parsing and Dictionary Filtering for High F-Score Gene Mention Tagging. Proceedings of the Second BioCreative Challenge Evaluation Workshop, pages 105-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ki-Joong Lee</author>
</authors>
<title>Young-Sook Hwang, Seonho Kim and Hae-Chang Rim.</title>
<date>2004</date>
<marker>Lee, 2004</marker>
<rawString>Ki-Joong Lee, Young-Sook Hwang, Seonho Kim and Hae-Chang Rim. 2004. Biomedical named entity recognition using two-phase model based on SVMs.</rawString>
</citation>
<citation valid="true">
<date>2004</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>37</volume>
<pages>436--447</pages>
<contexts>
<context position="6757" citStr="(2004)" startWordPosition="1043" endWordPosition="1043">nn et al., 2007; Kolarik et al., 2007; Wang et al., 2010). Applying machine learning techniques generally obtains superior performance for the biomedical NER task. The automated learning process can induce patterns for recognizing biomedical names and rules for pre- and post-processing. Generally speaking, there are two categories of machine learning based methods: one treats NER as a classification task, while the other treats NER as a sequence labeling task. For the first category, Support Vector Machine (SVM) was a commonly adopted model (Kazama et al., 2002; Zhou et al., 2004). Lee et al. (2004) proposed a twostep framework to perform biomedical NER using SVM: firstly detecting the boundaries of named entities using classifiers; secondly classifying each named entity into predefined target types. For the second category, a sentence was treated as a sequence of tokens and the objective was to find the optimal label sequence for these tokens. The label space was often defined as {B,I,O}, where B indicates the beginning token of an entity, I denotes the continuing token and O represents the token outside an entity. The sequence labeling task can be approached by Hidden Markov Model (HMM</context>
</contexts>
<marker>2004</marker>
<rawString>Journal of Biomedical Informatics, Volume 37, Issue 6, December 2004, Pages 436-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ICML-01,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="2388" citStr="Lafferty et al., 2001" startWordPosition="343" endWordPosition="346">biomedical NER systems, several challenge competitions had been held, such as BioNLP/NLPBA in 20041, BioCreAtIvE I in Corresponding author 1 http://research.nii.ac.jp/ collier/ workshops/JNLPBA04st.htm 2004 and BioCreAtIvE II in 20062. The overview reports from these competitions, presenting stateof-the-art of biomedical NER studies, show that linear-chain Conditional Random Fields (CRF) is one of the most commonly used models and has the most competitive results (Yeh et al., 2005; Smith et al., 2008). Linear-chain CRF has also been successfully applied to other NLP tasks such as POS-tagging (Lafferty et al., 2001) and sentence chunking (Sha and Pereira, 2003). However, in most of these applications, only linear-chain CRF was fully exploited, assuming that only adjacent words are inter-dependent. The dependency between distant words, which occurs frequently in the biomedical literature, is yet to be captured. In the biomedical literature, the repeated appearance of same or similar words in one sentence is a common type of long distance dependencies. This phenomenon is due to the complicated syntactic structures and the various biomedical terminologies in nature. See the following example: Both GH defici</context>
<context position="7517" citStr="Lafferty et al., 2001" startWordPosition="1166" endWordPosition="1169">econdly classifying each named entity into predefined target types. For the second category, a sentence was treated as a sequence of tokens and the objective was to find the optimal label sequence for these tokens. The label space was often defined as {B,I,O}, where B indicates the beginning token of an entity, I denotes the continuing token and O represents the token outside an entity. The sequence labeling task can be approached by Hidden Markov Model (HMM), Conditional Random Field (CRF) , or a combination of different models (Zhou et al., 2005; Tatar and Cicekli, 2009). Since proposed in (Lafferty et al., 2001), CRF has been applied to many sequence labeling tasks, including recognizing gene mentions from biomedical text (McDonald and Pereira, 2005). The Gene Mention Recognition task was included in both BioCreAtIvE I and BioCreAtIvE II challenges. CRF had been used in most of top performing systems in the Gene Mention Recognition task of BioCreAtIvE II (Smith et al., 2008). Some novel use of linear-chain CRF was proposed. For example, in (Kuo et al., 2007) labeling was performed in forward and backward directions on the same sentence and results were combined from the two directions. Huang et al. (</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ICML-01, pages 282-289, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Identifying gene and protein mentions in text using conditional random fields.</title>
<date>2005</date>
<journal>BMC Bioinformatics</journal>
<pages>6--1</pages>
<contexts>
<context position="7658" citStr="McDonald and Pereira, 2005" startWordPosition="1186" endWordPosition="1189">ens and the objective was to find the optimal label sequence for these tokens. The label space was often defined as {B,I,O}, where B indicates the beginning token of an entity, I denotes the continuing token and O represents the token outside an entity. The sequence labeling task can be approached by Hidden Markov Model (HMM), Conditional Random Field (CRF) , or a combination of different models (Zhou et al., 2005; Tatar and Cicekli, 2009). Since proposed in (Lafferty et al., 2001), CRF has been applied to many sequence labeling tasks, including recognizing gene mentions from biomedical text (McDonald and Pereira, 2005). The Gene Mention Recognition task was included in both BioCreAtIvE I and BioCreAtIvE II challenges. CRF had been used in most of top performing systems in the Gene Mention Recognition task of BioCreAtIvE II (Smith et al., 2008). Some novel use of linear-chain CRF was proposed. For example, in (Kuo et al., 2007) labeling was performed in forward and backward directions on the same sentence and results were combined from the two directions. Huang et al. (2007) combines a linear-chain CRF and two SVM models 11 \x0cto enhance the recall. Finkel et al. (2005) used Gibbs Sampling to add non-local </context>
<context position="12887" citStr="McDonald and Pereira, 2005" startWordPosition="2091" endWordPosition="2094"> 1: The illustration of linear-chain CRF and skip-chain CRF. The blue edges represent the linearchain edges belonging to one clique template, while the red edges represent the skip edges belonging to another clique template. skip edges to avoid making the model impractical. Therefore, it is absolutely necessary to study which kinds of edges will contribute to the performance while avoiding over-connected edges. 3.1 Features As our interest is in modifying the CRF graph structure rather than evaluating the effectiveness of features, we simply adopted features from the state-of-the-art such as (McDonald and Pereira, 2005) and (Kuo et al., 2007). Common Features: the original word, the stemmed word, the POS-tag of a word, the word length, is or not the beginning or ending word of the sentence etc. Regular Expression Features: a set of regular expressions to extract orthographic features for the word. Dictionary Features: We use several lexicons. For example, a protein name dictionary compiled from SWISS-PROT, a species dictionary from NCBI Taxonomy, a drug name dictionary from DrugBank database, and a disease name dictionary from several Internet web site. N-gram Features: For each token, we extract the corresp</context>
</contexts>
<marker>McDonald, Pereira, 2005</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2005. Identifying gene and protein mentions in text using conditional random fields. BMC Bioinformatics 2005, 6(Suppl 1):S6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<date>2008</date>
<note>Stanford typed dependencies manual.</note>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. Stanford typed dependencies manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Narayanaswamy</author>
<author>K E Ravikumar</author>
<author>K VijayShanker</author>
</authors>
<title>A biological named entity recognizer. Pacific Symposium on Biocomputing.</title>
<date>2003</date>
<contexts>
<context position="6001" citStr="Narayanaswamy et al., 2003" startWordPosition="918" endWordPosition="921">re seen in domain-specific applications, such as biomedical NER (Zhou et al., 2004). The dictionary based method is a common technique as biomedical thesauruses play a key role in understanding such text. Most dictionary based NER systems focused on: (1) integrating and normalizing different biomedical databases to improve the quality of the dictionary to be used; (2) improving matching strategies that are more suitable for biomedical terminologies; and (3) making filtering rules for postprocessing to refine the matching results or to adjust the boundary of entities, see (Fukuda et al., 1998; Narayanaswamy et al., 2003; Yang et al., 2008). Many information extraction systems had a dictionary matching module to perform preliminary detection of named entities (Schuhmann et al., 2007; Kolarik et al., 2007; Wang et al., 2010). Applying machine learning techniques generally obtains superior performance for the biomedical NER task. The automated learning process can induce patterns for recognizing biomedical names and rules for pre- and post-processing. Generally speaking, there are two categories of machine learning based methods: one treats NER as a classification task, while the other treats NER as a sequence </context>
</contexts>
<marker>Narayanaswamy, Ravikumar, VijayShanker, 2003</marker>
<rawString>M. Narayanaswamy, K.E. Ravikumar and K. VijayShanker. 2003. A biological named entity recognizer. Pacific Symposium on Biocomputing. 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dietrich Rebholz-Schuhmann</author>
<author>Harald Kirsch</author>
<author>Miguel Arregui</author>
<author>Sylvain Gaudan</author>
<author>Mark Riethoven</author>
<author>Peter Stoehr</author>
</authors>
<title>EBIMedtext crunching to gather facts for proteins from Medline. Bioinformatics</title>
<date>2007</date>
<booktitle>Proceedings of HLT-NAACL 2003, Main Papers,</booktitle>
<volume>23</volume>
<issue>2</issue>
<pages>134--141</pages>
<marker>Rebholz-Schuhmann, Kirsch, Arregui, Gaudan, Riethoven, Stoehr, 2007</marker>
<rawString>Dietrich Rebholz-Schuhmann, Harald Kirsch, Miguel Arregui, Sylvain Gaudan, Mark Riethoven and Peter Stoehr. 2007. EBIMedtext crunching to gather facts for proteins from Medline. Bioinformatics 2007 23(2):e237-e244 Fei Sha and Fernando Pereira. 2003. Shallow Parsing with Conditional Random Fields. Proceedings of HLT-NAACL 2003, Main Papers, pp.134-141 Larry Smith, Lorraine K Tanabe, et al. 2008.</rawString>
</citation>
<citation valid="true">
<title>Overview of BioCreative II gene mention recognition.</title>
<date>2008</date>
<journal>Genome Biology</journal>
<pages>9--2</pages>
<marker>2008</marker>
<rawString>Overview of BioCreative II gene mention recognition. Genome Biology 2008, 9(Suppl 2):S2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<date>2004</date>
<booktitle>Collective Segmentation and Labeling of Distant Entities in Information Extraction. ICML workshop on Statistical Relational Learning,</booktitle>
<contexts>
<context position="8912" citStr="Sutton and McCallum, 2004" startWordPosition="1397" endWordPosition="1401">in CRF model for information extraction. However, the CRF models used in these systems were all linear-chain CRFs. To the best of our knowledge, no previous work has been done on using non-linear-chain CRF in the biomedical NER task. Beyond the biomedical domain, skip-chain CRF has been used in several studies to model long distance dependency. In (Galley, 2006), skip edges were linked between sentences with nonlocal pragmatic dependencies to rank meetings. In (Ding et al., 2008), skip-chain CRF was used to detect the context and answers from online forums. The most close work to ours was in (Sutton and McCallum, 2004), which used skip-chain CRF to extract information from email messages announcing seminars. By linking the same words whose initial letter is capital, the method obtained improvements on extracting speakers name. Our work is in the spirit of this idea, but we approach it in a different way. We found that the problem is much more difficult in the biomedical NER task: that is why we systematically studied the principles of linking skip edges and the quality of connected edges. 3 linear-chain and skip-chain CRF Conditional Random Field is a probabilistic graphic model. The model predicts the outp</context>
<context position="13911" citStr="Sutton and McCallum, 2004" startWordPosition="2264" endWordPosition="2268">pecies dictionary from NCBI Taxonomy, a drug name dictionary from DrugBank database, and a disease name dictionary from several Internet web site. N-gram Features: For each token, we extract the corresponding 2-4 grams into the feature set. Each word will include the adjacent words features within {2, 1, 0, 1, 2} offsets. The features used in the linear-chain CRF and skip-chain CRF are all the same in our experiment. 4 Method As the limitations discussed above, detecting the necessary nodes to link should be the first step in constructing a skip-chain CRF. In the speaker name extraction task (Sutton and McCallum, 2004), only identical capitalized words are linked, because there is few variations in the speakers name. However, gene mentions often involve words without obvious orthographic features and such phenomena are common in the biomedical literature such as RGC DNA sequence and multisubunit TFIID protein. If we link all the words like DNA, sequence and protein, the efficiency and performance will drop due to overconnected edges. Therefore, the most important step of detecting gene mentions is to determine which edges should be connected. 4.1 Detect keywords in gene mention We found that many gene menti</context>
<context position="24652" citStr="Sutton and McCallum, 2004" startWordPosition="4017" endWordPosition="4021">near-chain CRF. Both sentences are correctly labeled by the skip-chain CRF. Similar improvements are also made by the skip-chain CRF model linked by typed dependencies. To study labeling consistency, we counted the statistics of inconsistency errors, as shown in Table 3. Two kinds of inconsistency errors were counted: false negatives correctable by consistency (FNCC) and false positives correctable by consistency (FPCC). An FNCC means that a gold-standard mention is missed by the system while its skip edge linked gene mention is correctly labeled, which is similar to the inconsistent miss in (Sutton and McCallum, 2004), as the IE94 in the first example. An FPCC means a non-gene mention is labeled as a gene while its skip edge linked mention (also nongene mention) is not recognized, as TBZ in the second example. These two kinds of inconsistency errors lead to inconsistent false negatives (FN) and false positives (FP). A good model should reduce as much inconsistency errors as possible. The inconsistency errors are reduced substantially as we expected, showing that the reduction of inconsistency errors is one reason for the performance improvements. The skip-chain CRF linked by similar words had better perfor</context>
</contexts>
<marker>Sutton, McCallum, 2004</marker>
<rawString>Charles Sutton and Andrew McCallum. 2004. Collective Segmentation and Labeling of Distant Entities in Information Extraction. ICML workshop on Statistical Relational Learning, 2004.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Lorraine Tanabe</author>
<author>Natalie Xie</author>
<author>Lynne H Thom</author>
<author>Wayne Matten</author>
<author>W John Wilbur</author>
</authors>
<title>GENETAG: a tagged corpus for gene/protein named entity recognition . BMC Bioinformatics 2005, 6(Suppl 1):S3 Serhan Tatar and Ilyas Cicekli.</title>
<date>2005</date>
<journal>Journal of Biomedical Informatics</journal>
<booktitle>Xinglong Wang, Junichi Tsujii and Sophia Ananiadou.</booktitle>
<volume>42</volume>
<issue>2009</issue>
<pages>1046--1055</pages>
<contexts>
<context position="18198" citStr="Tanabe et al., 2005" startWordPosition="2994" endWordPosition="2997">Sp1 and Sp3 can be detected to have a typed dependency of conj and, and the two words have a typed denpendency of prep in addition to with CBF, respectively. The most common type dependencies are conj and, nn and dep. The keywords having typed dependencies will be linked by a skip edge. 5 Experiment We tested our method on two datasets: the Gene Mention (GM) data in BioCreAtIvE II (BCIIGM) 3 http://nlp.stanford.edu/software/ lex-parser.shtml 14 \x0c4and GENIA corpus5. The BCIIGM dataset was used in the BioCreAtIvE II Gene Mention Recognition task in 2006. It was built from the GENETAG corpus (Tanabe et al., 2005) with some modification of the annotation. The dataset contains 15000 sentences for training and 5000 sentences for testing. Two gold-standard sets, GENE and ALTGENE, were provided for evaluation and an official evaluation procedure in Perl script was provided. The ALTGENE set provides alternate forms for genes in the GENE set. In the official evaluation, each identified string will be looked up in both GENE and ALTGENE. If the corresponding gene was found in either GENE or ALTGENE, the identified string will be counted as a correct answer. The GENIA corpus is a widely used dataset in many NER</context>
</contexts>
<marker>Tanabe, Xie, Thom, Matten, Wilbur, 2005</marker>
<rawString>Lorraine Tanabe, Natalie Xie, Lynne H Thom, Wayne Matten and W John Wilbur. 2005. GENETAG: a tagged corpus for gene/protein named entity recognition . BMC Bioinformatics 2005, 6(Suppl 1):S3 Serhan Tatar and Ilyas Cicekli. 2009. Two learning approaches for protein name extraction. Journal of Biomedical Informatics 42(2009) 1046-1055 Xinglong Wang, Junichi Tsujii and Sophia Ananiadou. 2010. Disambiguating the species of biomedical named entities using natural language parsers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bioinformatics</author>
</authors>
<title>26(5):661-667 Zhihao Yang, Hongfei Lin and Yanpeng Li.</title>
<date>2010</date>
<marker>Bioinformatics, 2010</marker>
<rawString>Bioinformatics 2010 26(5):661-667 Zhihao Yang, Hongfei Lin and Yanpeng Li. 2008. Exploiting the performance of dictionary-based bioentity name recognition in biomedical literature.</rawString>
</citation>
<citation valid="false">
<journal>Computational Biology and Chemistry</journal>
<volume>32</volume>
<issue>2008</issue>
<pages>287--291</pages>
<marker></marker>
<rawString>Computational Biology and Chemistry 32(2008) 287-291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
<author>Alexander Morgan</author>
<author>Marc Colosimo</author>
<author>Lynette Hirschman</author>
</authors>
<title>BioCreAtIvE Task 1A: gene mention finding evaluation.</title>
<date>2005</date>
<journal>BMC Bioinformatics</journal>
<pages>6--1</pages>
<contexts>
<context position="2251" citStr="Yeh et al., 2005" startWordPosition="321" endWordPosition="324">f biomedical terminologies and the complicated syntactic structures. Many studies have been devoted to biomedical NER. To evaluate biomedical NER systems, several challenge competitions had been held, such as BioNLP/NLPBA in 20041, BioCreAtIvE I in Corresponding author 1 http://research.nii.ac.jp/ collier/ workshops/JNLPBA04st.htm 2004 and BioCreAtIvE II in 20062. The overview reports from these competitions, presenting stateof-the-art of biomedical NER studies, show that linear-chain Conditional Random Fields (CRF) is one of the most commonly used models and has the most competitive results (Yeh et al., 2005; Smith et al., 2008). Linear-chain CRF has also been successfully applied to other NLP tasks such as POS-tagging (Lafferty et al., 2001) and sentence chunking (Sha and Pereira, 2003). However, in most of these applications, only linear-chain CRF was fully exploited, assuming that only adjacent words are inter-dependent. The dependency between distant words, which occurs frequently in the biomedical literature, is yet to be captured. In the biomedical literature, the repeated appearance of same or similar words in one sentence is a common type of long distance dependencies. This phenomenon is </context>
</contexts>
<marker>Yeh, Morgan, Colosimo, Hirschman, 2005</marker>
<rawString>Alexander Yeh, Alexander Morgan, Marc Colosimo and Lynette Hirschman. 2005. BioCreAtIvE Task 1A: gene mention finding evaluation. BMC Bioinformatics 2005, 6(Suppl 1):S2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Dan Shen</author>
<author>ChewLim Tan</author>
</authors>
<title>Recognizing names in biomedical texts: a machine learning approach.</title>
<date>2004</date>
<contexts>
<context position="5458" citStr="Zhou et al., 2004" startWordPosition="830" endWordPosition="833"> inconsistent labeling. We also investigat whether the quality of connected edges affect the labeling performance. The remainder of this paper is organized as follows: We survey related studies in Section 2. We introduce linear-chain CRF and skip-chain CRF in Section 3. The method of connecting skip-chain edges is described in Section 4 . In Section 5 we present our experiments and in-depth analysis. We summarize our work in Section 6. 2 Related work NER is a widely studied topic in text mining research, and many new challenges are seen in domain-specific applications, such as biomedical NER (Zhou et al., 2004). The dictionary based method is a common technique as biomedical thesauruses play a key role in understanding such text. Most dictionary based NER systems focused on: (1) integrating and normalizing different biomedical databases to improve the quality of the dictionary to be used; (2) improving matching strategies that are more suitable for biomedical terminologies; and (3) making filtering rules for postprocessing to refine the matching results or to adjust the boundary of entities, see (Fukuda et al., 1998; Narayanaswamy et al., 2003; Yang et al., 2008). Many information extraction systems</context>
<context position="6738" citStr="Zhou et al., 2004" startWordPosition="1036" endWordPosition="1039">tion of named entities (Schuhmann et al., 2007; Kolarik et al., 2007; Wang et al., 2010). Applying machine learning techniques generally obtains superior performance for the biomedical NER task. The automated learning process can induce patterns for recognizing biomedical names and rules for pre- and post-processing. Generally speaking, there are two categories of machine learning based methods: one treats NER as a classification task, while the other treats NER as a sequence labeling task. For the first category, Support Vector Machine (SVM) was a commonly adopted model (Kazama et al., 2002; Zhou et al., 2004). Lee et al. (2004) proposed a twostep framework to perform biomedical NER using SVM: firstly detecting the boundaries of named entities using classifiers; secondly classifying each named entity into predefined target types. For the second category, a sentence was treated as a sequence of tokens and the objective was to find the optimal label sequence for these tokens. The label space was often defined as {B,I,O}, where B indicates the beginning token of an entity, I denotes the continuing token and O represents the token outside an entity. The sequence labeling task can be approached by Hidde</context>
</contexts>
<marker>Zhou, Zhang, Su, Shen, Tan, 2004</marker>
<rawString>GuoDong Zhou, Jie Zhang, Jian Su, Dan Shen, ChewLim Tan. 2004. Recognizing names in biomedical texts: a machine learning approach.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bioinformatics</author>
</authors>
<date>2004</date>
<volume>20</volume>
<issue>7</issue>
<marker>Bioinformatics, 2004</marker>
<rawString>Bioinformatics 2004, Vol.20(7),pp.1178C1190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
<author>Dan Shen</author>
</authors>
<title>Jie Zhang, Jian Su1 and SoonHeng Tan.</title>
<date>2005</date>
<marker>Zhou, Shen, 2005</marker>
<rawString>GuoDong Zhou, Dan Shen, Jie Zhang, Jian Su1 and SoonHeng Tan. 2005. Recognition of protein/gene names from text using an ensemble of classifiers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BMC Bioinformatics</author>
</authors>
<date>2005</date>
<pages>6--1</pages>
<marker>Bioinformatics, 2005</marker>
<rawString>BMC Bioinformatics 2005, 6(Suppl 1):S7.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>