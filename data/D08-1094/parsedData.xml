<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<bodyText confidence="0.7986015">
b&apos;Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 897906,
Honolulu, October 2008. c
</bodyText>
<sectionHeader confidence="0.538987" genericHeader="abstract">
2008 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.986744">
A Structured Vector Space Model for Word Meaning in Context
</title>
<author confidence="0.982166">
Katrin Erk
</author>
<affiliation confidence="0.993606">
Department of Linguistics
University of Texas at Austin
</affiliation>
<email confidence="0.995452">
katrin.erk@mail.utexas.edu
</email>
<author confidence="0.965381">
Sebastian Pado
</author>
<affiliation confidence="0.976063">
Department of Linguistics
Stanford University
</affiliation>
<email confidence="0.995533">
pado@stanford.edu
</email>
<sectionHeader confidence="0.990731" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999709">
We address the task of computing vector space
representations for the meaning of word oc-
currences, which can vary widely according to
context. This task is a crucial step towards a
robust, vector-based compositional account of
sentence meaning. We argue that existing mod-
els for this task do not take syntactic structure
sufficiently into account.
We present a novel structured vector space
model that addresses these issues by incorpo-
rating the selectional preferences for words
argument positions. This makes it possible to
integrate syntax into the computation of word
meaning in context. In addition, the model per-
forms at and above the state of the art for mod-
eling the contextual adequacy of paraphrases.
</bodyText>
<sectionHeader confidence="0.998275" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986035461538461">
Semantic spaces are a popular framework for the rep-
resentation of word meaning, encoding the meaning
of lemmas as high-dimensional vectors. In the de-
fault case, the components of these vectors measure
the co-occurrence of the lemma with context features
over a large corpus. These vectors are able to pro-
vide a robust model of semantic similarity that has
been used in NLP (Salton et al., 1975; McCarthy and
Carroll, 2003; Manning et al., 2008) and to model
experimental results in cognitive science (Landauer
and Dumais, 1997; McDonald and Ramscar, 2001).
Semantic spaces are attractive because they provide a
model of word meaning that is independent of dictio-
nary senses and their much-discussed problems (Kil-
garriff, 1997; McCarthy and Navigli, 2007).
In a default semantic space as described above,
each vector represents one lemma, averaging over
all its possible usages (Landauer and Dumais, 1997;
Lund and Burgess, 1996). Since the meaning of
words can vary substantially between occurrences
(e.g., for polysemous words), the next necessary step
is to characterize the meaning of individual words in
context.
There have been several approaches in the liter-
ature (Smolensky, 1990; Schutze, 1998; Kintsch,
2001; McDonald and Brew, 2004; Mitchell and La-
pata, 2008) that compute meaning in context from
lemma vectors. Most of these studies phrase the prob-
lem as one of vector composition: The meaning of a
target occurrence a in context b is a single new vector
c that is a function (for example, the centroid) of the
vectors: c = a \x0c b.
The context b can consist of as little as one word,
as shown in Example (1). In (1a), the meaning of
catch combined with ball is similar to grab, while in
(1b), combined with disease, it can be paraphrased
by contract. Conversely, verbs can influence the in-
terpretation of nouns: In (1a), ball is understood as a
spherical object, and in (1c) as a dancing event.
(1) a. catch a ball
b. catch a disease
c. attend a ball
In this paper, we argue that models of word mean-
ing relying on this procedure of vector composition
are limited both in their scope and scalability. The
underlying shortcoming is a failure to consider syntax
in two important ways.
The syntactic relation is ignored. The first problem
concerns the manner of vector composition, which
ignores the relation between the target a and its con-
text b. This relation can have a decisive influence on
their interpretation, as Example (2) shows:
</bodyText>
<page confidence="0.991245">
897
</page>
<bodyText confidence="0.998168116666667">
\x0c(2) a. a horse draws
b. draw a horse
In (2a), the meaning of the verb draw can be para-
phrased as pull, while in (2b) it is similar to sketch.
This difference in meaning is due to the difference in
relation: in (2a), horse is the subject, while in (2b)
it is the object. On the modeling side, however, a
vector combination function that ignores the relation
will assign the same representation to (2a) and (2b).
Thus, existing models are systematically unable to
capture this class of phenomena.
Single vectors are too weak to represent phrases.
The second problem arises in the context of the im-
portant open question of how semantic spaces can
scale up to provide interesting meaning representa-
tions for entire sentences. We believe that the current
vector composition methods, which result in a single
vector c, are not informative enough for this purpose.
One proposal for scaling up is to straightforwardly
interpret c = a \x0c b as the meaning of the phrase
a + b (Kintsch, 2001; Mitchell and Lapata, 2008).
The problem is that the vector c can only encode a
fixed amount of structural information if its dimen-
sionality is fixed, but there is no upper limit on sen-
tence length, and hence on the amount of structure
to be encoded. It is difficult to conceive how c could
encode deeper semantic properties, like predicate-
argument structure (distinguishing dog bites man
and man bites dog), that are crucial for sentence-
level semantic tasks such as the recognition of textual
entailment (Dagan et al., 2006). An alternative ap-
proach to sentence meaning would be to use the vec-
tor space representation only for representing word
meaning, and to represent sentence structure sepa-
rately. Unfortunately, present models cannot provide
this grounding either, since they compute a single
vector c that provides the same representations for
both the meanings of a and b in context.
In this paper, we propose a new, structured vector
space model for word meaning (SVS) that addresses
these problems. A SVS representation of a lemma
comprises several vectors representing the words
lexical meaning as well as the selectional preferences
that it has for its argument positions. The meaning
of word a in context b is computed by combining a
with bs selectional preference vector specific to the
relation between a and b, addressing the first problem
above. In an expression a + b, the meanings of a
and b in this context are computed as two separate
vectors a0 and b0. These vectors can then be combined
with a representation of the structures expression
(e.g., a parse tree), to address the second problem
discussed above. We test the SVS model on the task
of recognizing contextually appropriate paraphrases,
finding that SVS performs at and above the state-of-
the-art.
Plan of the paper. Section 2 reviews related work.
Section 3 presents the SVS model for word meaning
in context. Sections 4 to 6 relate experiments on the
paraphrase appropriateness task.
</bodyText>
<sectionHeader confidence="0.999622" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997986066666667">
In this section we give a short overview over existing
vector space based approaches to computing word
meaning in context.
General context effects. The first category of
models aims at integrating the widest possible range
of context information without recourse to linguistic
structure. The best-known work in this category is
Schutze (1998). He first computes first-order vec-
tor representations for word meaning by collecting
co-occurrence counts from the entire corpus. Then,
he determines second-order vectors for individual
word instances in their context, which is taken to be a
simple surface window, by summing up all first-order
vectors of the words in this context. The resulting
vectors form sense clusters.
McDonald and Brew (2004) present a similar
model. They compute the expectation for a word
wi in a sequence by summing the first-order vectors
for the words w1 to wi1 and showed that the dis-
tance between expectation and first-order vector for
wi correlates with human reading times.
Predicate-argument combination. The second
category of prior studies concentrates on contexts
consisting of a single word only, typically modeling
the combination of a predicate p and an argument a.
Kintsch (2001) uses vector representations of p and
a to identify the set of words that are similar to both
p and a. After this set has been narrowed down in a
self-inhibitory network, the meaning of the predicate-
argument combination is obtained by computing the
</bodyText>
<page confidence="0.993979">
898
</page>
<bodyText confidence="0.9978656">
\x0ccentroid of its members vectors. The procedure does
not take the relation between p and a into account.
Mitchell and Lapata (2008) propose a framework
to represent the meaning of the combination p + a as
a function f operating on four components:
</bodyText>
<equation confidence="0.655475">
c = f(p, a, R, K) (3)
</equation>
<bodyText confidence="0.998140913043478">
R is the relation holding between p and a, and K
additional knowledge. This framework allows sen-
sitivity to the relation. However, the concrete in-
stantiations that Mitchell and Lapata consider disre-
gards K and R, thus sharing the other models limi-
tations. They focus instead on methods for the direct
combination of p and a: In a comparison between
component-wise addition and multiplication of p and
a, they find far superior results for the multiplication
approach.
Tensor product-based models. Smolensky (1990)
uses tensor product to combine two word vectors a
and b into a vector c representing the expression a+b.
The vector c is located in a very high-dimensional
space and is thus capable of encoding the structure
of the expression; however, this makes the model
infeasible in practice, as dimensionality rises with
every word added to the representation. Jones and
Mewhort (2007) represent lemma meaning by using
circular convolution to encode n-gram co-occurrence
information into vectors of fixed dimensionality. Sim-
ilar to Brew and McDonald (2004), they predict most
likely next words in a sequence, without taking syn-
tax into account.
Kernel methods. One of the main tests for the
quality of models of word meaning in context is the
ability to predict the appropriateness of paraphrases
in given a context. Typically, a paraphrase applies
only to some senses of a word, not all, as can be seen
in the paraphrases grab and contract of catch.
Vector space models generally predict paraphrase ap-
propriateness based on the similarity between vectors.
This task can also be addressed with kernel methods,
which project items into an implicit feature space
for efficient similarity computation. Consequently,
vector space methods and kernel methods have both
been used for NLP tasks based on similarity, no-
tably Information Retrieval and Textual Entailment.
Nevertheless, they place their emphasis on different
types of information. Current kernels are mostly tree
kernels that compare syntactic structure, and use se-
mantic information mostly for smoothing syntactic
similarity (Moschitti and Quarteroni, 2008). In con-
trast, vector-space models focus on the interaction
between the lexical meaning of words in composi-
tion.
</bodyText>
<sectionHeader confidence="0.626577" genericHeader="method">
3 A structured vector space model for
</sectionHeader>
<bodyText confidence="0.994133789473684">
word meaning in context
In this section, we define the structured vector space
(SVS) model of word meaning.
The main intuition behind our model is to view
the interpretation of a word in context as guided by
expectations about typical events. For example, in
(1a), we assume that upon hearing the phrase catch a
ball, the hearer will interpret the meaning of catch
to match typical actions that can be performed with a
ball. Similarly, the interpretation of ball will reflect
the hearers expectations about typical things that can
be caught. This move to include typical arguments
and predicates into a model of word meaning can be
motivated both on cognitive and linguistic grounds.
In cognitive science, the central role of expecta-
tions about typical events for human language pro-
cessing is well-established. Expectations affect read-
ing times (McRae et al., 1998), the interpretation of
participles (Ferretti et al., 2003), and sentence pro-
cessing generally (Narayanan and Jurafsky, 2002;
Pado et al., 2006). Expectations exist both for verbs
and nouns (McRae et al., 1998; McRae et al., 2005).
In linguistics, expectations, in the form of selec-
tional restrictions and selectional preferences, have
long been used in semantic theories (Katz and Fodor,
1964; Wilks, 1975), and more recently induced
from corpora (Resnik, 1996; Brockmann and Lapata,
2003). Attention has mostly been limited to selec-
tional preferences of verbs, which have been used
for example for syntactic disambiguation (Hindle
and Rooth, 1993), word sense disambiguation (Mc-
Carthy and Carroll, 2003) and semantic role label-
ing (Gildea and Jurafsky, 2002). Recently, a vector-
spaced model of selectional preferences has been
proposed that computes the typicality of an argument
simply through similarity to previously seen argu-
ments (Erk, 2007; Pado et al., 2007).
We first present the SVS model of word meaning
</bodyText>
<page confidence="0.972948">
899
</page>
<figure confidence="0.995223269230769">
\x0ccatch
he
fielder
dog
cold
baseball
drift
obj
subj
accuse
say
claim
comp-1
ball
whirl
fly
provide
throw
catch
organise
obj-1
subj-1
mod
red
golf
elegant
</figure>
<figureCaption confidence="0.99996">
Figure 1: Structured meaning representations for noun
</figureCaption>
<bodyText confidence="0.99944244117647">
ball and verb catch: lexical information plus expectations
that integrates lexical information with selectional
preferences. Then, we show how the SVS model pro-
vides a new way of computing meaning in context.
Representing lemma meaning. We abandon the
traditional choice of representing word meaning as
a single vector. Instead, we encode each word as
a combination of (a) one vector that models the
lexical meaning of the word, and (b) a set of vec-
tors, each of which represents the semantic expecta-
tions/selectional preferences for one particular rela-
tion that the word supports.1
The idea is illustrated in Fig. 1. In the representa-
tion of the verb catch, the central square stands for
the lexical vector of catch itself. The three arrows
link it to catchs preferences for its subjects (subj),
its objects (obj), and for verbs for which it appears
as a complement (comp1). The figure shows the se-
lectional preferences as word lists for readability; in
practice, each selectional preference is a single vector
(cf. Section 4). Likewise, ball is represented by one
vector for ball itself, one for balls preferences for its
modifiers (mod), one vector for the verbs of which it
is a subject (subj1), and one for the verbs of which
is an object (obj1).
This representation includes selectional prefer-
ences (like subj, obj, mod) exactly parallel to
inverse selectional preferences (subj1, obj1,
comp1). To our knowledge, preferences of the lat-
ter kind have not been studied in computational lin-
guistics. However, their existence is supported in
psycholinguistics by priming effects from nouns to
typical verbs (McRae et al., 2005).
Formally, let D be a vector space (the set of possi-
</bodyText>
<page confidence="0.777426">
1
</page>
<bodyText confidence="0.8348105">
We do not commit to a particular set of relations; see the
discussion at the end of this section.
</bodyText>
<figure confidence="0.996256636363636">
catch
...
cold
baseball
drift
obj
subj
...
comp
-1
ball
...
throw
catch
organise
obj
-1 subj
-1
mod
...
!
!
</figure>
<figureCaption confidence="0.779673">
Figure 2: Combining predicate and argument via relation-
specific semantic expectations
</figureCaption>
<bodyText confidence="0.967938666666667">
ble vectors), and let R be some set of relation labels.
In the structured vector space (SVS) model, we rep-
resent the meaning of a lemma w as a triple
</bodyText>
<equation confidence="0.938403">
w = (v, R, R1
)
</equation>
<bodyText confidence="0.98725685">
where v D is a lexical vector describing the word
w itself, R : R D maps each relation label onto
a vector that describes ws selectional preferences,
and R1 : R D maps from role labels to vec-
tors describing inverse selectional preferences of w.
Both R and R1 are partial functions. For example,
the direct object preference would be undefined for
intransitive verbs.
Computing meaning in context. The SVS model
of lemma meaning permits us to compute the mean-
ing of a word a in the context of another word b
in a new way, via their selectional preferences. Let
(va, Ra, R1
a ) and (vb, Rb, R1
b ) be the representa-
tions of the two words, and let r R be the relation
linking a to b. Then, we define the meaning of a and
b in this context as a pair (a0, b0) of vectors, where
a0 is the meaning of a in the context of b, and b0 the
meaning of b in the context of a:
</bodyText>
<equation confidence="0.999690714285714">
a0 = va \x0c R1
b (r), Ra {r}, R1
a
\x01
b0 = vb \x0c Ra(r), Rb, R1
b {r}
\x01 (4)
</equation>
<bodyText confidence="0.809671625">
where v1 \x0cv2 is a direct vector combination function
as in traditional models, e.g. addition or component-
wise multiplication. If either Ra(r) or R1
b (r) are
not defined, the combination fails. Afterwards, the ar-
gument position r is considered filled, and is deleted
from Ra and R1
b .
</bodyText>
<page confidence="0.858024">
900
</page>
<bodyText confidence="0.999092756756757">
\x0cFigure 2 illustrates this procedure on the represen-
tations from Figure 1. The dotted lines indicate that
the lexical vector for catch is combined with the in-
verse object preference of ball. Likewise, the lexical
vector for ball is combined with the object preference
vector of catch.
Note that our procedure for computing meaning
in context can be expressed within the framework of
Mitchell and Lapata (Eq. (3)). We can encode the
expectations of a and b as additional knowledge K.
The combined representation c is the pair (a0, b0) that
is computed according to our model (Eq. (4)).
The SVS scheme we have proposed incorporates
syntactic information in a more general manner than
previous models, and thus addresses the issues we
have discussed in Section 1. Since the representation
retains individual selectional preferences for all rela-
tions, combining the same words through different
relations can (and will in general) result in different
adapted representations. For instance, in the case of
Example (2), we would expect the inverse subject
preference of horse (things that a horse typically
does) to push the lexical vector of draw into the di-
rection of pulling, while its inverse object preference
(things that are done to horses) suggest a different
interpretation.
Rather than yielding a single, joint vector for the
whole expression, our procedure for computing mean-
ing in context results in one context-adapted meaning
representation per word, similar to the output of a
WSD system. As a consequence, our model can
be combined with any formalism representing the
structure of an expression. (The formalism used then
determines the set R of relations.) For example, com-
bining SVS with a dependency tree would yield a tree
in which each node is labeled by a SVS tuple that
represents the words meaning in context.
</bodyText>
<sectionHeader confidence="0.99565" genericHeader="evaluation">
4 Experimental setup
</sectionHeader>
<bodyText confidence="0.99371975">
This section provides the background to the following
experimental evaluation of SVS, including parameters
used for computing the SVS representations that will
be used in the experiments.
</bodyText>
<subsectionHeader confidence="0.993894">
4.1 Experimental rationale
</subsectionHeader>
<bodyText confidence="0.999481375">
In this paper, we evaluate the SVS model against the
task of predicting, given a predicate-argument pair,
how appropriate a paraphrase (of either the predicate
or the argument) is in that context. We perform two
experiments that both use the paraphrase task, but
differ in their emphasis. Experiment 1 replicates an
existing evaluation against human judgments. This
evaluation uses synthetic dataset, limited to one par-
ticular construction, and constructed to provide max-
imally distinct paraphrase candidates. Experiment 2
considers a broader class of constructions along with
annotator-generated paraphrase candidates that are
not screened for distinctness. In both experiments,
we compare the SVS model against the state-of-the-
art model by Mitchell and Lapata 2008 (henceforth
M&amp;L; cf. Sec. 2 for model details).
</bodyText>
<subsectionHeader confidence="0.987846">
4.2 Parameter choices
</subsectionHeader>
<bodyText confidence="0.9962082">
Vector space. In our parameterization of the vector
space, we largely follow M&amp;L because their model
has been rigorously evaluated and found to outper-
form a range of other models.
Our first space is a traditional bag-of-words vec-
tor space (BOW, (Lund and Burgess, 1996)). For
each pair of a target word and context word, the BOW
space records a function of their co-occurrence fre-
quency within a surface window of size 10. The
space is constructed from the British National Cor-
pus (BNC), and uses the 2,000 most frequent context
words as dimensions.
We also consider a dependency-based vector
space (SYN, (Pado and Lapata, 2007)). In this space,
target and context words have to be linked by a valid
dependency path in a dependency graph to count as
co-occurring.2 This space was built from BNC de-
pendency parses obtained from Minipar (Lin, 1993).
For both spaces, we used pre-experiments to com-
pare two methods for the computation of vector com-
ponents, namely raw co-occurrence counts, the stan-
dard model, and the pointwise mutual information
(PMI) definition employed by M&amp;L.
Selectional preferences. We use a simple,
knowledge-lean representation for selectional
preferences inspired by Erk (2007), who models
selectional preference through similarity to seen filler
vectors ~
va: We compute the selectional preference
vector for word b and relation r as the weighted
</bodyText>
<page confidence="0.933142">
2
</page>
<bodyText confidence="0.9946065">
More specifically, we used the minimal context specification
and plain weight function. See Pado and Lapata (2007).
</bodyText>
<page confidence="0.984965">
901
</page>
<bodyText confidence="0.9881238">
\x0ccentroid of seen filler vectors ~
va. We collect seen
fillers from the Minipar-parse of the BNC.
Let f(a, r, b) denote the frequency of a occurring
in relation r to b in the parsed BNC, then
</bodyText>
<equation confidence="0.9993342">
Rb(r)SELPREF =
X
a:f(a,r,b)&gt;0
f(a, r, b) ~
va (5)
</equation>
<bodyText confidence="0.998524">
We call this base model SELPREF. We will also
study two variants of SELPREF, based on two dif-
ferent hypotheses about what properties of the se-
lectional preferences are particularly important for
meaning adaption. The first model aims specifically
at alleviating noise introduced by infrequent fillers, a
common problem in data-driven approaches. It only
uses fillers seen more often than a threshold . We
call this model SELPREF-CUT:
</bodyText>
<equation confidence="0.9997294">
Rb(r)SELPREF-CUT =
X
a:f(a,r,b)&gt;
f(a, r, b) ~
va (6)
</equation>
<bodyText confidence="0.99719025">
Our second variant again aims at alleviating noise,
but noise introduced by low-valued dimensions rather
than infrequent fillers. It achieves this by taking each
component of the selectional preference vector to
the nth power. In this manner, dimensions with high
counts are further inflated, while dimensions with low
counts are depressed.3 This model, SELPREF-POW, is
defined as follows: If Rb(r)SELPREF = hv1, . . . , vmi,
</bodyText>
<equation confidence="0.999703666666667">
Rb(r)SELPREF-POW = hvn
1 , . . . , vn
mi (7)
</equation>
<bodyText confidence="0.990433928571429">
The inverse selectional preferences R1
b are de-
fined analogously for all three model variants. We
instantiate the vector combination function \x0c as
component-wise multiplication, following M&amp;L.
Baselines and significance testing. All tasks that
we consider below involve judgments for the mean-
ing of a word a in the context of a word b. A first
baseline that every model must beat is simply using
the original vector for a. We call this baseline target
only. Since we assume that the selectional prefer-
ences of b model the expectations for a, we use bs
selectional preference vector for the given relation as
a second baseline, selpref only.
</bodyText>
<page confidence="0.987303">
3
</page>
<bodyText confidence="0.689039571428571">
Since we focus on the size-invariant cosine similarity, the
use of this model does not require normalization.
verb subject landmark sim judgment
slump shoulder slouch high 7
slump shoulder decline low 2
slump value slouch low 3
slump value decline high 7
</bodyText>
<figureCaption confidence="0.900589">
Figure 3: Experiment 1: Human similarity judgements for
</figureCaption>
<bodyText confidence="0.9969925">
subject-verb pair with high- and low-similarity landmarks
Differences between the performance of mod-
els were tested for significance using a stratified
shuffling-based randomization test (Yeh, 2000).4.
5 Exp. 1: Predicting similarity ratings
In our first experiment, we attempt to predict human
similarity judgments. This experiment is a replication
of the evaluation of M&amp;L on their dataset5.
Dataset. The M&amp;L dataset comprises a total of
3,600 human similarity judgements for 120 experi-
mental items. Each item, as shown in Figure 3, con-
sists of an intransitive verb and a subject noun that
are combined with a landmark, a synonym of the
verb that is chosen to be either similar or dissimilar
to the verb in the context of the given subject.
The dataset was constructed by extracting pairs
of subjects and intransitive verbs from a parsed ver-
sion of the BNC. Each item was paired with two
landmarks, chosen to be as dissimilar as possible ac-
cording to a WordNet similarity measure. All nouns
and verbs were subjected to a pretest, where only
those with highly significant variations in human
judgments across landmarks were retained.
For each item of the final dataset, judgements on
a 7-point scale were elicited. For example, judges
considered the compatible landmark slouch to be
much more similar to shoulder slumps than the
incompatible landmark decline. In Figure 3, the
column sim shows whether the experiment designers
considered the respective landmark to have high or
low similarity to the verb, and the column judgment
shows a participants judgments.
Experimental procedure. We used cosine to com-
pute similarity to the lexical vector of the landmark.
</bodyText>
<page confidence="0.978617">
4
</page>
<bodyText confidence="0.9684295">
The software is available at http://www.nlpado.de/
sebastian/sigf.html.
</bodyText>
<page confidence="0.962805">
5
</page>
<bodyText confidence="0.998654">
We thank J. Mitchell and M. Lapata for providing their data.
</bodyText>
<page confidence="0.995954">
902
</page>
<table confidence="0.998221529411765">
\x0cModel high low
BOW space
Target only 0.32 0.32 0.0
Selpref only 0.46 0.4 0.06**
M&amp;L 0.25 0.15 0.20**
SELPREF 0.32 0.26 0.12**
SELPREF-CUT, =10 0.31 0.24 0.11**
SELPREF-POW, n=20 0.11 0.03 0.27**
Upper bound 0.4
SYN space
Target only 0.2 0.2 0.08**
Selpref only 0.27 0.21 0.16**
M&amp;L 0.13 0.06 0.24**
SELPREF 0.22 0.16 0.13**
SELPREF-CUT, =10 0.2 0.13 0.13**
SELPREF-POW, n=30 0.08 0.04 0.22**
Upper bound 0.4
</table>
<tableCaption confidence="0.997516">
Table 1: Experiment 1: Mean cosine similarity for items
</tableCaption>
<bodyText confidence="0.962974142857143">
with high- and low-similarity landmarks; correlation with
human judgements (). (**: p &lt; 0.01)
Target only compares the landmark against the lexi-
cal vector of the verb, and selpref only compares
it to the nouns subj1 preference. For the M&amp;L
model, the comparison is to the combined lexical
vectors of verb and noun. For our models SELPREF,
SELPREF-CUT and SELPREF-POW, we combine the
verbs lexical vector with the subj1 preference of
the noun. We used a held-out dataset of 10% of the
data to optimize the parameters of of SELPREF-CUT
and n of SELPREF-POW. Vectors with PMI compo-
nents could model the data, while raw frequency
components could not; we report only the former.
We use the same two evaluation scores as M&amp;L:
The first score is the average similarity to compatible
landmarks (high) and incompatible landmarks (low).
The second is Spearmans , a nonparametric corre-
lation coefficient. We compute between individual
human similarity scores and our predictions. Based
on agreement between human judges, M&amp;L estimate
an upper bound of 0.4 for the dataset.
Results and discussion. Table 1 shows the results
of Exp. 1 on the test set. In the upper half (BOW), we
replicate M&amp;Ls main finding that simple component-
wise multiplication of the predicate and argument
vectors results in a highly significant correlation of
Model lex. vector obj1 selpref
</bodyText>
<table confidence="0.99808">
SELPREF 0.23 (0.09) 0.88 (0.07)
SELPREF-CUT (10) 0.20 (0.10) 0.72 (0.18)
SELPREF-POW (30) 0.03 (0.08) 0.52 (0.48)
</table>
<tableCaption confidence="0.990415">
Table 2: Experiment 1: Average similarity (and standard
</tableCaption>
<bodyText confidence="0.999583487804878">
deviation) between the inverse subject preferences of a
noun and (left) its lexical vector and (right) inverse object
preferences vector (cosine similarity in SYN space)
= 0.2, significantly outperforming both baselines.
It is interesting, though, that the subj1 preference
itself (Selpref only) is already highly significantly
correlated with the human judgments.
A comparison of the upper half (BOW) with the
lower half (SYN) shows that the dependency-based
space generally shows better correlation with human
judgements. This corresponds to a beneficial effect of
syntactic information found for other applications of
semantic spaces (Lin, 1998; Pado and Lapata, 2007).
All instances of the SELPREF model show highly
significant correlations. SELPREF and SELPREF-CUT
show very similar performance. They do better than
both baselines in the BOW space; however, in the
cleaner SYN space, their performance is numerically
lower than using selectional preferences only ( =
0.13 vs. 0.16). SELPREF-POW is always significantly
better than SELPREF and SELPREF-CUT, and shows
the best result of all tested models ( = 0.27, BOW
space). The performance is somewhat lower in the
SYN space ( = 0.22). However, this difference, and
the difference to the best M&amp;L model at = 0.24,
are not statistically significant.
The SVS model computes meaning in context by
combining a words lexical representation with the
preference vector of its context. In this, it differs from
previous models, including that by M&amp;L, which used
what we have been calling direct combination. So
it is important to ask to what extent this difference
in method translate to a difference in predictions.
We analyzed this by measuring the similarity by the
nouns lexical vectors, used by direct combination
methods, and their inverse subject preferences, which
SVS uses. The result is shown in the first column
in Table 2, computed as mean cosine similarities
and standard deviations between noun vectors and
selectional preferences. The table shows that these
vectors have generally low similarity, which is further
</bodyText>
<page confidence="0.990641">
903
</page>
<bodyText confidence="0.992411088888889">
\x0creduced by applying cutoff and potentiation. Thus,
the predictions of SVS will differ from those of direct
combination models like M&amp;L.
A related question is whether syntax-aware vec-
tor combination makes a difference: Does the model
encode different expectations for different syntactic
relations (cf. Example 2)? The second column of Ta-
ble 2 explores this question by comparing inverse se-
lectional preferences for the subject and object slots.
We observe that the similarity is very high for raw
preferences, but becomes lower when noise is elim-
inated. Since the SELPREF-POW model performed
best in our evaluation, we read this as evidence that
potentiation helps to suppress noise introduced by
mis-identified subject and object fillers.
In Experiment 1, all experimental items were
verbs, which means that all disambiguation was done
through inverse selectional preferences. As inverse
selectional preferences are currently largely unex-
plored, it is interesting to note that the evidence that
they provide for the paraphrase task is as strong as
that of the context nouns themselves.
6 Exp. 2: Ranking paraphrases
This section reports on a second, more NLP-oriented
experiment whose task is to distinguish between ap-
propriate and inappropriate paraphrases on a broader
range of constructions.
Dataset. For this experiment, we use the SemEval-
1 lexical substitution (lexsub) dataset (McCarthy and
Navigli, 2007), which contains 10 instances each of
200 target words in sentential contexts, drawn from
Sharoffs (2006) English Internet Corpus. Contex-
tually appropriate paraphrases for each instance of
each target word were elicited from up to 6 partic-
ipants. Fig. 4 shows two instances for the verb to
work. The distribution over paraphrases can be seen
as a characterization of the target words meaning in
each context.
Experimental procedure. In this paper, we pre-
dict appropriate paraphrases solely on the basis of a
single context word that stands in a direct predicate-
argument relation to the target word. We extracted
all instances from the lexsub test data with such a
relation. After parsing all sentences with verbal and
nominal targets with Minipar, this resulted in three
</bodyText>
<subsectionHeader confidence="0.913592">
Sentence Substitutes
</subsectionHeader>
<bodyText confidence="0.893918571428571">
By asking people who work
there, I have since determined
that he didnt. (# 2002)
be employed 4;
labour 1
Remember how hard your an-
cestors worked. (# 2005)
</bodyText>
<figureCaption confidence="0.861036">
toil 4; labour 3;
task 1
Figure 4: Lexical substitution example items for work
</figureCaption>
<bodyText confidence="0.999218878787879">
sets of sentences: (a), target intransitive verbs with
noun subjects (V-SUBJ, 48 sentences); (b), target tran-
sitive verbs with noun objects (V-OBJ, 213 sent.); and
(c), target nouns occurring as objects of verbs (N-OBJ,
102 sent.).6 Note that since we use only part of the
lexical substitution dataset in this experiment, a di-
rect comparison with results from the SemEval task
is not possible.
As in the original SemEval task, we phrase the
task as a ranking problem. For each target word, the
paraphrases given for all 10 instances are pooled. The
task is to rank the list for each item so that appropriate
paraphrases (such as be employed for # 2002) rank
higher than paraphrases not given (e.g., toil).
Our model ranks paraphrases by their similarity
to the following combinations (Eq. (4)): for V-SUBJ,
verb plus the nouns subj1 preferences; for V-OBJ,
verb plus the nouns obj1 preferences; and for N-
OBJ, the noun plus the verbs obj preferences. Our
comparison model, M&amp;L, ranks all paraphrases by
their similarity to the direct noun-verb combination.
To avoid overfitting, we consider only the two mod-
els that performed optimally in in the SYN space in
Experiment 1 (SELPREF-POW with n=30 and M&amp;L).
However, since we found that vectors with raw fre-
quency components could model the data, while PMI
components could not, we only report the former.
For evaluation, we adopt the SemEval out of
ten precision metric POOT. It uses the models ten
top-ranked paraphrases as its guesses for appropri-
ate paraphrases. Let Gi be the gold paraphrases for
item i, Mi the models top ten paraphrases for i, and
f(s, i) the frequency of s as paraphrase for i:
</bodyText>
<equation confidence="0.677033">
POOT = 1/|I|
X
i
P
sMiGi
f(s, i)
P
sGi
f(s, i)
(8)
</equation>
<bodyText confidence="0.385499">
McCarthy and Navigli propose this metric for the
</bodyText>
<page confidence="0.722493">
6
</page>
<bodyText confidence="0.753787">
The specification of this dataset will be made available.
</bodyText>
<page confidence="0.993295">
904
</page>
<table confidence="0.9972">
\x0cModel V-SUBJ V-OBJ N-OBJ
Target only 47.9 47.4 49.6
Selpref only 54.8 51.4 55.0
M&amp;L 50.3 52.0 53.4
SELPREF-POW, n=30 63.1 55.8 56.9
</table>
<tableCaption confidence="0.998003">
Table 3: Experiment 2: Mean out of ten precision (POOT)
</tableCaption>
<bodyText confidence="0.993793424242424">
dataset for robustness. Due to the sparsity of para-
phrases, a metric that considers fewer guesses leads
to artificially low results when a good paraphrase
was not mentioned by the annotators by chance but
is ranked highly by a model.
Results and discussion. Table 6 shows the mean
out-of-ten precision for all models. The behavior is
fairly uniform across all three datasets. Unsurpris-
ingly, target only, which uses the same ranking for
all instances of a target, yields the worst results.7
M&amp;Ls direct combination model outperforms tar-
get only significantly (p &lt; 0.05). However, on both
the V-SUBJ and the N-OBJ the selpref only baseline
does better than direct combination. The best results
on all datasets are obtained by SELPREF-POW. The
difference between SELPREF-POW and the target
only baseline is highly significant (p &lt; 0.01). The
difference to M&amp;Ls model is significant at p = 0.05.
We interpret these results as encouraging evidence
for the usefulness of selectional preferences for judg-
ing substitutability in context. Knowledge about the
selectional preferences of a single context word can
already lead to a significant improvement in precision.
We find this overall effect even though the word is
not informative in all cases. For instance, the subject
of item 2002 in Fig. 4, who, presumably helps little
in determining the verbs context-adapted meaning.
It is interesting that the improvement of SELPREF-
POW over selpref only is smallest for the N-OBJ
dataset (1.9% POOT). N-OBJ uses selectional prefer-
ences for nouns that may fill the direct object position,
, while V-SUBJ and V-OBJ use inverse selectional
preferences for verbs (cf. the two graphs in Fig. 1).
</bodyText>
<page confidence="0.980595">
7
</page>
<bodyText confidence="0.9991535">
Target only still does very much better than a random
baseline, which performs at 22% POOT.
</bodyText>
<sectionHeader confidence="0.99723" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998245725">
In this paper, we have considered semantic space
models that can account for the meaning of word
occurrences in context. Arguing that existing models
do not sufficiently take syntax into account, we have
introduced the new structured vector space (SVS)
model of word meaning. In addition to a vector rep-
resenting a words lexical meaning, it contains vec-
tors representing the words selectional preferences.
These selectional preferences play a central role in
the computation of meaning in context.
We have evaluated the SVS model on two datasets
on the task of predicting the felicitousness of para-
phrases in given contexts. On the M&amp;L dataset,
SVS outperforms the state-of-the-art model of M&amp;L,
though the difference is not significant. On the Lex-
ical Substitution dataset, SVS significantly outper-
forms the state-of-the-art. This is especially interest-
ing as the Lexical Substitution dataset, in contrast to
the M&amp;L data, uses realistic paraphrase candidates
that are not necessarily maximally distinct.
The most important limitation of the evaluation
that we have given in this paper is that we have only
considered single words as context. Our next step
will be to integrate information from multiple rela-
tions (such as both the subject and object positions
of a verb) into the computation of context-specific
meaning. Our eventual aim is a model that can give
a compositional account of a words meaning in con-
text, where all words in an expression disambiguate
one another according to the relations between them.
We will explore the usability of vector space mod-
els of word meaning in NLP applications, formulated
as the question of how to perform inferences on them
in the context of the Textual Entailment task (Dagan
et al., 2006). Paraphrase-based inference rules play
a large role in several recent approaches to Textual
Entailment (e.g. Szpektor et al (2008)); appropriate-
ness judgments of paraphrases in context, the task of
Experiments 1 and 2 above, can be viewed as testing
the applicability of these inferences rules.
</bodyText>
<reference confidence="0.9463375">
Acknowledgments. Many thanks for helpful dis-
cussion to Jason Baldridge, David Beaver, Dedre
Gentner, James Hampton, Dan Jurafsky, Alexander
Koller, Brad Love, and Ray Mooney.
</reference>
<page confidence="0.948597">
905
</page>
<reference confidence="0.998679240384616">
\x0cReferences
C. Brockmann, M. Lapata. 2003. Evaluating and combin-
ing approaches to selectional preference acquisition. In
Proceedings of EACL, 2734.
I. Dagan, O. Glickman, B. Magnini. 2006. The PASCAL
Recognising Textual Entailment Challenge. In Ma-
chine Learning Challenges, Lecture Notes in Computer
Science, 177190. Springer.
K. Erk. 2007. A simple, similarity-based model for selec-
tional preferences. In Proceedings of ACL, 216223.
T. Ferretti, C. Gagne, K. McRae. 2003. Thematic role fo-
cusing by participle inflections: evidence form concep-
tual combination. Journal of Experimental Psychology,
29(1):118127.
D. Gildea, D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245
288.
D. Hindle, M. Rooth. 1993. Structural ambiguity and
lexical relations. Computational Linguistics, 19(1):103
120.
M. Jones, D. Mewhort. 2007. Representing word mean-
ing and order information in a composite holographic
lexicon. Psychological review, 114:137.
J. J. Katz, J. A. Fodor. 1964. The structure of a semantic
theory. In The Structure of Language. Prentice-Hall.
A. Kilgarriff. 1997. I dont believe in word senses. Com-
puters and the Humanities, 31(2):91113.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173202.
T. Landauer, S. Dumais. 1997. A solution to Platos prob-
lem: the latent semantic analysis theory of acquisition,
induction, and representation of knowledge. Psycho-
logical Review, 104(2):211240.
D. Lin. 1993. Principle-based parsing without overgener-
ation. In Proceedings of ACL, 112120.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING-ACL, 768774.
K. Lund, C. Burgess. 1996. Producing high-dimensional
semantic spaces from lexical co-occurrence. Behav-
ior Research Methods, Instruments, and Computers,
28:203208.
C. D. Manning, P. Raghavan, H. Schutze. 2008. Introduc-
tion to Information Retrieval. CUP.
D. McCarthy, J. Carroll. 2003. Disambiguating nouns,
verbs, and adjectives using automatically acquired
selectional preferences. Computational Linguistics,
29(4):639654.
D. McCarthy, R. Navigli. 2007. SemEval-2007 Task 10:
English Lexical Substitution Task. In Proceedings of
SemEval, 4853.
S. McDonald, C. Brew. 2004. A distributional model
of semantic context effects in lexical processing. In
Proceedings of ACL, 1724.
S. McDonald, M. Ramscar. 2001. Testing the distribu-
tional hypothesis: The influence of context on judge-
ments of semantic similarity. In Proceedings of CogSci,
611616.
K. McRae, M. Spivey-Knowlton, M. Tanenhaus. 1998.
Modeling the influence of thematic fit (and other con-
straints) in on-line sentence comprehension. Journal of
Memory and Language, 38:283312.
K. McRae, M. Hare, J. Elman, T. Ferretti. 2005. A
basis for generating expectancies for verbs from nouns.
Memory and Cognition, 33(7):11741184.
J. Mitchell, M. Lapata. 2008. Vector-based models of
semantic composition. In Proceedings of ACL, 236
244.
A. Moschitti, S. Quarteroni. 2008. Kernels on linguistic
structures for answer extraction. In Proceedings of
ACL, 113116, Columbus, OH.
S. Narayanan, D. Jurafsky. 2002. A Bayesian model
predicts human parse preference and reading time in
sentence processing. In Proceedings of NIPS, 5965.
S. Pado, M. Lapata. 2007. Dependency-based construc-
tion of semantic space models. Computational Linguis-
tics, 33(2):161199.
U. Pado, F. Keller, M. W. Crocker. 2006. Combining syn-
tax and thematic fit in a probabilistic model of sentence
processing. In Proceedings of CogSci, 657662.
S. Pado, U. Pado, K. Erk. 2007. Flexible, corpus-based
modelling of human plausibility judgements. In Pro-
ceedings of EMNLP/CoNLL, 400409.
P. Resnik. 1996. Selectional constraints: An information-
theoretic model and its computational realization. Cog-
nition, 61:127159.
G. Salton, A. Wang, C. Yang. 1975. A vector-space model
for information retrieval. Journal of the American So-
ciety for Information Science, 18:613620.
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97124.
S. Sharoff. 2006. Open-source corpora: Using the net to
fish for linguistic data. International Journal of Corpus
Linguistics, 11(4):435462.
P. Smolensky. 1990. Tensor product variable binding and
the representation of symbolic structures in connection-
ist systems. Artificial Intelligence, 46:159216.
I. Szpektor, I. Dagan, R. Bar-Haim, J. Goldberger. 2008.
Contextual preferences. In Proceedings of ACL, 683
691, Columbus, OH.
Y. Wilks. 1975. Preference semantics. In Formal Seman-
tics of Natural Language. CUP.
A. Yeh. 2000. More accurate tests for the statistical
significance of result differences. In Proceeedings of
COLING, 947953.
</reference>
<page confidence="0.98752">
906
</page>
<figure confidence="0.247596">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.636774">
<note confidence="0.85431">b&apos;Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 897906, Honolulu, October 2008. c 2008 Association for Computational Linguistics</note>
<title confidence="0.974874">A Structured Vector Space Model for Word Meaning in Context</title>
<author confidence="0.985672">Katrin Erk</author>
<affiliation confidence="0.999928">Department of Linguistics University of Texas at Austin</affiliation>
<email confidence="0.999384">katrin.erk@mail.utexas.edu</email>
<author confidence="0.999595">Sebastian Pado</author>
<affiliation confidence="0.999889">Department of Linguistics Stanford University</affiliation>
<email confidence="0.998792">pado@stanford.edu</email>
<abstract confidence="0.999548705882353">We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account. We present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences C Brockmann</author>
<author>M Lapata</author>
</authors>
<title>Evaluating and combining approaches to selectional preference acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>2734</pages>
<contexts>
<context position="11928" citStr="Brockmann and Lapata, 2003" startWordPosition="1935" endWordPosition="1938">pectations about typical events for human language processing is well-established. Expectations affect reading times (McRae et al., 1998), the interpretation of participles (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pado et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pado et al., 2007). We first present the SVS model of word meaning 899 \x0ccatch he fielder dog cold baseball drift obj subj accuse say claim</context>
</contexts>
<marker>Brockmann, Lapata, 2003</marker>
<rawString>\x0cReferences C. Brockmann, M. Lapata. 2003. Evaluating and combining approaches to selectional preference acquisition. In Proceedings of EACL, 2734.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Machine Learning Challenges, Lecture Notes in Computer Science,</booktitle>
<pages>177190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5110" citStr="Dagan et al., 2006" startWordPosition="840" endWordPosition="843">to straightforwardly interpret c = a \x0c b as the meaning of the phrase a + b (Kintsch, 2001; Mitchell and Lapata, 2008). The problem is that the vector c can only encode a fixed amount of structural information if its dimensionality is fixed, but there is no upper limit on sentence length, and hence on the amount of structure to be encoded. It is difficult to conceive how c could encode deeper semantic properties, like predicateargument structure (distinguishing dog bites man and man bites dog), that are crucial for sentencelevel semantic tasks such as the recognition of textual entailment (Dagan et al., 2006). An alternative approach to sentence meaning would be to use the vector space representation only for representing word meaning, and to represent sentence structure separately. Unfortunately, present models cannot provide this grounding either, since they compute a single vector c that provides the same representations for both the meanings of a and b in context. In this paper, we propose a new, structured vector space model for word meaning (SVS) that addresses these problems. A SVS representation of a lemma comprises several vectors representing the words lexical meaning as well as the sele</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>I. Dagan, O. Glickman, B. Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In Machine Learning Challenges, Lecture Notes in Computer Science, 177190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>216223</pages>
<contexts>
<context position="12385" citStr="Erk, 2007" startWordPosition="2006" endWordPosition="2007">ng been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pado et al., 2007). We first present the SVS model of word meaning 899 \x0ccatch he fielder dog cold baseball drift obj subj accuse say claim comp-1 ball whirl fly provide throw catch organise obj-1 subj-1 mod red golf elegant Figure 1: Structured meaning representations for noun ball and verb catch: lexical information plus expectations that integrates lexical information with selectional preferences. Then, we show how the SVS model provides a new way of computing meaning in context. Representing lemma meaning. We abandon the traditional choice of representing word meaning as a single vecto</context>
<context position="20150" citStr="Erk (2007)" startWordPosition="3317" endWordPosition="3318">or space (SYN, (Pado and Lapata, 2007)). In this space, target and context words have to be linked by a valid dependency path in a dependency graph to count as co-occurring.2 This space was built from BNC dependency parses obtained from Minipar (Lin, 1993). For both spaces, we used pre-experiments to compare two methods for the computation of vector components, namely raw co-occurrence counts, the standard model, and the pointwise mutual information (PMI) definition employed by M&amp;L. Selectional preferences. We use a simple, knowledge-lean representation for selectional preferences inspired by Erk (2007), who models selectional preference through similarity to seen filler vectors ~ va: We compute the selectional preference vector for word b and relation r as the weighted 2 More specifically, we used the minimal context specification and plain weight function. See Pado and Lapata (2007). 901 \x0ccentroid of seen filler vectors ~ va. We collect seen fillers from the Minipar-parse of the BNC. Let f(a, r, b) denote the frequency of a occurring in relation r to b in the parsed BNC, then Rb(r)SELPREF = X a:f(a,r,b)&gt;0 f(a, r, b) ~ va (5) We call this base model SELPREF. We will also study two varian</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>K. Erk. 2007. A simple, similarity-based model for selectional preferences. In Proceedings of ACL, 216223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ferretti</author>
<author>C Gagne</author>
<author>K McRae</author>
</authors>
<title>Thematic role focusing by participle inflections: evidence form conceptual combination.</title>
<date>2003</date>
<journal>Journal of Experimental Psychology,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="11497" citStr="Ferretti et al., 2003" startWordPosition="1869" endWordPosition="1872">e catch a ball, the hearer will interpret the meaning of catch to match typical actions that can be performed with a ball. Similarly, the interpretation of ball will reflect the hearers expectations about typical things that can be caught. This move to include typical arguments and predicates into a model of word meaning can be motivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events for human language processing is well-established. Expectations affect reading times (McRae et al., 1998), the interpretation of participles (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pado et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sen</context>
</contexts>
<marker>Ferretti, Gagne, McRae, 2003</marker>
<rawString>T. Ferretti, C. Gagne, K. McRae. 2003. Thematic role focusing by participle inflections: evidence form conceptual combination. Journal of Experimental Psychology, 29(1):118127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<pages>288</pages>
<contexts>
<context position="12198" citStr="Gildea and Jurafsky, 2002" startWordPosition="1976" endWordPosition="1979">al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pado et al., 2007). We first present the SVS model of word meaning 899 \x0ccatch he fielder dog cold baseball drift obj subj accuse say claim comp-1 ball whirl fly provide throw catch organise obj-1 subj-1 mod red golf elegant Figure 1: Structured meaning representations for noun ball and verb catch: lexical information plus expectations that integrates lexical information with selectional preferences. Then,</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea, D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245 288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
<author>M Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>120</pages>
<contexts>
<context position="12087" citStr="Hindle and Rooth, 1993" startWordPosition="1959" endWordPosition="1962">rticiples (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pado et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pado et al., 2007). We first present the SVS model of word meaning 899 \x0ccatch he fielder dog cold baseball drift obj subj accuse say claim comp-1 ball whirl fly provide throw catch organise obj-1 subj-1 mod red golf elegant Figure 1: Structured meaning representations for noun ball and verb catch</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>D. Hindle, M. Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103 120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jones</author>
<author>D Mewhort</author>
</authors>
<title>Representing word meaning and order information in a composite holographic lexicon. Psychological review,</title>
<date>2007</date>
<pages>114--137</pages>
<contexts>
<context position="9193" citStr="Jones and Mewhort (2007)" startWordPosition="1507" endWordPosition="1510">us instead on methods for the direct combination of p and a: In a comparison between component-wise addition and multiplication of p and a, they find far superior results for the multiplication approach. Tensor product-based models. Smolensky (1990) uses tensor product to combine two word vectors a and b into a vector c representing the expression a+b. The vector c is located in a very high-dimensional space and is thus capable of encoding the structure of the expression; however, this makes the model infeasible in practice, as dimensionality rises with every word added to the representation. Jones and Mewhort (2007) represent lemma meaning by using circular convolution to encode n-gram co-occurrence information into vectors of fixed dimensionality. Similar to Brew and McDonald (2004), they predict most likely next words in a sequence, without taking syntax into account. Kernel methods. One of the main tests for the quality of models of word meaning in context is the ability to predict the appropriateness of paraphrases in given a context. Typically, a paraphrase applies only to some senses of a word, not all, as can be seen in the paraphrases grab and contract of catch. Vector space models generally pred</context>
</contexts>
<marker>Jones, Mewhort, 2007</marker>
<rawString>M. Jones, D. Mewhort. 2007. Representing word meaning and order information in a composite holographic lexicon. Psychological review, 114:137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Katz</author>
<author>J A Fodor</author>
</authors>
<title>The structure of a semantic theory.</title>
<date>1964</date>
<booktitle>In The Structure of Language.</booktitle>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="11831" citStr="Katz and Fodor, 1964" startWordPosition="1921" endWordPosition="1924">ted both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events for human language processing is well-established. Expectations affect reading times (McRae et al., 1998), the interpretation of participles (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pado et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pado et al., 2007). We first present the SVS</context>
</contexts>
<marker>Katz, Fodor, 1964</marker>
<rawString>J. J. Katz, J. A. Fodor. 1964. The structure of a semantic theory. In The Structure of Language. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>I dont believe in word senses.</title>
<date>1997</date>
<journal>Computers and the Humanities,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="1870" citStr="Kilgarriff, 1997" startWordPosition="283" endWordPosition="285">s as high-dimensional vectors. In the default case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus. These vectors are able to provide a robust model of semantic similarity that has been used in NLP (Salton et al., 1975; McCarthy and Carroll, 2003; Manning et al., 2008) and to model experimental results in cognitive science (Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in the literature (Smolensky, 1990; Schutze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) that compute meaning in context from lemma vectors. Mos</context>
</contexts>
<marker>Kilgarriff, 1997</marker>
<rawString>A. Kilgarriff. 1997. I dont believe in word senses. Computers and the Humanities, 31(2):91113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<pages>25--173202</pages>
<contexts>
<context position="2361" citStr="Kintsch, 2001" startWordPosition="358" endWordPosition="359">ovide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in the literature (Smolensky, 1990; Schutze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) that compute meaning in context from lemma vectors. Most of these studies phrase the problem as one of vector composition: The meaning of a target occurrence a in context b is a single new vector c that is a function (for example, the centroid) of the vectors: c = a \x0c b. The context b can consist of as little as one word, as shown in Example (1). In (1a), the meaning of catch combined with ball is similar to grab, while in (1b), combined with disease, it can be paraphrased by contract. Conversely, verbs can influence the interpretation o</context>
<context position="4584" citStr="Kintsch, 2001" startWordPosition="753" endWordPosition="754">the same representation to (2a) and (2b). Thus, existing models are systematically unable to capture this class of phenomena. Single vectors are too weak to represent phrases. The second problem arises in the context of the important open question of how semantic spaces can scale up to provide interesting meaning representations for entire sentences. We believe that the current vector composition methods, which result in a single vector c, are not informative enough for this purpose. One proposal for scaling up is to straightforwardly interpret c = a \x0c b as the meaning of the phrase a + b (Kintsch, 2001; Mitchell and Lapata, 2008). The problem is that the vector c can only encode a fixed amount of structural information if its dimensionality is fixed, but there is no upper limit on sentence length, and hence on the amount of structure to be encoded. It is difficult to conceive how c could encode deeper semantic properties, like predicateargument structure (distinguishing dog bites man and man bites dog), that are crucial for sentencelevel semantic tasks such as the recognition of textual entailment (Dagan et al., 2006). An alternative approach to sentence meaning would be to use the vector s</context>
<context position="7779" citStr="Kintsch (2001)" startWordPosition="1270" endWordPosition="1271">window, by summing up all first-order vectors of the words in this context. The resulting vectors form sense clusters. McDonald and Brew (2004) present a similar model. They compute the expectation for a word wi in a sequence by summing the first-order vectors for the words w1 to wi1 and showed that the distance between expectation and first-order vector for wi correlates with human reading times. Predicate-argument combination. The second category of prior studies concentrates on contexts consisting of a single word only, typically modeling the combination of a predicate p and an argument a. Kintsch (2001) uses vector representations of p and a to identify the set of words that are similar to both p and a. After this set has been narrowed down in a self-inhibitory network, the meaning of the predicateargument combination is obtained by computing the 898 \x0ccentroid of its members vectors. The procedure does not take the relation between p and a into account. Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p + a as a function f operating on four components: c = f(p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge. Thi</context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>W. Kintsch. 2001. Predication. Cognitive Science, 25:173202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>S Dumais</author>
</authors>
<title>A solution to Platos problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="1671" citStr="Landauer and Dumais, 1997" startWordPosition="252" endWordPosition="255">and above the state of the art for modeling the contextual adequacy of paraphrases. 1 Introduction Semantic spaces are a popular framework for the representation of word meaning, encoding the meaning of lemmas as high-dimensional vectors. In the default case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus. These vectors are able to provide a robust model of semantic similarity that has been used in NLP (Salton et al., 1975; McCarthy and Carroll, 2003; Manning et al., 2008) and to model experimental results in cognitive science (Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There hav</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. Landauer, S. Dumais. 1997. A solution to Platos problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Principle-based parsing without overgeneration.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>112120</pages>
<contexts>
<context position="19796" citStr="Lin, 1993" startWordPosition="3266" endWordPosition="3267">ce (BOW, (Lund and Burgess, 1996)). For each pair of a target word and context word, the BOW space records a function of their co-occurrence frequency within a surface window of size 10. The space is constructed from the British National Corpus (BNC), and uses the 2,000 most frequent context words as dimensions. We also consider a dependency-based vector space (SYN, (Pado and Lapata, 2007)). In this space, target and context words have to be linked by a valid dependency path in a dependency graph to count as co-occurring.2 This space was built from BNC dependency parses obtained from Minipar (Lin, 1993). For both spaces, we used pre-experiments to compare two methods for the computation of vector components, namely raw co-occurrence counts, the standard model, and the pointwise mutual information (PMI) definition employed by M&amp;L. Selectional preferences. We use a simple, knowledge-lean representation for selectional preferences inspired by Erk (2007), who models selectional preference through similarity to seen filler vectors ~ va: We compute the selectional preference vector for word b and relation r as the weighted 2 More specifically, we used the minimal context specification and plain we</context>
</contexts>
<marker>Lin, 1993</marker>
<rawString>D. Lin. 1993. Principle-based parsing without overgeneration. In Proceedings of ACL, 112120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>768774</pages>
<contexts>
<context position="27045" citStr="Lin, 1998" startWordPosition="4431" endWordPosition="4432"> preferences of a noun and (left) its lexical vector and (right) inverse object preferences vector (cosine similarity in SYN space) = 0.2, significantly outperforming both baselines. It is interesting, though, that the subj1 preference itself (Selpref only) is already highly significantly correlated with the human judgments. A comparison of the upper half (BOW) with the lower half (SYN) shows that the dependency-based space generally shows better correlation with human judgements. This corresponds to a beneficial effect of syntactic information found for other applications of semantic spaces (Lin, 1998; Pado and Lapata, 2007). All instances of the SELPREF model show highly significant correlations. SELPREF and SELPREF-CUT show very similar performance. They do better than both baselines in the BOW space; however, in the cleaner SYN space, their performance is numerically lower than using selectional preferences only ( = 0.13 vs. 0.16). SELPREF-POW is always significantly better than SELPREF and SELPREF-CUT, and shows the best result of all tested models ( = 0.27, BOW space). The performance is somewhat lower in the SYN space ( = 0.22). However, this difference, and the difference to the bes</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL, 768774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical co-occurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, and Computers,</journal>
<pages>28--203208</pages>
<contexts>
<context position="2073" citStr="Lund and Burgess, 1996" startWordPosition="313" endWordPosition="316">ide a robust model of semantic similarity that has been used in NLP (Salton et al., 1975; McCarthy and Carroll, 2003; Manning et al., 2008) and to model experimental results in cognitive science (Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in the literature (Smolensky, 1990; Schutze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) that compute meaning in context from lemma vectors. Most of these studies phrase the problem as one of vector composition: The meaning of a target occurrence a in context b is a single new vector c that is a function (for example, the centroid) of the vector</context>
<context position="19219" citStr="Lund and Burgess, 1996" startWordPosition="3165" endWordPosition="3168">nct paraphrase candidates. Experiment 2 considers a broader class of constructions along with annotator-generated paraphrase candidates that are not screened for distinctness. In both experiments, we compare the SVS model against the state-of-theart model by Mitchell and Lapata 2008 (henceforth M&amp;L; cf. Sec. 2 for model details). 4.2 Parameter choices Vector space. In our parameterization of the vector space, we largely follow M&amp;L because their model has been rigorously evaluated and found to outperform a range of other models. Our first space is a traditional bag-of-words vector space (BOW, (Lund and Burgess, 1996)). For each pair of a target word and context word, the BOW space records a function of their co-occurrence frequency within a surface window of size 10. The space is constructed from the British National Corpus (BNC), and uses the 2,000 most frequent context words as dimensions. We also consider a dependency-based vector space (SYN, (Pado and Lapata, 2007)). In this space, target and context words have to be linked by a valid dependency path in a dependency graph to count as co-occurring.2 This space was built from BNC dependency parses obtained from Minipar (Lin, 1993). For both spaces, we u</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>K. Lund, C. Burgess. 1996. Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instruments, and Computers, 28:203208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>P Raghavan</author>
<author>H Schutze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>CUP.</publisher>
<contexts>
<context position="1589" citStr="Manning et al., 2008" startWordPosition="240" endWordPosition="243">he computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases. 1 Introduction Semantic spaces are a popular framework for the representation of word meaning, encoding the meaning of lemmas as high-dimensional vectors. In the default case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus. These vectors are able to provide a robust model of semantic similarity that has been used in NLP (Salton et al., 1975; McCarthy and Carroll, 2003; Manning et al., 2008) and to model experimental results in cognitive science (Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next neces</context>
</contexts>
<marker>Manning, Raghavan, Schutze, 2008</marker>
<rawString>C. D. Manning, P. Raghavan, H. Schutze. 2008. Introduction to Information Retrieval. CUP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>J Carroll</author>
</authors>
<title>Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="1566" citStr="McCarthy and Carroll, 2003" startWordPosition="236" endWordPosition="239">e to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases. 1 Introduction Semantic spaces are a popular framework for the representation of word meaning, encoding the meaning of lemmas as high-dimensional vectors. In the default case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus. These vectors are able to provide a robust model of semantic similarity that has been used in NLP (Salton et al., 1975; McCarthy and Carroll, 2003; Manning et al., 2008) and to model experimental results in cognitive science (Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous</context>
<context position="12143" citStr="McCarthy and Carroll, 2003" startWordPosition="1966" endWordPosition="1970">essing generally (Narayanan and Jurafsky, 2002; Pado et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pado et al., 2007). We first present the SVS model of word meaning 899 \x0ccatch he fielder dog cold baseball drift obj subj accuse say claim comp-1 ball whirl fly provide throw catch organise obj-1 subj-1 mod red golf elegant Figure 1: Structured meaning representations for noun ball and verb catch: lexical information plus expectations that integrates </context>
</contexts>
<marker>McCarthy, Carroll, 2003</marker>
<rawString>D. McCarthy, J. Carroll. 2003. Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences. Computational Linguistics, 29(4):639654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Navigli</author>
</authors>
<title>SemEval-2007 Task 10: English Lexical Substitution Task.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval,</booktitle>
<pages>4853</pages>
<contexts>
<context position="1899" citStr="McCarthy and Navigli, 2007" startWordPosition="286" endWordPosition="289">nal vectors. In the default case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus. These vectors are able to provide a robust model of semantic similarity that has been used in NLP (Salton et al., 1975; McCarthy and Carroll, 2003; Manning et al., 2008) and to model experimental results in cognitive science (Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in the literature (Smolensky, 1990; Schutze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) that compute meaning in context from lemma vectors. Most of these studies phrase the</context>
<context position="29895" citStr="McCarthy and Navigli, 2007" startWordPosition="4872" endWordPosition="4875">verbs, which means that all disambiguation was done through inverse selectional preferences. As inverse selectional preferences are currently largely unexplored, it is interesting to note that the evidence that they provide for the paraphrase task is as strong as that of the context nouns themselves. 6 Exp. 2: Ranking paraphrases This section reports on a second, more NLP-oriented experiment whose task is to distinguish between appropriate and inappropriate paraphrases on a broader range of constructions. Dataset. For this experiment, we use the SemEval1 lexical substitution (lexsub) dataset (McCarthy and Navigli, 2007), which contains 10 instances each of 200 target words in sentential contexts, drawn from Sharoffs (2006) English Internet Corpus. Contextually appropriate paraphrases for each instance of each target word were elicited from up to 6 participants. Fig. 4 shows two instances for the verb to work. The distribution over paraphrases can be seen as a characterization of the target words meaning in each context. Experimental procedure. In this paper, we predict appropriate paraphrases solely on the basis of a single context word that stands in a direct predicateargument relation to the target word. W</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>D. McCarthy, R. Navigli. 2007. SemEval-2007 Task 10: English Lexical Substitution Task. In Proceedings of SemEval, 4853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S McDonald</author>
<author>C Brew</author>
</authors>
<title>A distributional model of semantic context effects in lexical processing.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1724</pages>
<contexts>
<context position="2386" citStr="McDonald and Brew, 2004" startWordPosition="360" endWordPosition="363">f word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in the literature (Smolensky, 1990; Schutze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) that compute meaning in context from lemma vectors. Most of these studies phrase the problem as one of vector composition: The meaning of a target occurrence a in context b is a single new vector c that is a function (for example, the centroid) of the vectors: c = a \x0c b. The context b can consist of as little as one word, as shown in Example (1). In (1a), the meaning of catch combined with ball is similar to grab, while in (1b), combined with disease, it can be paraphrased by contract. Conversely, verbs can influence the interpretation of nouns: In (1a), ball is</context>
<context position="7308" citStr="McDonald and Brew (2004)" startWordPosition="1193" endWordPosition="1196">xt. General context effects. The first category of models aims at integrating the widest possible range of context information without recourse to linguistic structure. The best-known work in this category is Schutze (1998). He first computes first-order vector representations for word meaning by collecting co-occurrence counts from the entire corpus. Then, he determines second-order vectors for individual word instances in their context, which is taken to be a simple surface window, by summing up all first-order vectors of the words in this context. The resulting vectors form sense clusters. McDonald and Brew (2004) present a similar model. They compute the expectation for a word wi in a sequence by summing the first-order vectors for the words w1 to wi1 and showed that the distance between expectation and first-order vector for wi correlates with human reading times. Predicate-argument combination. The second category of prior studies concentrates on contexts consisting of a single word only, typically modeling the combination of a predicate p and an argument a. Kintsch (2001) uses vector representations of p and a to identify the set of words that are similar to both p and a. After this set has been na</context>
</contexts>
<marker>McDonald, Brew, 2004</marker>
<rawString>S. McDonald, C. Brew. 2004. A distributional model of semantic context effects in lexical processing. In Proceedings of ACL, 1724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S McDonald</author>
<author>M Ramscar</author>
</authors>
<title>Testing the distributional hypothesis: The influence of context on judgements of semantic similarity.</title>
<date>2001</date>
<booktitle>In Proceedings of CogSci,</booktitle>
<pages>611616</pages>
<contexts>
<context position="1700" citStr="McDonald and Ramscar, 2001" startWordPosition="256" endWordPosition="259">art for modeling the contextual adequacy of paraphrases. 1 Introduction Semantic spaces are a popular framework for the representation of word meaning, encoding the meaning of lemmas as high-dimensional vectors. In the default case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus. These vectors are able to provide a robust model of semantic similarity that has been used in NLP (Salton et al., 1975; McCarthy and Carroll, 2003; Manning et al., 2008) and to model experimental results in cognitive science (Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in </context>
</contexts>
<marker>McDonald, Ramscar, 2001</marker>
<rawString>S. McDonald, M. Ramscar. 2001. Testing the distributional hypothesis: The influence of context on judgements of semantic similarity. In Proceedings of CogSci, 611616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McRae</author>
<author>M Spivey-Knowlton</author>
<author>M Tanenhaus</author>
</authors>
<title>Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension.</title>
<date>1998</date>
<journal>Journal of Memory and Language,</journal>
<pages>38--283312</pages>
<contexts>
<context position="11438" citStr="McRae et al., 1998" startWordPosition="1861" endWordPosition="1864"> example, in (1a), we assume that upon hearing the phrase catch a ball, the hearer will interpret the meaning of catch to match typical actions that can be performed with a ball. Similarly, the interpretation of ball will reflect the hearers expectations about typical things that can be caught. This move to include typical arguments and predicates into a model of word meaning can be motivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events for human language processing is well-established. Expectations affect reading times (McRae et al., 1998), the interpretation of participles (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pado et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for </context>
</contexts>
<marker>McRae, Spivey-Knowlton, Tanenhaus, 1998</marker>
<rawString>K. McRae, M. Spivey-Knowlton, M. Tanenhaus. 1998. Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension. Journal of Memory and Language, 38:283312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McRae</author>
<author>M Hare</author>
<author>J Elman</author>
<author>T Ferretti</author>
</authors>
<title>A basis for generating expectancies for verbs from nouns.</title>
<date>2005</date>
<journal>Memory and Cognition,</journal>
<volume>33</volume>
<issue>7</issue>
<contexts>
<context position="11668" citStr="McRae et al., 2005" startWordPosition="1897" endWordPosition="1900">the hearers expectations about typical things that can be caught. This move to include typical arguments and predicates into a model of word meaning can be motivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events for human language processing is well-established. Expectations affect reading times (McRae et al., 1998), the interpretation of participles (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pado et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been p</context>
<context position="14296" citStr="McRae et al., 2005" startWordPosition="2316" endWordPosition="2319">or (cf. Section 4). Likewise, ball is represented by one vector for ball itself, one for balls preferences for its modifiers (mod), one vector for the verbs of which it is a subject (subj1), and one for the verbs of which is an object (obj1). This representation includes selectional preferences (like subj, obj, mod) exactly parallel to inverse selectional preferences (subj1, obj1, comp1). To our knowledge, preferences of the latter kind have not been studied in computational linguistics. However, their existence is supported in psycholinguistics by priming effects from nouns to typical verbs (McRae et al., 2005). Formally, let D be a vector space (the set of possi1 We do not commit to a particular set of relations; see the discussion at the end of this section. catch ... cold baseball drift obj subj ... comp -1 ball ... throw catch organise obj -1 subj -1 mod ... ! ! Figure 2: Combining predicate and argument via relationspecific semantic expectations ble vectors), and let R be some set of relation labels. In the structured vector space (SVS) model, we represent the meaning of a lemma w as a triple w = (v, R, R1 ) where v D is a lexical vector describing the word w itself, R : R D maps each relation </context>
</contexts>
<marker>McRae, Hare, Elman, Ferretti, 2005</marker>
<rawString>K. McRae, M. Hare, J. Elman, T. Ferretti. 2005. A basis for generating expectancies for verbs from nouns. Memory and Cognition, 33(7):11741184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="2414" citStr="Mitchell and Lapata, 2008" startWordPosition="364" endWordPosition="368">dependent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in the literature (Smolensky, 1990; Schutze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) that compute meaning in context from lemma vectors. Most of these studies phrase the problem as one of vector composition: The meaning of a target occurrence a in context b is a single new vector c that is a function (for example, the centroid) of the vectors: c = a \x0c b. The context b can consist of as little as one word, as shown in Example (1). In (1a), the meaning of catch combined with ball is similar to grab, while in (1b), combined with disease, it can be paraphrased by contract. Conversely, verbs can influence the interpretation of nouns: In (1a), ball is understood as a spherical o</context>
<context position="4612" citStr="Mitchell and Lapata, 2008" startWordPosition="755" endWordPosition="758">entation to (2a) and (2b). Thus, existing models are systematically unable to capture this class of phenomena. Single vectors are too weak to represent phrases. The second problem arises in the context of the important open question of how semantic spaces can scale up to provide interesting meaning representations for entire sentences. We believe that the current vector composition methods, which result in a single vector c, are not informative enough for this purpose. One proposal for scaling up is to straightforwardly interpret c = a \x0c b as the meaning of the phrase a + b (Kintsch, 2001; Mitchell and Lapata, 2008). The problem is that the vector c can only encode a fixed amount of structural information if its dimensionality is fixed, but there is no upper limit on sentence length, and hence on the amount of structure to be encoded. It is difficult to conceive how c could encode deeper semantic properties, like predicateargument structure (distinguishing dog bites man and man bites dog), that are crucial for sentencelevel semantic tasks such as the recognition of textual entailment (Dagan et al., 2006). An alternative approach to sentence meaning would be to use the vector space representation only for</context>
<context position="8166" citStr="Mitchell and Lapata (2008)" startWordPosition="1335" endWordPosition="1338">with human reading times. Predicate-argument combination. The second category of prior studies concentrates on contexts consisting of a single word only, typically modeling the combination of a predicate p and an argument a. Kintsch (2001) uses vector representations of p and a to identify the set of words that are similar to both p and a. After this set has been narrowed down in a self-inhibitory network, the meaning of the predicateargument combination is obtained by computing the 898 \x0ccentroid of its members vectors. The procedure does not take the relation between p and a into account. Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p + a as a function f operating on four components: c = f(p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge. This framework allows sensitivity to the relation. However, the concrete instantiations that Mitchell and Lapata consider disregards K and R, thus sharing the other models limitations. They focus instead on methods for the direct combination of p and a: In a comparison between component-wise addition and multiplication of p and a, they find far superior results for the multiplication app</context>
<context position="18879" citStr="Mitchell and Lapata 2008" startWordPosition="3109" endWordPosition="3112">her the predicate or the argument) is in that context. We perform two experiments that both use the paraphrase task, but differ in their emphasis. Experiment 1 replicates an existing evaluation against human judgments. This evaluation uses synthetic dataset, limited to one particular construction, and constructed to provide maximally distinct paraphrase candidates. Experiment 2 considers a broader class of constructions along with annotator-generated paraphrase candidates that are not screened for distinctness. In both experiments, we compare the SVS model against the state-of-theart model by Mitchell and Lapata 2008 (henceforth M&amp;L; cf. Sec. 2 for model details). 4.2 Parameter choices Vector space. In our parameterization of the vector space, we largely follow M&amp;L because their model has been rigorously evaluated and found to outperform a range of other models. Our first space is a traditional bag-of-words vector space (BOW, (Lund and Burgess, 1996)). For each pair of a target word and context word, the BOW space records a function of their co-occurrence frequency within a surface window of size 10. The space is constructed from the British National Corpus (BNC), and uses the 2,000 most frequent context </context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>J. Mitchell, M. Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL, 236 244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>S Quarteroni</author>
</authors>
<title>Kernels on linguistic structures for answer extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>113116</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="10422" citStr="Moschitti and Quarteroni, 2008" startWordPosition="1694" endWordPosition="1697">edict paraphrase appropriateness based on the similarity between vectors. This task can also be addressed with kernel methods, which project items into an implicit feature space for efficient similarity computation. Consequently, vector space methods and kernel methods have both been used for NLP tasks based on similarity, notably Information Retrieval and Textual Entailment. Nevertheless, they place their emphasis on different types of information. Current kernels are mostly tree kernels that compare syntactic structure, and use semantic information mostly for smoothing syntactic similarity (Moschitti and Quarteroni, 2008). In contrast, vector-space models focus on the interaction between the lexical meaning of words in composition. 3 A structured vector space model for word meaning in context In this section, we define the structured vector space (SVS) model of word meaning. The main intuition behind our model is to view the interpretation of a word in context as guided by expectations about typical events. For example, in (1a), we assume that upon hearing the phrase catch a ball, the hearer will interpret the meaning of catch to match typical actions that can be performed with a ball. Similarly, the interpret</context>
</contexts>
<marker>Moschitti, Quarteroni, 2008</marker>
<rawString>A. Moschitti, S. Quarteroni. 2008. Kernels on linguistic structures for answer extraction. In Proceedings of ACL, 113116, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
<author>D Jurafsky</author>
</authors>
<title>A Bayesian model predicts human parse preference and reading time in sentence processing.</title>
<date>2002</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>5965</pages>
<contexts>
<context position="11562" citStr="Narayanan and Jurafsky, 2002" startWordPosition="1878" endWordPosition="1881">tch to match typical actions that can be performed with a ball. Similarly, the interpretation of ball will reflect the hearers expectations about typical things that can be caught. This move to include typical arguments and predicates into a model of word meaning can be motivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events for human language processing is well-established. Expectations affect reading times (McRae et al., 1998), the interpretation of participles (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pado et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role </context>
</contexts>
<marker>Narayanan, Jurafsky, 2002</marker>
<rawString>S. Narayanan, D. Jurafsky. 2002. A Bayesian model predicts human parse preference and reading time in sentence processing. In Proceedings of NIPS, 5965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pado</author>
<author>M Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="19578" citStr="Pado and Lapata, 2007" startWordPosition="3226" endWordPosition="3229"> space. In our parameterization of the vector space, we largely follow M&amp;L because their model has been rigorously evaluated and found to outperform a range of other models. Our first space is a traditional bag-of-words vector space (BOW, (Lund and Burgess, 1996)). For each pair of a target word and context word, the BOW space records a function of their co-occurrence frequency within a surface window of size 10. The space is constructed from the British National Corpus (BNC), and uses the 2,000 most frequent context words as dimensions. We also consider a dependency-based vector space (SYN, (Pado and Lapata, 2007)). In this space, target and context words have to be linked by a valid dependency path in a dependency graph to count as co-occurring.2 This space was built from BNC dependency parses obtained from Minipar (Lin, 1993). For both spaces, we used pre-experiments to compare two methods for the computation of vector components, namely raw co-occurrence counts, the standard model, and the pointwise mutual information (PMI) definition employed by M&amp;L. Selectional preferences. We use a simple, knowledge-lean representation for selectional preferences inspired by Erk (2007), who models selectional pre</context>
<context position="27069" citStr="Pado and Lapata, 2007" startWordPosition="4433" endWordPosition="4436">s of a noun and (left) its lexical vector and (right) inverse object preferences vector (cosine similarity in SYN space) = 0.2, significantly outperforming both baselines. It is interesting, though, that the subj1 preference itself (Selpref only) is already highly significantly correlated with the human judgments. A comparison of the upper half (BOW) with the lower half (SYN) shows that the dependency-based space generally shows better correlation with human judgements. This corresponds to a beneficial effect of syntactic information found for other applications of semantic spaces (Lin, 1998; Pado and Lapata, 2007). All instances of the SELPREF model show highly significant correlations. SELPREF and SELPREF-CUT show very similar performance. They do better than both baselines in the BOW space; however, in the cleaner SYN space, their performance is numerically lower than using selectional preferences only ( = 0.13 vs. 0.16). SELPREF-POW is always significantly better than SELPREF and SELPREF-CUT, and shows the best result of all tested models ( = 0.27, BOW space). The performance is somewhat lower in the SYN space ( = 0.22). However, this difference, and the difference to the best M&amp;L model at = 0.24, a</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>S. Pado, M. Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Pado</author>
<author>F Keller</author>
<author>M W Crocker</author>
</authors>
<title>Combining syntax and thematic fit in a probabilistic model of sentence processing.</title>
<date>2006</date>
<booktitle>In Proceedings of CogSci,</booktitle>
<pages>657662</pages>
<contexts>
<context position="11582" citStr="Pado et al., 2006" startWordPosition="1882" endWordPosition="1885">hat can be performed with a ball. Similarly, the interpretation of ball will reflect the hearers expectations about typical things that can be caught. This move to include typical arguments and predicates into a model of word meaning can be motivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events for human language processing is well-established. Expectations affect reading times (McRae et al., 1998), the interpretation of participles (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pado et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and</context>
</contexts>
<marker>Pado, Keller, Crocker, 2006</marker>
<rawString>U. Pado, F. Keller, M. W. Crocker. 2006. Combining syntax and thematic fit in a probabilistic model of sentence processing. In Proceedings of CogSci, 657662.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pado</author>
<author>U Pado</author>
<author>K Erk</author>
</authors>
<title>Flexible, corpus-based modelling of human plausibility judgements.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP/CoNLL,</booktitle>
<pages>400409</pages>
<contexts>
<context position="12405" citStr="Pado et al., 2007" startWordPosition="2008" endWordPosition="2011">d in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pado et al., 2007). We first present the SVS model of word meaning 899 \x0ccatch he fielder dog cold baseball drift obj subj accuse say claim comp-1 ball whirl fly provide throw catch organise obj-1 subj-1 mod red golf elegant Figure 1: Structured meaning representations for noun ball and verb catch: lexical information plus expectations that integrates lexical information with selectional preferences. Then, we show how the SVS model provides a new way of computing meaning in context. Representing lemma meaning. We abandon the traditional choice of representing word meaning as a single vector. Instead, we encod</context>
</contexts>
<marker>Pado, Pado, Erk, 2007</marker>
<rawString>S. Pado, U. Pado, K. Erk. 2007. Flexible, corpus-based modelling of human plausibility judgements. In Proceedings of EMNLP/CoNLL, 400409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selectional constraints: An informationtheoretic model and its computational realization.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--127159</pages>
<contexts>
<context position="11899" citStr="Resnik, 1996" startWordPosition="1933" endWordPosition="1934">ral role of expectations about typical events for human language processing is well-established. Expectations affect reading times (McRae et al., 1998), the interpretation of participles (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pado et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pado et al., 2007). We first present the SVS model of word meaning 899 \x0ccatch he fielder dog cold baseball dr</context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>P. Resnik. 1996. Selectional constraints: An informationtheoretic model and its computational realization. Cognition, 61:127159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wang</author>
<author>C Yang</author>
</authors>
<title>A vector-space model for information retrieval.</title>
<date>1975</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>18--613620</pages>
<contexts>
<context position="1538" citStr="Salton et al., 1975" startWordPosition="232" endWordPosition="235">This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases. 1 Introduction Semantic spaces are a popular framework for the representation of word meaning, encoding the meaning of lemmas as high-dimensional vectors. In the default case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus. These vectors are able to provide a robust model of semantic similarity that has been used in NLP (Salton et al., 1975; McCarthy and Carroll, 2003; Manning et al., 2008) and to model experimental results in cognitive science (Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occur</context>
</contexts>
<marker>Salton, Wang, Yang, 1975</marker>
<rawString>G. Salton, A. Wang, C. Yang. 1975. A vector-space model for information retrieval. Journal of the American Society for Information Science, 18:613620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schutze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="2346" citStr="Schutze, 1998" startWordPosition="356" endWordPosition="357">because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in the literature (Smolensky, 1990; Schutze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) that compute meaning in context from lemma vectors. Most of these studies phrase the problem as one of vector composition: The meaning of a target occurrence a in context b is a single new vector c that is a function (for example, the centroid) of the vectors: c = a \x0c b. The context b can consist of as little as one word, as shown in Example (1). In (1a), the meaning of catch combined with ball is similar to grab, while in (1b), combined with disease, it can be paraphrased by contract. Conversely, verbs can influence the i</context>
<context position="6907" citStr="Schutze (1998)" startWordPosition="1134" endWordPosition="1135">phrases, finding that SVS performs at and above the state-ofthe-art. Plan of the paper. Section 2 reviews related work. Section 3 presents the SVS model for word meaning in context. Sections 4 to 6 relate experiments on the paraphrase appropriateness task. 2 Related Work In this section we give a short overview over existing vector space based approaches to computing word meaning in context. General context effects. The first category of models aims at integrating the widest possible range of context information without recourse to linguistic structure. The best-known work in this category is Schutze (1998). He first computes first-order vector representations for word meaning by collecting co-occurrence counts from the entire corpus. Then, he determines second-order vectors for individual word instances in their context, which is taken to be a simple surface window, by summing up all first-order vectors of the words in this context. The resulting vectors form sense clusters. McDonald and Brew (2004) present a similar model. They compute the expectation for a word wi in a sequence by summing the first-order vectors for the words w1 to wi1 and showed that the distance between expectation and firs</context>
</contexts>
<marker>Schutze, 1998</marker>
<rawString>H. Schutze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sharoff</author>
</authors>
<title>Open-source corpora: Using the net to fish for linguistic data.</title>
<date>2006</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>11</volume>
<issue>4</issue>
<marker>Sharoff, 2006</marker>
<rawString>S. Sharoff. 2006. Open-source corpora: Using the net to fish for linguistic data. International Journal of Corpus Linguistics, 11(4):435462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>46--159216</pages>
<contexts>
<context position="2331" citStr="Smolensky, 1990" startWordPosition="354" endWordPosition="355">s are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in the literature (Smolensky, 1990; Schutze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) that compute meaning in context from lemma vectors. Most of these studies phrase the problem as one of vector composition: The meaning of a target occurrence a in context b is a single new vector c that is a function (for example, the centroid) of the vectors: c = a \x0c b. The context b can consist of as little as one word, as shown in Example (1). In (1a), the meaning of catch combined with ball is similar to grab, while in (1b), combined with disease, it can be paraphrased by contract. Conversely, verbs can </context>
<context position="8818" citStr="Smolensky (1990)" startWordPosition="1447" endWordPosition="1448">e meaning of the combination p + a as a function f operating on four components: c = f(p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge. This framework allows sensitivity to the relation. However, the concrete instantiations that Mitchell and Lapata consider disregards K and R, thus sharing the other models limitations. They focus instead on methods for the direct combination of p and a: In a comparison between component-wise addition and multiplication of p and a, they find far superior results for the multiplication approach. Tensor product-based models. Smolensky (1990) uses tensor product to combine two word vectors a and b into a vector c representing the expression a+b. The vector c is located in a very high-dimensional space and is thus capable of encoding the structure of the expression; however, this makes the model infeasible in practice, as dimensionality rises with every word added to the representation. Jones and Mewhort (2007) represent lemma meaning by using circular convolution to encode n-gram co-occurrence information into vectors of fixed dimensionality. Similar to Brew and McDonald (2004), they predict most likely next words in a sequence, w</context>
</contexts>
<marker>Smolensky, 1990</marker>
<rawString>P. Smolensky. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence, 46:159216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>I Dagan</author>
<author>R Bar-Haim</author>
<author>J Goldberger</author>
</authors>
<title>Contextual preferences.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL, 683 691,</booktitle>
<location>Columbus, OH.</location>
<marker>Szpektor, Dagan, Bar-Haim, Goldberger, 2008</marker>
<rawString>I. Szpektor, I. Dagan, R. Bar-Haim, J. Goldberger. 2008. Contextual preferences. In Proceedings of ACL, 683 691, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>Preference semantics. In Formal Semantics of Natural Language.</title>
<date>1975</date>
<publisher>CUP.</publisher>
<contexts>
<context position="11845" citStr="Wilks, 1975" startWordPosition="1925" endWordPosition="1926">and linguistic grounds. In cognitive science, the central role of expectations about typical events for human language processing is well-established. Expectations affect reading times (McRae et al., 1998), the interpretation of participles (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pado et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pado et al., 2007). We first present the SVS model of word</context>
</contexts>
<marker>Wilks, 1975</marker>
<rawString>Y. Wilks. 1975. Preference semantics. In Formal Semantics of Natural Language. CUP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceeedings of COLING,</booktitle>
<pages>947953</pages>
<contexts>
<context position="22801" citStr="Yeh, 2000" startWordPosition="3751" endWordPosition="3752">we use bs selectional preference vector for the given relation as a second baseline, selpref only. 3 Since we focus on the size-invariant cosine similarity, the use of this model does not require normalization. verb subject landmark sim judgment slump shoulder slouch high 7 slump shoulder decline low 2 slump value slouch low 3 slump value decline high 7 Figure 3: Experiment 1: Human similarity judgements for subject-verb pair with high- and low-similarity landmarks Differences between the performance of models were tested for significance using a stratified shuffling-based randomization test (Yeh, 2000).4. 5 Exp. 1: Predicting similarity ratings In our first experiment, we attempt to predict human similarity judgments. This experiment is a replication of the evaluation of M&amp;L on their dataset5. Dataset. The M&amp;L dataset comprises a total of 3,600 human similarity judgements for 120 experimental items. Each item, as shown in Figure 3, consists of an intransitive verb and a subject noun that are combined with a landmark, a synonym of the verb that is chosen to be either similar or dissimilar to the verb in the context of the given subject. The dataset was constructed by extracting pairs of subj</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>A. Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceeedings of COLING, 947953.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>