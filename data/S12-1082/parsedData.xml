<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<reference confidence="0.20202">
b&apos;First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565570,
Montreal, Canada, June 7-8, 2012. c
2012 Association for Computational Linguistics
DeepPurple: Estimating Sentence Semantic Similarity using
N-gram Regression Models and Web Snippets
</reference>
<author confidence="0.98185">
Nikos Malandrakis, Elias Iosif, Alexandros Potamianos
</author>
<affiliation confidence="0.995847">
Department of ECE, Technical University of Crete, 73100 Chania, Greece
</affiliation>
<email confidence="0.987717">
[nmalandrakis,iosife,potam]@telecom.tuc.gr
</email>
<sectionHeader confidence="0.990413" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999751470588235">
We estimate the semantic similarity between
two sentences using regression models with
features: 1) n-gram hit rates (lexical matches)
between sentences, 2) lexical semantic sim-
ilarity between non-matching words, and 3)
sentence length. Lexical semantic similarity is
computed via co-occurrence counts on a cor-
pus harvested from the web using a modified
mutual information metric. State-of-the-art re-
sults are obtained for semantic similarity com-
putation at the word level, however, the fusion
of this information at the sentence level pro-
vides only moderate improvement on Task 6
of SemEval12. Despite the simple features
used, regression models provide good perfor-
mance, especially for shorter sentences, reach-
ing correlation of 0.62 on the SemEval test set.
</bodyText>
<sectionHeader confidence="0.998329" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9991898125">
Recently, there has been significant research activ-
ity on the area of semantic similarity estimation
motivated both by abundance of relevant web data
and linguistic resources for this task. Algorithms
for computing semantic textual similarity (STS) are
relevant for a variety of applications, including in-
formation extraction (Szpektor and Dagan, 2008),
question answering (Harabagiu and Hickl, 2006)
and machine translation (Mirkin et al., 2009). Word-
or term-level STS (a special case of sentence level
STS) has also been successfully applied to the prob-
lem of grammar induction (Meng and Siu, 2002)
and affective text categorization (Malandrakis et al.,
2011). In this work, we built on previous research
on word-level semantic similarity estimation to de-
sign and implement a system for sentence-level STS
for Task6 of the SemEval12 campaign.
Semantic similarity between words can be re-
garded as the graded semantic equivalence at the
lexeme level and is tightly related with the tasks of
word sense discovery and disambiguation (Agirre
and Edmonds, 2007). Metrics of word semantic sim-
ilarity can be divided into: (i) knowledge-based met-
rics (Miller, 1990; Budanitsky and Hirst, 2006) and
(ii) corpus-based metrics (Baroni and Lenci, 2010;
Iosif and Potamianos, 2010).
When more complex structures, such as phrases
and sentences, are considered, it is much harder
to estimate semantic equivalence due to the non-
compositional nature of sentence-level semantics
and the exponential explosion of possible interpre-
tations. STS is closely related to the problems of
paraphrasing, which is bidirectional and based on
semantic equivalence (Madnani and Dorr, 2010) and
textual entailment, which is directional and based
on relations between semantics (Dagan et al., 2006).
Related methods incorporate measurements of sim-
ilarity at various levels: lexical (Malakasiotis and
Androutsopoulos, 2007), syntactic (Malakasiotis,
2009; Zanzotto et al., 2009), and semantic (Rinaldi
et al., 2003; Bos and Markert, 2005). Measures
from machine translation evaluation are often used
to evaluate lexical level approaches (Finch et al.,
2005; Perez and Alfonseca, 2005), including BLEU
(Papineni et al., 2002), a metric based on word n-
gram hit rates.
Motivated by BLEU, we use n-gram hit rates and
word-level semantic similarity scores as features in
</bodyText>
<page confidence="0.995681">
565
</page>
<bodyText confidence="0.834709">
\x0ca linear regression model to estimate sentence level
semantic similarity. We also propose sigmoid scal-
ing of similarity scores and sentence-length depen-
dent modeling. The models are evaluated on the Se-
mEval12 sentence similarity task.
</bodyText>
<sectionHeader confidence="0.571774" genericHeader="method">
2 Semantic similarity between words
</sectionHeader>
<bodyText confidence="0.999230028571429">
In this section, two different metrics of word simi-
larity are presented. The first is a language-agnostic,
corpus-based metric requiring no knowledge re-
sources, while the second metric relies on WordNet.
Corpus-based metric: Given a corpus, the se-
mantic similarity between two words, wi and wj,
is estimated as their pointwise mutual information
(Church and Hanks, 1990): I(i, j) = log p(i,j)
p(i)p(j),
where p(i) and p(j) are the occurrence probabili-
ties of wi and wj, respectively, while the probability
of their co-occurrence is denoted by p(i, j). These
probabilities are computed according to maximum
likelihood estimation. The assumption of this met-
ric is that co-occurrence implies semantic similarity.
During the past decade the web has been used for
estimating the required probabilities (Turney, 2001;
Bollegala et al., 2007), by querying web search en-
gines and retrieving the number of hits required
to estimate the frequency of individual words and
their co-occurrence. However, these approaches
have failed to obtain state-of-the-art results (Bolle-
gala et al., 2007), unless expensive conjunctive
AND queries are used for harvesting a corpus and
then using this corpus to estimate similarity scores
(Iosif and Potamianos, 2010).
Recently, a scalable approach1 for harvesting a
corpus has been proposed where web snippets are
downloaded using individual queries for each word
(Iosif and Potamianos, 2012b). Semantic similar-
ity can then be estimated using the I(i, j) metric
and within-snippet word co-occurrence frequencies.
Under the maximum sense similarity assumption
(Resnik, 1995), it is relatively easy to show that a
(more) lexically-balanced corpus2 (as the one cre-
</bodyText>
<page confidence="0.936848">
1
</page>
<bodyText confidence="0.996893333333333">
The scalability of this approach has been demonstrated in
(Iosif and Potamianos, 2012b) for a 10K vocabulary, here we
extend it to the full 60K WordNet vocabulary.
</bodyText>
<page confidence="0.980906">
2
</page>
<bodyText confidence="0.9979204">
According to this assumption the semantic similarity of two
words can be estimated as the minimum pairwise similarity of
their senses. The gist of the argument is that although words
often co-occur with their closest senses, word occurrences cor-
ated above) can significantly reduce the semantic
similarity estimation error of the mutual information
metric I(i, j). This is also experimentally verified in
(Iosif and Potamianos, 2012c).
In addition, one can modify the mutual informa-
tion metric to further reduce estimation error (for
the theoretical foundation behind this see (Iosif and
Potamianos, 2012a)). Specifically, one may intro-
duce exponential weights in order to reduce the
contribution of p(i) and p(j) in the similarity met-
ric. The modified metric Ia(i, j), is defined as:
</bodyText>
<equation confidence="0.927399916666667">
Ia(i, j)=
1
2
\x14
log
p(i, j)
p(i)p(j)
+ log
p(i, j)
p(i)p(j)
\x15
. (1)
</equation>
<bodyText confidence="0.9953807">
The weight was estimated on the corpus of (Iosif
and Potamianos, 2012b) in order to maximize word
sense coverage in the semantic neighborhood of
each word. The Ia(i, j) metric using the estimated
value of = 0.8 was shown to significantly out-
perform I(i, j) and to achieve state-of-the-art results
on standard semantic similarity datasets (Rubenstein
and Goodenough, 1965; Miller and Charles, 1998;
Finkelstein et al., 2002). For more details see (Iosif
and Potamianos, 2012a).
WordNet-based metrics: For comparison pur-
poses, we evaluated various similarity metrics on
the task of word similarity computation on three
standard datasets (same as above). The best re-
sults were obtained by the Vector metric (Patward-
han and Pedersen, 2006), which exploits the lexical
information that is included in the WordNet glosses.
This metric was incorporated to our proposed ap-
proach. All metrics were computed using the Word-
Net::Similarity module (Pedersen, 2005).
</bodyText>
<sectionHeader confidence="0.972393" genericHeader="method">
3 N-gram Regression Models
</sectionHeader>
<bodyText confidence="0.99543625">
Inspired by BLEU (Papineni et al., 2002), we pro-
pose a simple regression model that combines evi-
dence from two sources: number of n-gram matches
and degree of similarity between non-matching
words between two sentences. In order to incorpo-
rate a word semantic similarity metric into BLEU,
we apply the following two-pass process: first lexi-
cal hits are identified and counted, and then the se-
mantic similarity between n-grams not matched dur-
respond to all senses, i.e., the denominator of I(i, j) is overes-
timated causing large underestimation error for similarities be-
tween polysemous words.
</bodyText>
<page confidence="0.994941">
566
</page>
<bodyText confidence="0.97585565">
\x0cing the first pass is estimated. All word similar-
ity metrics used are peak-to-peak normalized in the
[0,1] range, so they serve as a degree-of-match.
The semantic similarity scores from word pairs are
summed together (just like n-gram hits) to obtain
a BLEU-like semantic similarity score. The main
problem here is one of alignment, since we need
to compare each non-matched n-gram from the hy-
pothesis with an n-gram from the reference. We
use a simple approach: we iterate on the hypoth-
esis n-grams, left-to-right, and compare each with
the most similar non-matched n-gram in the refer-
ence. This modification to BLEU is only applied
to 1-grams, since semantic similarity scores for bi-
grams (or higher) were not available.
Thus, our list of features are the hit rates obtained
by BLEU (for 1-, 2-, 3-, 4-grams) and the total se-
mantic similarity (SS) score for 1-grams3. These
features are then combined using a multiple linear
regression model:
</bodyText>
<equation confidence="0.9873586">
DL = a0 +
4
X
n=1
an Bn + a5 M1, (2)
</equation>
<bodyText confidence="0.996421071428572">
where DL is the estimated similarity, Bn is the
BLEU hit rate for n-grams, M1 is the total semantic
similarity score (SS) for non-matching 1-grams and
an are the trainable parameters of the model.
Motivated by evidence of cognitive scaling of
semantic similarity scores (Iosif and Potamianos,
2010), we propose the use of a sigmoid function to
scale DL sentence similarities. We have also ob-
served in the SemEval data that the way humans rate
sentence similarity is very much dependent on sen-
tence length4. To capture the effect of length and
cognitive scaling we propose next two modifications
to the linear regression model. The sigmoid fusion
scheme is described by the following equation:
</bodyText>
<equation confidence="0.999729375">
DS = a6DL + a7DL
\x14
1 + exp
\x12
a8 l
a9
\x13\x151
, (3)
</equation>
<bodyText confidence="0.997554">
where we assume that sentence length l (average
</bodyText>
<page confidence="0.972087">
3
</page>
<bodyText confidence="0.999702333333333">
Note that the features are computed twice on each sentence
in a forward and backward fashion (where the word order is
reversed), and then averaged between the two runs.
</bodyText>
<page confidence="0.962991">
4
</page>
<bodyText confidence="0.994187055555555">
We speculate that shorter sentences are mostly compared at
the lexical level using the short-term memory language buffers,
while longer sentences tend to be compared at a higher cogni-
tive level, where the non-compositional nature of sentence se-
mantics dominate.
length for each sentence pair, in words) acts as a
scaling factor for the linearly estimated similarity.
The hierarchical fusion scheme is actually a col-
lection of (overlapping) linear regression models,
each matching a range of sentence lengths. For ex-
ample, the first model DL1 is trained with sentences
with length up to l1, i.e., l l1, the second model
DL2 up to length l2 etc. During testing, sentences
with length l [1, l1] are decoded with DL1, sen-
tences with length l (l1, l2] with model DL2 etc.
Each of these partial models is a linear fusion model
as shown in (2). In this work, we use four models
with l1 = 10, l2 = 20, l3 = 30, l4 = .
</bodyText>
<sectionHeader confidence="0.9912" genericHeader="method">
4 Experimental Procedure and Results
</sectionHeader>
<bodyText confidence="0.99765328125">
Initially all sentences are pre-processed by the
CoreNLP (Finkel et al., 2005; Toutanova et al.,
2003) suite of tools, a process that includes named
entity recognition, normalization, part of speech tag-
ging, lemmatization and stemming. The exact type
of pre-processing used depends on the metric used.
For the plain lexical BLEU, we use lemmatization,
stemming (of lemmas) and remove all non-content
words, keeping only nouns, adjectives, verbs and ad-
verbs. For computing semantic similarity scores, we
dont use stemming and keep only noun words, since
we only have similarities between non-noun words.
For the computation of semantic similarity we have
created a dictionary containing all the single-word
nouns included in WordNet (approx. 60K) and then
downloaded snippets of the 500 top-ranked docu-
ments for each word by formulating single-word
queries and submitting them to the Yahoo! search
engine.
Next, results are reported in terms of correlation
between the automatically computed scores and the
ground truth, for each of the corpora in Task 6 of
SemEval12 (paraphrase, video, europarl, WordNet,
news). Overall correlation (Ovrl) computed on the
join of the dataset, as well as, average (Mean) cor-
relation across all task is also reported. Training is
performed on a subset of the first three corpora and
testing on all five corpora.
Baseline BLEU: The first set of results in Ta-
ble 1, shows the correlation performance of the
plain BLEU hit rates (per training data set and over-
all/average). The best performing hit rate is the one
</bodyText>
<page confidence="0.997795">
567
</page>
<tableCaption confidence="0.9508255">
\x0ccalculated using unigrams.
Table 1: Correlation performance of BLEU hit rates.
</tableCaption>
<table confidence="0.944213666666667">
par vid euro Mean Ovrl
BLEU 1-grams 0.62 0.67 0.49 0.59 0.57
BLEU 2-grams 0.40 0.39 0.37 0.39 0.34
BLEU 3-grams 0.32 0.36 0.30 0.33 0.33
BLEU 4-grams 0.26 0.25 0.24 0.25 0.28
Semantic Similarity BLEU (Purple): The perfor-
</table>
<bodyText confidence="0.992018416666667">
mance of the modified version of BLEU that in-
corporates various word-level similarity metrics is
shown in Table 2. Here the BLEU hits (exact
matches) are summed together with the normalized
similarity scores (approximate matches) to obtain a
single B1+M1 (Purple) score5. As we can see, there
are definite benefits to using the modified version,
particularly with regards to mean correlation. Over-
all the best performers, when taking into account
both mean and overall correlation, are the WordNet-
based and Ia metrics, with the Ia metric winning by
a slight margin, earning a place in the final models.
</bodyText>
<tableCaption confidence="0.9140225">
Table 2: Correlation performance of 1-gram BLEU
scores with semantic similarity metrics (nouns-only).
</tableCaption>
<table confidence="0.942451666666667">
par vid euro Mean Ovrl
BLEU 0.54 0.60 0.39 0.51 0.58
SS-BLEU WordNet 0.56 0.64 0.41 0.54 0.58
SS-BLEU I(i, j) 0.56 0.63 0.39 0.53 0.59
SS-BLEU Ia(i, j) 0.57 0.64 0.40 0.54 0.58
Regression models (DeepPurple): Next, the per-
</table>
<bodyText confidence="0.99474725">
formance of the various regression models (fusion
schemes) is investigated. Each regression model is
evaluated by performing 10-fold cross-validation on
the SemEval training set. Correlation performance
is shown in Table 3 both with and without seman-
tic similarity. The baseline in this case is the Pur-
ple metric (corresponding to no fusion). Clearly
the use of regression models significantly improves
performance compared to the 1-gram BLEU and
Purple baselines for almost all datasets, and espe-
cially for the combined dataset (overall). Among
the fusion schemes, the hierarchical models perform
the best. Following fusion, the performance gain
from incorporating semantic similarity (SS) is much
smaller. Finally, in Table 4, correlation performance
of our submissions on the official SemEval test set is
</bodyText>
<page confidence="0.965733">
5
</page>
<bodyText confidence="0.997126333333333">
It should be stressed that the plain BLEU unigram scores
shown in this table are not comparable to those in Table 1, since
here scores are calculated over only the nouns of each sentence.
</bodyText>
<tableCaption confidence="0.927511333333333">
Table 3: Correlation performance of regression model
with (SS) and without semantic similarities on the train-
ing set (using 10-fold cross-validation).
</tableCaption>
<table confidence="0.95812525">
par vid euro Mean Ovrl
None (SS-BLEU Ia) 0.57 0.64 0.40 0.54 0.58
Linear (DL, a5 =0) 0.62 0.72 0.47 0.60 0.66
Sigmoid (DS, a5 =0) 0.64 0.73 0.42 0.60 0.73
Hierarchical 0.64 0.74 0.48 0.62 0.73
SS-Linear (DL) 0.64 0.73 0.47 0.61 0.66
SS-Sigmoid (DS) 0.65 0.74 0.42 0.60 0.74
SS-Hierarchical 0.65 0.74 0.48 0.62 0.73
</table>
<bodyText confidence="0.99812">
shown. The overall correlation performance of the
Hierarchical model ranks somewhere in the middle
(43rd out of 89 systems), while the mean correla-
tion (weighted by number of samples per set) is no-
tably better: 23rd out of 89. Comparing the individ-
ual dataset results, our systems underperform for the
two datasets that originate from the machine transla-
tion (MT) literature (and contain longer sentences),
while we achieve good results for the rest (19th for
paraphrase, 37th for video and 29th for WN).
</bodyText>
<tableCaption confidence="0.995767">
Table 4: Correlation performance on test set.
</tableCaption>
<table confidence="0.87042525">
par vid euro WN news Mean Ovrl
None 0.50 0.71 0.44 0.49 0.24 0.51 0.49
Sigm. 0.60 0.76 0.26 0.60 0.34 0.56 0.55
Hier. 0.60 0.77 0.43 0.65 0.37 0.60 0.62
</table>
<sectionHeader confidence="0.991981" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999688894736842">
We have shown that: 1) a regression model that
combines counts of exact and approximate n-gram
matches provides good performance for sentence
similarity computation (especially for short and
medium length sentences), 2) the non-linear scal-
ing of hit-rates with respect to sentence length im-
proves performance, 3) incorporating word semantic
similarity scores (soft-match) into the model can im-
prove performance, and 4) web snippet corpus cre-
ation and the modified mutual information metric
is a language agnostic approach that can (at least)
match semantic similarity performance of the best
resource-based metrics for this task. Future work,
should involve the extension of this approach to
model larger lexical chunks, the incorporation of
compositional models of meaning, and in general
the phrase-level modeling of semantic similarity, in
order to compete with MT-based systems trained on
massive external parallel corpora.
</bodyText>
<page confidence="0.986161">
568
</page>
<bodyText confidence="0.631654675">
\x0cReferences
E. Agirre and P. Edmonds, editors. 2007. Word
Sense Disambiguation: Algorithms and Applications.
Springer.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673721.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using web
search engines. In Proc. of International Conference
on World Wide Web, pages 757766.
J. Bos and K. Markert. 2005. Recognising textual en-
tailment with logical inference. In Proceedings of the
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, page 628635.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32:1347.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):2229.
I. Dagan, O. Glickman, and B. Magnini. 2006.
The pascal recognising textual entailment challenge.
In Joaquin Quionero-Candela, Ido Dagan, Bernardo
Magnini, and Florence dAlch Buc, editors, Machine
Learning Challenges. Evaluating Predictive Uncer-
tainty, Visual Object Classification, and Recognising
Tectual Entailment, volume 3944 of Lecture Notes in
Computer Science, pages 177190. Springer Berlin /
Heidelberg.
A. Finch, S. Y. Hwang, and E. Sumita. 2005. Using ma-
chine translation evaluation techniques to determine
sentence-level semantic equivalence. In Proceedings
of the 3rd International Workshop on Paraphrasing,
page 1724.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
</bodyText>
<reference confidence="0.888871876923077">
tional Linguistics, pages 363370.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions on Information Systems, 20(1):116131.
S. Harabagiu and A. Hickl. 2006. Methods for Us-
ing Textual Entailment in Open-Domain Question An-
swering. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 905912.
E. Iosif and A. Potamianos. 2010. Unsupervised seman-
tic similarity computation between terms using web
documents. IEEE Transactions on Knowledge and
Data Engineering, 22(11):16371647.
E. Iosif and A. Potamianos. 2012a. Minimum error se-
mantic similarity using text corpora constructed from
web queries. IEEE Transactions on Knowledge and
Data Engineering (submitted to).
E. Iosif and A. Potamianos. 2012b. Semsim: Resources
for normalized semantic similarity computation using
lexical networks. Proc. of Eighth International Con-
ference on Language Resources and Evaluation (to ap-
pear).
E. Iosif and A. Potamianos. 2012c. Similarity com-
putation using semantic networks created from web-
harvested data. Natural Language Engineering (sub-
mitted to).
N. Madnani and B. J. Dorr. 2010. Generating phrasal and
sentential paraphrases: A survey of data-driven meth-
ods. Computational Linguistics, 36(3):341387.
P. Malakasiotis and I. Androutsopoulos. 2007. Learn-
ing textual entailment using svms and string similar-
ity measures. In Proceedings of of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 4247.
P. Malakasiotis. 2009. Paraphrase recognition using ma-
chine learning to combine similarity measures. In Pro-
ceedings of the 47th Annual Meeting of ACL and the
4th Int. Joint Conference on Natural Language Pro-
cessing of AFNLP, pages 4247.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
29772980.
H. Meng and K.-C. Siu. 2002. Semi-automatic acquisi-
tion of semantic structures for understanding domain-
specific natural language queries. IEEE Transactions
on Knowledge and Data Engineering, 14(1):172181.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):128.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235312.
S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-
man, and S. Idan. 2009. Source-language entailment
modeling for translating unknown terms. In Proceed-
ings of the 47th Annual Meeting of ACL and the 4th Int.
Joint Conference on Natural Language Processing of
AFNLP, pages 791799.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311318.
</reference>
<page confidence="0.959344">
569
</page>
<reference confidence="0.993850175">
\x0cS. Patwardhan and T. Pedersen. 2006. Using WordNet-
based context vectors to estimate the semantic related-
ness of concepts. In Proc. of the EACL Workshop on
Making Sense of Sense: Bringing Computational Lin-
guistics and Psycholinguistics Together, pages 18.
T. Pedersen. 2005. WordNet::Similarity.
http://search.cpan.org/dist/
WordNet-Similarity/.
D. Perez and E. Alfonseca. 2005. Application of the
bleu algorithm for recognizing textual entailments. In
Proceedings of the PASCAL Challenges Worshop on
Recognising Textual Entailment.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxanomy. In Proc. of In-
ternational Joint Conference for Artificial Intelligence,
pages 448453.
F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and
D. Molla. 2003. Exploiting paraphrases in a question
answering system. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing, pages 2532.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627633.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 849856.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology, pages 173180.
P. D. Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In Proc. of the European
Conference on Machine Learning, pages 491502.
F. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine-learning approach to textual en-
tailment recognition. Natural Language Engineering,
15(4):551582.
</reference>
<page confidence="0.676579">
570
</page>
<figure confidence="0.45604">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.558600">
<note confidence="0.846250666666667">b&apos;First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565570, Montreal, Canada, June 7-8, 2012. c 2012 Association for Computational Linguistics</note>
<title confidence="0.9770185">DeepPurple: Estimating Sentence Semantic Similarity using N-gram Regression Models and Web Snippets</title>
<author confidence="0.990083">Nikos Malandrakis</author>
<author confidence="0.990083">Elias Iosif</author>
<author confidence="0.990083">Alexandros Potamianos</author>
<affiliation confidence="0.889567">Department of ECE, Technical University of Crete, 73100 Chania, Greece</affiliation>
<email confidence="0.98119">[nmalandrakis,iosife,potam]@telecom.tuc.gr</email>
<abstract confidence="0.994950555555555">We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, and 3) sentence length. Lexical semantic similarity is computed via co-occurrence counts on a corpus harvested from the web using a modified mutual information metric. State-of-the-art results are obtained for semantic similarity computation at the word level, however, the fusion of this information at the sentence level provides only moderate improvement on Task 6 of SemEval12. Despite the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the SemEval test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>c 2012 Association for Computational Linguistics DeepPurple: Estimating Sentence Semantic Similarity using N-gram Regression Models and Web Snippets tional Linguistics,</title>
<date>2012</date>
<booktitle>b&apos;First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>565570</pages>
<location>Montreal, Canada,</location>
<marker>2012</marker>
<rawString>b&apos;First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565570, Montreal, Canada, June 7-8, 2012. c 2012 Association for Computational Linguistics DeepPurple: Estimating Sentence Semantic Similarity using N-gram Regression Models and Web Snippets tional Linguistics, pages 363370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>A Hickl</author>
</authors>
<title>Methods for Using Textual Entailment in Open-Domain Question Answering.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>905912</pages>
<marker>Harabagiu, Hickl, 2006</marker>
<rawString>S. Harabagiu and A. Hickl. 2006. Methods for Using Textual Entailment in Open-Domain Question Answering. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 905912.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Iosif</author>
<author>A Potamianos</author>
</authors>
<title>Unsupervised semantic similarity computation between terms using web documents.</title>
<date>2010</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>22</volume>
<issue>11</issue>
<marker>Iosif, Potamianos, 2010</marker>
<rawString>E. Iosif and A. Potamianos. 2010. Unsupervised semantic similarity computation between terms using web documents. IEEE Transactions on Knowledge and Data Engineering, 22(11):16371647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Iosif</author>
<author>A Potamianos</author>
</authors>
<title>Minimum error semantic similarity using text corpora constructed from web queries.</title>
<date>2012</date>
<journal>IEEE Transactions on Knowledge and Data Engineering</journal>
<note>(submitted to).</note>
<marker>Iosif, Potamianos, 2012</marker>
<rawString>E. Iosif and A. Potamianos. 2012a. Minimum error semantic similarity using text corpora constructed from web queries. IEEE Transactions on Knowledge and Data Engineering (submitted to).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Iosif</author>
<author>A Potamianos</author>
</authors>
<title>Semsim: Resources for normalized semantic similarity computation using lexical networks.</title>
<date>2012</date>
<booktitle>Proc. of Eighth International Conference on Language Resources and Evaluation</booktitle>
<note>(to appear).</note>
<marker>Iosif, Potamianos, 2012</marker>
<rawString>E. Iosif and A. Potamianos. 2012b. Semsim: Resources for normalized semantic similarity computation using lexical networks. Proc. of Eighth International Conference on Language Resources and Evaluation (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Iosif</author>
<author>A Potamianos</author>
</authors>
<title>Similarity computation using semantic networks created from webharvested data. Natural Language Engineering</title>
<date>2012</date>
<note>(submitted to).</note>
<marker>Iosif, Potamianos, 2012</marker>
<rawString>E. Iosif and A. Potamianos. 2012c. Similarity computation using semantic networks created from webharvested data. Natural Language Engineering (submitted to).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>B J Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of data-driven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<marker>Madnani, Dorr, 2010</marker>
<rawString>N. Madnani and B. J. Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of data-driven methods. Computational Linguistics, 36(3):341387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Malakasiotis</author>
<author>I Androutsopoulos</author>
</authors>
<title>Learning textual entailment using svms and string similarity measures.</title>
<date>2007</date>
<booktitle>In Proceedings of of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>4247</pages>
<marker>Malakasiotis, Androutsopoulos, 2007</marker>
<rawString>P. Malakasiotis and I. Androutsopoulos. 2007. Learning textual entailment using svms and string similarity measures. In Proceedings of of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 4247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Malakasiotis</author>
</authors>
<title>Paraphrase recognition using machine learning to combine similarity measures.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of ACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP,</booktitle>
<pages>4247</pages>
<marker>Malakasiotis, 2009</marker>
<rawString>P. Malakasiotis. 2009. Paraphrase recognition using machine learning to combine similarity measures. In Proceedings of the 47th Annual Meeting of ACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP, pages 4247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Malandrakis</author>
<author>A Potamianos</author>
<author>E Iosif</author>
<author>S Narayanan</author>
</authors>
<title>Kernel models for affective lexicon creation.</title>
<date>2011</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>29772980</pages>
<marker>Malandrakis, Potamianos, Iosif, Narayanan, 2011</marker>
<rawString>N. Malandrakis, A. Potamianos, E. Iosif, and S. Narayanan. 2011. Kernel models for affective lexicon creation. In Proc. Interspeech, pages 29772980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Meng</author>
<author>K-C Siu</author>
</authors>
<title>Semi-automatic acquisition of semantic structures for understanding domainspecific natural language queries.</title>
<date>2002</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>14</volume>
<issue>1</issue>
<marker>Meng, Siu, 2002</marker>
<rawString>H. Meng and K.-C. Siu. 2002. Semi-automatic acquisition of semantic structures for understanding domainspecific natural language queries. IEEE Transactions on Knowledge and Data Engineering, 14(1):172181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>W Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1998</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<marker>Miller, Charles, 1998</marker>
<rawString>G. Miller and W. Charles. 1998. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<marker>Miller, 1990</marker>
<rawString>G. Miller. 1990. Wordnet: An on-line lexical database. International Journal of Lexicography, 3(4):235312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mirkin</author>
<author>L Specia</author>
<author>N Cancedda</author>
<author>I Dagan</author>
<author>M Dymetman</author>
<author>S Idan</author>
</authors>
<title>Source-language entailment modeling for translating unknown terms.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of ACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP,</booktitle>
<pages>791799</pages>
<marker>Mirkin, Specia, Cancedda, Dagan, Dymetman, Idan, 2009</marker>
<rawString>S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymetman, and S. Idan. 2009. Source-language entailment modeling for translating unknown terms. In Proceedings of the 47th Annual Meeting of ACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP, pages 791799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311318</pages>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patwardhan</author>
<author>T Pedersen</author>
</authors>
<title>Using WordNetbased context vectors to estimate the semantic relatedness of concepts.</title>
<date>2006</date>
<booktitle>In Proc. of the EACL Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together,</booktitle>
<pages>18</pages>
<marker>Patwardhan, Pedersen, 2006</marker>
<rawString>\x0cS. Patwardhan and T. Pedersen. 2006. Using WordNetbased context vectors to estimate the semantic relatedness of concepts. In Proc. of the EACL Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together, pages 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<date>2005</date>
<note>WordNet::Similarity. http://search.cpan.org/dist/ WordNet-Similarity/.</note>
<marker>Pedersen, 2005</marker>
<rawString>T. Pedersen. 2005. WordNet::Similarity. http://search.cpan.org/dist/ WordNet-Similarity/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Perez</author>
<author>E Alfonseca</author>
</authors>
<title>Application of the bleu algorithm for recognizing textual entailments.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Worshop on Recognising Textual Entailment.</booktitle>
<marker>Perez, Alfonseca, 2005</marker>
<rawString>D. Perez and E. Alfonseca. 2005. Application of the bleu algorithm for recognizing textual entailments. In Proceedings of the PASCAL Challenges Worshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxanomy.</title>
<date>1995</date>
<booktitle>In Proc. of International Joint Conference for Artificial Intelligence,</booktitle>
<pages>448453</pages>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxanomy. In Proc. of International Joint Conference for Artificial Intelligence, pages 448453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rinaldi</author>
<author>J Dowdall</author>
<author>K Kaljurand</author>
<author>M Hess</author>
<author>D Molla</author>
</authors>
<title>Exploiting paraphrases in a question answering system.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2nd International Workshop on Paraphrasing,</booktitle>
<pages>2532</pages>
<marker>Rinaldi, Dowdall, Kaljurand, Hess, Molla, 2003</marker>
<rawString>F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and D. Molla. 2003. Exploiting paraphrases in a question answering system. In Proceedings of the 2nd International Workshop on Paraphrasing, pages 2532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H. Rubenstein and J. B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>I Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>849856</pages>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>I. Szpektor and I. Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 849856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>173180</pages>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 173180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Mining the web for synonyms: PMIIR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proc. of the European Conference on Machine Learning,</booktitle>
<pages>491502</pages>
<marker>Turney, 2001</marker>
<rawString>P. D. Turney. 2001. Mining the web for synonyms: PMIIR versus LSA on TOEFL. In Proc. of the European Conference on Machine Learning, pages 491502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Zanzotto</author>
<author>M Pennacchiotti</author>
<author>A Moschitti</author>
</authors>
<title>A machine-learning approach to textual entailment recognition.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>4</issue>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2009</marker>
<rawString>F. Zanzotto, M. Pennacchiotti, and A. Moschitti. 2009. A machine-learning approach to textual entailment recognition. Natural Language Engineering, 15(4):551582.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>