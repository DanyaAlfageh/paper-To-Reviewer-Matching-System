Examples include model scores and word posterior probabilities (WPP) CITATION,,
3.1 Binary Indicators MTranslatability CITATION gives a notion of the structural complexity of a sentence that relates to the quality of the produced translation,,
2 Resources The organizers have made available a baseline QE system that consists of a number of well established features CITATION and serves as a starting point for development,,
# sentences europarl-nc 1,714,385 train 1,832 test 422 Table 1: Corpus statistics 3 Features In the literature CITATION a large number of features have been considered for confidence estimation,,
1 using a 2.66 GHz Intel Xeon and 2 threads Our second approach to reduce dimensionality uses the hashing trick CITATION: a hash function is applied to each word and the sentence is represented by the hashed values which are again transformed using vector space model as above,,
The resulting vector representation closely resembles a Bloom Filter CITATION,,
They often resemble the language 91 \x0cmodel used in the noisy channel formulation CITATION but can also pinpoint more specific issues,,
Engine features are often referred to as glass box features CITATION,,
Examples include model scores and word posterior probabilities (WPP) CITATION,,
We used the Stanford NER Tagger CITATION to detect words that belong to one of four groups: Person, Location, Organization and Misc,,
3.3 Backoff Behavior In related work CITATION the backoff behavior of a 3-gram LM was found to be the most powerful feature for word level QE,,
3.4 Discriminative Word Lexicon Following the approach of CITATION we train log-linear binary classifiers that directly model p(e|fJ 1 ) for each word e eI 1: p(e|fJ 1 ) = exp \x10P ffJ 1 e,f \x11 1 + exp \x10P ffJ 1 e,f \x11 (1) where e,f are the trained model weights,,
3.3 Backoff Behavior In related work CITATION the backoff behavior of a 3-gram LM was found to be the most powerful feature for word level QE,,
3.4 Discriminative Word Lexicon Following the approach of CITATION we train log-linear binary classifiers that directly model p(e|fJ 1 ) for each word e eI 1: p(e|fJ 1 ) = exp \x10P ffJ 1 e,f \x11 1 + exp \x10P ffJ 1 e,f \x11 (1) w,,
They often resemble the language 91 \x0cmodel used in the noisy channel formulation CITATION but can also pinpoint more specific issues,,
Engine features are often referred to as glass box features CITATION,,
Examples include model scores and word posterior probabilities (WPP) CITATION,,
They often resemble the language 91 \x0cmodel used in the noisy channel formulation CITATION but can also pinpoint more specific issues,,
Engine features are often referred to as glass box features CITATION,,
Examples include model scores and word posterior probabilities (WPP) CITATION,,
3.1 Binary Indicators MTranslatability CITATION gives a notion of the structural complexity of a sentence that relates to the quality of the produced translation,,
1 using a 2.66 GHz Intel Xeon and 2 threads Our second approach to reduce dimensionality uses the hashing trick CITATION: a hash function is applied to each word and the sentence is represented by the hashed values which are again transformed using vector space model as above,,
The resulting vector representation closely resembles a Bloom Filter CITATION,,
