<<<<<<< HEAD
	coring tree according to a scoring function that decomposes under an exhaustive search CITATION,,0
Using only a subset of the between-tags as features has been shown to improve speed but impair parser performance CITATION,,0
One can interpret the learning objective as minimizing regularized, weighted loss: min w 1 2 ||w||2 + C1 X i:yi=1 l(w, yi, xi) +C2 X i:yi=1 l(w, yi, xi) (1) where l() is the learning methods loss function, xi and yi are the features and label for the ith 3 Learning with a cost model is generally preferable to first optimizing error rate and then thresholding the prediction values to select a high-confidence subset CITATION, but the latter approach was used successfully for cell classification in CITATION,,0
CITATION use coarse-to-fine pruning with dependency parsing, but in that case, a graphbased dependency p,,0
We solve the SVM objective using LIBLINEAR CITATION,,0
2nd-order searches, which consider two siblings at a time, are available with no increase in asymptotic complexity (CITATION; CITATION),,0
This could allow extremely large feature sets CITATION, or the look-up of expensive corpus-based features such as word-pair mutual information CITATION,,0
Its features are a mixture of those described by CITATION, and those used in the CITATION baseline system; we do not use word-cluster features,,0
en shown to improve speed but impair parser performance CITATION,,0
3 Related Work 3.1 Vine Parsing Filtering dependency arcs has been explored primarily in the form of vine parsing (CITATION; CITATION),,0
DepPercep We also test an in-house dependency parser, which conducts projective first and 2nd-order searches using the split-head CFG described by CITATION, with a weight vector trained using an averaged perceptron (Collins, 6 http://sourceforge.net/projects/mstparser/ 59 \x0cDepPercep-1 DepPercep-2 MST-1 MST-2 Filter Cost Acc,,0
With a linear scoring function, the parser solves: parse(s) = argmaxts X [h,m]t w f(h, m, s) The weights w are typically learned using an online method, such as an averaged perceptron CITATION or MIRA CITATION,,0
These filters could also facilitate expensive learning algorithms, such as semi-supervised approaches CITATION,,0
Also, inspired by the reported utility of mixing PoS tags and word-clusters CITATION, we created versions of all of the Between and Surrounding Word features described by CITATION where we mix tags and words.7 DepPercep was developed with quadratic filters in place, which enabled a fast development cycle for feature engineering,,0
This suggests that filtering could have a dramatic effect on a parser that uses more than a few triple features, such as CITATION,,0
Both parsers require scores for arcs connecting each possible [h, m] pair in s; therefore, the cost of arc scoring is also O(n2), and may become O(n3) if the features include words in s between h and m CITATION,,0
3.3 Coarse-to-fine Parsing Another common method employed to speed up exhaustive parsers is a coarse-to-fine approach, where a cheap, coarse model prunes the search space for later, more expensive models (Charniak et al., 2006; CITATION),,0
57 \x0c5 Filter Experiments Data We extract dependency structures from the Penn Treebank using the Penn2Malt extraction tool,5 which implements the head rules of CITATION,,0
2 Dependency Parsing A dependency tree represents the syntactic structure of a sentence as a directed graph (Figure 1), with a node for each word, and arcs indicating head-modifier pairs CITATION,,0
., 2007), semantic parsing CITATION, and machine translation CITATION,,0
This could allow extremely large feature sets CITATION, or the look-up of expensive corpus-based features such as word-pair mutual informatio,,0
We follow CITATION in using only between-tags within a fixed range of the head or modifier, so that the extraction for each pair is O(1) and the overall feature extraction is O(n2),,1
To that end, there are two dominant approaches: graph-based methods, characterized by arc features in an exhaustive search, and transition-based methods, characterized by operational features in a greedy search CITATION,,0
3.2 CFG Cell Classification CITATION speed up another exhaustive parsing algorithm, the CKY parser for CFGs, by classifying each word in the sentence according to whether it can open (or close) a multi-word constituent,,0
This enables search by either minimum spanning tree CITATION or by Eisners (1996) projective parser,,0
CITATION use coarse-to-fine pruning with dependency parsing, but in that case, a graphbased dependency parser provides the coarse pass, with the fine pass being a far-more-expensive treeadjoining grammar,,0
The development and test sets are re-tagged using the Stanford tagger CITATION,,0
CITATION reports that when arc scores have been precomputed, the dynamic prog,,0
This could allow extremely large feature sets CITATION, or the look-up of expensive corpu,,0
We test on two stateof-the art parsers: MST We modified the publicly-available MST parser CITATION6 to employ our filters before carrying out feature extraction,,0
CITATION previously proposed a tagger to further constrain a vine parser,,0
 CITATION,,0
 which consider two siblings at a time, are available with no increase in asymptotic complexity (CITATION; CITATION),,0
 a scoring function that decomposes under an exhaustive search CITATION,,0
O(n3) if the features include words in s between h and m CITATION,,0
With the cluster-based features suggested by CITATION, this could easily grow by a factor of 3 or 4,,0
Dependency information is useful for a wealth of natural language processing tasks, including question answering CITATION, semantic parsing CITATION, and machine translation CITATION,,0
CITATION reports that when arc scores have been precomputed, the dynamic programming component of his 1st-order parser can process an amazing 3,580 sentences per second.1 Beyond reducing the number of features, the easiest way to reduce the computational burden of arc scoring is to score only plausible arcs,,0
 coarse model prunes the search space for later, more expensive models (Charniak et al., 2006; CITATION),,0
Graph-based dependency parsing finds the highest-scoring tree according to a scoring function that decomposes under an exhaustive search CITATION,,0
raph-based methods, characterized by arc features in an exhaustive search, and transition-based methods, characterized by operational features in a greedy search CITATION,,0
We view these as a dependencyparsing analogue to the span-pruning proposed by CITATION,,1
By focusing on weighted loss as opposed to arc frequency, the classifier discovers structural zeros CITATION, events which could have been observed, but were not,,0
We also generalize the between-tag features used in CITATION to be the count of each tag between the head and modifier,,1
=======
, 2006; CITATION),,
 CITATION use coarse-to-fine pruning with dependency parsing, but in that case, a graphbased dependency parser provides the coarse pass, with the fine pass being a far-more-expensive treeadjoining grammar,,
 This enables search by either minimum spanning tree CITATION or by Eisners (1996) projective parser,,
 With a linear scoring function, the parser solves: parse(s) = argmaxts X [h,m]t w f(h, m, s) The weights w are typically learned using an online method, such as an averaged perceptron CITATION or MIRA CITATION,,
 2nd-order searches, which consider two siblings at a time, are available with no increase in asymptotic complexity (CITATION; CITATION),,
 Both parsers require scores for arcs connecting each possible [h, m] pair in s; therefore, the cost of arc scoring is also O(n2), and may become O(n3) if the features include words in s between h and m CITATION,,
coring tree according to a scoring function that decomposes under an exhaustive search CITATION,,
 This enables search by either minimum spanning tree CITATION or by Eisners (1996) projective parser,,
 With a linear scoring function, the parser solves: parse(s) = argmaxts X [h,m]t w f(h, m, s) The weights w are typically learned using an online method, such as an averaged perceptron CITATION or MIRA CITATION,,
 2nd-order searches, which consider two siblings at a time, are available with no increase in asymptotic complexity (CITATION; CITATION),,
 a scoring function that decomposes under an exhaustive search CITATION,,
 This enables search by either minimum spanning tree CITATION or by Eisners (1996) projective parser,,
 With a linear scoring function, the parser solves: parse(s) = argmaxts X [h,m]t w f(h, m, s) The weights w are typically learned using an online method, such as an averaged perceptron CITATION or MIRA CITATION,,
 2nd-order searches, which consider two siblings at a time, are available with no increase in asymptotic complexity (CITATION; CITATION),,
 CITATION reports that when arc scores have been precomputed, the dynamic programming component of his 1st-order parser can process an amazing 3,580 sentences per second,,
1 Vine Parsing Filtering dependency arcs has been explored primarily in the form of vine parsing (CITATION; CITATION),,
 CITATION reports that when arc scores have been precomputed, the dynamic programming component of his 1st-order parser can process an amazing 3,580 sentences per second,,
1 Vine Parsing Filtering dependency arcs has been explored primarily in the form of vine parsing (CITATION; CITATION),,
 CITATION,,
 We solve the SVM objective using LIBLINEAR CITATION,,
 Dependency information is useful for a wealth of natural language processing tasks, including question answering CITATION, semantic parsing CITATION, and machine translation CITATION,,
 This could allow extremely large feature sets CITATION, or the look-up of expensive corpus-based features such as word-pair mutual informatio,,
 which consider two siblings at a time, are available with no increase in asymptotic complexity (CITATION; CITATION),,
 Both parsers require scores for arcs connecting each possible [h, m] pair in s; therefore, the cost of arc scoring is also O(n2), and may become O(n3) if the features include words in s between h and m CITATION,,
 With the cluster-based features suggested by CITATION, this could easily grow by a factor of 3 or 4,,
 CITATION reports that when arc scores have been precomputed, the dynamic prog,,
 We also generalize the between-tag features used in CITATION to be the count of each tag between the head and modifier,,
 We follow CITATION in using only between-tags within a fixed range of the head or modifier, so that the extraction for each pair is O(1) and the overall feature extraction is O(n2),,
 Using only a subset of the between-tags as features has been shown to improve speed but impair parser performance CITATION,,
 One can interpret the learning objective as minimizing regularized, weighted loss: min w 1 2 ||w||2 + C1 X i:yi=1 l(w, yi, xi) +C2 X i:yi=1 l(w, yi, xi) (1) where l() is the learning methods loss function, xi and yi are the features and label for the ith 3 Learning with a cost model is generally preferable to first optimizing error rate and then thresholding the prediction values to select a high-confidence subset CITATION, but the latter approach was used successfully for cell classification in CITATION,,
O(n3) if the features include words in s between h and m CITATION,,
 With the cluster-based features suggested by CITATION, this could easily grow by a factor of 3 or 4,,
 CITATION reports that when arc scores have been precomputed, the dynamic programming component of his 1st-order parser can process an amazing 3,580 sentences per second,,
1 Vine Parsing Filtering dependency arcs has been explored primarily in the form of vine parsing (CITATION; CITATION),,
 We test on two stateof-the art parsers: MST We modified the publicly-available MST parser CITATION6 to employ our filters before carrying out feature extraction,,
 DepPercep We also test an in-house dependency parser, which conducts projective first and 2nd-order searches using the split-head CFG described by CITATION, with a weight vector trained using an averaged perceptron (Collins, 6 http://sourceforge,,
, 2007), semantic parsing CITATION, and machine translation CITATION,,
 This could allow extremely large feature sets CITATION, or the look-up of expensive corpus-based features such as word-pair mutual information CITATION,,
 These filters could also facilitate expensive learning algorithms, such as semi-supervised approaches CITATION,,
 Both parsers require scores for arcs connecting each possible [h, m] pair in s; therefore, the cost of arc scoring is also O(n2), and may become O(n3) if the features include words in s between h and m CITATION,,
 With the cluster-based features suggested by CITATION, this could easily grow by a factor of 3 or 4,,
 CITATION reports that when arc scores have been precomputed, the dynamic programming component of his 1st-order parser can process an amazing 3,580 sentences per second,,
 Its features are a mixture of those described by CITATION, and those used in the CITATION baseline system; we do not use word-cluster features,,
 Also, inspired by the reported utility of mixing PoS tags and word-clusters CITATION, we created versions of all of the Between and Surrounding Word features described by CITATION where we mix tags and words,,
 This suggests that filtering could have a dramatic effect on a parser that uses more than a few triple features, such as CITATION,,
 2 Dependency Parsing A dependency tree represents the syntactic structure of a sentence as a directed graph (Figure 1), with a node for each word, and arcs indicating head-modifier pairs CITATION,,
 To that end, there are two dominant approaches: graph-based methods, characterized by arc features in an exhaustive search, and transition-based methods, characterized by operational features in a greedy search CITATION,,
 Graph-based dependency parsing finds the highest-scoring tree according to a scoring function that decomposes under an exhaustive search CITATION,,
 This enables search by either minimum spanning tree CITATION or by Eisners (1996) projective parser,,
 This enables search by either minimum spanning tree CITATION or by Eisners (1996) projective parser,,
 With a linear scoring function, the parser solves: parse(s) = argmaxts X [h,m]t w f(h, m, s) The weights w are typically learned using an online method, such as an averaged perceptron CITATION or MIRA CITATION,,
 2nd-order searches, which consider two siblings at a time, are available with no increase in asymptotic complexity (CITATION; CITATION),,
 Both parsers require scores for arcs connecting each possible [h, m] pair in s; therefore, the cost of arc scoring is also O(n2), and may become O(n3) if the features include words in s between h and m CITATION,,
 To that end, there are two dominant approaches: graph-based methods, characterized by arc features in an exhaustive search, and transition-based methods, characterized by operational features in a greedy search CITATION,,
 Graph-based dependency parsing finds the highest-scoring tree according to a scoring function that decomposes under an exhaustive search CITATION,,
 This enables search by either minimum spanning tree CITATION or by Eisners (1996) projective parser,,
 With a linear scoring function, the parser solves: parse(s) = argmaxts X [h,m]t w f(h, m, s) The weights w are typically learned using an online method, such as an averaged perceptron CITATION or MIRA CITATION,,
 We also generalize the between-tag features used in CITATION to be the count of each tag between the head and modifier,,
 We follow CITATION in using only between-tags within a fixed range of the head or modifier, so that the extraction for each pair is O(1) and the overall feature extraction is O(n2),,
 Using only a subset of the between-tags as features has been shown to improve speed but impair parser performance CITATION,,
 We test on two stateof-the art parsers: MST We modified the publicly-available MST parser CITATION6 to employ our filters before carrying out feature extraction,,
 DepPercep We also test an in-house dependency parser, which conducts projective first and 2nd-order searches using the split-head CFG described by CITATION, with a weight vector trained using an averaged perceptron (Collins, 6 http://sourceforge,,
 Its features are a mixture of those described by CITATION, and those used in the CITATION baseline system; we do not use word-cluster features,,
 Also, inspired by the reported utility of mixing PoS tags and word-clusters CITATION, we created versions of all of the Between and Surrounding Word features described by CITATION where we mix tags and words,,
 2 Dependency Parsing A dependency tree represents the syntactic structure of a sentence as a directed graph (Figure 1), with a node for each word, and arcs indicating head-modifier pairs CITATION,,
 To that end, there are two dominant approaches: graph-based methods, characterized by arc features in an exhaustive search, and transition-based methods, characterized by operational features in a greedy search CITATION,,
 By focusing on weighted loss as opposed to arc frequency, the classifier discovers structural zeros CITATION, events which could have been observed, but were not,,
, 2006; CITATION),,
 CITATION use coarse-to-fine pruning with dependency parsing, but in that case, a graphbased dependency p,,
 Dependency information is useful for a wealth of natural language processing tasks, including question answering CITATION, semantic parsing CITATION, and machine translation CITATION,,
 This could allow extremely large feature sets CITATION, or the look-up of expensive corpu,,
 We view these as a dependencyparsing analogue to the span-pruning proposed by CITATION,,
2 CFG Cell Classification CITATION speed up another exhaustive parsing algorithm, the CKY parser for CFGs, by classifying each word in the sentence according to whether it can open (or close) a multi-word constituent,,
 One can interpret the learning objective as minimizing regularized, weighted loss: min w 1 2 ||w||2 + C1 X i:yi=1 l(w, yi, xi) +C2 X i:yi=1 l(w, yi, xi) (1) where l() is the learning methods loss function, xi and yi are the features and label for the ith 3 Learning with a cost model is generally preferable to first optimizing error rate and then thresholding the prediction values to select a high-confidence subset CITATION, but the latter approach was used successfully for cell classification in CITATION,,
 We solve the SVM objective using LIBLINEAR CITATION,,
 CITATION previously proposed a tagger to further constrain a vine parser,,
en shown to improve speed but impair parser performance CITATION,,
 57 \x0c5 Filter Experiments Data We extract dependency structures from the Penn Treebank using the Penn2Malt extraction tool,5 which implements the head rules of CITATION,,
 The development and test sets are re-tagged using the Stanford tagger CITATION,,
 This could allow extremely large feature sets CITATION, or the look-up of expensive corpus-based features such as word-pair mutual information CITATION,,
 These filters could also facilitate expensive learning algorithms, such as semi-supervised approaches CITATION,,
 Dependency information is useful for a wealth of natural language processing tasks, including question answering CITATION, semantic parsing CITATION, and machine translation CITATION,,
 This could allow extremely large feature sets CITATION, or the look-up of expensive corpus-based features such as word-pair mutual information CITATION,,
 These filters could also facilitate expensive learning algorithms, such as semi-supervised approaches CITATION,,
raph-based methods, characterized by arc features in an exhaustive search, and transition-based methods, characterized by operational features in a greedy search CITATION,,
 Graph-based dependency parsing finds the highest-scoring tree according to a scoring function that decomposes under an exhaustive search CITATION,,
 This enables search by either minimum spanning tree CITATION or by Eisners (1996) projective parser,,
 With a linear scoring function, the parser solves: parse(s) = argmaxts X [h,m]t w f(h, m, s) The weights w are typically learned using an online method, such as an averaged perceptron CITATION or MIRA CITATION,,
 2nd-order searches, which consider two siblings at a time, are available with no increase in asymptotic complexity (CITATION; CITATION),,
 Using only a subset of the between-tags as features has been shown to improve speed but impair parser performance CITATION,,
 57 \x0c5 Filter Experiments Data We extract dependency structures from the Penn Treebank using the Penn2Malt extraction tool,5 which implements the head rules of CITATION,,
 The development and test sets are re-tagged using the Stanford tagger CITATION,,
>>>>>>> 198ba63628f3d20f872eceed27271a167777efde
