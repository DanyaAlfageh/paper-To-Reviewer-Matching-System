 We performed feature selection by incrementally growing a log-linear model with order0 features f(x, yt) using a forward feature selection procedure similar to CITATION,,
 In particular, we used the latent semantic analysis (LSA) feature discussed in CITATION, which attempts to determine sentence importance through singular value decomposition, and whose resulting singular values and singular vectors can be exploited to associate each utterance a degree of relevance to one of the top-n concepts of the meetings (where n represents the number of dimensions in the LSA),,
 We used the same scoring mechanism as CITATION, though we extracted features for many different n values,,
 Acoustic features extracted with Praat CITATION were normalized by channel and speaker, including many raw features such as f0 and energy,,
 6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (CITATION; CITATION; CITATION) or text summarization CITATION,,
 There is strong evidence that lexical cues such as significant and great are strong predictors in many summarization tasks CITATION,,
1 Inference and Parameter Estimation Our CRF and BN models were designed using MALLET CITATION, which provides tools for training log-linear models with L-BFGS optimization techniques and maximize the loglikelihood of our training data D = (x(i), y(i)) N i=1, and provides probabilistic inference algorithms for linear-chain BNs and CRFs,,
 Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP CITATION and Gibbs sampling CITATION,,
 For example, the worse case reported in CITATION is a clique of 61 nodes,,
902 accuracy CITATION,,
 is the previous utterance of the same speaker? number of APs initiated in yt Discourse features: lexical cohesion score (for topic shifts) CITATION first and second word of utterance, if in cue word list number of pronouns number of fillers and fluency devices (e,,
 6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (CITATION; CITATION; CITATION) or text summarization (Mani ,,
f APs initiated in yt Discourse features: lexical cohesion score (for topic shifts) CITATION first and second word of utterance, if in cue word list number of pronouns number of fillers and fluency devices (e,,
 6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (CITATION; CITATION; CITATION) or text summarization CITATION,,
 There is strong evidence that lexical cues such as significant and great are strong predictors in many summarization tasks CITATION,,
 Empirical evaluations using two standard summarization metricsthe Pyramid method (CITATIONb) and ROUGE CITATIONshow that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91,,
 2 Corpus The work presented here was applied to the ICSI Meeting Corpus CITATION, a corpus of naturally-occurring meetings, i,,
 Many probabilistic approaches to modeling sequences have relied on directed graphical models, also known as Bayesian networks (BN),1 in particular hidden Markov models CITATION and conditional Markov models CITATION,,
 However, prominent recent approaches have focused on undirected graphical models, in particular conditional random fields (CRF) CITATION, and provided state-of-the-art performance in many NLP tasks,,
 Empirical evaluations using two standard summarization metricsthe Pyramid method (CITATIONb) and ROUGE CITATIONshow that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91,,
 2 Corpus The work presented here was applied to the ICSI Meeting Corpus CITATION, a corpus of naturally-occurring meetings, i,,
 Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (CITATIONb) and ROUGE CITATION,,
 6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (CITATION; CITATION; CITATION) or text summarization CITATION,,
 There is strong evidence that lexical cues such as significant and great are strong predictors in many summarization tasks CITATION,,
t Discourse features: lexical cohesion score (for topic shifts) CITATION first and second word of utterance, if in cue word list number of pronouns number of fillers and fluency devices (e,,
 6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (CITATION; CITATION; CITATION) or text summarization CITATION,,
 There is strong evidence that lexical cues such as significant and great are strong predictors in many summarization tasks CITATION,,
 Many probabilistic approaches to modeling sequences have relied on directed graphical models, also known as Bayesian networks (BN),1 in particular hidden Markov models CITATION and conditional Markov models CITATION,,
 However, prominent recent approaches have focused on undirected graphical models, in particular conditional random fields (CRF) CITATION, and provided state-of-the-art performance in many NLP tasks,,
1 Inference and Parameter Estimation Our CRF and BN models were designed using MALLET CITATION, which provides tools for training log-linear models with L-BFGS optimization techniques and maximize the loglikelihood of our training data D = (x(i), y(i)) N i=1, and provides probabilistic inference algorithms for linear-chain BNs and CRFs,,
 Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP CITATION and Gibbs sampling CITATION,,
 For automatic recognition, we used the ICSISRI-UW speech recognition system CITATION, a state-of-the-art conversational telephone speech (CTS) recognizer whose language and acoustic models were adapted to the meeting domain,,
 We also used additional annotation that has been developed to support higher-level analyses of meeting structure, in particular the ICSI Meeting Recorder Dialog act (MRDA) corpus CITATION,,
 To train and evaluate our summarizer, we used a corpus of extractive summaries produced at the University of Edinburgh CITATION,,
 cohesion score (for topic shifts) CITATION first and second word of utterance, if in cue word list number of pronouns number of fillers and fluency devices (e,,
 6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (CITATION; CITATION; CITATION) or text summarization CITATION,,
 There is strong evidence that lexical cues such as significant and great are strong predictors in many summarization tasks CITATION,,
 In particular, we used the latent semantic analysis (LSA) feature discussed in CITATION, which attempts to determine sentence importance through singular value decomposition, and whose resulting singular values and singular vectors can be exploited to associate each utterance a degree of relevance to one of the top-n concepts of the meetings (where n represents the number of dimensions in the LSA),,
 We used the same scoring mechanism as CITATION, though we extracted features for many different n values,,
 Acoustic features extracted with Praat CITATION were normalized by channel and speaker, including many raw features such as f0 and energy,,
 8 Experiments We follow CITATION in using the same six meetings as test data, since each of these meetings has multiple reference summaries,,
 We performed a last experiment to compare our best system against CITATION, who used the same test data, but constrained summary sizes in terms of number of DA units instead of words,,
 Empirical evaluations using two standard summarization metricsthe Pyramid method (CITATIONb) and ROUGE CITATIONshow that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91,,
 2 Corpus The work presented here was applied to the ICSI Meeting Corpus CITATION, a corpus of naturally-occurring meetings, i,,
 Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (CITATIONb) and ROUGE CITATION,,
 Once all SCUs have been identified, the Pyramid method is applied as in (CITATIONb): we compute a score D by adding for each SCU present in the summary a score equal to the number of model summaries in which that SCU appears,,
 We now assess the significance of our results by comparing our best system against: (1) a lead summarizer that always selects the first N utterances to match the predefined length; (2) human performance, which is obtained by leave-one-out comparisons among references (Table 7); (3) optimal summaries generated using the procedure explained in (CITATIONb) by ranking document utterances by the number of model summaries in which they appear,,
 Empirical evaluations using two standard summarization metricsthe Pyramid method (CITATIONb) and ROUGE CITATIONshow that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91,,
 2 Corpus The work presented here was applied to the ICSI Meeting Corpus CITATION, a corpus of naturally-occurring meetings, i,,
 Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (CITATIONb) and ROUGE CITATION,,
 Once all SCUs have been identified, the Pyramid method is applied as in (CITATIONb): we compute a score D by adding for each SCU present in the summary a score equal to the number of model summaries in which that SCU appears,,
 We now assess the significance of our results by comparing our best system against: (1) a lead summarizer that always selects the first N utterances to match the predefined length; (2) human performance, which is obtained by leave-one-out comparisons among references (Table 7); (3) optimal summaries generated using the procedure explained in (CITATIONb) by ranking document utterances by the number of model summaries in which they appear,,
 Markov models (HMMs) CITATION, are linear chains that only encode local dependencies between utterances to be labeled,,
 We use instead skip-chain sequence models CITATION, which allow us to explicitly model dependencies between distant utterances, and turn out to be particularly effective in the summarization task,,
 3 Content selection State sequence Markov models such as hidden Markov models CITATION have been highly successful in many speech and natural language processing applications, including summarization,,
 In the case of summarization of conversational speech, CITATION found, for,,
 Many probabilistic approaches to modeling sequences have relied on directed graphical models, also known as Bayesian networks (BN),1 in particular hidden Markov models CITATION and conditional Markov models CITATION,,
 However, prominent recent approaches have focused on undirected graphical models, in particular conditional random fields (CRF) CITATION, and provided state-of-the-art performance in many NLP tasks,,
 In the case of summarization of conversational speech, CITATION found, for instance, that a simple technique consisting of linking together questions and answers in summariesand thus preventing the selection of orphan questions or answerssignificantly improved their readability according to various human summary evaluations,,
 In email summarization CITATION, CITATION obtained good performance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange,,
 In a combined chat and email summarization task, a technique CITATION consisting of identifying APs and appending any relevant responses to topic initiating messages was instrumental in outperforming two competitive summarization baselines,,
 In particular, our analyses of patterns in the verbal exchange between participants found that adjacency pairs (AP), a concept drawn from the conversational analysis literature CITATION, have particular relevance to summarization,,
 For example, the worse case reported in CITATION is a clique of 61 nodes,,
 Specifically, to account for skip-edges, we used a technique inspired by CITATION, in which multiple state dependencies, such as an order-2 Markov model, are encoded using auxiliary tags,,
 In the case of summarization of conversational speech, CITATION found, for instance, that a simple technique consisting of linking together questions and answers in summariesand thus preventing the selection of orphan questions or answerssignificantly improved their readability according to various human summary evaluations,,
 In email summarization CITATION, CITATION obtained good performance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange,,
 In a combined chat and email summarization task, a technique CITATION consisting of identifying APs and appending any relevant responses to topic initiating messages was instrumental in outperforming two competitive summarization baselines,,
 For automatic recognition, we used the ICSISRI-UW speech recognition system CITATION, a state-of-the-art conversational telephone speech (CTS) recognizer whose language and acoustic models were adapted to the meeting domain,,
 We also used additional annotation that has been developed to support higher-level analyses of meeting structure, in particular the ICSI Meeting Recorder Dialog act (MRDA) corpus CITATION,,
 Markov models (HMMs) CITATION, are linear chains that only encode local dependencies between utterances to be labeled,,
 We use instead skip-chain sequence models CITATION, which allow us to explicitly model dependencies between distant utterances, and turn out to be particularly effective in the summarization task,,
 In the current work, we use skip-chain sequence models CITATION to represent dependencies between both contiguous utterances and paired utterances appearing in the same AP constructions,,
1 Inference and Parameter Estimation Our CRF and BN models were designed using MALLET CITATION, which provides tools for training log-linear models with L-BFGS optimization techniques and maximize the loglikelihood of our training data D = (x(i), y(i)) N i=1, and provides probabilistic inference algorithms for linear-chain BNs and CRFs,,
 Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP CITATION and Gibbs sampling CITATION,,
 For example, the worse case reported in CITATION is a clique of 61 nodes,,
ov models CITATION have been highly successful in many speech and natural language processing applications, including summarization,,
 In the case of summarization of conversational speech, CITATION found, for instance, that a simple technique consisting of linking together questions and answers in summariesand thus preventing the selection of orphan questions or answerssignificantly improved their readability according to various human summary evaluations,,
 In email summarization CITATION, CITATION obtained good performance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange,,
 In a combined chat and email summarization task, a technique CITATION consisting,,
sational speech, CITATION found, for instance, that a simple technique consisting of linking together questions and answers in summariesand thus preventing the selection of orphan questions or answerssignificantly improved their readability according to various human summary evaluations,,
 In email summarization CITATION, CITATION obtained good performance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange,,
 In a combined chat and email summarization task, a technique CITATION consisting of identifying APs and appending any relevant responses to topic initiating messages was instrumental in outperforming two competitive summarization baselines,,
