CITATION applied HMMs as an unsupervised model of discourse,,
stribution over words which is independent of the latent classes in a document; this was also done by CITATIONb),,
Addressing these concerns, there has been recent work with unsupervised models of Web conversations based on hidden Markov models CITATION, where each state corresponds to a conversational class or act,,
(See also CITATION for more details on Bayesian HMMs with Dirichlet priors.) We also compare against LDA, which makes latent assignments at the token-level, but blocks of text are independent of 5 Three messages in this corpus have multiple parents,,
mulations of latent class distributions4 are utilized in correlated topic models CITATION as a means of incorporating covariance structure among topic probabilities,,
We show that M4 increases thread reconstruction accuracy by up to 15% compared to the HMM of CITATION, and we reduce variation of information against speech act annotations by an average of 18% from HMM and LDA baselines,,
into sentences) and constraint each segment to belong to one class or speech act; modifications along these lines have been applied to topic models as well CITATION,,
Applying log-linear regression to potentially many features was combined with LDA by CITATION, who model the Dirichlet prior over topics as a function of document features,,
This corpus includes 321 threads and a total of 1309 messages, with an average message length of 78 tokens after preprocessing.5 Second, we use the Twitter data set created by CITATION,,
An approach to unsupervised discourse modeling that does not use HMMs is 3 Incremental updates are justified under the generalized EM algorithm CITATION,,
In the case of conversation data, our hope is that some of the latent classes represent speech acts or dialog acts CITATION,,
5.1 Data Sets First, we use a corpus of discussion threads from CNET forums CITATION, which are mostly technical discussion and support,,
One topic model that imposes sequential dependencies between documents is Sequential LDA CITATION, which models a document as a sequence of segments (such as paragraphs) governed by a Pitman-Yor process, in which the latent topic distribution of one segment serves as the base distribution for the next segment,,
In topic models, log-linear formulations of latent class distributions4 are utilized in correlated topic models CITATION as a means of incorporating covariance structure among topic probabilities,,
 CITATION), the variety of conversation domains on the Web motivates the use of unsupervised ap5 10 15 20 25 Number of classes 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Variation of Information (VI) Speech Act Clustering (CNET) Random LDA BHMM M4 Figure 3: The variation of information between the human-created speech act annotations of the CNET corpus and the latent class assignments by various models,,
Unsupervised HMMs were applied to conversational data by CITATION who experimented with Twitter conversations,,
Given only a flat structure, can we recover the reply structure of messages in the conversation? Previous work with BHMM found the optimal structure by computing the likelihood of all permutations of a thread or sequence (CITATION; CITATIONb),,
In topic models, this is sometimes assumed to be Poisson CITATION,,
If it is desired to have a large number of latent topics as is common in LDA, this model could be combined with a standard topic model without sequential dependencies, as explored by CITATION,,
Similar arguments are made by CITATION when designing supervised topic models,,
CITATION extended this work by enriching the emission distributions and using additional features such as speaker and position information,,
This is common practice and we will not go into detail; see CITATION for a general example on sampling switching variables,,
The CNET corpus is annotated with twelve speech act classes: QUESTION and ANSWER, which are both broken down into multiple sub-classes, as well as RESOLUTION, REPRODUCTION, and OTHER CITATION,,
Under the block HMM, as utilized by CITATION, messages in a conversation flow according to a Markov process, where the words of messages are generated according to language models ass,,
nversation structure have been shown to improve tasks such as forum search (CITATION; CITATION), question answering and expert finding (CITATION; CITATIONa), and interpersonal relationship identification CITATION,,
In the conversation domain, this corresponds to the task of thread reconstruction (CITATION; CITATIONc),,
We use optimized asymmetric priors as described in 5.2, and we use a symmetric Dirichlet for the word distributions, following CITATION,,
5.2 Baseline Models Our work is motivated by the Bayesian HMM approach of CITATION the model we refer to as the block HMM (BHMM) and we consider this our primary baseline,,
Compared to the naive approach of treating conversations as flat documents, models which include conversation structure have been shown to improve tasks such as forum search (CITATION; CITATION), question answering and expert finding (CITATION; CITATIONa), and interpersonal relationship identification CITATION,,
This sampling distribution is very similar to that of LDA CITATION, but the distribution over topics is now a function of the previous block, which gives the leftmost term,,
in part-ofspeech clustering CITATION,,
For example, topic models such as Latent Dirichlet Allocation (LDA) CITATION assume each document has its own distribution over multiple classes (often called topics),,
Extensions to the block HMM have incorporated mixed membership properties within blocks, notably the Markov Clustering Topic Model CITATION, which allows each HMM state to be associat,,
Decoupling HMM states from latent classes was considered by CITATION with the Factorial HMM, which uses factorized state representations,,
Standard topic model extensions such as n-gram models CITATION can straightforwardly be applied here, and indeed we already applied such an extension by incorporating background distributions in 5.3,,
justified under the generalized EM algorithm CITATION,,
Extensions to the block HMM have incorporated mixed membership properties within blocks, notably the Markov Clustering Topic Model CITATION, which allows each HMM state to be associated with its own distribution over topics in a topic model,,
One topic model that imposes sequential dependencies between documents is Sequential LDA CITATION, which models a document as a sequence of segments (such as paragraphs) governed by a Pit,,
the latent permutation model of CITATION,,
More recently, CITATIONb) experimented with similar tasks using a related HMMbased model called the Structural Topic Model,,
Much as LDA topics do not always correspond to what humans would judge to be semantic classes CITATION, the conversation classes inferred by unsupervised sequence models are similarly unlikely to be a perfect fit to human-assigned classes,,
While there is a body of work in supervised speech act classification (Cohen et al., 2004; Bangalore et al., 2006; CITATION; CITATION), the variety of conversation domains on the Web motivates the use of unsupervised ap5 10 15 20 25 Number of classes 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Variation of Information (VI) Speech Act Clustering (CNET) Random LDA BHMM M4 Figure 3: The variation of information between the human-created speech act annotations of the CNET corpus and the latent class assignments by various models,,
We instead handle this by extending our model to include a background distribution over words which is independent of the latent classes in a document; this was also done by CITATIONb),,
Specifically, we build off the Bayesian block HMMs used by CITATION for modeling Twitter conversations, which will be our primary baseline,,
This figure depicts the Bayesian variant of the block HMM CITATION where the transition distributions depend on a Dirichlet() prior,,
We would like to quantitatively measure how closely the latent states induced by our model match these annotations.7 We can measure this with variation of information CITATION, which has been used in recent years for unsupervised evaluation, e.g,,
