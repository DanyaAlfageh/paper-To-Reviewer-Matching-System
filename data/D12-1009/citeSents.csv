CITATION applied HMMs as an unsupervised model of discourse,,
More recently, CITATIONb) experimented with similar tasks using a related HMMbased model called the Structural Topic Model,,
Unsupervised HMMs were applied to conversational data by CITATION who experimented with Twitter conversations,,
Extensions to the block HMM have incorporated mixed membership properties within blocks, notably the Markov Clustering Topic Model CITATION, which allows each HMM state to be associated with its own distribution over topics in a topic model,,
Decoupling HMM states from latent classes was considered by CITATION with the Factorial HMM, which uses factorized state representations,,
In topic models, log-linear formulations of latent class distributions4 are utilized in correlated topic models CITATION as a means of incorporating covariance structure among topic probabilities,,
Applying log-linear regression to potentially many features was combined with LDA by CITATION, who model the Dirichlet prior over topics as a function of document features,,
One topic model that imposes sequential dependencies between documents is Sequential LDA CITATION, which models a document as a sequence of segments (such as paragraphs) governed by a Pit,,
Similar arguments are made by CITATION when designing supervised topic models,,
For example, topic models such as Latent Dirichlet Allocation (LDA) CITATION assume each document has its own distribution over multiple classes (often called topics),,
In topic models, this is sometimes assumed to be Poisson CITATION,,
Much as LDA topics do not always correspond to what humans would judge to be semantic classes CITATION, the conversation classes inferred by unsupervised sequence models are similarly unlikely to be a perfect fit to human-assigned classes,,
stribution over words which is independent of the latent classes in a document; this was also done by CITATIONb),,
This is common practice and we will not go into detail; see CITATION for a general example on sampling switching variables,,
CITATION extended this work by enriching the emission distributions and using additional features such as speaker and position information,,
An approach to unsupervised discourse modeling that does not use HMMs is 3 Incremental updates are justified under the generalized EM algorithm CITATION,,
the latent permutation model of CITATION,,
Extensions to the block HMM have incorporated mixed membership properties within blocks, notably the Markov Clustering Topic Model CITATION, which allows each HMM state to be associated with its own distribution over topics in a topic model,,
Unsupervised HMMs were applied to conversational data by CITATION who experimented with Twitter conversations,,
CITATION extended this work by enriching the emission distributions and using additional features such as speaker and position information,,
An approach to unsupervised discourse modeling that does not use HMMs is 3 Incremental updates are justified under the generalized EM algorithm CITATION,,
the latent permutation model of CITATION,,
Extensions to the block HMM have incorporated mixed membership properties within blocks, notably the Markov Clustering Topic Model CITATION, which allows each HMM state to be associat,,
Compared to the naive approach of treating conversations as flat documents, models which include conversation structure have been shown to improve tasks such as forum search (CITATION; CITATION), question answering and expert finding (CITATION; CITATIONa), and interpersonal relationship identification CITATION,,
Addressing these concerns, there has been recent work with unsupervised models of Web conversations based on hidden Markov models CITATION, where each state corresponds to a conversational class or act,,
mulations of latent class distributions4 are utilized in correlated topic models CITATION as a means of incorporating covariance structure among topic probabilities,,
Applying log-linear regression to potentially many features was combined with LDA by CITATION, who model the Dirichlet prior over topics as a function of document features,,
One topic model that imposes sequential dependencies between documents is Sequential LDA CITATION, which models a document as a sequence of segments (such as paragraphs) governed by a Pitman-Yor process, in which the latent topic distribution of one segment serves as the base distribution for the next segment,,
Compared to the naive approach of treating conversations as flat documents, models which include conversation structure have been shown to improve tasks such as forum search (CITATION; CITATION), question answering and expert finding (CITATION; CITATIONa), and interpersonal relationship identification CITATION,,
Addressing these concerns, there has been recent work with unsupervised models of Web conversations based on hidden Markov models CITATION, where each state corresponds to a conversational class or act,,
5.2 Baseline Models Our work is motivated by the Bayesian HMM approach of CITATION the model we refer to as the block HMM (BHMM) and we consider this our primary baseline,,
(See also CITATION for more details on Bayesian HMMs with Dirichlet priors.) We also compare against LDA, which makes latent assignments at the token-level, but blocks of text are independent of 5 Three messages in this corpus have multiple parents,,
The CNET corpus is annotated with twelve speech act classes: QUESTION and ANSWER, which are both broken down into multiple sub-classes, as well as RESOLUTION, REPRODUCTION, and OTHER CITATION,,
We would like to quantitatively measure how closely the latent states induced by our model match these annotations.7 We can measure this with variation of information CITATION, which has been used in recent years for unsupervised evaluation, e.g,,
in part-ofspeech clustering CITATION,,
This sampling distribution is very similar to that of LDA CITATION, but the distribution over topics is now a function of the previous block, which gives the leftmost term,,
Standard topic model extensions such as n-gram models CITATION can straightforwardly be applied here, and indeed we already applied such an extension by incorporating background distributions in 5.3,,
into sentences) and constraint each segment to belong to one class or speech act; modifications along these lines have been applied to topic models as well CITATION,,
justified under the generalized EM algorithm CITATION,,
the latent permutation model of CITATION,,
Extensions to the block HMM have incorporated mixed membership properties within blocks, notably the Markov Clustering Topic Model CITATION, which allows each HMM state to be associated with its own distribution over topics in a topic model,,
Decoupling HMM states from latent classes was considered by CITATION with the Factorial HMM, which uses factorized state representations,,
More recently, CITATIONb) experimented with similar tasks using a related HMMbased model called the Structural Topic Model,,
Unsupervised HMMs were applied to conversational data by CITATION who experimented with Twitter conversations,,
CITATION extended this work by enriching the emission distributions and using additional features such as speaker and position information,,
An approach to unsupervised discourse modeling that does not use HMMs is 3 Incremental updates are justified under the generalized EM algorithm CITATION,,
the latent permutation model of CITATION,,
5.1 Data Sets First, we use a corpus of discussion threads from CNET forums CITATION, which are mostly technical discussion and support,,
This corpus includes 321 threads and a total of 1309 messages, with an average message length of 78 tokens after preprocessing.5 Second, we use the Twitter data set created by CITATION,,
 CITATION), the variety of conversation domains on the Web motivates the use of unsupervised ap5 10 15 20 25 Number of classes 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Variation of Information (VI) Speech Act Clustering (CNET) Random LDA BHMM M4 Figure 3: The variation of information between the human-created speech act annotations of the CNET corpus and the latent class assignments by various models,,
The CNET corpus is annotated with twelve speech act classes: QUESTION and ANSWER, which are both broken down into multiple sub-classes, as well as RESOLUTION, REPRODUCTION, and OTHER CITATION,,
We would like to quantitatively measure how closely the latent states induced by our model match these annotations.7 We can measure this with variation of information CITATION, which has been used in recent years for unsupervised evaluation, e.g,,
in part-ofspeech clustering CITATION,,
The CNET corpus is annotated with twelve speech act classes: QUESTION and ANSWER, which are both broken down into multiple sub-classes, as well as RESOLUTION, REPRODUCTION, and OTHER CITATION,,
We would like to quantitatively measure how closely the latent states induced by our model match these annotations.7 We can measure this with variation of information CITATION, which has been used in recent years for unsupervised evaluation, e.g,,
in part-ofspeech clustering CITATION,,
In topic models, log-linear formulations of latent class distributions4 are utilized in correlated topic models CITATION as a means of incorporating covariance structure among topic probabilities,,
Applying log-linear regression to potentially many features was combined with LDA by CITATION, who model the Dirichlet prior over topics as a function of document features,,
One topic model that imposes sequential dependencies between documents is Sequential LDA CITATION, which models a document as a sequence of segments (such as paragraphs) governed by a Pitman-Yor process, in which the latent topic distribution of one segment serves as the base distribution for the next segment,,
In the case of conversation data, our hope is that some of the latent classes represent speech acts or dialog acts CITATION,,
While there is a body of work in supervised speech act classification (Cohen et al., 2004; Bangalore et al., 2006; CITATION; CITATION), the variety of conversation domains on the Web motivates the use of unsupervised ap5 10 15 20 25 Number of classes 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Variation of Information (VI) Speech Act Clustering (CNET) Random LDA BHMM M4 Figure 3: The variation of information between the human-created speech act annotations of the CNET corpus and the latent class assignments by various models,,
The CNET corpus is annotated with twelve speech act classes: QUESTION and ANSWER, which are both broken down into multiple sub-classes, as well as RESOLUTION, REPRODUCTION, and OTHER CITATION,,
nversation structure have been shown to improve tasks such as forum search (CITATION; CITATION), question answering and expert finding (CITATION; CITATIONa), and interpersonal relationship identification CITATION,,
Addressing these concerns, there has been recent work with unsupervised models of Web conversations based on hidden Markov models CITATION, where each state corresponds to a conversational class or act,,
This figure depicts the Bayesian variant of the block HMM CITATION where the transition distributions depend on a Dirichlet() prior,,
Specifically, we build off the Bayesian block HMMs used by CITATION for modeling Twitter conversations, which will be our primary baseline,,
We show that M4 increases thread reconstruction accuracy by up to 15% compared to the HMM of CITATION, and we reduce variation of information against speech act annotations by an average of 18% from HMM and LDA baselines,,
Under the block HMM, as utilized by CITATION, messages in a conversation flow according to a Markov process, where the words of messages are generated according to language models ass,,
If it is desired to have a large number of latent topics as is common in LDA, this model could be combined with a standard topic model without sequential dependencies, as explored by CITATION,,
CITATION applied HMMs as an unsupervised model of discourse,,
More recently, CITATIONb) experimented with similar tasks using a related HMMbased model called the Structural Topic Model,,
Unsupervised HMMs were applied to conversational data by CITATION who experimented with Twitter conversations,,
CITATION extended this work by enriching the emission distributions and using additional features such as speaker and position information,,
An approach to unsupervised discourse modeling that does not use HMMs is 3 Incremental updates are justified under the generalized EM algorithm CITATION,,
5.1 Data Sets First, we use a corpus of discussion threads from CNET forums CITATION, which are mostly technical discussion and support,,
This corpus includes 321 threads and a total of 1309 messages, with an average message length of 78 tokens after preprocessing.5 Second, we use the Twitter data set created by CITATION,,
5.2 Baseline Models Our work is motivated by the Bayesian HMM approach of CITATION the model we refer to as the block HMM (BHMM) and we consider this our primary baseline,,
In the conversation domain, this corresponds to the task of thread reconstruction (CITATION; CITATIONc),,
Given only a flat structure, can we recover the reply structure of messages in the conversation? Previous work with BHMM found the optimal structure by computing the likelihood of all permutations of a thread or sequence (CITATION; CITATIONb),,
In the case of conversation data, our hope is that some of the latent classes represent speech acts or dialog acts CITATION,,
While there is a body of work in supervised speech act classification (Cohen et al., 2004; Bangalore et al., 2006; CITATION; CITATION), the variety of conversation domains on the Web motivates the use of unsupervised ap5 10 15 20 25 Number of classes 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Variation of Information (VI) Speech Act Clustering (CNET) Random LDA BHMM M4 Figure 3: The variation of information between the human-created speech act annotations of the CNET corpus and the latent class assignments by various models,,
Compared to the naive approach of treating conversations as flat documents, models which include conversation structure have been shown to improve tasks such as forum search (CITATION; CITATION), question answering and expert finding (CITATION; CITATIONa), and interpersonal relationship identification CITATION,,
Addressing these concerns, there has been recent work with unsupervised models of Web conversations based on hidden Markov models CITATION, where each state corresponds to a conversational class or act,,
In the case of conversation data, our hope is that some of the latent classes represent speech acts or dialog acts CITATION,,
While there is a body of work in supervised speech act classification (Cohen et al., 2004; Bangalore et al., 2006; CITATION; CITATION), the variety of conversation domains on the Web motivates the use of unsupervised ap5 10 15 20 25 Number of classes 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Variation of Information (VI) Speech Act Clustering (CNET) Random LDA BHMM M4 Figure 3: The variation of information between the human-created speech act annotations of the CNET corpus and the latent class assignments by various models,,
We use optimized asymmetric priors as described in 5.2, and we use a symmetric Dirichlet for the word distributions, following CITATION,,
Standard topic model extensions such as n-gram models CITATION can straightforwardly be applied here, and indeed we already applied such an extension by incorporating background distributions in 5.3,,
into sentences) and constraint each segment to belong to one class or speech act; modifications along these lines have been applied to topic models as well CITATION,,
Compared to the naive approach of treating conversations as flat documents, models which include conversation structure have been shown to improve tasks such as forum search (CITATION; CITATION), question answering and expert finding (CITATION; CITATIONa), and interpersonal relationship identification CITATION,,
Addressing these concerns, there has been recent work with unsupervised models of Web conversations based on hidden Markov models CITATION, where each state corresponds to a conversational class or act,,
CITATION applied HMMs as an unsupervised model of discourse,,
More recently, CITATIONb) experimented with similar tasks using a related HMMbased model called the Structural Topic Model,,
Unsupervised HMMs were applied to conversational data by CITATION who experimented with Twitter conversations,,
CITATION extended this work by enriching the emission distributions and using additional features such as speaker and position information,,
We instead handle this by extending our model to include a background distribution over words which is independent of the latent classes in a document; this was also done by CITATIONb),,
This is common practice and we will not go into detail; see CITATION for a general example on sampling switching variables,,
In the conversation domain, this corresponds to the task of thread reconstruction (CITATION; CITATIONc),,
Given only a flat structure, can we recover the reply structure of messages in the conversation? Previous work with BHMM found the optimal structure by computing the likelihood of all permutations of a thread or sequence (CITATION; CITATIONb),,
Compared to the naive approach of treating conversations as flat documents, models which include conversation structure have been shown to improve tasks such as forum search (CITATION; CITATION), question answering and expert finding (CITATION; CITATIONa), and interpersonal relationship identification CITATION,,
Addressing these concerns, there has been recent work with unsupervised models of Web conversations based on hidden Markov models CITATION, where each state corresponds to a conversational class or act,,
CITATION applied HMMs as an unsupervised model of discourse,,
More recently, CITATIONb) experimented with similar tasks using a related HMMbased model called the Structural Topic Model,,
Unsupervised HMMs were applied to conversational data by CITATION who experimented with Twitter conversations,,
CITATION extended this work by enriching the emission distributions and using additional features such as speaker and position information,,
We instead handle this by extending our model to include a background distribution over words which is independent of the latent classes in a document; this was also done by CITATIONb),,
This is common practice and we will not go into detail; see CITATION for a general example on sampling switching variables,,
In the conversation domain, this corresponds to the task of thread reconstruction (CITATION; CITATIONc),,
Given only a flat structure, can we recover the reply structure of messages in the conversation? Previous work with BHMM found the optimal structure by computing the likelihood of all permutations of a thread or sequence (CITATION; CITATIONb),,
Compared to the naive approach of treating conversations as flat documents, models which include conversation structure have been shown to improve tasks such as forum search (CITATION; CITATION), question answering and expert finding (CITATION; CITATIONa), and interpersonal relationship identification CITATION,,
Addressing these concerns, there has been recent work with unsupervised models of Web conversations based on hidden Markov models CITATION, where each state corresponds to a conversational class or act,,
CITATION applied HMMs as an unsupervised model of discourse,,
More recently, CITATIONb) experimented with similar tasks using a related HMMbased model called the Structural Topic Model,,
Unsupervised HMMs were applied to conversational data by CITATION who experimented with Twitter conversations,,
CITATION extended this work by enriching the emission distributions and using additional features such as speaker and position information,,
We instead handle this by extending our model to include a background distribution over words which is independent of the latent classes in a document; this was also done by CITATIONb),,
This is common practice and we will not go into detail; see CITATION for a general example on sampling switching variables,,
In the conversation domain, this corresponds to the task of thread reconstruction (CITATION; CITATIONc),,
Given only a flat structure, can we recover the reply structure of messages in the conversation? Previous work with BHMM found the optimal structure by computing the likelihood of all permutations of a thread or sequence (CITATION; CITATIONb),,
Compared to the naive approach of treating conversations as flat documents, models which include conversation structure have been shown to improve tasks such as forum search (CITATION; CITATION), question answering and expert finding (CITATION; CITATIONa), and interpersonal relationship identification CITATION,,
Addressing these concerns, there has been recent work with unsupervised models of Web conversations based on hidden Markov models CITATION, where each state corresponds to a conversational class or act,,
In the conversation domain, this corresponds to the task of thread reconstruction (CITATION; CITATIONc),,
Given only a flat structure, can we recover the reply structure of messages in the conversation? Previous work with BHMM found the optimal structure by computing the likelihood of all permutations of a thread or sequence (CITATION; CITATIONb),,
