<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.280291">
b&quot;A Class-based Probabilistic approach to Structural
Disambiguation
</title>
<author confidence="0.938793">
Stephen Clark and David Weir
</author>
<affiliation confidence="0.990361">
School of Cognitive and Computing Sciences
University of Sussex
</affiliation>
<address confidence="0.993664">
Brighton, BN1 9HQ, UK
</address>
<email confidence="0.996186">
fstephecl,davidwg@cogs.susx.ac.uk
</email>
<sectionHeader confidence="0.989682" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9974052">
Knowledge of which words are able to \x0cll partic-
ular argument slots of a predicate can be used
for structural disambiguation. This paper de-
scribes a proposal for acquiring such knowledge,
and in line with much of the recent work in this
area, a probabilistic approach is taken. We de-
velop a novel way of using a semantic hierar-
chy to estimate the probabilities, and demon-
strate the general approach using a preposi-
tional phrase attachment experiment.
</bodyText>
<sectionHeader confidence="0.997827" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998971473684211">
Knowledge of which words are able to \x0cll
particular argument slots of a predicate can
be used for structural disambiguation. In
the following example (Charniak, 1993), the
fact that dog, rather than prize, is often
the subject of run, can be used to decide
on the attachment site of the relative clause:
Fred awarded a prize for the dog that ran the fastest
We describe a proposal for acquiring such
knowledge, and as in other recent work in this
area (Resnik, 1993; Li and Abe, 1998), a prob-
abilistic approach is taken. Using probabilities
accords with the intuition that there are no ab-
solute constraints on the arguments of predi-
cates, but rather that constraints are satis\x0ced
to a certain degree (Resnik, 1993). Unfortu-
nately, de\x0cning probabilities in terms of words
leads to a model with a vast number of param-
eters, resulting in a sparse data problem. To
overcome this, we propose to de\x0cne a probabil-
ity model in terms of senses from a semantic hi-
erarchy, exploiting the fact that senses of nouns
can be grouped together into semantically sim-
ilar classes.
We use the semantic hierarchy of noun senses
in WordNet (Fellbaum, 1998), which consists of
`lexicalised concepts&apos; related by the `is-a-kind-
of&apos; relation. If c0 is a kind of c, then c is a hy-
pernym of c0, and c0 a hyponym of c. Counts are
passed up the hierarchy from the senses of nouns
appearing in the data. Thus if eat chicken ap-
pears in the data, the count for this item passes
up to hmeati, hfoodi, and all the other hyper-
nyms of that sense of chicken.1
In order to es-
timate the probability that a sense of chicken
appears as the object of the verb eat, we repre-
sent hchickeni using a suitable hypernym, such
as hfoodi, and base our probability estimate on
that instead. The level at which hchickeni is
represented is crucial: it should be high enough
for adequate counts to have accumulated, but
not too high so that the hypernym is no longer
representative of hchickeni. An example of a
hypernym which would be too high is hentityi,
as not all entities are semantically similar with
respect to the object position of eat.
The problem of choosing an appropriate level
in the hierarchy at which to represent a par-
ticular noun sense (given a predicate and argu-
ment position) has been investigated by Resnik
(1993), Li and Abe (1998) and Ribas (1995).
The learning mechanism presented here is a
novel approach based on \x0cnding semantically
similar sets of concepts in a hierarchy. We
demonstrate the e\x0bectiveness of our approach
using a PP-attachment experiment.
</bodyText>
<sectionHeader confidence="0.661404" genericHeader="method">
2 The Input Data and Semantic
Hierarchy
</sectionHeader>
<bodyText confidence="0.9936815">
The data used to estimate the probabilities is
a multiset of `co-occurrence triples&apos;: a noun
</bodyText>
<page confidence="0.917654">
1
</page>
<bodyText confidence="0.984406411764706">
We use italics when referring to words, and angled
brackets for concepts. This notation does not always
pick out a concept uniquely, but the context should make
clear the concept being referred to.
\x0clemma, verb lemma, and argument position.2
Let the universe of verbs, argument posi-
tions and nouns that can appear in the in-
put data be denoted V = fv1;::: ;vkV
g, R =
fr1;::: ;rkR
g and N = fn1;::: ;nkN
g, respec-
tively. Such data can be obtained from a tree-
bank, or from a shallow parser. Note that we
do not distinguish between alternative senses of
verbs, and assume that each instance of a noun
in the data refers to exactly one concept.
The semantic hierarchy used is the noun hy-
pernym taxonomy of WordNet (version 1.6).3
Let C = fc1;::: ;ckC
g be the set of concepts
in WordNet (kC \x19 66;000). A concept is repre-
sented in WordNet by a synset: a set of synony-
mous words which can be used to denote that
concept. For example, the concept `cocaine&apos;,
as in the drug, is represented by the following
synset: fcocaine, cocain, coke, snow, Cg. Let
syn(c) \x12 N be the synset for the concept c,
and let cn(n) = fc jn 2 syn(c) g be the set of
concepts that can be denoted by the noun n.
The hierarchy has the structure of a directed
acyclic graph, although the number of nodes in
the graph with more than one parent is only
around one percent of the total. The edges in
the graph form what we call the direct-isa rela-
tion (direct-isa \x12 C \x02 C). Let isa = direct-isa\x03
be the transitive, re
exive closure of direct-isa,
so that (c0;c) 2 isa ) c is a hypernym of c0; and
let c = fc0 j(c0;c) 2 isa g be the set consisting
of the concept c and all of its hyponyms. Thus,
the set hfoodi contains all the concepts which
are kinds of food, including hfoodi.
Note that words in the data can appear in
synsets anywhere in the hierarchy. Even con-
cepts such as hentityi, which appear near the
root of the hierarchy, have synsets containing
words which may appear in the data. The
synset for hentityi is fentity, somethingg, and
the words entity and something can appear in
the argument positions of verbs in the data.
</bodyText>
<sectionHeader confidence="0.984821" genericHeader="method">
3 Probability Estimation
</sectionHeader>
<bodyText confidence="0.999395">
The problem being addressed in this section is
to estimate p(cjv;r), for c 2 C, v 2 V, and
</bodyText>
<page confidence="0.980919">
2
</page>
<bodyText confidence="0.991707">
Only verbs are considered here, but this work applies
to other predicates which take arguments that can be
organised into a semantic hierarchy.
</bodyText>
<page confidence="0.990957">
3
</page>
<bodyText confidence="0.999578481481481">
When we refer to concepts in WordNet, we mean
concepts in WordNet&apos;s noun taxonomy.
r 2 R. The probability p(cjv;r) is the prob-
ability that some noun in syn(c), when denot-
ing concept c, appears in position r of verb v
(given r and v). Using the relative clause ex-
ample from the introduction, the probabilities
p(hdogijrun;subj) and p(hprizeijrun;subj) can
be compared to decide on the attachment site
in Fred awarded a prize for the dog that ran
the fastest. We expect p(hdogijrun;subj) to be
greater than p(hprizeijrun;subj). Although the
focus is on p(cjv;r), the techniques described
here can be used to estimate other probabilities,
such as p(c;rjv). (In fact, the latter probabil-
ity is used in the PP-attachment experiments
described in Section 5.)
Using maximum likelihood to estimate
p(cjv;r) is not viable because of the huge num-
ber of parameters involved. Many combinations
of c, v and r will not occur in the data. To re-
duce the number of parameters which need to
be estimated, we utilise the fact that concepts
can be grouped into classes, and represent c us-
ing a class c0, for some hypernym c0 of c. How-
ever, p(c0jv;r) cannot be used as an estimate of
p(cjv;r), as p(c0jv;r) is given by the following:
</bodyText>
<equation confidence="0.993716166666667">
p(c0jv;r) =
X
c00
2c0
p(c00
jv;r)
</equation>
<bodyText confidence="0.9908806">
The probability p(c0jv;r) increases as c0
moves up the hierarchy. For example,
p(hfoodijeat;obj) is not a good estimate of
p(hchickenijeat;obj). What can be done
though, is to condition on sets of concepts, and
use the probability p(vjc0;r). If it can be shown
that p(vjc0;r), for some hypernym c0 of c, is a
reasonable estimate of p(vjc;r), then we have a
way of estimating p(cjv;r). To get p(vjc;r)from
p(cjv;r) Bayes rule is used:
</bodyText>
<equation confidence="0.668128">
p(cjv;r)= p(vjc;r)p(cjr)
p(vjr)
</equation>
<bodyText confidence="0.9858382">
The probabilities p(cjr) and p(vjr) can be esti-
mated using maximum likelihood estimates, as
the conditioning event is likely to occur often
enough for sparse data not to be a problem.
(Alternatively one could back-o\x0b to p(c) and
p(v) respectively, or use a linear combination
of p(cjr) and p(c), and p(vjr) and p(v), respec-
tively.) The formulae for these estimates will
be given shortly. This only leaves p(vjc;r). The
\x0cproposal is to estimate p(eatjhchickeni;obj) us-
ing p(eatjhfoodi;obj), or something similar. The
following proposition shows that if p(vjc00;r) is
the same for each c00 in c0, where c0 is some
hypernym of c, then p(vjc0;r) will be equal to
p(vjc;r):
</bodyText>
<equation confidence="0.990050266666667">
p(vjc00
;r) = k for all c00
2 c0 ) p(vjc0;r) = k
The proof is as follows:
p(vjc0;r) = p(c0jv;r)p(vjr)
p(c0jr)
= p(vjr)
p(c0jr)
X
c00
2c0
p(c00jv;r)
= p(vjr)
p(c0jr)
X
c00
2c0
p(vjc00
;r)p(c00jr)
p(vjr)
= 1
p(c0jr)
k
X
c
00
2c0
p(c00
jr)
= k
</equation>
<bodyText confidence="0.999641071428572">
So in order to estimate p(vjc;r), we need a
way of searching for a set c0, where c0 is a hy-
pernym of c, which consists of concepts c00 which
have similar p(vjc00;r). Of course we cannot ex-
pect to \x0cnd a set consisting of concepts which
have identical p(vjc00;r), which the proposition
strictly requires, but if the p(vjc00;r) are similar,
then we can expect p(vjc0;r) to be a reasonable
estimate of p(vjc;r). We refer to the set c0 as
the `similarity-class&apos; of c, and the suitable hy-
pernym, c0, as top(c;v;r). The next section ex-
plains how we determine similarity classes. The
maximum likelihood estimates for the relevant
probabilities are given in Table 1.4
</bodyText>
<sectionHeader confidence="0.99805" genericHeader="method">
4 Finding Similarity-classes
</sectionHeader>
<bodyText confidence="0.9945785">
First we explain how we determine if a set of
concepts has similar p(vjc00;r) for each concept
c00 in the set. Then we explain howwe determine
top(c;v;r).
</bodyText>
<page confidence="0.985875">
4
</page>
<bodyText confidence="0.99889975">
Since we are assuming the data is not sense dis-
ambiguated, freq(c;v;r) cannot be obtained by sim-
ply counting senses. The standard approach, which is
adopted here, is to estimate freq(c;v;r) by distributing
the count for each noun n in syn(c) evenly among all
senses of the noun. Yarowsky (1992) and Resnik (1993)
explain how the noise introduced by this technique tends
to dissipate as counts are passed up the hierarchy.
</bodyText>
<tableCaption confidence="0.831364">
Table 1: Maximum Likelihood Estimates {
</tableCaption>
<bodyText confidence="0.758961">
freq(c;v;r) is the number of (n;v;r) triples in
the data in which n is being used to denote c.
</bodyText>
<equation confidence="0.750441157894737">
^
p(cjr) = freq(c;r)
freq(r)
=
P
v02V
freq(c;v
0
;r)
Pv02V
Pc02C
freq(c0
;v0
;r)
^
p(vjr) = freq(v;r)
freq(r)
=
Pc02C
</equation>
<figure confidence="0.884976142857143">
freq(c
0
;v;r)
Pv02V
Pc02C
freq(c
0
;v
0
;r)
^
p(vjc0;r) = freq(c0
;v;r)
freq(c
0
;r)
=
P
c00 2c0
freq(c
00
;v;r)
Pv02V
P
c002c0
freq(c00
;v0
;r)
</figure>
<bodyText confidence="0.941087222222222">
The method used for comparing the p(vjc00;r)
for c00 in some set c0, is based on the technique
in Clark and Weir (1999) used for \x0cnding homo-
geneous sets of concepts in the WordNet noun
hierarchy. Rather than directly compare esti-
mates of p(vjc00;r), which are likely to be unreli-
able, we consider the children of c0, and use esti-
mates based on counts which have accumulated
at the children. If c0 has children c0
</bodyText>
<equation confidence="0.854713666666667">
1;c0
2;::: ;c0
n
,
we compare p(vjc0
i
</equation>
<bodyText confidence="0.9643945">
;r) for each i. This is an
approximation, but if the p(vjc0
</bodyText>
<equation confidence="0.762034">
i
;r) are similar,
</equation>
<bodyText confidence="0.96737175">
then we assume that the p(vjc00;r) for c00 in c0
are similar too.
To determine whether the children of some
hypernym c0 have similar p(vjc0
</bodyText>
<equation confidence="0.935422">
i
), where c0
i
is the
</equation>
<bodyText confidence="0.997435625">
ith child, we apply a \x1f2
test to a contingency
table of frequency counts. Table 2 shows some
example frequencies for c0 equal to hnutrimenti,
in the object position of eat. The \x0cgures in
brackets are the expected values, based on the
marginal totals in the table. The null hypoth-
esis of the test is that p(vjc0
</bodyText>
<equation confidence="0.52512">
i
</equation>
<bodyText confidence="0.972944666666667">
;r) is the same for
each i. For Table 2 the null hypothesis is that
for every child, c0
</bodyText>
<equation confidence="0.99486125">
i
, of hnutrimenti, the probabil-
ity p(eatjc0
i
</equation>
<bodyText confidence="0.849169076923077">
;obj) is the same.
The log-likelihood \x1f2
statistic corresponding
to Table 2 is 4:8. The log-likelihood \x1f2
statistic
is used rather than the Pearson&apos;s \x1f2
statistic
because it is thought to be more appropriate
when the counts in the contingency table are
low (Dunning, 1993). This tends to occur when
the test is being applied to a set of concepts
near the foot of the hierarchy.5
We compared
</bodyText>
<page confidence="0.967352">
5
</page>
<bodyText confidence="0.983534">
Fisher&apos;s exact test could be used for tables with low
counts, but we do not do so because tables dominated
by low counts are likely to have a high percentage of
noise, due to the way counts for a noun are split among
</bodyText>
<equation confidence="0.944696272727273">
\x0cTable 2: Contingency table for children of hnutrimenti
ci
^
freq(ci;eat;obj) ^
freq(ci;obj), ^
freq(ci;obj) =
^
freq(ci;eat;obj)
Pv2V
^
freq(ci;v;obj)
</equation>
<table confidence="0.8303055">
hmilki 0.0 (0.6) 9.0 (8.4) 9.0
hmeali 8.5 (5.6) 78.0 (80.9) 86.5
hcoursei 1.3 (1.7) 24.7 (24.3) 26.0
hdishi 5.3 (5.7) 82.3 (81.9) 87.6
hdelicacyi 0.3 (1.8) 27.4 (25.9) 27.7
15.4 221.4 236.8
the performance of log-likelihood \x1f2
and Pear-
son&apos;s \x1f2
using the PP-attachment experiment
</table>
<bodyText confidence="0.996570333333333">
described in Section 5. It was found that the
log-likelihood \x1f2
test did perform slightly bet-
ter. For a signi\x0ccance level of 0:05 (which is the
level used in the experiments), with 4 degrees
of freedom, the critical value is 14.86 (Howell,
1997). Thus in this case, the null hypothesis
would not be rejected.
In order to determine top(c;v;r),we compare
p(vjci;r) for the children of the hypernyms of
c. Initially top(c;v;r) is assigned to be the con-
cept c itself. Then, by working up the hierarchy,
top(c;v;r) is reassigned to be successive hyper-
nyms of c until the siblings of top(c;v;r) have
signi\x0ccantly di\x0berent probabilities. In cases
where a concept has more than one parent, the
parent is chosen which results in the lowest \x1f2
value as this indicates the p(vjci;r) are more
similar. The set top(c;v;r) is the similarity-
class of c for verb v and position r.
The next section provides evidence that the
technique for choosing top(c;v;r),which we call
the `similarity-class&apos; technique, does select an
appropriate level of generalisation.
</bodyText>
<sectionHeader confidence="0.869862" genericHeader="method">
5 Experiments using PP-attachment
</sectionHeader>
<bodyText confidence="0.986430666666667">
ambiguity
The PP-attachment problem we address con-
siders 4-tuples of the form v;n1;pr;n2, and
the problem is to decide whether the prepo-
sitional phrase pr n2 attaches to the verb v
or the noun n1. For example, in the fol-
lowing case the problem is to decide whether
alternative senses. We rely on the log-likelihood \x1f2
test
returning a non-signi\x0ccant result in these cases.
from minister attaches to await or approval:
await approval from minister
We chose the PP-attachment problem because
PP-attachmentis a pervasive form of ambiguity,
and there exist standard training and test data,
which makes for easy comparisons with other
approaches. This problem has been tackled by a
number of researchers. Brill and Resnik (1994),
Ratnaparkhi et al. (1994), Collins (1995), Za-
vrel and Daelemans (1997) all report results be-
tween 81% and 85%, with Stetina and Nagao
(1997) reporting a result of 88%, which matches
the human performance on this task reported by
Ratnaparkhi et al. (1994).
Although the PP-attachment problem has
characteristics that make it suitable for evalua-
tion, it presents a much bigger sparse data prob-
lem than would be expected in other problems
such as relative clause attachment. The reason
for this is that we need to consider how a con-
cept is associated with combinations of predi-
cates and prepositions. The approach described
here uses probabilities of the form p(c;prjv)
and p(c;prjn1), where c 2 cn(n2). This means
that for many predicate/preposition combina-
tions which occur infrequently in the data, there
are few examples of n2 which can be used for
populating WordNet in these cases. Despite
this, we were still able to carry out an evalu-
ation by considering subsets of the test data for
which the relevant predicate/preposition com-
binations did occur frequently in the training
data.
We decide on the attachment site by compar-
\x0cing p(cv;prjv) and p(cn1;prjn1), where
</bodyText>
<equation confidence="0.995170666666667">
cv = arg max
c2cn(n2)
p(c;prjv)
cn1 = arg max
c2cn(n2)
p(c;prjn1)
</equation>
<bodyText confidence="0.999076238095238">
The sense of n2 is chosen which maximises
the relevant probability in each potential at-
tachment case. If p(cv;prjv) is greater than
p(cn1;prjn1), the attachment is made to v, oth-
erwise to n1. If n2 is not in WordNet we com-
pare p(prjv) and p(prjn1). Probabilities of the
form p(c;prjv) and p(c;prjn1) are used rather
than p(cjv;pr) and p(cjn1;pr), because the as-
sociation between the preposition and v and n1
contains useful information. In fact, for a lot
of cases this information alone can be used to
decide on the correct attachment site. The orig-
inal corpus-based method of Hindle and Rooth
(1993) used exactly this information. Thus the
method described here can be thoughtof as Hin-
dle and Rooth&apos;s method with additional class-
based information about n2.
In order to estimate p(cv;prjv) (and
p(cn1;prjn1)) we apply the same procedure
as described in Section 3, \x0crst rewriting the
probability using Bayes&apos; rule:
</bodyText>
<equation confidence="0.97140675">
p(cv ;prjv) = p(vjcv;pr)p(cv;pr)
p(v)
= p(vjcv;pr)p(prjcv)p(cv )
p(v)
</equation>
<bodyText confidence="0.994497066666667">
The probabilities p(cv) and p(v) can be es-
timated using maximum likelihood estimates,
and p(vjcv;pr) and p(prjcv) can be esti-
mated using maximum likelihood estimates of
p(vjtop(cv;v;pr);pr) and p(prjtop(cv;pr)) re-
spectively.6
We used the training and test data described
in Ratnaparkhi et al. (1994), which was taken
from the Penn Treebank and has now become
the standard data set for this task. The data
set consists of tuples of the form (v, n1, pr, n2),
together with the attachment site for each tu-
ple. There is also a development set to prevent
implicit training on the test set during develop-
ment. We extracted (v, pr, n2) and (n1, pr, n2)
</bodyText>
<page confidence="0.991222">
6
</page>
<bodyText confidence="0.99544508">
In Section 4 we only gave the procedure for deter-
mining top(cv;v;pr), but top(cv;pr) can be determined
in an analogous fashion.
triples from the training set, and in order to in-
crease the number of training triples, we also
extracted triples from unambiguous cases of at-
tachment in the Penn Treebank. We prepro-
cessed the training and test databy lemmatising
the words, replacing numerical amounts with
the words `de\x0cnite quantity&apos;, replacing mone-
tary amounts with the words `sum of money&apos;
etc. We then ignored those triples in the re-
sulting training set (but not test set) for which
n2 was not in WordNet, which left a total of
66;881 triples of training data. The test set
contains 3;097 examples.
Table 3 gives some examples of the ex-
tent to which the similarity-class technique
is generalising, using the training data just
described, and a signi\x0ccance level of 0:05.
The chosen hypernym is shown in upper
case. Note that the WordNet hierarchy con-
sists of nine separate sub-hierarchies, headed
by such concepts as hentityi, habstractioni,
hpsychological featurei, but we assume the ex-
istence of a single root which dominates each
of the sub-hierarchies, which is referred to as
hrooti. In cases where WordNet is very sparsely
populated, it is preferable to go to hrooti,
rather than stay at the root of one of the sub-
hierarchies where the data may be noisy or too
sparse to be of any use. The table shows that
with the amount ofdata available fromthe Tree-
bank, the similarity-class technique is selecting
a level at or close to hrooti in many cases.
We compared the similarity-class technique
with \x0cxing the level of generalisation. Two \x0cxed
levels were used: the root of the entire hierar-
chy (hrooti), and the set consisting of the roots
of each of the 9 sub-hierarchies. The procedure
which always selects hrootiignores any informa-
tion about n2, and is equivalent to comparing
p(prjv) and p(prjn1), which is the Hindle and
Rooth approach. The results on the 3;097 test
cases are shown in Table 4. We used a signi\x0c-
cance level \x0b of 0:05 for the \x1f2
test.7
As the table shows, the disambiguation ac-
curacy is below the state of the art. However,
the results are comparable with those of Li and
</bodyText>
<page confidence="0.980578">
7
</page>
<bodyText confidence="0.957311">
Similar results were obtained using alternative levels
of signi\x0ccance. Rather than simply selecting a value for
\x0b such as 0:05, \x0b can be treated as a parameter of the
model, whose optimum value can be obtained by running
the disambiguation method on some held-out supervised
data.
</bodyText>
<tableCaption confidence="0.44832">
\x0cTable 3: How the similarity-class technique chooses top(c;v;pr) and top(c;n1;pr)
</tableCaption>
<figure confidence="0.96438175">
(n1;pr;c) Hypernyms of c
(bid,for,hcompanyi) hcompanyihestablishmentihorganisationihsocial groupihGROUPihrooti
(concern,about,hriski) hriskihventureihtaskihworkihactivityihactihROOTi
(billion,in,hcashi) hcashihcurrencyihmonetary systemihassetihPOSSESSIONihrooti
(v;pr;c)
(notify,of,htransactioni) htransactionihgroup actionihactihROOTi
(close,at,hdefinite quantityi) hDEFINITE QUANTITYihmeasureihabstractionihrooti
(meet,with,hofficiali) hofficialihadjudicatorihpersonihlife formihCAUSAL AGENTihentityihrooti
</figure>
<tableCaption confidence="0.977692">
Table 4: Complete test set {3097 test cases
</tableCaption>
<table confidence="0.9230194">
Generalisation technique % correct
Similarity-class 80:3
Select root of sub-hierarchy 77:9
Always select hrooti 79:0
Abe (1998) who adopt a similar approach us-
</table>
<bodyText confidence="0.9844814375">
ing WordNet, but with a di\x0berent training and
test set. Li and Abe improved on the Hin-
dle and Rooth technique by 1:5%, which is in
line with our results. As an evaluation of the
similarity-class technique, the result is incon-
clusive. The reason for this is that when the
technique was being used to estimate p(vjcv;pr)
and p(n1jcn1;pr), in many cases the root of the
hierarchy was being chosen as the appropriate
level of generalisation, due to a sparsely popu-
lated WordNet in that instance. Recall that this
is largely due to the fact that we are attempt-
ing to populate WordNet for combinations of
predicates and prepositions. In such cases the
similarity-class technique is not helping because
there is very little or no information about n2.8
</bodyText>
<page confidence="0.971768">
8
</page>
<bodyText confidence="0.993947714285714">
In an e\x0bort to obtain more data we applied the ex-
traction heuristic of Ratnaparkhi (1998) to Wall Street
Journal text, which increased the number of training
triples by a factor of 10. This only achieved comparable
results, however, presumably because the high volume
of noise in the data outweighs the bene\x0ct of the increase
in data size. Ratnaparkhi reports only 69% accuracy for
</bodyText>
<tableCaption confidence="0.978278">
Table 5: hrooti being selected for both attach-
</tableCaption>
<table confidence="0.9765342">
ment points { 113 test cases
Generalisation technique % correct
Similarity-class 90:3
Select root of sub-hierarchy 81:4
Always select hrooti 79:6
</table>
<tableCaption confidence="0.840128">
Table 6: hrooti being selected for at most one
</tableCaption>
<table confidence="0.9316498">
of the attachment points { 1032 test cases
Generalisation technique % correct
Similarity-class 88:1
Select root of sub-hierarchy 85:5
Always select hrooti 85:5
</table>
<bodyText confidence="0.99857">
In order to evaluate the similarity-class tech-
nique further, we took those test cases for which
the root was not being selected when estimating
both p(vjcv;pr) and p(n1jcn1;pr). This applied
to 113 cases. The results are given in Table 5.
We also took those test cases for which the root
was being selected when estimating at most one
of p(vjcv;pr) and p(n1jcn1;pr). This applied to
1032 test cases. The results are shown in Ta-
ble 6.
the extraction heuristic when applied to the Penn Tree-
bank (excluding cases where the preposition is of).
</bodyText>
<sectionHeader confidence="0.923485" genericHeader="conclusions">
\x0c6 Conclusions
</sectionHeader>
<bodyText confidence="0.998906090909091">
We have shown that when instances of Word-
Net are well populated with examples of
n2, the method described here for solving
PP-attachment ambiguities is highly accurate.
When WordNet is sparsely populated, the
method automatically resorts to comparing just
the preposition and each of the potential attach-
ment sites, as the similarity-class technique will
select hrooti as the appropriate level of general-
isation for n2 in such cases. We have also shown
the similarity-class technique to be superior to
using a \x0cxed level of generalisation in WordNet.
Further work will look at how to integrate
probabilities such as p(cjv;r) into a model of
dependency structure, similar to that of Collins
(1996) and Collins (1997), which can be used
for parse selection. However, knowledge of se-
lectional preferences cannot by itself solve the
problem of structural disambiguation, and this
further work will also look at using additional
knowledge, such as subcategorisation informa-
tion.
</bodyText>
<sectionHeader confidence="0.989933" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998008546666667">
Eric Brill and Philip Resnik. 1994. A rule-based
approach to prepositional phrase attachment
disambiguation. In Proceedings of the \x0cf-
teenth International Conference on Compu-
tational Linguistics.
Eugene Charniak. 1993. Statistical Language
Learning. The MIT Press.
Stephen Clark and David Weir. 1999. An it-
erative approach to estimating frequencies
over a semantic hierarchy. In Proceedings of
the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and
Very Large Corpora, pages 258{265.
Michael Collins. 1995. Prepositional phrase
attachment through a backed-o\x0b model. In
Proceedings of the Third Workshop on Very
Large Corpora, pages 27{38, Cambridge,
Massachusetts.
Michael Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In
Proceedings of the 34th Annual Meeting of the
ACL, pages 184{191.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the
Association for Computational Linguistics,
pages 16{23.
Ted Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Com-
putational Linguistics, 19(1):61{74.
Christiane Fellbaum, editor. 1998. WordNet
An Electronic Lexical Database. The MIT
Press.
Donald Hindle and Mats Rooth. 1993. Struc-
tural ambiguity and lexical relations. Com-
putational Linguistics, 19(1):103{120.
David Howell. 1997. Statistical Methods for
Psychology: 4th ed. Duxbury Press.
Hang Li and Naoki Abe. 1998. Generaliz-
ing case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217{244.
Adwait Ratnaparkhi, Je\x0b Reynar, and Salim
Roukos. 1994. A maximum entropy model
for prepositional phrase attachment. In Pro-
ceedings of the ARPA Human Language Tech-
nology Workshop, pages 250{255.
Adwait Ratnaparkhi. 1998. Unsupervised sta-
tistical models for prepositional phrase at-
tachment. In Proceedings of the Seventeenth
International Conference on Computational
Linguistics, Montreal, Canada, Aug.
Philip Resnik. 1993. Selection and Informa-
tion: A Class-Based Approach to Lexical Re-
lationships. Ph.D. thesis, University of Penn-
sylvania.
Francesc Ribas. 1995. On learning more appro-
priate selectional restrictions. In Proceedings
of the Seventh Conference of the European
Chapter of the Association for Computational
Linguistics, Dublin, Ireland.
Jiri Stetina and Makoto Nagao. 1997. Corpus
based PP attachment ambiguity resolution
with a semantic dictionary. In Proceedings of
the Fifth Workshop on Very Large Corpora,
pages 66{80, Beijing and Hong Kong.
David Yarowsky. 1992. Word-sense disam-
biguation using statistical models of Roget&apos;s
categories trained on large corpora. In Pro-
ceedings of COLING-92, pages 454{460.
Jakub Zavrel and Walter Daelemans. 1997.
Memory-based learning: Using similarity for
smoothing. In Proceedings of ACL/EACL-
97, Madrid, Spain.
\x0c&quot;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960489">
<title confidence="0.9987415">b&quot;A Class-based Probabilistic approach to Structural Disambiguation</title>
<author confidence="0.999951">Stephen Clark</author>
<author confidence="0.999951">David Weir</author>
<affiliation confidence="0.999786">School of Cognitive and Computing Sciences University of Sussex</affiliation>
<address confidence="0.999811">Brighton, BN1 9HQ, UK</address>
<email confidence="0.999135">fstephecl,davidwg@cogs.susx.ac.uk</email>
<abstract confidence="0.996639818181818">Knowledge of which words are able to \x0cll particular argument slots of a predicate can be used for structural disambiguation. This paper describes a proposal for acquiring such knowledge, and in line with much of the recent work in this area, a probabilistic approach is taken. We develop a novel way of using a semantic hierarchy to estimate the probabilities, and demonstrate the general approach using a prepositional phrase attachment experiment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Philip Resnik</author>
</authors>
<title>A rule-based approach to prepositional phrase attachment disambiguation.</title>
<date>1994</date>
<booktitle>In Proceedings of the \x0cfteenth International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="14082" citStr="Brill and Resnik (1994)" startWordPosition="2449" endWordPosition="2452">is to decide whether the prepositional phrase pr n2 attaches to the verb v or the noun n1. For example, in the following case the problem is to decide whether alternative senses. We rely on the log-likelihood \x1f2 test returning a non-signi\x0ccant result in these cases. from minister attaches to await or approval: await approval from minister We chose the PP-attachment problem because PP-attachmentis a pervasive form of ambiguity, and there exist standard training and test data, which makes for easy comparisons with other approaches. This problem has been tackled by a number of researchers. Brill and Resnik (1994), Ratnaparkhi et al. (1994), Collins (1995), Zavrel and Daelemans (1997) all report results between 81% and 85%, with Stetina and Nagao (1997) reporting a result of 88%, which matches the human performance on this task reported by Ratnaparkhi et al. (1994). Although the PP-attachment problem has characteristics that make it suitable for evaluation, it presents a much bigger sparse data problem than would be expected in other problems such as relative clause attachment. The reason for this is that we need to consider how a concept is associated with combinations of predicates and prepositions. </context>
</contexts>
<marker>Brill, Resnik, 1994</marker>
<rawString>Eric Brill and Philip Resnik. 1994. A rule-based approach to prepositional phrase attachment disambiguation. In Proceedings of the \x0cfteenth International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="863" citStr="Charniak, 1993" startWordPosition="130" endWordPosition="131">ds are able to \x0cll particular argument slots of a predicate can be used for structural disambiguation. This paper describes a proposal for acquiring such knowledge, and in line with much of the recent work in this area, a probabilistic approach is taken. We develop a novel way of using a semantic hierarchy to estimate the probabilities, and demonstrate the general approach using a prepositional phrase attachment experiment. 1 Introduction Knowledge of which words are able to \x0cll particular argument slots of a predicate can be used for structural disambiguation. In the following example (Charniak, 1993), the fact that dog, rather than prize, is often the subject of run, can be used to decide on the attachment site of the relative clause: Fred awarded a prize for the dog that ran the fastest We describe a proposal for acquiring such knowledge, and as in other recent work in this area (Resnik, 1993; Li and Abe, 1998), a probabilistic approach is taken. Using probabilities accords with the intuition that there are no absolute constraints on the arguments of predicates, but rather that constraints are satis\x0ced to a certain degree (Resnik, 1993). Unfortunately, de\x0cning probabilities in term</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>Eugene Charniak. 1993. Statistical Language Learning. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>An iterative approach to estimating frequencies over a semantic hierarchy.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>258--265</pages>
<contexts>
<context position="10171" citStr="Clark and Weir (1999)" startWordPosition="1782" endWordPosition="1785">n how the noise introduced by this technique tends to dissipate as counts are passed up the hierarchy. Table 1: Maximum Likelihood Estimates { freq(c;v;r) is the number of (n;v;r) triples in the data in which n is being used to denote c. ^ p(cjr) = freq(c;r) freq(r) = P v02V freq(c;v 0 ;r) Pv02V Pc02C freq(c0 ;v0 ;r) ^ p(vjr) = freq(v;r) freq(r) = Pc02C freq(c 0 ;v;r) Pv02V Pc02C freq(c 0 ;v 0 ;r) ^ p(vjc0;r) = freq(c0 ;v;r) freq(c 0 ;r) = P c00 2c0 freq(c 00 ;v;r) Pv02V P c002c0 freq(c00 ;v0 ;r) The method used for comparing the p(vjc00;r) for c00 in some set c0, is based on the technique in Clark and Weir (1999) used for \x0cnding homogeneous sets of concepts in the WordNet noun hierarchy. Rather than directly compare estimates of p(vjc00;r), which are likely to be unreliable, we consider the children of c0, and use estimates based on counts which have accumulated at the children. If c0 has children c0 1;c0 2;::: ;c0 n , we compare p(vjc0 i ;r) for each i. This is an approximation, but if the p(vjc0 i ;r) are similar, then we assume that the p(vjc00;r) for c00 in c0 are similar too. To determine whether the children of some hypernym c0 have similar p(vjc0 i ), where c0 i is the ith child, we apply a </context>
</contexts>
<marker>Clark, Weir, 1999</marker>
<rawString>Stephen Clark and David Weir. 1999. An iterative approach to estimating frequencies over a semantic hierarchy. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 258{265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Prepositional phrase attachment through a backed-o\x0b model.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>27--38</pages>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="14125" citStr="Collins (1995)" startWordPosition="2457" endWordPosition="2458">attaches to the verb v or the noun n1. For example, in the following case the problem is to decide whether alternative senses. We rely on the log-likelihood \x1f2 test returning a non-signi\x0ccant result in these cases. from minister attaches to await or approval: await approval from minister We chose the PP-attachment problem because PP-attachmentis a pervasive form of ambiguity, and there exist standard training and test data, which makes for easy comparisons with other approaches. This problem has been tackled by a number of researchers. Brill and Resnik (1994), Ratnaparkhi et al. (1994), Collins (1995), Zavrel and Daelemans (1997) all report results between 81% and 85%, with Stetina and Nagao (1997) reporting a result of 88%, which matches the human performance on this task reported by Ratnaparkhi et al. (1994). Although the PP-attachment problem has characteristics that make it suitable for evaluation, it presents a much bigger sparse data problem than would be expected in other problems such as relative clause attachment. The reason for this is that we need to consider how a concept is associated with combinations of predicates and prepositions. The approach described here uses probabilit</context>
</contexts>
<marker>Collins, 1995</marker>
<rawString>Michael Collins. 1995. Prepositional phrase attachment through a backed-o\x0b model. In Proceedings of the Third Workshop on Very Large Corpora, pages 27{38, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL,</booktitle>
<pages>184--191</pages>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Annual Meeting of the ACL, pages 184{191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16{23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="11484" citStr="Dunning, 1993" startWordPosition="2024" endWordPosition="2025">0 equal to hnutrimenti, in the object position of eat. The \x0cgures in brackets are the expected values, based on the marginal totals in the table. The null hypothesis of the test is that p(vjc0 i ;r) is the same for each i. For Table 2 the null hypothesis is that for every child, c0 i , of hnutrimenti, the probability p(eatjc0 i ;obj) is the same. The log-likelihood \x1f2 statistic corresponding to Table 2 is 4:8. The log-likelihood \x1f2 statistic is used rather than the Pearson&apos;s \x1f2 statistic because it is thought to be more appropriate when the counts in the contingency table are low (Dunning, 1993). This tends to occur when the test is being applied to a set of concepts near the foot of the hierarchy.5 We compared 5 Fisher&apos;s exact test could be used for tables with low counts, but we do not do so because tables dominated by low counts are likely to have a high percentage of noise, due to the way counts for a noun are split among \x0cTable 2: Contingency table for children of hnutrimenti ci ^ freq(ci;eat;obj) ^ freq(ci;obj), ^ freq(ci;obj) = ^ freq(ci;eat;obj) Pv2V ^ freq(ci;v;obj) hmilki 0.0 (0.6) 9.0 (8.4) 9.0 hmeali 8.5 (5.6) 78.0 (80.9) 86.5 hcoursei 1.3 (1.7) 24.7 (24.3) 26.0 hdishi</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61{74.</rawString>
</citation>
<citation valid="true">
<title>WordNet An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="3004" citStr="(1998)" startWordPosition="514" endWordPosition="514">ase our probability estimate on that instead. The level at which hchickeni is represented is crucial: it should be high enough for adequate counts to have accumulated, but not too high so that the hypernym is no longer representative of hchickeni. An example of a hypernym which would be too high is hentityi, as not all entities are semantically similar with respect to the object position of eat. The problem of choosing an appropriate level in the hierarchy at which to represent a particular noun sense (given a predicate and argument position) has been investigated by Resnik (1993), Li and Abe (1998) and Ribas (1995). The learning mechanism presented here is a novel approach based on \x0cnding semantically similar sets of concepts in a hierarchy. We demonstrate the e\x0bectiveness of our approach using a PP-attachment experiment. 2 The Input Data and Semantic Hierarchy The data used to estimate the probabilities is a multiset of `co-occurrence triples&apos;: a noun 1 We use italics when referring to words, and angled brackets for concepts. This notation does not always pick out a concept uniquely, but the context should make clear the concept being referred to. \x0clemma, verb lemma, and argum</context>
<context position="20229" citStr="(1998)" startWordPosition="3412" endWordPosition="3412">nisationihsocial groupihGROUPihrooti (concern,about,hriski) hriskihventureihtaskihworkihactivityihactihROOTi (billion,in,hcashi) hcashihcurrencyihmonetary systemihassetihPOSSESSIONihrooti (v;pr;c) (notify,of,htransactioni) htransactionihgroup actionihactihROOTi (close,at,hdefinite quantityi) hDEFINITE QUANTITYihmeasureihabstractionihrooti (meet,with,hofficiali) hofficialihadjudicatorihpersonihlife formihCAUSAL AGENTihentityihrooti Table 4: Complete test set {3097 test cases Generalisation technique % correct Similarity-class 80:3 Select root of sub-hierarchy 77:9 Always select hrooti 79:0 Abe (1998) who adopt a similar approach using WordNet, but with a di\x0berent training and test set. Li and Abe improved on the Hindle and Rooth technique by 1:5%, which is in line with our results. As an evaluation of the similarity-class technique, the result is inconclusive. The reason for this is that when the technique was being used to estimate p(vjcv;pr) and p(n1jcn1;pr), in many cases the root of the hierarchy was being chosen as the appropriate level of generalisation, due to a sparsely populated WordNet in that instance. Recall that this is largely due to the fact that we are attempting to pop</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="15922" citStr="Hindle and Rooth (1993)" startWordPosition="2756" endWordPosition="2759">x c2cn(n2) p(c;prjn1) The sense of n2 is chosen which maximises the relevant probability in each potential attachment case. If p(cv;prjv) is greater than p(cn1;prjn1), the attachment is made to v, otherwise to n1. If n2 is not in WordNet we compare p(prjv) and p(prjn1). Probabilities of the form p(c;prjv) and p(c;prjn1) are used rather than p(cjv;pr) and p(cjn1;pr), because the association between the preposition and v and n1 contains useful information. In fact, for a lot of cases this information alone can be used to decide on the correct attachment site. The original corpus-based method of Hindle and Rooth (1993) used exactly this information. Thus the method described here can be thoughtof as Hindle and Rooth&apos;s method with additional classbased information about n2. In order to estimate p(cv;prjv) (and p(cn1;prjn1)) we apply the same procedure as described in Section 3, \x0crst rewriting the probability using Bayes&apos; rule: p(cv ;prjv) = p(vjcv;pr)p(cv;pr) p(v) = p(vjcv;pr)p(prjcv)p(cv ) p(v) The probabilities p(cv) and p(v) can be estimated using maximum likelihood estimates, and p(vjcv;pr) and p(prjcv) can be estimated using maximum likelihood estimates of p(vjtop(cv;v;pr);pr) and p(prjtop(cv;pr)) re</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Donald Hindle and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103{120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Howell</author>
</authors>
<date>1997</date>
<booktitle>Statistical Methods for Psychology: 4th</booktitle>
<editor>ed.</editor>
<publisher>Duxbury Press.</publisher>
<contexts>
<context position="12513" citStr="Howell, 1997" startWordPosition="2201" endWordPosition="2202">j) ^ freq(ci;obj), ^ freq(ci;obj) = ^ freq(ci;eat;obj) Pv2V ^ freq(ci;v;obj) hmilki 0.0 (0.6) 9.0 (8.4) 9.0 hmeali 8.5 (5.6) 78.0 (80.9) 86.5 hcoursei 1.3 (1.7) 24.7 (24.3) 26.0 hdishi 5.3 (5.7) 82.3 (81.9) 87.6 hdelicacyi 0.3 (1.8) 27.4 (25.9) 27.7 15.4 221.4 236.8 the performance of log-likelihood \x1f2 and Pearson&apos;s \x1f2 using the PP-attachment experiment described in Section 5. It was found that the log-likelihood \x1f2 test did perform slightly better. For a signi\x0ccance level of 0:05 (which is the level used in the experiments), with 4 degrees of freedom, the critical value is 14.86 (Howell, 1997). Thus in this case, the null hypothesis would not be rejected. In order to determine top(c;v;r),we compare p(vjci;r) for the children of the hypernyms of c. Initially top(c;v;r) is assigned to be the concept c itself. Then, by working up the hierarchy, top(c;v;r) is reassigned to be successive hypernyms of c until the siblings of top(c;v;r) have signi\x0ccantly di\x0berent probabilities. In cases where a concept has more than one parent, the parent is chosen which results in the lowest \x1f2 value as this indicates the p(vjci;r) are more similar. The set top(c;v;r) is the similarityclass of c</context>
</contexts>
<marker>Howell, 1997</marker>
<rawString>David Howell. 1997. Statistical Methods for Psychology: 4th ed. Duxbury Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="1181" citStr="Li and Abe, 1998" startWordPosition="188" endWordPosition="191">timate the probabilities, and demonstrate the general approach using a prepositional phrase attachment experiment. 1 Introduction Knowledge of which words are able to \x0cll particular argument slots of a predicate can be used for structural disambiguation. In the following example (Charniak, 1993), the fact that dog, rather than prize, is often the subject of run, can be used to decide on the attachment site of the relative clause: Fred awarded a prize for the dog that ran the fastest We describe a proposal for acquiring such knowledge, and as in other recent work in this area (Resnik, 1993; Li and Abe, 1998), a probabilistic approach is taken. Using probabilities accords with the intuition that there are no absolute constraints on the arguments of predicates, but rather that constraints are satis\x0ced to a certain degree (Resnik, 1993). Unfortunately, de\x0cning probabilities in terms of words leads to a model with a vast number of parameters, resulting in a sparse data problem. To overcome this, we propose to de\x0cne a probability model in terms of senses from a semantic hierarchy, exploiting the fact that senses of nouns can be grouped together into semantically similar classes. We use the se</context>
<context position="3004" citStr="Li and Abe (1998)" startWordPosition="511" endWordPosition="514">oodi, and base our probability estimate on that instead. The level at which hchickeni is represented is crucial: it should be high enough for adequate counts to have accumulated, but not too high so that the hypernym is no longer representative of hchickeni. An example of a hypernym which would be too high is hentityi, as not all entities are semantically similar with respect to the object position of eat. The problem of choosing an appropriate level in the hierarchy at which to represent a particular noun sense (given a predicate and argument position) has been investigated by Resnik (1993), Li and Abe (1998) and Ribas (1995). The learning mechanism presented here is a novel approach based on \x0cnding semantically similar sets of concepts in a hierarchy. We demonstrate the e\x0bectiveness of our approach using a PP-attachment experiment. 2 The Input Data and Semantic Hierarchy The data used to estimate the probabilities is a multiset of `co-occurrence triples&apos;: a noun 1 We use italics when referring to words, and angled brackets for concepts. This notation does not always pick out a concept uniquely, but the context should make clear the concept being referred to. \x0clemma, verb lemma, and argum</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the MDL principle. Computational Linguistics, 24(2):217{244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>Je\x0b Reynar</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy model for prepositional phrase attachment.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop,</booktitle>
<pages>250--255</pages>
<contexts>
<context position="14109" citStr="Ratnaparkhi et al. (1994)" startWordPosition="2453" endWordPosition="2456">prepositional phrase pr n2 attaches to the verb v or the noun n1. For example, in the following case the problem is to decide whether alternative senses. We rely on the log-likelihood \x1f2 test returning a non-signi\x0ccant result in these cases. from minister attaches to await or approval: await approval from minister We chose the PP-attachment problem because PP-attachmentis a pervasive form of ambiguity, and there exist standard training and test data, which makes for easy comparisons with other approaches. This problem has been tackled by a number of researchers. Brill and Resnik (1994), Ratnaparkhi et al. (1994), Collins (1995), Zavrel and Daelemans (1997) all report results between 81% and 85%, with Stetina and Nagao (1997) reporting a result of 88%, which matches the human performance on this task reported by Ratnaparkhi et al. (1994). Although the PP-attachment problem has characteristics that make it suitable for evaluation, it presents a much bigger sparse data problem than would be expected in other problems such as relative clause attachment. The reason for this is that we need to consider how a concept is associated with combinations of predicates and prepositions. The approach described here</context>
<context position="16608" citStr="Ratnaparkhi et al. (1994)" startWordPosition="2858" endWordPosition="2861">e can be thoughtof as Hindle and Rooth&apos;s method with additional classbased information about n2. In order to estimate p(cv;prjv) (and p(cn1;prjn1)) we apply the same procedure as described in Section 3, \x0crst rewriting the probability using Bayes&apos; rule: p(cv ;prjv) = p(vjcv;pr)p(cv;pr) p(v) = p(vjcv;pr)p(prjcv)p(cv ) p(v) The probabilities p(cv) and p(v) can be estimated using maximum likelihood estimates, and p(vjcv;pr) and p(prjcv) can be estimated using maximum likelihood estimates of p(vjtop(cv;v;pr);pr) and p(prjtop(cv;pr)) respectively.6 We used the training and test data described in Ratnaparkhi et al. (1994), which was taken from the Penn Treebank and has now become the standard data set for this task. The data set consists of tuples of the form (v, n1, pr, n2), together with the attachment site for each tuple. There is also a development set to prevent implicit training on the test set during development. We extracted (v, pr, n2) and (n1, pr, n2) 6 In Section 4 we only gave the procedure for determining top(cv;v;pr), but top(cv;pr) can be determined in an analogous fashion. triples from the training set, and in order to increase the number of training triples, we also extracted triples from unam</context>
</contexts>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>Adwait Ratnaparkhi, Je\x0b Reynar, and Salim Roukos. 1994. A maximum entropy model for prepositional phrase attachment. In Proceedings of the ARPA Human Language Technology Workshop, pages 250{255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Unsupervised statistical models for prepositional phrase attachment.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Computational Linguistics,</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="21104" citStr="Ratnaparkhi (1998)" startWordPosition="3562" endWordPosition="3563">conclusive. The reason for this is that when the technique was being used to estimate p(vjcv;pr) and p(n1jcn1;pr), in many cases the root of the hierarchy was being chosen as the appropriate level of generalisation, due to a sparsely populated WordNet in that instance. Recall that this is largely due to the fact that we are attempting to populate WordNet for combinations of predicates and prepositions. In such cases the similarity-class technique is not helping because there is very little or no information about n2.8 8 In an e\x0bort to obtain more data we applied the extraction heuristic of Ratnaparkhi (1998) to Wall Street Journal text, which increased the number of training triples by a factor of 10. This only achieved comparable results, however, presumably because the high volume of noise in the data outweighs the bene\x0ct of the increase in data size. Ratnaparkhi reports only 69% accuracy for Table 5: hrooti being selected for both attachment points { 113 test cases Generalisation technique % correct Similarity-class 90:3 Select root of sub-hierarchy 81:4 Always select hrooti 79:6 Table 6: hrooti being selected for at most one of the attachment points { 1032 test cases Generalisation techniq</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Unsupervised statistical models for prepositional phrase attachment. In Proceedings of the Seventeenth International Conference on Computational Linguistics, Montreal, Canada, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1162" citStr="Resnik, 1993" startWordPosition="186" endWordPosition="187">ierarchy to estimate the probabilities, and demonstrate the general approach using a prepositional phrase attachment experiment. 1 Introduction Knowledge of which words are able to \x0cll particular argument slots of a predicate can be used for structural disambiguation. In the following example (Charniak, 1993), the fact that dog, rather than prize, is often the subject of run, can be used to decide on the attachment site of the relative clause: Fred awarded a prize for the dog that ran the fastest We describe a proposal for acquiring such knowledge, and as in other recent work in this area (Resnik, 1993; Li and Abe, 1998), a probabilistic approach is taken. Using probabilities accords with the intuition that there are no absolute constraints on the arguments of predicates, but rather that constraints are satis\x0ced to a certain degree (Resnik, 1993). Unfortunately, de\x0cning probabilities in terms of words leads to a model with a vast number of parameters, resulting in a sparse data problem. To overcome this, we propose to de\x0cne a probability model in terms of senses from a semantic hierarchy, exploiting the fact that senses of nouns can be grouped together into semantically similar cla</context>
<context position="2985" citStr="Resnik (1993)" startWordPosition="509" endWordPosition="510">nym, such as hfoodi, and base our probability estimate on that instead. The level at which hchickeni is represented is crucial: it should be high enough for adequate counts to have accumulated, but not too high so that the hypernym is no longer representative of hchickeni. An example of a hypernym which would be too high is hentityi, as not all entities are semantically similar with respect to the object position of eat. The problem of choosing an appropriate level in the hierarchy at which to represent a particular noun sense (given a predicate and argument position) has been investigated by Resnik (1993), Li and Abe (1998) and Ribas (1995). The learning mechanism presented here is a novel approach based on \x0cnding semantically similar sets of concepts in a hierarchy. We demonstrate the e\x0bectiveness of our approach using a PP-attachment experiment. 2 The Input Data and Semantic Hierarchy The data used to estimate the probabilities is a multiset of `co-occurrence triples&apos;: a noun 1 We use italics when referring to words, and angled brackets for concepts. This notation does not always pick out a concept uniquely, but the context should make clear the concept being referred to. \x0clemma, ve</context>
<context position="9543" citStr="Resnik (1993)" startWordPosition="1664" endWordPosition="1665">termine similarity classes. The maximum likelihood estimates for the relevant probabilities are given in Table 1.4 4 Finding Similarity-classes First we explain how we determine if a set of concepts has similar p(vjc00;r) for each concept c00 in the set. Then we explain howwe determine top(c;v;r). 4 Since we are assuming the data is not sense disambiguated, freq(c;v;r) cannot be obtained by simply counting senses. The standard approach, which is adopted here, is to estimate freq(c;v;r) by distributing the count for each noun n in syn(c) evenly among all senses of the noun. Yarowsky (1992) and Resnik (1993) explain how the noise introduced by this technique tends to dissipate as counts are passed up the hierarchy. Table 1: Maximum Likelihood Estimates { freq(c;v;r) is the number of (n;v;r) triples in the data in which n is being used to denote c. ^ p(cjr) = freq(c;r) freq(r) = P v02V freq(c;v 0 ;r) Pv02V Pc02C freq(c0 ;v0 ;r) ^ p(vjr) = freq(v;r) freq(r) = Pc02C freq(c 0 ;v;r) Pv02V Pc02C freq(c 0 ;v 0 ;r) ^ p(vjc0;r) = freq(c0 ;v;r) freq(c 0 ;r) = P c00 2c0 freq(c 00 ;v;r) Pv02V P c002c0 freq(c00 ;v0 ;r) The method used for comparing the p(vjc00;r) for c00 in some set c0, is based on the techni</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Philip Resnik. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesc Ribas</author>
</authors>
<title>On learning more appropriate selectional restrictions.</title>
<date>1995</date>
<booktitle>In Proceedings of the Seventh Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="3021" citStr="Ribas (1995)" startWordPosition="516" endWordPosition="517">bability estimate on that instead. The level at which hchickeni is represented is crucial: it should be high enough for adequate counts to have accumulated, but not too high so that the hypernym is no longer representative of hchickeni. An example of a hypernym which would be too high is hentityi, as not all entities are semantically similar with respect to the object position of eat. The problem of choosing an appropriate level in the hierarchy at which to represent a particular noun sense (given a predicate and argument position) has been investigated by Resnik (1993), Li and Abe (1998) and Ribas (1995). The learning mechanism presented here is a novel approach based on \x0cnding semantically similar sets of concepts in a hierarchy. We demonstrate the e\x0bectiveness of our approach using a PP-attachment experiment. 2 The Input Data and Semantic Hierarchy The data used to estimate the probabilities is a multiset of `co-occurrence triples&apos;: a noun 1 We use italics when referring to words, and angled brackets for concepts. This notation does not always pick out a concept uniquely, but the context should make clear the concept being referred to. \x0clemma, verb lemma, and argument position.2 Le</context>
</contexts>
<marker>Ribas, 1995</marker>
<rawString>Francesc Ribas. 1995. On learning more appropriate selectional restrictions. In Proceedings of the Seventh Conference of the European Chapter of the Association for Computational Linguistics, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiri Stetina</author>
<author>Makoto Nagao</author>
</authors>
<title>Corpus based PP attachment ambiguity resolution with a semantic dictionary.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Workshop on Very Large Corpora,</booktitle>
<pages>66--80</pages>
<location>Beijing</location>
<contexts>
<context position="14224" citStr="Stetina and Nagao (1997)" startWordPosition="2473" endWordPosition="2476">to decide whether alternative senses. We rely on the log-likelihood \x1f2 test returning a non-signi\x0ccant result in these cases. from minister attaches to await or approval: await approval from minister We chose the PP-attachment problem because PP-attachmentis a pervasive form of ambiguity, and there exist standard training and test data, which makes for easy comparisons with other approaches. This problem has been tackled by a number of researchers. Brill and Resnik (1994), Ratnaparkhi et al. (1994), Collins (1995), Zavrel and Daelemans (1997) all report results between 81% and 85%, with Stetina and Nagao (1997) reporting a result of 88%, which matches the human performance on this task reported by Ratnaparkhi et al. (1994). Although the PP-attachment problem has characteristics that make it suitable for evaluation, it presents a much bigger sparse data problem than would be expected in other problems such as relative clause attachment. The reason for this is that we need to consider how a concept is associated with combinations of predicates and prepositions. The approach described here uses probabilities of the form p(c;prjv) and p(c;prjn1), where c 2 cn(n2). This means that for many predicate/prep</context>
</contexts>
<marker>Stetina, Nagao, 1997</marker>
<rawString>Jiri Stetina and Makoto Nagao. 1997. Corpus based PP attachment ambiguity resolution with a semantic dictionary. In Proceedings of the Fifth Workshop on Very Large Corpora, pages 66{80, Beijing and Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>454--460</pages>
<contexts>
<context position="9525" citStr="Yarowsky (1992)" startWordPosition="1661" endWordPosition="1662">n explains how we determine similarity classes. The maximum likelihood estimates for the relevant probabilities are given in Table 1.4 4 Finding Similarity-classes First we explain how we determine if a set of concepts has similar p(vjc00;r) for each concept c00 in the set. Then we explain howwe determine top(c;v;r). 4 Since we are assuming the data is not sense disambiguated, freq(c;v;r) cannot be obtained by simply counting senses. The standard approach, which is adopted here, is to estimate freq(c;v;r) by distributing the count for each noun n in syn(c) evenly among all senses of the noun. Yarowsky (1992) and Resnik (1993) explain how the noise introduced by this technique tends to dissipate as counts are passed up the hierarchy. Table 1: Maximum Likelihood Estimates { freq(c;v;r) is the number of (n;v;r) triples in the data in which n is being used to denote c. ^ p(cjr) = freq(c;r) freq(r) = P v02V freq(c;v 0 ;r) Pv02V Pc02C freq(c0 ;v0 ;r) ^ p(vjr) = freq(v;r) freq(r) = Pc02C freq(c 0 ;v;r) Pv02V Pc02C freq(c 0 ;v 0 ;r) ^ p(vjc0;r) = freq(c0 ;v;r) freq(c 0 ;r) = P c00 2c0 freq(c 00 ;v;r) Pv02V P c002c0 freq(c00 ;v0 ;r) The method used for comparing the p(vjc00;r) for c00 in some set c0, is b</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>David Yarowsky. 1992. Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora. In Proceedings of COLING-92, pages 454{460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Memory-based learning: Using similarity for smoothing.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL/EACL97,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="14154" citStr="Zavrel and Daelemans (1997)" startWordPosition="2459" endWordPosition="2463">verb v or the noun n1. For example, in the following case the problem is to decide whether alternative senses. We rely on the log-likelihood \x1f2 test returning a non-signi\x0ccant result in these cases. from minister attaches to await or approval: await approval from minister We chose the PP-attachment problem because PP-attachmentis a pervasive form of ambiguity, and there exist standard training and test data, which makes for easy comparisons with other approaches. This problem has been tackled by a number of researchers. Brill and Resnik (1994), Ratnaparkhi et al. (1994), Collins (1995), Zavrel and Daelemans (1997) all report results between 81% and 85%, with Stetina and Nagao (1997) reporting a result of 88%, which matches the human performance on this task reported by Ratnaparkhi et al. (1994). Although the PP-attachment problem has characteristics that make it suitable for evaluation, it presents a much bigger sparse data problem than would be expected in other problems such as relative clause attachment. The reason for this is that we need to consider how a concept is associated with combinations of predicates and prepositions. The approach described here uses probabilities of the form p(c;prjv) and</context>
</contexts>
<marker>Zavrel, Daelemans, 1997</marker>
<rawString>Jakub Zavrel and Walter Daelemans. 1997. Memory-based learning: Using similarity for smoothing. In Proceedings of ACL/EACL97, Madrid, Spain. \x0c&quot;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>