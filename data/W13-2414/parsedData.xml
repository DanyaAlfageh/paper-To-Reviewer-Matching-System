<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.350223">
b&amp;apos;Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 9499,
Sofia, Bulgaria, 8-9 August 2013. c
</note>
<title confidence="0.56893">
2010 Association for Computational Linguistics
Recognition of Named Entities Boundaries in Polish Texts
</title>
<author confidence="0.646031">
Micha Marcinczuk and Jan Kocon
</author>
<affiliation confidence="0.672579">
Institute of Informatics, Wrocaw University of Technology
</affiliation>
<address confidence="0.378735">
Wybrzeze Wyspianskiego 27
Wrocaw, Poland
</address>
<email confidence="0.971084">
{michal.marcinczuk,jan.kocon}@pwr.wroc.pl
</email>
<sectionHeader confidence="0.987413" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996567923076923">
In the paper we discuss the problem of low
recall for the named entity (NE) recogni-
tion task for Polish. We discuss to what
extent the recall of NE recognition can be
improved by reducing the space of NE cat-
egories. We also present several exten-
sions to the binary model which give an
improvement of the recall. The extensions
include: new features, application of ex-
ternal knowledge and post-processing. For
the partial evaluation the final model ob-
tained 90.02% recall with 91.30% preci-
sion on the corpus of economic news.
</bodyText>
<sectionHeader confidence="0.997859" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9959238">
Named entity recognition (NER) aims at identi-
fying text fragments which refer to some objects
and assigning a category of that object from a pre-
defined set (for example: person, location, orga-
nization, artifact, other). According to the ACE
</bodyText>
<subsectionHeader confidence="0.583454">
(Automatic Content Extraction) English Annota-
</subsectionHeader>
<bodyText confidence="0.995816588235294">
tion Guidelines for Entities (LDC, 2008) there are
several types of named entities, including: proper
names, definite descriptions and noun phrases.
In this paper we focus on recognition of proper
names (PNs) in Polish texts.
For Polish there are only a few accessible mod-
els for PN recognition. Marcinczuk and Jan-
icki (2012) presented a hybrid model (a statisti-
cal model combined with some heuristics) which
obtained 70.53% recall with 91.44% precision for
a limited set of PN categories (first names, last
names, names of countries, cities and roads) tested
on the CEN corpus1 (Marcinczuk et al., 2013).
A model for an extended set of PN categories
(56 categories) presented by Marcinczuk et al.
(2013) obtained much lower recall of 54% with
93% precision tested on the same corpus. Savary
</bodyText>
<page confidence="0.872586">
1
</page>
<bodyText confidence="0.996407025">
Home page: http://nlp.pwr.wroc.pl/cen.
and Waszczuk (2012) presented a statistical model
which obtained 76% recall with 83% precision for
names of people, places, organizations, time ex-
pressions and name derivations tested on the Na-
tional Corpus of Polish2 (Przepiorkowski et al.,
2012).
There are also several other works on PN recog-
nition for Polish where a rule-based approach was
used. Piskorski et al. (2004) constructed a set of
rules and tested them on 100 news from the Rzecz-
pospolita newspaper. The rules obtained 90.6%
precision and 85.3% recall for person names and
87.9% precision and 56.6% recall for company
names. Urbanska and Mykowiecka (2005) also
constructed a set of rules for recognition of person
and organization names. The rules were tested on
100 short texts from the Internet. The rules ob-
tained 98% precision and 89% recall for person
names and 85% precision and 73% recall for orga-
nization names. Another rule-based approach for
an extended set of proper names was presented by
Abramowicz et al. (2006). The rules were tested
on 156 news from the Rzeczpospolita newspaper,
the Tygodnik Powszechny newspaper and the news
web portals. The rules obtained 91% precision and
93% recall for country names, 55% precision and
73% recall for city names, 87% precision and 70%
recall for road names and 82% precision and 66%
recall for person names.
The accessible models for PN recognition for
Polish obtain relatively good performance in terms
of precision. However, in some NLP tasks like
recognition of semantic relations between PNs
(Marcinczuk and Ptak, 2012), coreference reso-
lution (Kopec and Ogrodniczuk, 2012; Broda et
al., 2012a), machine translation (Gralinski et al.,
2009a) or sensitive data anonymization (Gralinski
et al., 2009b) the recall is much more impor-
tant than the fine-grained categorization of PNs.
</bodyText>
<page confidence="0.867551">
2
</page>
<footnote confidence="0.622784">
Home page: http://nkjp.pl
</footnote>
<page confidence="0.99956">
94
</page>
<bodyText confidence="0.999717">
\x0cUnfortunately, the only model recognising wide
range of PN categories obtains only 54% recall.
Therefore, our goal is to evaluate to what extent
the recall for this model can be improved.
</bodyText>
<sectionHeader confidence="0.970047" genericHeader="introduction">
2 Evaluation methodology
</sectionHeader>
<bodyText confidence="0.972521789473684">
In the evaluation we used two corpora anno-
tated with 56 categories of proper names: KPWr3
(Broda et al., 2012b) and CEN (already men-
tioned in Section 1). The KPWr corpus consists of
747 documents containing near 200K tokens and
16.5K NEs. The CEN corpus consists of 797 doc-
uments containing 148K tokens and 13.6K NEs.
Both corpora were tagged using the morphologi-
cal tagger WCRFT (Radziszewski, 2013).
We used a 10-fold cross validation on the KPWr
corpus to select the optimal model. The CEN cor-
pus was used for a cross-corpus evaluation of the
selected model. In this case the model was trained
on the KPWr corpus and evaluated on the CEN
corpus. We presented results for strict and partial
matching evaluation (Chinchor, 1992). The ex-
periments were conducted using an open-source
framework for named entity recognition called
Liner24 (Marcinczuk et al., 2013).
</bodyText>
<sectionHeader confidence="0.974799" genericHeader="method">
3 Reduction of NE categories
</sectionHeader>
<bodyText confidence="0.996201136363636">
In this section we investigate to what extent the re-
call of NE recognition can be improved by reduc-
ing the number of NE categories. As a reference
model we used the statistical model presented by
Marcinczuk and Janicki (2012). The model uses
the Conditional Random Fields method and uti-
lize four types of features, i.e. orthographic (18
features), morphological (6 features), wordnet (4
features) and lexicon (10 features) 38 features
in total. The model uses only local features from
a window of two preceding and two following to-
kens. The detailed description of the features is
presented in Marcinczuk et al. (2013). We did
not used any post-processing methods described
by Marcinczuk and Janicki (2012) (unambiguous
gazetteer chunker, heuristic chunker) because they
were tuned for the specific set of NE categories.
We have evaluated two schemas with a limited
number of the NE categories. In the first more
common (Finkel et al., 2005) schema, all PNs
are divided into four MUC categories, i.e. per-
son, organization, location and other. In the other
</bodyText>
<page confidence="0.850927">
3
</page>
<footnote confidence="0.469487">
Home page: http://nlp.pwr.wroc.pl/kpwr.
</footnote>
<page confidence="0.810834">
4
</page>
<equation confidence="0.303664">
http://nlp.pwr.wroc.pl/liner2
</equation>
<bodyText confidence="0.993372714285714">
schema, assuming a separate phases for PN recog-
nition and classification (Al-Rfou and Skiena,
2012), we mapped all the PN categories to a single
category, namely NAM.
For the MUC schema we have tested two ap-
proaches. In the first approach we trained a sin-
gle classifier for all the NE categories and in the
second approach we trained four classifiers one
for each category. This way we have evaluated
three models: Multi-MUC a cascade of four
classifiers, one classifier for every NE category;
One-MUC a single classifier for all MUC cat-
egories; One-NAM a single classifier for NAM
category.
</bodyText>
<table confidence="0.992796">
Model P R F
Multi-MUC 76.09% 57.41% 65.44%
One-MUC 70.66% 65.39% 67.92%
One-NAM 80.46% 78.59% 79.52%
</table>
<tableCaption confidence="0.999489">
Table 1: Strict evaluation of the three NE models
</tableCaption>
<bodyText confidence="0.998024111111111">
For each model we performed the 10-fold cross-
validation on the KPWr corpus and the results are
presented in Table 1. As we expected the high-
est performance was obtained for the One-NAM
model where the problem of PN classification was
ignored. The model obtained recall of 78% with
80% precision. The results also show that the lo-
cal features used in the model are insufficient to
predict the PN category.
</bodyText>
<sectionHeader confidence="0.947218" genericHeader="method">
4 Improving the binary model
</sectionHeader>
<bodyText confidence="0.9993046">
In this section we present and evaluate several ex-
tensions which were introduced to the One-NAM
model in order to increase its recall. The exten-
sions include: new features, application of exter-
nal resources and post processing.
</bodyText>
<subsectionHeader confidence="0.996442">
4.1 Extensions
</subsectionHeader>
<subsubsectionHeader confidence="0.507707">
4.1.1 Extended gazetteer features
</subsubsectionHeader>
<bodyText confidence="0.97926675">
The reference model (Marcinczuk and Janicki,
2012) uses only five gazetteers of PNs (first names,
last names, names of countries, cities and roads).
To include the other categories of PNs we used two
existing resources: a gazetteer of proper names
called NELexicon5 containing ca. 1.37 million
of forms and a gazetteer of PNs extracted from
the National Corpus of Polish6 containing 153,477
</bodyText>
<figure confidence="0.32887125">
5
http://nlp.pwr.wroc.pl/nelexicon.
6
http://clip.ipipan.waw.pl/Gazetteer
</figure>
<page confidence="0.982096">
95
</page>
<bodyText confidence="0.99719325">
\x0cforms. The categories of PNs were mapped into
four MUC categories: person, location, organi-
zation and other. The numbers of PNs for each
category are presented in Table 2.
</bodyText>
<table confidence="0.990371333333333">
Category Symbol Form count
person per 455,376
location loc 156,886
organization org 832,339
other oth 13,612
TOTAL 1,441,634
</table>
<tableCaption confidence="0.998498">
Table 2: The statistics of the gazetteers.
</tableCaption>
<bodyText confidence="0.9988825">
We added four features, one for every category.
The features were defined as following:
</bodyText>
<equation confidence="0.5870345">
gaz(n, c) =
B if n-th token starts a sequence of words
found in gazetteer c
I if n-th token is part of a sequence of
</equation>
<bodyText confidence="0.842637">
words found in gazetteer c excluding
the first token
</bodyText>
<sectionHeader confidence="0.756015" genericHeader="evaluation">
0 otherwise
</sectionHeader>
<bodyText confidence="0.99884825">
where c {per, loc, org, oth} and n is the token
index in a sentence. If two or more PNs from the
same gazetteer overlap, then the first and longest
PN is taken into account.
</bodyText>
<subsubsectionHeader confidence="0.998516">
4.1.2 Trigger features
</subsubsectionHeader>
<bodyText confidence="0.9681995">
A trigger is a word which can indicate presence
of a proper name. Triggers can be divided into
two groups: external (appear before or after PNs)
and internal (are part of PNs). We used a lexi-
con of triggers called PNET (Polish Named En-
tity Triggers)7. The lexicon contains 28,000 in-
flected forms divided into 8 semantic categories
(bloc, country, district, geogName, orgName, per-
sName, region and settlement) semi-automatically
extracted from Polish Wikipedia8. We divided the
lexicon into 16 sets two for every semantic cat-
egory (with internal and external triggers). We de-
fined one feature for every lexicon what gives 16
features in total. The feature were defined as fol-
</bodyText>
<figure confidence="0.729959">
lowing:
trigger(n, s) =
1 if n-th token base is found
in set s
0 otherwise
7
http://zil.ipipan.waw.pl/PNET.
8
http://pl.wikipedia.org
</figure>
<subsubsectionHeader confidence="0.976659">
4.1.3 Agreement feature
</subsubsectionHeader>
<bodyText confidence="0.994694714285714">
An agreement of the morphological attributes be-
tween two consecutive words can be an indicator
of phrase continuity. This observation was used by
Radziszewski and Pawlaczek (2012) to recognize
noun phrases. This information can be also help-
ful in PN boundaries recognition. The feature was
defined as following:
</bodyText>
<equation confidence="0.9732426">
agr(n) =
1 if number[n] = number[n 1]
and case[n] = case[n 1]
and gender[n] = gender[n 1]
0 otherwise
</equation>
<bodyText confidence="0.8844885">
The agr(n) feature for a token n has value 1
when the n-th and n 1-th words have the same
</bodyText>
<figureCaption confidence="0.840607333333333">
case, gender and number. In other cases the value
is 0. If one of the attributes is not set, the value is
also 0.
</figureCaption>
<subsubsectionHeader confidence="0.904701">
4.1.4 Unambiguous gazetteer look-up
</subsubsectionHeader>
<bodyText confidence="0.9983177">
There are many proper names which are well
known and can be easily recognized using
gazetteers. However, some of the proper names
present in the gazetteers can be also common
words. In order to avoid this problem we used an
unambiguous gazetteer look-up (Marcinczuk and
Janicki, 2012). We created one gazetteer contain-
ing all categories of PNs (see Section 4.1.1) and
discarded all entries which were found in the SJP
dictionary9 in a lower case form.
</bodyText>
<subsubsectionHeader confidence="0.778015">
4.1.5 Heuristics
</subsubsectionHeader>
<bodyText confidence="0.998418">
We created several simple rules to recognize PNs
on the basis of the orthographic features. The fol-
lowing phrases are recognized as proper names re-
gardless the context:
a camel case word a single word contain-
ing one or more internal upper case letters
and at least one lower case letter, for exam-
ple RoboRally a name of board game,
a sequence of words in the quotation
marks the first word must be capitalised
and shorter than 5 characters to avoid match-
ing ironic or apologetic words and citations,
a sequence of all-uppercase words we
discard words which are roman numbers and
ignore all-uppercase sentences.
</bodyText>
<page confidence="0.832591">
9
</page>
<bodyText confidence="0.206453">
http://www.sjp.pl/slownik/ort.
</bodyText>
<page confidence="0.902089">
96
</page>
<bodyText confidence="0.9645369375">
\x0c4.1.6 Names propagation
The reference model does not contain any
document-based features. This can be a prob-
lem for documents where the proper names oc-
cur several times but only a few of its occur-
rences are recognised by the statistical model. The
other may not be recognized because of the un-
seen or unambiguous contexts. In such cases the
global information about the recognized occur-
rences could be used to recognize the other unrec-
ognized names. However, a simple propagation of
all recognized names might cause loss in the preci-
sion because of the common words which are also
proper names. To handle this problem we defined
a set of patterns and propagate only those proper
names which match one of the following pattern:
</bodyText>
<listItem confidence="0.94407775">
(1) a sequence of two or more capitalised words;
(2) all-uppercase word ended with a number; or
(3) all-uppercase word ended with hyphen and in-
flectional suffix.
</listItem>
<subsectionHeader confidence="0.981902">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.9510885">
Table 3 contains results of the 10-fold cross valida-
tion on the KPWr corpus for the One-NAM model,
One-NAM with every single extension and a com-
plete model with all extensions. The bold values
indicate an improvement comparing to the base
One-NAM model. To check the statistical signif-
icance of precision, recall and F-measure differ-
ence we used Students t-test with a significance
level = 0.01 (Dietterich, 1998). The asterisk
indicates the statistically significant improvement.
</bodyText>
<table confidence="0.998682888888889">
Model P R F
One-NAM 80.46% 78.59% 79.52%
Gazetteers 80.60% 78.71% 79.64%
Triggers 80.60% 78.58% 79.58%
Agreement 80.73% 78.90% 79.80%
Look-up 80.18% 79.56%* 79.87%
Heuristics 79.98% 79.20%* 79.59%
Propagate 80.46% 78.59% 79.52%
Complete 80.33% 80.61%* 80.47%*
</table>
<tableCaption confidence="0.999861">
Table 3: The 10-fold cross validation on the KPWr
</tableCaption>
<bodyText confidence="0.968343541666667">
corpus for One-NAM model with different exten-
sions.
Five out of six extensions improved the perfor-
mance. Only for the name propagation we did
not observe any improvement because the KPWr
corpus contains only short documents (up to 300
words) and it is uncommon that a name will appear
more than one time in the same fragment. How-
ever, tests on random documents from the Internet
showed the usefulness of this extension.
For the unambiguous gazetteer look-up and the
heuristics we obtained a statistically significant
improvement of the recall. In the final model we
included all the presented extensions. The final
model achieved a statistically significant improve-
ment of the recall and the F-measure.
To check the generality of the extensions, we
performed the cross-domain evaluation on the
CEN corpus (see Section 2). The results for the
56nam, the One-NAM and the Improved One-
NAM models are presented in Table 4. For the
strict evaluation, the recall was improved by al-
most 4 percentage points with a small precision
improvement by almost 2 percentage points.
</bodyText>
<table confidence="0.999178888888889">
Evaluation P R F
56nam model (Marcinczuk et al., 2013)
Strict 93% 54% 68%
One-NAM model
Strict 85.98% 81.31% 83.58%
Partial 91.12% 86.65% 88.83%
Improved One-NAM model
Strict 86.61% 85.05% 85.82%
Partial 91.30% 90.02% 90.65%
</table>
<tableCaption confidence="0.823012">
Table 4: The cross-domain evaluation of the basic
and improved One-NAM models on CEN.
</tableCaption>
<sectionHeader confidence="0.991925" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.996126529411765">
In the paper we discussed the problem of low re-
call of models for recognition of a wide range of
PNs for Polish. We tested to what extent the reduc-
tion of the PN categories can improve the recall.
As we expected the model without PN classifica-
tion obtained the best results in terms of precision
and recall.
Then we presented a set of extensions to the
One-NAM model, including new features (mor-
phological agreement, triggers, gazetteers), ap-
plication of external knowledge (a set of heuris-
tics and a gazetteer-based recogniser) and post-
processing (proper names propagation). The final
model obtained 90.02% recall with 91.30% preci-
sion on the CEN corpus for the partial evaluation
what is a good start of further NE categorization
phase.
</bodyText>
<page confidence="0.975424">
97
</page>
<table confidence="0.653509166666667">
\x0cAcknowledgments
This work was financed by Innovative Economy
Programme project POIG.01.01.02-14-013/09.
References
Witold Abramowicz, Agata Filipowska, Jakub Pisko-
rski, Krzysztof W
</table>
<reference confidence="0.998255123809524">
ecel, and Karol Wieloch. 2006.
Linguistic Suite for Polish Cadastral System. In
Proceedings of the LREC06, pages 5358, Genoa,
Italy.
Rami Al-Rfou and Steven Skiena. 2012. SpeedRead:
A fast named entity recognition pipeline. In Pro-
ceedings of COLING 2012, pages 5166, Mumbai,
India, December. The COLING 2012 Organizing
Committee.
Bartosz Broda, Lukasz Burdka, and Marek Maziarz.
2012a. Ikar: An improved kit for anaphora resolu-
tion for polish. In Martin Kay and Christian Boitet,
editors, COLING (Demos), pages 2532. Indian In-
stitute of Technology Bombay.
Bartosz Broda, Micha Marcinczuk, Marek Maziarz,
Adam Radziszewski, and Adam Wardynski. 2012b.
KPWr: Towards a Free Corpus of Polish. In Nico-
letta Calzolari, Khalid Choukri, Thierry Declerck,
Mehmet Ugur Dogan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of LREC12. ELRA.
Nancy Chinchor. 1992. MUC-4 Evaluation Metrics.
In Proceedings of the Fourth Message Understand-
ing Conference, pages 2229.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learn-
ing algorithms. Neural Computation, 10(7):1895
1924.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2005. Incorporating Non-local
Information into Information Extraction Systems by
Gibbs Sampling. In The Association for Com-
puter Linguistics, editor, Proceedings of the 43nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2005), pages 363370.
Filip Gralinski, Krzysztof Jassem, and Micha Mar-
cinczuk. 2009a. An Environment for Named En-
tity Recognition and Translation. In L Marquez and
H Somers, editors, Proceedings of the 13th Annual
Conference of the European Association for Ma-
chine Translation, pages 8895, Barcelona, Spain.
Filip Gralinski, Krzysztof Jassem, Micha Marcinczuk,
and Pawe Wawrzyniak. 2009b. Named Entity
Recognition in Machine Anonymization. In M A
Kopotek, A Przepiorkowski, A T Wierzchon, and
K Trojanowski, editors, Recent Advances in Intel-
ligent Information Systems., pages 247260. Aca-
demic Pub. House Exit.
Mateusz Kopec and Maciej Ogrodniczuk. 2012.
Creating a coreference resolution system for pol-
ish. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Ugur
Dogan, Bente Maegaard, Joseph Mariani, Jan
Odijk, and Stelios Piperidis, editors, Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC12), Istanbul,
Turkey, may. European Language Resources Asso-
ciation (ELRA).
LDC. 2008. ACE (Automatic Content Extraction) En-
glish Annotation Guidelines for Relations (Version
6.2).
Micha Marcinczuk and Maciej Janicki. 2012. Opti-
mizing CRF-Based Model for Proper Name Recog-
nition in Polish Texts. In Alexander F. Gelbukh, ed-
itor, CICLing (1), volume 7181 of Lecture Notes in
Computer Science, pages 258269. Springer.
Micha Marcinczuk, Jan Kocon, and Maciej Janicki.
2013. Liner2 - A Customizable Framework for
Proper Names Recognition for Polish. In Robert
Bembenik, ukasz Skonieczny, Henryk Rybinski,
Marzena Kryszkiewicz, and Marek Niezgodka, ed-
itors, Intelligent Tools for Building a Scientific In-
formation Platform, volume 467 of Studies in Com-
putational Intelligence, pages 231253. Springer.
Micha Marcinczuk and Marcin Ptak. 2012. Prelimi-
nary study on automatic induction of rules for recog-
nition of semantic relations between proper names
in polish texts. In Petr Sojka, Ales Horak, Ivan
Kopecek, and Karel Pala, editors, Text, Speech and
Dialogue 15th International Conference, TSD
2012, Brno, Czech Republic, September 3-7, 2012.
Proceedings, volume 7499 of Lecture Notes in Arti-
ficial Intelligence (LNAI). Springer-Verlag, Septem-
ber.
Jakub Piskorski, Peter Homola, Magorzata Marciniak,
Agnieszka Mykowiecka, Adam Przepiorkowski,
and Marcin Wolinski. 2004. Information Extrac-
tion for Polish Using the SProUT Platform. In
Mieczyslaw A. Kopotek, Slawomir T. Wierzchon,
and Krzysztof Trojanowski, editors, Intelligent In-
formation Processing and Web Mining, Proceed-
ings of the International IIS: IIPWM04 Conference,
Advances in Soft Computing, Zakopane. Springer-
Verlag.
Adam Przepiorkowski, Mirosaw Banko, Rafa L.
Gorski, and Barbara Lewandowska-Tomaszczyk,
editors. 2012. Narodowy Korpus Jezyka Polskiego
[Eng.: National Corpus of Polish]. Wydawnictwo
Naukowe PWN, Warsaw.
Adam Radziszewski and Adam Pawlaczek. 2012.
Large-Scale Experiments with NP Chunking of Pol-
ish. In Petr Sojka, Ales Horak, Ivan Kopecek,
and Karel Pala, editors, TSD, volume 7499 of Lec-
ture Notes in Computer Science, pages 143149.
Springer Berlin Heidelberg.
</reference>
<page confidence="0.991278">
98
</page>
<reference confidence="0.99975405">
\x0cAdam Radziszewski. 2013. A Tiered CRF Tagger for
Polish. In Robert Bembenik, ukasz Skonieczny,
Henryk Rybinski, Marzena Kryszkiewicz, and
Marek Niezgodka, editors, Intelligent Tools for
Building a Scientific Information Platform, volume
467 of Studies in Computational Intelligence, pages
215230. Springer Berlin Heidelberg.
Agata Savary and Jakub Waszczuk. 2012. Narzedzia
do anotacji jednostek nazewniczych. In Adam
Przepiorkowski, Mirosaw Banko, Rafa L. Gorski,
and Barbara Lewandowska-Tomaszczyk, editors,
Narodowy Korpus Jezyka Polskiego. Wydawnictwo
Naukowe PWN. Creative Commons Uznanie Au-
torstwa 3.0 Polska.
Dominika Urbanska and Agnieszka Mykowiecka.
2005. Multi-words Named Entity Recognition in
Polish texts. In Radovan Grabik, editor, SLOVKO
2005 Third International Seminar Computer
Treatment of Slavic and East European Languages,
Bratislava, Slovakia, pages 208215. VEDA.
</reference>
<page confidence="0.963504">
99
</page>
<figure confidence="0.264418">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.401910">
<note confidence="0.717253">b&amp;apos;Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 9499, Sofia, Bulgaria, 8-9 August 2013. c 2010 Association for Computational Linguistics</note>
<title confidence="0.928663">Recognition of Named Entities Boundaries in Polish Texts</title>
<author confidence="0.999351">Micha Marcinczuk</author>
<author confidence="0.999351">Jan Kocon</author>
<affiliation confidence="0.999986">Institute of Informatics, Wrocaw University of Technology</affiliation>
<address confidence="0.92772">Wybrzeze Wyspianskiego 27 Wrocaw, Poland</address>
<email confidence="0.993378">michal.marcinczuk@pwr.wroc.pl</email>
<email confidence="0.993378">jan.kocon@pwr.wroc.pl</email>
<abstract confidence="0.995885428571428">In the paper we discuss the problem of low recall for the named entity (NE) recognition task for Polish. We discuss to what extent the recall of NE recognition can be improved by reducing the space of NE categories. We also present several extensions to the binary model which give an improvement of the recall. The extensions include: new features, application of external knowledge and post-processing. For the partial evaluation the final model obtained 90.02% recall with 91.30% precision on the corpus of economic news.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ecel</author>
<author>Karol Wieloch</author>
</authors>
<title>Linguistic Suite for Polish Cadastral System.</title>
<date>2006</date>
<booktitle>In Proceedings of the LREC06,</booktitle>
<pages>5358</pages>
<location>Genoa, Italy.</location>
<marker>ecel, Wieloch, 2006</marker>
<rawString>ecel, and Karol Wieloch. 2006. Linguistic Suite for Polish Cadastral System. In Proceedings of the LREC06, pages 5358, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rami Al-Rfou</author>
<author>Steven Skiena</author>
</authors>
<title>SpeedRead: A fast named entity recognition pipeline.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>5166</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="6267" citStr="Al-Rfou and Skiena, 2012" startWordPosition="990" endWordPosition="993"> et al. (2013). We did not used any post-processing methods described by Marcinczuk and Janicki (2012) (unambiguous gazetteer chunker, heuristic chunker) because they were tuned for the specific set of NE categories. We have evaluated two schemas with a limited number of the NE categories. In the first more common (Finkel et al., 2005) schema, all PNs are divided into four MUC categories, i.e. person, organization, location and other. In the other 3 Home page: http://nlp.pwr.wroc.pl/kpwr. 4 http://nlp.pwr.wroc.pl/liner2 schema, assuming a separate phases for PN recognition and classification (Al-Rfou and Skiena, 2012), we mapped all the PN categories to a single category, namely NAM. For the MUC schema we have tested two approaches. In the first approach we trained a single classifier for all the NE categories and in the second approach we trained four classifiers one for each category. This way we have evaluated three models: Multi-MUC a cascade of four classifiers, one classifier for every NE category; One-MUC a single classifier for all MUC categories; One-NAM a single classifier for NAM category. Model P R F Multi-MUC 76.09% 57.41% 65.44% One-MUC 70.66% 65.39% 67.92% One-NAM 80.46% 78.59% 79.52% Table </context>
</contexts>
<marker>Al-Rfou, Skiena, 2012</marker>
<rawString>Rami Al-Rfou and Steven Skiena. 2012. SpeedRead: A fast named entity recognition pipeline. In Proceedings of COLING 2012, pages 5166, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bartosz Broda</author>
<author>Lukasz Burdka</author>
<author>Marek Maziarz</author>
</authors>
<title>Ikar: An improved kit for anaphora resolution for polish.</title>
<date>2012</date>
<pages>2532</pages>
<editor>In Martin Kay and Christian Boitet, editors, COLING (Demos),</editor>
<institution>Indian Institute of Technology Bombay.</institution>
<contexts>
<context position="3710" citStr="Broda et al., 2012" startWordPosition="581" endWordPosition="584">les were tested on 156 news from the Rzeczpospolita newspaper, the Tygodnik Powszechny newspaper and the news web portals. The rules obtained 91% precision and 93% recall for country names, 55% precision and 73% recall for city names, 87% precision and 70% recall for road names and 82% precision and 66% recall for person names. The accessible models for PN recognition for Polish obtain relatively good performance in terms of precision. However, in some NLP tasks like recognition of semantic relations between PNs (Marcinczuk and Ptak, 2012), coreference resolution (Kopec and Ogrodniczuk, 2012; Broda et al., 2012a), machine translation (Gralinski et al., 2009a) or sensitive data anonymization (Gralinski et al., 2009b) the recall is much more important than the fine-grained categorization of PNs. 2 Home page: http://nkjp.pl 94 \x0cUnfortunately, the only model recognising wide range of PN categories obtains only 54% recall. Therefore, our goal is to evaluate to what extent the recall for this model can be improved. 2 Evaluation methodology In the evaluation we used two corpora annotated with 56 categories of proper names: KPWr3 (Broda et al., 2012b) and CEN (already mentioned in Section 1). The KPWr co</context>
</contexts>
<marker>Broda, Burdka, Maziarz, 2012</marker>
<rawString>Bartosz Broda, Lukasz Burdka, and Marek Maziarz. 2012a. Ikar: An improved kit for anaphora resolution for polish. In Martin Kay and Christian Boitet, editors, COLING (Demos), pages 2532. Indian Institute of Technology Bombay.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bartosz Broda</author>
<author>Micha Marcinczuk</author>
<author>Marek Maziarz</author>
<author>Adam Radziszewski</author>
<author>Adam Wardynski</author>
</authors>
<title>KPWr: Towards a Free Corpus of Polish.</title>
<date>2012</date>
<booktitle>Proceedings of LREC12. ELRA.</booktitle>
<editor>In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors,</editor>
<contexts>
<context position="3710" citStr="Broda et al., 2012" startWordPosition="581" endWordPosition="584">les were tested on 156 news from the Rzeczpospolita newspaper, the Tygodnik Powszechny newspaper and the news web portals. The rules obtained 91% precision and 93% recall for country names, 55% precision and 73% recall for city names, 87% precision and 70% recall for road names and 82% precision and 66% recall for person names. The accessible models for PN recognition for Polish obtain relatively good performance in terms of precision. However, in some NLP tasks like recognition of semantic relations between PNs (Marcinczuk and Ptak, 2012), coreference resolution (Kopec and Ogrodniczuk, 2012; Broda et al., 2012a), machine translation (Gralinski et al., 2009a) or sensitive data anonymization (Gralinski et al., 2009b) the recall is much more important than the fine-grained categorization of PNs. 2 Home page: http://nkjp.pl 94 \x0cUnfortunately, the only model recognising wide range of PN categories obtains only 54% recall. Therefore, our goal is to evaluate to what extent the recall for this model can be improved. 2 Evaluation methodology In the evaluation we used two corpora annotated with 56 categories of proper names: KPWr3 (Broda et al., 2012b) and CEN (already mentioned in Section 1). The KPWr co</context>
</contexts>
<marker>Broda, Marcinczuk, Maziarz, Radziszewski, Wardynski, 2012</marker>
<rawString>Bartosz Broda, Micha Marcinczuk, Marek Maziarz, Adam Radziszewski, and Adam Wardynski. 2012b. KPWr: Towards a Free Corpus of Polish. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of LREC12. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<date>1992</date>
<note>MUC-4 Evaluation Metrics.</note>
<contexts>
<context position="4874" citStr="Chinchor, 1992" startWordPosition="774" endWordPosition="775">d CEN (already mentioned in Section 1). The KPWr corpus consists of 747 documents containing near 200K tokens and 16.5K NEs. The CEN corpus consists of 797 documents containing 148K tokens and 13.6K NEs. Both corpora were tagged using the morphological tagger WCRFT (Radziszewski, 2013). We used a 10-fold cross validation on the KPWr corpus to select the optimal model. The CEN corpus was used for a cross-corpus evaluation of the selected model. In this case the model was trained on the KPWr corpus and evaluated on the CEN corpus. We presented results for strict and partial matching evaluation (Chinchor, 1992). The experiments were conducted using an open-source framework for named entity recognition called Liner24 (Marcinczuk et al., 2013). 3 Reduction of NE categories In this section we investigate to what extent the recall of NE recognition can be improved by reducing the number of NE categories. As a reference model we used the statistical model presented by Marcinczuk and Janicki (2012). The model uses the Conditional Random Fields method and utilize four types of features, i.e. orthographic (18 features), morphological (6 features), wordnet (4 features) and lexicon (10 features) 38 features i</context>
</contexts>
<marker>Chinchor, 1992</marker>
<rawString>Nancy Chinchor. 1992. MUC-4 Evaluation Metrics.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the Fourth Message Understanding Conference,</booktitle>
<pages>2229</pages>
<marker></marker>
<rawString>In Proceedings of the Fourth Message Understanding Conference, pages 2229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Approximate statistical tests for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<volume>10</volume>
<issue>7</issue>
<contexts>
<context position="12824" citStr="Dietterich, 1998" startWordPosition="2089" endWordPosition="2090"> of the following pattern: (1) a sequence of two or more capitalised words; (2) all-uppercase word ended with a number; or (3) all-uppercase word ended with hyphen and inflectional suffix. 4.2 Evaluation Table 3 contains results of the 10-fold cross validation on the KPWr corpus for the One-NAM model, One-NAM with every single extension and a complete model with all extensions. The bold values indicate an improvement comparing to the base One-NAM model. To check the statistical significance of precision, recall and F-measure difference we used Students t-test with a significance level = 0.01 (Dietterich, 1998). The asterisk indicates the statistically significant improvement. Model P R F One-NAM 80.46% 78.59% 79.52% Gazetteers 80.60% 78.71% 79.64% Triggers 80.60% 78.58% 79.58% Agreement 80.73% 78.90% 79.80% Look-up 80.18% 79.56%* 79.87% Heuristics 79.98% 79.20%* 79.59% Propagate 80.46% 78.59% 79.52% Complete 80.33% 80.61%* 80.47%* Table 3: The 10-fold cross validation on the KPWr corpus for One-NAM model with different extensions. Five out of six extensions improved the performance. Only for the name propagation we did not observe any improvement because the KPWr corpus contains only short document</context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10(7):1895 1924.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In The Association for Computer Linguistics, editor, Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>363370</pages>
<contexts>
<context position="5979" citStr="Finkel et al., 2005" startWordPosition="950" endWordPosition="953">rthographic (18 features), morphological (6 features), wordnet (4 features) and lexicon (10 features) 38 features in total. The model uses only local features from a window of two preceding and two following tokens. The detailed description of the features is presented in Marcinczuk et al. (2013). We did not used any post-processing methods described by Marcinczuk and Janicki (2012) (unambiguous gazetteer chunker, heuristic chunker) because they were tuned for the specific set of NE categories. We have evaluated two schemas with a limited number of the NE categories. In the first more common (Finkel et al., 2005) schema, all PNs are divided into four MUC categories, i.e. person, organization, location and other. In the other 3 Home page: http://nlp.pwr.wroc.pl/kpwr. 4 http://nlp.pwr.wroc.pl/liner2 schema, assuming a separate phases for PN recognition and classification (Al-Rfou and Skiena, 2012), we mapped all the PN categories to a single category, namely NAM. For the MUC schema we have tested two approaches. In the first approach we trained a single classifier for all the NE categories and in the second approach we trained four classifiers one for each category. This way we have evaluated three mode</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In The Association for Computer Linguistics, editor, Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 363370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Gralinski</author>
<author>Krzysztof Jassem</author>
<author>Micha Marcinczuk</author>
</authors>
<title>An Environment for Named Entity Recognition and Translation.</title>
<date>2009</date>
<booktitle>Proceedings of the 13th Annual Conference of the European Association for Machine Translation,</booktitle>
<pages>8895</pages>
<editor>In L Marquez and H Somers, editors,</editor>
<location>Barcelona, Spain.</location>
<contexts>
<context position="3757" citStr="Gralinski et al., 2009" startWordPosition="587" endWordPosition="590">ospolita newspaper, the Tygodnik Powszechny newspaper and the news web portals. The rules obtained 91% precision and 93% recall for country names, 55% precision and 73% recall for city names, 87% precision and 70% recall for road names and 82% precision and 66% recall for person names. The accessible models for PN recognition for Polish obtain relatively good performance in terms of precision. However, in some NLP tasks like recognition of semantic relations between PNs (Marcinczuk and Ptak, 2012), coreference resolution (Kopec and Ogrodniczuk, 2012; Broda et al., 2012a), machine translation (Gralinski et al., 2009a) or sensitive data anonymization (Gralinski et al., 2009b) the recall is much more important than the fine-grained categorization of PNs. 2 Home page: http://nkjp.pl 94 \x0cUnfortunately, the only model recognising wide range of PN categories obtains only 54% recall. Therefore, our goal is to evaluate to what extent the recall for this model can be improved. 2 Evaluation methodology In the evaluation we used two corpora annotated with 56 categories of proper names: KPWr3 (Broda et al., 2012b) and CEN (already mentioned in Section 1). The KPWr corpus consists of 747 documents containing near </context>
</contexts>
<marker>Gralinski, Jassem, Marcinczuk, 2009</marker>
<rawString>Filip Gralinski, Krzysztof Jassem, and Micha Marcinczuk. 2009a. An Environment for Named Entity Recognition and Translation. In L Marquez and H Somers, editors, Proceedings of the 13th Annual Conference of the European Association for Machine Translation, pages 8895, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Gralinski</author>
<author>Krzysztof Jassem</author>
<author>Micha Marcinczuk</author>
<author>Pawe Wawrzyniak</author>
</authors>
<title>Named Entity Recognition in Machine Anonymization.</title>
<date>2009</date>
<journal>In M A Kopotek, A Przepiorkowski, A T Wierzchon, and</journal>
<booktitle>Recent Advances in Intelligent Information Systems.,</booktitle>
<pages>247260</pages>
<editor>K Trojanowski, editors,</editor>
<publisher>Academic Pub. House Exit.</publisher>
<contexts>
<context position="3757" citStr="Gralinski et al., 2009" startWordPosition="587" endWordPosition="590">ospolita newspaper, the Tygodnik Powszechny newspaper and the news web portals. The rules obtained 91% precision and 93% recall for country names, 55% precision and 73% recall for city names, 87% precision and 70% recall for road names and 82% precision and 66% recall for person names. The accessible models for PN recognition for Polish obtain relatively good performance in terms of precision. However, in some NLP tasks like recognition of semantic relations between PNs (Marcinczuk and Ptak, 2012), coreference resolution (Kopec and Ogrodniczuk, 2012; Broda et al., 2012a), machine translation (Gralinski et al., 2009a) or sensitive data anonymization (Gralinski et al., 2009b) the recall is much more important than the fine-grained categorization of PNs. 2 Home page: http://nkjp.pl 94 \x0cUnfortunately, the only model recognising wide range of PN categories obtains only 54% recall. Therefore, our goal is to evaluate to what extent the recall for this model can be improved. 2 Evaluation methodology In the evaluation we used two corpora annotated with 56 categories of proper names: KPWr3 (Broda et al., 2012b) and CEN (already mentioned in Section 1). The KPWr corpus consists of 747 documents containing near </context>
</contexts>
<marker>Gralinski, Jassem, Marcinczuk, Wawrzyniak, 2009</marker>
<rawString>Filip Gralinski, Krzysztof Jassem, Micha Marcinczuk, and Pawe Wawrzyniak. 2009b. Named Entity Recognition in Machine Anonymization. In M A Kopotek, A Przepiorkowski, A T Wierzchon, and K Trojanowski, editors, Recent Advances in Intelligent Information Systems., pages 247260. Academic Pub. House Exit.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mateusz Kopec</author>
<author>Maciej Ogrodniczuk</author>
</authors>
<title>Creating a coreference resolution system for polish.</title>
<date>2012</date>
<booktitle>Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC12),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors,</editor>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="3690" citStr="Kopec and Ogrodniczuk, 2012" startWordPosition="577" endWordPosition="580">amowicz et al. (2006). The rules were tested on 156 news from the Rzeczpospolita newspaper, the Tygodnik Powszechny newspaper and the news web portals. The rules obtained 91% precision and 93% recall for country names, 55% precision and 73% recall for city names, 87% precision and 70% recall for road names and 82% precision and 66% recall for person names. The accessible models for PN recognition for Polish obtain relatively good performance in terms of precision. However, in some NLP tasks like recognition of semantic relations between PNs (Marcinczuk and Ptak, 2012), coreference resolution (Kopec and Ogrodniczuk, 2012; Broda et al., 2012a), machine translation (Gralinski et al., 2009a) or sensitive data anonymization (Gralinski et al., 2009b) the recall is much more important than the fine-grained categorization of PNs. 2 Home page: http://nkjp.pl 94 \x0cUnfortunately, the only model recognising wide range of PN categories obtains only 54% recall. Therefore, our goal is to evaluate to what extent the recall for this model can be improved. 2 Evaluation methodology In the evaluation we used two corpora annotated with 56 categories of proper names: KPWr3 (Broda et al., 2012b) and CEN (already mentioned in Sec</context>
</contexts>
<marker>Kopec, Ogrodniczuk, 2012</marker>
<rawString>Mateusz Kopec and Maciej Ogrodniczuk. 2012. Creating a coreference resolution system for polish. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC12), Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>LDC</author>
</authors>
<date>2008</date>
<journal>ACE (Automatic Content Extraction) English Annotation Guidelines for Relations (Version</journal>
<volume>6</volume>
<contexts>
<context position="1301" citStr="LDC, 2008" startWordPosition="193" endWordPosition="194">e binary model which give an improvement of the recall. The extensions include: new features, application of external knowledge and post-processing. For the partial evaluation the final model obtained 90.02% recall with 91.30% precision on the corpus of economic news. 1 Introduction Named entity recognition (NER) aims at identifying text fragments which refer to some objects and assigning a category of that object from a predefined set (for example: person, location, organization, artifact, other). According to the ACE (Automatic Content Extraction) English Annotation Guidelines for Entities (LDC, 2008) there are several types of named entities, including: proper names, definite descriptions and noun phrases. In this paper we focus on recognition of proper names (PNs) in Polish texts. For Polish there are only a few accessible models for PN recognition. Marcinczuk and Janicki (2012) presented a hybrid model (a statistical model combined with some heuristics) which obtained 70.53% recall with 91.44% precision for a limited set of PN categories (first names, last names, names of countries, cities and roads) tested on the CEN corpus1 (Marcinczuk et al., 2013). A model for an extended set of PN </context>
</contexts>
<marker>LDC, 2008</marker>
<rawString>LDC. 2008. ACE (Automatic Content Extraction) English Annotation Guidelines for Relations (Version 6.2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Marcinczuk</author>
<author>Maciej Janicki</author>
</authors>
<title>Optimizing CRF-Based Model for Proper Name Recognition in Polish Texts.</title>
<date>2012</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>1</volume>
<pages>258269</pages>
<editor>In Alexander F. Gelbukh, editor,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1586" citStr="Marcinczuk and Janicki (2012)" startWordPosition="237" endWordPosition="241">s. 1 Introduction Named entity recognition (NER) aims at identifying text fragments which refer to some objects and assigning a category of that object from a predefined set (for example: person, location, organization, artifact, other). According to the ACE (Automatic Content Extraction) English Annotation Guidelines for Entities (LDC, 2008) there are several types of named entities, including: proper names, definite descriptions and noun phrases. In this paper we focus on recognition of proper names (PNs) in Polish texts. For Polish there are only a few accessible models for PN recognition. Marcinczuk and Janicki (2012) presented a hybrid model (a statistical model combined with some heuristics) which obtained 70.53% recall with 91.44% precision for a limited set of PN categories (first names, last names, names of countries, cities and roads) tested on the CEN corpus1 (Marcinczuk et al., 2013). A model for an extended set of PN categories (56 categories) presented by Marcinczuk et al. (2013) obtained much lower recall of 54% with 93% precision tested on the same corpus. Savary 1 Home page: http://nlp.pwr.wroc.pl/cen. and Waszczuk (2012) presented a statistical model which obtained 76% recall with 83% precisi</context>
<context position="5263" citStr="Marcinczuk and Janicki (2012)" startWordPosition="836" endWordPosition="839">CEN corpus was used for a cross-corpus evaluation of the selected model. In this case the model was trained on the KPWr corpus and evaluated on the CEN corpus. We presented results for strict and partial matching evaluation (Chinchor, 1992). The experiments were conducted using an open-source framework for named entity recognition called Liner24 (Marcinczuk et al., 2013). 3 Reduction of NE categories In this section we investigate to what extent the recall of NE recognition can be improved by reducing the number of NE categories. As a reference model we used the statistical model presented by Marcinczuk and Janicki (2012). The model uses the Conditional Random Fields method and utilize four types of features, i.e. orthographic (18 features), morphological (6 features), wordnet (4 features) and lexicon (10 features) 38 features in total. The model uses only local features from a window of two preceding and two following tokens. The detailed description of the features is presented in Marcinczuk et al. (2013). We did not used any post-processing methods described by Marcinczuk and Janicki (2012) (unambiguous gazetteer chunker, heuristic chunker) because they were tuned for the specific set of NE categories. We h</context>
<context position="7672" citStr="Marcinczuk and Janicki, 2012" startWordPosition="1226" endWordPosition="1229">the highest performance was obtained for the One-NAM model where the problem of PN classification was ignored. The model obtained recall of 78% with 80% precision. The results also show that the local features used in the model are insufficient to predict the PN category. 4 Improving the binary model In this section we present and evaluate several extensions which were introduced to the One-NAM model in order to increase its recall. The extensions include: new features, application of external resources and post processing. 4.1 Extensions 4.1.1 Extended gazetteer features The reference model (Marcinczuk and Janicki, 2012) uses only five gazetteers of PNs (first names, last names, names of countries, cities and roads). To include the other categories of PNs we used two existing resources: a gazetteer of proper names called NELexicon5 containing ca. 1.37 million of forms and a gazetteer of PNs extracted from the National Corpus of Polish6 containing 153,477 5 http://nlp.pwr.wroc.pl/nelexicon. 6 http://clip.ipipan.waw.pl/Gazetteer 95 \x0cforms. The categories of PNs were mapped into four MUC categories: person, location, organization and other. The numbers of PNs for each category are presented in Table 2. Catego</context>
<context position="10671" citStr="Marcinczuk and Janicki, 2012" startWordPosition="1726" endWordPosition="1729">g: agr(n) = 1 if number[n] = number[n 1] and case[n] = case[n 1] and gender[n] = gender[n 1] 0 otherwise The agr(n) feature for a token n has value 1 when the n-th and n 1-th words have the same case, gender and number. In other cases the value is 0. If one of the attributes is not set, the value is also 0. 4.1.4 Unambiguous gazetteer look-up There are many proper names which are well known and can be easily recognized using gazetteers. However, some of the proper names present in the gazetteers can be also common words. In order to avoid this problem we used an unambiguous gazetteer look-up (Marcinczuk and Janicki, 2012). We created one gazetteer containing all categories of PNs (see Section 4.1.1) and discarded all entries which were found in the SJP dictionary9 in a lower case form. 4.1.5 Heuristics We created several simple rules to recognize PNs on the basis of the orthographic features. The following phrases are recognized as proper names regardless the context: a camel case word a single word containing one or more internal upper case letters and at least one lower case letter, for example RoboRally a name of board game, a sequence of words in the quotation marks the first word must be capitalised and s</context>
</contexts>
<marker>Marcinczuk, Janicki, 2012</marker>
<rawString>Micha Marcinczuk and Maciej Janicki. 2012. Optimizing CRF-Based Model for Proper Name Recognition in Polish Texts. In Alexander F. Gelbukh, editor, CICLing (1), volume 7181 of Lecture Notes in Computer Science, pages 258269. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Marcinczuk</author>
<author>Jan Kocon</author>
<author>Maciej Janicki</author>
</authors>
<title>Liner2 - A Customizable Framework for Proper Names Recognition for Polish.</title>
<date>2013</date>
<booktitle>In Robert Bembenik, ukasz Skonieczny, Henryk Rybinski, Marzena Kryszkiewicz, and Marek Niezgodka, editors, Intelligent Tools for Building a Scientific Information Platform, volume 467 of Studies in Computational Intelligence,</booktitle>
<pages>231253</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1865" citStr="Marcinczuk et al., 2013" startWordPosition="283" endWordPosition="286">tion) English Annotation Guidelines for Entities (LDC, 2008) there are several types of named entities, including: proper names, definite descriptions and noun phrases. In this paper we focus on recognition of proper names (PNs) in Polish texts. For Polish there are only a few accessible models for PN recognition. Marcinczuk and Janicki (2012) presented a hybrid model (a statistical model combined with some heuristics) which obtained 70.53% recall with 91.44% precision for a limited set of PN categories (first names, last names, names of countries, cities and roads) tested on the CEN corpus1 (Marcinczuk et al., 2013). A model for an extended set of PN categories (56 categories) presented by Marcinczuk et al. (2013) obtained much lower recall of 54% with 93% precision tested on the same corpus. Savary 1 Home page: http://nlp.pwr.wroc.pl/cen. and Waszczuk (2012) presented a statistical model which obtained 76% recall with 83% precision for names of people, places, organizations, time expressions and name derivations tested on the National Corpus of Polish2 (Przepiorkowski et al., 2012). There are also several other works on PN recognition for Polish where a rule-based approach was used. Piskorski et al. (20</context>
<context position="5007" citStr="Marcinczuk et al., 2013" startWordPosition="791" endWordPosition="794"> The CEN corpus consists of 797 documents containing 148K tokens and 13.6K NEs. Both corpora were tagged using the morphological tagger WCRFT (Radziszewski, 2013). We used a 10-fold cross validation on the KPWr corpus to select the optimal model. The CEN corpus was used for a cross-corpus evaluation of the selected model. In this case the model was trained on the KPWr corpus and evaluated on the CEN corpus. We presented results for strict and partial matching evaluation (Chinchor, 1992). The experiments were conducted using an open-source framework for named entity recognition called Liner24 (Marcinczuk et al., 2013). 3 Reduction of NE categories In this section we investigate to what extent the recall of NE recognition can be improved by reducing the number of NE categories. As a reference model we used the statistical model presented by Marcinczuk and Janicki (2012). The model uses the Conditional Random Fields method and utilize four types of features, i.e. orthographic (18 features), morphological (6 features), wordnet (4 features) and lexicon (10 features) 38 features in total. The model uses only local features from a window of two preceding and two following tokens. The detailed description of the </context>
<context position="14321" citStr="Marcinczuk et al., 2013" startWordPosition="2324" endWordPosition="2327">istically significant improvement of the recall. In the final model we included all the presented extensions. The final model achieved a statistically significant improvement of the recall and the F-measure. To check the generality of the extensions, we performed the cross-domain evaluation on the CEN corpus (see Section 2). The results for the 56nam, the One-NAM and the Improved OneNAM models are presented in Table 4. For the strict evaluation, the recall was improved by almost 4 percentage points with a small precision improvement by almost 2 percentage points. Evaluation P R F 56nam model (Marcinczuk et al., 2013) Strict 93% 54% 68% One-NAM model Strict 85.98% 81.31% 83.58% Partial 91.12% 86.65% 88.83% Improved One-NAM model Strict 86.61% 85.05% 85.82% Partial 91.30% 90.02% 90.65% Table 4: The cross-domain evaluation of the basic and improved One-NAM models on CEN. 5 Conclusions In the paper we discussed the problem of low recall of models for recognition of a wide range of PNs for Polish. We tested to what extent the reduction of the PN categories can improve the recall. As we expected the model without PN classification obtained the best results in terms of precision and recall. Then we presented a s</context>
</contexts>
<marker>Marcinczuk, Kocon, Janicki, 2013</marker>
<rawString>Micha Marcinczuk, Jan Kocon, and Maciej Janicki. 2013. Liner2 - A Customizable Framework for Proper Names Recognition for Polish. In Robert Bembenik, ukasz Skonieczny, Henryk Rybinski, Marzena Kryszkiewicz, and Marek Niezgodka, editors, Intelligent Tools for Building a Scientific Information Platform, volume 467 of Studies in Computational Intelligence, pages 231253. Springer.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Micha Marcinczuk</author>
<author>Marcin Ptak</author>
</authors>
<title>Preliminary study on automatic induction of rules for recognition of semantic relations between proper names in polish texts.</title>
<date>2012</date>
<booktitle>Speech and Dialogue 15th International Conference, TSD 2012,</booktitle>
<volume>7499</volume>
<editor>In Petr Sojka, Ales Horak, Ivan Kopecek, and Karel Pala, editors, Text,</editor>
<publisher>Springer-Verlag, September.</publisher>
<location>Brno, Czech Republic,</location>
<contexts>
<context position="3637" citStr="Marcinczuk and Ptak, 2012" startWordPosition="570" endWordPosition="573">an extended set of proper names was presented by Abramowicz et al. (2006). The rules were tested on 156 news from the Rzeczpospolita newspaper, the Tygodnik Powszechny newspaper and the news web portals. The rules obtained 91% precision and 93% recall for country names, 55% precision and 73% recall for city names, 87% precision and 70% recall for road names and 82% precision and 66% recall for person names. The accessible models for PN recognition for Polish obtain relatively good performance in terms of precision. However, in some NLP tasks like recognition of semantic relations between PNs (Marcinczuk and Ptak, 2012), coreference resolution (Kopec and Ogrodniczuk, 2012; Broda et al., 2012a), machine translation (Gralinski et al., 2009a) or sensitive data anonymization (Gralinski et al., 2009b) the recall is much more important than the fine-grained categorization of PNs. 2 Home page: http://nkjp.pl 94 \x0cUnfortunately, the only model recognising wide range of PN categories obtains only 54% recall. Therefore, our goal is to evaluate to what extent the recall for this model can be improved. 2 Evaluation methodology In the evaluation we used two corpora annotated with 56 categories of proper names: KPWr3 (B</context>
</contexts>
<marker>Marcinczuk, Ptak, 2012</marker>
<rawString>Micha Marcinczuk and Marcin Ptak. 2012. Preliminary study on automatic induction of rules for recognition of semantic relations between proper names in polish texts. In Petr Sojka, Ales Horak, Ivan Kopecek, and Karel Pala, editors, Text, Speech and Dialogue 15th International Conference, TSD 2012, Brno, Czech Republic, September 3-7, 2012. Proceedings, volume 7499 of Lecture Notes in Artificial Intelligence (LNAI). Springer-Verlag, September.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jakub Piskorski</author>
<author>Peter Homola</author>
<author>Magorzata Marciniak</author>
<author>Agnieszka Mykowiecka</author>
<author>Adam Przepiorkowski</author>
<author>Marcin Wolinski</author>
</authors>
<title>Information Extraction for Polish Using the SProUT Platform.</title>
<date>2004</date>
<booktitle>Intelligent Information Processing and Web Mining, Proceedings of the International IIS: IIPWM04 Conference, Advances in Soft Computing,</booktitle>
<editor>In Mieczyslaw A. Kopotek, Slawomir T. Wierzchon, and Krzysztof Trojanowski, editors,</editor>
<location>Zakopane.</location>
<contexts>
<context position="2468" citStr="Piskorski et al. (2004)" startWordPosition="379" endWordPosition="382">inczuk et al., 2013). A model for an extended set of PN categories (56 categories) presented by Marcinczuk et al. (2013) obtained much lower recall of 54% with 93% precision tested on the same corpus. Savary 1 Home page: http://nlp.pwr.wroc.pl/cen. and Waszczuk (2012) presented a statistical model which obtained 76% recall with 83% precision for names of people, places, organizations, time expressions and name derivations tested on the National Corpus of Polish2 (Przepiorkowski et al., 2012). There are also several other works on PN recognition for Polish where a rule-based approach was used. Piskorski et al. (2004) constructed a set of rules and tested them on 100 news from the Rzeczpospolita newspaper. The rules obtained 90.6% precision and 85.3% recall for person names and 87.9% precision and 56.6% recall for company names. Urbanska and Mykowiecka (2005) also constructed a set of rules for recognition of person and organization names. The rules were tested on 100 short texts from the Internet. The rules obtained 98% precision and 89% recall for person names and 85% precision and 73% recall for organization names. Another rule-based approach for an extended set of proper names was presented by Abramowi</context>
</contexts>
<marker>Piskorski, Homola, Marciniak, Mykowiecka, Przepiorkowski, Wolinski, 2004</marker>
<rawString>Jakub Piskorski, Peter Homola, Magorzata Marciniak, Agnieszka Mykowiecka, Adam Przepiorkowski, and Marcin Wolinski. 2004. Information Extraction for Polish Using the SProUT Platform. In Mieczyslaw A. Kopotek, Slawomir T. Wierzchon, and Krzysztof Trojanowski, editors, Intelligent Information Processing and Web Mining, Proceedings of the International IIS: IIPWM04 Conference, Advances in Soft Computing, Zakopane. SpringerVerlag. Adam Przepiorkowski, Mirosaw Banko, Rafa L.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gorski</author>
<author>Barbara Lewandowska-Tomaszczyk</author>
<author>editors</author>
</authors>
<date>2012</date>
<booktitle>Narodowy Korpus Jezyka Polskiego [Eng.: National Corpus of Polish]. Wydawnictwo Naukowe PWN,</booktitle>
<location>Warsaw.</location>
<marker>Gorski, Lewandowska-Tomaszczyk, editors, 2012</marker>
<rawString>Gorski, and Barbara Lewandowska-Tomaszczyk, editors. 2012. Narodowy Korpus Jezyka Polskiego [Eng.: National Corpus of Polish]. Wydawnictwo Naukowe PWN, Warsaw.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Radziszewski</author>
<author>Adam Pawlaczek</author>
</authors>
<title>Large-Scale Experiments with NP Chunking of Polish.</title>
<date>2012</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>7499</volume>
<pages>143149</pages>
<editor>In Petr Sojka, Ales Horak, Ivan Kopecek, and Karel Pala, editors, TSD,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="9912" citStr="Radziszewski and Pawlaczek (2012)" startWordPosition="1590" endWordPosition="1593">rsName, region and settlement) semi-automatically extracted from Polish Wikipedia8. We divided the lexicon into 16 sets two for every semantic category (with internal and external triggers). We defined one feature for every lexicon what gives 16 features in total. The feature were defined as following: trigger(n, s) = 1 if n-th token base is found in set s 0 otherwise 7 http://zil.ipipan.waw.pl/PNET. 8 http://pl.wikipedia.org 4.1.3 Agreement feature An agreement of the morphological attributes between two consecutive words can be an indicator of phrase continuity. This observation was used by Radziszewski and Pawlaczek (2012) to recognize noun phrases. This information can be also helpful in PN boundaries recognition. The feature was defined as following: agr(n) = 1 if number[n] = number[n 1] and case[n] = case[n 1] and gender[n] = gender[n 1] 0 otherwise The agr(n) feature for a token n has value 1 when the n-th and n 1-th words have the same case, gender and number. In other cases the value is 0. If one of the attributes is not set, the value is also 0. 4.1.4 Unambiguous gazetteer look-up There are many proper names which are well known and can be easily recognized using gazetteers. However, some of the proper n</context>
</contexts>
<marker>Radziszewski, Pawlaczek, 2012</marker>
<rawString>Adam Radziszewski and Adam Pawlaczek. 2012. Large-Scale Experiments with NP Chunking of Polish. In Petr Sojka, Ales Horak, Ivan Kopecek, and Karel Pala, editors, TSD, volume 7499 of Lecture Notes in Computer Science, pages 143149. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cAdam Radziszewski</author>
</authors>
<title>A Tiered CRF Tagger for Polish. In</title>
<date>2013</date>
<booktitle>Intelligent Tools for Building a Scientific Information Platform, volume 467 of Studies in Computational Intelligence,</booktitle>
<pages>215230</pages>
<editor>Robert Bembenik, ukasz Skonieczny, Henryk Rybinski, Marzena Kryszkiewicz, and Marek Niezgodka, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="4545" citStr="Radziszewski, 2013" startWordPosition="717" endWordPosition="718">4 \x0cUnfortunately, the only model recognising wide range of PN categories obtains only 54% recall. Therefore, our goal is to evaluate to what extent the recall for this model can be improved. 2 Evaluation methodology In the evaluation we used two corpora annotated with 56 categories of proper names: KPWr3 (Broda et al., 2012b) and CEN (already mentioned in Section 1). The KPWr corpus consists of 747 documents containing near 200K tokens and 16.5K NEs. The CEN corpus consists of 797 documents containing 148K tokens and 13.6K NEs. Both corpora were tagged using the morphological tagger WCRFT (Radziszewski, 2013). We used a 10-fold cross validation on the KPWr corpus to select the optimal model. The CEN corpus was used for a cross-corpus evaluation of the selected model. In this case the model was trained on the KPWr corpus and evaluated on the CEN corpus. We presented results for strict and partial matching evaluation (Chinchor, 1992). The experiments were conducted using an open-source framework for named entity recognition called Liner24 (Marcinczuk et al., 2013). 3 Reduction of NE categories In this section we investigate to what extent the recall of NE recognition can be improved by reducing the </context>
</contexts>
<marker>Radziszewski, 2013</marker>
<rawString>\x0cAdam Radziszewski. 2013. A Tiered CRF Tagger for Polish. In Robert Bembenik, ukasz Skonieczny, Henryk Rybinski, Marzena Kryszkiewicz, and Marek Niezgodka, editors, Intelligent Tools for Building a Scientific Information Platform, volume 467 of Studies in Computational Intelligence, pages 215230. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Agata Savary</author>
<author>Jakub Waszczuk</author>
</authors>
<title>Narzedzia do anotacji jednostek nazewniczych.</title>
<date>2012</date>
<booktitle>Narodowy Korpus Jezyka Polskiego. Wydawnictwo Naukowe PWN. Creative Commons Uznanie Autorstwa 3.0 Polska.</booktitle>
<editor>In Adam Przepiorkowski, Mirosaw Banko, Rafa L. Gorski, and Barbara Lewandowska-Tomaszczyk, editors,</editor>
<marker>Savary, Waszczuk, 2012</marker>
<rawString>Agata Savary and Jakub Waszczuk. 2012. Narzedzia do anotacji jednostek nazewniczych. In Adam Przepiorkowski, Mirosaw Banko, Rafa L. Gorski, and Barbara Lewandowska-Tomaszczyk, editors, Narodowy Korpus Jezyka Polskiego. Wydawnictwo Naukowe PWN. Creative Commons Uznanie Autorstwa 3.0 Polska.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominika Urbanska</author>
<author>Agnieszka Mykowiecka</author>
</authors>
<title>Multi-words Named Entity Recognition in Polish texts.</title>
<date>2005</date>
<booktitle>In Radovan Grabik, editor, SLOVKO 2005 Third International Seminar Computer Treatment of Slavic and East European Languages,</booktitle>
<pages>208215</pages>
<publisher>VEDA.</publisher>
<location>Bratislava, Slovakia,</location>
<contexts>
<context position="2714" citStr="Urbanska and Mykowiecka (2005)" startWordPosition="419" endWordPosition="422">c.pl/cen. and Waszczuk (2012) presented a statistical model which obtained 76% recall with 83% precision for names of people, places, organizations, time expressions and name derivations tested on the National Corpus of Polish2 (Przepiorkowski et al., 2012). There are also several other works on PN recognition for Polish where a rule-based approach was used. Piskorski et al. (2004) constructed a set of rules and tested them on 100 news from the Rzeczpospolita newspaper. The rules obtained 90.6% precision and 85.3% recall for person names and 87.9% precision and 56.6% recall for company names. Urbanska and Mykowiecka (2005) also constructed a set of rules for recognition of person and organization names. The rules were tested on 100 short texts from the Internet. The rules obtained 98% precision and 89% recall for person names and 85% precision and 73% recall for organization names. Another rule-based approach for an extended set of proper names was presented by Abramowicz et al. (2006). The rules were tested on 156 news from the Rzeczpospolita newspaper, the Tygodnik Powszechny newspaper and the news web portals. The rules obtained 91% precision and 93% recall for country names, 55% precision and 73% recall for</context>
</contexts>
<marker>Urbanska, Mykowiecka, 2005</marker>
<rawString>Dominika Urbanska and Agnieszka Mykowiecka. 2005. Multi-words Named Entity Recognition in Polish texts. In Radovan Grabik, editor, SLOVKO 2005 Third International Seminar Computer Treatment of Slavic and East European Languages, Bratislava, Slovakia, pages 208215. VEDA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>