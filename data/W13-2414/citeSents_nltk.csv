We did not used any post-processing methods described by CITATION (unambiguous gazetteer chunker, heuristic chunker) because they were tuned for the specific set of NE categories.,,
In the first more common CITATION schema, all PNs are divided into four MUC categories, i.e.,,
4 http://nlp.pwr.wroc.pl/liner2 schema, assuming a separate phases for PN recognition and classification CITATION, we mapped all the PN categories to a single category, namely NAM.,,
However, in some NLP tasks like recognition of semantic relations between PNs CITATION, coreference resolution (CITATION; CITATIONa), machine translation (CITATIONa) or sensitive data anonymization (CITATIONb) the recall is much more important than the fine-grained categorization of PNs.,,
2 Evaluation methodology In the evaluation we used two corpora annotated with 56 categories of proper names: KPWr3 (CITATIONb) and CEN (already mentioned in Section 1).,,
However, in some NLP tasks like recognition of semantic relations between PNs CITATION, coreference resolution (CITATION; CITATIONa), machine translation (CITATIONa) or sensitive data anonymization (CITATIONb) the recall is much more important than the fine-grained categorization of PNs.,,
2 Evaluation methodology In the evaluation we used two corpora annotated with 56 categories of proper names: KPWr3 (CITATIONb) and CEN (already mentioned in Section 1).,,
Both corpora were tagged using the morphological tagger WCRFT CITATION.,,
We presented results for strict and partial matching evaluation CITATION.,,
The experiments were conducted using an open-source framework for named entity recognition called Liner24 CITATION.,,
As a reference model we used the statistical model presented by CITATION.,,
To check the statistical significance of precision, recall and F-measure difference we used Students t-test with a significance level = 0.01 CITATION.,,
We did not used any post-processing methods described by CITATION (unambiguous gazetteer chunker, heuristic chunker) because they were tuned for the specific set of NE categories.,,
In the first more common CITATION schema, all PNs are divided into four MUC categories, i.e.,,
4 http://nlp.pwr.wroc.pl/liner2 schema, assuming a separate phases for PN recognition and classification CITATION, we mapped all the PN categories to a single category, namely NAM.,,
However, in some NLP tasks like recognition of semantic relations between PNs CITATION, coreference resolution (CITATION; CITATIONa), machine translation (CITATIONa) or sensitive data anonymization (CITATIONb) the recall is much more important than the fine-grained categorization of PNs.,,
2 Evaluation methodology In the evaluation we used two corpora annotated with 56 categories of proper names: KPWr3 (CITATIONb) and CEN (already mentioned in Section 1).,,
However, in some NLP tasks like recognition of semantic relations between PNs CITATION, coreference resolution (CITATION; CITATIONa), machine translation (CITATIONa) or sensitive data anonymization (CITATIONb) the recall is much more important than the fine-grained categorization of PNs.,,
2 Evaluation methodology In the evaluation we used two corpora annotated with 56 categories of proper names: KPWr3 (CITATIONb) and CEN (already mentioned in Section 1).,,
However, in some NLP tasks like recognition of semantic relations between PNs CITATION, coreference resolution (CITATION; CITATIONa), machine translation (CITATIONa) or sensitive data anonymization (CITATIONb) the recall is much more important than the fine-grained categorization of PNs.,,
2 Evaluation methodology In the evaluation we used two corpora annotated with 56 categories of proper names: KPWr3 (CITATIONb) and CEN (already mentioned in Sec,,
According to the ACE (Automatic Content Extraction) English Annotation Guidelines for Entities CITATION there are several types of named entities, including: proper names, definite descriptions and noun phrases.,,
CITATION presented a hybrid model (a statistical model combined with some heuristics) which obtained 70.53% recall with 91.44% precision for a limited set of PN categories (first names, last names, names of countries, cities and roads) tested on the CEN corpus1 CITATION.,,
According to the ACE (Automatic Content Extraction) English Annotation Guidelines for Entities CITATION there are several types of named entities, including: proper names, definite descriptions and noun phrases.,,
CITATION presented a hybrid model (a statistical model combined with some heuristics) which obtained 70.53% recall with 91.44% precision for a limited set of PN categories (first names, last names, names of countries, cities and roads) tested on the CEN corpus1 CITATION.,,
We presented results for strict and partial matching evaluation CITATION.,,
The experiments were conducted using an open-source framework for named entity recognition called Liner24 CITATION.,,
As a reference model we used the statistical model presented by CITATION.,,
We did not used any post-processing methods described by CITATION (unambiguous gazetteer chunker, heuristic chunker) because they were tuned for the specific set of NE categories.,,
4.1 Extensions 4.1.1 Extended gazetteer features The reference model CITATION uses only five gazetteers of PNs (first names, last names, names of countries, cities and roads).,,
In order to avoid this problem we used an unambiguous gazetteer look-up CITATION.,,
tion) English Annotation Guidelines for Entities CITATION there are several types of named entities, including: proper names, definite descriptions and noun phrases.,,
CITATION presented a hybrid model (a statistical model combined with some heuristics) which obtained 70.53% recall with 91.44% precision for a limited set of PN categories (first names, last names, names of countries, cities and roads) tested on the CEN corpus1 CITATION.,,
Both corpora were tagged using the morphological tagger WCRFT CITATION.,,
We presented results for strict and partial matching evaluation CITATION.,,
The experiments were conducted using an open-source framework for named entity recognition called Liner24 CITATION.,,
As a reference model we used the statistical model presented by CITATION.,,
Evaluation P R F 56nam model CITATION Strict 93% 54% 68% One-NAM model Strict 85.98% 81.31% 83.58% Partial 91.12% 86.65% 88.83% Improved One-NAM model Strict 86.61% 85.05% 85.82% Partial 91.30% 90.02% 90.65% Table 4: The cross-domain evaluation of the basic and improved One-NAM models on CEN.,,
However, in some NLP tasks like recognition of semantic relations between PNs CITATION, coreference resolution (CITATION; CITATIONa), machine translation (CITATIONa) or sensitive data anonymization (CITATIONb) the recall is much more important than the fine-grained categorization of PNs.,,
CITATION constructed a set of rules and tested them on 100 news from the Rzeczpospolita newspaper.,,
CITATION also constructed a set of rules for recognition of person and organization names.,,
This observation was used by CITATION to recognize noun phrases.,,
2 Evaluation methodology In the evaluation we used two corpora annotated with 56 categories of proper names: KPWr3 (CITATIONb) and CEN (already mentioned in Section 1).,,
Both corpora were tagged using the morphological tagger WCRFT CITATION.,,
We presented results for strict and partial matching evaluation CITATION.,,
The experiments were conducted using an open-source framework for named entity recognition called Liner24 CITATION.,,
CITATION constructed a set of rules and tested them on 100 news from the Rzeczpospolita newspaper.,,
CITATION also constructed a set of rules for recognition of person and organization names.,,
