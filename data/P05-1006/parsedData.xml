<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.6712765">
b&amp;apos;Proceedings of the 43rd Annual Meeting of the ACL, pages 4249,
Ann Arbor, June 2005. c
</bodyText>
<sectionHeader confidence="0.450327" genericHeader="abstract">
2005 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.787706">
The Role of Semantic Roles in Disambiguating Verb Senses
</title>
<author confidence="0.913672">
Hoa Trang Dang
</author>
<affiliation confidence="0.960322">
National Institute of Standards and Technology
</affiliation>
<address confidence="0.866868">
Gaithersburg, MD 20899
</address>
<email confidence="0.968">
hoa.dang@nist.gov
</email>
<author confidence="0.945489">
Martha Palmer
</author>
<affiliation confidence="0.994856">
Department of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.96811">
Philadelphia, PA 19104
</address>
<email confidence="0.997237">
mpalmer@cis.upenn.edu
</email>
<sectionHeader confidence="0.990828" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.997244333333333">
We describe an automatic Word Sense
Disambiguation (WSD) system that dis-
ambiguates verb senses using syntactic
and semantic features that encode infor-
mation about predicate arguments and se-
mantic classes. Our system performs at
the best published accuracy on the English
verbs of Senseval-2. We also experiment
with using the gold-standard predicate-
argument labels from PropBank for dis-
ambiguating fine-grained WordNet senses
and course-grained PropBank framesets,
and show that disambiguation of verb
senses can be further improved with bet-
ter extraction of semantic roles.
</bodyText>
<sectionHeader confidence="0.9983" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.944583448979592">
A word can have different meanings depending
on the context in which it is used. Word Sense
Disambiguation (WSD) is the task of determining
the correct meaning (sense) of a word in con-
text, and several efforts have been made to develop
automatic WSD systems. Early work on WSD
(Yarowsky, 1995) was successful for easily distin-
guishable homonyms like bank, which have multi-
ple unrelated meanings. While homonyms are fairly
tractable, highly polysemous verbs, which have re-
lated but subtly distinct senses, pose the greatest
challenge for WSD systems (Palmer et al., 2001).
Verbs are syntactically complex, and their syntax
is thought to be determined by their underlying se-
mantics (Grimshaw, 1990; Levin, 1993). Levin verb
classes, for example, are based on the ability of a
verb to occur in pairs of syntactic frames (diathe-
sis alternations); different senses of a verb belong to
different verb classes, which have different sets of
syntactic frames that are supposed to reflect under-
lying semantic components that constrain allowable
arguments. If this is true, then the correct sense of
a verb should be revealed (at least partially) in its
arguments.
In this paper we show that the performance of
automatic WSD systems can be improved by us-
ing richer linguistic features that capture informa-
tion about predicate arguments and their semantic
classes. We describe our approach to automatic
WSD of verbs using maximum entropy models to
combine information from lexical collocations, syn-
tax, and semantic class constraints on verb argu-
ments. The system performs at the best published
accuracy on the English verbs of the Senseval-2
(Palmer et al., 2001) exercise on evaluating au-
tomatic WSD systems. The Senseval-2 verb in-
stances have been manually tagged with their Word-
Net sense and come primarily from the Penn Tree-
bank WSJ. The WSJ corpus has also been manually
annotated for predicate arguments as part of Prop-
Bank (Kingsbury and Palmer, 2002), and the inter-
section of PropBank and Senseval-2 forms a corpus
containing gold-standard annotations of WordNet
senses and PropBank semantic role labels. This pro-
vides a unique opportunity to investigate the role of
predicate arguments in verb sense disambiguation.
We show that our systems accuracy improves sig-
nificantly by adding features from PropBank, which
explicitly encodes the predicate-argument informa-
</bodyText>
<page confidence="0.997771">
42
</page>
<bodyText confidence="0.9947395">
\x0ction that our original set of syntactic and semantic
class features attempted to capture.
</bodyText>
<sectionHeader confidence="0.849486" genericHeader="method">
2 Basic automatic system
</sectionHeader>
<bodyText confidence="0.999386263157895">
Our WSD system was built to combine information
from many different sources, using as much linguis-
tic knowledge as could be gathered automatically
by NLP tools. In particular, our goal was to see
the extent to which sense-tagging of verbs could be
improved by adding features that capture informa-
tion about predicate-arguments and selectional re-
strictions.
We used the Mallet toolkit (McCallum, 2002) for
learning maximum entropy models with Gaussian
priors for all our experiments. In order to extract
the linguistic features necessary for the models, all
sentences containing the target word were automat-
ically part-of-speech-tagged using a maximum en-
tropy tagger (Ratnaparkhi, 1998) and parsed using
the Collins parser (Collins, 1997). In addition, an
automatic named entity tagger (Bikel et al., 1997)
was run on the sentences to map proper nouns to a
small set of semantic classes.1
</bodyText>
<subsectionHeader confidence="0.97674">
2.1 Topical features
</subsectionHeader>
<bodyText confidence="0.999574470588235">
We categorized the possible model features into top-
ical features and several types of local contextual
features. Topical features for a verb in a sentence
look for the presence of keywords occurring any-
where in the sentence and any surrounding sentences
provided as context (usually one or two sentences).
These features are supposed to show the domain in
which the verb is being used, since some verb senses
are used in only certain domains. The set of key-
words is specific to each verb lemma to be disam-
biguated and is determined automatically from train-
ing data so as to minimize the entropy of the proba-
bility of the senses conditioned on the keyword. All
alphabetic characters are converted to lower case.
Words occuring less than twice in the training data
or that are in a stoplist2 of pronouns, prepositions,
and conjunctions are ignored.
</bodyText>
<page confidence="0.935932">
1
</page>
<bodyText confidence="0.9852105">
The inclusion or omission of a particular company or prod-
uct implies neither endorsement nor criticism by NIST. Any
opinions, findings, and conclusions expressed are the authors
own and do not necessarily reflect those of NIST.
</bodyText>
<page confidence="0.812704">
2
</page>
<bodyText confidence="0.217864">
http://www.d.umn.edu/ tpederse/Group01/
</bodyText>
<subsectionHeader confidence="0.8262845">
WordNet/words.txt
2.2 Local features
</subsectionHeader>
<bodyText confidence="0.996780545454546">
The local features for a verb in a particular sen-
tence tend to look only within the smallest clause
containing . They include collocational features
requiring no linguistic preprocessing beyond part-
of-speech tagging, syntactic features that capture re-
lations between the verb and its complements, and
semantic features that incorporate information about
noun classes for subjects and objects:
Collocational features: Collocational features re-
fer to ordered sequences of part-of-speech tags or
word tokens immediately surrounding . They in-
</bodyText>
<table confidence="0.8692521">
clude:
\x01 unigrams: words \x03\x02\x05\x04 , \x03\x02\x07\x06 , \t\x08 , \x03
\x07\x06 , \x03
\x05\x04 and
parts of speech \x0b \x02\x05\x04 , \x0b \x02\x07\x06 , \x0b \x08 , \x0b
\x07\x06 , \x0b
\x05\x04 , where
\x0c and \x0b \x0c are at position \x0e relative to
\x01 bigrams: \x03\x02\x05\x04\x0f\x03\x02\x07\x06 , \x03\x02\x07\x06\x10\x03
\x07\x06 , \x03
</table>
<equation confidence="0.986355">
\x07\x06\x11\x03
\x05\x04 ;
\x0b \x02\x05\x04 \x0b \x02\x07\x06 , \x0b \x02\x07\x06 \x0b
\x07\x06 , \x0b
\x07\x06 \x0b
\x05\x04
\x01 trigrams: \x03\x02\x05\x12\x0f\x03\x02\x05\x04\x0f\x03\x02\x07\x06 , \x03\x02\x05\x04\x0f\x03\x02\x07\x06\x10\x03
\x07\x06 ,
\x03\x02\x07\x06\x11\x03
\x07\x06\x10\x03
\x05\x04 , \x03
\x07\x06\x10\x03
\x05\x04\x13\x03
\x05\x12 ; \x0b \x02\x05\x12 \x0b \x02\x05\x04 \x0b \x02\x07\x06 ,
\x0b \x02\x05\x04 \x0b \x02\x07\x06 \x0b
\x07\x06 , \x0b \x02\x07\x06 \x0b
\x07\x06 \x0b
\x05\x04 , \x0b
\x07\x06 \x0b
\x05\x04 \x0b
\x05\x12
</equation>
<bodyText confidence="0.972384727272727">
Syntactic features: The system uses heuristics to
extract syntactic elements from the parse for the sen-
tence containing . Let commander VP be the low-
est VP that dominates and that is not immediately
dominated by another VP, and let head VP be the
lowest VP dominating (See Figure 1). Then we
define the subject of to be the leftmost NP sib-
ling of commander VP, and a complement of to
be a node that is a child of the head VP, excluding
NPs whose head is a number or a noun from a list
of common temporal nouns (week, tomorrow,
</bodyText>
<listItem confidence="0.8376204">
Monday, etc.). The system extracts the following
binary syntactic features:
\x01 Is the sentence passive?
\x01 Is there a subject, direct object (leftmost NP
complement of ), indirect object (second left-
most NP complement of ), or clausal comple-
ment (S complement of )?
\x01 What is the word (if any) that is the particle
or head of the subject, direct object, or indirect
object?
</listItem>
<page confidence="0.999035">
43
</page>
<figure confidence="0.9968202">
\x0cS
NP
John
(commander) VP
VB
had
(head) VP
VB
pulled
NP
the blanket
PP
across the carpet
S
to create static
</figure>
<figureCaption confidence="0.820434">
Figure 1: Example parse tree for =pulled, from which is extracted the syntactic features: morph=normal
subj dobj sent-comp subj=john dobj=blanket prep=across across-obj=carpet.
</figureCaption>
<bodyText confidence="0.9919664">
\x01 If there is a PP complement, what is the prepo-
sition, and what is the object of the preposition?
Semantic features:
\x01 What is the Named Entity tag (PERSON, OR-
GANIZATION, LOCATION, UNKNOWN)
for each proper noun in the syntactic positions
above?
\x01 What are the possible WordNet synsets and hy-
pernyms for each noun in the syntactic posi-
tions above? (Nouns are not explicitly disam-
biguated; all possible synsets and hypernyms
for the noun are included.)
This set of local features relies on access to syn-
tactic structure as well as semantic class informa-
tion, and attempts to model richer linguistic infor-
mation about predicate arguments. However, the
heuristics for extracting the syntactic features are
able to identify subjects and objects of only simple
clauses. The heuristics also do not differentiate be-
tween arguments and adjuncts; for example, the fea-
ture sent-comp is intended to identify clausal com-
plements such as in (S (NP Mary) (VP (VB called)
(S him a bastard))), but Figure 1 shows how a pur-
pose clause can be mistakenly labeled as a clausal
complement.
</bodyText>
<subsectionHeader confidence="0.99563">
2.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.993571176470588">
We tested the system on the 1806 test instances of
the 29 verbs from the English lexical sample task for
Senseval-2 (Palmer et al., 2001). Accuracy was de-
fined to be the fraction of the instances for which the
system got the correct sense. All significance testing
between different accuracies was done using a one-
tailed z-test, assuming a binomial distribution of the
successes; differences in accuracy were considered
to be significant if \x0b\x15\x14\x17\x16\x19\x18\x1b\x1a\x1c\x18\x1b\x1d\x1b\x18 .
In Senseval-2, senses involving multi-word con-
structions could be identified directly from the sense
tags themselves, and the head word and satellites of
multi-word constructions were explicitly marked in
the training and test data. We trained one model
for each of the verbs and used a filter to consider
only phrasal senses whenever there were satellites
of multi-word constructions marked in the test data.
</bodyText>
<table confidence="0.58098825">
Feature Accuracy
co 0.571
co+syn 0.598
co+syn+sem 0.625
</table>
<tableCaption confidence="0.991422">
Table 1: Accuracy of system on Senseval-2 verbs
</tableCaption>
<bodyText confidence="0.868794">
using topical features and different subsets of local
features.
Table 1 shows the accuracy of the system using
topical features and different subsets of local fea-
</bodyText>
<page confidence="0.996591">
44
</page>
<bodyText confidence="0.994479428571428">
\x0ctures. Adding features from richer linguistic sources
always improves accuracy. Adding lexical syntac-
tic (syn) features improves accuracy significantly
over using just collocational (co) features (\x0b\x1e\x16
\x18\x1b\x1a\x1c\x18\x1b\x1d\x1b\x18 ). When semantic class (sem) features are
added, the improvement is also significant.
Adding topical information to all the local fea-
tures improves accuracy, but not significantly; when
the topical features are removed the accuracy of our
system falls only slightly, to 62.0%. Senses based
on domain or topic occur rarely in the Senseval-2
corpus. Most of the information provided by topi-
cal features already seem to be captured by the local
features for the frequent senses.
</bodyText>
<table confidence="0.8634096">
Features Accuracy
co+syn 0.598
co+syn+ne 0.597
co+syn+wn 0.623
co+syn+ne+wn 0.625
</table>
<tableCaption confidence="0.784169333333333">
Table 2: Accuracy of system on Senseval-2 verbs,
using topical features and different subsets of se-
mantic class features.
</tableCaption>
<bodyText confidence="0.990217375">
Semantic class information plays a significant
role in sense distinctions. Table 2 shows the
relative contribution of adding only named en-
tity tags to the collocational and syntactic features
(co+syn+ne), versus adding only the WordNet
classes (co+syn+wn), versus adding both named
entity and WordNet classes (co+syn+ne+wn).
Adding all possible WordNet noun class features for
arguments contributes a large number of parameters
to the model, but this use of WordNet with no sepa-
rate disambiguation of noun arguments proves to be
very useful. In fact, the use of WordNet for com-
mon nouns proves to be even more beneficial than
the use of a named entity tagger for proper nouns.
Given enough data, the maximum entropy model is
able to assign high weights to the correct hypernyms
of the correct noun sense if they represent defining
selectional restrictions.
Incorporating topical keywords as well as collo-
cational, syntactic, and semantic local features, our
system achieves 62.5% accuracy. This is in com-
parison to the 61.1% accuracy achieved by (Lee and
Ng, 2002), which has been the best published result
on this corpus.
</bodyText>
<sectionHeader confidence="0.725902" genericHeader="method">
3 PropBank semantic annotations
</sectionHeader>
<bodyText confidence="0.989084725">
Our WSD system uses heuristics to attempt to detect
predicate arguments from parsed sentences. How-
ever, recognition of predicate argument structures is
not straightforward, because a natural language will
have several different syntactic realizations of the
same predicate argument relations.
PropBank is a corpus in which verbs are anno-
tated with semantic tags, including coarse-grained
sense distinctions and predicate-argument struc-
tures. PropBank adds a layer of semantic annota-
tion to the Penn Wall Street Journal Treebank II.
An important goal is to provide consistent predicate-
argument structures across different syntactic real-
izations of the same verb. Polysemous verbs are also
annotated with different framesets. Frameset tags
are based on differences in subcategorization frames
and correspond to a coarse notion of word senses.
A verbs semantic arguments in PropBank are
numbered beginning with 0. Arg0 is roughly equiv-
alent to the thematic role of Agent, and Arg1 usually
corresponds to Theme or Patient; however, argument
labels are not necessarily consistent across different
senses of the same verb, or across different verbs, as
thematic roles are usually taken to be. In addition
to the core, numbered arguments, verbs can take any
of a set of general, adjunct-like arguments (ARGM),
whose labels are derived from the Treebank func-
tional tags (DIRection, LOCation, etc.).
PropBank provides manual annotation of
predicate-argument information for a large number
of verb instances in the Senseval-2 data set. The
intersection of PropBank and Senseval-2 forms
a corpus containing gold-standard annotations
of fine-grained WordNet senses, coarse-grained
PropBank framesets, and PropBank role labels.
The combination of such gold-standard semantic
annotations provides a unique opportunity to in-
vestigate the role of predicate-argument features in
word sense disambiguation, for both coarse-grained
framesets and fine-grained WordNet senses.
</bodyText>
<subsectionHeader confidence="0.997621">
3.1 PropBank features
</subsectionHeader>
<bodyText confidence="0.97377975">
We conducted experiments on the effect of using
features from PropBank for sense-tagging verbs.
Both PropBank role labels and PropBank frame-
sets were used. In the case of role labels, only the
</bodyText>
<page confidence="0.990901">
45
</page>
<bodyText confidence="0.933231166666667">
\x0cgold-standard labels found in PropBank were used,
because the best automatic semantic role labelers
only perform at about 84% precision and 75% recall
(Pradhan et al., 2004).
From the PropBank annotation for each sentence,
we extracted the following features:
</bodyText>
<listItem confidence="0.984683583333333">
1. Labels of the semantic roles: rel, ARG0,
ARG1, ARG2-WITH, ARG2, ..., ARGM-
LOC, ARGM-TMP, ARGM-NEG, ...
2. Syntactic labels of the constituent instantiat-
ing each semantic role: ARG0=NP, ARGM-
TMP=PP, ARG2-WITH=PP, ...
3. Head word of each constituent in (2):
rel=called, sats=up, ARG0=company, ARGM-
TMP=day, ...
4. Semantic classes (named entity tag,
WordNet hypernyms) of the nouns in
(3): ARGOsyn=ORGANIZATION, AR-
</listItem>
<bodyText confidence="0.9164997">
GOsyn=16185, ARGM-TMPsyn=13018, ...
When a numbered role appears in a preposi-
tional phrase (e.g., ARG2-WITH), we take the head
word to be the object of the preposition. If a con-
stituent instantiating some semantic role is a trace,
we take the head of its referent instead.
\x01 [\x1f!#&amp;quot; \x08 Mr. Bush] has [$&amp;%(\&amp;apos; called] [\x1f!#&amp;quot; \x06)\x02\x05*,+ $ for
an agreement by next September at the latest] .
For example, the PropBank features that we
extract for the sentence above are:
</bodyText>
<equation confidence="0.982915">
arg0 arg0=bush arg0syn=person arg0syn=1740 ...
rel rel=called
arg1-for arg1 arg1=agreement arg1syn=12865 ...
</equation>
<subsectionHeader confidence="0.941666">
3.2 Role labels for frameset tagging
</subsectionHeader>
<bodyText confidence="0.992357814814815">
We collected all instances of the Senseval-2 verbs
from the PropBank corpus. Only 20 of these verbs
had more than one frameset in the PropBank corpus,
resulting in 4887 instances of polysemous verbs.
The instances for each word were partitioned ran-
domly into 10 equal parts, and the system was tested
on each part after being trained on the remain-
ing nine. For these 20 verbs with more than one
PropBank frameset tag, choosing the most frequent
frameset gives a baseline accuracy of 76.0%.
The sentences were automatically pos-tagged
with the Ratnaparki tagger and parsed with the
Collins parser. We extracted local contextual fea-
tures as for WordNet sense-tagging and used the lo-
cal features to train our WSD system on the coarse-
grained sense-tagging task of automatically assign-
ing PropBank frameset tags. We tested the effect of
using only collocational features (co) for frameset
tagging, as well as using only PropBank role fea-
tures (pb) or only our original syntactic/semantic
features (synsem) for this task, and found that
the combination of collocational features with Prop-
Bank features worked best. The system has the
worst performance on the word strike, which has a
high number of framesets and a low number of train-
ing instances. Table 3 shows the performance of the
system on different subsets of local features.
</bodyText>
<table confidence="0.82539475">
Feature Accuracy
baseline 0.760
co 0.853
synsem 0.859
co+synsem 0.883
pb 0.901
co+pb 0.908
co+synsem+pb 0.907
</table>
<tableCaption confidence="0.998585">
Table 3: Accuracy of system on frameset-tagging
</tableCaption>
<bodyText confidence="0.975860263157895">
task for verbs with more than one frameset, using
different types of local features (no topical features);
all features except pb were extracted from automati-
cally pos-tagged and parsed sentences.
We obtained an overall accuracy of 88.3% using
our original local contextual features. However, the
systems performance improved significantly when
we used only PropBank role features, achieving an
accuracy of 90.1%. Furthermore, adding colloca-
tional features and heuristically extracted syntac-
tic/semantic features to the PropBank features do not
provide additional information and affects the accu-
racy of frameset-tagging only negligibly. It is not
surprising that for the coarse-grained sense-tagging
task of assigning the correct PropBank frameset
tag to a verb, using the PropBank role labels is
better than syntactic/semantic features heuristically
extracted from parses because these heuristics are
meant to capture the predicate-argument informa-
</bodyText>
<page confidence="0.992492">
46
</page>
<bodyText confidence="0.997975055555556">
\x0ction that is encoded more directly in the PropBank
role labels.
Even when the original local features were
extracted from the gold-standard pos-tagged and
parsed sentences of the Penn Treebank, the system
performed significantly worse than when PropBank
role features were used. This suggests that more ef-
fort should be applied to improving the heuristics for
extracting syntactic features.
We also experimented with adding topical fea-
tures and ARGM features from PropBank. In all
cases, these additional features reduced overall ac-
curacy, but the difference was never significant
(\x0b.-\x17\x16/\x18\x1b\x1a\x1c0\x1b\x18\x1b\x18 ). Topical features do not help because
frameset tags are based on differences in subcate-
gorization frames and not on the domain or topic.
ARGM features do not help because they are sup-
posedly used uniformly across verbs and framesets.
</bodyText>
<subsectionHeader confidence="0.9837">
3.3 Role labels for WordNet sense-tagging
</subsectionHeader>
<bodyText confidence="0.993199888888889">
We experimented with using PropBank role labels
for fine-grained WordNet sense-tagging. While
ARGM features are not useful for coarse-grained
frameset-tagging, some sense distinctions in Word-
Net are based on adverbial modifiers, such as live
well or serves someone well. Therefore, we in-
cluded PropBank ARGM features in our models for
WordNet sense-tagging to capture a wider range of
linguistic behavior. We looked at the 2571 instances
</bodyText>
<table confidence="0.872702333333333">
of 29 Senseval-2 verbs that were in both Senseval-2
and the PropBank corpus.
Features Accuracy
co 0.628
synsem 0.638
co+synsem 0.666
pb 0.656
co+pb 0.681
co+synsem+pb 0.694
</table>
<tableCaption confidence="0.998757">
Table 4: Accuracy of system on WordNet sense-
</tableCaption>
<bodyText confidence="0.996031555555556">
tagging for instances in both Senseval-2 and Prop-
Bank, using different types of local features (no top-
ical features).
Table 4 shows the accuracy of the system on
WordNet sense-tagging using different subsets of
features; all features except pb were extracted from
automatically pos-tagged and parsed sentences. By
adding PropBank role features to our original local
feature set, accuracy rose from 0.666 to to 0.694
on this subset of the Senseval-2 verbs (\x0b1\x162\x18\x1b\x1a\x1c\x18\x1b3\x1b\x18 );
the extraction of syntactic features from the parsed
sentences is again not successfully capturing all the
predicate-argument information that is explicit in
PropBank.
The verb match illustrates why accuracy im-
proves using additional PropBank features. As
shown in Figure 2, the matched objects may oc-
cur in different grammatical relations with respect
to the verb (subject, direct object, object of a prepo-
sition), but they each have an ARG1 semantic role
label in PropBank.3 Furthermore, only one of the
matched objects needs to be specified, as in Exam-
ple 3 where the second matched object (presumably
the companys prices) is unstated. Our heuristics do
not handle these alternations, and cannot detect that
the syntactic subject in Example 1 has a different se-
mantic role than the subject of Example 3.
</bodyText>
<table confidence="0.877465666666667">
Roleset match.01 match:
Arg0: person performing match
Arg1: matching objects
Ex1: [4!576 \x06 the wallpaper] [8:9&amp;lt;; matched] [475!6 \x06 the
paint]
Ex2: [475!6 \x08 The architect] [8:9&amp;lt;; matched] [4!576 \x06 the
paint] [4 8&amp;lt;= \x06)\x02?&amp;gt;A@CBED with the wallpaper]
Ex3: [475!6 \x08 The company] [8:9&amp;lt;; matched] [4!576 \x06 Ko-
daks higher prices]
</table>
<figureCaption confidence="0.945181">
Figure 2: PropBank roleset for match
</figureCaption>
<bodyText confidence="0.865749363636364">
Our basic WSD system (using local features ex-
tracted from automatic parses) confused WordNet
Sense 1 with Sense 4:
1. match, fit, correspond, check, jibe, gibe, tally,
agree (be compatible, similar or consis-
tent; coincide in their characteristics; The
two stories dont agree in many details; The
handwriting checks with the signature on the
check; The suspects fingerprints dont match
those on the gun)
4. equal, touch, rival, match (be equal to in
</bodyText>
<page confidence="0.978871">
3
</page>
<bodyText confidence="0.99610925">
PropBank annotation for match allows multiple ARG1
labels, one for each of the matching objects. Other verbs that
have more than a single ARG1 in PropBank include: attach,
bolt, coincide, connect, differ, fit, link, lock, pin, tack, tie.
</bodyText>
<page confidence="0.995675">
47
</page>
<bodyText confidence="0.996016769230769">
\x0cquality or ability; Nothing can rival cotton for
durability; Your performance doesnt even
touch that of your colleagues; Her persis-
tence and ambition only matches that of her
parents)
The senses are differentiated in that the matching
objects (ARG1) in Sense 4 have some quantifiable
characteristic that can be measured on some scale,
whereas those in Sense 1 are more general. Gold-
standard PropBank annotation of ARG1 allows the
system to generalize over the semantic classes of the
arguments and distinguish these two senses more ac-
curately.
</bodyText>
<subsectionHeader confidence="0.844185">
3.4 Frameset tags for WordNet sense-tagging
</subsectionHeader>
<bodyText confidence="0.9990501875">
PropBank frameset tags (either gold-standard or au-
tomatically tagged) were incorporated as features
in our WSD system to see if knowing the coarse-
grained sense tags would be useful in assigning fine-
grained WordNet sense tags. A frameset tag for
the instance was appended to each feature; this ef-
fectively partitions the feature set according to the
coarse-grained sense provided by the frameset. To
automatically tag an instance of a verb with its
frameset, the set of all instances of the verb in Prop-
Bank was partitioned into 10 subsets, and an in-
stance in one subset was tagged by training a max-
imum entropy model on the instances in the other
nine subsets. Various local features were consid-
ered, and the same feature types were used to train
the frameset tagger and the WordNet sense tagger
that used the automatically-assigned frameset.
For the 20 Senseval-2 verbs that had more than
one frameset in PropBank, we extracted all instances
that were in both Senseval-2 and PropBank, yield-
ing 1468 instances. We examined the effect of
incorporating the gold-standard PropBank frameset
tags into our maximum entropy models for these 20
verbs by partitioning the instances according to their
frameset tag. Table 5 shows a breakdown of the ac-
curacy by feature type. Adding the gold-standard
frameset tag (*fset) to our original local features
(orig) did not increase the accuracy significantly.
However, the increase in accuracy (from 59.7% to
62.8%) was significant when these frameset tags
were incorporated into the model that used both our
original features and all the PropBank features.
</bodyText>
<table confidence="0.7293506">
Feature Accuracy
orig 0.564
orig*fset 0.587
orig+pb 0.597
(orig+pb)*fset 0.628
</table>
<tableCaption confidence="0.993166">
Table 5: Accuracy of system on WordNet sense-
</tableCaption>
<bodyText confidence="0.972686888888889">
tagging of 20 Senseval-2 verbs with more than one
frameset, with and without gold-standard frameset
tag.
However, partitioning the instances using the au-
tomatically generated frameset tags has no signif-
icant effect on the systems performance; the in-
formation provided by the automatically assigned
coarse-grained sense tag is already encoded in the
features used for fine-grained sense-tagging.
</bodyText>
<sectionHeader confidence="0.999813" genericHeader="evaluation">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999774892857143">
Our approach of using rich linguistic features com-
bined in a single maximum entropy framework con-
trasts with that of (Florian et al., 2002). Their fea-
ture space was much like ours, but did not include
semantic class features for noun complements. With
this more impoverished feature set, they experi-
mented with combining diverse classifiers to achieve
an improvement of 2.1% over all parts of speech
(noun, verb, adjective) in the Senseval-2 lexical sam-
ple task; however, this improvement was over an ini-
tial accuracy of 56.6% on verbs, indicating that their
performance is still below ours for verbs.
(Lee and Ng, 2002) explored the relative contri-
bution of different knowledge sources and learning
algorithms to WSD; they used Support Vector Ma-
chines (SVM) and included local collocations and
syntactic relations, and also found that adding syn-
tactic features improved accuracy. Our features are
similar to theirs, but we added semantic class fea-
tures for the verb arguments. We found that the dif-
ference in machine learning algorithms did not play
a large role in performance; when we used our fea-
tures in SVM we obtained almost no difference in
performance over using maximum entropy models
with Gaussian priors.
(Gomez, 2001) described an algorithm using
WordNet to simultaneously determine verb senses
and attachments of prepositional phrases, and iden-
</bodyText>
<page confidence="0.987228">
48
</page>
<bodyText confidence="0.99628425">
\x0ctify thematic roles and adjuncts; our work is differ-
ent in that it is trained on manually annotated cor-
pora to show the relevance of semantic roles for verb
sense disambiguation.
</bodyText>
<sectionHeader confidence="0.999023" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999587518518519">
We have shown that disambiguation of verb senses
can be improved by leveraging information about
predicate arguments and their semantic classes. Our
system performs at the best published accuracy on
the English verbs of Senseval-2 even though our
heuristics for extracting syntactic features fail to
identify all and only the arguments of a verb. We
show that associating WordNet semantic classes
with nouns is beneficial even without explicit disam-
biguation of the noun senses because, given enough
data, maximum entropy models are able to assign
high weights to the correct hypernyms of the cor-
rect noun sense if they represent defining selec-
tional restrictions. Knowledge of gold-standard
predicate-argument information from PropBank im-
proves WSD on both coarse-grained senses (Prop-
Bank framesets) and fine-grained WordNet senses.
Furthermore, partitioning instances according to
their gold-standard frameset tags, which are based
on differences in subcategorization frames, also im-
proves the systems accuracy on fine-grained Word-
Net sense-tagging. Our experiments suggest that
sense disambiguation for verbs can be improved
through more accurate extraction of features rep-
resenting information such as that contained in the
framesets and predicate argument structures anno-
tated in PropBank.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99977625">
The authors would like to thank the anonymous re-
viewers for their valuable comments. This paper de-
scribes research that was conducted while the first
author was at the University of Pennsylvania.
</bodyText>
<sectionHeader confidence="0.992134" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999660148148148">
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: A high-
performance learning name-finder. In Proceedings of
the Fifth Conference on Applied Natural Language
Processing, Washington, DC.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, Madrid, Spain, July.
Radu Florian, Silviu Cucerzan, Charles Schafer, and
David Yarowsky. 2002. Combining classifiers for
word sense disambiguation. Natural Language Engi-
neering, 8(4):327341.
Fernando Gomez. 2001. An algorithm for aspects of
semantic interpretation using an enhanced wordnet. In
Proceedings of the Second Meeting of the North Amer-
ican Chapter of the Association for Computational
Linguistics.
Jane Grimshaw. 1990. Argument Structure. MIT Press,
Cambridge, MA.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to PropBank. In Proceedings of Third Interna-
tional Conference on Language Resources and Evalu-
ation, Las Palmas, Canary Islands, Spain, May.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empiri-
cal evaluation of knowledge sources and learning algo-
rithms for word sense disambiguation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, Philadelphia, PA.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Martha Palmer, Christiane Fellbaum, Scott Cotton, Lau-
ren Delfs, and Hoa Trang Dang. 2001. English
tasks: All-words and verb lexical sample. In Proceed-
ings of SENSEVAL-2: Second International Workshop
on Evaluating Word Sense Disambiguation Systems,
Toulouse, France, July.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H.
Martin, and Daniel Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-
ings of the Human Language Technology Conference
and Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, May.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
D. Yarowsky. 1995. Three Machine Learning Algo-
rithms for Lexical Ambiguity Resolution. Ph.D. thesis,
University of Pennsylvania Department of Computer
and Information Sciences.
</reference>
<page confidence="0.985934">
49
</page>
<figure confidence="0.249866">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.820291">
<note confidence="0.963702666666667">b&amp;apos;Proceedings of the 43rd Annual Meeting of the ACL, pages 4249, Ann Arbor, June 2005. c 2005 Association for Computational Linguistics</note>
<title confidence="0.995252">The Role of Semantic Roles in Disambiguating Verb Senses</title>
<author confidence="0.983306">Hoa Trang Dang</author>
<affiliation confidence="0.999837">National Institute of Standards and Technology</affiliation>
<address confidence="0.999828">Gaithersburg, MD 20899</address>
<email confidence="0.946012">hoa.dang@nist.gov</email>
<author confidence="0.999851">Martha Palmer</author>
<affiliation confidence="0.9999005">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.997869">Philadelphia, PA 19104</address>
<email confidence="0.999558">mpalmer@cis.upenn.edu</email>
<abstract confidence="0.9990830625">We describe an automatic Word Sense Disambiguation (WSD) system that disambiguates verb senses using syntactic and semantic features that encode information about predicate arguments and semantic classes. Our system performs at the best published accuracy on the English verbs of Senseval-2. We also experiment with using the gold-standard predicateargument labels from PropBank for disambiguating fine-grained WordNet senses and course-grained PropBank framesets, and show that disambiguation of verb senses can be further improved with better extraction of semantic roles.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Scott Miller</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
</authors>
<title>Nymble: A highperformance learning name-finder.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="4315" citStr="Bikel et al., 1997" startWordPosition="659" endWordPosition="662">as to see the extent to which sense-tagging of verbs could be improved by adding features that capture information about predicate-arguments and selectional restrictions. We used the Mallet toolkit (McCallum, 2002) for learning maximum entropy models with Gaussian priors for all our experiments. In order to extract the linguistic features necessary for the models, all sentences containing the target word were automatically part-of-speech-tagged using a maximum entropy tagger (Ratnaparkhi, 1998) and parsed using the Collins parser (Collins, 1997). In addition, an automatic named entity tagger (Bikel et al., 1997) was run on the sentences to map proper nouns to a small set of semantic classes.1 2.1 Topical features We categorized the possible model features into topical features and several types of local contextual features. Topical features for a verb in a sentence look for the presence of keywords occurring anywhere in the sentence and any surrounding sentences provided as context (usually one or two sentences). These features are supposed to show the domain in which the verb is being used, since some verb senses are used in only certain domains. The set of keywords is specific to each verb lemma to</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>Daniel M. Bikel, Scott Miller, Richard Schwartz, and Ralph Weischedel. 1997. Nymble: A highperformance learning name-finder. In Proceedings of the Fifth Conference on Applied Natural Language Processing, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Madrid, Spain,</location>
<contexts>
<context position="4247" citStr="Collins, 1997" startWordPosition="650" endWordPosition="651"> gathered automatically by NLP tools. In particular, our goal was to see the extent to which sense-tagging of verbs could be improved by adding features that capture information about predicate-arguments and selectional restrictions. We used the Mallet toolkit (McCallum, 2002) for learning maximum entropy models with Gaussian priors for all our experiments. In order to extract the linguistic features necessary for the models, all sentences containing the target word were automatically part-of-speech-tagged using a maximum entropy tagger (Ratnaparkhi, 1998) and parsed using the Collins parser (Collins, 1997). In addition, an automatic named entity tagger (Bikel et al., 1997) was run on the sentences to map proper nouns to a small set of semantic classes.1 2.1 Topical features We categorized the possible model features into topical features and several types of local contextual features. Topical features for a verb in a sentence look for the presence of keywords occurring anywhere in the sentence and any surrounding sentences provided as context (usually one or two sentences). These features are supposed to show the domain in which the verb is being used, since some verb senses are used in only ce</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Silviu Cucerzan</author>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Combining classifiers for word sense disambiguation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="25317" citStr="Florian et al., 2002" startWordPosition="3910" endWordPosition="3913"> 0.597 (orig+pb)*fset 0.628 Table 5: Accuracy of system on WordNet sensetagging of 20 Senseval-2 verbs with more than one frameset, with and without gold-standard frameset tag. However, partitioning the instances using the automatically generated frameset tags has no significant effect on the systems performance; the information provided by the automatically assigned coarse-grained sense tag is already encoded in the features used for fine-grained sense-tagging. 4 Related Work Our approach of using rich linguistic features combined in a single maximum entropy framework contrasts with that of (Florian et al., 2002). Their feature space was much like ours, but did not include semantic class features for noun complements. With this more impoverished feature set, they experimented with combining diverse classifiers to achieve an improvement of 2.1% over all parts of speech (noun, verb, adjective) in the Senseval-2 lexical sample task; however, this improvement was over an initial accuracy of 56.6% on verbs, indicating that their performance is still below ours for verbs. (Lee and Ng, 2002) explored the relative contribution of different knowledge sources and learning algorithms to WSD; they used Support Ve</context>
</contexts>
<marker>Florian, Cucerzan, Schafer, Yarowsky, 2002</marker>
<rawString>Radu Florian, Silviu Cucerzan, Charles Schafer, and David Yarowsky. 2002. Combining classifiers for word sense disambiguation. Natural Language Engineering, 8(4):327341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Gomez</author>
</authors>
<title>An algorithm for aspects of semantic interpretation using an enhanced wordnet.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26408" citStr="Gomez, 2001" startWordPosition="4089" endWordPosition="4090">2) explored the relative contribution of different knowledge sources and learning algorithms to WSD; they used Support Vector Machines (SVM) and included local collocations and syntactic relations, and also found that adding syntactic features improved accuracy. Our features are similar to theirs, but we added semantic class features for the verb arguments. We found that the difference in machine learning algorithms did not play a large role in performance; when we used our features in SVM we obtained almost no difference in performance over using maximum entropy models with Gaussian priors. (Gomez, 2001) described an algorithm using WordNet to simultaneously determine verb senses and attachments of prepositional phrases, and iden48 \x0ctify thematic roles and adjuncts; our work is different in that it is trained on manually annotated corpora to show the relevance of semantic roles for verb sense disambiguation. 5 Conclusion We have shown that disambiguation of verb senses can be improved by leveraging information about predicate arguments and their semantic classes. Our system performs at the best published accuracy on the English verbs of Senseval-2 even though our heuristics for extracting </context>
</contexts>
<marker>Gomez, 2001</marker>
<rawString>Fernando Gomez. 2001. An algorithm for aspects of semantic interpretation using an enhanced wordnet. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Grimshaw</author>
</authors>
<title>Argument Structure.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1723" citStr="Grimshaw, 1990" startWordPosition="255" endWordPosition="256">Word Sense Disambiguation (WSD) is the task of determining the correct meaning (sense) of a word in context, and several efforts have been made to develop automatic WSD systems. Early work on WSD (Yarowsky, 1995) was successful for easily distinguishable homonyms like bank, which have multiple unrelated meanings. While homonyms are fairly tractable, highly polysemous verbs, which have related but subtly distinct senses, pose the greatest challenge for WSD systems (Palmer et al., 2001). Verbs are syntactically complex, and their syntax is thought to be determined by their underlying semantics (Grimshaw, 1990; Levin, 1993). Levin verb classes, for example, are based on the ability of a verb to occur in pairs of syntactic frames (diathesis alternations); different senses of a verb belong to different verb classes, which have different sets of syntactic frames that are supposed to reflect underlying semantic components that constrain allowable arguments. If this is true, then the correct sense of a verb should be revealed (at least partially) in its arguments. In this paper we show that the performance of automatic WSD systems can be improved by using richer linguistic features that capture informat</context>
</contexts>
<marker>Grimshaw, 1990</marker>
<rawString>Jane Grimshaw. 1990. Argument Structure. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>From Treebank to PropBank.</title>
<date>2002</date>
<booktitle>In Proceedings of Third International Conference on Language Resources and Evaluation,</booktitle>
<location>Las Palmas, Canary Islands, Spain,</location>
<contexts>
<context position="2975" citStr="Kingsbury and Palmer, 2002" startWordPosition="458" endWordPosition="461">ents and their semantic classes. We describe our approach to automatic WSD of verbs using maximum entropy models to combine information from lexical collocations, syntax, and semantic class constraints on verb arguments. The system performs at the best published accuracy on the English verbs of the Senseval-2 (Palmer et al., 2001) exercise on evaluating automatic WSD systems. The Senseval-2 verb instances have been manually tagged with their WordNet sense and come primarily from the Penn Treebank WSJ. The WSJ corpus has also been manually annotated for predicate arguments as part of PropBank (Kingsbury and Palmer, 2002), and the intersection of PropBank and Senseval-2 forms a corpus containing gold-standard annotations of WordNet senses and PropBank semantic role labels. This provides a unique opportunity to investigate the role of predicate arguments in verb sense disambiguation. We show that our systems accuracy improves significantly by adding features from PropBank, which explicitly encodes the predicate-argument informa42 \x0ction that our original set of syntactic and semantic class features attempted to capture. 2 Basic automatic system Our WSD system was built to combine information from many differe</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In Proceedings of Third International Conference on Language Resources and Evaluation, Las Palmas, Canary Islands, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="12503" citStr="Lee and Ng, 2002" startWordPosition="1936" endWordPosition="1939">his use of WordNet with no separate disambiguation of noun arguments proves to be very useful. In fact, the use of WordNet for common nouns proves to be even more beneficial than the use of a named entity tagger for proper nouns. Given enough data, the maximum entropy model is able to assign high weights to the correct hypernyms of the correct noun sense if they represent defining selectional restrictions. Incorporating topical keywords as well as collocational, syntactic, and semantic local features, our system achieves 62.5% accuracy. This is in comparison to the 61.1% accuracy achieved by (Lee and Ng, 2002), which has been the best published result on this corpus. 3 PropBank semantic annotations Our WSD system uses heuristics to attempt to detect predicate arguments from parsed sentences. However, recognition of predicate argument structures is not straightforward, because a natural language will have several different syntactic realizations of the same predicate argument relations. PropBank is a corpus in which verbs are annotated with semantic tags, including coarse-grained sense distinctions and predicate-argument structures. PropBank adds a layer of semantic annotation to the Penn Wall Stree</context>
<context position="25798" citStr="Lee and Ng, 2002" startWordPosition="3989" endWordPosition="3992"> approach of using rich linguistic features combined in a single maximum entropy framework contrasts with that of (Florian et al., 2002). Their feature space was much like ours, but did not include semantic class features for noun complements. With this more impoverished feature set, they experimented with combining diverse classifiers to achieve an improvement of 2.1% over all parts of speech (noun, verb, adjective) in the Senseval-2 lexical sample task; however, this improvement was over an initial accuracy of 56.6% on verbs, indicating that their performance is still below ours for verbs. (Lee and Ng, 2002) explored the relative contribution of different knowledge sources and learning algorithms to WSD; they used Support Vector Machines (SVM) and included local collocations and syntactic relations, and also found that adding syntactic features improved accuracy. Our features are similar to theirs, but we added semantic class features for the verb arguments. We found that the difference in machine learning algorithms did not play a large role in performance; when we used our features in SVM we obtained almost no difference in performance over using maximum entropy models with Gaussian priors. (Go</context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="1737" citStr="Levin, 1993" startWordPosition="257" endWordPosition="258">biguation (WSD) is the task of determining the correct meaning (sense) of a word in context, and several efforts have been made to develop automatic WSD systems. Early work on WSD (Yarowsky, 1995) was successful for easily distinguishable homonyms like bank, which have multiple unrelated meanings. While homonyms are fairly tractable, highly polysemous verbs, which have related but subtly distinct senses, pose the greatest challenge for WSD systems (Palmer et al., 2001). Verbs are syntactically complex, and their syntax is thought to be determined by their underlying semantics (Grimshaw, 1990; Levin, 1993). Levin verb classes, for example, are based on the ability of a verb to occur in pairs of syntactic frames (diathesis alternations); different senses of a verb belong to different verb classes, which have different sets of syntactic frames that are supposed to reflect underlying semantic components that constrain allowable arguments. If this is true, then the correct sense of a verb should be revealed (at least partially) in its arguments. In this paper we show that the performance of automatic WSD systems can be improved by using richer linguistic features that capture information about pred</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="3910" citStr="McCallum, 2002" startWordPosition="601" endWordPosition="602">tly by adding features from PropBank, which explicitly encodes the predicate-argument informa42 \x0ction that our original set of syntactic and semantic class features attempted to capture. 2 Basic automatic system Our WSD system was built to combine information from many different sources, using as much linguistic knowledge as could be gathered automatically by NLP tools. In particular, our goal was to see the extent to which sense-tagging of verbs could be improved by adding features that capture information about predicate-arguments and selectional restrictions. We used the Mallet toolkit (McCallum, 2002) for learning maximum entropy models with Gaussian priors for all our experiments. In order to extract the linguistic features necessary for the models, all sentences containing the target word were automatically part-of-speech-tagged using a maximum entropy tagger (Ratnaparkhi, 1998) and parsed using the Collins parser (Collins, 1997). In addition, an automatic named entity tagger (Bikel et al., 1997) was run on the sentences to map proper nouns to a small set of semantic classes.1 2.1 Topical features We categorized the possible model features into topical features and several types of local</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Christiane Fellbaum</author>
<author>Scott Cotton</author>
<author>Lauren Delfs</author>
<author>Hoa Trang Dang</author>
</authors>
<title>English tasks: All-words and verb lexical sample.</title>
<date>2001</date>
<booktitle>In Proceedings of SENSEVAL-2: Second International Workshop on Evaluating Word Sense Disambiguation Systems,</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="1598" citStr="Palmer et al., 2001" startWordPosition="234" endWordPosition="237">tter extraction of semantic roles. 1 Introduction A word can have different meanings depending on the context in which it is used. Word Sense Disambiguation (WSD) is the task of determining the correct meaning (sense) of a word in context, and several efforts have been made to develop automatic WSD systems. Early work on WSD (Yarowsky, 1995) was successful for easily distinguishable homonyms like bank, which have multiple unrelated meanings. While homonyms are fairly tractable, highly polysemous verbs, which have related but subtly distinct senses, pose the greatest challenge for WSD systems (Palmer et al., 2001). Verbs are syntactically complex, and their syntax is thought to be determined by their underlying semantics (Grimshaw, 1990; Levin, 1993). Levin verb classes, for example, are based on the ability of a verb to occur in pairs of syntactic frames (diathesis alternations); different senses of a verb belong to different verb classes, which have different sets of syntactic frames that are supposed to reflect underlying semantic components that constrain allowable arguments. If this is true, then the correct sense of a verb should be revealed (at least partially) in its arguments. In this paper we</context>
<context position="9462" citStr="Palmer et al., 2001" startWordPosition="1476" endWordPosition="1479">ormation about predicate arguments. However, the heuristics for extracting the syntactic features are able to identify subjects and objects of only simple clauses. The heuristics also do not differentiate between arguments and adjuncts; for example, the feature sent-comp is intended to identify clausal complements such as in (S (NP Mary) (VP (VB called) (S him a bastard))), but Figure 1 shows how a purpose clause can be mistakenly labeled as a clausal complement. 2.3 Evaluation We tested the system on the 1806 test instances of the 29 verbs from the English lexical sample task for Senseval-2 (Palmer et al., 2001). Accuracy was defined to be the fraction of the instances for which the system got the correct sense. All significance testing between different accuracies was done using a onetailed z-test, assuming a binomial distribution of the successes; differences in accuracy were considered to be significant if \x0b\x15\x14\x17\x16\x19\x18\x1b\x1a\x1c\x18\x1b\x1d\x1b\x18 . In Senseval-2, senses involving multi-word constructions could be identified directly from the sense tags themselves, and the head word and satellites of multi-word constructions were explicitly marked in the training and test data. </context>
</contexts>
<marker>Palmer, Fellbaum, Cotton, Delfs, Dang, 2001</marker>
<rawString>Martha Palmer, Christiane Fellbaum, Scott Cotton, Lauren Delfs, and Hoa Trang Dang. 2001. English tasks: All-words and verb lexical sample. In Proceedings of SENSEVAL-2: Second International Workshop on Evaluating Word Sense Disambiguation Systems, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="14945" citStr="Pradhan et al., 2004" startWordPosition="2296" endWordPosition="2299"> of such gold-standard semantic annotations provides a unique opportunity to investigate the role of predicate-argument features in word sense disambiguation, for both coarse-grained framesets and fine-grained WordNet senses. 3.1 PropBank features We conducted experiments on the effect of using features from PropBank for sense-tagging verbs. Both PropBank role labels and PropBank framesets were used. In the case of role labels, only the 45 \x0cgold-standard labels found in PropBank were used, because the best automatic semantic role labelers only perform at about 84% precision and 75% recall (Pradhan et al., 2004). From the PropBank annotation for each sentence, we extracted the following features: 1. Labels of the semantic roles: rel, ARG0, ARG1, ARG2-WITH, ARG2, ..., ARGMLOC, ARGM-TMP, ARGM-NEG, ... 2. Syntactic labels of the constituent instantiating each semantic role: ARG0=NP, ARGMTMP=PP, ARG2-WITH=PP, ... 3. Head word of each constituent in (2): rel=called, sats=up, ARG0=company, ARGMTMP=day, ... 4. Semantic classes (named entity tag, WordNet hypernyms) of the nouns in (3): ARGOsyn=ORGANIZATION, ARGOsyn=16185, ARGM-TMPsyn=13018, ... When a numbered role appears in a prepositional phrase (e.g., AR</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Daniel Jurafsky. 2004. Shallow semantic parsing using support vector machines. In Proceedings of the Human Language Technology Conference and Meeting of the North American Chapter of the Association for Computational Linguistics, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="4195" citStr="Ratnaparkhi, 1998" startWordPosition="642" endWordPosition="643"> sources, using as much linguistic knowledge as could be gathered automatically by NLP tools. In particular, our goal was to see the extent to which sense-tagging of verbs could be improved by adding features that capture information about predicate-arguments and selectional restrictions. We used the Mallet toolkit (McCallum, 2002) for learning maximum entropy models with Gaussian priors for all our experiments. In order to extract the linguistic features necessary for the models, all sentences containing the target word were automatically part-of-speech-tagged using a maximum entropy tagger (Ratnaparkhi, 1998) and parsed using the Collins parser (Collins, 1997). In addition, an automatic named entity tagger (Bikel et al., 1997) was run on the sentences to map proper nouns to a small set of semantic classes.1 2.1 Topical features We categorized the possible model features into topical features and several types of local contextual features. Topical features for a verb in a sentence look for the presence of keywords occurring anywhere in the sentence and any surrounding sentences provided as context (usually one or two sentences). These features are supposed to show the domain in which the verb is be</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Three Machine Learning Algorithms for Lexical Ambiguity Resolution.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania Department of Computer and Information Sciences.</institution>
<contexts>
<context position="1321" citStr="Yarowsky, 1995" startWordPosition="194" endWordPosition="195">sh verbs of Senseval-2. We also experiment with using the gold-standard predicateargument labels from PropBank for disambiguating fine-grained WordNet senses and course-grained PropBank framesets, and show that disambiguation of verb senses can be further improved with better extraction of semantic roles. 1 Introduction A word can have different meanings depending on the context in which it is used. Word Sense Disambiguation (WSD) is the task of determining the correct meaning (sense) of a word in context, and several efforts have been made to develop automatic WSD systems. Early work on WSD (Yarowsky, 1995) was successful for easily distinguishable homonyms like bank, which have multiple unrelated meanings. While homonyms are fairly tractable, highly polysemous verbs, which have related but subtly distinct senses, pose the greatest challenge for WSD systems (Palmer et al., 2001). Verbs are syntactically complex, and their syntax is thought to be determined by their underlying semantics (Grimshaw, 1990; Levin, 1993). Levin verb classes, for example, are based on the ability of a verb to occur in pairs of syntactic frames (diathesis alternations); different senses of a verb belong to different ver</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Three Machine Learning Algorithms for Lexical Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania Department of Computer and Information Sciences.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>