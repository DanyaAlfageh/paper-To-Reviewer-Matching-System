<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.503664">
b&amp;apos;Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 985992
</bodyText>
<figure confidence="0.8098101">
Manchester, August 2008
Extractive Summarization Using Supervised and Semi-supervised
Learning
Kam-Fai Wong
*
, Mingli Wu
*
*
Department of Systems Engineering and
Engineering Management
</figure>
<affiliation confidence="0.94003">
The Chinese University of Hong Kong
</affiliation>
<address confidence="0.906193">
New Territories, Hong Kong
</address>
<email confidence="0.989015">
{kfwong,mlwu}@se.cuhk.edu.hk
</email>
<author confidence="0.928148">
Wenjie Li
</author>
<affiliation confidence="0.74622">
Department of Computing
The Hong Kong Polytechnic University
Kowloon, Hong Kong
</affiliation>
<email confidence="0.986613">
cswjli@comp.polyu.edu.hk
</email>
<sectionHeader confidence="0.990444" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999366">
It is difficult to identify sentence impor-
tance from a single point of view. In this
paper, we propose a learning-based ap-
proach to combine various sentence fea-
tures. They are categorized as surface,
content, relevance and event features.
Surface features are related to extrinsic
aspects of a sentence. Content features
measure a sentence based on content-
conveying words. Event features repre-
sent sentences by events they contained.
Relevance features evaluate a sentence
from its relatedness with other sentences.
Experiments show that the combined fea-
tures improved summarization perform-
ance significantly. Although the evalua-
tion results are encouraging, supervised
learning approach requires much labeled
data. Therefore we investigate co-training
by combining labeled and unlabeled data.
Experiments show that this semi-
supervised learning approach achieves
comparable performance to its supervised
counterpart and saves about half of the
labeling time cost.
</bodyText>
<sectionHeader confidence="0.982151" genericHeader="keywords">
1 Introduction
</sectionHeader>
<page confidence="0.832436">
1
</page>
<bodyText confidence="0.990267886792453">
Automatic text summarization involves con-
densing a document or a document set to produce
a human comprehensible summary. Two kinds of
summarization approaches were suggested in the
past, i.e., extractive (Radev et al., 2004; Li et al.,
2006) and abstractive summarization (Dejong,
1978). The abstractive approaches typically need
2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
to understand and then paraphrase the salient
concepts across documents. Due to the limita-
tions in natural language processing technology,
abstractive approaches are restricted to specific
domains. In contrast, extractive approaches
commonly select sentences that contain the most
significant concepts in the documents. These ap-
proaches tend to be more practical.
Recently various effective sentence features
have been proposed for extractive summarization,
such as signature word, event and sentence rele-
vance. Although encouraging results have been
reported, most of these features are investigated
individually. We argue that it is ineffective to
identify sentence importance from a single point
of view. Each sentence feature has its unique
contribution, and combing them would be advan-
tageous. Therefore we investigate combined sen-
tence features for extractive summarization. To
determine weights of different features, we em-
ploy a supervised learning framework to identify
how likely a sentence is important. Some re-
searchers explored learning based summarization,
but the new emerging features are not concerned,
such as event features (Li et. al, 2006).
We investigate the effectiveness of different
sentence features with supervised learning to de-
cide which sentences are important for summari-
zation. After feature vectors of sentences are ex-
amined, a supervised learning classifier is then
employed. Particularly, considering the length of
final summaries is fixed, candidate sentences are
re-ranked. Finally, the top sentences are ex-
tracted to compile the final summaries. Experi-
ments show that combined features improve
summarization performance significantly.
Our supervised learning approach generates
promising results based on combined features.
However, it requires much labeled data. As this
procedure is time consuming and costly, we in-
vestigate semi-supervised learning to combine
labeled data and unlabeled data. A semi-
</bodyText>
<page confidence="0.994731">
985
</page>
<bodyText confidence="0.9984028">
\x0csupervised learning classifier is used instead of a
supervised one in our extractive summarization
framework. Two classifiers are co-trained itera-
tively to exploit unlabeled data. In each iteration
step, the unlabeled training examples with top
classifying confidence are included in the labeled
training set, and the two classifiers are trained on
the new training data. Experiments show that the
performance of our semi-supervised learning
approach is comparable to its supervised learning
counterpart and it can reduce the labeling time
cost by 50%.
The remainder of this paper is organized as
follows. Section 2 gives related work and Section
3 describes our learning-based extractive summa-
rization framework. Section 4 outlines the vari-
ous sentence features and Section 5 describes
supervised/semi-supervised learning approaches.
Section 6 presents experiments and results. Fi-
nally, Section 7 concludes the paper.
</bodyText>
<sectionHeader confidence="0.999633" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998664569444445">
Traditionally, features for summarization were
studied separately. Radev et al. (2004) reported
that position and length are useful surface fea-
tures. They observed that sentences located at the
document head most likely contained important
information. Recently, content features were also
well studied, including centroid (Radev et al.,
2004), signature terms (Lin and Hovy, 2000) and
high frequency words (Nenkova e t al., 2006).
Radev et al. (2004) defined centroid words as
those whose average tf*idf score were higher
than a threshold. Lin and Hovy (2000) identified
signature terms that were strongly associated
with documents based on statistics measures.
Nenkova et al. (2006) later reported that high
frequency words were crucial in reflecting the
focus of the document.
Bag of words is somewhat loose and omits
structural information. Document structure is
another possible feature for summarization. Bar-
zilay and Elhadad (1997) constructed lexical
chains and extracted strong chains in summaries.
Marcu (1997) parsed documents as rhetorical
trees and identified important sentences based on
the trees. However, only moderate results were
reported. On the other hand, Dejong (1978) rep-
resented documents using predefined templates.
The procedure to create and fill the templates
was time consuming and it was hard to adapt the
method to different domains.
Recently, semi-structure events (Filatovia and
Hatzivassiloglou, 2004; Li et al., 2006; Wu, 2006)
have been investigated by many researchers as
they balanced document representation with
words and structures. They defined events as
verbs (or action nouns) plus the associated
named entities. For instance, given the sentence
Yasser Arafat on Tuesday accused the United
States of threatening to kill PLO officials, they
first identified accused, threatening and
kill as event terms; and Yasser Arafat,
United States, PLO and Tuesday as event
elements. Encouraging results based on events
were reported for news stories.
From another point of view, sentences in a
document are somehow connected. Sentence
relevance has been used as an alternative means
to identify important sentences. Erkan and Radev
(2004) and Yoshioka (2004) evaluate the rele-
vance (similarity) between any two sentences
first. Then a web analysis approach, PageRank,
was used to select important sentences from a
sentence map built on relevance. Promising re-
sults were reported. However, the combination of
these features is not well studied. Wu et al. (2007)
conducted preliminary research on this problem,
but event features were not considered.
Normally labeling procedure in supervised
learning is very time consuming. Blum and
Mitchell (1998) proposed co-training approach to
exploit labeled and unlabeled data. Promising
results were reported from their experiments on
web page classification. A number of successful
studies emerged thereafter for other natural lan-
guage processing tasks, such as text classification
(Denis and Gilleron, 2003), noun phrase chunk-
ing (Pierce and Cardie, 2001), parsing (Sarkar,
2001) and reference or relation resolution (Mul-
ler et al., 2001; Li et al., 2004). To our knowl-
edge, there is little research in the application of
co-training techniques to extractive summariza-
tion.
</bodyText>
<sectionHeader confidence="0.891817" genericHeader="method">
3 The Framework for Extractive Sum-
</sectionHeader>
<bodyText confidence="0.9726225">
marization
Extractive summarization can be regarded as a
classification problem. Given the features of a
sentence, a machine-learning based classification
model will judge how likely the sentence is im-
portant. The classification model can be super-
vised or semi-supervised learning. Supervised
approaches normally perform better, but require
more labeled training data. SVMs perform well
in many classification problems. Thus we em-
ploy it for supervised learning. For semi-
supervised learning, we co-trained a probabilistic
</bodyText>
<page confidence="0.999552">
986
</page>
<figureCaption confidence="0.877677">
\x0cSVM and a Naive Bayesian classifier to exploit
unlabeled data.
Figure 1. Learning-based Extractive Summariza-
</figureCaption>
<bodyText confidence="0.970094764705882">
tion Framework
The automatic summarization procedure is
shown in Figure 1. First, each input sentence is
examined by going through the pre-specified fea-
ture functions. The classification model will then
predict the importance of each sentence accord-
ing to its feature values. A re-ranking algorithm
is then used to revise the order. Finally, the top
sentences are included in the summaries until the
length limitation is reached. The re-ranking algo-
rithm is crucial, as more important content are
expected to be contained in the final summary
with fixed length. Important sentences above a
threshold are regarded as candidates. The one
with less words and located at the beginning part
of a document is ranked first. The re-ranking al-
gorithm is described as follows.
</bodyText>
<equation confidence="0.941575">
Ranki = RankPosi + RankLengthi
</equation>
<bodyText confidence="0.99572525">
where RankPosi is the rank of sentence i accord-
ing to its position in a document (i.e. the sentence
no.) and RankLengthi is rank of sentence i ac-
cording to its length.
</bodyText>
<sectionHeader confidence="0.6156605" genericHeader="method">
4 Sentence Features for Extractive
Summarization
</sectionHeader>
<bodyText confidence="0.99963675">
This section provides a detailed description on
the four types of sentence features, i.e., surface,
content, event and relevance features, which will
be examined systematically.
</bodyText>
<subsectionHeader confidence="0.998467">
4.1 Surface Features
</subsectionHeader>
<bodyText confidence="0.9780418">
Surface features are based on structure of
documents or sentences, including sentence
position in the document, the number of words in
the sentence, and the number of quoted words in
the sentence (see Table 1).
</bodyText>
<figure confidence="0.97613075">
Name Description
Position 1/sentence no.
Doc_First
Whether it is the first sentence of a
document
Para_First
Whether it is the first sentence of a
paragraph
Length The number of words in a sentence
Quote
The number of quoted words in a sen-
tence
</figure>
<tableCaption confidence="0.978796">
Table 1. Types of surface features
</tableCaption>
<bodyText confidence="0.956651777777778">
The intuition with respect to the importance of
a sentence stems from the following observations:
(1) the first sentence in a document or a para-
graph is important; (2) the sentences in the ear-
lier parts of a document is more important than
sentences in later parts; (3) a sentence is impor-
tant if the number of words (except stop words)
in it is within a certain range; (4) a sentence con-
taining too many quoted words is unimportant.
</bodyText>
<subsectionHeader confidence="0.998962">
4.2 Content Features
</subsectionHeader>
<bodyText confidence="0.997363333333333">
We integrate three well-known sentence features
based on content-bearing words i.e., centroid
words, signature terms, and high frequency
words. Both unigram and bigram representations
have been investigated. Table 2 summarizes the
six content features we studied.
</bodyText>
<figure confidence="0.988012352941176">
Name Description
Centroid_Uni
The sum of the weights of cen-
troid uni-gram
Centroid_Bi
The sum of the weights of cen-
troid bi-grams
SigTerm_Uni
The number of signature uni-
grams
SigTerm_Bi The number of signature bi-grams
FreqWord_Uni
The sum of the weights of fre-
quent uni-grams
FreqWord_Bi
The sum of the weights of fre-
quent bi-grams
</figure>
<tableCaption confidence="0.973245">
Table 2. Types of content features
</tableCaption>
<subsectionHeader confidence="0.995574">
4.3 Event Features
</subsectionHeader>
<bodyText confidence="0.9992225">
An event is comprised of an event term and asso-
ciated event elements. In this study, we choose
verbs (such as elect and incorporate) and ac-
tion nouns (such as election and incorporation)
as event terms that can characterize actions. They
relate to did what. One or more associated
named entities are considered as event elements.
Four types of named entities are currently under
</bodyText>
<page confidence="0.993041">
987
</page>
<bodyText confidence="0.988470727272727">
\x0cconsideration. The GATE system (Cunningham
et al., 2002) is used to tag named entities, which
are categorized as &amp;lt;Person&amp;gt;, &amp;lt;Organization&amp;gt;,
&amp;lt;Location&amp;gt; and &amp;lt;Date&amp;gt;. They convey the infor-
mation about who, whom, when and
where. A verb or an action noun is deemed an
event term only when it appears at least once
between two named entities.
Event summarization approaches based on in-
stances or concepts are investigated. An occur-
rence of an event term (or event element) in a
document is considered as an instance, while the
collection of the same event terms (or event ele-
ments) is considered as a concept. Given a
document set, instances of event terms and event
elements are identified first. An event map is
then built based on event instances or concepts
(Wu , 2006; Li et al., 2006). PageRank algorithm
is used to assign weight to each node (an instance
or concept) in the event map. The final weight of
a sentence is the sum of weights of event in-
stances contained in the sentence.
</bodyText>
<subsectionHeader confidence="0.981617">
4.4 Relevance Features
</subsectionHeader>
<bodyText confidence="0.998040052631579">
Relevance features are incorporated to exploit
inter-sentence relationships. It is assumed that: (1)
sentences related to important sentences are im-
portant; (2) sentences related to many other sen-
tences are important. The first sentence in a
document or a paragraph is important, and other
sentences in a document are compared with the
leading ones. Two types of sentence relevance,
FirstRel_Doc and FirstRel_Para (see Table 3),
are measured by comparing pairs of sentences
using word-based cosine similarity.
Another way to exploit sentence relevance is
to build a sentence map. Every two sentences are
regarded relevant if their similarity is above a
threshold. Every two relevant sentences are con-
nected with a unidirectional link. Based on this
map, PageRank algorithm is applied to evaluate
the importance of a sentence. These relevance
features are shown in Table 3.
</bodyText>
<figure confidence="0.968368333333333">
Name Description
FirstRel_Doc
Similarity with the first sentence in
the document
FirstRel_Para
Similarity with the first sentence in
the paragraph
PageRankRel
PageRank value of the sentence
</figure>
<tableCaption confidence="0.7161825">
based on the sentence map
Table 3. Types of relevance features
</tableCaption>
<sectionHeader confidence="0.779154" genericHeader="method">
5 Supervised/Semi-supervised Learning
Approaches
</sectionHeader>
<bodyText confidence="0.995388444444445">
To incorporate features described in Section 4,
we investigate supervised and semi-supervised
learning approaches. Probabilistic Support Vec-
tor Machine (PSVM) is employed as supervised
learning (Wu et al., 2004), while the co-training
of PSVM and Naive Bayesian Classifier (NBC)
is used for semi-supervised learning. The two
learning-based classification approaches, PSVM
and NBC, are described in following sections.
</bodyText>
<subsectionHeader confidence="0.969614">
5.1 Probabilistic Support Vector Machine
</subsectionHeader>
<bodyText confidence="0.7743745">
(PSVM)
For a set of training examples ( i
</bodyText>
<equation confidence="0.9193406">
x , i
y ),
l
i ,...,
1
</equation>
<bodyText confidence="0.8965254">
= , where i
x is an instance and i
y the
corresponding label, basic SVM requires the so-
lution of the following optimization problem.
</bodyText>
<equation confidence="0.9329443125">
=
+
l
i
i
T
b
w
C
w
w
1
,
, 2
1
min
subject to
0
1
)
)
(
( ,
+
i
i
i
T
i b
x
w
y
</equation>
<bodyText confidence="0.97807">
Here the SVM classifier is expected to find a
hyper-plane to separate testing examples as posi-
tive and negative. Wu et al. (2004) extend the
basic SVM to a probabilistic version. Its goal is
</bodyText>
<equation confidence="0.980818297297297">
to estimate
k
i
x
i
y
p
pi ,...
1
),
|
( =
=
= .
First the pairwise (one-against-one) probabilities
)
,
or
|
( x
j
i
y
i
y
p
rij =
=
is estimated using
B
Af
ij
e
r +
+
1
1
</equation>
<bodyText confidence="0.9768148">
where A and B are estimated by minimizing the
negative log-likelihood function using training
data and their decision values f. Then i
p is ob-
tained by solving the following optimization
</bodyText>
<equation confidence="0.943429058823529">
problem
= =
k
i i
j
j
j
ij
i
ji
p
p
r
p
r
1 :
2
)
(
2
1
min
subject to
0
1
)
)
(
( ,
+
i
i
i
T
i b
x
w
y
The problem can be reformulated as
QP
P T
P 2
1
min
988
\x0cwhere
=
=
= =
j
i
if
j
i
if
Q
2
:
ij
ij
ji
si
i
s
s
r
r
r
</equation>
<bodyText confidence="0.9142065">
The problem is convex and the optimality condi-
tions a scalar b such that
</bodyText>
<equation confidence="0.9279991">
=
1
z
b
P
0
T
e
e
Q
</equation>
<bodyText confidence="0.833193333333333">
where e is the vector of all 1s and z is the vector
of all 0s, and b is the Lagrangian multiplier of the
equality constraint
</bodyText>
<equation confidence="0.95779875">
=
=
k
i
i
p
1
1.
</equation>
<subsectionHeader confidence="0.950395">
5.2 Naive Bayesian Classier (NBC)
</subsectionHeader>
<bodyText confidence="0.951153833333333">
Naive Bayesian Classier assumes features are
independent. It learns prior probability and con-
ditional probability of each feature, and predicts
the class label by highest posterior probability.
Given a feature vector (F1, F2, F3,..., Fn), the
classifier need to decide the label c:
</bodyText>
<figure confidence="0.976113702127659">
)
,...
,
,
|
(
max
arg 3
2
1 n
c
F
F
F
F
c
P
c =
By applying Bayesian rule, we have
)
,...,
,
,
(
)
|
,...,
,
,
(
)
(
)
,...,
,
,
|
(
3
2
1
3
2
1
3
2
1
</figure>
<equation confidence="0.819269590909091">
n
n
n
F
F
F
F
P
c
F
F
F
F
P
c
P
F
F
F
F
c
P =
</equation>
<bodyText confidence="0.99947075">
Since the denominator does not depend on c and
the values of Fi are given, therefore the denomi-
nator is a constant and we are only interested in
the numerator. As features are assumed inde-
</bodyText>
<equation confidence="0.916770857142857">
pendent,
=
=
n
i
i
n
n
c
F
P
c
P
c
F
F
F
F
P
c
P
</equation>
<figure confidence="0.677384972222222">
F
F
F
F
c
P
1
3
2
1
3
2
1
)
|
(
)
(
)
|
,...
,
,
(
)
(
)
,...
,
,
|
(
where )
|
( c
F
</figure>
<bodyText confidence="0.542816">
P i is estimated with MLE from
training data with Laplace Smoothing.
</bodyText>
<subsectionHeader confidence="0.907057">
5.3 Co-Training (COT)
</subsectionHeader>
<bodyText confidence="0.977871967741936">
Supervised learning approaches require much
labeled data and the labeling procedure is very
time-consuming. Literature (Blum and Mitchell,
1998; Collins, 1999) has suggested that unla-
beled data can be exploited together with labeled
data by co-training two classifiers. (Blum and
Mitchell, 1998) trained two classifiers of same
type on different features, and (Li et al., 2004)
trained two classifiers of different types. In this
paper, as the number of involved features is not
too many, we train two different classifiers,
PSVM and NBC, on the same feature spaces.
The co-training algorithm is described as follows.
Given:
L is the set of labeled training examples
U is the set of unlabeled training examples
Loop: until the unlabeled data is exhausted
Train the first classifier C1 (PSVM) on L
Train the second classifier C2 (NBC) on L
For each classifier Ci
Ci labels examples from U
Ci chooses p positive and n negative ex-
amples E from U. These examples have
top classifying confidence.
Ci removes examples E from U
Ci adds examples E with the correspond-
ing labels to L
End
Output: label the test examples by the optimal
classifier which is evaluated on training data ac-
cording to the classification performance.
</bodyText>
<sectionHeader confidence="0.9242645" genericHeader="method">
6 Experiments
DUC 20012
</sectionHeader>
<bodyText confidence="0.998961708333333">
has been used in our experiments. It
contains 30 clusters of relevant documents and
308 documents in total. Each cluster deals with a
specific topic (e.g. a hurricane) and comes with
model summaries created by NIST assessors. 50,
100, 200 and 400 word summaries are provided.
Twenty-five of the thirty document clusters are
used as training data and the remaining five are
used as testing. The training/testing configuration
is same in experiments of supervised learning
and semi-supervised learning, while the differ-
ence is that some sentences in training data are
not tagged for semi-supervised learning.
An automatic evaluation package, i.e.,
ROUGE (Lin and Hovy, 2003) is employed to
evaluate the summarization performance. It
compares machine-generated summaries with
model summaries based on the overlap. Precision
and recall measures are used to evaluate the clas-
sification performance. For comparison, we
evaluate our approaches on DUC 2004 data set
also. It contains 50 clusters of documents. Only
665-character summaries are given by assessors
for each cluster.
</bodyText>
<subsectionHeader confidence="0.882156">
6.1 Experiments on Supervised Learning
Approach
</subsectionHeader>
<bodyText confidence="0.9771624">
We use LibSVM3
as our classification model for
SVM classifiers normally perform better. Types
of features presented in previous section are
evaluated individually first. Precision measures
</bodyText>
<page confidence="0.564798">
2
</page>
<footnote confidence="0.765868">
http://duc.nist.gov/
3
http://www.csie.ntu.edu.tw/~cjlin/libsvm/
</footnote>
<page confidence="0.997949">
989
</page>
<bodyText confidence="0.989031357142857">
\x0cthe percentage of true important sentences
among all important sentences labeled by the
classifier. Recall measures the percentage of true
important sentences labeled by the classifier
among all true important sentences.
Table 4 shows the precisions and recalls of
different feature groups under the PSVM classi-
fier. Table 5 records the ROUGE evaluation re-
sults ROUGE-1, ROUGE-2 and ROUGE-L.
They evaluate the overlap between machine-
generated summaries and model summaries
based on unigram, bigram and long distance re-
spectively. The summary length is limited to 200
words here.
</bodyText>
<table confidence="0.995456125">
Feature Precision Recall
Sur 0.488 0.146
Con 0.407 0.167
Rel 0.488 0.146
Event 0.344 0.146
Sur+Con 0.575 0.160
Sur+Rel 0.488 0.146
Con+Rel 0.588 0.139
Sur+Event 0.600 0.125
Con+Event 0.384 0.194
Rel+Event 0.543 0.132
Sur+Con+Event 0.595 0.153
Sur+Rel+Event 0.553 0.146
Con+Rel+Event 0.581 0.125
Sur+Con+Rel 0.595 0.174
Sur+Con+Rel+Event 0.579 0.153
</table>
<tableCaption confidence="0.999682">
Table 4. Classification performance based on
</tableCaption>
<figure confidence="0.904781625">
different feature groups
Feature
Rouge-
1
Rouge-
2
Rouge-
L
</figure>
<table confidence="0.999780428571429">
Sur 0.373 0.103 0.356
Con 0.352 0.074 0.334
Rel 0.373 0.103 0.356
Event 0.344 0.064 0.325
Sur+Con 0.380 0.109 0.363
Sur+Rel 0.373 0.103 0.356
Con+Rel 0.375 0.103 0.358
Sur+Event 0.348 0.091 0.332
Con+Event 0.344 0.071 0.330
Rel+Event 0.349 0.089 0.356
Sur+Con+Event 0.379 0.106 0.363
Sur+Rel+Event 0.371 0.101 0.353
Sur+Con+Rel 0.396 0.116 0.358
Sur+Con+Rel+Event 0.375 0.106 0.359
</table>
<tableCaption confidence="0.7435655">
Table 5. ROUGE evaluation results for differ-
ent feature groups
</tableCaption>
<bodyText confidence="0.999658421052631">
From Table 4, we can see the most useful fea-
ture groups are surface and relevance, i.e.
the external characteristics of a sentence in the
document and the relationships of a sentence
with other sentences in a cluster. The evaluation
scores from surface features and relevance fea-
tures are the same. We found that the reason is
that the dominating feature in each feature group
is about whether a sentence is the first sentence
in a document. The influence of event features is
not very positive. Based on our analysis the rea-
son is that not all clusters contain enough event
terms/elements to build a good event map.
From Table 5, it can be seen that the combina-
tion of multiple features or multiple feature
groups outperforms individual feature or feature
groups. When surface, content and relevance fea-
tures are employed, the best performance is
achieved, i.e., ROUGE-1 and ROUGE-2 score
are 0.396 and 0.116 respectively. In our prelimi-
nary experiments, we find ROUGE-1 score of a
model summary is 0.422 (without stemming and
filtering stop words). Therefore summaries gen-
erated by our supervised learning approach re-
ceived comparable performance with model
summaries when evaluated by ROUGE. Al-
though ROUGE is not perfect at this time, it is
automatic and good complement to subjective
evaluations.
We also find that the Rouge scores are similar
for variations on the feature set. Sentences from
original documents are selected to build the final
summaries. Normally, only four to six sentences
are contained in one 200-word summary in our
experiments, i.e., few sentences will be kept in a
summary. As variations of the feature set only
induce little change of the order of most impor-
tant sentences, the ROUGE scores change little.
</bodyText>
<subsectionHeader confidence="0.882853">
6.2 Experiments on Semi-supervised Learn-
ing Approach
</subsectionHeader>
<bodyText confidence="0.9993296875">
Supervised learning approaches normally
achieve good performance but require manually
labeled data. Recent literature (Blum and
Mitchell, 1998; Collins, 199) has suggested that
co-training techniques reduce the amount of la-
beled data. They trained two homogeneous clas-
sifiers based on different feature spaces. How-
ever this method is unsuitable for our application
as the number of required features in our case is
not too many. Therefore we develop a co-
training approach to train different classifiers
based on same feature space. PSVM and NBC
are applied to the combination of surface, content
and relevance features.
The capability of different learning approaches
to identify important sentences is shown in Fig-
</bodyText>
<page confidence="0.968038">
990
</page>
<figureCaption confidence="0.74304375">
\x0cure 2. The x axis shows the number of labeled
sentences employed. The remained training sen-
tences in DUC 2001 are employed as unlabeled
training data. The y axis shows f-measures of
</figureCaption>
<bodyText confidence="0.99755375">
important sentences identified from the test set.
The size of the training seed set is investigated.
For each size, three different seed sets which are
chose randomly are used. The average evaluation
scores are used as the final performance. This
procedure avoids the variance of the final evalua-
tion results. The ROUGE evaluation results of
these supervised learning approaches and semi-
supervised learning approaches are shown in Ta-
ble 6 (2000 labeled sentences). It can be seen that
the ROUGE performance of co-trained classifiers
is better than that of individual classifiers.
</bodyText>
<figure confidence="0.996486090909091">
0
0.1
0.2
0.3
0.4
50 100 200 500 1000 2000
Number of Labeled Sentences
F-Measure
Cotrain
Bayes
Svm
</figure>
<figureCaption confidence="0.959502">
Figure 2. Performance of supervised learning
and semi-supervised learning approaches
</figureCaption>
<table confidence="0.986112833333333">
Learning
Approaches
Rouge-1 Rouge-2 Rouge-L
PSVM 0.358 0.082 0.323
NBC 0.353 0.061 0.317
COT 0.366 0.090 0.329
</table>
<tableCaption confidence="0.9911475">
Table 6. ROUGE evaluation results of supervised
learning and semi-supervised learning
</tableCaption>
<subsectionHeader confidence="0.978456">
6.3 Experiments on Summary Length
</subsectionHeader>
<bodyText confidence="0.996619">
In DUC 2001 dataset, 50, 100, 200 and 400-word
summaries are provided to evaluate summaries
with different length. Our supervised approach,
which generates the best performance in previous
experiments, is employed. The ROUGE scores of
evaluations on different summary length are
shown in Table 7. Our summaries consist of ex-
tracted sentences. It can be seen that these sum-
maries achieve lower ROUGE scores when the
length of summary is reduced. The reason is that
when people try to write a more concise sum-
mary, condensed contents are included in the
summaries, which may not use the original con-
tents directly. Therefore the word-overlapping
test tool in ROUGE generates lower scores.
We then tested the same classifier and same
features on DUC 2004. The length of summaries
is only 665 characters (about 100 words).
ROUGE-1 and ROUGE-2 are 0.329 and 0.073
respectively. It confirms that the performance of
our approach is sensitive to the length of the
summary.
</bodyText>
<table confidence="0.9992456">
Sum_length Rouge-1 Rouge-2 Rouge-L
50 0.241 0.036 0.205
100 0.309 0.085 0.277
200 0.396 0.116 0.358
400 0.423 0.118 0.402
</table>
<tableCaption confidence="0.98313">
Table 7. ROUGE evaluation results for differ-
ent summary length
</tableCaption>
<sectionHeader confidence="0.993756" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.998912333333333">
We explore surface, content, event, relevance
features and their combinations for extractive
summarization with supervised learning ap-
proach. Experiments show that the combination
of surface, content and relevance features per-
form best. The highest ROUGE-1, ROUGE-2
scores are 0.396 and 0.116 respectively. The
Rouge-1 score of manually generated summaries
is 0.422. This shows the ROUGE performance of
our supervised learning approach is comparable
to that of manually generated summaries. The
ROUGE-1 scores of extractive summarization
based on centroid, signature word, high fre-
quency word and event individually are 0.319,
0.356, 0.371 and 0.374 respectively. It can be
seen that our summarization approach based on
combination of features improves the perform-
ance obviously.
Although the results of supervised learning
approach are encouraging, it required much la-
beled data. To reduce labeling cost, we apply co-
training to combine labeled and unlabeled data.
Experiments show that compare with supervised
learning, semi-supervised learning approach
saves half of the labeling cost and maintains
comparable performance (0.366 vs 0.396). We
also find that our extractive summarization is
sensitive to length of the summary. When the
length is extended, the ROUGE scores of same
summarization method are improved. In the fu-
ture, we plan to investigate sentence compression
to improve performance of our summarization
approaches on short summaries.
</bodyText>
<sectionHeader confidence="0.854868" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.770831">
The research described in this paper is partially
supported by Research Grants Council of Hong
</bodyText>
<reference confidence="0.731566333333333">
Kong (RGC: PolyU5217/07E), CUHK Strategic
Grant Scheme (No: 4410001) and Direct Grant
Scheme (No: 2050417).
</reference>
<page confidence="0.936872">
991
</page>
<reference confidence="0.996920788990825">
\x0cReferences
Regina Barzilay, and Michael Elhadad. 1997. Using
lexical chains for text summarization. In Proceed-
ings of the 35th Annual Meeting of the Association
for Computational Linguistics Workshop on Intel-
ligent Scalable Text Summarization, pages 10-17.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Proceedings of the 11th Annual Conference on
Computational Learning Theory, pages 92-100.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan. 2002. GATE: a
framework and graphical development environ-
ment for robust NLP tools and applications. In
Proceedings of the 40th Annual Meeting of the As-
sociation for computational Linguistics.
Francois Denis and Remi Gilleron. 2003. Text classi-
fication and co-training from positive and unla-
beled examples. In Proceedings of the 20th Inter-
national Conference on Machine Learning Work-
shop: the Continuum from Labeled Data to Unla-
beled Data in Machine Learning and Data Mining.
Gunes Erkan and Dragomir R. Radev. 2004. LexPag-
eRank: prestige in multi-document text summariza-
tion. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Process-
ing, pages 365-371.
Elena Filatova and Vasileios Hatzivassiloglou. Event-
based extractive summarization. 2004. In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics Workshop, pages
104-111.
Gerald Francis DeJong. 1978. Fast skimming of news
stories: the FRUMP system. Ph.D. thesis, Yale
University.
Wenjie Li, Guihong Cao, Kam-Fai Wong and Chunfa
Yuan. 2004. Applying machine learning to Chinese
temporal relation resolution. In Proceedings of the
42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 583-589.
Wenjie Li, Wei Xu, Mingli Wu, Chunfa Yuan, Qin Lu.
2006. Extractive summarization using inter- and in-
tra- event relevance. In proceedings of Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
369-376.
Chin-Yew Lin; Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summariza-
tion. In Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 495-
501.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Human Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics, Edmonton, Canada.
Daniel Marcu. 1997. The rhetorical parsing of natural
language texts. In Proceedings of the 35th Annual
Meeting of the Association for computational Lin-
guistics, pages 96-103.
Christoph Muller, Stefan Rapp and Michael Strube.
2001. Applying co-training to reference resolution.
In Proceedings of the 40th Annual Meeting on As-
sociation for Computational Linguistics.
Ani Nenkova, Lucy Vanderwende and Kathleen
McKeown. 2006. A compositional context sensi-
tive multi-document summarizer: exploring the
factors that influence summarization. In Proceed-
ings of the 29th Annual International ACM SIGIR
Conference on Research and Development in In-
formation Retrieval.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1-9.
Dragomir R. Radev, Timothy Allison, et al. 2004.
MEAD - a platform for multidocument multilin-
gual text summarization. In Proceedings of 4th In-
ternational Conference on Language Resources
and Evaluation.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of 2nd Meeting
of the North American Chapter of the Association
for Computational Linguistics on Language Tech-
nologies.
Mingli Wu. 2006. Investigations on event-based
summarization. In proceedings of the 21st Interna-
tional Conference on Computational Linguistics
and 44th Annual Meeting of the Association for
Computational Linguistics Student Research Work-
shop, pages 37-42.
Mingli Wu, Wenjie Li, Furu Wei, Qin Lu and Kam-
Fai Wong. 2007. Exploiting surface, content and
relevance features for learning-based extractive
summarization. In Proceedings of 2007 IEEE In-
ternational Conference on Natural Language
Processing and Knowledge Engineering.
Ting-Fan Wu, Chih-Jen Lin and Ruby C. Weng. 2004.
Probability estimates for multi-class classification
by pairwise coupling. Journal of Machine Learning
Research, 5:975-1005.
Masaharu Yoshioka and Makoto Haraguchi. 2004.
Multiple news articles summarization based on
event reference information. In Working Notes of
the Fourth NTCIR Workshop Meeting, National In-
stitute of Informatics.
</reference>
<page confidence="0.965554">
992
</page>
<figure confidence="0.256832">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.031614">
<note confidence="0.7506245">b&amp;apos;Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 985992 Manchester, August 2008</note>
<title confidence="0.9946915">Extractive Summarization Using Supervised and Semi-supervised Learning</title>
<author confidence="0.958239">Kam-Fai Wong</author>
<email confidence="0.44284">*</email>
<author confidence="0.799432">Mingli Wu</author>
<affiliation confidence="0.5825822">Department of Systems Engineering and Engineering Management The Chinese University of Hong Kong</affiliation>
<address confidence="0.647441">New Territories, Hong Kong</address>
<email confidence="0.963223">kfwong@se.cuhk.edu.hk</email>
<email confidence="0.963223">mlwu@se.cuhk.edu.hk</email>
<author confidence="0.992206">Wenjie Li</author>
<affiliation confidence="0.9991275">Department of Computing The Hong Kong Polytechnic University</affiliation>
<address confidence="0.95487">Kowloon, Hong Kong</address>
<email confidence="0.993183">cswjli@comp.polyu.edu.hk</email>
<abstract confidence="0.999569884615385">It is difficult to identify sentence importance from a single point of view. In this paper, we propose a learning-based approach to combine various sentence features. They are categorized as surface, content, relevance and event features. Surface features are related to extrinsic aspects of a sentence. Content features measure a sentence based on contentconveying words. Event features represent sentences by events they contained. Relevance features evaluate a sentence from its relatedness with other sentences. Experiments show that the combined features improved summarization performance significantly. Although the evaluation results are encouraging, supervised learning approach requires much labeled data. Therefore we investigate co-training by combining labeled and unlabeled data. Experiments show that this semisupervised learning approach achieves comparable performance to its supervised counterpart and saves about half of the labeling time cost.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using lexical chains for text summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>10--17</pages>
<contexts>
<context position="5817" citStr="Barzilay and Elhadad (1997)" startWordPosition="822" endWordPosition="826">ng centroid (Radev et al., 2004), signature terms (Lin and Hovy, 2000) and high frequency words (Nenkova e t al., 2006). Radev et al. (2004) defined centroid words as those whose average tf*idf score were higher than a threshold. Lin and Hovy (2000) identified signature terms that were strongly associated with documents based on statistics measures. Nenkova et al. (2006) later reported that high frequency words were crucial in reflecting the focus of the document. Bag of words is somewhat loose and omits structural information. Document structure is another possible feature for summarization. Barzilay and Elhadad (1997) constructed lexical chains and extracted strong chains in summaries. Marcu (1997) parsed documents as rhetorical trees and identified important sentences based on the trees. However, only moderate results were reported. On the other hand, Dejong (1978) represented documents using predefined templates. The procedure to create and fill the templates was time consuming and it was hard to adapt the method to different domains. Recently, semi-structure events (Filatovia and Hatzivassiloglou, 2004; Li et al., 2006; Wu, 2006) have been investigated by many researchers as they balanced document repre</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>\x0cReferences Regina Barzilay, and Michael Elhadad. 1997. Using lexical chains for text summarization. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics Workshop on Intelligent Scalable Text Summarization, pages 10-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="7561" citStr="Blum and Mitchell (1998)" startWordPosition="1084" endWordPosition="1087">. Sentence relevance has been used as an alternative means to identify important sentences. Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first. Then a web analysis approach, PageRank, was used to select important sentences from a sentence map built on relevance. Promising results were reported. However, the combination of these features is not well studied. Wu et al. (2007) conducted preliminary research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results were reported from their experiments on web page classification. A number of successful studies emerged thereafter for other natural language processing tasks, such as text classification (Denis and Gilleron, 2003), noun phrase chunking (Pierce and Cardie, 2001), parsing (Sarkar, 2001) and reference or relation resolution (Muller et al., 2001; Li et al., 2004). To our knowledge, there is little research in the application of co-training techniques to extractive summarization. 3 The Framework for Extractive </context>
<context position="16998" citStr="Blum and Mitchell, 1998" startWordPosition="2791" endWordPosition="2794">( 3 2 1 3 2 1 3 2 1 n n n F F F F P c F F F F P c P F F F F c P = Since the denominator does not depend on c and the values of Fi are given, therefore the denominator is a constant and we are only interested in the numerator. As features are assumed independent, = = n i i n n c F P c P c F F F F P c P F F F F c P 1 3 2 1 3 2 1 ) | ( ) ( ) | ,... , , ( ) ( ) ,... , , | ( where ) | ( c F P i is estimated with MLE from training data with Laplace Smoothing. 5.3 Co-Training (COT) Supervised learning approaches require much labeled data and the labeling procedure is very time-consuming. Literature (Blum and Mitchell, 1998; Collins, 1999) has suggested that unlabeled data can be exploited together with labeled data by co-training two classifiers. (Blum and Mitchell, 1998) trained two classifiers of same type on different features, and (Li et al., 2004) trained two classifiers of different types. In this paper, as the number of involved features is not too many, we train two different classifiers, PSVM and NBC, on the same feature spaces. The co-training algorithm is described as follows. Given: L is the set of labeled training examples U is the set of unlabeled training examples Loop: until the unlabeled data i</context>
<context position="22884" citStr="Blum and Mitchell, 1998" startWordPosition="3708" endWordPosition="3711">o find that the Rouge scores are similar for variations on the feature set. Sentences from original documents are selected to build the final summaries. Normally, only four to six sentences are contained in one 200-word summary in our experiments, i.e., few sentences will be kept in a summary. As variations of the feature set only induce little change of the order of most important sentences, the ROUGE scores change little. 6.2 Experiments on Semi-supervised Learning Approach Supervised learning approaches normally achieve good performance but require manually labeled data. Recent literature (Blum and Mitchell, 1998; Collins, 199) has suggested that co-training techniques reduce the amount of labeled data. They trained two homogeneous classifiers based on different feature spaces. However this method is unsuitable for our application as the number of required features in our case is not too many. Therefore we develop a cotraining approach to train different classifiers based on same feature space. PSVM and NBC are applied to the combination of surface, content and relevance features. The capability of different learning approaches to identify important sentences is shown in Fig990 \x0cure 2. The x axis s</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 92-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
<author>Diana Maynard</author>
<author>Kalina Bontcheva</author>
<author>Valentin Tablan</author>
</authors>
<title>GATE: a framework and graphical development environment for robust NLP tools and applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for computational Linguistics.</booktitle>
<contexts>
<context position="12056" citStr="Cunningham et al., 2002" startWordPosition="1798" endWordPosition="1801">ure bi-grams FreqWord_Uni The sum of the weights of frequent uni-grams FreqWord_Bi The sum of the weights of frequent bi-grams Table 2. Types of content features 4.3 Event Features An event is comprised of an event term and associated event elements. In this study, we choose verbs (such as elect and incorporate) and action nouns (such as election and incorporation) as event terms that can characterize actions. They relate to did what. One or more associated named entities are considered as event elements. Four types of named entities are currently under 987 \x0cconsideration. The GATE system (Cunningham et al., 2002) is used to tag named entities, which are categorized as &amp;lt;Person&amp;gt;, &amp;lt;Organization&amp;gt;, &amp;lt;Location&amp;gt; and &amp;lt;Date&amp;gt;. They convey the information about who, whom, when and where. A verb or an action noun is deemed an event term only when it appears at least once between two named entities. Event summarization approaches based on instances or concepts are investigated. An occurrence of an event term (or event element) in a document is considered as an instance, while the collection of the same event terms (or event elements) is considered as a concept. Given a document set, instances of event terms and eve</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>Hamish Cunningham, Diana Maynard, Kalina Bontcheva, Valentin Tablan. 2002. GATE: a framework and graphical development environment for robust NLP tools and applications. In Proceedings of the 40th Annual Meeting of the Association for computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francois Denis</author>
<author>Remi Gilleron</author>
</authors>
<title>Text classification and co-training from positive and unlabeled examples.</title>
<date>2003</date>
<booktitle>In Proceedings of the 20th International Conference on Machine Learning Workshop: the Continuum from Labeled Data to Unlabeled Data in Machine Learning and Data Mining.</booktitle>
<contexts>
<context position="7863" citStr="Denis and Gilleron, 2003" startWordPosition="1126" endWordPosition="1129">p built on relevance. Promising results were reported. However, the combination of these features is not well studied. Wu et al. (2007) conducted preliminary research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results were reported from their experiments on web page classification. A number of successful studies emerged thereafter for other natural language processing tasks, such as text classification (Denis and Gilleron, 2003), noun phrase chunking (Pierce and Cardie, 2001), parsing (Sarkar, 2001) and reference or relation resolution (Muller et al., 2001; Li et al., 2004). To our knowledge, there is little research in the application of co-training techniques to extractive summarization. 3 The Framework for Extractive Summarization Extractive summarization can be regarded as a classification problem. Given the features of a sentence, a machine-learning based classification model will judge how likely the sentence is important. The classification model can be supervised or semi-supervised learning. Supervised approa</context>
</contexts>
<marker>Denis, Gilleron, 2003</marker>
<rawString>Francois Denis and Remi Gilleron. 2003. Text classification and co-training from positive and unlabeled examples. In Proceedings of the 20th International Conference on Machine Learning Workshop: the Continuum from Labeled Data to Unlabeled Data in Machine Learning and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>LexPageRank: prestige in multi-document text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>365--371</pages>
<contexts>
<context position="7051" citStr="Erkan and Radev (2004)" startWordPosition="1007" endWordPosition="1010">th words and structures. They defined events as verbs (or action nouns) plus the associated named entities. For instance, given the sentence Yasser Arafat on Tuesday accused the United States of threatening to kill PLO officials, they first identified accused, threatening and kill as event terms; and Yasser Arafat, United States, PLO and Tuesday as event elements. Encouraging results based on events were reported for news stories. From another point of view, sentences in a document are somehow connected. Sentence relevance has been used as an alternative means to identify important sentences. Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first. Then a web analysis approach, PageRank, was used to select important sentences from a sentence map built on relevance. Promising results were reported. However, the combination of these features is not well studied. Wu et al. (2007) conducted preliminary research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results we</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>Gunes Erkan and Dragomir R. Radev. 2004. LexPageRank: prestige in multi-document text summarization. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 365-371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Eventbased extractive summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics Workshop,</booktitle>
<pages>104--111</pages>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. Eventbased extractive summarization. 2004. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics Workshop, pages 104-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Francis DeJong</author>
</authors>
<title>Fast skimming of news stories: the FRUMP system.</title>
<date>1978</date>
<tech>Ph.D. thesis,</tech>
<institution>Yale University.</institution>
<marker>DeJong, 1978</marker>
<rawString>Gerald Francis DeJong. 1978. Fast skimming of news stories: the FRUMP system. Ph.D. thesis, Yale University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenjie Li</author>
<author>Guihong Cao</author>
<author>Kam-Fai Wong</author>
<author>Chunfa Yuan</author>
</authors>
<title>Applying machine learning to Chinese temporal relation resolution.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>583--589</pages>
<contexts>
<context position="8011" citStr="Li et al., 2004" startWordPosition="1151" endWordPosition="1154"> research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results were reported from their experiments on web page classification. A number of successful studies emerged thereafter for other natural language processing tasks, such as text classification (Denis and Gilleron, 2003), noun phrase chunking (Pierce and Cardie, 2001), parsing (Sarkar, 2001) and reference or relation resolution (Muller et al., 2001; Li et al., 2004). To our knowledge, there is little research in the application of co-training techniques to extractive summarization. 3 The Framework for Extractive Summarization Extractive summarization can be regarded as a classification problem. Given the features of a sentence, a machine-learning based classification model will judge how likely the sentence is important. The classification model can be supervised or semi-supervised learning. Supervised approaches normally perform better, but require more labeled training data. SVMs perform well in many classification problems. Thus we employ it for super</context>
<context position="17232" citStr="Li et al., 2004" startWordPosition="2828" endWordPosition="2831"> assumed independent, = = n i i n n c F P c P c F F F F P c P F F F F c P 1 3 2 1 3 2 1 ) | ( ) ( ) | ,... , , ( ) ( ) ,... , , | ( where ) | ( c F P i is estimated with MLE from training data with Laplace Smoothing. 5.3 Co-Training (COT) Supervised learning approaches require much labeled data and the labeling procedure is very time-consuming. Literature (Blum and Mitchell, 1998; Collins, 1999) has suggested that unlabeled data can be exploited together with labeled data by co-training two classifiers. (Blum and Mitchell, 1998) trained two classifiers of same type on different features, and (Li et al., 2004) trained two classifiers of different types. In this paper, as the number of involved features is not too many, we train two different classifiers, PSVM and NBC, on the same feature spaces. The co-training algorithm is described as follows. Given: L is the set of labeled training examples U is the set of unlabeled training examples Loop: until the unlabeled data is exhausted Train the first classifier C1 (PSVM) on L Train the second classifier C2 (NBC) on L For each classifier Ci Ci labels examples from U Ci chooses p positive and n negative examples E from U. These examples have top classifyi</context>
</contexts>
<marker>Li, Cao, Wong, Yuan, 2004</marker>
<rawString>Wenjie Li, Guihong Cao, Kam-Fai Wong and Chunfa Yuan. 2004. Applying machine learning to Chinese temporal relation resolution. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 583-589.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenjie Li</author>
<author>Wei Xu</author>
<author>Mingli Wu</author>
<author>Chunfa Yuan</author>
<author>Qin Lu</author>
</authors>
<title>Extractive summarization using inter- and intra- event relevance.</title>
<date>2006</date>
<booktitle>In proceedings of Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>369--376</pages>
<contexts>
<context position="1733" citStr="Li et al., 2006" startWordPosition="240" endWordPosition="243">ly. Although the evaluation results are encouraging, supervised learning approach requires much labeled data. Therefore we investigate co-training by combining labeled and unlabeled data. Experiments show that this semisupervised learning approach achieves comparable performance to its supervised counterpart and saves about half of the labeling time cost. 1 Introduction 1 Automatic text summarization involves condensing a document or a document set to produce a human comprehensible summary. Two kinds of summarization approaches were suggested in the past, i.e., extractive (Radev et al., 2004; Li et al., 2006) and abstractive summarization (Dejong, 1978). The abstractive approaches typically need 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. to understand and then paraphrase the salient concepts across documents. Due to the limitations in natural language processing technology, abstractive approaches are restricted to specific domains. In contrast, extractive approaches commonly select sentences that contain the most significant concepts in the documents. These approaches</context>
<context position="6331" citStr="Li et al., 2006" startWordPosition="898" endWordPosition="901">mation. Document structure is another possible feature for summarization. Barzilay and Elhadad (1997) constructed lexical chains and extracted strong chains in summaries. Marcu (1997) parsed documents as rhetorical trees and identified important sentences based on the trees. However, only moderate results were reported. On the other hand, Dejong (1978) represented documents using predefined templates. The procedure to create and fill the templates was time consuming and it was hard to adapt the method to different domains. Recently, semi-structure events (Filatovia and Hatzivassiloglou, 2004; Li et al., 2006; Wu, 2006) have been investigated by many researchers as they balanced document representation with words and structures. They defined events as verbs (or action nouns) plus the associated named entities. For instance, given the sentence Yasser Arafat on Tuesday accused the United States of threatening to kill PLO officials, they first identified accused, threatening and kill as event terms; and Yasser Arafat, United States, PLO and Tuesday as event elements. Encouraging results based on events were reported for news stories. From another point of view, sentences in a document are somehow con</context>
<context position="12782" citStr="Li et al., 2006" startWordPosition="1925" endWordPosition="1928"> convey the information about who, whom, when and where. A verb or an action noun is deemed an event term only when it appears at least once between two named entities. Event summarization approaches based on instances or concepts are investigated. An occurrence of an event term (or event element) in a document is considered as an instance, while the collection of the same event terms (or event elements) is considered as a concept. Given a document set, instances of event terms and event elements are identified first. An event map is then built based on event instances or concepts (Wu , 2006; Li et al., 2006). PageRank algorithm is used to assign weight to each node (an instance or concept) in the event map. The final weight of a sentence is the sum of weights of event instances contained in the sentence. 4.4 Relevance Features Relevance features are incorporated to exploit inter-sentence relationships. It is assumed that: (1) sentences related to important sentences are important; (2) sentences related to many other sentences are important. The first sentence in a document or a paragraph is important, and other sentences in a document are compared with the leading ones. Two types of sentence rele</context>
</contexts>
<marker>Li, Xu, Wu, Yuan, Lu, 2006</marker>
<rawString>Wenjie Li, Wei Xu, Mingli Wu, Chunfa Yuan, Qin Lu. 2006. Extractive summarization using inter- and intra- event relevance. In proceedings of Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 369-376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="5260" citStr="Lin and Hovy, 2000" startWordPosition="737" endWordPosition="740">zation framework. Section 4 outlines the various sentence features and Section 5 describes supervised/semi-supervised learning approaches. Section 6 presents experiments and results. Finally, Section 7 concludes the paper. 2 Related Work Traditionally, features for summarization were studied separately. Radev et al. (2004) reported that position and length are useful surface features. They observed that sentences located at the document head most likely contained important information. Recently, content features were also well studied, including centroid (Radev et al., 2004), signature terms (Lin and Hovy, 2000) and high frequency words (Nenkova e t al., 2006). Radev et al. (2004) defined centroid words as those whose average tf*idf score were higher than a threshold. Lin and Hovy (2000) identified signature terms that were strongly associated with documents based on statistics measures. Nenkova et al. (2006) later reported that high frequency words were crucial in reflecting the focus of the document. Bag of words is somewhat loose and omits structural information. Document structure is another possible feature for summarization. Barzilay and Elhadad (1997) constructed lexical chains and extracted s</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin; Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th International Conference on Computational Linguistics, pages 495-501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram cooccurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="18769" citStr="Lin and Hovy, 2003" startWordPosition="3082" endWordPosition="3085">ters of relevant documents and 308 documents in total. Each cluster deals with a specific topic (e.g. a hurricane) and comes with model summaries created by NIST assessors. 50, 100, 200 and 400 word summaries are provided. Twenty-five of the thirty document clusters are used as training data and the remaining five are used as testing. The training/testing configuration is same in experiments of supervised learning and semi-supervised learning, while the difference is that some sentences in training data are not tagged for semi-supervised learning. An automatic evaluation package, i.e., ROUGE (Lin and Hovy, 2003) is employed to evaluate the summarization performance. It compares machine-generated summaries with model summaries based on the overlap. Precision and recall measures are used to evaluate the classification performance. For comparison, we evaluate our approaches on DUC 2004 data set also. It contains 50 clusters of documents. Only 665-character summaries are given by assessors for each cluster. 6.1 Experiments on Supervised Learning Approach We use LibSVM3 as our classification model for SVM classifiers normally perform better. Types of features presented in previous section are evaluated in</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The rhetorical parsing of natural language texts.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for computational Linguistics,</booktitle>
<pages>96--103</pages>
<contexts>
<context position="5899" citStr="Marcu (1997)" startWordPosition="836" endWordPosition="837">Nenkova e t al., 2006). Radev et al. (2004) defined centroid words as those whose average tf*idf score were higher than a threshold. Lin and Hovy (2000) identified signature terms that were strongly associated with documents based on statistics measures. Nenkova et al. (2006) later reported that high frequency words were crucial in reflecting the focus of the document. Bag of words is somewhat loose and omits structural information. Document structure is another possible feature for summarization. Barzilay and Elhadad (1997) constructed lexical chains and extracted strong chains in summaries. Marcu (1997) parsed documents as rhetorical trees and identified important sentences based on the trees. However, only moderate results were reported. On the other hand, Dejong (1978) represented documents using predefined templates. The procedure to create and fill the templates was time consuming and it was hard to adapt the method to different domains. Recently, semi-structure events (Filatovia and Hatzivassiloglou, 2004; Li et al., 2006; Wu, 2006) have been investigated by many researchers as they balanced document representation with words and structures. They defined events as verbs (or action nouns</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Daniel Marcu. 1997. The rhetorical parsing of natural language texts. In Proceedings of the 35th Annual Meeting of the Association for computational Linguistics, pages 96-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Muller</author>
<author>Stefan Rapp</author>
<author>Michael Strube</author>
</authors>
<title>Applying co-training to reference resolution.</title>
<date>2001</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7993" citStr="Muller et al., 2001" startWordPosition="1146" endWordPosition="1150">conducted preliminary research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results were reported from their experiments on web page classification. A number of successful studies emerged thereafter for other natural language processing tasks, such as text classification (Denis and Gilleron, 2003), noun phrase chunking (Pierce and Cardie, 2001), parsing (Sarkar, 2001) and reference or relation resolution (Muller et al., 2001; Li et al., 2004). To our knowledge, there is little research in the application of co-training techniques to extractive summarization. 3 The Framework for Extractive Summarization Extractive summarization can be regarded as a classification problem. Given the features of a sentence, a machine-learning based classification model will judge how likely the sentence is important. The classification model can be supervised or semi-supervised learning. Supervised approaches normally perform better, but require more labeled training data. SVMs perform well in many classification problems. Thus we e</context>
</contexts>
<marker>Muller, Rapp, Strube, 2001</marker>
<rawString>Christoph Muller, Stefan Rapp and Michael Strube. 2001. Applying co-training to reference resolution. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
<author>Kathleen McKeown</author>
</authors>
<title>A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="5563" citStr="Nenkova et al. (2006)" startWordPosition="785" endWordPosition="788">ly. Radev et al. (2004) reported that position and length are useful surface features. They observed that sentences located at the document head most likely contained important information. Recently, content features were also well studied, including centroid (Radev et al., 2004), signature terms (Lin and Hovy, 2000) and high frequency words (Nenkova e t al., 2006). Radev et al. (2004) defined centroid words as those whose average tf*idf score were higher than a threshold. Lin and Hovy (2000) identified signature terms that were strongly associated with documents based on statistics measures. Nenkova et al. (2006) later reported that high frequency words were crucial in reflecting the focus of the document. Bag of words is somewhat loose and omits structural information. Document structure is another possible feature for summarization. Barzilay and Elhadad (1997) constructed lexical chains and extracted strong chains in summaries. Marcu (1997) parsed documents as rhetorical trees and identified important sentences based on the trees. However, only moderate results were reported. On the other hand, Dejong (1978) represented documents using predefined templates. The procedure to create and fill the templ</context>
</contexts>
<marker>Nenkova, Vanderwende, McKeown, 2006</marker>
<rawString>Ani Nenkova, Lucy Vanderwende and Kathleen McKeown. 2006. A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pierce</author>
<author>Claire Cardie</author>
</authors>
<title>Limitations of co-training for natural language learning from large datasets.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="7911" citStr="Pierce and Cardie, 2001" startWordPosition="1134" endWordPosition="1137">rted. However, the combination of these features is not well studied. Wu et al. (2007) conducted preliminary research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results were reported from their experiments on web page classification. A number of successful studies emerged thereafter for other natural language processing tasks, such as text classification (Denis and Gilleron, 2003), noun phrase chunking (Pierce and Cardie, 2001), parsing (Sarkar, 2001) and reference or relation resolution (Muller et al., 2001; Li et al., 2004). To our knowledge, there is little research in the application of co-training techniques to extractive summarization. 3 The Framework for Extractive Summarization Extractive summarization can be regarded as a classification problem. Given the features of a sentence, a machine-learning based classification model will judge how likely the sentence is important. The classification model can be supervised or semi-supervised learning. Supervised approaches normally perform better, but require more l</context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>David Pierce and Claire Cardie. 2001. Limitations of co-training for natural language learning from large datasets. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 1-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Timothy Allison</author>
</authors>
<title>MEAD - a platform for multidocument multilingual text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of 4th International Conference on Language Resources and Evaluation.</booktitle>
<marker>Radev, Allison, 2004</marker>
<rawString>Dragomir R. Radev, Timothy Allison, et al. 2004. MEAD - a platform for multidocument multilingual text summarization. In Proceedings of 4th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of 2nd Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies.</booktitle>
<contexts>
<context position="7935" citStr="Sarkar, 2001" startWordPosition="1139" endWordPosition="1140">hese features is not well studied. Wu et al. (2007) conducted preliminary research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results were reported from their experiments on web page classification. A number of successful studies emerged thereafter for other natural language processing tasks, such as text classification (Denis and Gilleron, 2003), noun phrase chunking (Pierce and Cardie, 2001), parsing (Sarkar, 2001) and reference or relation resolution (Muller et al., 2001; Li et al., 2004). To our knowledge, there is little research in the application of co-training techniques to extractive summarization. 3 The Framework for Extractive Summarization Extractive summarization can be regarded as a classification problem. Given the features of a sentence, a machine-learning based classification model will judge how likely the sentence is important. The classification model can be supervised or semi-supervised learning. Supervised approaches normally perform better, but require more labeled training data. SV</context>
</contexts>
<marker>Sarkar, 2001</marker>
<rawString>Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings of 2nd Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingli Wu</author>
</authors>
<title>Investigations on event-based summarization.</title>
<date>2006</date>
<booktitle>In proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics Student Research Workshop,</booktitle>
<pages>37--42</pages>
<contexts>
<context position="6342" citStr="Wu, 2006" startWordPosition="902" endWordPosition="903">structure is another possible feature for summarization. Barzilay and Elhadad (1997) constructed lexical chains and extracted strong chains in summaries. Marcu (1997) parsed documents as rhetorical trees and identified important sentences based on the trees. However, only moderate results were reported. On the other hand, Dejong (1978) represented documents using predefined templates. The procedure to create and fill the templates was time consuming and it was hard to adapt the method to different domains. Recently, semi-structure events (Filatovia and Hatzivassiloglou, 2004; Li et al., 2006; Wu, 2006) have been investigated by many researchers as they balanced document representation with words and structures. They defined events as verbs (or action nouns) plus the associated named entities. For instance, given the sentence Yasser Arafat on Tuesday accused the United States of threatening to kill PLO officials, they first identified accused, threatening and kill as event terms; and Yasser Arafat, United States, PLO and Tuesday as event elements. Encouraging results based on events were reported for news stories. From another point of view, sentences in a document are somehow connected. Sen</context>
</contexts>
<marker>Wu, 2006</marker>
<rawString>Mingli Wu. 2006. Investigations on event-based summarization. In proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics Student Research Workshop, pages 37-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingli Wu</author>
<author>Wenjie Li</author>
<author>Furu Wei</author>
<author>Qin Lu</author>
<author>KamFai Wong</author>
</authors>
<title>Exploiting surface, content and relevance features for learning-based extractive summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of 2007 IEEE International Conference on Natural Language Processing and Knowledge Engineering.</booktitle>
<contexts>
<context position="7373" citStr="Wu et al. (2007)" startWordPosition="1058" endWordPosition="1061">es, PLO and Tuesday as event elements. Encouraging results based on events were reported for news stories. From another point of view, sentences in a document are somehow connected. Sentence relevance has been used as an alternative means to identify important sentences. Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first. Then a web analysis approach, PageRank, was used to select important sentences from a sentence map built on relevance. Promising results were reported. However, the combination of these features is not well studied. Wu et al. (2007) conducted preliminary research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results were reported from their experiments on web page classification. A number of successful studies emerged thereafter for other natural language processing tasks, such as text classification (Denis and Gilleron, 2003), noun phrase chunking (Pierce and Cardie, 2001), parsing (Sarkar, 2001) and reference or relation resolution </context>
</contexts>
<marker>Wu, Li, Wei, Lu, Wong, 2007</marker>
<rawString>Mingli Wu, Wenjie Li, Furu Wei, Qin Lu and KamFai Wong. 2007. Exploiting surface, content and relevance features for learning-based extractive summarization. In Proceedings of 2007 IEEE International Conference on Natural Language Processing and Knowledge Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting-Fan Wu</author>
<author>Chih-Jen Lin</author>
<author>Ruby C Weng</author>
</authors>
<date>2004</date>
<contexts>
<context position="14392" citStr="Wu et al., 2004" startWordPosition="2170" endWordPosition="2173">hm is applied to evaluate the importance of a sentence. These relevance features are shown in Table 3. Name Description FirstRel_Doc Similarity with the first sentence in the document FirstRel_Para Similarity with the first sentence in the paragraph PageRankRel PageRank value of the sentence based on the sentence map Table 3. Types of relevance features 5 Supervised/Semi-supervised Learning Approaches To incorporate features described in Section 4, we investigate supervised and semi-supervised learning approaches. Probabilistic Support Vector Machine (PSVM) is employed as supervised learning (Wu et al., 2004), while the co-training of PSVM and Naive Bayesian Classifier (NBC) is used for semi-supervised learning. The two learning-based classification approaches, PSVM and NBC, are described in following sections. 5.1 Probabilistic Support Vector Machine (PSVM) For a set of training examples ( i x , i y ), l i ,..., 1 = , where i x is an instance and i y the corresponding label, basic SVM requires the solution of the following optimization problem. = + l i i T b w C w w 1 , , 2 1 min subject to 0 1 ) ) ( ( , + i i i T i b x w y Here the SVM classifier is expected to find a hyper-plane to separate tes</context>
</contexts>
<marker>Wu, Lin, Weng, 2004</marker>
<rawString>Ting-Fan Wu, Chih-Jen Lin and Ruby C. Weng. 2004.</rawString>
</citation>
<citation valid="false">
<title>Probability estimates for multi-class classification by pairwise coupling.</title>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--975</pages>
<marker></marker>
<rawString>Probability estimates for multi-class classification by pairwise coupling. Journal of Machine Learning Research, 5:975-1005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaharu Yoshioka</author>
<author>Makoto Haraguchi</author>
</authors>
<title>Multiple news articles summarization based on event reference information.</title>
<date>2004</date>
<booktitle>In Working Notes of the Fourth NTCIR Workshop Meeting, National Institute of Informatics.</booktitle>
<marker>Yoshioka, Haraguchi, 2004</marker>
<rawString>Masaharu Yoshioka and Makoto Haraguchi. 2004. Multiple news articles summarization based on event reference information. In Working Notes of the Fourth NTCIR Workshop Meeting, National Institute of Informatics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>