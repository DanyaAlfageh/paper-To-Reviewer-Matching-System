<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.8014255">
b&amp;apos;Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 2130,
Honolulu, October 2008. c
</bodyText>
<sectionHeader confidence="0.542244" genericHeader="abstract">
2008 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.959243">
Regular Expression Learning for Information Extraction
</title>
<author confidence="0.998641">
Yunyao Li, Rajasekar Krishnamurthy, Sriram Raghavan, Shivakumar Vaithyanathan
</author>
<affiliation confidence="0.996286">
IBM Almaden Research Center
</affiliation>
<address confidence="0.993048">
San Jose, CA 95120
</address>
<email confidence="0.940962">
{yunyaoli, rajase, rsriram}@us.ibm.com, shiv@almaden.ibm.com
</email>
<author confidence="0.637661">
H. V. Jagadish
</author>
<affiliation confidence="0.9771555">
Department of EECS
University of Michigan
</affiliation>
<address confidence="0.981269">
Ann Arbor, MI 48109
</address>
<email confidence="0.997517">
jag@umich.edu
</email>
<sectionHeader confidence="0.990619" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.998886176470588">
Regular expressions have served as the dom-
inant workhorse of practical information ex-
traction for several years. However, there has
been little work on reducing the manual ef-
fort involved in building high-quality, com-
plex regular expressions for information ex-
traction tasks. In this paper, we propose Re-
LIE, a novel transformation-based algorithm
for learning such complex regular expressions.
We evaluate the performance of our algorithm
on multiple datasets and compare it against the
CRF algorithm. We show that ReLIE, in ad-
dition to being an order of magnitude faster,
outperforms CRF under conditions of limited
training data and cross-domain data. Finally,
we show how the accuracy of CRF can be im-
proved by using features extracted by ReLIE.
</bodyText>
<sectionHeader confidence="0.99832" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996815720930232">
A large class of entity extraction tasks can be ac-
complished by the use of carefully constructed reg-
ular expressions (regexes). Examples of entities
amenable to such extractions include email ad-
dresses and software names (web collections), credit
card numbers and social security numbers (email
compliance), and gene and protein names (bioinfor-
matics), etc. These entities share the characteristic
that their key representative patterns (features) are
expressible in standard constructs of regular expres-
sions. At first glance, it may seem that constructing
Supported in part by NSF 0438909 and NIH 1-U54-
DA021519.
a regex to extract such entities is fairly straightfor-
ward. In reality, robust extraction requires the use
of rather complex expressions, as illustrated by the
following example.
Example 1 (Phone number extraction). An obvious
pattern for identifying phone numbers is blocks of
digits separated by hyphens represented as R1 =
(\\d+\\-)+\\d+.1
While R1 matches valid phone numbers
like 800-865-1125 and 725-1234, it suffers from both
precision and recall problems. Not only does R1
produce incorrect matches (e.g., social security numbers
like 123-45-6789), it also fails to identify valid phone
numbers such as 800.865.1125, and (800)865-CARE. An
improved regex that addresses these problems is R2 =
(\\d{3}[-.\\ ()]){1,2}[\\dA-Z]{4}.
While multiple machine learning approaches have
been proposed for information extraction in recent
years (McCallum et al., 2000; Cohen and McCal-
lum, 2003; Klein et al., 2003; Krishnan and Man-
ning, 2006), manually created regexes remain a
widely adopted practical solution for information
extraction (Appelt and Onyshkevych, 1998; Fukuda
et al., 1998; Cunningham, 1999; Tanabe and Wilbur,
2002; Li et al., 2006; DeRose et al., 2007; Zhu et al.,
2007). Yet, with a few notable exceptions, which we
discuss later in Section 1.1, there has been very little
work in reducing this human effort through the use
of automatic learning techniques. In this paper, we
propose a novel formulation of the problem of learn-
</bodyText>
<page confidence="0.866117">
1
</page>
<bodyText confidence="0.8923955">
Throughout this paper, we use the syntax of the standard
Java regex engine (Java, 2008).
</bodyText>
<page confidence="0.996617">
21
</page>
<bodyText confidence="0.998155833333333">
\x0cing regexes for information extraction tasks. We
demonstrate that high quality regex extractors can be
learned with significantly reduced manual effort. To
motivate our approach, we first discuss prior work
in the area of learning regexes and describe some of
the limitations of these techniques.
</bodyText>
<subsectionHeader confidence="0.999703">
1.1 Learning Regular Expressions
</subsectionHeader>
<bodyText confidence="0.999513358490566">
The problem of inducing regular languages from
positive and negative examples has been studied in
the past, even outside the context of information
extraction (Alquezar and Sanfeliu, 1994; Dupont,
1996; Firoiu et al., 1998; Garofalakis et al., 2000;
Denis, 2001; Denis et al., 2004; Fernau, 2005;
Galassi and Giordana, 2005; Bex et al., 2006).
Much of this work assumes that the target regex
is small and compact thereby allowing the learn-
ing algorithm to exploit this information. Consider,
for example, the learning of patterns motivated by
DNA sequencing applications (Galassi and Gior-
dana, 2005). Here the input sequence is viewed
as multiple atomic events separated by gaps. Since
each atomic event is easily described by a small and
compact regex, the problem reduces to one of learn-
ing simple regexes. Similarly, in XML DTD infer-
ence (Garofalakis et al., 2000; Bex et al., 2006), it
is possible to exploit the fact that the XML docu-
ments of interest are often described using simple
DTDs. E.g., in an online books store, each book
has a title, one or more authors and price. This in-
formation can be described in a DTD as hbooki
htitleihauthori + hpricei. However, as shown in Ex-
ample 1, regexes for information extraction rely on
more complex constructs.
In the context of information extraction, prior
work has concentrated primarily on learning regexes
over relatively small alphabet sizes. A common
theme in (Soderland, 1999; Ciravegna, 2001; Wu
and Pottenger, 2005; Feldman et al., 2006) is the
problem of learning regexes over tagged tokens
produced by other text-processing steps such as
POS tagging, morphological analysis, and gazetteer
matching. Thus, the alphabet is defined by the space
of possible tags output by these analysis steps. A
similar approach has been proposed in (Brill, 2000)
for POS disambiguation. In contrast, our paper ad-
dresses extraction tasks that require fine-grained
control to accurately capture the structural features
of the entity of interest. Consequently, the domain
of interest consists of all characters thereby dramat-
ically increasing the size of the alphabet. To enable
this scale-up, the techniques presented in this paper
exploit advanced syntactic constructs (such as char-
acter classes and quantifiers) supported by modern
regex languages.
Finally, we note that almost all of the above de-
scribed work define the learning problem over a
restricted class of regexes. Typically, the restric-
tions involve either disallowing or limiting the use of
Kleene disclosure and disjunction operations. How-
ever, our work imposes no such restrictions.
</bodyText>
<subsectionHeader confidence="0.984759">
1.2 Contributions
</subsectionHeader>
<bodyText confidence="0.995235870967742">
In a key departure from prior formulations, the
learning algorithm presented in this work takes as
input not just labeled examples but also an initial
regular expression. The use of an initial regex has
two major advantages. First, this expression pro-
vides a natural mechanism for a domain expert to
provide domain knowledge about the structure of the
entity being extracted. Second, as we show in Sec-
tion 2, the space of output regular expressions un-
der consideration can be meaningfully restricted by
appropriately defining their relationship to the input
expression. Such a principled approach to restrict
the search space permits the learning algorithm to
consider complex regexes in a tractable manner. In
contrast, prior work defined a tractable search space
by placing restrictions on the target class of regular
expressions. Our specific contributions are:
A novel regex learning problem consisting of learn-
ing an improved regex given an initial regex and
labeled examples
Formulation of this learning task as an optimization
problem over a search space of regexes
ReLIE, a regex learning algorithm that employs
transformations to navigate the search space
Extensive experimental results over multiple
datasets to show the effectiveness of ReLIE and
a comparison study with the Conditional Random
Field (CRF) algorithm
Finally, experiments that demonstrate the benefits
of using ReLIE as a feature extractor for CRF and
possibly other machine learning algorithms.
</bodyText>
<page confidence="0.995486">
22
</page>
<sectionHeader confidence="0.574711" genericHeader="method">
\x0c2 The Regex Learning Problem
</sectionHeader>
<bodyText confidence="0.960168888888889">
Consider the task of identifying instances of some
entity E. Let R0 denote the input regex provided by
the user and let M(R0 , D) denote the set of matches
obtained by evaluating R0 over a document col-
lection D. Let Mp(R0 , D) = {x M(R0 , D) :
x instance of E} and Mn(R0 , D) = {x M(R0 , D) :
x not an instance of E} denote the set of positive and
negative matches for R0 . Note that a match is pos-
itive if it corresponds to an instance of the entity of
interest and is negative otherwise. The goal of our
learning task is to produce a regex that is better
than R0 at identifying instances of E.
Given a candidate regex R, we need a mechanism
to judge whether R is indeed a better extractor for
E than R0 . To make this judgment even for just the
original document collection D, we must be able to
label each instance matched by R (i.e., each element
of M(R, D)) as positive or negative. Clearly, this
can be accomplished if the set of matches produced
by R are contained within the set of available labeled
examples, i.e., if M(R, D) M(R0 , D). Based on
this observation, we make the following assumption:
Assumption 1. Given an input regex R0 over some al-
phabet , any other regex R over is a candidate for our
learning algorithm only if L(R) L(R0 ). (L(R) denotes
the language accepted by R).
Even with this assumption, we are left with a po-
tentially infinite set of candidate regexes from which
our learning algorithm must choose one. To explore
this set in a principled fashion, we need a mecha-
nism to move from one element in this space to an-
other, i.e., from one candidate regex to another. In
addition, we need an objective function to judge the
extraction quality of each candidate regex. We ad-
dress these two issues below.
Regex Transformations To systematically ex-
plore the search space, we introduce the concept of
regex transformations.
Definition 1 (Regex Transformation). Let R denote
the set of all regular expressions over some alphabet . A
regex transformation is a function T : R 2R
such
that R0
T (R), L(R0
) L(R).
For example, by replacing different occurrences
of the quantifier + in R1 from Example 1 with
specific ranges (such as {1,2} or {3}), we obtain
expressions such as R3 = (\\d+\\-){1,2}\\d+ and
R4 = (\\d{3}\\-)+\\d+. The operation of replacing
quantifiers with restricted ranges is an example of a
particular class of transformations that we describe
further in Section 3. For the present, it is sufficient
to view a transformation as a function applied to a
regex R that produces, as output, a set of regexes that
accept sublanguages of L(R). We now define the
search space of our learning algorithm as follows:
Definition 2 (Search Space). Given an input regex R0
and a set of transformations T , the search space of our
learning algorithm is T (R0 ), the set of all regexes ob-
tained by (repeatedly) applying the transformations in T
to R0 .
For instance, if the operation of restricting quanti-
fiers that we described above is part of the transfor-
mation set, then R3 and R4 are in the search space
of our algorithm, given R1 as input.
Objective Function We now define an objective
function, based on the well known F-measure, to
compare the extraction quality of different candidate
regexes in our search space. Using Mp(R, D) (resp.
Mn(R, D)) to denote the set of positive (resp. nega-
tive) matches of a regex R, we define
</bodyText>
<equation confidence="0.996840111111111">
precision(R, D) =
Mp(R, D)
Mp(R, D) + Mn(R, D)
recall(R, D) =
Mp(R, D)
Mp(R0, D)
F(R, D) =
2 precision(R, D) recall(R, D)
precision(R, D) + recall(R, D)
</equation>
<bodyText confidence="0.99421625">
The regex learning task addressed in this paper
can now be formally stated as the following opti-
mization problem:
Definition 3 (Regex Learning Problem). Given
an input regex R0 , a document collection D, labeled
sets of positive and negative examples Mp(R0 ,D) and
Mn(R0 ,D), and a set of transformations T , compute the
output regex Rf = argmaxRT (R0 ) F(R, D).
</bodyText>
<sectionHeader confidence="0.987218" genericHeader="method">
3 Instantiating Regex Transformations
</sectionHeader>
<bodyText confidence="0.990841666666667">
In this section, we describe how transformations
can be implemented by exploiting the syntactic con-
structs of modern regex engines. To help with our
description, we introduce the following task:
Example 2 (Software name extraction). Consider the
task of identifying names of software products in text.
A simple pattern for this task is: one or more capital-
ized words followed by a version number, represented
as R5 = ([A-Z]\\w*\\s*)+[Vv]?(\\d+\\.?)+.
</bodyText>
<page confidence="0.997431">
23
</page>
<bodyText confidence="0.97380114893617">
\x0cWhen applied to a collection of University web
pages, we discovered that R5 identified correct in-
stances such as Netscape 2.0, Windows 2000 and
Installation Designer v1.1. However, R5 also ex-
tracted incorrect instances such as course numbers
(e.g. ENGLISH 317), room numbers (e.g. Room
330), and section headings (e.g. Chapter 2.2). To
eliminate spurious matches such as ENGLISH 317,
let us enforce the condition that each word is a
single upper-case letter followed by one or more
lower-case letters. To accomplish this, we focus
on the sub-expression of R5 that identifies capital-
ized words, R51 = ([A-Z]\\w*\\s*)+, and replace it
with R51a = ([A-Z][a-z]*\\s*)+. The regex result-
ing from R5 by replacing R51 with R51a will avoid
matches such as ENGLISH 317.
An alternate way to improve R5 is by explicitly
disallowing matches against strings like ENGLISH,
Room and Chapter. To accomplish this, we can
exploit the negative lookahead operator supported
in modern regex engines. Lookaheads are special
constructs that allow a sequence of characters
to be checked for matches against a regex with-
out the characters themselves being part of the
match. As an example, (?!Ra)Rb (?! being
the negative lookahead operator) returns matches
of regex Rb but only if they do not match Ra.
Thus, by replacing R51 in our original regex with
R51b
=(?! ENGLISH|Room|Chapter)[A-Z]\\w*\\s*,
we produce an improved regex for software names.
The above examples illustrate the general prin-
ciple of our transformation technique. In essence,
we isolate a sub-expression of a given regex R and
modify it such that the resulting regex accepts a sub-
language of R. We consider two kinds of modifica-
tions drop-disjunct and include-intersect. In drop-
disjunct, we operate on a sub-expression that corre-
sponds to a disjunct and drop one or more operands
of that disjunct. In include-intersect, we restrict the
chosen sub-expression by intersecting it with some
other regex. Formally,
Definition 4 (Drop-disjunct Transformation). Let
R R be a regex of the form R = Ra(X)Rb,
where (X) denotes the disjunction R1|R2 |. . . |Rn of
any non-empty set of regexes X = {R1, R2, . . . , Rn}.
The drop-disjunct transformation DD(R, X, Y ) for some
</bodyText>
<equation confidence="0.261278333333333">
Y X, Y 6= results in the new regex Ra(Y )Rb.
Definition 5 (Include-Intersect Transformation). Let
.
\\W \\s \\w
[a-zA-Z] \\d|[0-9] _
[a-z] [A-Z]
</equation>
<figureCaption confidence="0.975682">
Figure 1: Sample Character Classes in Regex
</figureCaption>
<construct confidence="0.54206175">
R R be a regex of the form R = RaXRb for some
X R, X 6= . The include-intersect transformation
II(R, X, Y ) for some Y R, Y 6= results in the new
regex Ra(X Y )Rb.
</construct>
<bodyText confidence="0.982520416666667">
We state the following proposition (proof omit-
ted in the interest of space) that guarantees that both
drop-disjunct and include-intersect restrict the lan-
guage of the resulting regex, and therefore are valid
transformations according to Definition 1.
Proposition 1. Given regexes R, X1, Y1, X2 and Y2
from R such that DD(R, X1, Y1) and II(R, X2, Y2)
are applicable, L(DD(R, X1, Y1)) L(R) and
L(II(R, X2, Y2)) L(R).
We now proceed to describe how we use differ-
ent syntactic constructs to apply drop-disjunct and
include-intersect transformations.
</bodyText>
<subsectionHeader confidence="0.735979">
Character Class Restrictions Character
</subsectionHeader>
<bodyText confidence="0.994136363636364">
classes are short-hand notations for denoting
the disjunction of a set of characters (\\d is
equivalent to (0|1...|9); \\w is equivalent to
(a|. . .|z|A|. . .|Z|0|1. . .|9 |); etc.).2 Figure 1
illustrates a character class hierarchy in which
each node is a stricter class than its parent (e.g.,
\\d is stricter than \\w). A replacement of any of
these character classes by one of its descendants
is an instance of the drop-disjunct transformation.
Notice that in Example 2, when replacing R51 with
R51a , we were in effect applying a character class
restriction.
Quantifier Restrictions Quantifiers are used to
define the range of valid counts of a repetitive se-
quence. For instance, a{m,n} looks for a sequence
of as of length at least m and at most n. Since
quantifiers are also disjuncts (e.g., a{1,3} is equiv-
alent to a|aa|aaa), the replacement of an expres-
sion R{m, n} with an expression R{m1, n1} (m
m1 n1 n) is an instance of the drop-disjunct
transformation. For example, given a subexpres-
sion of the form a{1,3}, we can replace it with
</bodyText>
<page confidence="0.91922">
2
</page>
<bodyText confidence="0.968442">
Note that there are two distinct character classes \\W and \\w
</bodyText>
<page confidence="0.980811">
24
</page>
<bodyText confidence="0.941840488888889">
\x0cone of a{1,1}, a{1,2}, a{2,2}, a{2,3}, or a{3,3}.
Note that, before applying this transformation, wild-
card expressions such as a+ and a* are replaced by
a{0,maxCount} and a{1,maxCount} respectively,
where maxCount is a user configured maximum
length for the entity being extracted.
Negative Dictionaries Observe that the include-
intersect transformation (Definition 5) is applicable
for every possible sub-expression of a given regex
R. Note that a valid sub-expression in R is any
portion of R where a capturing group can be intro-
duced.3 Consider a regex R = RaXRb with a sub-
expression X; the application of include-intersect
requires another regex Y to yield Ra(X Y )Rb. We
would like to construct Y such that Ra(X Y )Rb is
better than R for the task at hand. Therefore, we
construct Y as Y 0 where Y 0 is a regex constructed
from negative matches of R. Specifically, we look at
each negative match of R and identify the substring
of the match that corresponds to X. We then apply
a greedy heuristic (see below) to these substrings to
yield a negative dictionary Y 0. Finally, the trans-
formed regex Ra(X Y 0)Rb is implemented using
the negative lookahead expression Ra(?! Y)XRb.
Greedy Heuristic for Negative Dictionaries Im-
plementation of the above procedure requires cer-
tain judicious choices in the construction of the neg-
ative dictionary to ensure tractability of this trans-
formation. Let S(X) denote the distinct strings
that correspond to the sub-expression X in the neg-
ative matches of R.4 Since any subset of S(X)
is a candidate negative dictionary, we are left with
an exponential number of possible transformations.
In our implementation, we used a greedy heuris-
tic to pick a single negative dictionary consisting
of all those elements of S(X) that individually
improve the F-measure. For instance, in Exam-
ple 2, if the independent substitution of R51 with
(?!ENGLISH)[A-Z]\\w*\\s*, (?!Room)[A-Z]
\\w*\\s*, and (?!Chapter)[A-Z]\\w*\\s* each im-
proves the F-measure, we produce a nega-
tive dictionary consisting of ENGLISH, Room, and
Chapter. This is precisely how the disjunct
ENGLISH|Room|Chapter is constructed in R51b
.
</bodyText>
<page confidence="0.512273">
3
</page>
<bodyText confidence="0.4837045">
For instance, the sub-expressions of ab{1,2}c are a,
ab{1,2}, ab{1,2}c, b, b{1,2}, b{1,2}c, and c.
</bodyText>
<page confidence="0.976409">
4
</page>
<bodyText confidence="0.9972865">
S(X) can be obtained automatically by identifying the sub-
string corresponding to the group X in each entry in Mn(R,D)
</bodyText>
<subsubsectionHeader confidence="0.856802">
Procedure ReLIE(Mtr,Mval,R0 ,T )
</subsubsectionHeader>
<listItem confidence="0.786819166666667">
// Mtr: set of labeled matches used as training data
// Mval: set of labeled matches used as validation data
// R0 : user-provided regular expression
// T : set of transformations
begin
1. Rnew = R0
2. do {
3. for each transformation ti T
4. Candidatei=ApplyTransformations(Rnew, ti)
5. let Candidates =
S
i Candidatei
6. let R0 = argmaxRCandidates F(R, Mtr)
7. if (F(R0, Mtr) &lt;= F(Rnew, Mtr)) return Rnew
8. if (F(R0, Mval) &lt; F(Rnew, Mval)) return Rnew
9. Rnew = R0
10. } while(true)
end
</listItem>
<figureCaption confidence="0.962856">
Figure 2: ReLIE Search Algorithm
</figureCaption>
<sectionHeader confidence="0.364328" genericHeader="method">
4 ReLIE Search Algorithm
</sectionHeader>
<bodyText confidence="0.873922594594595">
Figure 2 describes the ReLIE algorithm for the
Regex Learning Problem (Definition 3) based on the
transformations described in Section 3. ReLIE is a
greedy hill climbing search procedure that chooses,
at every iteration, the regex with the highest F-
measure. An iteration in ReLIE consists of:
Applying every transformation on the current regex
Rnew to obtain a set of candidate regexes
From the candidates, choosing the regex R0 whose
F-measure over the training dataset is maximum
To avoid overfitting, ReLIE terminates when either
of the following conditions is true: (i) there is no
improvement in F-measure over the training set;
(ii) there is a drop in F-measure when applying R0
on the validation set.
The following proposition provides an upper
bound for the running time of the ReLIE algorithm.
Proposition 2. Given any valid set of inputs Mtr,
Mval, R0 , and T , the ReLIE algorithm terminates in at
most  |Mn(R0 , Mtr ) |iterations. The running time of the
algorithm TT otal(R0 , Mtr , Mval )  |Mn(R0 , Mtr )|
t0 , where t0 is the time taken for the first iteration of the
algorithm.
Proof. With reference to Figure 2, in each iteration, the
F-measure of the best regex R0
is strictly better than
Rnew. Since L(R0
) L(Rnew), R0
eliminates at least
one additional negative match compared to Rnew. Hence,
the maximum number of iterations is  |Mn(R0 , Mtr )|.
For a regular expression R, let ncc(R) and nq(R) de-
note, respectively, the number of character classes and
quantifiers in R. The maximum number of possible sub-
expressions in R is |R|2
, where |R |is the length of R.
Let MaxQ(R) denote the maximum number of ways in
</bodyText>
<page confidence="0.980105">
25
</page>
<bodyText confidence="0.992399066666667">
\x0cwhich a single quantifier appearing in R can be restricted
to a smaller range. Let Fcc denote the maximum fanout5
of the character class hierarchy. Let TReEval(D) denote
the average time taken to evaluate a regex over dataset D.
Let Ri denote the regex at the beginning of iteration
i. The number of candidate regexes obtained by applying
the three transformations is
NumRE(Ri, Mtr) ncc(Ri)Fcc+nq(Ri)MaxQ(Ri)+|Ri|2
The time taken to enumerate the character class and
quantifier restriction transformations is proportional to
the resulting number of candidate regexes. The time
taken for the negative dictionaries transformation is given
by the running time of the greedy heuristic (Section 3).
The total time taken to enumerate all candidate regexes is
given by (for some constant c)
</bodyText>
<equation confidence="0.951187666666667">
TEnum(Ri, Mtr) c (ncc(Ri) Fcc + nq(Ri) MaxQ(Ri)
+ |Ri|2
Mn(Ri, Mtr) TReEval(Mtr))
</equation>
<bodyText confidence="0.968649">
Choosing the best transformation involves evaluating
each candidate regex over the training and validation cor-
pus and the time taken for this step is
</bodyText>
<equation confidence="0.7597942">
TP ickBest(Ri, Mtr, Mval) = NumRE(Ri, Mtr)
(TReEval(Mtr) + TReEval(Mval))
The total time taken for an iteration can be written as
TI (Ri, Mtr, Mval) =TEnum(Ri, Mtr)
+ TP ickBest(Ri, Mtr, Mval)
</equation>
<bodyText confidence="0.879944">
It can be shown that the time taken in each iteration
decreases monotonically (details omitted in the interest of
space). Therefore, the total running time of the algorithm
is given by
</bodyText>
<equation confidence="0.99411925">
TT otal(R0 , Mtr , Mval ) =
X
TI (Ri, Mtr, Mval)
 |Mn(R0 , Mtr ) |t0 .
</equation>
<bodyText confidence="0.971915">
where t0 = TI(R0 , Mtr , Mval ) is the running time
of the first iteration of the algorithm.
</bodyText>
<sectionHeader confidence="0.999121" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9963585">
In this section, we present an empirical study of
the ReLIE algorithm using four extraction tasks over
three real-life data sets. The goal of this study is to
evaluate the effectiveness of ReLIE in learning com-
plex regexes and to investigate how it compares with
standard machine learning algorithms.
</bodyText>
<subsectionHeader confidence="0.990766">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.822422333333333">
Data Set The datasets used in our experiments are:
EWeb: A collection of 50,000 web pages crawled
from a corporate intranet.
</bodyText>
<page confidence="0.968843">
5
</page>
<bodyText confidence="0.973968318181818">
Fanout is the number of ways in which a character class
may be restricted as defined by the hierarchy (e.g. Figure 1).
AWeb: A set of 50,000 web pages obtained from
the publicly available University of Michigan Web
page collection (Li et al., 2006), including a sub-
collection of 10,000 pages (AWeb-S).
Email: A collection of 10,000 emails obtained
from the publicly available Enron email collec-
tion (Minkov et al., 2005).
Extraction Tasks SoftwareNameTask, CourseNum-
berTask and PhoneNumberTask were evaluated on
EWeb, AWeb and Email, respectively. Since web
pages have large number of URLs, to keep the la-
beling task manageable, URLTask was evaluated on
AWeb-S.
Gold Standard For each task, the gold standard
was created by manually labeling all matches for the
initial regex. Note that only exact matches with the
gold standard are considered correct in our evalua-
tions. 6
Comparison Study To evaluate ReLIE for entity
extraction vis-a-vis existing algorithms, we used the
popular conditional random field (CRF). Specifi-
cally, we used the MinorThird (Cohen, 2004) imple-
mentation of CRF to train models for all four extrac-
tion tasks. For training the CRF we provided it with
the set of positive and negative matches from the ini-
tial regex with a context of 200 characters on either
side of each match7. Since it is unlikely that useful
features are located far away from the entity, we be-
lieve that 200 characters on either side is sufficient
context. The CRF used the base features described
in (Cohen et al., 2005). To ensure fair compari-
son with ReLIE, we also included the matches corre-
sponding to the input regex as a feature to the CRF.
In practice, more complex features (e.g., dictionar-
ies, simple regexes) derived by domain experts are
often provided to CRFs. However, such features can
also be used to refine the initial regex given to ReLIE.
Hence, with a view to investigating the raw learn-
ing capability of the two approaches, we chose to
run all our experiments without any additional man-
ually derived features. In fact, the patterns learned
by ReLIE through transformations are often similar
</bodyText>
<page confidence="0.97105">
6
</page>
<bodyText confidence="0.894835">
The labeled data will be made publicly available at
http://www.eecs.umich.edu/db/regexLearning/.
</bodyText>
<page confidence="0.993869">
7
</page>
<bodyText confidence="0.983743666666667">
Ideally, we would have preferred to let MinorThird extract
appropriate features from complete documents in the training-
set but could not get it to load our large datasets.
</bodyText>
<page confidence="0.617449">
26
</page>
<figure confidence="0.998965520833333">
\x0c(a) SoftwareNameTask
0.5
0.6
0.7
0.8
0.9
1
10% 40% 80%
Percentage of Data Used for Training
F-Measure
ReLIE CRF
(b) CourseNumberTask
0.5
0.6
0.7
0.8
0.9
1
10% 40% 80%
Percentage of Data Used for Training
F-Measure
ReLIE CRF
(c) URLTask
0.5
0.6
0.7
0.8
0.9
1
10% 40% 80%
Percentage of Data Used for Training
F-M
e
a
s
ure
ReLIE CRF
(d) PhoneNumberTask
0.5
0.6
0.7
0.8
0.9
1
10% 40% 80%
Percentage of Data Used for Training
F-Measure
ReLIE CRF
</figure>
<figureCaption confidence="0.805527">
Figure 3: Extraction Qualitya
a
</figureCaption>
<bodyText confidence="0.9968749">
For SoftwareNameTask, with 80% training data we could not obtain results for CRF as the program
failed repeatedly during the training phase.
to the features that domain experts may provide to
CRF. We will revisit this issue in Section 5.4.
Evaluation We used the standard F-measure to
evaluate the effectiveness of ReLIE and CRF. We di-
vided each dataset into 10 equal parts and used X%
of the dataset for training (X=10, 40 and 80), 10%
for validation, and remaining (90-X)% for testing.
All results are reported on the test set.
</bodyText>
<subsectionHeader confidence="0.7585">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.997154">
Four extraction tasks were chosen to reflect the enti-
ties commonly present in the three datasets.
SoftwareNameTask: Extracting software names such
as Lotus Notes 8.0, Open Office Suite 2007.
CourseNumberTask: Extracting university course
numbers such as EECS 584, Pharm 101.
PhoneNumberTask: Extracting phone numbers such
as 1-800-COMCAST, (425)123 5678.
URLTask: Extracting URLs such as
http:\\\\www.abc.com and lsa.umich.edu/ foo/.8
This section summarizes the results of our empir-
ical evaluation comparing ReLIE and CRF.
</bodyText>
<page confidence="0.989343">
8
</page>
<bodyText confidence="0.963489">
URLTask may appear to be simplistic. However, extracting
URLs without the leading protocol definitions (e.g. http) can
be challenging.
</bodyText>
<subsectionHeader confidence="0.914831">
Raw Extraction Quality The cross-validated re-
</subsectionHeader>
<bodyText confidence="0.9995373">
sults across all four tasks are presented in Figure 3.
With 10% training data, ReLIE outperforms CRF
on three out of four tasks with a difference in F-
measure ranging from 0.1 to 0.2.
As training data increases, both algorithms perform
better with the gap between the two reducing for
all the four tasks. For CourseNumberTask and URL-
Task, CRF does slightly better than ReLIE for larger
training dataset. For the other two tasks, ReLIE re-
tains its advantage over CRF.9
The above results indicate that ReLIE performs
comparably with CRF with a slight edge in condi-
tions of limited training data. Indeed, the capability
to learn high-quality extractors using a small train-
ing set is important because labeled data is often ex-
pensive to obtain. For precisely this same reason, we
would ideally like to learn the extractors once and
then apply them to other datasets as needed. Since
these other datasets may be from a different domain,
we next performed a cross-domain test (i.e., training
</bodyText>
<page confidence="0.978286">
9
</page>
<bodyText confidence="0.993434666666667">
For SoftwareNameTask, with 80% training data we could
not obtain results for CRF as the program failed repeatedly dur-
ing the training phase.
</bodyText>
<page confidence="0.995468">
27
</page>
<table confidence="0.982092142857143">
\x0cand testing on different domains).
Task(Training, Testing)
Data for Training 10% 40% 80%
ReLIE CRF ReLIE CRF ReLIE CRF
SoftwareNameTask(EWeb,AWeb) 0.920 0.297 0.977 0.503 0.971 N/A
URLTask(AWeb-S,Email) 0.690 0.209 0.784 0.380 0.801 0.507
PhoneNumberTask(Email,AWeb) 0.357 0.130 0.475 0.125 0.513 0.120
</table>
<tableCaption confidence="0.889936">
Table 1: Cross Domain Test (F-measure).
</tableCaption>
<table confidence="0.977099375">
Technique
SoftwareNameTask CourseNumberTask URLTask PhoneNumberTask
training testing training testing training testing training testing
ReLIE 511.7 20.6 69.3 18.4 73.8 7.7 39.4 1.1
CRF 7597.0 2315.8 482.5 75.4 438.7 53.8 434.8 57.7
t(ReLIE)
t(CRF)
0.067 0.009 0.144 0.244 0.168 0.143 0.091 0.019
</table>
<tableCaption confidence="0.677781">
Table 2: Average Training/Testing Time (sec)(with 40% data for training)
</tableCaption>
<table confidence="0.9919514">
Task(Extra Feature)
Data for Training 10% 40% 80%
CRF C+RL CRF C+RL CRF C+RL
CourseNumberTask(Negative Dictionary) 0.553 0.624 0.644 0.764 0.854 0.845
PhoneNumberTask(Quantifier) 0.695 0.893 0.820 0.937 0.821 0.964
</table>
<tableCaption confidence="0.854918">
Table 3: ReLIE as Feature Extractor (C+RL is CRF enhanced with
features learned by ReLIE).
</tableCaption>
<subsubsectionHeader confidence="0.359826">
Cross-domain Evaluation Table 1
</subsubsectionHeader>
<bodyText confidence="0.994283388888889">
summarizes the results of training
the algorithms on one data set and
testing on another. The scenarios
chosen are: (i) SoftwareNameTask
trained on EWeb and tested on
AWeb, (ii) URLTask trained on AWeb
and tested on Email, and (iii) Pho-
neNumberTask trained on Email
and tested on AWeb.10 We can
see that ReLIE significantly out-
performs CRF for all three tasks,
even when provided with a large
training dataset. Compared to test-
ing on the same dataset, there is a
reduction in F-measure (less than
0.1 in many cases) when the regex
learned by ReLIE is applied to a dif-
ferent dataset, while the drop for
CRF is much more significant (over 0.5 in many
cases).11
Training Time Another issue of practical consid-
eration is the efficiency of the learning algorithm.
Table 2 reports the average training and testing time
for both algorithms on the four tasks. On average Re-
LIE is an order of magnitude faster than CRF in both
building the model and applying the learnt model.
Robustness to Variations in Input Regexes The
transformations done by ReLIE are based on the
structure of the input regex. Therefore given differ-
ent input regexes, the final regexes learned by ReLIE
will be different. To evaluate the impact of the struc-
ture of the input regex on the quality of the regex
learned by ReLIE, we started with different regexes12
for the same task. We found that ReLIE is robust
to variations in input regexes. For instance, on Soft-
wareNameTask, the standard deviation in F-measure
</bodyText>
<page confidence="0.987764">
10
</page>
<bodyText confidence="0.999866">
We do not report results for CourseNumberTask as course
numbers are specific to academic webpages and do not appear
in the other two domains
</bodyText>
<page confidence="0.996271">
11
</page>
<bodyText confidence="0.993719333333333">
Similar cross-domain performance deterioration for a ma-
chine learning approach has been observed by (Guo et al.,
2006).
</bodyText>
<page confidence="0.986969">
12
</page>
<bodyText confidence="0.988393">
Recall that the search space of ReLIE is limited by L(R0)
(Assumption 1). Thus to ensure meaningful comparison, for
the same task any two given input regexes R0 and R0
</bodyText>
<figure confidence="0.528398">
0 are cho-
sen in such a way that although their structures are different,
Mp(R0, D) = Mp(R0
0, D) and Mn(R0, D) = Mn(R0
0, D).
</figure>
<bodyText confidence="0.997943666666667">
of the final regexes generated from six different in-
put regexes was less than 0.05. Further details of this
experiment are omitted in the interest of space.
</bodyText>
<subsectionHeader confidence="0.978731">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.995030380952381">
The results of our comparison study (Figure 3) in-
dicates that for raw extraction quality ReLIE has a
slight edge over CRF for small training data. How-
ever, in cross-domain performance (Table 1) ReLIE
is significantly better than CRF (by 0.41 on aver-
age) . To understand this discrepancy, we examined
the final regex learned by ReLIE and compared that
with the features learned by CRF. Examples of ini-
tial regexes with corresponding final regexes learnt
by ReLIE with 10% training data are listed in Ta-
ble 4. Recall, from Section 3, that ReLIE transfor-
mations include character class restrictions, quanti-
fier restrictions and addition of negative dictionar-
ies. For instance, in the SoftwareNameTask, the final
regex listed was obtained by restricting [a-zA-Z]
to [a-z], \\w to [a-zA-Z], and adding the nega-
tive dictionary (Copyright|Fall ||Issue). Sim-
ilarly, for the PhoneNumberTask, the final regex
involved two negative dictionaries (expressed as
(?![,]) and (?![,:])) 13 and quantifier restric-
tions (e.g. the first [A-Z\\d]{2,4} was transformed
</bodyText>
<page confidence="0.995544">
13
</page>
<bodyText confidence="0.99839675">
To obtain these negative dictionaries, ReLIE not only
needs to correctly identify the dictionary entries from negative
matches but also has to place the corresponding negative looka-
head expression at the appropriate place in the regex.
</bodyText>
<page confidence="0.743992">
28
</page>
<figure confidence="0.998020555555555">
\x0cSoftwareNameTask
R0 \\b([A-Z][a-zA-Z]{1,10}\\s){1,5}\\s*(\\w{0,2}\\d[\\.]?){1,4}\\b
Rfinal
\\b((?!(Copyright|Page|Physics|Question ||Article|Issue))[A-Z][a-z]{1,10}
\\s){1,5}\\s*([a-zA-Z]{0,2}\\d[\\.]?){1,4}\\b
PhoneNumberTask
R0 \\b(1\\W+)?\\W?\\d{3,3}\\W*\\s*\\W?[A-Z\\d]{2,4}\\s*\\W?[A-Z\\d]{2,4}\\b
Rfinal \\b(1\\W+)?\\W?\\d{3,3}((?![,])\\W*)\\s*\\W?[A-Z\\d]{3,3}\\s*((?![,:])\\W?)[A-Z\\d]{3,4}\\b
CourseNumberTask
R0 \\b([A-Z][a-zA-Z]+)\\s+\\d{3,3}\\b
Rfinal \\b(((?!(At|Between |Contact|Some|Suite|Volume))[A-Z][a-zA-Z]+))\\s+\\d{3,3}\\b
URLTask
R0 \\b(\\w+://)?(\\w+\\.){0,2}\\w+\\.\\w+(/[
\\s]+){0,20}\\b
Rfinal
\\b((?!(Response 20010702 1607.csv |))((\\w+://)?(\\w+\\.){0,2}\\w+\\.(?!(ppt
 |doc))[a-zA-Z]{2,3}))(/[
\\s]+){0,20}\\b
</figure>
<tableCaption confidence="0.83548">
Table 4: Sample Regular Expressions Learned by ReLIE(R0: input regex; Rfinal: final regex learned; the parts of R0
modified by ReLIE and the corresponding parts in Rfinal are highlighted.)
</tableCaption>
<bodyText confidence="0.989098607843137">
into [A-Z\\d]{3,3}).
After examining the features learnt by CRF, it was
clear that while CRF could learn features such as the
negative dictionary it is unable to learn character-
level features. This should not be surprising since
our CRF was trained with primarily tokens as fea-
tures (cf. Section 5.1). While this limitation was less
of a factor in experiments involving data from the
same domain (some effects were seen with smaller
training data), it does explain the significant differ-
ence between the two algorithms in cross-domain
tasks where the vocabulary can be significantly dif-
ferent. Indeed, in practical usage of CRF, the main
challenge is to come up with additional complex fea-
tures (often in the form of dictionary and regex pat-
terns) that need to be given to the CRF (Minkov et
al., 2005). Such complex features are largely hand-
crafted and thus expensive to obtain. Since the Re-
LIE transformations are operations over characters,
a natural question to ask is: Can the regex learned
by ReLIE be used to provide features to CRF? We
answer this question below.
5.4 ReLIE as Feature Extractor for CRF
To understand the effect of incorporating ReLIE-
identified features into CRF, we chose the two tasks
(CourseNumberTask and PhoneNumberTask) with the
least F-measure in our experiments to determine raw
extraction quality. We examined the final regex pro-
duced by ReLIE and manually extracted portions
to serve as features. For example, the negative
dictionary learned by ReLIE for the CourseNumber-
Task (At|Between ||Volume) was incorporated as
a feature into CRF. To help isolate the effects, for
each task, we only incorporated features correspond-
ing to a single transformation: negative dictionar-
ies for CourseNumberTask and quantifier restrictions
for PhoneNumberTask. The results of these experi-
ments are shown in Table 3. The first point worthy of
note is that performance has improved in all but one
case. Second, despite the F-measure on CourseNum-
berTask being lower than PhoneNumberTask (presum-
ably more potential for improvement), the improve-
ments on PhoneNumberTask are significantly higher.
This observation is consistent with our conjecture
in Section 5.1 that CRF learns token-level features;
therefore incorporating negative dictionaries as extra
feature provides only limited improvement. Admit-
tedly more experiments are needed to understand the
full impact of incorporating ReLIE-identified fea-
tures into CRF. However, we do believe that this is
an exciting direction of future research.
</bodyText>
<sectionHeader confidence="0.995936" genericHeader="conclusions">
6 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.999900333333333">
We proposed a novel formulation of the problem of
learning complex character-level regexes for entity
extraction tasks. We introduced the concept of regex
transformations and described how these could be
realized using the syntactic constructs of modern
regex languages. We presented ReLIE, a powerful
regex learning algorithm that exploits these ideas.
Our experiments demonstrate that ReLIE is very ef-
fective for certain classes of entity extraction, partic-
ularly under conditions of cross-domain and limited
training data. Our preliminary results also indicate
the possibility of using ReLIE as a powerful feature
extractor for CRF and other machine learning algo-
rithms. Further investigation of this aspect of ReLIE
presents an interesting avenue of future work.
</bodyText>
<sectionHeader confidence="0.978335" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98954475">
We thank the anonymous reviewers for their insight-
ful and constructive comments and suggestions. We
are also grateful for comments from David Gondek
and Sebastian Blohm.
</bodyText>
<page confidence="0.994591">
29
</page>
<reference confidence="0.996665797468354">
\x0cReferences
R. Alquezar and A. Sanfeliu. 1994. Incremental gram-
matical inference from positive and negative data using
unbiased finite state automata. In SSPR.
Douglas E. Appelt and Boyan Onyshkevych. 1998. The
common pattern specification language. In TIPSTER
TEXT PROGRAM.
Geert Jan Bex et al. 2006. Inference of concise DTDs
from XML data. In VLDB.
Eric Brill. 2000. Pattern-based disambiguation for natu-
ral language processing. In SIGDAT.
William W. Cohen and Andrew McCallum. 2003. Infor-
mation Extraction from the World Wide Web. in KDD
William W. Cohen. 2004. Minorthird: Methods for
identifying names and ontological relations in text
using heuristics for inducing regularities from data.
http://minorthird.sourceforge.net.
William W. Cohen et al. 2005. Learning to Understand
Web Site Update Requests. In IJCAI.
Fabio Ciravegna. 2001. Adaptive information extraction
from text by rule induction and generalization. In IJ-
CAI.
H. Cunningham. 1999. JAPE a java annotation patterns
engine.
Francois Denis et al. 2004. Learning regular languages
using RFSAs. Theor. Comput. Sci., 313(2):267294.
Francois Denis. 2001. Learning regular languages
from simple positive examples. Machine Learning,
44(1/2):3766.
Pedro DeRose et al. 2007. DBLife: A Community In-
formation Management Platform for the Database Re-
search Community In CIDR
Pierre Dupont. 1996. Incremental regular inference. In
ICGI.
Ronen Feldman et all. 2006. Self-supervised Relation
Extraction from the Web. In ISMIS.
Henning Fernau. 2005. Algorithms for learning regular
expressions. In ALT.
Laura Firoiu et al. 1998. Learning regular languages
from positive evidence. In CogSci.
K. Fukuda et al. 1998. Toward information extraction:
identifying protein names from biological papers. Pac
Symp Biocomput., 1998:707718
Ugo Galassi and Attilio Giordana. 2005. Learning regu-
lar expressions from noisy sequences. In SARA.
Minos Garofalakis et al. 2000. XTRACT: a system for
extracting document type descriptors from XML doc-
uments. In SIGMOD.
Hong Lei Guo et al. 2006. Empirical Study on the
Performance Stability of Named Entity Recognition
Model across Domains In EMNLP.
Java Regular Expressions. 2008. http://java.sun.com
/javase/6/docs/api/java/util/regex/package-
summary.html.
Dan Klein et al. 2003. Named Entity Recognition with
Character-Level Models. In HLT-NAACL.
Vijay Krishnan and Christopher D. Manning. 2006. An
Effective Two-Stage Model for Exploiting Non-Local
Dependencies in Named Entity Recognition. In ACL.
Yunyao Li et al. 2006. Getting work done on the web:
Supporting transactional queries. In SIGIR.
Andrew McCallum et al. 2000. Maximum Entropy
Markov Models for Information Extraction and Seg-
mentation. In ICML.
Einat Minkov et al. 2005. Extracting personal names
from emails: Applying named entity recognition to in-
formal text. In HLT/EMNLP.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34:233272.
Lorraine Tanabe and W. John Wilbur 2002. Tagging
gene and protein names in biomedical text. Bioinfor-
matics, 18:11241132.
Tianhao Wu and William M. Pottenger. 2005. A semi-
supervised active learning algorithm for information
extraction from textual data. JASIST, 56(3):258271.
Huaiyu Zhu, Alexander Loeser, Sriram Raghavan, Shiv-
akumar Vaithyanathan 2007. Navigating the intranet
with high precision. In WWW.
</reference>
<page confidence="0.918535">
30
</page>
<figure confidence="0.295304">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.429475">
<note confidence="0.862155666666667">b&amp;apos;Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 2130, Honolulu, October 2008. c 2008 Association for Computational Linguistics</note>
<title confidence="0.992823">Regular Expression Learning for Information Extraction</title>
<author confidence="0.999205">Yunyao Li</author>
<author confidence="0.999205">Rajasekar Krishnamurthy</author>
<author confidence="0.999205">Sriram Raghavan</author>
<author confidence="0.999205">Shivakumar Vaithyanathan</author>
<affiliation confidence="0.999267">IBM Almaden Research Center</affiliation>
<address confidence="0.999867">San Jose, CA 95120</address>
<email confidence="0.997122">yunyaoli@us.ibm.com,shiv@almaden.ibm.com</email>
<email confidence="0.997122">rajase@us.ibm.com,shiv@almaden.ibm.com</email>
<email confidence="0.997122">rsriram@us.ibm.com,shiv@almaden.ibm.com</email>
<author confidence="0.999826">H V Jagadish</author>
<affiliation confidence="0.9999615">Department of EECS University of Michigan</affiliation>
<address confidence="0.997246">Ann Arbor, MI 48109</address>
<email confidence="0.999814">jag@umich.edu</email>
<abstract confidence="0.997202529411765">Regular expressions have served as the dominant workhorse of practical information extraction for several years. However, there has been little work on reducing the manual effort involved in building high-quality, complex regular expressions for information extraction tasks. In this paper, we propose Re- LIE, a novel transformation-based algorithm for learning such complex regular expressions. We evaluate the performance of our algorithm on multiple datasets and compare it against the CRF algorithm. We show that ReLIE, in addition to being an order of magnitude faster, outperforms CRF under conditions of limited training data and cross-domain data. Finally, we show how the accuracy of CRF can be im-</abstract>
<note confidence="0.651653">proved by using features extracted by ReLIE.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences R Alquezar</author>
<author>A Sanfeliu</author>
</authors>
<title>Incremental grammatical inference from positive and negative data using unbiased finite state automata.</title>
<date>1994</date>
<booktitle>In SSPR.</booktitle>
<contexts>
<context position="3954" citStr="Alquezar and Sanfeliu, 1994" startWordPosition="588" endWordPosition="591">lem of learn1 Throughout this paper, we use the syntax of the standard Java regex engine (Java, 2008). 21 \x0cing regexes for information extraction tasks. We demonstrate that high quality regex extractors can be learned with significantly reduced manual effort. To motivate our approach, we first discuss prior work in the area of learning regexes and describe some of the limitations of these techniques. 1.1 Learning Regular Expressions The problem of inducing regular languages from positive and negative examples has been studied in the past, even outside the context of information extraction (Alquezar and Sanfeliu, 1994; Dupont, 1996; Firoiu et al., 1998; Garofalakis et al., 2000; Denis, 2001; Denis et al., 2004; Fernau, 2005; Galassi and Giordana, 2005; Bex et al., 2006). Much of this work assumes that the target regex is small and compact thereby allowing the learning algorithm to exploit this information. Consider, for example, the learning of patterns motivated by DNA sequencing applications (Galassi and Giordana, 2005). Here the input sequence is viewed as multiple atomic events separated by gaps. Since each atomic event is easily described by a small and compact regex, the problem reduces to one of lea</context>
</contexts>
<marker>Alquezar, Sanfeliu, 1994</marker>
<rawString>\x0cReferences R. Alquezar and A. Sanfeliu. 1994. Incremental grammatical inference from positive and negative data using unbiased finite state automata. In SSPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
<author>Boyan Onyshkevych</author>
</authors>
<title>The common pattern specification language. In</title>
<date>1998</date>
<tech>TIPSTER TEXT PROGRAM.</tech>
<contexts>
<context position="2960" citStr="Appelt and Onyshkevych, 1998" startWordPosition="428" endWordPosition="431">ecision and recall problems. Not only does R1 produce incorrect matches (e.g., social security numbers like 123-45-6789), it also fails to identify valid phone numbers such as 800.865.1125, and (800)865-CARE. An improved regex that addresses these problems is R2 = (\\d{3}[-.\\ ()]){1,2}[\\dA-Z]{4}. While multiple machine learning approaches have been proposed for information extraction in recent years (McCallum et al., 2000; Cohen and McCallum, 2003; Klein et al., 2003; Krishnan and Manning, 2006), manually created regexes remain a widely adopted practical solution for information extraction (Appelt and Onyshkevych, 1998; Fukuda et al., 1998; Cunningham, 1999; Tanabe and Wilbur, 2002; Li et al., 2006; DeRose et al., 2007; Zhu et al., 2007). Yet, with a few notable exceptions, which we discuss later in Section 1.1, there has been very little work in reducing this human effort through the use of automatic learning techniques. In this paper, we propose a novel formulation of the problem of learn1 Throughout this paper, we use the syntax of the standard Java regex engine (Java, 2008). 21 \x0cing regexes for information extraction tasks. We demonstrate that high quality regex extractors can be learned with signifi</context>
</contexts>
<marker>Appelt, Onyshkevych, 1998</marker>
<rawString>Douglas E. Appelt and Boyan Onyshkevych. 1998. The common pattern specification language. In TIPSTER TEXT PROGRAM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geert Jan Bex</author>
</authors>
<title>Inference of concise DTDs from XML data.</title>
<date>2006</date>
<booktitle>In VLDB.</booktitle>
<marker>Bex, 2006</marker>
<rawString>Geert Jan Bex et al. 2006. Inference of concise DTDs from XML data. In VLDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Pattern-based disambiguation for natural language processing.</title>
<date>2000</date>
<booktitle>In SIGDAT.</booktitle>
<contexts>
<context position="5574" citStr="Brill, 2000" startWordPosition="856" endWordPosition="857">egexes for information extraction rely on more complex constructs. In the context of information extraction, prior work has concentrated primarily on learning regexes over relatively small alphabet sizes. A common theme in (Soderland, 1999; Ciravegna, 2001; Wu and Pottenger, 2005; Feldman et al., 2006) is the problem of learning regexes over tagged tokens produced by other text-processing steps such as POS tagging, morphological analysis, and gazetteer matching. Thus, the alphabet is defined by the space of possible tags output by these analysis steps. A similar approach has been proposed in (Brill, 2000) for POS disambiguation. In contrast, our paper addresses extraction tasks that require fine-grained control to accurately capture the structural features of the entity of interest. Consequently, the domain of interest consists of all characters thereby dramatically increasing the size of the alphabet. To enable this scale-up, the techniques presented in this paper exploit advanced syntactic constructs (such as character classes and quantifiers) supported by modern regex languages. Finally, we note that almost all of the above described work define the learning problem over a restricted class </context>
</contexts>
<marker>Brill, 2000</marker>
<rawString>Eric Brill. 2000. Pattern-based disambiguation for natural language processing. In SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Andrew McCallum</author>
</authors>
<title>Information Extraction from the World Wide Web. in KDD</title>
<date>2003</date>
<note>http://minorthird.sourceforge.net.</note>
<contexts>
<context position="2785" citStr="Cohen and McCallum, 2003" startWordPosition="402" endWordPosition="406"> is blocks of digits separated by hyphens represented as R1 = (\\d+\\-)+\\d+.1 While R1 matches valid phone numbers like 800-865-1125 and 725-1234, it suffers from both precision and recall problems. Not only does R1 produce incorrect matches (e.g., social security numbers like 123-45-6789), it also fails to identify valid phone numbers such as 800.865.1125, and (800)865-CARE. An improved regex that addresses these problems is R2 = (\\d{3}[-.\\ ()]){1,2}[\\dA-Z]{4}. While multiple machine learning approaches have been proposed for information extraction in recent years (McCallum et al., 2000; Cohen and McCallum, 2003; Klein et al., 2003; Krishnan and Manning, 2006), manually created regexes remain a widely adopted practical solution for information extraction (Appelt and Onyshkevych, 1998; Fukuda et al., 1998; Cunningham, 1999; Tanabe and Wilbur, 2002; Li et al., 2006; DeRose et al., 2007; Zhu et al., 2007). Yet, with a few notable exceptions, which we discuss later in Section 1.1, there has been very little work in reducing this human effort through the use of automatic learning techniques. In this paper, we propose a novel formulation of the problem of learn1 Throughout this paper, we use the syntax of </context>
</contexts>
<marker>Cohen, McCallum, 2003</marker>
<rawString>William W. Cohen and Andrew McCallum. 2003. Information Extraction from the World Wide Web. in KDD William W. Cohen. 2004. Minorthird: Methods for identifying names and ontological relations in text using heuristics for inducing regularities from data. http://minorthird.sourceforge.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
</authors>
<title>Learning to Understand Web Site Update Requests.</title>
<date>2005</date>
<booktitle>In IJCAI.</booktitle>
<marker>Cohen, 2005</marker>
<rawString>William W. Cohen et al. 2005. Learning to Understand Web Site Update Requests. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Ciravegna</author>
</authors>
<title>Adaptive information extraction from text by rule induction and generalization.</title>
<date>2001</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="5218" citStr="Ciravegna, 2001" startWordPosition="800" endWordPosition="801">ence (Garofalakis et al., 2000; Bex et al., 2006), it is possible to exploit the fact that the XML documents of interest are often described using simple DTDs. E.g., in an online books store, each book has a title, one or more authors and price. This information can be described in a DTD as hbooki htitleihauthori + hpricei. However, as shown in Example 1, regexes for information extraction rely on more complex constructs. In the context of information extraction, prior work has concentrated primarily on learning regexes over relatively small alphabet sizes. A common theme in (Soderland, 1999; Ciravegna, 2001; Wu and Pottenger, 2005; Feldman et al., 2006) is the problem of learning regexes over tagged tokens produced by other text-processing steps such as POS tagging, morphological analysis, and gazetteer matching. Thus, the alphabet is defined by the space of possible tags output by these analysis steps. A similar approach has been proposed in (Brill, 2000) for POS disambiguation. In contrast, our paper addresses extraction tasks that require fine-grained control to accurately capture the structural features of the entity of interest. Consequently, the domain of interest consists of all character</context>
</contexts>
<marker>Ciravegna, 2001</marker>
<rawString>Fabio Ciravegna. 2001. Adaptive information extraction from text by rule induction and generalization. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
</authors>
<title>JAPE a java annotation patterns engine.</title>
<date>1999</date>
<contexts>
<context position="2999" citStr="Cunningham, 1999" startWordPosition="436" endWordPosition="437">ce incorrect matches (e.g., social security numbers like 123-45-6789), it also fails to identify valid phone numbers such as 800.865.1125, and (800)865-CARE. An improved regex that addresses these problems is R2 = (\\d{3}[-.\\ ()]){1,2}[\\dA-Z]{4}. While multiple machine learning approaches have been proposed for information extraction in recent years (McCallum et al., 2000; Cohen and McCallum, 2003; Klein et al., 2003; Krishnan and Manning, 2006), manually created regexes remain a widely adopted practical solution for information extraction (Appelt and Onyshkevych, 1998; Fukuda et al., 1998; Cunningham, 1999; Tanabe and Wilbur, 2002; Li et al., 2006; DeRose et al., 2007; Zhu et al., 2007). Yet, with a few notable exceptions, which we discuss later in Section 1.1, there has been very little work in reducing this human effort through the use of automatic learning techniques. In this paper, we propose a novel formulation of the problem of learn1 Throughout this paper, we use the syntax of the standard Java regex engine (Java, 2008). 21 \x0cing regexes for information extraction tasks. We demonstrate that high quality regex extractors can be learned with significantly reduced manual effort. To motiva</context>
</contexts>
<marker>Cunningham, 1999</marker>
<rawString>H. Cunningham. 1999. JAPE a java annotation patterns engine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francois Denis</author>
</authors>
<title>Learning regular languages using RFSAs.</title>
<date>2004</date>
<journal>Theor. Comput. Sci.,</journal>
<volume>313</volume>
<issue>2</issue>
<marker>Denis, 2004</marker>
<rawString>Francois Denis et al. 2004. Learning regular languages using RFSAs. Theor. Comput. Sci., 313(2):267294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francois Denis</author>
</authors>
<title>Learning regular languages from simple positive examples.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>44--1</pages>
<contexts>
<context position="4028" citStr="Denis, 2001" startWordPosition="602" endWordPosition="603">Java, 2008). 21 \x0cing regexes for information extraction tasks. We demonstrate that high quality regex extractors can be learned with significantly reduced manual effort. To motivate our approach, we first discuss prior work in the area of learning regexes and describe some of the limitations of these techniques. 1.1 Learning Regular Expressions The problem of inducing regular languages from positive and negative examples has been studied in the past, even outside the context of information extraction (Alquezar and Sanfeliu, 1994; Dupont, 1996; Firoiu et al., 1998; Garofalakis et al., 2000; Denis, 2001; Denis et al., 2004; Fernau, 2005; Galassi and Giordana, 2005; Bex et al., 2006). Much of this work assumes that the target regex is small and compact thereby allowing the learning algorithm to exploit this information. Consider, for example, the learning of patterns motivated by DNA sequencing applications (Galassi and Giordana, 2005). Here the input sequence is viewed as multiple atomic events separated by gaps. Since each atomic event is easily described by a small and compact regex, the problem reduces to one of learning simple regexes. Similarly, in XML DTD inference (Garofalakis et al.,</context>
</contexts>
<marker>Denis, 2001</marker>
<rawString>Francois Denis. 2001. Learning regular languages from simple positive examples. Machine Learning, 44(1/2):3766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro DeRose</author>
</authors>
<title>DBLife: A Community Information Management Platform for the Database Research Community In</title>
<date>2007</date>
<booktitle>CIDR Pierre Dupont.</booktitle>
<marker>DeRose, 2007</marker>
<rawString>Pedro DeRose et al. 2007. DBLife: A Community Information Management Platform for the Database Research Community In CIDR Pierre Dupont. 1996. Incremental regular inference. In ICGI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronen Feldman</author>
</authors>
<title>Self-supervised Relation Extraction from the Web. In ISMIS.</title>
<date>2006</date>
<marker>Feldman, 2006</marker>
<rawString>Ronen Feldman et all. 2006. Self-supervised Relation Extraction from the Web. In ISMIS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henning Fernau</author>
</authors>
<title>Algorithms for learning regular expressions.</title>
<date>2005</date>
<booktitle>In ALT.</booktitle>
<contexts>
<context position="4062" citStr="Fernau, 2005" startWordPosition="608" endWordPosition="609">or information extraction tasks. We demonstrate that high quality regex extractors can be learned with significantly reduced manual effort. To motivate our approach, we first discuss prior work in the area of learning regexes and describe some of the limitations of these techniques. 1.1 Learning Regular Expressions The problem of inducing regular languages from positive and negative examples has been studied in the past, even outside the context of information extraction (Alquezar and Sanfeliu, 1994; Dupont, 1996; Firoiu et al., 1998; Garofalakis et al., 2000; Denis, 2001; Denis et al., 2004; Fernau, 2005; Galassi and Giordana, 2005; Bex et al., 2006). Much of this work assumes that the target regex is small and compact thereby allowing the learning algorithm to exploit this information. Consider, for example, the learning of patterns motivated by DNA sequencing applications (Galassi and Giordana, 2005). Here the input sequence is viewed as multiple atomic events separated by gaps. Since each atomic event is easily described by a small and compact regex, the problem reduces to one of learning simple regexes. Similarly, in XML DTD inference (Garofalakis et al., 2000; Bex et al., 2006), it is po</context>
</contexts>
<marker>Fernau, 2005</marker>
<rawString>Henning Fernau. 2005. Algorithms for learning regular expressions. In ALT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Firoiu</author>
</authors>
<title>Learning regular languages from positive evidence.</title>
<date>1998</date>
<booktitle>In CogSci.</booktitle>
<marker>Firoiu, 1998</marker>
<rawString>Laura Firoiu et al. 1998. Learning regular languages from positive evidence. In CogSci.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Fukuda</author>
</authors>
<title>Toward information extraction: identifying protein names from biological papers.</title>
<date>1998</date>
<booktitle>Pac Symp Biocomput., 1998:707718 Ugo Galassi and Attilio Giordana.</booktitle>
<marker>Fukuda, 1998</marker>
<rawString>K. Fukuda et al. 1998. Toward information extraction: identifying protein names from biological papers. Pac Symp Biocomput., 1998:707718 Ugo Galassi and Attilio Giordana. 2005. Learning regular expressions from noisy sequences. In SARA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minos Garofalakis</author>
</authors>
<title>XTRACT: a system for extracting document type descriptors from XML documents.</title>
<date>2000</date>
<booktitle>In SIGMOD.</booktitle>
<marker>Garofalakis, 2000</marker>
<rawString>Minos Garofalakis et al. 2000. XTRACT: a system for extracting document type descriptors from XML documents. In SIGMOD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Lei Guo</author>
</authors>
<title>Empirical Study on the Performance Stability of Named Entity Recognition Model across Domains In EMNLP.</title>
<date>2006</date>
<marker>Guo, 2006</marker>
<rawString>Hong Lei Guo et al. 2006. Empirical Study on the Performance Stability of Named Entity Recognition Model across Domains In EMNLP.</rawString>
</citation>
<citation valid="true">
<title>Java Regular Expressions.</title>
<date>2008</date>
<note>http://java.sun.com /javase/6/docs/api/java/util/regex/packagesummary.html.</note>
<marker>2008</marker>
<rawString>Java Regular Expressions. 2008. http://java.sun.com /javase/6/docs/api/java/util/regex/packagesummary.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
</authors>
<title>Named Entity Recognition with Character-Level Models.</title>
<date>2003</date>
<booktitle>In HLT-NAACL.</booktitle>
<marker>Klein, 2003</marker>
<rawString>Dan Klein et al. 2003. Named Entity Recognition with Character-Level Models. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vijay Krishnan</author>
<author>Christopher D Manning</author>
</authors>
<title>An Effective Two-Stage Model for Exploiting Non-Local Dependencies in Named Entity Recognition.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2834" citStr="Krishnan and Manning, 2006" startWordPosition="411" endWordPosition="415">esented as R1 = (\\d+\\-)+\\d+.1 While R1 matches valid phone numbers like 800-865-1125 and 725-1234, it suffers from both precision and recall problems. Not only does R1 produce incorrect matches (e.g., social security numbers like 123-45-6789), it also fails to identify valid phone numbers such as 800.865.1125, and (800)865-CARE. An improved regex that addresses these problems is R2 = (\\d{3}[-.\\ ()]){1,2}[\\dA-Z]{4}. While multiple machine learning approaches have been proposed for information extraction in recent years (McCallum et al., 2000; Cohen and McCallum, 2003; Klein et al., 2003; Krishnan and Manning, 2006), manually created regexes remain a widely adopted practical solution for information extraction (Appelt and Onyshkevych, 1998; Fukuda et al., 1998; Cunningham, 1999; Tanabe and Wilbur, 2002; Li et al., 2006; DeRose et al., 2007; Zhu et al., 2007). Yet, with a few notable exceptions, which we discuss later in Section 1.1, there has been very little work in reducing this human effort through the use of automatic learning techniques. In this paper, we propose a novel formulation of the problem of learn1 Throughout this paper, we use the syntax of the standard Java regex engine (Java, 2008). 21 \</context>
</contexts>
<marker>Krishnan, Manning, 2006</marker>
<rawString>Vijay Krishnan and Christopher D. Manning. 2006. An Effective Two-Stage Model for Exploiting Non-Local Dependencies in Named Entity Recognition. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunyao Li</author>
</authors>
<title>Getting work done on the web: Supporting transactional queries.</title>
<date>2006</date>
<booktitle>In SIGIR.</booktitle>
<marker>Li, 2006</marker>
<rawString>Yunyao Li et al. 2006. Getting work done on the web: Supporting transactional queries. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Maximum Entropy Markov Models for Information Extraction and Segmentation.</title>
<date>2000</date>
<booktitle>In ICML.</booktitle>
<marker>McCallum, 2000</marker>
<rawString>Andrew McCallum et al. 2000. Maximum Entropy Markov Models for Information Extraction and Segmentation. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
</authors>
<title>Extracting personal names from emails: Applying named entity recognition to informal text.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<marker>Minkov, 2005</marker>
<rawString>Einat Minkov et al. 2005. Extracting personal names from emails: Applying named entity recognition to informal text. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
</authors>
<title>Learning information extraction rules for semi-structured and free text.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--233272</pages>
<contexts>
<context position="5201" citStr="Soderland, 1999" startWordPosition="798" endWordPosition="799"> in XML DTD inference (Garofalakis et al., 2000; Bex et al., 2006), it is possible to exploit the fact that the XML documents of interest are often described using simple DTDs. E.g., in an online books store, each book has a title, one or more authors and price. This information can be described in a DTD as hbooki htitleihauthori + hpricei. However, as shown in Example 1, regexes for information extraction rely on more complex constructs. In the context of information extraction, prior work has concentrated primarily on learning regexes over relatively small alphabet sizes. A common theme in (Soderland, 1999; Ciravegna, 2001; Wu and Pottenger, 2005; Feldman et al., 2006) is the problem of learning regexes over tagged tokens produced by other text-processing steps such as POS tagging, morphological analysis, and gazetteer matching. Thus, the alphabet is defined by the space of possible tags output by these analysis steps. A similar approach has been proposed in (Brill, 2000) for POS disambiguation. In contrast, our paper addresses extraction tasks that require fine-grained control to accurately capture the structural features of the entity of interest. Consequently, the domain of interest consists</context>
</contexts>
<marker>Soderland, 1999</marker>
<rawString>Stephen Soderland. 1999. Learning information extraction rules for semi-structured and free text. Machine Learning, 34:233272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorraine Tanabe</author>
<author>W John Wilbur</author>
</authors>
<title>Tagging gene and protein names in biomedical text.</title>
<date>2002</date>
<journal>Bioinformatics,</journal>
<pages>18--11241132</pages>
<contexts>
<context position="3024" citStr="Tanabe and Wilbur, 2002" startWordPosition="438" endWordPosition="441">es (e.g., social security numbers like 123-45-6789), it also fails to identify valid phone numbers such as 800.865.1125, and (800)865-CARE. An improved regex that addresses these problems is R2 = (\\d{3}[-.\\ ()]){1,2}[\\dA-Z]{4}. While multiple machine learning approaches have been proposed for information extraction in recent years (McCallum et al., 2000; Cohen and McCallum, 2003; Klein et al., 2003; Krishnan and Manning, 2006), manually created regexes remain a widely adopted practical solution for information extraction (Appelt and Onyshkevych, 1998; Fukuda et al., 1998; Cunningham, 1999; Tanabe and Wilbur, 2002; Li et al., 2006; DeRose et al., 2007; Zhu et al., 2007). Yet, with a few notable exceptions, which we discuss later in Section 1.1, there has been very little work in reducing this human effort through the use of automatic learning techniques. In this paper, we propose a novel formulation of the problem of learn1 Throughout this paper, we use the syntax of the standard Java regex engine (Java, 2008). 21 \x0cing regexes for information extraction tasks. We demonstrate that high quality regex extractors can be learned with significantly reduced manual effort. To motivate our approach, we first</context>
</contexts>
<marker>Tanabe, Wilbur, 2002</marker>
<rawString>Lorraine Tanabe and W. John Wilbur 2002. Tagging gene and protein names in biomedical text. Bioinformatics, 18:11241132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tianhao Wu</author>
<author>William M Pottenger</author>
</authors>
<title>A semisupervised active learning algorithm for information extraction from textual data.</title>
<date>2005</date>
<journal>JASIST,</journal>
<volume>56</volume>
<issue>3</issue>
<contexts>
<context position="5242" citStr="Wu and Pottenger, 2005" startWordPosition="802" endWordPosition="805"> et al., 2000; Bex et al., 2006), it is possible to exploit the fact that the XML documents of interest are often described using simple DTDs. E.g., in an online books store, each book has a title, one or more authors and price. This information can be described in a DTD as hbooki htitleihauthori + hpricei. However, as shown in Example 1, regexes for information extraction rely on more complex constructs. In the context of information extraction, prior work has concentrated primarily on learning regexes over relatively small alphabet sizes. A common theme in (Soderland, 1999; Ciravegna, 2001; Wu and Pottenger, 2005; Feldman et al., 2006) is the problem of learning regexes over tagged tokens produced by other text-processing steps such as POS tagging, morphological analysis, and gazetteer matching. Thus, the alphabet is defined by the space of possible tags output by these analysis steps. A similar approach has been proposed in (Brill, 2000) for POS disambiguation. In contrast, our paper addresses extraction tasks that require fine-grained control to accurately capture the structural features of the entity of interest. Consequently, the domain of interest consists of all characters thereby dramatically i</context>
</contexts>
<marker>Wu, Pottenger, 2005</marker>
<rawString>Tianhao Wu and William M. Pottenger. 2005. A semisupervised active learning algorithm for information extraction from textual data. JASIST, 56(3):258271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huaiyu Zhu</author>
</authors>
<title>Alexander Loeser, Sriram Raghavan, Shivakumar Vaithyanathan</title>
<date>2007</date>
<marker>Zhu, 2007</marker>
<rawString>Huaiyu Zhu, Alexander Loeser, Sriram Raghavan, Shivakumar Vaithyanathan 2007. Navigating the intranet with high precision. In WWW.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>