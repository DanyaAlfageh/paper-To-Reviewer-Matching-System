 AL has been successfully applied to many NLP tasks; CITATION compare the effectiveness of several AL approaches for sequence labeling tasks of NLP,,
 Self-training CITATION is a form of semi-supervised learning,,
 Similar to self-training, cotraining CITATION augments the training set by automatically labeled examples,,
 CITATION showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a highperforming model from being learned,,
 bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility CITATION,,
 A combination of active and semi-supervised learning has first been proposed by CITATION for text classification,,
 The committee members are first trained on the labeled examples and then augmented by means of Expectation Maximization (EM) CITATION including the unlabeled examples,,
 Similarly, co-testing CITATION, a multi-view AL algorithms, selects examples for the multi-view, semi-supervised Co-EM algorithm,,
 Our approach to semisupervised AL is different as, firstly, we augment the training data using a self-tagging mechanism (CITATION and Muslea et al,,
 Different approaches to AL have been successfully applied to a wide range of NLP tasks (CITATION; CITATION; CITATION; CITATION),,
 Our approach to semisupervised AL is different as, firstly, we augment the training data using a self-tagging mechanism (CITATION and Muslea et al,,
 Another work also closely related to ours is that of CITATION,,
-language newspaper domain, we took the training part of the MUC7 corpus (Linguistic Data CITATION) which incorporates seven different entity types, viz,,
 From the sublanguage biology domain, we used the oncology part of the PENNBIOIE corpus CITATION and removed all but three gene entity subtypes (generic, protein, and rna),,
 Conditional Random Fields (CRFs) CITATION are a probabilistic framework for labeling structured data and model P~ (~ y|~ x),,
 4 Related Work Common approaches to AL are variants of the Query-By-Committee approach CITATION or based on uncertainty sampling CITATION,,
 AL has been successfully applied to many NLP tasks; CITATION compare the effectiveness of several AL approaches for sequence labeling tasks of NLP,,
 Self-training CITATION is a form of semi-supervised learning,,
 Although the assumption of uniform costs per token has already been subject of legitimate criticism CITATION, we believe that the number of annotated tokens is still a reasonable approximation in the absence of an empirically more adequate task-specific annotation cost model,,
 From the general-language newspaper domain, we took the training part of the MUC7 corpus (Linguistic Data CITATION) which incorporates seven different entity types, viz,,
 From the sublanguage biology domain, we used the oncology part of the PENNBIOIE corpus CITATION and removed all but three gene entity subtypes (generic, protein, and rna),,
 Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility CITATION,,
 A combination of active and semi-supervised learning has first been proposed by CITATION for text classification,,
 The committee members are first trained on the labeled examples and then augmented by means of Expectation Maximization (EM) CITATION including the unlabeled examples,,
 Similarly, co-testing CITATION, a multi-view AL algorithms, selects examples for the multi-view, semi-supervised Co-EM algorithm,,
 A combination of active and semi-supervised learning has first been proposed by CITATION for text classification,,
 The committee members are first trained on the labeled examples and then augmented by means of Expectation Maximization (EM) CITATION including the unlabeled examples,,
 Similarly, co-testing CITATION, a multi-view AL algorithms, selects examples for the multi-view, semi-supervised Co-EM algorithm,,
 Our approach to semisupervised AL is different as, firstly, we augment the training data using a self-tagging mechanism (CITATION and Muslea et al,,
 Different approaches to AL have been successfully applied to a wide range of NLP tasks (CITATION; CITATION; CITATION; CITATION),,
 Similar to self-training, cotraining CITATION augments the training set by automatically labeled examples,,
 CITATION showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a highperforming model from being learned,,
 To address the problem of data pollution by tagging errors, CITATION propose corrected co-training,,
 Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility CITATION,,
 A combination of active and semi-supervised learning has first been proposed by CITATION for text classification,,
 The committee members are first trained on the labeled examples and then augmented by means of Expectation Maximization (EM) CITATION including the unlabeled examples,,
 The Forward-Backward algorithm CITATION solves this problem efficiently,,
 Different approaches to AL have been successfully applied to a wide range of NLP tasks (CITATION; CITATION; CITATION; CITATION),,
 We have chosen this straightforward one for simplicity and because it has proven to be very effective CITATION,,
 4 Related Work Common approaches to AL are variants of the Query-By-Committee approach CITATION or based on uncertainty sampling CITATION,,
 AL has been successfully applied to many NLP tasks; CITATION compare the effectiveness of several AL approaches for sequence labeling tasks of NLP,,
 Self-training CITATION is a form of semi-supervised learning,,
 Similar to self-training, cotraining CITATION augments the training set by automatically labeled examples,,
 Although the assumption of uniform costs per token has already been subject of legitimate criticism CITATION, we believe that the number of annotated tokens is still a reasonable approximation in the absence of an empirically more adequate task-specific annotation cost model,,
 From the general-language newspaper domain, we took the training part of the MUC7 corpus (Linguistic Data CITATION) which incorporates seven different entity types, viz,,
 4 Related Work Common approaches to AL are variants of the Query-By-Committee approach CITATION or based on uncertainty sampling CITATION,,
 AL has been successfully applied to many NLP tasks; CITATION compare the effectiveness of several AL approaches for sequence labeling tasks of NLP,,
 Self-training CITATION is a form of semi-supervised learning,,
 Different approaches to AL have been successfully applied to a wide range of NLP tasks (CITATION; CITATION; CITATION; CITATION),,
 AL are variants of the Query-By-Committee approach CITATION or based on uncertainty sampling CITATION,,
 AL has been successfully applied to many NLP tasks; CITATION compare the effectiveness of several AL approaches for sequence labeling tasks of NLP,,
 Self-training CITATION is a form of semi-supervised learning,,
 Similar to self-training, cotraining CITATION augments the training set by automatically labeled examples,,
