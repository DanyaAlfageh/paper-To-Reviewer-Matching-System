 For the Bayesian runs, we compared two inference methods: Gibbs sampling, as described above, and Variational Bayesian EM CITATION, both of which are implemented in Carmel,,
 We focus on training using Gibbs sampling CITATION, because it has been popularly applied in the natural language literature, e,,
, (CITATION; CITATION; CITATION),,
 Many natural language models can be captured by weighted finite-state transducers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), which offer several benefits: WFSTs provide a uniform knowledge representation,,
 We focus on training using Gibbs sampling CITATION, because it has been popularly applied in the natural language literature, e,,
, (CITATION; CITATION; CITATION),,
 We focus on training using Gibbs sampling CITATION, because it has been popularly applied in the natural language literature, e,,
, (CITATION; CITATION; CITATION),,
 CITATION employed blocked sampling for POS tagging, and the approach works nicely for arbitrary derivation lattices,,
 First, is there an efficient method for performing pointwise sampling on general FSTs, and would pointwise sampling deliver better empirical results than blocked sampling across a range of tasks? Second, can generic methods similar to the ones described here be developed for cascades of tree transducers? It is straightforward to adapt our methods to train a single tree transducer CITATION, but as most types of tree transducers are not closed under composition CITATION, the compose/de-compose method cannot be directly applied to train cascades,,
 We focus on training using Gibbs sampling CITATION, because it has been popularly applied in the natural language literature, e,,
, (CITATION; CITATION; CITATION),,
 The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (CITATION; CITATION), (2) letter substitution decipherment (CITATION; CITATION; CITATION), (3) segmentation of space-free English CITATION, and (4) Japanese/English phoneme alignment (CITATION; CITATIONa),,
1 Particular Case We start with a well-known application of Bayesian inference, unsupervised POS tagging CITATION,,
 CITATION choose the re-sampling operator change the tag of a single word, and they derive the corresponding incremental scoring formula for unsupervised tagging,,
 The approximation can be corrected using the Metropolis-Hastings algorithm, in which the sample drawn from the proposal lattice is accepted only with a certain probability ; but CITATION report that &gt; 0,,
 First, if we want to find the MAP derivations of the training strings, then following CITATION, we can use annealing: raise the probabilities in the sampling distribution to the 1 T power, where T is a temperature parameter, decrease T towards zero, and take a single sample,,
2% from CITATION, who use a trigram model) and close to the best accuracy reported on this task (91,,
8% from CITATIONb), who use an integer linear program to minimize the model directly),,
 We adopt the common problem formulation for this task described by CITATION, in which we are given a raw 24,115-word sequence and a dictionary of legal tags for each word type,,
 CITATION choose the re-sampling operator change the tag of a single word, and they derive the corresponding incremental scoring formula for unsupervised tagging,,
 The approximation can be corrected using the Metropolis-Hastings algorithm, in which the sample drawn from the proposal lattice is accepted only with a certain probability ; but CITATION report that &gt; 0,,
 First, if we want to find the MAP derivations of the training strings, then following CITATION, we can use annealing: raise the probabilities in the sampling distribution to the 1 T power, where T is a temperature parameter, decrease T towards zero, and take a single sample,,
2% from CITATION, who use a trigram model) and close to the best accuracy reported on this task (91,,
8% from CITATIONb), who use an integer linear program to minimize the model directly),,
 We adopt the common problem formulation for this task described by CITATION, in which we are given a raw 24,115-word sequence and a dictionary of legal tags for each word type,,
 The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (CITATION; CITATION), (2) letter substitution decipherment (CITATION; CITATION; CITATION), (3) segmentation of space-free English CITATION, and (4) Japanese/English phoneme alignment (CITATION; CITATIONa),,
 This composition follows CITATION, treating ep,,
 First, is there an efficient method for performing pointwise sampling on general FSTs, and would pointwise sampling deliver better empirical results than blocked sampling across a range of tasks? Second, can generic methods similar to the ones described here be developed for cascades of tree transducers? It is straightforward to adapt our methods to train a single tree transducer CITATION, but as most types of tree transducers are not closed under composition CITATION, the compose/de-compose method cannot be directly applied to train cascades,,
 Many natural language models can be captured by weighted finite-state transducers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), which offer several benefits: WFSTs provide a uniform knowledge representation,,
 The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (CITATION; CITATION), (2) letter substitution decipherment (CITATION; CITATION; CITATION), (3) segmentation of space-free English CITATION, and (4) Japanese/English phoneme alignment (CITATION; CITATIONa),,
 This composition follows CITATION, treating epsilon input and output transitions correctly, especially with regards,,
 We use the problem formulation of CITATION,,
 Weighted tree transducers play the same role for problems that involve the creation and transformation of tree structures CITATION,,
 The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (CITATION; CITATION), (2) letter substitution decipherment (CITATION; CITATION; CITATION), (3) segmentation of space-free English CITATION, and (4) Japanese/English phoneme alignment (CITATION; CITATIONa),,
 Many natural language models can be captured by weighted finite-state transducers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), which offer several benefits: WFSTs provide a uniform knowledge representation,,
 Many natural language models can be captured by weighted finite-state transducers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), which offer several benefits: WFSTs provide a uniform knowledge representation,,
 The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (CITATION; CITATION), (2) letter substitution decipherment (CITATION; CITATION; CITATION), (3) segmentation of space-free English CITATION, and (4) Japanese/English phoneme alignment (CITATION; CITATIONa),,
2% from CITATION, who use a trigram model) and close to the best accuracy reported on this task (91,,
8% from CITATIONb), who use an integer linear program to minimize the model directly),,
 We adopt the common problem formulation for this task described by CITATION, in which we are given a raw 24,115-word sequence and a dictionary of legal tags for each word type,,
 We use the same modeling approach as as CITATION, using a probabilistic tag bigram model in conjunction with a tag-to-word model,,
 The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (CITATION; CITATION), (2) letter substitution decipherment (CITATION; CITATION; CITATION), (3) segmentation of space-free English CITATION, and (4) Japanese/English phoneme alignment (CITATION; CITATIONa),,
free English CITATION, and (4) Japanese/English phoneme alignment (CITATION; CITATIONa),,
 This composition follows CITATION, treating epsilon input and output transitions correctly, especially with regards to their weighted interleaving,,
 Many natural language models can be captured by weighted finite-state transducers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), which offer several benefits: WFSTs provide a uniform knowledge representation,,
 The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (CITATION; CITATION), (2) letter substitution decipherment (CITATION; CITATION; CITATION), (3) segmentation of space-free English CITATION, and (4) Japanese/English phoneme alignment (CITATION; CITATIONa),,
 We use the same modeling approach as as CITATION, using a probabilistic tag bigram model in conjunction with a tag-to-word model,,
 We work on the same standard cipher described in previous literature CITATION,,
 The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (CITATION; CITATION), (2) letter substitution decipherment (CITATION; CITATION; CITATION), (3) segmentation of space-free English CITATION, and (4) Japanese/English phoneme alignment (CITATION; CITATIONa),,
 This composition follows CITATION, treating epsilon input and output transitions correctly, especially with regards to their weighted inte,,
2% from CITATION, who use a trigram model) and close to the best accuracy reported on this task (91,,
8% from CITATIONb), who use an integer linear program to minimize the model directly),,
 We adopt the common problem formulation for this task described by CITATION, in which we are given a raw 24,115-word sequence and a dictionary of legal tags for each word type,,
 We use the same modeling approach as as CITATION,,
 The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (CITATION; CITATION), (2) letter substitution decipherment (CITATION; CITATION; CITATION), (3) segmentation of space-free English CITATION, and (4) Japanese/English phoneme alignment (CITATION; CITATIONa),,
 This composition follows CITATION, treating epsilon input and output transitions correctly, especially with regards to their weighted inte,,
2% from CITATION, who use a trigram model) and close to the best accuracy reported on this task (91,,
8% from CITATIONb), who use an integer linear program to minimize the model directly),,
 We adopt the common problem formulation for this task described by CITATION, in which we are given a raw 24,115-word sequence and a dictionary of legal tags for each word type,,
 We use the same modeling approach as as CITATION,,
 Many natural language models can be captured by weighted finite-state transducers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), which offer several benefits: WFSTs provide a uniform knowledge representation,,
