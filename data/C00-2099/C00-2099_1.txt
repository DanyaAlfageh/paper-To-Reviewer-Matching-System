b'A Statistical Theory of Dependency Syntax
Christer Samuelsson
Xerox Research Centre Europe
6, chemin de Maupertuis
38240 Meylan, FRANCE
Christer.Samuelsson@xrce.xerox.com
Abstract
A generative statistical model of dependency syntax
is proposed based on Tesni\x12
ere\'s classical theory. It
provides a stochastic formalization of the original
model of syntactic structure and augments it with
a model of the string realization process, the latter
which is lacking in Tesni\x12
ere\'s original work. The
resulting theory models crossing dependency links,
discontinuous nuclei and string merging, and it has
been given an e\x0ecient computational rendering.
1 Introduction
The theory of dependency grammar culminated in
the seminal book by Lucien Tesni\x12
ere, (Tesni\x12
ere,
1959), to which also today\'s leading scholars pay
homage, see, e.g., (Mel\'\x14
cuk, 1987). Unfortunately,
Tesni\x12
ere\'s book is only available in French, with a
partial translation into German, and subsequent de-
scriptions of his work reported in English, (Hays,
1964), (Gaifman, 1965), (Robinson, 1970), etc.,
stray increasingly further from the original, see (En-
gel, 1996) or (J\x7f
arvinen, 1998) for an account of this.
The \x0crst step when assigning a dependency de-
scription to an input string is to segment the input
string into nuclei. A nucleus can be a word, a part
of a word, or a sequence of words and subwords,
and these need not appear contiguously in the input
string. The best way to visualize this is perhaps the
following: the string is tokenized into a sequence of
tokens and each nucleus consists of a subsequence of
these tokens. Alternative readings may implydi\x0ber-
ent ways of dividing the token sequence into nuclei,
and segmenting the input string into nuclei is there-
fore in general a nondeterministic process.
The next step is to relate the nuclei to each other
through dependency links, which are directed and
typed. If there is a dependency link from one nu-
cleus to another, the former is called a dependent
of the latter, and the latter a regent of the former.
Theories ofdependency syntax typicallyrequire that
each nucleus, save a single root nucleus, is assigned
a unique regent, and that there is no chain of de-
pendency links that constitutes a cycle. This means
that the dependency links establish a tree structure,
subj dobj
main
John beans
ate
.
ate
subj dobj
main
John beans
.
ate
main
.
dobj subj
beans John
Figure 1: Dependency trees for John ate beans.
where each node is labeled by a nucleus. Thus, the
label assigned to a node is a dependent of the label
assigned to its parent node, and conversely, the label
assigned to a node is the regent of the labels assigned
to its child nodes. Figure 1 shows two dependency
trees for the sentence John ate beans.
In Tesni\x12
ere\'s dependency syntax, only the depen-
dency structure, not the order of the dependents, is
represented by a dependency tree. This means that
dependency trees are unordered, and thus that the
two trees of Figure 1 are equivalent. This also means
that specifying the surface-string realization of a de-
pendency description becomes a separate issue.
We will model dependency descriptions as two
separate stochastic processes: one top-down process
generating the tree structure T and one bottom-up
process generating the surface string(s) S given the
tree structure:
P(T ;S) = P(T )\x01 P(S j T )
This can be viewed as a variant of Shannon\'s noisy
channel model, consisting of a language model of
tree structures and a signal model converting trees
to surface strings. In Section 2 we describe the top-
down process generating tree structures and in Sec-
tion 3 we propose a series of increasingly more so-
phisticated bottom-up processes generating surface
strings, which result in grammars with increasingly
greater expressive power. Section 4 describes how
the proposed stochastic model of dependency syn-
tax was realized as a probabilistic chart parser.
2 Generating Dependency Trees
To describe a tree structure T , we will use a string
notation, introduced in (Gorn, 1962), for the nodes
\x0cbeans
ate/1
/12
John/11
subj dobj
main
. /
Figure 2: Gorn\'s tree notation for John ate beans.
N L F S
\x0f . [main] s(1) .
1 ate [subj,dobj] s(11) ate s(12)
11 John ; John
12 beans ; beans
Figure 3: Dependency encoding of John ate beans.
of the tree, where the node name speci\x0ces the path
from the root node \x0f to the node in question.
If \x1ej is a node of the tree T ,
with j 2 N+ and \x1e 2 N\x03
+,
then \x1e is also a node of the tree T
and \x1ej is a child of \x1e.
Here, N+ denotes the set of positive integers
f1;2;:::g and N\x03
+ is the set of strings over N+. This
means that the label assigned to node \x1ej is a de-
pendent of the label assigned to node \x1e. The \x0crst
dependency tree of Figure 1 is shown in Figure 2
using this notation.
We introduce three basic random variables, which
incrementally generate the tree structure:
\x0f L(\x1e) = l assigns the label l to node \x1e, where
l is a nucleus, i.e., it is drawn from the set of
strings over the set of tokens.
\x0f D(\x1ej) = dindicates thedependency type dlink-
ing the label of node \x1ej to its regent, the label
of node \x1e.
\x0f V(\x1e) = v indicates that node \x1e has exactly v
child nodes.
Note the use of V(\x1e) = 0, rather than a partitioning
of the labels into terminal and nonterminalsymbols,
to indicate that \x1e is a leaf node.
Let D be the (\x0cnite) set of possible dependency
types. We next introduce the composite variables
F (\x1e) ranging over the power bag1 ND, indicating
the bag of dependency types of \x1e\'s children:
F (\x1e) = f = [d1;:::;dv] ,
, V(\x1e) = v ^ 8j2f1;:::;vg D(\x1ej) = dj
Figure 3 encodes the dependency tree of Figure 2
accordingly. We will ignore the last column for now.
1A bag (multiset) can contain several tokens of the same
type. We denotesets f: : :g, bags[: : :] and orderedtuplesh: : :i,
but overload [; \x12, etc.
We introduce the probabilities
PL(\x0f) =
= P(L(\x0f) = l)
PL(\x1ej) =
= P(L(\x1ej) = lj j L(\x1e) = l;D(\x1ej) = dj)
PF(\x0f) =
= P(F (\x0f) = f j L(\x0f) = l)
PF(\x1e) = f for \x1e 6= \x0f g
= P(F (\x1e) = f j L(\x1e) = l;D(\x1e) = d)
These probabilities are typically model parameters,
orfurther decomposedintosuch. PL(\x1ej) is the prob-
ability of the label L(\x1ej) of a node given the label
L(\x1e) of its regent and the dependency type D(\x1ej)
linkingthem. Relating L(\x1ej) and L(\x1e) yields lexical
collocation statistics and including D(\x1ej) makes the
collocation statistics lexical-functional. PF(\x1e) is the
probability of the bag of dependency types F (\x1e) of
a node given its label L(\x1e) and its relation D(\x1e) to
its regent. This re
ects the probability of the label\'s
valency,or lexical-functionalcomplement,and ofop-
tional adjuncts. Including D(\x1e) makes this proba-
bilitysituated in taking its current role into account.
These allow us to de\x0cne the tree probability
P(T ) =
Y
\x1e2N
PL(\x1e) \x01 PF(\x1e)
where the product is taken over the set of nodes N
of the tree.
We generate the random variables L and F using
a top-down stochastic process, where L(\x1e) is gener-
ated before F (\x1e). The probability of the condition-
ingmaterialofPL(\x1ej) isthen knownfromPL(\x1e)and
PF(\x1e), and that of PF(\x1ej) is known from PL(\x1ej)
and PF(\x1e). Figure 3 shows the process generating
the dependency tree of Figure 2 by reading the L
and F columns downwards in parallel, L before F :
L(\x0f) = :; F (\x0f) = [main];
L(1) = ate; F (1) = [subj,dobj];
L(11) = John; F (11) = ;;
L(12) = beans; F (12) = ;
Consider calculating the probabilities at node 1:
PL(1) =
= P(L(1) = ate j L(\x0f) = :;D(1) = main)
PF(1) =
= P(F (1) = [subj;dobj] j
j L(1) = ate;D(1) = main)
3 String Realization
The string realization cannot be uniquely deter-
mined from the tree structure. To model the string-
realization process, we introduce another fundamen-
tal random variable S(\x1e), which denotes the string
\x0cassociated with node \x1e and which should not be con-
fused with the node label L(\x1e). We will introduce
yet another fundamental random variable M(\x1e) in
Section 3.2, when we accommodate crossing depen-
dency links. In Section 3.1, we present a projective
stochastic dependency grammar with an expressive
power not exceeding that of stochastic context-free
grammars.
3.1 Projective Dependency Grammars
We let the stochastic process generating the L and F
variables be as described above. We then de\x0cne the
stochastic string-realization process by letting the
S(\x1e) variables, given \x1e\'s label l(\x1e) and the bag of
strings s(\x1ej) of \x1e\'s child nodes, randomly permute
and concatenate them according to the probability
distributions of the model:
PS(\x0f) =
= P(S(\x0f) = s(\x0f) j L(\x0f);F (\x0f);C(\x0f))
PS(\x1e) = f for \x1e 6= \x0f g
= P(S(\x1e) = s(\x1e) j D(\x1e);L(\x1e);F (\x1e);C(\x1e))
where
C(\x1e) =
v
[
j=1
[s(\x1ej)]
S(\x1e) = adjoin(C(\x1e);l(\x1e))
adjoin(A;\x0c) = concat(permute(A[ [\x0c]))
The latter equations should be interpreted as de\x0cn-
ing the randomvariable S, rather than specifying its
probability distribution or some possible outcome.
This means that each dependent is realized adjacent
to its regent, where we allow intervening siblings,
and that we thus stay within the expressive power
of stochastic context-free grammars.
We de\x0cne the string-realization probability
P(S j T ) =
Y
\x1e2N
PS(\x1e)
and the tree-string probability as
P(T ;S) = P(T )\x01 P(S j T )
The stochastic process generating the tree struc-
ture is as described above. We then generate the
string variables S using a bottom-up stochastic pro-
cess. Figure 3 also shows the process realizing the
surface string John ate beans from the dependency
tree of Figure 2 by reading the S column upwards:
S(12) = beans; S(11) = John;
S(1) = s(11) ate s(12); S(\x0f) = s(1) .
Considercalculatingthe string probabilityatnode
1. PS is the probability of the particular permuta-
tion observed of the strings of the children and the
Mary
did say/1
/121
/
/11
John
that ate/12
What beans/122
?
subj sconj
subj dobj
whq
Figure 4: Dependency tree for What beans did Mary
say that John ate?
labelofthe node. Toovercome the sparse-data prob-
lem, we will generalize over the actual strings of the
children to their dependency types. For example,
s(subj) denotes the string of the subject child, re-
gardless of what it actually might be.
PS(1) = P(S(1) = s(subj) ate s(dobj) j
j D(1) = main;L(1) = ate;
C(1) = [s(subj);s(dobj)])
This is the probability of the permutation
hs(subj);ate;s(dobj)i
of the bag
[s(subj);ate;s(dobj)]
given this bag and the fact that we wish to form a
main, declarative clause. This example highlights
the relationship between the node strings and both
Saussure\'s notion of constituency and the positional
schemata of, amongst others, Didrichsen.
3.2 Crossing Dependency Links
To accommodate long-distance dependencies, we al-
low a dependent to be realized adjacent to the la-
bel of any node that dominates it, immediately or
not. For example, consider the dependency tree of
Figure 4 for the sentence What beans did Mary say
that John ate? as encoded in Figure 5. Here, What
beans is a dependent of that ate, which in turn is a
dependent of did say, and What beans is realized be-
tween did and say. Thisphenomenonis called move-
ment in conjunction with phrase-structure gram-
mars. It makes the dependency grammar non-
projective, since it creates crossing dependency links
if the dependency trees also depict the word order.
We introduce variables M(\x1e) that randomly se-
lect from C(\x1e) a subbag CM(\x1e) of strings passed up
to \x1e\'s regent:
C(\x1e) =
v
[
j=1
([s(\x1ej)] [ CM(\x1ej))
CM(\x1e) \x12 C(\x1e)
PM(\x1e) = P(M(\x1e) = CM(\x1e) j
j D(\x1e);L(\x1e);F (\x1e);C(\x1e))
\x0cN L F
\x0f ? [whq]
1 did say [subj,sconj]
11 Mary ;
12 that ate [subj,dobj]
121 John ;
122 What beans ;
Figure 5: Dependency encoding of What beans did
Mary say that John ate?
N M S
\x0f ; s(1) ?
1 ; CM(12)(s(122)) did s(11) say s(12)
11 ; Mary
12 [s(122)] that s(121) ate
121 ; John
122 ; What beans
Figure 6: Process generating What beans did Mary
say that John ate?
The rest of the strings, CS(\x1e), are realized here:
CS(\x1e) = C(\x1e)n CM(\x1e)
PS(\x1e) = P(S(\x1e) = s(\x1e) j D(\x1e);L(\x1e);F (\x1e);CS(\x1e))
S(\x1e) = adjoin(CS(\x1e);l(\x1e))
3.3 Discontinuous Nuclei
We generalize the scheme to discontinuous nuclei by
allowing S(\x1e) to insert the strings of CS(\x1e) any-
where in l(\x1e): 2
adjoin(A;\x0c) =
= concat(permutei<j)bi\x1ebj
(A [
m
[
j=1
[bj]))
\x0c = b1 :::bm
This means that strings can onlybe inserted into an-
cestor labels, not into other strings, which enforces
a type of reverse island constraint. Note how in Fig-
ure 6 John is inserted between that and ate to form
the subordinate clause that John ate.
We de\x0cne the string-realization probability
P(S j T ) =
Y
\x1e2N
PM(\x1e) \x01 PS(\x1e)
and again de\x0cne the tree-string probability
P(T ;S) = P(T )\x01 P(S j T )
2x \x1e y indicates that x precedes y in the resulting per-
mutation. Tesni\x12
ere\'s original implicit de\x0cnition of a nucleus
actually does not require that the order be preserved when
realizing it; if has eaten is a nucleus, so is eaten has. This is
obviouslya useful featurefor modelingverb chainsin German
subordinate clauses.
To avoid derivational ambiguity when generating a
tree-string pair, i.e., have more than one derivation
generate the same tree-string pair, we require that
no string be realized adjacent to the string of any
node it was passed up through. This introduces the
practical problem of ensuring that zero probability
mass is assigned to all derivations violatingthis con-
straint. Otherwise, the result will be approximating
the parse probability with a derivation probability,
as described in detailin (Samuelsson,2000)based on
the seminal work of (Sima\'an, 1996). Schemes like
(Alshawi, 1996) tacitly make this approximation.
The tree-structure variables L and F are gener-
ated just as before. Wethen generate the string vari-
ables S andM usingabottom-upstochastic process,
where M(\x1e) is generated before S(\x1e). The proba-
bility of the conditioning material of PM(\x1e) is then
known either from the top-down process or from
PM(\x1ej) and PS(\x1ej), and that of PS(\x1e) is known
either from the top-down process, or from PM(\x1e),
PM(\x1ej) and PS(\x1ej). The coherence of S(\x1e) and
M(\x1e) is enforced by explicit conditioning.
Figure 5 shows a top-down process generating the
dependency tree of Figure 4; the columns L and
F should be read downwards in parallel, L before
F . Figure 6 shows a bottom-up process generating
the string What beans did Mary say that John ate?
from the dependency description of Figure 5. The
columns M and S should be read upwards in paral-
lel, M before S.
3.4 String Merging
We have increased the expressive power of our de-
pendency grammars by modifying the S variables,
i.e., by extending the adjoin operation. In the \x0crst
version, the adjoinoperation randomlypermutes the
node label and the strings of the child nodes, and
concatenates the result. In the second version, it
randomly inserts the strings of the child nodes, and
any movedstrings to be realized at the current node,
into the node label.
The adjoin operation can be further re\x0cned to al-
low handling an even wider range of phenomena,
such as negation in French. Here, the dependent
stringismerged withthe labelofthe regent, asne :::
pas is wrapped around portions of the verb phrase,
e.g., Ne me quitte pas!, see (Brel, 1959). Figure 7
shows a dependency tree forthis. In additionto this,
the node labels may be linguistic abstractions, e.g.
\
egation", calling on the S variables also for their
surface-string realization.
Note that the expressive power of the grammar
depends on the possible distributions of the string
probabilitiesPS. Sinceeach node labelcan be moved
and realized at the root node, any language can be
recognized to which the string probabilities allowas-
signingthe entire probablitymass, and the grammar
will possess at least this expressive power.
\x0cneg dobj
!
imp
/
me/12
Ne pas/11
/1
quitte
Figure 7: Dependency tree for Ne me quitte pas!
4 A Computational Rendering
A close approximation of the described stochastic
model of dependency syntax has been realized as a
type of probabilistic bottom-up chart parser.
4.1 Model Specialization
The following modi\x0ccations, which are really just
specializations, were made to the proposed model
for e\x0eciency reasons and to cope with sparse data.
According to Tesni\x12
ere, a nucleus is a unit that
contains both the syntactic and semantic head and
that does not exhibit any internal syntactic struc-
ture. We take the view that a nucleus consists of a
content word, i.e., an open-class word, and all func-
tionwords addinginformationtoitthatcould justas
well have been realized morphologically. For exam-
ple, the de\x0cnite article associates de\x0cniteness with a
word, whichcouldjust haswellhavebeen manifested
in the word form, as it is done in North-Germanic
languages; a preposition could be realized as a loca-
tional or temporal in
ection, as is done in Finnish.
The longest nuclei we currently alloware verb chains
of the form that have been eaten, as in John knows
that the beans have been eaten.
The F variables were decomposed into generating
the set of obligatory arguments, i.e., the valency or
lexicalcomplement,atonce, asinthe originalmodel.
Optional modi\x0cers (adjuncts) are attached through
one memory-less process for each modi\x0cer type, re-
sulting in geometric distributions for these. This is
the same separation of arguments and adjuncts as
that employed by (Collins, 1997). However, the L
variables remained as described above, thus leaving
the lexical collocation statistics intact.
The movement probability was divided into three
parts: the probability of moving the string of a par-
ticular argument dependent from its regent, that of
a moved dependency type passing through a par-
ticular other dependency type, and that of a de-
pendency type landing beneath a particular other
dependency type. The one type of movement that
is not yet properly handled is assigning arguments
and adjuncts to dislocated heads, as in What book
did John read by Chomsky?
The string-realization probability is a straight-
forward generalization of that given at the end of
Section 3.1, and they are de\x0cned through regu-
lar expressions. Basically, each unmoved depen-
dent string, each moved string landed at the cur-
beans /12
John/11
subj dobj
/
?
ynq
John/11
subj dobj
/
?
Did eat/1
ynq
Did xxx/1
xxx/12
Figure 8: Dependency trees for Did John xxx beans?
and Did John eat xxx?
rent node, and each token of the nucleus labelingthe
current node are treated as units that are randomly
permuted. Whenever possible, strings are general-
ized to their dependency types, but accurately mod-
elling dependent order in French requires inspecting
the actual strings of dependent clitics. Open-class
words are typically generalized to their word class.
String mergingonly applies to a smallclass of nuclei,
where we treat the individual tokens of the depen-
dent string, which is typically its label, as separate
units when performing the permutation.
4.2 The Chart Parser
The parsing algorithm, which draws on the Cocke-
Kasami-Younger (CKY) algorithm, see (Younger,
1967), is formulated as a probabilistic deduction
scheme, whichin turn is realized as anagenda-driven
chart-parser. The top-level control is similar to that
of (Pereira and Shieber, 1987), pp. 196{210. The
parser is implemented in Prolog, and it relies heav-
ily on using set and bag operations as primitives,
utilizing and extending existing SICStus libraries.
The parser \x0crst nondeterministicallysegments the
input string into nuclei, using a lexicon, and each
possible nucleus spawns edges for the initial chart.
Due to discontinuous nuclei, each edge spans not a
single pair of string positions, indicating its start
and end position, but a set of such string-position
pairs, and we call this set an index. If the index
is a singleton set, then it is continuous. We extend
the notion of adjacent indices to be any two non-
overlapping indices where one has a start position
that equals an end position of the other.
The lexicon contains information about the roles
(dependency types linking it to its regent) and va-
lencies (sets3 of types of argument dependents) that
are possible for each nucleus. These are hard con-
straints. Unknown words are included in nuclei in a
judicious way and the resulting nuclei are assigned
all reasonable role/valency pairs in the lexicon. For
example, the parser \\correctly" analyzes the sen-
tences Did John xxx beans? and Did John eat xxx?
as shown in Figure 8, where xxx is not in the lexicon.
For each edge added to the initial chart, the lexi-
con predicts a single valency, but a set of alternative
roles. Edges are added to cover all possible valen-
3Due to the uniqueness principle of arguments, these are
sets, rather than bags.
\x0ccies for each nucleus. The roles correspond to the
\\goal"of dotted itemsused in traditionalchart pars-
ing, and the un\x0clled valency slots play the part of
the \\body", i.e., the portion of the RHS following
the dot that remains to be found. If an argument is
attached to the edge, the corresponding valency slot
is \x0clled in the resulting new edge; no argument can
be attached to an edge unless there is a correspond-
ing un\x0clled valency slot for it, or it is licensed by a
moved argument. For obvious reasons, the lexicon
cannot predict all possible combinations of adjuncts
for each nucleus, and in fact predicts none at all.
There will in general be multiple derivations of any
edge with more than one dependent, but the parser
avoids adding duplicate edges to the chart in the
same way as a traditional chart parser does.
The parser employs a packed parse forest (PPF)
to represent the set of all possible analyses and the
probability of each analysis is recoverable from the
PPF entries. Since optional modi\x0cers are not pre-
dicted by the lexicon, the chart does not contain
any edges that correspond directly to passive edges
in traditionalchart parsing; at any point, an adjunct
can always be added to an existing edge to form a
new edge. In some sense, though, the PPF nodes
play the role of passive edges, since the parser never
attempts to combine two edges, only one edge and
one PPF node, and the latter will always be a de-
pendent ofthe former, directly, or indirectly through
the lists of moved dependents. The edge and PPF
node to be combined are required to have adjacent
indices, and their union is the index of the new edge.
The mainpoint in using a packed parse forest is to
perform local ambiguity packing, which means ab-
stracting over di\x0berences in internal structure that
do not matter for further parsing. When attching a
PPF node to some edge as a direct or indirect de-
pendent, the only relevant features are its index, its
nucleus, its role and its moved dependents. Other
features necessary for recovering the complete anal-
ysis are recorded in the PPF entries of the node, but
are not used for parsing.
To indicate the alternative that no more depen-
dents are added to an edge, it is converted into a
set of PPF updates, where each alternative role of
the edge adds or updates one PPF entry. When
doing this, any un\x0clled valency slots are added to
the edge\'s set of moved arguments, which in turn is
inherited by the resulting PPF update. The edges
are actually not assigned probabilities, since they
contain enough information to derive the appropri-
ate probabilities once they are converted into PPF
entries. To avoid the combinatorial explosion of un-
restricted string merging, we only allow edges with
continuous indices to be converted into PPF entries,
with the exception of a very limited class of lexically
signaled nuclei, such as the ne pas, ne jamais, etc.,
scheme of French negation.
4.3 Pruning
As opposed to traditional chart parsing, meaningful
upper and lower bounds of the supply and demand
for the dependency types of the \\goal" (roles) and
\\body" (valency) of each edge can be determined
from the initial chart, which allows performing so-
phisticated pruning. The basic idea is that if some
edge is proposed with a role that is not sought out-
side its index, this role can safely be removed. For
example, the word me could potentially be an in-
direct object, but if there is no other word in the
input string that can have an indirect object as an
argument, this alternative can be discarded.
This idea is generalized to a variant of pigeonhole
reasoning, in the vein of
If we select this role or edge, then there are
by necessity too few or too many of some
dependency type sought or o\x0bered in the
chart.
or alternatively
If we select this nucleus or edge, then we
cannot span the entire input string.
Pruning is currently only applied to the initial chart
to remove logically impossible alternatives and used
to \x0clter out impossibleedges produced in the predic-
tion step. Nonetheless, it reduces parsing times by
an order of magnitude or more for many of the test
examples. It would however be possible to apply
similar ideas to intermittently remove alternatives
that are known to be suboptimal, or to heuristically
prune unlikely search branches.
5 Discussion
We have proposed a generative, statistical theory
of dependency syntax, based on Tesni\x12
ere\'s classical
theory, that models crossing dependency links, dis-
continuous nuclei and string merging. The key in-
sight was to separate the tree-generation and string-
realization processes. The model has been real-
ized as a type of probabilistic chart parser. The
only other high-\x0cdelity computational rendering of
Tesni\x12
ere\'s dependency syntax that we are aware of
is that of (Tapanainen and J\x7f
arvinen, 1997), which is
neither generative nor statistical.
The stochastic modelgenerating dependency trees
is very similar to other statistical dependency mod-
els, e.g., to that of (Alshawi, 1996). Formulating it
using Gorn\'s notation and the L and F variables,
though, is concise, elegant and novel. Nothing pre-
vents conditioningthe randomvariableson arbitrary
portions of the partial tree generated this far, using,
e.g., maximum-entropy or decision-tree models to
extract relevant features of it; there is no di\x0berence
\x0cin principle between our model and history-based
parsing, see (Black et al., 1993; Magerman, 1995).
The proposed treatment of string realization
throughthe use ofthe S andM variablesis alsoboth
truly novel and important. While phrase-structure
grammars overemphasize word order by making the
processes generating the S variables deterministic,
Tesni\x12
ere treats string realization as a secondary is-
sue. We \x0cnd a middle ground by using stochastic
processes to generate the S and M variables, thus
reinstating word order as a parameter of equal im-
portance as, say, lexical collocation statistics. It is
however not elevated to the hard-constraint status
it enjoys in phrase-structure grammars.
Due to the subordinate role of string realization in
classical dependency grammar, the technical prob-
lems related to incorporating movement into the
string-realization process have not been investigated
in great detail. Our use of the M variables is moti-
vated partly by practical considerations, and partly
by linguistic ones. The former in the sense that
this allows designing e\x0ecient parsing algorithms for
handling also crossing dependency links. The lat-
ter as this gives us a quantitative handle on the
empirically observed resistance against crossing de-
pendency links. As Tesni\x12
ere points out, there is
locality in string realization in the sense that de-
pendents tend to be realized adjacent to their re-
gents. This fact is re
ected by the model parame-
ters, which also model, probabilistically,barrier con-
straints, constraints on landing sites, etc. It is note-
worthy that treating movement as in GPSG, with
the use of the \\slash" feature, see (Gazdar et al.,
1985), pp. 137{168, or as is done in (Collins, 1997),
is the converse of that proposed here for dependency
grammars: the former pass constituents down the
tree, the M variables pass strings up the tree.
The relationship between the proposed stochastic
model of dependency syntax and a number of other
prominent stochastic grammars is explored in detail
in (Samuelsson, 2000).
References
Hiyan Alshawi. 1996. Head automata and bilingual
tiling: Translation with minimal representations.
Procs. 34th Annual Meeting of the Association for
Computational Linguistics, pages 167{176.
Ezra Black, Fred Jelinek, John La\x0berty, David
Magerman, Robert Mercer, and Salim Roukos.
1993. Towards history-based grammars: Using
richer models for probabilistic parsing. Procs.
28th Annual Meeting of the Association for Com-
putational Linguistics, pages 31{37.
Jacques Brel. 1959. Ne me quitte pas. La Valse \x12
a
Mille Temps (PHI 6325.205).
Michael Collins. 1997. Three generative, lexical-
ized models for statistical parsing. Procs. 35th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 16{23.
Ulrich Engel, 1996. Tesni\x12
ere Mi\x19verstanden: Lu-
cien Tesni\x12
ere { Syntaxe Structurale et Opera-
tion Mentales. Akten des deutsch-franz\x7f
osischen
Kolloquiums anl\x7f
a\x19lich der 100 Wiederkehr seines
Gebursttages, Strasbourg 1993, volume348 of Lin-
guistische Arbeiten, pages 53{61. Niedermeyer,
T\x7f
ubingen.
Haim Gaifman. 1965. Dependency systems and
phrase-structure systems. Information and Con-
trol, 8:304{337.
Gerald Gazdar, Ewan Klein, Geo\x0brey K. Pullum,
and Ivan A. Sag. 1985. Generalized Phrase Struc-
ture Grammar. Basil Blackwell Publishing, Ox-
ford, England. Also published by Harvard Uni-
versity Press, Cambridge, MA.
Saul Gorn. 1962. Processors for in\x0cnite codes of
shannon-fano type. Symp. Math. Theory of Au-
tomata.
David Hays. 1964. Dependency theory: A formal-
ism and some observations. Language, 40(4):511{
525.
Timo J\x7f
arvinen. 1998. Tesni\x13
ere\'s Structural Syntax
Reworked. University of Helsinki, Helsinki.
David Magerman. 1995. Statistical decision-tree
models for parsing. Procs. 33rd Annual Meeting
of the Association for Computational Linguistics,
pages 276{283.
Igor Mel\'\x14
cuk. 1987. Dependency Syntax. State Uni-
versity of New York Press, Albany.
Fernando Pereira and Stuart Shieber. 1987. Pro-
log and Natural-Language Analysis. CSLI Lecture
Note 10.
Jane Robinson. 1970. Dependency structures and
transformational rules. Language, 46:259{285.
Christer Samuelsson. 2000. A theory of stochastic
grammars. InProceedings of NLP-2000, pages 92{
105. Springer Verlag.
Khalil Sima\'an. 1996. Computationalcomplexity of
probabilistic disambiguations by means of tree-
grammars. Procs. 16th International Conference
on Computational Linguistics, at the very end.
Pasi Tapanainen and Timo J\x7f
arvinen. 1997. A non-
projective dependency parser. Procs. 5th Con-
ference on Applied Natural Language Processing,
pages 64{71.
Lucien Tesni\x12
ere. 1959. \x13
El\x13
ements de Syntaxe Struc-
turale. Libraire C. Klincksieck, Paris.
David H. Younger. 1967. Recognition and parsing
of context-free languages in time n3. Information
and Control, 10(2):189{208.
\x0c'