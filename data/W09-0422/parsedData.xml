<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.5189255">
b&amp;apos;Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 125129,
Athens, Greece, 30 March 31 March 2009. c
</bodyText>
<sectionHeader confidence="0.453086" genericHeader="abstract">
2009 Association for Computational Linguistics
</sectionHeader>
<table confidence="0.360642">
English-Czech MT in 2008
Ondrej Bojar, David Marecek, Vaclav Novak, Martin Popel,
</table>
<author confidence="0.991367">
Jan Ptacek, Jan Rous, Zdenek Zabokrtsky
</author>
<affiliation confidence="0.977934">
Charles University, Institute of Formal and Applied Linguistics
</affiliation>
<address confidence="0.559844">
Malostranske nam. 25, Praha 1, CZ-118 00, Czech Republic
</address>
<email confidence="0.963709">
{bojar,marecek,novak,ptacek,zabokrtsky}@ufal.mff.cuni.cz
{popel,jan.rous}@matfyz.cz
</email>
<sectionHeader confidence="0.990082" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.987236857142857">
We describe two systems for English-to-
Czech machine translation that took part
in the WMT09 translation task. One of
the systems is a tuned phrase-based system
and the other one is based on a linguisti-
cally motivated analysis-transfer-synthesis
approach.
</bodyText>
<sectionHeader confidence="0.998031" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992056666666667">
We participated in WMT09 with two very dif-
ferent systems: (1) a phrase-based MT based
on Moses (Koehn et al., 2007) and tuned for
EnglishCzech translation, and (2) a complex
system in the TectoMT platform (Zabokrtsky et
al., 2008).
</bodyText>
<sectionHeader confidence="0.991278" genericHeader="method">
2 Data
</sectionHeader>
<subsectionHeader confidence="0.979483">
2.1 Monolingual Data
</subsectionHeader>
<bodyText confidence="0.837527333333333">
Our Czech monolingual data consist of (1)
the Czech National Corpus (CNC, versions
SYN200[056], 72.6%, Kocek et al. (2000)), (2)
a collection of web pages downloaded by Pavel
Pecina (Web, 17.1%), and (3) the Czech mono-
lingual data provided by WMT09 organizers
(10.3%). Table 1 lists sentence and token counts
(see Section 2.3 for the explanation of a- and t-
layer).
</bodyText>
<construct confidence="0.531979">
Sentences 52 M
with nonempty t-layer 51 M
a-nodes (i.e. tokens) 0.9 G
t-nodes 0.6 G
</construct>
<tableCaption confidence="0.985507">
Table 1: Czech monolingual training data.
</tableCaption>
<bodyText confidence="0.52314425">
The work on this project was supported by the grants
MSM0021620838, 1ET201120505, 1ET101120503, GAUK
52408/2008, MSMT CR LC536 and FP6-IST-5-034291-STP
(EuroMatrix).
</bodyText>
<subsectionHeader confidence="0.99982">
2.2 Parallel Data
</subsectionHeader>
<bodyText confidence="0.971703230769231">
As the source of parallel data we use an internal
release of Czech-English parallel corpus CzEng
(Bojar et al., 2008) extended with some additional
texts. One of the added sections was gathered
from two major websites containing Czech sub-
titles to movies and TV series1. The matching of
the Czech and English movies is rather straight-
forward thanks to the naming conventions. How-
ever, we were unable to reliably determine the se-
ries number and the episode number from the file
names. We employed a two-step procedure to au-
tomatically pair the TV series subtitle files. For
every TV series:
</bodyText>
<listItem confidence="0.553950666666667">
1. We clustered the files on both sides to remove
duplicates
2. We found the best matching using a provi-
sional translation dictionary. This proved to
be a successful technique on a small sample
of manually paired test data. The process was
facilitated by the fact that the correct pairs of
episodes usually share some named entities
which the human translator chose to keep in
</listItem>
<tableCaption confidence="0.4161055">
the original English form.
Table 2 lists parallel corpus sizes and the distri-
</tableCaption>
<table confidence="0.9746423">
bution of text domains.
English Czech
Sentences 6.91 M
with nonempty t-layer 6.89 M
a-nodes (i.e. tokens) 61 M 50 M
t-nodes 41 M 33 M
Distribution: [%] [%]
Subtitles 68.2 Novels 3.3
Software Docs 17.0 Commentaries/News 1.5
EU (Legal) Texts 9.5 Volunteer-supplied 0.4
</table>
<tableCaption confidence="0.98952">
Table 2: Czech-English data sizes and sources.
</tableCaption>
<footnote confidence="0.4042675">
1
www.opensubtitles.org and titulky.com
</footnote>
<page confidence="0.974637">
125
</page>
<subsubsectionHeader confidence="0.246586">
\x0c2.3 Data Preprocessing using TectoMT
</subsubsectionHeader>
<bodyText confidence="0.996457340425532">
platform: Analysis and Alignment
As we believe that various kinds of linguistically
relevant information might be helpful in MT, we
performed automatic analysis of the data. The
data were analyzed using the layered annotation
scheme of the Prague Dependency Treebank 2.0
(PDT 2.0, Hajic and others (2006)), i.e. we used
three layers of sentence representation: morpho-
logical layer, surface-syntax layer (called analyti-
cal (a-) layer), and deep-syntax layer (called tec-
togrammatical (t-) layer).
The analysis was implemented using TectoMT,
(Zabokrtsky et al., 2008). TectoMT is a highly
modular software framework aimed at creating
MT systems (focused, but by far not limited to
translation using tectogrammatical transfer) and
other NLP applications. Numerous existing NLP
tools such as taggers, parsers, and named entity
recognizers are already integrated in TectoMT, es-
pecially for (but again, not limited to) English and
Czech.
During the analysis of the large Czech mono-
lingual data, we used Jan Hajics Czech tagger
shipped with PDT 2.0, Maximum Spanning Tree
parser (McDonald et al., 2005) with optimized set
of features as described in Novak and Zabokrtsky
(2007), and a tool for assigning functors (seman-
tic roles) from Klimes (2006), and numerous other
components of our own (e.g. for conversion of an-
alytical trees into tectogrammatical ones).
In the parallel data, we analyzed the Czech side
using more or less the same scenario as used for
the monolingual data. English sentences were an-
alyzed using (among other tools) Morce tagger
Spoustova et al. (2007) and Maximum Spanning
Tree parser.2
The resulting deep syntactic (tectogrammatical)
Czech and English trees are then aligned using T-
alignera feature based greedy algorithm imple-
mented for this purpose (Marecek et al., 2008). T-
aligner finds corresponding nodes between the two
given trees and links them. For deciding whether
to link two nodes or not, T-aligner makes use of
a bilingual lexicon of tectogrammatical lemmas,
morphosyntactic similarities between the two can-
didate nodes, their positions in the trees and other
similarities between their parent/child nodes. It
</bodyText>
<page confidence="0.972886">
2
</page>
<bodyText confidence="0.998526076923077">
In some previous experiments (e.g.Zabokrtsky et al.
(2008)), we used phrase-structure parser Collins (1999) with
subsequent constituency-dependency conversion.
also uses word alignment generated from surface
shapes of sentences by GIZA++ tool, Och and Ney
(2003). We use acquired aligned tectogrammatical
trees for training some models for the transfer.
As analysis of such amounts of data is obvi-
ously computationally very demanding, we run it
in parallel using Sun Grid Engine3 cluster of 40
4-CPU computers. For this purpose, we imple-
mented a rather generic tool that submits any Tec-
toMT pipeline to the cluster.
</bodyText>
<sectionHeader confidence="0.99516" genericHeader="method">
3 Factored Phrase-Based MT
</sectionHeader>
<bodyText confidence="0.996597294117647">
We essentially repeat our experiments from last
year (Bojar and Hajic, 2008): GIZA++ align-
ments4 on a-layer lemmas (a-layer nodes corre-
spond 1-1 to surface tokens), symmetrized using
grow-diag-final (no -and) heuristic5.
Probably due to the domain difference (the test
set is news), including Subtitles in the parallel data
and Web in the monolingual data did not bring any
improvement that would justify the additional per-
formance costs. For most of the phrase-based ex-
periments, we thus used only 2.2M parallel sen-
tences (27M Czech and 32M English tokens) and
43M Czech sentences (694 M tokens).
In Table 3 below, we report the scores for the
following setups selected from about 50 experi-
ments we ran in total:
Moses T is a simple phrase-based translation (T)
with no additional factors. The translation is
performed on truecased word forms (i.e. sen-
tence capitalization removed unless the first
word seems to be a name). The 4-gram lan-
guage model is based on the 43M sentences.
Moses T+C is a factored setup with form-to-form
translation (T) and target-side morphological
coherence check following Bojar and Hajic
(2008). The setup uses two language mod-
els: 4-grams of word forms and 7-grams of
morphological tags.
Moses T+C+C&amp;T+T+G 84k is a setup desirable
from the linguistic point of view. Two in-
dependent translation paths are used: (1)
formform translation with two target-side
checks (lemma and tag generated from the
target-side form) as a fine-grained baseline
</bodyText>
<figure confidence="0.65662025">
3
http://gridengine.sunsource.net/
4
Default settings, IBM models and iterations: 15
</figure>
<page confidence="0.9164175">
33
43
</page>
<equation confidence="0.425413">
.
</equation>
<page confidence="0.805033">
5
</page>
<bodyText confidence="0.9984515">
Later, we found out that the grow-diag-final-and heuris-
tic provides insignificantly superior results.
</bodyText>
<page confidence="0.994467">
126
</page>
<bodyText confidence="0.997311266666667">
\x0cwith the option to resort to (2) an independent
translation of lemmalemma and tagtag
finished by a generation step that combines
target-side lemma and tag to produce the fi-
nal target-side form.
We use three language models in this setup
(3-grams of forms, 3-grams of lemmas, and
10-grams of tags).
Due to the increased complexity of the setup,
we were able to train this model on 84k par-
allel sentences only (the Commentaries sec-
tion) and we use the target-side of this small
training data for language models, too.
For all the setups we perform standard MERT
training on the provided development set.6
</bodyText>
<sectionHeader confidence="0.875064" genericHeader="method">
4 Translation Setup Based on
</sectionHeader>
<subsectionHeader confidence="0.388985">
Tectogrammatical Transfer
</subsectionHeader>
<bodyText confidence="0.9984124">
In this translation experiment, we follow the tradi-
tional analysis-transfer-synthesis approach, using
the set of PDT 2.0 layers: we analyze the input
English sentence up to the tectogrammatical layer
(through the morphological and analytical ones),
then perform the tectogrammatical transfer, and
then synthesize the target Czech sentence from its
tectogrammatical representation. The whole pro-
cedure consists of about 80 steps, so the following
description is necessarily very high level.
</bodyText>
<subsectionHeader confidence="0.998178">
4.1 Analysis
</subsectionHeader>
<bodyText confidence="0.999631117647059">
Each sentence is tokenized (roughly according to
the Penn Treebank conventions), tagged by the En-
glish version of the Morce tagger Spoustova et al.
(2007), and lemmatized by our lemmatizer. Then
the dependency parser (McDonald et al., 2005) is
applied. Then the analytical trees resulting from
the parser are converted to the tectogrammatical
ones (i.e. functional words are removed, only
morphologically indispensable categories are left
with the nodes using a sequence of heuristic proce-
dures). Unlike in PDT 2.0, the information about
the original syntactic form is stored with each t-
node (values such as v:inf for an infinitive verb
form, v:since+fin for the head of a subor-
dinate clause of a certain type, adj:attr for
an adjective in attribute position, n:for+X for a
given prepositional group are distinguished).
</bodyText>
<page confidence="0.982074">
6
</page>
<bodyText confidence="0.991551923076923">
We used the full development set of 2k sentences for
Moses T and a subset of 1k sentences for the other two
setups due to time constraints.
One of the steps in the analysis of English is
named entity recognition using Stanford Named
Entity Recognizer (Finkel et al., 2005). The nodes
in the English t-layer are grouped according to the
detected named entities and they are assigned the
type of entity (location, person, or organization).
This information is preserved in the transfer of the
deep English trees to the deep Czech trees to al-
low for the appropriate capitalization of the Czech
translation.
</bodyText>
<subsectionHeader confidence="0.972576">
4.2 Transfer
</subsectionHeader>
<bodyText confidence="0.988949484848485">
The transfer phase consists of the following steps:
Initiate the target-side (Czech) t-trees sim-
ply by cloning the source-side (English) t-
trees. Subsequent steps usually iterate over
all t-nodes. In the following, we denote a
source-side t-node as S and the correspond-
ing target-side node as T.
Translate formemes using
two probabilistic dictionaries
(p(T.formeme|S.formeme, S.parent.lemma)
and p(T.formeme|S.formeme)) and a few
manual rules. The formeme translation
probability estimates were extracted from a
part of the parallel data mentioned above.
Translate lemmas using a probabilistic dictio-
nary (p(T.lemma|S.lemma)) and a few rules
that ensure compatibility with the previously
chosen formeme. Again, this probabilistic
dictionary was obtained using the aligned
tectogrammatical trees from the parallel cor-
pus.
Fill the grammatemes (deep-syntactic equiv-
alent of morphological categories) gender
(for denotative nouns) and aspect (for verbs)
according to the chosen lemma. We also
fix grammateme values where the English-
Czech grammateme correspondence is non-
trivial (e.g. if an English gerund expression is
translated to Czech as a subordinating clause,
the tense grammateme has to be filled). How-
ever, the transfer of grammatemes is defi-
nitely much easier task than the transfer of
formemes and lemmas.
</bodyText>
<subsectionHeader confidence="0.999231">
4.3 Synthesis
</subsectionHeader>
<bodyText confidence="0.995501">
The transfer step yields an abstract deep
syntactico-semantical tree structure. Firstly,
</bodyText>
<page confidence="0.993092">
127
</page>
<bodyText confidence="0.97411268">
\x0cwe derive surface morphological categories
from their deep counterparts taking care of their
agreement where appropriate and we also remove
personal pronouns in subject positions (because
Czech is a pro-drop language).
To arrive at the surface tree structure, auxil-
iary nodes of several types are added, including
(1) reflexive particles, (2) prepositions, (3) subor-
dinating conjunctions, (4) modal verbs, (5) ver-
bal auxiliaries, and (6) punctuation nodes. Also,
grammar-based node ordering changes (imple-
mented by rules) are performed: e.g. if an English
possessive attribute is translated using Czech gen-
itive, it is shifted into post-modification position.
After finishing the inflection of nouns, verbs,
adjectives and adverbs (according to the values of
morphological categories derived from agreement
etc.), prepositions may need to be vocalized: the
vowel -e or -u is attached to the preposition if the
pronunciation of prepositional group would be dif-
ficult otherwise.
After the capitalization of the beginning of each
sentence (and each named entity instance), we ob-
tain the final translation by flattening the surface
tree.
</bodyText>
<subsectionHeader confidence="0.999751">
4.4 Preliminary Error Analysis
</subsectionHeader>
<bodyText confidence="0.999798">
According to our observations most errors happen
during the transfer of lemmas and formemes.
Usually, there are acceptable translations of
lemma and formeme in respective n-best lists
but we fail to choose the best one. The sce-
nario described in Section 4.2 uses quite a
primitive transfer algorithm where formemes
and lemmas are translated separately in two
steps. We hope that big improvements could
be achieved with more sophisticated algo-
rithms (optimizing the probability of the whole
tree) and smoothed probabilistic models (such
as p(T.lemma|S.lemma, T.parent.lemma) and
p(T.formeme|S.formeme, T.lemma, T.parent.lemma)).
Other common errors include:
Analysis: parsing (especially coordinations
are problematic with McDonalds parser).
Transfer: the translation of idioms and col-
locations, including named entities. In these
cases, the classical transfer at the t-layer
is not appropriate and utilization of some
phrase-based MT would help.
Synthesis: reflexive particles, word order.
</bodyText>
<sectionHeader confidence="0.983314" genericHeader="evaluation">
5 Experimental Results and Discussion
</sectionHeader>
<bodyText confidence="0.960077">
Table 3 reports lowercase BLEU and NIST scores
and preliminary manual ranks of our submissions
in contrast with other systems participating in
EnglishCzech translation, as evaluated on the
official WMT09 unseen test set. Note that auto-
matic metrics are known to correlate quite poorly
with human judgements, see the best ranking but
lower scoring PC Translator this year and also
</bodyText>
<table confidence="0.963908">
in Callison-Burch et al. (2008).
System BLEU NIST Rank
Moses T 14.24 5.175 -3.02 (4)
Moses T+C 13.86 5.110
Google 13.59 4.964 -2.82 (3)
U. of Edinburgh 13.55 5.039 -3.24 (5)
Moses T+C+C&amp;T+T+G 84k 10.01 4.360 -
Eurotran XP 09.51 4.381 -2.81 (2)
PC Translator 09.42 4.335 -2.77 (1)
TectoMT 07.29 4.173 -3.35 (6)
</table>
<tableCaption confidence="0.895059">
Table 3: Automatic scores and preliminary human
rank for EnglishCzech translation. Systems in
</tableCaption>
<bodyText confidence="0.998978315789474">
italics are provided for comparison only. Best re-
sults in bold.
Unfortunately, this preliminary evaluation sug-
gests that simpler models perform better, partly
because it is easier to tune them properly both
from computational point of view (e.g. MERT
not stable and prone to overfitting with more fea-
tures7), as well as from software engineering point
of view (debugging of complex pipelines of tools
is demanding). Moreover, simpler models run
faster: Moses T with 12 sents/minute is 4.6
times faster than Moses T+C. (Note that we have
not tuned either of the models for speed.)
While Moses T is probably nearly identical
setup as Google and Univ. of Edinburgh use,
the knowledge of correct language-dependent to-
kenization and the use of relatively high quality
large language model data seems to bring moder-
ate improvements.
</bodyText>
<sectionHeader confidence="0.997812" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998339857142857">
We described our experiments with a complex lin-
guistically motivated translation system and vari-
ous (again linguistically-motivated) setups of fac-
tored phrase-based translation. An automatic eval-
uation seems to suggest that simpler is better, but
we are well aware that a reliable judgement comes
only from human annotators.
</bodyText>
<page confidence="0.98076">
7
</page>
<bodyText confidence="0.9921835">
For Moses T+C+C&amp;T+T+G, we observed BLEU
scores on the test set varying by up to five points absolute
for various weight settings yielding nearly identical dev set
scores.
</bodyText>
<page confidence="0.968586">
128
</page>
<reference confidence="0.991575068181818">
\x0cReferences
Ondrej Bojar and Jan Hajic. 2008. Phrase-Based and
Deep Syntactic English-to-Czech Statistical Ma-
chine Translation. In Proceedings of the Third
Workshop on Statistical Machine Translation, pages
143146, Columbus, Ohio, June. Association for
Computational Linguistics.
Ondrej Bojar, Miroslav Jancek, Zdenek Zabokrtsky,
Pavel Ceska, and Peter Bena. 2008. CzEng 0.7:
Parallel Corpus with Community-Supplied Transla-
tions. In Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC08), Mar-
rakech, Morocco, May. ELRA.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70106, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL 05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 363370, Morristown, NJ, USA.
Association for Computational Linguistics.
Jan Hajic et al. 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T0 1, Philadelphia.
Vaclav Klimes. 2006. Analytical and Tectogrammat-
ical Analysis of a Natural Language. Ph.D. thesis,
Faculty of Mathematics and Physics, Charles Uni-
versity, Prague, Czech Rep.
Jan Kocek, Marie Koprivova, and Karel Kucera, edi-
tors. 2000. Cesky narodn korpus - uvod a prrucka
uzivatele. FF UK - UCNK, Praha.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
David Marecek, Zdenek Zabokrtsky, and Vaclav
Novak. 2008. Automatic Alignment of Czech and
English Deep Syntactic Dependency Trees. In Pro-
ceedings of European Machine Translation Confer-
ence (EAMT 08), pages 102111, Hamburg, Ger-
many.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency
parsing using spanning tree algorithms. In HLT
05: Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, pages 523530, Vancou-
ver, British Columbia, Canada.
Vaclav Novak and Zdenek Zabokrtsky. 2007. Feature
engineering in maximum spanning tree dependency
parser. In Vaclav Matousek and Pavel Mautner, ed-
itors, Lecture Notes in Artificial Intelligence, Pro-
ceedings of the 10th I nternational Conference on
Text, Speech and Dialogue, Lecture Notes in Com-
puter Science, pages 9298, Pilsen, Czech Repub-
lic. Springer Science+Business Media Deutschland
GmbH.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):1951.
Drahomra Spoustova, Jan Hajic, Jan Votrubec, Pavel
Krbec, and Pavel Kveton. 2007. The best of two
worlds: Cooperation of statistical and rule-based
taggers for czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 6774, Praha.
Zdenek Zabokrtsky, Jan Ptacek, and Petr Pajas. 2008.
TectoMT: Highly Modular Hybrid MT System
with Tectogrammatics Used as Transfer Layer. In
Proc. of the ACL Workshop on Statistical Machine
Translation, pages 167170, Columbus, Ohio, USA.
</reference>
<page confidence="0.973172">
129
</page>
<figure confidence="0.257362">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.241696">
<note confidence="0.98221625">b&amp;apos;Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 125129, Athens, Greece, 30 March 31 March 2009. c 2009 Association for Computational Linguistics English-Czech MT in 2008</note>
<author confidence="0.702379">Ondrej Bojar</author>
<author confidence="0.702379">David Marecek</author>
<author confidence="0.702379">Vaclav Novak</author>
<author confidence="0.702379">Martin Popel</author>
<author confidence="0.702379">Jan Ptacek</author>
<author confidence="0.702379">Jan Rous</author>
<author confidence="0.702379">Zdenek Zabokrtsky</author>
<affiliation confidence="0.988837">Charles University, Institute of Formal and Applied Linguistics</affiliation>
<address confidence="0.977655">Malostranske nam. 25, Praha 1, CZ-118 00, Czech Republic</address>
<email confidence="0.8836755">bojar@matfyz.cz</email>
<email confidence="0.8836755">marecek@matfyz.cz</email>
<email confidence="0.8836755">novak@matfyz.cz</email>
<email confidence="0.8836755">ptacek@matfyz.cz</email>
<email confidence="0.8836755">zabokrtsky}@ufal.mff.cuni.cz{popel@matfyz.cz</email>
<email confidence="0.8836755">jan.rous@matfyz.cz</email>
<abstract confidence="0.97854925">We describe two systems for English-to- Czech machine translation that took part in the WMT09 translation task. One of the systems is a tuned phrase-based system and the other one is based on a linguistically motivated analysis-transfer-synthesis approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences Ondrej Bojar</author>
<author>Jan Hajic</author>
</authors>
<title>Phrase-Based and Deep Syntactic English-to-Czech Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>143146</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="6022" citStr="Bojar and Hajic, 2008" startWordPosition="931" endWordPosition="934">ins (1999) with subsequent constituency-dependency conversion. also uses word alignment generated from surface shapes of sentences by GIZA++ tool, Och and Ney (2003). We use acquired aligned tectogrammatical trees for training some models for the transfer. As analysis of such amounts of data is obviously computationally very demanding, we run it in parallel using Sun Grid Engine3 cluster of 40 4-CPU computers. For this purpose, we implemented a rather generic tool that submits any TectoMT pipeline to the cluster. 3 Factored Phrase-Based MT We essentially repeat our experiments from last year (Bojar and Hajic, 2008): GIZA++ alignments4 on a-layer lemmas (a-layer nodes correspond 1-1 to surface tokens), symmetrized using grow-diag-final (no -and) heuristic5. Probably due to the domain difference (the test set is news), including Subtitles in the parallel data and Web in the monolingual data did not bring any improvement that would justify the additional performance costs. For most of the phrase-based experiments, we thus used only 2.2M parallel sentences (27M Czech and 32M English tokens) and 43M Czech sentences (694 M tokens). In Table 3 below, we report the scores for the following setups selected from </context>
</contexts>
<marker>Bojar, Hajic, 2008</marker>
<rawString>\x0cReferences Ondrej Bojar and Jan Hajic. 2008. Phrase-Based and Deep Syntactic English-to-Czech Statistical Machine Translation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 143146, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Miroslav Jancek</author>
<author>Zdenek Zabokrtsky</author>
<author>Pavel Ceska</author>
<author>Peter Bena</author>
</authors>
<title>CzEng 0.7: Parallel Corpus with Community-Supplied Translations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC08),</booktitle>
<publisher>ELRA.</publisher>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="1835" citStr="Bojar et al., 2008" startWordPosition="271" endWordPosition="274">l Pecina (Web, 17.1%), and (3) the Czech monolingual data provided by WMT09 organizers (10.3%). Table 1 lists sentence and token counts (see Section 2.3 for the explanation of a- and tlayer). Sentences 52 M with nonempty t-layer 51 M a-nodes (i.e. tokens) 0.9 G t-nodes 0.6 G Table 1: Czech monolingual training data. The work on this project was supported by the grants MSM0021620838, 1ET201120505, 1ET101120503, GAUK 52408/2008, MSMT CR LC536 and FP6-IST-5-034291-STP (EuroMatrix). 2.2 Parallel Data As the source of parallel data we use an internal release of Czech-English parallel corpus CzEng (Bojar et al., 2008) extended with some additional texts. One of the added sections was gathered from two major websites containing Czech subtitles to movies and TV series1. The matching of the Czech and English movies is rather straightforward thanks to the naming conventions. However, we were unable to reliably determine the series number and the episode number from the file names. We employed a two-step procedure to automatically pair the TV series subtitle files. For every TV series: 1. We clustered the files on both sides to remove duplicates 2. We found the best matching using a provisional translation dict</context>
</contexts>
<marker>Bojar, Jancek, Zabokrtsky, Ceska, Bena, 2008</marker>
<rawString>Ondrej Bojar, Miroslav Jancek, Zdenek Zabokrtsky, Pavel Ceska, and Peter Bena. 2008. CzEng 0.7: Parallel Corpus with Community-Supplied Translations. In Proceedings of the Sixth International Language Resources and Evaluation (LREC08), Marrakech, Morocco, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>70106</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="14251" citStr="Callison-Burch et al. (2008)" startWordPosition="2187" endWordPosition="2190">entities. In these cases, the classical transfer at the t-layer is not appropriate and utilization of some phrase-based MT would help. Synthesis: reflexive particles, word order. 5 Experimental Results and Discussion Table 3 reports lowercase BLEU and NIST scores and preliminary manual ranks of our submissions in contrast with other systems participating in EnglishCzech translation, as evaluated on the official WMT09 unseen test set. Note that automatic metrics are known to correlate quite poorly with human judgements, see the best ranking but lower scoring PC Translator this year and also in Callison-Burch et al. (2008). System BLEU NIST Rank Moses T 14.24 5.175 -3.02 (4) Moses T+C 13.86 5.110 Google 13.59 4.964 -2.82 (3) U. of Edinburgh 13.55 5.039 -3.24 (5) Moses T+C+C&amp;T+T+G 84k 10.01 4.360 - Eurotran XP 09.51 4.381 -2.81 (2) PC Translator 09.42 4.335 -2.77 (1) TectoMT 07.29 4.173 -3.35 (6) Table 3: Automatic scores and preliminary human rank for EnglishCzech translation. Systems in italics are provided for comparison only. Best results in bold. Unfortunately, this preliminary evaluation suggests that simpler models perform better, partly because it is easier to tune them properly both from computational p</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 70106, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="5410" citStr="Collins (1999)" startWordPosition="837" endWordPosition="838">ctogrammatical) Czech and English trees are then aligned using Talignera feature based greedy algorithm implemented for this purpose (Marecek et al., 2008). Taligner finds corresponding nodes between the two given trees and links them. For deciding whether to link two nodes or not, T-aligner makes use of a bilingual lexicon of tectogrammatical lemmas, morphosyntactic similarities between the two candidate nodes, their positions in the trees and other similarities between their parent/child nodes. It 2 In some previous experiments (e.g.Zabokrtsky et al. (2008)), we used phrase-structure parser Collins (1999) with subsequent constituency-dependency conversion. also uses word alignment generated from surface shapes of sentences by GIZA++ tool, Och and Ney (2003). We use acquired aligned tectogrammatical trees for training some models for the transfer. As analysis of such amounts of data is obviously computationally very demanding, we run it in parallel using Sun Grid Engine3 cluster of 40 4-CPU computers. For this purpose, we implemented a rather generic tool that submits any TectoMT pipeline to the cluster. 3 Factored Phrase-Based MT We essentially repeat our experiments from last year (Bojar and </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL 05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9879" citStr="Finkel et al., 2005" startWordPosition="1545" endWordPosition="1548">heuristic procedures). Unlike in PDT 2.0, the information about the original syntactic form is stored with each tnode (values such as v:inf for an infinitive verb form, v:since+fin for the head of a subordinate clause of a certain type, adj:attr for an adjective in attribute position, n:for+X for a given prepositional group are distinguished). 6 We used the full development set of 2k sentences for Moses T and a subset of 1k sentences for the other two setups due to time constraints. One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer (Finkel et al., 2005). The nodes in the English t-layer are grouped according to the detected named entities and they are assigned the type of entity (location, person, or organization). This information is preserved in the transfer of the deep English trees to the deep Czech trees to allow for the appropriate capitalization of the Czech translation. 4.2 Transfer The transfer phase consists of the following steps: Initiate the target-side (Czech) t-trees simply by cloning the source-side (English) ttrees. Subsequent steps usually iterate over all t-nodes. In the following, we denote a source-side t-node as S and t</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In ACL 05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363370, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajic</author>
</authors>
<date>2006</date>
<booktitle>Prague Dependency Treebank 2.0. CD-ROM, Linguistic Data Consortium, LDC Catalog No.: LDC2006T0 1,</booktitle>
<location>Philadelphia.</location>
<marker>Hajic, 2006</marker>
<rawString>Jan Hajic et al. 2006. Prague Dependency Treebank 2.0. CD-ROM, Linguistic Data Consortium, LDC Catalog No.: LDC2006T0 1, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vaclav Klimes</author>
</authors>
<title>Analytical and Tectogrammatical Analysis of a Natural Language.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Faculty of Mathematics and Physics, Charles University,</institution>
<location>Prague, Czech Rep.</location>
<contexts>
<context position="4401" citStr="Klimes (2006)" startWordPosition="682" endWordPosition="683">ing MT systems (focused, but by far not limited to translation using tectogrammatical transfer) and other NLP applications. Numerous existing NLP tools such as taggers, parsers, and named entity recognizers are already integrated in TectoMT, especially for (but again, not limited to) English and Czech. During the analysis of the large Czech monolingual data, we used Jan Hajics Czech tagger shipped with PDT 2.0, Maximum Spanning Tree parser (McDonald et al., 2005) with optimized set of features as described in Novak and Zabokrtsky (2007), and a tool for assigning functors (semantic roles) from Klimes (2006), and numerous other components of our own (e.g. for conversion of analytical trees into tectogrammatical ones). In the parallel data, we analyzed the Czech side using more or less the same scenario as used for the monolingual data. English sentences were analyzed using (among other tools) Morce tagger Spoustova et al. (2007) and Maximum Spanning Tree parser.2 The resulting deep syntactic (tectogrammatical) Czech and English trees are then aligned using Talignera feature based greedy algorithm implemented for this purpose (Marecek et al., 2008). Taligner finds corresponding nodes between the t</context>
</contexts>
<marker>Klimes, 2006</marker>
<rawString>Vaclav Klimes. 2006. Analytical and Tectogrammatical Analysis of a Natural Language. Ph.D. thesis, Faculty of Mathematics and Physics, Charles University, Prague, Czech Rep.</rawString>
</citation>
<citation valid="true">
<title>Cesky narodn korpus - uvod a prrucka uzivatele.</title>
<date>2000</date>
<editor>Jan Kocek, Marie Koprivova, and Karel Kucera, editors.</editor>
<location>Praha.</location>
<contexts>
<context position="1165" citStr="(2000)" startWordPosition="166" endWordPosition="166">chine translation that took part in the WMT09 translation task. One of the systems is a tuned phrase-based system and the other one is based on a linguistically motivated analysis-transfer-synthesis approach. 1 Introduction We participated in WMT09 with two very different systems: (1) a phrase-based MT based on Moses (Koehn et al., 2007) and tuned for EnglishCzech translation, and (2) a complex system in the TectoMT platform (Zabokrtsky et al., 2008). 2 Data 2.1 Monolingual Data Our Czech monolingual data consist of (1) the Czech National Corpus (CNC, versions SYN200[056], 72.6%, Kocek et al. (2000)), (2) a collection of web pages downloaded by Pavel Pecina (Web, 17.1%), and (3) the Czech monolingual data provided by WMT09 organizers (10.3%). Table 1 lists sentence and token counts (see Section 2.3 for the explanation of a- and tlayer). Sentences 52 M with nonempty t-layer 51 M a-nodes (i.e. tokens) 0.9 G t-nodes 0.6 G Table 1: Czech monolingual training data. The work on this project was supported by the grants MSM0021620838, 1ET201120505, 1ET101120503, GAUK 52408/2008, MSMT CR LC536 and FP6-IST-5-034291-STP (EuroMatrix). 2.2 Parallel Data As the source of parallel data we use an intern</context>
</contexts>
<marker>2000</marker>
<rawString>Jan Kocek, Marie Koprivova, and Karel Kucera, editors. 2000. Cesky narodn korpus - uvod a prrucka uzivatele. FF UK - UCNK, Praha.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="898" citStr="Koehn et al., 2007" startWordPosition="121" endWordPosition="124">, Zdenek Zabokrtsky Charles University, Institute of Formal and Applied Linguistics Malostranske nam. 25, Praha 1, CZ-118 00, Czech Republic {bojar,marecek,novak,ptacek,zabokrtsky}@ufal.mff.cuni.cz {popel,jan.rous}@matfyz.cz Abstract We describe two systems for English-toCzech machine translation that took part in the WMT09 translation task. One of the systems is a tuned phrase-based system and the other one is based on a linguistically motivated analysis-transfer-synthesis approach. 1 Introduction We participated in WMT09 with two very different systems: (1) a phrase-based MT based on Moses (Koehn et al., 2007) and tuned for EnglishCzech translation, and (2) a complex system in the TectoMT platform (Zabokrtsky et al., 2008). 2 Data 2.1 Monolingual Data Our Czech monolingual data consist of (1) the Czech National Corpus (CNC, versions SYN200[056], 72.6%, Kocek et al. (2000)), (2) a collection of web pages downloaded by Pavel Pecina (Web, 17.1%), and (3) the Czech monolingual data provided by WMT09 organizers (10.3%). Table 1 lists sentence and token counts (see Section 2.3 for the explanation of a- and tlayer). Sentences 52 M with nonempty t-layer 51 M a-nodes (i.e. tokens) 0.9 G t-nodes 0.6 G Table </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>In ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker></marker>
<rawString>In ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Marecek</author>
<author>Zdenek Zabokrtsky</author>
<author>Vaclav Novak</author>
</authors>
<title>Automatic Alignment of Czech and English Deep Syntactic Dependency Trees.</title>
<date>2008</date>
<booktitle>In Proceedings of European Machine Translation Conference (EAMT 08),</booktitle>
<pages>102111</pages>
<location>Hamburg, Germany.</location>
<contexts>
<context position="4951" citStr="Marecek et al., 2008" startWordPosition="767" endWordPosition="770">, and a tool for assigning functors (semantic roles) from Klimes (2006), and numerous other components of our own (e.g. for conversion of analytical trees into tectogrammatical ones). In the parallel data, we analyzed the Czech side using more or less the same scenario as used for the monolingual data. English sentences were analyzed using (among other tools) Morce tagger Spoustova et al. (2007) and Maximum Spanning Tree parser.2 The resulting deep syntactic (tectogrammatical) Czech and English trees are then aligned using Talignera feature based greedy algorithm implemented for this purpose (Marecek et al., 2008). Taligner finds corresponding nodes between the two given trees and links them. For deciding whether to link two nodes or not, T-aligner makes use of a bilingual lexicon of tectogrammatical lemmas, morphosyntactic similarities between the two candidate nodes, their positions in the trees and other similarities between their parent/child nodes. It 2 In some previous experiments (e.g.Zabokrtsky et al. (2008)), we used phrase-structure parser Collins (1999) with subsequent constituency-dependency conversion. also uses word alignment generated from surface shapes of sentences by GIZA++ tool, Och </context>
</contexts>
<marker>Marecek, Zabokrtsky, Novak, 2008</marker>
<rawString>David Marecek, Zdenek Zabokrtsky, and Vaclav Novak. 2008. Automatic Alignment of Czech and English Deep Syntactic Dependency Trees. In Proceedings of European Machine Translation Conference (EAMT 08), pages 102111, Hamburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In HLT 05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>523530</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="4255" citStr="McDonald et al., 2005" startWordPosition="656" endWordPosition="659">ammatical (t-) layer). The analysis was implemented using TectoMT, (Zabokrtsky et al., 2008). TectoMT is a highly modular software framework aimed at creating MT systems (focused, but by far not limited to translation using tectogrammatical transfer) and other NLP applications. Numerous existing NLP tools such as taggers, parsers, and named entity recognizers are already integrated in TectoMT, especially for (but again, not limited to) English and Czech. During the analysis of the large Czech monolingual data, we used Jan Hajics Czech tagger shipped with PDT 2.0, Maximum Spanning Tree parser (McDonald et al., 2005) with optimized set of features as described in Novak and Zabokrtsky (2007), and a tool for assigning functors (semantic roles) from Klimes (2006), and numerous other components of our own (e.g. for conversion of analytical trees into tectogrammatical ones). In the parallel data, we analyzed the Czech side using more or less the same scenario as used for the monolingual data. English sentences were analyzed using (among other tools) Morce tagger Spoustova et al. (2007) and Maximum Spanning Tree parser.2 The resulting deep syntactic (tectogrammatical) Czech and English trees are then aligned us</context>
<context position="9025" citStr="McDonald et al., 2005" startWordPosition="1404" endWordPosition="1407">layers: we analyze the input English sentence up to the tectogrammatical layer (through the morphological and analytical ones), then perform the tectogrammatical transfer, and then synthesize the target Czech sentence from its tectogrammatical representation. The whole procedure consists of about 80 steps, so the following description is necessarily very high level. 4.1 Analysis Each sentence is tokenized (roughly according to the Penn Treebank conventions), tagged by the English version of the Morce tagger Spoustova et al. (2007), and lemmatized by our lemmatizer. Then the dependency parser (McDonald et al., 2005) is applied. Then the analytical trees resulting from the parser are converted to the tectogrammatical ones (i.e. functional words are removed, only morphologically indispensable categories are left with the nodes using a sequence of heuristic procedures). Unlike in PDT 2.0, the information about the original syntactic form is stored with each tnode (values such as v:inf for an infinitive verb form, v:since+fin for the head of a subordinate clause of a certain type, adj:attr for an adjective in attribute position, n:for+X for a given prepositional group are distinguished). 6 We used the full d</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In HLT 05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 523530, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vaclav Novak</author>
<author>Zdenek Zabokrtsky</author>
</authors>
<title>Feature engineering in maximum spanning tree dependency parser.</title>
<date>2007</date>
<booktitle>Lecture Notes in Artificial Intelligence, Proceedings of the 10th I nternational Conference on Text, Speech and Dialogue, Lecture Notes in Computer Science,</booktitle>
<pages>9298</pages>
<editor>In Vaclav Matousek and Pavel Mautner, editors,</editor>
<publisher>Springer Science+Business Media Deutschland GmbH.</publisher>
<location>Pilsen, Czech Republic.</location>
<contexts>
<context position="4330" citStr="Novak and Zabokrtsky (2007)" startWordPosition="668" endWordPosition="671">bokrtsky et al., 2008). TectoMT is a highly modular software framework aimed at creating MT systems (focused, but by far not limited to translation using tectogrammatical transfer) and other NLP applications. Numerous existing NLP tools such as taggers, parsers, and named entity recognizers are already integrated in TectoMT, especially for (but again, not limited to) English and Czech. During the analysis of the large Czech monolingual data, we used Jan Hajics Czech tagger shipped with PDT 2.0, Maximum Spanning Tree parser (McDonald et al., 2005) with optimized set of features as described in Novak and Zabokrtsky (2007), and a tool for assigning functors (semantic roles) from Klimes (2006), and numerous other components of our own (e.g. for conversion of analytical trees into tectogrammatical ones). In the parallel data, we analyzed the Czech side using more or less the same scenario as used for the monolingual data. English sentences were analyzed using (among other tools) Morce tagger Spoustova et al. (2007) and Maximum Spanning Tree parser.2 The resulting deep syntactic (tectogrammatical) Czech and English trees are then aligned using Talignera feature based greedy algorithm implemented for this purpose (</context>
</contexts>
<marker>Novak, Zabokrtsky, 2007</marker>
<rawString>Vaclav Novak and Zdenek Zabokrtsky. 2007. Feature engineering in maximum spanning tree dependency parser. In Vaclav Matousek and Pavel Mautner, editors, Lecture Notes in Artificial Intelligence, Proceedings of the 10th I nternational Conference on Text, Speech and Dialogue, Lecture Notes in Computer Science, pages 9298, Pilsen, Czech Republic. Springer Science+Business Media Deutschland GmbH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="5565" citStr="Och and Ney (2003)" startWordPosition="856" endWordPosition="859">008). Taligner finds corresponding nodes between the two given trees and links them. For deciding whether to link two nodes or not, T-aligner makes use of a bilingual lexicon of tectogrammatical lemmas, morphosyntactic similarities between the two candidate nodes, their positions in the trees and other similarities between their parent/child nodes. It 2 In some previous experiments (e.g.Zabokrtsky et al. (2008)), we used phrase-structure parser Collins (1999) with subsequent constituency-dependency conversion. also uses word alignment generated from surface shapes of sentences by GIZA++ tool, Och and Ney (2003). We use acquired aligned tectogrammatical trees for training some models for the transfer. As analysis of such amounts of data is obviously computationally very demanding, we run it in parallel using Sun Grid Engine3 cluster of 40 4-CPU computers. For this purpose, we implemented a rather generic tool that submits any TectoMT pipeline to the cluster. 3 Factored Phrase-Based MT We essentially repeat our experiments from last year (Bojar and Hajic, 2008): GIZA++ alignments4 on a-layer lemmas (a-layer nodes correspond 1-1 to surface tokens), symmetrized using grow-diag-final (no -and) heuristic5</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):1951.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahomra Spoustova</author>
<author>Jan Hajic</author>
<author>Jan Votrubec</author>
<author>Pavel Krbec</author>
<author>Pavel Kveton</author>
</authors>
<title>The best of two worlds: Cooperation of statistical and rule-based taggers for czech.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing, ACL</booktitle>
<pages>6774</pages>
<location>Praha.</location>
<contexts>
<context position="4728" citStr="Spoustova et al. (2007)" startWordPosition="734" endWordPosition="737">alysis of the large Czech monolingual data, we used Jan Hajics Czech tagger shipped with PDT 2.0, Maximum Spanning Tree parser (McDonald et al., 2005) with optimized set of features as described in Novak and Zabokrtsky (2007), and a tool for assigning functors (semantic roles) from Klimes (2006), and numerous other components of our own (e.g. for conversion of analytical trees into tectogrammatical ones). In the parallel data, we analyzed the Czech side using more or less the same scenario as used for the monolingual data. English sentences were analyzed using (among other tools) Morce tagger Spoustova et al. (2007) and Maximum Spanning Tree parser.2 The resulting deep syntactic (tectogrammatical) Czech and English trees are then aligned using Talignera feature based greedy algorithm implemented for this purpose (Marecek et al., 2008). Taligner finds corresponding nodes between the two given trees and links them. For deciding whether to link two nodes or not, T-aligner makes use of a bilingual lexicon of tectogrammatical lemmas, morphosyntactic similarities between the two candidate nodes, their positions in the trees and other similarities between their parent/child nodes. It 2 In some previous experime</context>
<context position="8939" citStr="Spoustova et al. (2007)" startWordPosition="1391" endWordPosition="1394"> follow the traditional analysis-transfer-synthesis approach, using the set of PDT 2.0 layers: we analyze the input English sentence up to the tectogrammatical layer (through the morphological and analytical ones), then perform the tectogrammatical transfer, and then synthesize the target Czech sentence from its tectogrammatical representation. The whole procedure consists of about 80 steps, so the following description is necessarily very high level. 4.1 Analysis Each sentence is tokenized (roughly according to the Penn Treebank conventions), tagged by the English version of the Morce tagger Spoustova et al. (2007), and lemmatized by our lemmatizer. Then the dependency parser (McDonald et al., 2005) is applied. Then the analytical trees resulting from the parser are converted to the tectogrammatical ones (i.e. functional words are removed, only morphologically indispensable categories are left with the nodes using a sequence of heuristic procedures). Unlike in PDT 2.0, the information about the original syntactic form is stored with each tnode (values such as v:inf for an infinitive verb form, v:since+fin for the head of a subordinate clause of a certain type, adj:attr for an adjective in attribute posi</context>
</contexts>
<marker>Spoustova, Hajic, Votrubec, Krbec, Kveton, 2007</marker>
<rawString>Drahomra Spoustova, Jan Hajic, Jan Votrubec, Pavel Krbec, and Pavel Kveton. 2007. The best of two worlds: Cooperation of statistical and rule-based taggers for czech. In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing, ACL 2007, pages 6774, Praha.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zdenek Zabokrtsky</author>
<author>Jan Ptacek</author>
<author>Petr Pajas</author>
</authors>
<title>TectoMT: Highly Modular Hybrid MT System with Tectogrammatics Used as Transfer Layer.</title>
<date>2008</date>
<booktitle>In Proc. of the ACL Workshop on Statistical Machine Translation,</booktitle>
<pages>167170</pages>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="1013" citStr="Zabokrtsky et al., 2008" startWordPosition="139" endWordPosition="142">a 1, CZ-118 00, Czech Republic {bojar,marecek,novak,ptacek,zabokrtsky}@ufal.mff.cuni.cz {popel,jan.rous}@matfyz.cz Abstract We describe two systems for English-toCzech machine translation that took part in the WMT09 translation task. One of the systems is a tuned phrase-based system and the other one is based on a linguistically motivated analysis-transfer-synthesis approach. 1 Introduction We participated in WMT09 with two very different systems: (1) a phrase-based MT based on Moses (Koehn et al., 2007) and tuned for EnglishCzech translation, and (2) a complex system in the TectoMT platform (Zabokrtsky et al., 2008). 2 Data 2.1 Monolingual Data Our Czech monolingual data consist of (1) the Czech National Corpus (CNC, versions SYN200[056], 72.6%, Kocek et al. (2000)), (2) a collection of web pages downloaded by Pavel Pecina (Web, 17.1%), and (3) the Czech monolingual data provided by WMT09 organizers (10.3%). Table 1 lists sentence and token counts (see Section 2.3 for the explanation of a- and tlayer). Sentences 52 M with nonempty t-layer 51 M a-nodes (i.e. tokens) 0.9 G t-nodes 0.6 G Table 1: Czech monolingual training data. The work on this project was supported by the grants MSM0021620838, 1ET20112050</context>
<context position="3725" citStr="Zabokrtsky et al., 2008" startWordPosition="573" endWordPosition="576">nd titulky.com 125 \x0c2.3 Data Preprocessing using TectoMT platform: Analysis and Alignment As we believe that various kinds of linguistically relevant information might be helpful in MT, we performed automatic analysis of the data. The data were analyzed using the layered annotation scheme of the Prague Dependency Treebank 2.0 (PDT 2.0, Hajic and others (2006)), i.e. we used three layers of sentence representation: morphological layer, surface-syntax layer (called analytical (a-) layer), and deep-syntax layer (called tectogrammatical (t-) layer). The analysis was implemented using TectoMT, (Zabokrtsky et al., 2008). TectoMT is a highly modular software framework aimed at creating MT systems (focused, but by far not limited to translation using tectogrammatical transfer) and other NLP applications. Numerous existing NLP tools such as taggers, parsers, and named entity recognizers are already integrated in TectoMT, especially for (but again, not limited to) English and Czech. During the analysis of the large Czech monolingual data, we used Jan Hajics Czech tagger shipped with PDT 2.0, Maximum Spanning Tree parser (McDonald et al., 2005) with optimized set of features as described in Novak and Zabokrtsky (</context>
<context position="5361" citStr="Zabokrtsky et al. (2008)" startWordPosition="829" endWordPosition="832">mum Spanning Tree parser.2 The resulting deep syntactic (tectogrammatical) Czech and English trees are then aligned using Talignera feature based greedy algorithm implemented for this purpose (Marecek et al., 2008). Taligner finds corresponding nodes between the two given trees and links them. For deciding whether to link two nodes or not, T-aligner makes use of a bilingual lexicon of tectogrammatical lemmas, morphosyntactic similarities between the two candidate nodes, their positions in the trees and other similarities between their parent/child nodes. It 2 In some previous experiments (e.g.Zabokrtsky et al. (2008)), we used phrase-structure parser Collins (1999) with subsequent constituency-dependency conversion. also uses word alignment generated from surface shapes of sentences by GIZA++ tool, Och and Ney (2003). We use acquired aligned tectogrammatical trees for training some models for the transfer. As analysis of such amounts of data is obviously computationally very demanding, we run it in parallel using Sun Grid Engine3 cluster of 40 4-CPU computers. For this purpose, we implemented a rather generic tool that submits any TectoMT pipeline to the cluster. 3 Factored Phrase-Based MT We essentially </context>
</contexts>
<marker>Zabokrtsky, Ptacek, Pajas, 2008</marker>
<rawString>Zdenek Zabokrtsky, Jan Ptacek, and Petr Pajas. 2008. TectoMT: Highly Modular Hybrid MT System with Tectogrammatics Used as Transfer Layer. In Proc. of the ACL Workshop on Statistical Machine Translation, pages 167170, Columbus, Ohio, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>