 Assuming that each xt j, then the posterior distribution for j is Dirichlet with vector parameter nj +0 CITATION,,
 This is common practice for this task, as the desired number of segments may be determined by the user CITATION,,
 The most commonly-used sampling-based technique is Gibbs sampling, which iteratively samples from the conditional distribution of each hidden variable CITATION,,
 This is related to topicmodeling methods such as latent Dirichlet allocation (LDA; CITATION), but here the induced topics are tied to a linear discourse structure,,
 3 Lexical Cohesion in a Bayesian Framework The core idea of lexical cohesion is that topicallycoherent segments demonstrate compact and consistent lexical distributions CITATION,,
 This is similar in spirit to hidden topic models such as latent Dirichlet allocation CITATION, but rather than assigning a hidden topic to each word, we constrain the topics to yield a linear segmentation of the document,,
ure transcript corpus described by CITATION,,
 The CITATION stemming algorithm is applied to group equivalent lexical items,,
 A set of stop-words is also removed, using the same list originally employed by several competitive systems (CITATION; Textbook Pk WD U&I ,,
is topic, see CITATION,,
 CITATION were the first to investigate the relationship between cue phrases and linear segmentation,,
 CITATION specify a list of cue phrases by hand; the cue phrases are used as a feature in a maximum-entropy classifier for conversation disentanglement,,
 3 Lexical Cohesion in a Bayesian Framework The core idea of lexical cohesion is that topicallycoherent segments demonstrate compact and consistent lexical distributions CITATION,,
 We can re-estimate this prior based on the observed data by interleaving gradient-based search in a Viterbi expectation-maximization framework CITATION,,
 The maximization is performed using a gradient-based search; the gradients are dervied by CITATION,,
including the benchmark ICSI meeting dataset CITATION and a new textual corpus constructed from the contents of a medical textbook,,
, CITATION),,
 In addition, we show that the benchmark segmentation system of CITATION can be viewed as another special case of our Bayesian model,,
, CITATION; CITATION),,
, CITATION; Galley et al,,
2003; CITATION), and is currently the dominant approach to unsupervised topic segmentation,,
 For example, consider cue phrases, which are explicit discourse markers such as now or however (CITATION; CITATION; CITATION),,
 Cue phrases have been shown to be a useful feature for supervised topic segmentation (CITATION; Galley et al,,
 One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (CITATION; CITATION),,
 We are especially interested in cue phrases, which are explicit markers for discourse structure, such as now or first (CITATION; CITATION; CITATION),,
 335 \x0cThe relationship between discourse structure and cue phrases has been studied extensively; for an early example of computational work on this topic, see CITATION,,
 CITATION were the first to investigate the relationship between cue phrases and linear segmentation,,
 CITATION ,,
 CITATION specify a list of cue phrases by hand; the cue phrases are used as a feature in a maximum-entropy classifier for conversation disentanglement,,
 3 Lexical Cohesion in a Bayesian Framework The core idea of lexical cohesion is that topicallycoherent segments demonstrate compact and consistent lexical distributions CITATION,,
 This is similar in spirit to hidden topic models such as latent Dirichlet allocation CITATION, but rather than assigning a hidden topic to each word, we constrain the topics to yield a linear segmentation of the document,,
 For example, consider cue phrases, which are explicit discourse markers such as now or however (CITATION; CITATION; CITATION),,
 Cue phrases have been shown to be a useful feature for supervised topic segmentation (CITATION; Galley et al,,
 One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (CITATION; CITATION),,
 Moreover, such hand-crafted metrics may not generalize well across multiple datasets, and often include parameters which must be tuned on development sets (CITATION; Galley et al,,
, CITATION; Galley et al,,
2003; CITATION), and is currently the dominant approach to unsupervised topic segmentation,,
 For example, consider cue phrases, which are explicit discourse markers such as now or however (CITATION; CITATION; CITATION),,
 Cue phrases have been shown to be a useful feature for supervised topic segmentation (CITATION; Galley et al,,
 One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (CITATION; CITATION),,
 We are especially interested in cue phrases, which are explicit markers for discourse structure, such as now or first (CITATION; CITATION; CITATION),,
 We use a list of cue phrases identified by CITATION,,
 We evaluate our algorithm on corpora of spoken and written language, including the benchmark ICSI meeting dataset CITATION and a new textual corpus constructed from the contents of a medical textbook,,
, CITATION),,
 Interleaving samplingbased inference with direct optimization of parameters can be considered a form of Monte Carlo Expectation-Maximization (MCEM; CITATION),,
 For multi-speaker meetings, we use the ICSI corpus of meeting transcripts CITATION, which is becoming a standard for speech segmentation (e,,
 2003; CITATION),,
 However, point estimates are theoretically unsatisfying from a Bayesian perspective, and better performance may be obtained by marginalizing over all possible lan337 \x0cguage models: p(X|z, 0) = K Y j Y {t:zt=j} p(xt|0) = K Y j Z dj Y {t:zt=j} p(xt|j)p(j|0) = K Y j pdcm({xt : zt = j}|0), (7) where pdcm refers to the Dirichlet compound multinomial distribution (DCM), also known as the multivariate Polya distribution CITATION,,
, CITATION; Galley et al,,
2003; CITATION), and is currently the dominant approach to unsupervised topic segmentation,,
 For example, consider cue phrases, which are explicit discourse markers such as now or however (CITATION; CITATION; CITATION),,
 Cue phrases have been shown to be a useful feature for supervised topic segmentation (CITATION; Galley et al,,
 One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (CITATION; CITATION),,
 We are especially interested in cue phrases, which are explicit markers for discourse structure, such as now or first (CITATION; CITATION; CITATION),,
, 2003; CITATION), we consider only the first word of each sentence as a potential cue phrase; thus, we set ` = 1 in all experiments,,
, CITATION; Galley et al,,
2003; CITATION), and is currently the dominant approach to unsupervised topic segmentation,,
 For example, consider cue phrases, which are explicit discourse markers such as now or however (CITATION; CITATION; CITATION),,
 Cue phrases have been shown to be a useful feature for supervised topic segmentation (CITATION; Galley et al,,
 In addition, we show that the benchmark segmentation system of CITATION can be viewed as another special case of our Bayesian model,,
 CITATION optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences,,
 Most similar to our work is the approach of CITATION, who search for segmentations with compact language models; as shown in Section 3,,
 An alternative Bayesian approach to segmentation was proposed by CITATION,,
sterior distribution for j is Dirichlet with vector parameter nj +0 CITATION,,
 This is common practice for this task, as the desired number of segments may be determined by the user CITATION,,
1 Connection to previous work In this section, we explain how our model generalizes the well-known method of CITATION; hereafter U&I),,
 1999) and WindowDiff (WD) CITATION scores,,
 We use the evaluation source code provided by CITATION,,
 Baselines We compare against three competitive alternative systems from the literature: U&I CITATION; LCSEG (Galley et al,,
, 2003); MCS CITATION,,
 Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by CITATION,,
 CITATION; CITATION),,
1 Connection to previous work In this section, we explain how our model generalizes the well-known method of CITATION; hereafter U&I),,
 This form is equivalent to Laplacian smoothing CITATION, and is a special case of our equation 2, with 0 = 1,,
 We can re-estimate this prior based on the observed data by interleaving gradient-based search in a Viterbi expectation-maximization framework CITATION,,
 The maximization is performed using a gradient-based search; the gradients are dervied by CITATION,,
2003; CITATION), and is currently the dominant approach to unsupervised topic segmentation,,
 For example, consider cue phrases, which are explicit discourse markers such as now or however (CITATION; CITATION; CITATION),,
 Cue phrases have been shown to be a useful feature for supervised topic segmentation (CITATION; Galley et al,,
 One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (CITATION; CITATION),,
 Moreover, such hand-crafted metrics may not generalize well across multiple datasets, and often include parameters which must be tuned on development sets (CITATION; Galley et al,,
 335 \x0cThe relationship between discourse structure and cue phrases has been studied extensively; for an early example of computational work on this topic, see CITATION,,
 CITATION were the first to investigate the relationship between cue phrases and linear segmentation,,
 CITATION specify a list of cue phrases ,,
, 1999) and WindowDiff (WD) CITATION scores,,
 We use the evaluation source code provided by CITATION,,
 Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by CITATION,,
 The CITATION stemming algorithm is applied to group equivalent lexical items,,
 A set of stop-words is also removed, using the same list originally employed by several competitive systems (CITATION; Textbook Pk WD U&I ,,
 CITATION optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences,,
 Most similar to our work is the approach of CITATION, who search for segmentations with compact language models; as shown in Section 3,,
 An alternative Bayesian approach to segmentation was proposed by CITATION,,
 Interleaving samplingbased inference with direct optimization of parameters can be considered a form of Monte Carlo Expectation-Maximization (MCEM; CITATION),,
 For multi-speaker meetings, we use the ICSI corpus of meeting transcripts CITATION, which is becoming a standard for speech segmentation (e,,
 2003; CITATION),,
 The results on the meeting corpus also compare favorably with the topic-modeling method of CITATION, who report a Pk of ,,
, CITATION; CITATION),,
, CITATION; Galley et al,,
2003; CITATION), and is currently the dominant approach to unsupervised topic segmentation,,
 For example, consider cue phrases, which are explicit discourse markers such as now or however (CITATION; CITATION; CITATION),,
, CITATION),,
 In addition, we show that the benchmark segmentation system of CITATION can be viewed as another special case of our Bayesian model,,
 CITATION optimize a normalized minimum-cut criteria based on a variation of t,,
 This is common practice for this task, as the desired number of segments may be determined by the user CITATION,,
1 Connection to previous work In this section, we explain how our model generalizes the well-known method of CITATION; hereafter U&I),,
 This form is equivalent to Laplacian smoothing CITATION, and is a special case of our equation 2, with 0 = 1,,
 Baselines We compare against three competitive alternative systems from the literature: U&I CITATION; LCSEG (Galley et al,,
, 2003); MCS CITATION,,
 Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by CITATION,,
CITATION; Textbook Pk WD U&I ,,
 CITATION; CITATION),,
 Interleaving samplingbased inference with direct optimization of parameters can be considered a form of Monte Carlo Expectation-Maximization (MCEM; CITATION),,
 For multi-speaker meetings, we use the ICSI corpus of meeting transcripts CITATION, which is becoming a standard for speech segmentation (e,,
 2003; CITATION),,
