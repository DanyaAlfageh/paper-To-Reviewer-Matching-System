<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.3054955">
b&apos;Proceedings of the ACL 2010 Conference Short Papers, pages 301306,
Uppsala, Sweden, 11-16 July 2010. c
</bodyText>
<sectionHeader confidence="0.279619" genericHeader="abstract">
2010 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.725891">
Domain Adaptation of Maximum Entropy Language Models
</title>
<author confidence="0.751439">
Tanel Alumae
</author>
<affiliation confidence="0.963146666666667">
Adaptive Informatics Research Centre
School of Science and Technology
Aalto University
</affiliation>
<address confidence="0.732125">
Helsinki, Finland
</address>
<email confidence="0.978978">
tanel@cis.hut.fi
</email>
<author confidence="0.896899">
Mikko Kurimo
</author>
<affiliation confidence="0.982477">
Adaptive Informatics Research Centre
School of Science and Technology
Aalto University
</affiliation>
<address confidence="0.729098">
Helsinki, Finland
</address>
<email confidence="0.964714">
Mikko.Kurimo@tkk.fi
</email>
<sectionHeader confidence="0.989139" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.998894">
We investigate a recently proposed
Bayesian adaptation method for building
style-adapted maximum entropy language
models for speech recognition, given a
large corpus of written language data
and a small corpus of speech transcripts.
Experiments show that the method con-
sistently outperforms linear interpolation
which is typically used in such cases.
</bodyText>
<sectionHeader confidence="0.997944" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997347025">
In large vocabulary speech recognition, a language
model (LM) is typically estimated from large
amounts of written text data. However, recogni-
tion is typically applied to speech that is stylisti-
cally different from written language. For exam-
ple, in an often-tried setting, speech recognition is
applied to broadcast news, that includes introduc-
tory segments, conversations and spontaneous in-
terviews. To decrease the mismatch between train-
ing and test data, often a small amount of speech
data is human-transcribed. A LM is then built
by interpolating the models estimated from large
corpus of written language and the small corpus
of transcribed data. However, in practice, differ-
ent models might be of different importance de-
pending on the word context. Global interpola-
tion doesnt take such variability into account and
all predictions are weighted across models identi-
cally, regardless of the context.
In this paper we investigate a recently proposed
Bayesian adaptation approach (Daume III, 2007;
Finkel and Manning, 2009) for adapting a con-
ditional maximum entropy (ME) LM (Rosenfeld,
1996) to a new domain, given a large corpus of
out-of-domain training data and a small corpus
of in-domain data. The main contribution of this
Currently with Tallinn University of Technology, Esto-
nia
paper is that we show how the suggested hierar-
chical adaptation can be used with suitable pri-
ors and combined with the class-based speedup
technique (Goodman, 2001) to adapt ME LMs
in large-vocabulary speech recognition when the
amount of target data is small. The results outper-
form the conventional linear interpolation of back-
ground and target models in both N-grams and
ME models. It seems that with the adapted ME
models, the same recognition accuracy for the tar-
get evaluation data can be obtained with 50% less
adaptation data than in interpolated ME models.
</bodyText>
<sectionHeader confidence="0.816301" genericHeader="method">
2 Review of Conditional Maximum
Entropy Language Models
</sectionHeader>
<bodyText confidence="0.99616325">
Maximum entropy (ME) modeling is a framework
that has been used in a wide area of natural lan-
guage processing (NLP) tasks. A conditional ME
model has the following form:
</bodyText>
<equation confidence="0.999712555555555">
P(x|h) =
e
P
i ifi(x,h)
P
x0 e
P
j jfj(x0,h)
(1)
</equation>
<bodyText confidence="0.998860333333333">
where x is an outcome (in case of a LM, a word),
h is a context (the word history), and x0 a set of all
possible outcomes (words). The functions fi are
(typically binary) feature functions. During ME
training, the optimal weights i corresponding to
features fi(x, h) are learned. More precisely, find-
ing the ME model is equal to finding weights that
maximize the log-likelihood L(X; ) of the train-
ing data X. The weights are learned via improved
iterative scaling algorithm or some of its modern
fast counterparts (i.e., conjugate gradient descent).
Since LMs typically have a vocabulary of tens
of thousands of words, the use of a normalization
factor over all possible outcomes makes estimat-
ing a ME LM very memory and time consuming.
Goodman (2001) proposed a class-based method
that drastically reduces the resource requirements
for training such models. The idea is to cluster
</bodyText>
<page confidence="0.996647">
301
</page>
<bodyText confidence="0.9982696">
\x0cwords in the vocabulary into classes (e.g., based
on their distributional similarity). Then, we can
decompose the prediction of a word given its his-
tory into prediction of its class given the history,
and prediction of the word given the history and
</bodyText>
<equation confidence="0.8686105">
its class :
P(w|h) = P(C(w)|h) P(w|h, C(w)) (2)
</equation>
<bodyText confidence="0.996973888888889">
Using such decomposition, we can create two ME
models: one corresponding to P(C(w)|h) and the
other corresponding to P(w|h, C(w)). It is easy to
see that computing the normalization factor of the
first component model now requires only looping
over all classes. It turns out that normalizing the
second model is also easier: for a context h, C(w),
we only need to normalize over words that belong
to class C(w), since other words cannot occur in
this context. This decomposition can be further
extended by using hierarchical classes.
To avoid overfitting, ME models are usually
smoothed (regularized). The most widely used
smoothing method for ME LMs is Gaussian pri-
ors (Chen and Rosenfeld, 2000): a zero-mean
prior with a given variance is added to all feature
weights, and the model optimization criteria be-
comes:
</bodyText>
<equation confidence="0.9505854">
L0
(X; ) = L(X; )
F
X
i=1
2
i
22
i
(3)
</equation>
<bodyText confidence="0.981131">
where F is the number of feature functions. Typi-
cally, a fixed hyperparameter i = is used for
all parameters. The optimal variance is usually
estimated on a development set. Intuitively, this
method encourages feature weights to be smaller,
by penalizing weights with big absolute values.
</bodyText>
<sectionHeader confidence="0.870147" genericHeader="method">
3 Domain Adaptation of Maximum
</sectionHeader>
<subsectionHeader confidence="0.681849">
Entropy Models
</subsectionHeader>
<bodyText confidence="0.988161447368421">
Recently, a hierarchical Bayesian adaptation
method was proposed that can be applied to a large
family of discriminative learning tasks (such as
ME models, SVMs) (Daume III, 2007; Finkel and
Manning, 2009). In NLP problems, data often
comes from different sources (e.g., newspapers,
web, textbooks, speech transcriptions). There are
three classic approaches for building models from
multiple sources. We can pool all training data and
estimate a single model, and apply it for all tasks.
The second approach is to unpool the data, i.e,
only use training data from the test domain. The
third and often the best performing approach is to
train separate models for each data source, apply
them to test data and interpolate the results.
The hierarchical Bayesian adaptation method
is a generalization of the three approaches de-
scribed above. The hierarchical model jointly
optimizes global and domain-specific parameters,
using parameters built from pooled data as priors
for domain-specific parameters. In other words,
instead of using smoothing to encourage param-
eters to be closer to zero, it encourages domain-
specific model parameters to be closer to the
corresponding global parameters, while a zero
mean Gaussian prior is still applied for global pa-
rameters. For processing test data during run-
time, the domain-specific model is applied. Intu-
itively, this approach can be described as follows:
the domain-specific parameters are largely deter-
mined by global data, unless there is good domain-
specific evidence that they should be different.
The key to this approach is that the global and
domain-specific parameters are learned jointly, not
hierarchically. This allows domain-specific pa-
rameters to influence the global parameters, and
vice versa. Formally, the joint optimization crite-
ria becomes:
</bodyText>
<equation confidence="0.878329111111111">
Lhier(X; ) =
X
d
Lorig(Xd, d)
F
X
i=1
(d,i ,i)2
22
d
!
F
X
i=1
2
,i
22
(4)
</equation>
<bodyText confidence="0.9949945">
where Xd is data for domain d, ,i the global
parameters, d,i the domain-specific parameters,
</bodyText>
<page confidence="0.972638">
2
</page>
<bodyText confidence="0.9971539375">
the global variance and 2
d the domain-specific
variances. The global and domain-specific vari-
ances are optimized on the heldout data. Usually,
larger values are used for global parameters and
for domains with more data, while for domains
with less data, the variance is typically set to be
smaller, encouraging the domain-specific parame-
ters to be closer to global values.
This adaptation scheme is very similar to the ap-
proaches proposed by (Chelba and Acero, 2006)
and (Chen, 2009b): both use a model estimated
from background data as a prior when learning
a model from in-domain data. The main differ-
ence is the fact that in this method, the models are
estimated jointly while in the other works, back-
</bodyText>
<page confidence="0.990453">
302
</page>
<bodyText confidence="0.924164">
\x0cground model has to be estimated before learning
the in-domain model.
</bodyText>
<sectionHeader confidence="0.99929" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999368">
In this section, we look at experimental results
over two speech recognition tasks.
</bodyText>
<subsectionHeader confidence="0.996962">
4.1 Tasks
</subsectionHeader>
<tableCaption confidence="0.537757">
Task 1: English Broadcast News. This recog-
</tableCaption>
<bodyText confidence="0.995488722222222">
nition task consists of the English broadcast news
section of the 2003 NIST Rich Transcription Eval-
uation Data. The data includes six news record-
ings from six different sources with a total length
of 176 minutes.
As acoustic models, the CMU Sphinx open
source triphone HUB4 models for wideband
(16kHz) speech1 were used. The models have
been trained using 140 hours of speech.
For training the LMs, two sources were used:
first 5M sentences from the Gigaword (2nd ed.)
corpus (99.5M words), and broadcast news tran-
scriptions from the TDT4 corpus (1.19M words).
The latter was treated as in-domain data in the
adaptation experiments. A vocabulary of 26K
words was used. It is a subset of a bigger 60K
vocabulary, and only includes words that occurred
in the training data. The OOV rate against the test
set was 2.4%.
The audio used for testing was segmented
into parts of up to 20 seconds in length.
Speaker diarization was applied using the
LIUM SpkDiarization toolkit (Deleglise et al.,
2005). The CMU Sphinx 3.7 was used for
decoding. A three-pass recognition strategy was
applied: the first pass recognition hypotheses
were used for calculating MLLR-adapted models
for each speaker. In the second pass, the adapted
acoustic models were used for generating a
5000-best list of hypotheses for each segment. In
the third pass, the ME LM was used to re-rank the
hypotheses and select the best one. During decod-
ing, a trigram LM model was used. The trigram
model was an interpolation of source-specific
models which were estimated using Kneser-Ney
discounting.
</bodyText>
<subsubsectionHeader confidence="0.744044">
Task 2: Estonian Broadcast Conversations.
</subsubsectionHeader>
<bodyText confidence="0.987753">
The second recognition task consists of four
recordings from different live talk programs from
</bodyText>
<equation confidence="0.837562666666667">
1
http://www.speech.cs.cmu.edu/sphinx/
models/
</equation>
<bodyText confidence="0.998373166666667">
three Estonian radio stations. Their format con-
sists of hosts and invited guests, spontaneously
discussing current affairs. There are 40 minutes
of transcriptions, with 11 different speakers.
The acoustic models were trained on various
wideband Estonian speech corpora: the BABEL
speech database (9h), transcriptions of Estonian
broadcast news (7.5h) and transcriptions of radio
live talk programs (10h). The models are triphone
HMMs, using MFCC features.
For training the LMs, two sources were used:
about 10M sentences from various Estonian news-
papers, and manual transcriptions of 10 hours of
live talk programs from three Estonian radio sta-
tions. The latter is identical in style to the test data,
although it originates from a different time period
and covers a wider variety of programs, and was
treated as in-domain data.
As Estonian is a highly inflective language,
morphemes are used as basic units in the LM.
We use a morphological analyzer (Kaalep and
Vaino, 2001) for splitting the words into mor-
phemes. After such processing, the newspaper
corpus includes of 185M tokens, and the tran-
scribed data 104K tokens. A vocabulary of 30K
tokens was used for this task, with an OOV rate
of 1.7% against the test data. After recognition,
morphemes were concatenated back to words.
As with English data, a three-pass recognition
strategy involving MLLR adaptation was applied.
</bodyText>
<sectionHeader confidence="0.621956" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999287789473684">
For both tasks, we rescored the N-best lists in
two different ways: (1) using linear interpolation
of source-specific ME models and (2) using hi-
erarchically domain-adapted ME model (as de-
scribed in previous chapter). The English ME
models had a three-level and Estonian models a
four-level class hierarchy. The classes were de-
rived using the word exchange algorithm (Kneser
and Ney, 1993). The number of classes at each
level was determined experimentally so as to op-
timize the resource requirements for training ME
models (specifically, the number of classes was
150, 1000 and 5000 for the English models and
20, 150, 1000 and 6000 for the Estonian models).
We used unigram, bigram and trigram features that
occurred at least twice in the training data. The
feature cut-off was applied in order to accommo-
date the memory requirements. The feature set
was identical for interpolated and adapted models.
</bodyText>
<page confidence="0.996114">
303
</page>
<figure confidence="0.845490722222222">
\x0cInterp. models Adapted models
Adapta-
tion data
(No of
words)
2
OD 2
ID 2
2
OD 2
ID
English Broadcast News
147K 2e8 3e5 5e7 2e7 2e6
292K 2e8 5e5 5e7 2e7 2e6
591K 2e8 1e6 5e7 2e7 2e6
1119K 2e8 2e6 5e7 2e7 5e6
Estonian Broadcast Conversations
104K 5e8 3e5 5e7 1e7 2e6
</figure>
<tableCaption confidence="0.996394">
Table 1: The unnormalized values of Gaus-
</tableCaption>
<bodyText confidence="0.988353588235294">
sian prior variances for interpolated out-of-domain
(OD) and in-domain (ID) ME models, and hierar-
chically adapted global (*), out-of-odomain (OD)
and in-domain (ID) models that were used in the
experiments.
For the English task, we also explored the ef-
ficiency of these two approaches with varying
size of adaptation data: we repeated the exper-
iments when using one eighth, one quarter, half
and all of the TDT4 transcription data for interpo-
lation/adaptation. The amount of used Gigaword
data was not changed. In all cases, interpolation
weights were re-optimized and new Gaussian vari-
ance values were heuristically determined.
The TADM toolkit2 was used for estimating ME
models, utilizing its implementation of the conju-
gate gradient algorithm.
The models were regularized using Gaussian
priors. The variance parameters were chosen
heuristically based on light tuning on develop-
ment set perplexity. For the source-specific ME
models, the variance was fixed on per-model ba-
sis. For the adapted model, that jointly models
global and domain-specific data, the Gaussian pri-
ors were fixed for each hierarchy node (i.e., the
variance was fixed across global, out-of-domain,
and in-domain parameters). Table 1 lists values
for the variances of Gaussian priors (as in equa-
tions 3 and 4) that we used in the experiments. In
other publications, the variance values are often
normalized to the size of the data. We chose not
to normalize the values, since in the hierarchical
adaptation scheme, also data from other domains
have impact on the learned model parameters, thus
</bodyText>
<page confidence="0.8952">
2
</page>
<bodyText confidence="0.874992">
http://tadm.sourceforge.net/
its not possible to simply normalize the variances.
The experimental results are presented in Table
</bodyText>
<listItem confidence="0.817593">
2. Perplexity and word error rate (WER) results of
</listItem>
<bodyText confidence="0.980810190476191">
the interpolated and adapted models are compared.
For the Estonian task, letter error rate (LER) is
also reported, since it tends to be a more indicative
measure of speech recognition quality for highly
inflected languages. In all experiments, using the
adapted models resulted in lower perplexity and
lower error rate. Improvements in the English ex-
periment were less evident than in the Estonian
system, with under 10% improvement in perplex-
ity and 1-3% in WER, against 15% and 4% for the
Estonian experiment. In most cases, there was a
significant improvement in WER when using the
adapted ME model (according to the Wilcoxon
test), with and exception of the English experi-
ments on the 292K and 591K data sets.
The comparison between N-gram models and
ME models is not entirely fair since ME models
are actually class-based. Such transformation in-
troduces additional smoothing into the model and
can improve model perplexity, as also noticed by
Goodman (2001).
</bodyText>
<sectionHeader confidence="0.993746" genericHeader="discussions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99415632">
In this paper we have tested a hierarchical adapta-
tion method (Daume III, 2007; Finkel and Man-
ning, 2009) on building style-adapted LMs for
speech recognition. We showed that the method
achieves consistently lower error rates than when
using linear interpolation which is typically used
in such scenarios.
The tested method is ideally suited for language
modeling in speech recognition: we almost always
have access to large amounts of data from written
sources but commonly the speech to be recognized
is stylistically noticeably different. The hierarchi-
cal adaptation method enables to use even a small
amount of in-domain data to modify the parame-
ters estimated from out-of-domain data, if there is
enough evidence.
As Finkel and Manning (2009) point out, the
hierarchical nature of the method makes it possi-
ble to estimate highly specific models: we could
draw style-specific models from general high-level
priors, and topic-and-style specific models from
style-specific priors. Furthermore, the models
dont have to be hierarchical: it is easy to gen-
eralize the method to general multilevel approach
where a model is drawn from multiple priors. For
</bodyText>
<page confidence="0.992577">
304
</page>
<figure confidence="0.971907379310345">
\x0cPerplexity WER LER
Adaptation
data (No.
of words)
Pooled
N-
gram
Interp.
N-
gram
Interp.
ME
Adapted
ME
Interp.
N-
gram
Interp.
ME
Adapted
ME
Interp.
N-
gram
Interp.
ME
Adapted
ME
English Broadcast News
</figure>
<table confidence="0.974020833333333">
147K 290 255 243 230 27.2 26.3 25.9
292K 286 250 236 223 26.7 25.8 25.6
591K 280 243 228 215 26.6 25.9 25.6
1119K 272 232 217 204 26.2 25.6 24.9
Estonian Broadcast Conversations
104K 237 197 200 169 40.5 38.9 37.4 17.7 17.3 16.6
</table>
<tableCaption confidence="0.990975">
Table 2: Perplexity, WER and LER results comparing pooled and interpolated N-gram models and
</tableCaption>
<bodyText confidence="0.998753741935484">
interpolated and adapted ME models, with changing amount of available in-domain data.
instance, we could build a model for recognizing
computer science lectures, given data from text-
books, including those about computer science,
and transcripts of lectures on various topics (which
dont even need to include lectures about computer
science).
The method has some considerable shortcom-
ings from the practical perspective. First, train-
ing ME LMs in general has much higher resource
requirements than training N-gram models which
are typically used in speech recognition. More-
over, training hierarchical ME models requires
even more memory than training simple ME mod-
els, proportional to the number of nodes in the hi-
erarchy. However, it should be possible to allevi-
ate this problem by profiting from the hierarchi-
cal nature of n-gram features, as proposed in (Wu
and Khudanpur, 2002). It is also difficult to deter-
mine good variance values 2
i for the global and
domain-specific priors. While good variance val-
ues for simple ME models can be chosen quite re-
liably based on the size of the training data (Chen,
2009a), we have found that it is more demand-
ing to find good hyperparameters for hierarchical
models since weights for the same feature in dif-
ferent nodes in the hierarchy are all related to each
other. We plan to investigate this problem in the
future since the choice of hyperparameters has a
strong impact on the performance of the model.
</bodyText>
<sectionHeader confidence="0.978331" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.959409166666667">
This research was partly funded by the Academy
of Finland in the project Adaptive Informatics,
by the target-financed theme No. 0322709s06 of
the Estonian Ministry of Education and Research
and by the National Programme for Estonian Lan-
guage Technology.
</bodyText>
<sectionHeader confidence="0.978869" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999384727272727">
Ciprian Chelba and Alex Acero. 2006. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. Computer Speech &amp; Language, 20(4):382399,
October.
S. F. Chen and R. Rosenfeld. 2000. A survey of
smoothing techniques for ME models. IEEE Trans-
actions on Speech and Audio Processing, 8(1):37
50.
S. F. Chen. 2009a. Performance prediction for expo-
nential language models. In Proceedings of HLT-
NAACL, pages 450458, Boulder, Colorado.
Stanley F. Chen. 2009b. Shrinking exponential lan-
guage models. In Proceedings of HLT-NAACL,
pages 468476, Boulder, Colorado.
H. Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL, pages 256263.
P. Deleglise, Y. Esteve, S. Meignier, and T. Merlin.
2005. The LIUM speech transcription system: a
CMU Sphinx III-based system for French broadcast
news. In Proceedings of Interspeech, Lisboa, Portu-
gal.
J. R. Finkel and Ch. Manning. 2009. Hierarchi-
cal Bayesian domain adaptation. In Proceedings of
HLT-NAACL, pages 602610, Boulder, Colorado.
J. Goodman. 2001. Classes for fast maximum entropy
training. In Proceedings of ICASSP, Utah, USA.
H.-J. Kaalep and T. Vaino. 2001. Complete morpho-
logical analysis in the linguists toolbox. In Con-
gressus Nonus Internationalis Fenno-Ugristarum
Pars V, pages 916, Tartu, Estonia.
R. Kneser and H. Ney. 1993. Improved clustering
techniques for class-based statistical language mod-
elling. In Proceedings of the European Conference
</reference>
<page confidence="0.966913">
305
</page>
<reference confidence="0.998565111111111">
\x0con Speech Communication and Technology, pages
973976.
R. Rosenfeld. 1996. A maximum entropy approach to
adaptive statistical language modeling. Computer,
Speech and Language, 10:187228.
J. Wu and S. Khudanpur. 2002. Building a topic-
dependent maximum entropy model for very large
corpora. In Proceedings of ICASSP, Orlando,
Florida, USA.
</reference>
<page confidence="0.994395">
306
</page>
<figure confidence="0.24512">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.532975">
<note confidence="0.851230666666667">b&apos;Proceedings of the ACL 2010 Conference Short Papers, pages 301306, Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics</note>
<title confidence="0.97742">Domain Adaptation of Maximum Entropy Language Models</title>
<author confidence="0.913341">Tanel Alumae</author>
<affiliation confidence="0.992703333333333">Adaptive Informatics Research Centre School of Science and Technology Aalto University</affiliation>
<address confidence="0.999563">Helsinki, Finland</address>
<email confidence="0.990706">tanel@cis.hut.fi</email>
<author confidence="0.997153">Mikko Kurimo</author>
<affiliation confidence="0.999106333333333">Adaptive Informatics Research Centre School of Science and Technology Aalto University</affiliation>
<address confidence="0.99933">Helsinki, Finland</address>
<email confidence="0.923366">Mikko.Kurimo@tkk.fi</email>
<abstract confidence="0.9933961">We investigate a recently proposed Bayesian adaptation method for building style-adapted maximum entropy language models for speech recognition, given a large corpus of written language data and a small corpus of speech transcripts. Experiments show that the method consistently outperforms linear interpolation which is typically used in such cases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of maximum entropy capitalizer: Little data can help a lot.</title>
<date>2006</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="7800" citStr="Chelba and Acero, 2006" startWordPosition="1246" endWordPosition="1249"> d Lorig(Xd, d) F X i=1 (d,i ,i)2 22 d ! F X i=1 2 ,i 22 (4) where Xd is data for domain d, ,i the global parameters, d,i the domain-specific parameters, 2 the global variance and 2 d the domain-specific variances. The global and domain-specific variances are optimized on the heldout data. Usually, larger values are used for global parameters and for domains with more data, while for domains with less data, the variance is typically set to be smaller, encouraging the domain-specific parameters to be closer to global values. This adaptation scheme is very similar to the approaches proposed by (Chelba and Acero, 2006) and (Chen, 2009b): both use a model estimated from background data as a prior when learning a model from in-domain data. The main difference is the fact that in this method, the models are estimated jointly while in the other works, back302 \x0cground model has to be estimated before learning the in-domain model. 4 Experiments In this section, we look at experimental results over two speech recognition tasks. 4.1 Tasks Task 1: English Broadcast News. This recognition task consists of the English broadcast news section of the 2003 NIST Rich Transcription Evaluation Data. The data includes six </context>
</contexts>
<marker>Chelba, Acero, 2006</marker>
<rawString>Ciprian Chelba and Alex Acero. 2006. Adaptation of maximum entropy capitalizer: Little data can help a lot. Computer Speech &amp; Language, 20(4):382399, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for ME models.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>50</pages>
<contexts>
<context position="4869" citStr="Chen and Rosenfeld, 2000" startWordPosition="769" endWordPosition="772">g to P(C(w)|h) and the other corresponding to P(w|h, C(w)). It is easy to see that computing the normalization factor of the first component model now requires only looping over all classes. It turns out that normalizing the second model is also easier: for a context h, C(w), we only need to normalize over words that belong to class C(w), since other words cannot occur in this context. This decomposition can be further extended by using hierarchical classes. To avoid overfitting, ME models are usually smoothed (regularized). The most widely used smoothing method for ME LMs is Gaussian priors (Chen and Rosenfeld, 2000): a zero-mean prior with a given variance is added to all feature weights, and the model optimization criteria becomes: L0 (X; ) = L(X; ) F X i=1 2 i 22 i (3) where F is the number of feature functions. Typically, a fixed hyperparameter i = is used for all parameters. The optimal variance is usually estimated on a development set. Intuitively, this method encourages feature weights to be smaller, by penalizing weights with big absolute values. 3 Domain Adaptation of Maximum Entropy Models Recently, a hierarchical Bayesian adaptation method was proposed that can be applied to a large family of </context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>S. F. Chen and R. Rosenfeld. 2000. A survey of smoothing techniques for ME models. IEEE Transactions on Speech and Audio Processing, 8(1):37 50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
</authors>
<title>Performance prediction for exponential language models.</title>
<date>2009</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>450458</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="7816" citStr="Chen, 2009" startWordPosition="1251" endWordPosition="1252">,i)2 22 d ! F X i=1 2 ,i 22 (4) where Xd is data for domain d, ,i the global parameters, d,i the domain-specific parameters, 2 the global variance and 2 d the domain-specific variances. The global and domain-specific variances are optimized on the heldout data. Usually, larger values are used for global parameters and for domains with more data, while for domains with less data, the variance is typically set to be smaller, encouraging the domain-specific parameters to be closer to global values. This adaptation scheme is very similar to the approaches proposed by (Chelba and Acero, 2006) and (Chen, 2009b): both use a model estimated from background data as a prior when learning a model from in-domain data. The main difference is the fact that in this method, the models are estimated jointly while in the other works, back302 \x0cground model has to be estimated before learning the in-domain model. 4 Experiments In this section, we look at experimental results over two speech recognition tasks. 4.1 Tasks Task 1: English Broadcast News. This recognition task consists of the English broadcast news section of the 2003 NIST Rich Transcription Evaluation Data. The data includes six news recordings </context>
<context position="18128" citStr="Chen, 2009" startWordPosition="2924" endWordPosition="2925">ning N-gram models which are typically used in speech recognition. Moreover, training hierarchical ME models requires even more memory than training simple ME models, proportional to the number of nodes in the hierarchy. However, it should be possible to alleviate this problem by profiting from the hierarchical nature of n-gram features, as proposed in (Wu and Khudanpur, 2002). It is also difficult to determine good variance values 2 i for the global and domain-specific priors. While good variance values for simple ME models can be chosen quite reliably based on the size of the training data (Chen, 2009a), we have found that it is more demanding to find good hyperparameters for hierarchical models since weights for the same feature in different nodes in the hierarchy are all related to each other. We plan to investigate this problem in the future since the choice of hyperparameters has a strong impact on the performance of the model. Acknowledgments This research was partly funded by the Academy of Finland in the project Adaptive Informatics, by the target-financed theme No. 0322709s06 of the Estonian Ministry of Education and Research and by the National Programme for Estonian Language Tech</context>
</contexts>
<marker>Chen, 2009</marker>
<rawString>S. F. Chen. 2009a. Performance prediction for exponential language models. In Proceedings of HLTNAACL, pages 450458, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Shrinking exponential language models.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>468476</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="7816" citStr="Chen, 2009" startWordPosition="1251" endWordPosition="1252">,i)2 22 d ! F X i=1 2 ,i 22 (4) where Xd is data for domain d, ,i the global parameters, d,i the domain-specific parameters, 2 the global variance and 2 d the domain-specific variances. The global and domain-specific variances are optimized on the heldout data. Usually, larger values are used for global parameters and for domains with more data, while for domains with less data, the variance is typically set to be smaller, encouraging the domain-specific parameters to be closer to global values. This adaptation scheme is very similar to the approaches proposed by (Chelba and Acero, 2006) and (Chen, 2009b): both use a model estimated from background data as a prior when learning a model from in-domain data. The main difference is the fact that in this method, the models are estimated jointly while in the other works, back302 \x0cground model has to be estimated before learning the in-domain model. 4 Experiments In this section, we look at experimental results over two speech recognition tasks. 4.1 Tasks Task 1: English Broadcast News. This recognition task consists of the English broadcast news section of the 2003 NIST Rich Transcription Evaluation Data. The data includes six news recordings </context>
<context position="18128" citStr="Chen, 2009" startWordPosition="2924" endWordPosition="2925">ning N-gram models which are typically used in speech recognition. Moreover, training hierarchical ME models requires even more memory than training simple ME models, proportional to the number of nodes in the hierarchy. However, it should be possible to alleviate this problem by profiting from the hierarchical nature of n-gram features, as proposed in (Wu and Khudanpur, 2002). It is also difficult to determine good variance values 2 i for the global and domain-specific priors. While good variance values for simple ME models can be chosen quite reliably based on the size of the training data (Chen, 2009a), we have found that it is more demanding to find good hyperparameters for hierarchical models since weights for the same feature in different nodes in the hierarchy are all related to each other. We plan to investigate this problem in the future since the choice of hyperparameters has a strong impact on the performance of the model. Acknowledgments This research was partly funded by the Academy of Finland in the project Adaptive Informatics, by the target-financed theme No. 0322709s06 of the Estonian Ministry of Education and Research and by the National Programme for Estonian Language Tech</context>
</contexts>
<marker>Chen, 2009</marker>
<rawString>Stanley F. Chen. 2009b. Shrinking exponential language models. In Proceedings of HLT-NAACL, pages 468476, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daume</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>256263</pages>
<marker>Daume, 2007</marker>
<rawString>H. Daume III. 2007. Frustratingly easy domain adaptation. In Proceedings of ACL, pages 256263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Deleglise</author>
<author>Y Esteve</author>
<author>S Meignier</author>
<author>T Merlin</author>
</authors>
<title>The LIUM speech transcription system: a CMU Sphinx III-based system for French broadcast news.</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<location>Lisboa, Portugal.</location>
<contexts>
<context position="9259" citStr="Deleglise et al., 2005" startWordPosition="1493" endWordPosition="1496">peech. For training the LMs, two sources were used: first 5M sentences from the Gigaword (2nd ed.) corpus (99.5M words), and broadcast news transcriptions from the TDT4 corpus (1.19M words). The latter was treated as in-domain data in the adaptation experiments. A vocabulary of 26K words was used. It is a subset of a bigger 60K vocabulary, and only includes words that occurred in the training data. The OOV rate against the test set was 2.4%. The audio used for testing was segmented into parts of up to 20 seconds in length. Speaker diarization was applied using the LIUM SpkDiarization toolkit (Deleglise et al., 2005). The CMU Sphinx 3.7 was used for decoding. A three-pass recognition strategy was applied: the first pass recognition hypotheses were used for calculating MLLR-adapted models for each speaker. In the second pass, the adapted acoustic models were used for generating a 5000-best list of hypotheses for each segment. In the third pass, the ME LM was used to re-rank the hypotheses and select the best one. During decoding, a trigram LM model was used. The trigram model was an interpolation of source-specific models which were estimated using Kneser-Ney discounting. Task 2: Estonian Broadcast Convers</context>
</contexts>
<marker>Deleglise, Esteve, Meignier, Merlin, 2005</marker>
<rawString>P. Deleglise, Y. Esteve, S. Meignier, and T. Merlin. 2005. The LIUM speech transcription system: a CMU Sphinx III-based system for French broadcast news. In Proceedings of Interspeech, Lisboa, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manning</author>
</authors>
<title>Hierarchical Bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>602610</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="1880" citStr="Manning, 2009" startWordPosition="269" endWordPosition="270">the mismatch between training and test data, often a small amount of speech data is human-transcribed. A LM is then built by interpolating the models estimated from large corpus of written language and the small corpus of transcribed data. However, in practice, different models might be of different importance depending on the word context. Global interpolation doesnt take such variability into account and all predictions are weighted across models identically, regardless of the context. In this paper we investigate a recently proposed Bayesian adaptation approach (Daume III, 2007; Finkel and Manning, 2009) for adapting a conditional maximum entropy (ME) LM (Rosenfeld, 1996) to a new domain, given a large corpus of out-of-domain training data and a small corpus of in-domain data. The main contribution of this Currently with Tallinn University of Technology, Estonia paper is that we show how the suggested hierarchical adaptation can be used with suitable priors and combined with the class-based speedup technique (Goodman, 2001) to adapt ME LMs in large-vocabulary speech recognition when the amount of target data is small. The results outperform the conventional linear interpolation of background </context>
<context position="5568" citStr="Manning, 2009" startWordPosition="890" endWordPosition="891">del optimization criteria becomes: L0 (X; ) = L(X; ) F X i=1 2 i 22 i (3) where F is the number of feature functions. Typically, a fixed hyperparameter i = is used for all parameters. The optimal variance is usually estimated on a development set. Intuitively, this method encourages feature weights to be smaller, by penalizing weights with big absolute values. 3 Domain Adaptation of Maximum Entropy Models Recently, a hierarchical Bayesian adaptation method was proposed that can be applied to a large family of discriminative learning tasks (such as ME models, SVMs) (Daume III, 2007; Finkel and Manning, 2009). In NLP problems, data often comes from different sources (e.g., newspapers, web, textbooks, speech transcriptions). There are three classic approaches for building models from multiple sources. We can pool all training data and estimate a single model, and apply it for all tasks. The second approach is to unpool the data, i.e, only use training data from the test domain. The third and often the best performing approach is to train separate models for each data source, apply them to test data and interpolate the results. The hierarchical Bayesian adaptation method is a generalization of the t</context>
<context position="15445" citStr="Manning, 2009" startWordPosition="2486" endWordPosition="2488">nst 15% and 4% for the Estonian experiment. In most cases, there was a significant improvement in WER when using the adapted ME model (according to the Wilcoxon test), with and exception of the English experiments on the 292K and 591K data sets. The comparison between N-gram models and ME models is not entirely fair since ME models are actually class-based. Such transformation introduces additional smoothing into the model and can improve model perplexity, as also noticed by Goodman (2001). 5 Discussion In this paper we have tested a hierarchical adaptation method (Daume III, 2007; Finkel and Manning, 2009) on building style-adapted LMs for speech recognition. We showed that the method achieves consistently lower error rates than when using linear interpolation which is typically used in such scenarios. The tested method is ideally suited for language modeling in speech recognition: we almost always have access to large amounts of data from written sources but commonly the speech to be recognized is stylistically noticeably different. The hierarchical adaptation method enables to use even a small amount of in-domain data to modify the parameters estimated from out-of-domain data, if there is eno</context>
</contexts>
<marker>Manning, 2009</marker>
<rawString>J. R. Finkel and Ch. Manning. 2009. Hierarchical Bayesian domain adaptation. In Proceedings of HLT-NAACL, pages 602610, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Classes for fast maximum entropy training.</title>
<date>2001</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<location>Utah, USA.</location>
<contexts>
<context position="2308" citStr="Goodman, 2001" startWordPosition="339" endWordPosition="340">e weighted across models identically, regardless of the context. In this paper we investigate a recently proposed Bayesian adaptation approach (Daume III, 2007; Finkel and Manning, 2009) for adapting a conditional maximum entropy (ME) LM (Rosenfeld, 1996) to a new domain, given a large corpus of out-of-domain training data and a small corpus of in-domain data. The main contribution of this Currently with Tallinn University of Technology, Estonia paper is that we show how the suggested hierarchical adaptation can be used with suitable priors and combined with the class-based speedup technique (Goodman, 2001) to adapt ME LMs in large-vocabulary speech recognition when the amount of target data is small. The results outperform the conventional linear interpolation of background and target models in both N-grams and ME models. It seems that with the adapted ME models, the same recognition accuracy for the target evaluation data can be obtained with 50% less adaptation data than in interpolated ME models. 2 Review of Conditional Maximum Entropy Language Models Maximum entropy (ME) modeling is a framework that has been used in a wide area of natural language processing (NLP) tasks. A conditional ME mo</context>
<context position="3736" citStr="Goodman (2001)" startWordPosition="586" endWordPosition="587">nctions fi are (typically binary) feature functions. During ME training, the optimal weights i corresponding to features fi(x, h) are learned. More precisely, finding the ME model is equal to finding weights that maximize the log-likelihood L(X; ) of the training data X. The weights are learned via improved iterative scaling algorithm or some of its modern fast counterparts (i.e., conjugate gradient descent). Since LMs typically have a vocabulary of tens of thousands of words, the use of a normalization factor over all possible outcomes makes estimating a ME LM very memory and time consuming. Goodman (2001) proposed a class-based method that drastically reduces the resource requirements for training such models. The idea is to cluster 301 \x0cwords in the vocabulary into classes (e.g., based on their distributional similarity). Then, we can decompose the prediction of a word given its history into prediction of its class given the history, and prediction of the word given the history and its class : P(w|h) = P(C(w)|h) P(w|h, C(w)) (2) Using such decomposition, we can create two ME models: one corresponding to P(C(w)|h) and the other corresponding to P(w|h, C(w)). It is easy to see that computing</context>
<context position="15325" citStr="Goodman (2001)" startWordPosition="2466" endWordPosition="2467">experiment were less evident than in the Estonian system, with under 10% improvement in perplexity and 1-3% in WER, against 15% and 4% for the Estonian experiment. In most cases, there was a significant improvement in WER when using the adapted ME model (according to the Wilcoxon test), with and exception of the English experiments on the 292K and 591K data sets. The comparison between N-gram models and ME models is not entirely fair since ME models are actually class-based. Such transformation introduces additional smoothing into the model and can improve model perplexity, as also noticed by Goodman (2001). 5 Discussion In this paper we have tested a hierarchical adaptation method (Daume III, 2007; Finkel and Manning, 2009) on building style-adapted LMs for speech recognition. We showed that the method achieves consistently lower error rates than when using linear interpolation which is typically used in such scenarios. The tested method is ideally suited for language modeling in speech recognition: we almost always have access to large amounts of data from written sources but commonly the speech to be recognized is stylistically noticeably different. The hierarchical adaptation method enables </context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>J. Goodman. 2001. Classes for fast maximum entropy training. In Proceedings of ICASSP, Utah, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H-J Kaalep</author>
<author>T Vaino</author>
</authors>
<title>Complete morphological analysis in the linguists toolbox.</title>
<date>2001</date>
<booktitle>In Congressus Nonus Internationalis Fenno-Ugristarum Pars V,</booktitle>
<pages>916</pages>
<location>Tartu, Estonia.</location>
<contexts>
<context position="10984" citStr="Kaalep and Vaino, 2001" startWordPosition="1758" endWordPosition="1761">nd transcriptions of radio live talk programs (10h). The models are triphone HMMs, using MFCC features. For training the LMs, two sources were used: about 10M sentences from various Estonian newspapers, and manual transcriptions of 10 hours of live talk programs from three Estonian radio stations. The latter is identical in style to the test data, although it originates from a different time period and covers a wider variety of programs, and was treated as in-domain data. As Estonian is a highly inflective language, morphemes are used as basic units in the LM. We use a morphological analyzer (Kaalep and Vaino, 2001) for splitting the words into morphemes. After such processing, the newspaper corpus includes of 185M tokens, and the transcribed data 104K tokens. A vocabulary of 30K tokens was used for this task, with an OOV rate of 1.7% against the test data. After recognition, morphemes were concatenated back to words. As with English data, a three-pass recognition strategy involving MLLR adaptation was applied. 4.2 Results For both tasks, we rescored the N-best lists in two different ways: (1) using linear interpolation of source-specific ME models and (2) using hierarchically domain-adapted ME model (as</context>
</contexts>
<marker>Kaalep, Vaino, 2001</marker>
<rawString>H.-J. Kaalep and T. Vaino. 2001. Complete morphological analysis in the linguists toolbox. In Congressus Nonus Internationalis Fenno-Ugristarum Pars V, pages 916, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved clustering techniques for class-based statistical language modelling.</title>
<date>1993</date>
<booktitle>In Proceedings of the European Conference \x0con Speech Communication and Technology,</booktitle>
<pages>973976</pages>
<contexts>
<context position="11788" citStr="Kneser and Ney, 1993" startWordPosition="1887" endWordPosition="1890"> for this task, with an OOV rate of 1.7% against the test data. After recognition, morphemes were concatenated back to words. As with English data, a three-pass recognition strategy involving MLLR adaptation was applied. 4.2 Results For both tasks, we rescored the N-best lists in two different ways: (1) using linear interpolation of source-specific ME models and (2) using hierarchically domain-adapted ME model (as described in previous chapter). The English ME models had a three-level and Estonian models a four-level class hierarchy. The classes were derived using the word exchange algorithm (Kneser and Ney, 1993). The number of classes at each level was determined experimentally so as to optimize the resource requirements for training ME models (specifically, the number of classes was 150, 1000 and 5000 for the English models and 20, 150, 1000 and 6000 for the Estonian models). We used unigram, bigram and trigram features that occurred at least twice in the training data. The feature cut-off was applied in order to accommodate the memory requirements. The feature set was identical for interpolated and adapted models. 303 \x0cInterp. models Adapted models Adaptation data (No of words) 2 OD 2 ID 2 2 OD </context>
</contexts>
<marker>Kneser, Ney, 1993</marker>
<rawString>R. Kneser and H. Ney. 1993. Improved clustering techniques for class-based statistical language modelling. In Proceedings of the European Conference \x0con Speech Communication and Technology, pages 973976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling.</title>
<date>1996</date>
<journal>Computer, Speech and Language,</journal>
<pages>10--187228</pages>
<contexts>
<context position="1949" citStr="Rosenfeld, 1996" startWordPosition="280" endWordPosition="281">f speech data is human-transcribed. A LM is then built by interpolating the models estimated from large corpus of written language and the small corpus of transcribed data. However, in practice, different models might be of different importance depending on the word context. Global interpolation doesnt take such variability into account and all predictions are weighted across models identically, regardless of the context. In this paper we investigate a recently proposed Bayesian adaptation approach (Daume III, 2007; Finkel and Manning, 2009) for adapting a conditional maximum entropy (ME) LM (Rosenfeld, 1996) to a new domain, given a large corpus of out-of-domain training data and a small corpus of in-domain data. The main contribution of this Currently with Tallinn University of Technology, Estonia paper is that we show how the suggested hierarchical adaptation can be used with suitable priors and combined with the class-based speedup technique (Goodman, 2001) to adapt ME LMs in large-vocabulary speech recognition when the amount of target data is small. The results outperform the conventional linear interpolation of background and target models in both N-grams and ME models. It seems that with t</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>R. Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer, Speech and Language, 10:187228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wu</author>
<author>S Khudanpur</author>
</authors>
<title>Building a topicdependent maximum entropy model for very large corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<location>Orlando, Florida, USA.</location>
<contexts>
<context position="17897" citStr="Wu and Khudanpur, 2002" startWordPosition="2879" endWordPosition="2882">on various topics (which dont even need to include lectures about computer science). The method has some considerable shortcomings from the practical perspective. First, training ME LMs in general has much higher resource requirements than training N-gram models which are typically used in speech recognition. Moreover, training hierarchical ME models requires even more memory than training simple ME models, proportional to the number of nodes in the hierarchy. However, it should be possible to alleviate this problem by profiting from the hierarchical nature of n-gram features, as proposed in (Wu and Khudanpur, 2002). It is also difficult to determine good variance values 2 i for the global and domain-specific priors. While good variance values for simple ME models can be chosen quite reliably based on the size of the training data (Chen, 2009a), we have found that it is more demanding to find good hyperparameters for hierarchical models since weights for the same feature in different nodes in the hierarchy are all related to each other. We plan to investigate this problem in the future since the choice of hyperparameters has a strong impact on the performance of the model. Acknowledgments This research w</context>
</contexts>
<marker>Wu, Khudanpur, 2002</marker>
<rawString>J. Wu and S. Khudanpur. 2002. Building a topicdependent maximum entropy model for very large corpora. In Proceedings of ICASSP, Orlando, Florida, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>