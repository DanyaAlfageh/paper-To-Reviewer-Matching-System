 This adaptation scheme is very similar to the approaches proposed by CITATION and (CITATIONb): both use a model estimated from background data as a prior when learning a model from in-domain data,,
 The most widely used smoothing method for ME LMs is Gaussian priors CITATION: a zero-mean prior with a given variance is added to all feature weights, and the model optimization criteria becomes: L0 (X; ) = L(X; ) F X i=1 2 i 22 i (3) where F is the number of feature functions,,
 This adaptation scheme is very similar to the approaches proposed by CITATION and (CITATIONb): both use a model estimated from background data as a prior when learning a model from in-domain data,,
 However, it should be possible to alleviate this problem by profiting from the hierarchical nature of n-gram features, as proposed in CITATION,,
 While good variance values for simple ME models can be chosen quite reliably based on the size of the training data (CITATIONa), we have found that it is more demanding to find good hyperparameters for hierarchical models since weights for the same feature in different nodes in the hierarchy are all related to each other,,
 This adaptation scheme is very similar to the approaches proposed by CITATION and (CITATIONb): both use a model estimated from background data as a prior when learning a model from in-domain data,,
 However, it should be possible to alleviate this problem by profiting from the hierarchical nature of n-gram features, as proposed in CITATION,,
 While good variance values for simple ME models can be chosen quite reliably based on the size of the training data (CITATIONa), we have found that it is more demanding to find good hyperparameters for hierarchical models since weights for the same feature in different nodes in the hierarchy are all related to each other,,
 Speaker diarization was applied using the LIUM SpkDiarization toolkit CITATION,,
 In this paper we investigate a recently proposed Bayesian adaptation approach (Daume III, 2007; Finkel and CITATION) for adapting a conditional maximum entropy (ME) LM CITATION to a new domain, given a large corpus of out-of-domain training data and a small corpus of in-domain data,,
 The main contribution of this Currently with Tallinn University of Technology, Estonia paper is that we show how the suggested hierarchical adaptation can be used with suitable priors and combined with the class-based speedup technique CITATION to adapt ME LMs in large-vocabulary speech recognition when the amount of target data is small,,
 3 Domain Adaptation of Maximum Entropy Models Recently, a hierarchical Bayesian adaptation method was proposed that can be applied to a large family of discriminative learning tasks (such as ME models, SVMs) (Daume III, 2007; Finkel and CITATION),,
 Such transformation introduces additional smoothing into the model and can improve model perplexity, as also noticed by CITATION,,
 5 Discussion In this paper we have tested a hierarchical adaptation method (Daume III, 2007; Finkel and CITATION) on building style-adapted LMs for speech recognition,,
 In this paper we investigate a recently proposed Bayesian adaptation approach (Daume III, 2007; Finkel and CITATION) for adapting a conditional maximum entropy (ME) LM CITATION to a new domain, given a large corpus of out-of-domain training data and a small corpus of in-domain data,,
 The main contribution of this Currently with Tallinn University of Technology, Estonia paper is that we show how the suggested hierarchical adaptation can be used with suitable priors and combined with the class-based speedup technique CITATION to adapt ME LMs in large-vocabulary speech recognition when the amount of target data is small,,
 CITATION proposed a class-based method that drastically reduces the resource requirements for training such models,,
 Such transformation introduces additional smoothing into the model and can improve model perplexity, as also noticed by CITATION,,
 5 Discussion In this paper we have tested a hierarchical adaptation method (Daume III, 2007; Finkel and CITATION) on building style-adapted LMs for speech recognition,,
 We use a morphological analyzer CITATION for splitting the words into morphemes,,
 The classes were derived using the word exchange algorithm CITATION,,
 In this paper we investigate a recently proposed Bayesian adaptation approach (Daume III, 2007; Finkel and CITATION) for adapting a conditional maximum entropy (ME) LM CITATION to a new domain, given a large corpus of out-of-domain training data and a small corpus of in-domain data,,
 The main contribution of this Currently with Tallinn University of Technology, Estonia paper is that we show how the suggested hierarchical adaptation can be used with suitable priors and combined with the class-based speedup technique CITATION to adapt ME LMs in large-vocabulary speech recognition when the amount of target data is small,,
 However, it should be possible to alleviate this problem by profiting from the hierarchical nature of n-gram features, as proposed in CITATION,,
 While good variance values for simple ME models can be chosen quite reliably based on the size of the training data (CITATIONa), we have found that it is more demanding to find good hyperparameters for hierarchical models since weights for the same feature in different nodes in the hierarchy are all related to each other,,
