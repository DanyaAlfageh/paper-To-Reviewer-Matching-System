<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.407853">
b&apos;Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 212,
</note>
<address confidence="0.293377">
Edinburgh, Scotland, UK, July 2731, 2011. c
</address>
<title confidence="0.673362">
2011 Association for Computational Linguistics
Structured Databases of Named Entities from Bayesian Nonparametrics
</title>
<author confidence="0.9859">
Jacob Eisenstein Tae Yano William W. Cohen Noah A. Smith Eric P. Xing
</author>
<affiliation confidence="0.9794715">
School of Computer Science
Carnegie Mellon University
</affiliation>
<address confidence="0.813952">
Pittsburgh, PA 15213, USA
</address>
<email confidence="0.998565">
{jacobeis,taey,wcohen,nasmith,epxing}@cs.cmu.edu
</email>
<sectionHeader confidence="0.990862" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999651153846154">
We present a nonparametric Bayesian ap-
proach to extract a structured database of enti-
ties from text. Neither the number of entities
nor the fields that characterize each entity are
provided in advance; the only supervision is
a set of five prototype examples. Our method
jointly accomplishes three tasks: (i) identify-
ing a set of canonical entities, (ii) inferring a
schema for the fields that describe each entity,
and (iii) matching entities to their references in
raw text. Empirical evaluation shows that the
approach learns an accurate database of enti-
ties and a sensible model of name structure.
</bodyText>
<sectionHeader confidence="0.99822" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995091944444444">
Consider the task of building a set of structured
records from a collection of text: for example, ex-
tracting the names of people or businesses from
blog posts, where each full name decomposes into
fields corresponding to first-name, last-name, title,
etc. To instruct a person to perform this task, one
might begin with a few examples of the records to
be obtained; assuming that the mapping from text to
records is relatively straightforward, no additional
instruction would be necessary. In this paper, we
present a method for training information extraction
software in the same way: starting from a small table
of partially-complete prototype records (Table 1),
our system learns to add new entries and fields to
the table, while simultaneously aligning the records
to text.
We assume that the dimensionality of the database
is unknown, so that neither the number of entries
</bodyText>
<table confidence="0.2623412">
John McCain Sen. Mr.
George Bush W. Mr.
Hillary Clinton Rodham Mrs.
Barack Obama Sen.
Sarah Palin
</table>
<tableCaption confidence="0.996356">
Table 1: A set of partially-complete prototype records,
</tableCaption>
<bodyText confidence="0.995968076923077">
which constitutes the only supervision for the system.
nor the number of fields is specified in advance. To
accommodate this uncertainty, we apply a Bayesian
model which is nonparametric along three dimen-
sions: the assignment of text mentions to entities
(making popular entries more likely while always al-
lowing new entries); the alignment of individual text
tokens to fields (encouraging the re-use of common
fields, but permitting the creation of new fields); and
the assignment of values to entries in the database
itself (encouraging the reuse of values across entries
in a given field). By adaptively updating the con-
centration parameter of stick-breaking distribution
controlling the assignment of values to entries in the
database, our model can learn domain-specific infor-
mation about each field: for example, that titles are
often repeated, while names are more varied.
Our systems input consists of a very small proto-
type table and a corpus of text which has been au-
tomatically segmented to identify names. Our de-
sired output is a set of structured records in which
each field contains a single string not a distribu-
tion over strings, which would be more difficult to
interpret. This requirement induces a tight proba-
bilistic coupling between the assignment of text to
cells in the table, so special care is required to ob-
</bodyText>
<page confidence="0.966353">
2
</page>
<bodyText confidence="0.999380666666667">
\x0ctain efficient inference. Our procedure alternates
between two phases. In the first phase, we per-
form collapsed Gibbs sampling on the assignments
of string mentions to rows and columns in the table,
while marginalizing the values of the table itself. In
the second phase, we apply Metropolis-Hastings to
swap the values of columns in the table, while simul-
taneously relabeling the affected strings in the text.
Our model performs three tasks: it constructs a
set of entities from raw text, matches mentions in
text with the entities to which they refer, and discov-
ers general categories of tokens that appear in names
(such as titles and first names). We are aware of
no existing system that performs all three of these
tasks jointly. We evaluate on a dataset of political
blogs, measuring our systems ability to discover
a set of reference entities (recall) while maintain-
ing a compact number of rows and columns (pre-
cision). With as few as five partially-complete pro-
totype examples, our approach gives accurate tables
that match well against a manually-annotated refer-
ence list. Our method outperforms a baseline single-
link clustering approach inspired by one of the most
successful entries (Elmacioglu et al., 2007) in the
</bodyText>
<sectionHeader confidence="0.544702" genericHeader="introduction">
SEMEVAL Web People Search shared task (Ar-
</sectionHeader>
<bodyText confidence="0.775865">
tiles et al., 2007).
</bodyText>
<sectionHeader confidence="0.994437" genericHeader="method">
2 Task Definition
</sectionHeader>
<bodyText confidence="0.999807647058824">
In this work, we assume that a bag of M mentions
in text have been identified. The mth mention wm
is a sequence of contiguous word tokens (its length
is denoted Nm) understood to refer to a real-world
entity. The entities (and the mapping of mentions to
entities) are not known in advance. While our focus
in this paper is names of people, the task is defined
in a more generic way.
Formally, the task is to construct a table x where
rows correspond to entities and columns to func-
tional fields. The number of entities and the num-
ber of fields are not prespecified. x,j denotes the
jth column of x, and xi,j is a single word type fill-
ing the cell in row i, column j. An example is Ta-
ble 1, where the fields are first-name, last-name, ti-
tle, middle-name, and so on. In addition to the table,
we require that each mention be mapped to an en-
tity (i.e., a row in the table). Success at this task
therefore requires (i) identifying entities, (ii) discov-
ering the internal structure of mentions (effectively
canonicalizing them), and (iii) mapping mentions
to entities (therefore resolving coreference relation-
ships among mentions). Note that this task differs
from previous work on knowledge base population
(e.g., McNamee, 2009) because the schema is not
formally defined in advance; rather, the number of
fields and their meaning must be induced from just
a few prototype examples.
To incorporate partial supervision, a subset of the
table x is specified manually by an annotator. We
denote this subset of prototypes by x; for entries
that are unspecified by the user, we write xi,j = .
Prototypes are not assumed to provide complete in-
formation for any entity.
</bodyText>
<sectionHeader confidence="0.9969" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.975501708333333">
We now craft a nonparametric generative story that
explains both the latent table and the observed men-
tions. The model incorporates three nonparamet-
ric components, allowing an unbounded number of
rows (entities) and columns (fields), as well as an un-
bounded number of values per column (field values).
A plate diagram for the graphical model is shown in
Figure 1.
A key point is that the column distributions
range over possible values at the entity level, not
over mentions in text. For example, 2 might be
the distribution over possible last names and 3 the
distribution over elected office titles. Note that 2
would contain a low value for the last name Obama
which indicates that few people have this last
name even though a very high proportion of men-
tions in our data include the string Obama.
The user-generated entries (x) can still be treated
as the outcome of the generative process: using ex-
changeability, we treat these entries as the first sam-
ples drawn in each column. In this work, we treat
them as fully observed, but it is possible to treat
them as noisy and incorporate a stochastic depen-
dency between xi,j and xi,j.
</bodyText>
<sectionHeader confidence="0.999636" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.94716175">
We now develop sampling-based inference for the
model described in the previous section. We be-
gin with a token-based collapsed Gibbs sampler, and
then add larger-scale Metropolis-Hastings moves.
</bodyText>
<page confidence="0.873683">
3
</page>
<equation confidence="0.8442554">
\x0c 2
x
w r r
c r
c c
</equation>
<figureCaption confidence="0.9495265">
Figure 1: A plate diagram for the
text-and-tables graphical model.
</figureCaption>
<bodyText confidence="0.991382782608696">
The upper plate is the table x, and
the lower plate is the set of textual
mentions. Notation is defined in the
generative model to the right.
Generate the table entries. For each column j,
Draw a concentration parameter j from a log-normal distribution,
log j N(, 2
).
Draw a distribution over strings from a Dirichlet process j
DP(j, G0), where the base distribution G0 is a uniform distribution
over strings in a fixed character alphabet, up to an arbitrary finite length.
For each row i, draw the entry xi,j j.
Generate the text mentions.
Draw a prior distribution over rows from a stick-breaking distribution,
r Stick(r).
Draw a prior distribution over columns from a stick-breaking distribu-
tion, c Stick(c).
For each mention wm,
Draw a row in the table rm r.
For each word token wm,n (n {1, . . . , Nm}),
Draw a column in the table cm,n c.
Set the text wm,n = xrm,cm,n
.
</bodyText>
<subsectionHeader confidence="0.998823">
4.1 Gibbs sampling
</subsectionHeader>
<bodyText confidence="0.993061290322581">
A key aspect of the generative process is that the
word token wm,n is completely determined by the
table x and the row and column indicators rm and
cm,n: given that a token was generated by row i
and column j of the table, it must be identical to
the value of xi,j. Using Bayes rule, we can reverse
this deterministic dependence: given the values for
the row and column indices, the entries in the table
are restricted to exact matches with the text men-
tions that they generate. This allows us to marginal-
ize the unobserved entries in the table. We can also
marginalize the distributions r, c, and j, using
the standard collapsed Gibbs sampling equations for
Dirichlet processes. Thus, sampling the row and col-
umn indices is all that is required to explore the en-
tire space of model configurations.
4.1.1 Conditional probability for word tokens
The conditional sampling distributions for both
rows and columns will marginalize the table (be-
sides the prototypes x). To do this, we must be
able to compute P(wm,n  |rm = i, cm,n =
j, x, w(m,n), rm, c(m,n), j), which represents
the probability of generating word wm,n, given
rm = i and cm,n = j. The notation w(m,n), rm,
and cm,n represent the words, row indices, and col-
umn indices for all mentions besides wm,n. For sim-
plicity, we will elide these variables in much of the
subsequent notation.
We first consider the case where we have a user-
specified entry for the row and column hi, ji that
is, if xij 6= . Then the probability is simply,
</bodyText>
<equation confidence="0.9994678">
P(wm,n  |rm = i, cm,n = j, x, . . .) =
(
1, if xij = wm,n
0, if xij 6= wm,n.
(1)
</equation>
<bodyText confidence="0.986855142857143">
Because the table cell xij is observed, we do not
marginalize over it; we have a generative probability
of one if the word matches, and zero otherwise. If
the table cell xij is not specified by the user, then we
marginalize over its possible values. For any given
xij, the probability P(wm,n  |xij, rm = i, cm,n =
j) is still a delta function, so we have:
</bodyText>
<equation confidence="0.9942586">
Z
P(wm,n  |xrm,cm,n
)P(xrm,cm,n
 |. . .) dxrm,cm,n
= P(x = wm,n  |w(m,n), rm, c(m,n), x, . . .)
</equation>
<bodyText confidence="0.992064666666667">
The integral is equal to the probability of the value
of the cell xrm,cm,n being identical to the string
wm,n, given assignments to all other variables. To
compute this probability, we again must consider
two cases: if the cell xi,j has generated some other
string wm0,n0 then its value must be identical to that
</bodyText>
<page confidence="0.987916">
4
</page>
<bodyText confidence="0.9725975">
\x0cstring; otherwise it is unknown. More formally, for
any cell hi, ji, if wm0,n0 : rm0 = i cm0,n0 =
j hm0, n0i 6= hm, ni, then P(xi,j = wm0,n0 ) = 1;
all other strings have zero probability. If xi,j has not
generated any other entry, then its probability is con-
ditioned on the other elements of the table x. The
known elements of this table are themselves deter-
mined by either the user entries x or the observa-
tions w(m,n). We can define these known elements
as x, where xij = if xij = @hm, ni : rm =
i cm,n = j. Then we can apply the standard Chi-
nese restaurant process marginalization to obtain:
</bodyText>
<equation confidence="0.9999265">
P(xij  |x(i,j), ) =
( N(x(i,j)=xij )
N(x(i,j)6=)+ , N(x(i,j) = xij) &gt; 0
N(x(i,j)6=)+ , N(x(i,j) = xij) = 0
</equation>
<bodyText confidence="0.958360714285714">
(2)
In our implementation, we maintain the table x,
updating it as we resample the row and column as-
signments. To construct the conditional distribution
for any given entry, we first consult this table, and
then compute the probability in Equation 2 for en-
tries where xij = .
</bodyText>
<subsubsectionHeader confidence="0.991199">
4.1.2 Sampling columns
</subsubsectionHeader>
<bodyText confidence="0.843534555555556">
We can now derive sampling equations for the
column indices cm,n. We first apply Bayes rule
to obtain P(cm,n  |wm,n, rm, . . .) P(cm,n |
c(m,n), c) P(wm,n  |cm,n, rm, x, . . .). The like-
lihood term P(wm,n  |cm,n, . . .) is defined in the
previous section; we can compute the first factor us-
ing the standard Dirichlet process marginalization
over c. Writing N(c(m,n) = j) for the count of
occurrences of column j in the set c(m,n), we ob-
</bodyText>
<equation confidence="0.962376888888889">
tain
P(cm,n = j  |c(m,n), c) =
( N(c(m,n)=j)
N(c(m,n))+c
, if N(c(m,n) = j) &gt; 0
c
N(c(m,n))+c
, if N(c(m,n) = j) = 0
(3)
</equation>
<subsubsectionHeader confidence="0.974306">
4.1.3 Sampling rows
</subsubsectionHeader>
<bodyText confidence="0.9827485">
In principle the row indicators can be sampled
identically to the columns, with the caveat that the
generative probability P(wm  |rm, . . .) is a product
across all Nm tokens in wm.1 However, because of
</bodyText>
<page confidence="0.887368">
1
</page>
<bodyText confidence="0.998313857142857">
This relies on the assumption that the values of {cm,n} are
mutually independent given cm. Future work might apply
the tight probabilistic coupling between the row and
column indicators, straightforward Gibbs sampling
mixes slowly. Instead, we marginalize the column
indicators while sampling r. Only the likelihood
term is affected by this change:
</bodyText>
<equation confidence="0.997316666666667">
P(wm  |rm, wm, rm, . . .)
=
X
j
P(c = j  |cm, c)P(wm,n  |cm,n = j, rm, x, ).
(4)
</equation>
<bodyText confidence="0.999192125">
The tokens are conditionally independent given the
row, so we factor and then explicitly marginalize
over each cm,n. The chain rule gives the form in
Equation 4, which contains terms for the prior over
columns and the likelihood of the word; these are
defined in Equations 2 and 3. Note that neither the
inferred table x nor the heldout column counts cm
include counts from any of the cells in row m.
</bodyText>
<subsectionHeader confidence="0.994407">
4.2 Column swaps
</subsectionHeader>
<bodyText confidence="0.999597346153846">
Suppose that during initialization, we encounter the
string Barry Obama before encountering Barack
Obama. We would then put Barry in the first-name
column, and put Barack in some other column for
nicknames. After making these initial decisions,
they would be very difficult to undo using Gibbs
sampling we would have to first shift all instances
of Barry to another column, then move an instance
of Barack to the first-name column, and then move
the instances of Barry to the nickname column. To
rectify this issue, we perform sampling on the table
itself, swapping the columns of entries in the table,
while simultaneously updating the relevant column
indices of the mentions.
In the proposal, we select at random a row t and
indices i and j. In the table, we will swap xt,i with
xt,j; in the text we will swap the values of each cm,n
whenever rm = t and cm,n = i or j. This pro-
posal is symmetric, so no Hastings correction is re-
quired. Because we are simultaneously updating the
table and the column indices, the generative likeli-
hood of the words is unchanged; the only changes
a more structured model of the ways that fields are combined
when mentioning an entity. For example, a first-order Markov
model could learn that family names often follow given names,
but the reverse rarely occurs (in English).
</bodyText>
<page confidence="0.97301">
5
</page>
<bodyText confidence="0.9172526">
\x0cin the overall likelihood come from the column in-
dices and the values of the cells in the table. Letting
x, c indicate the state of the table and column in-
dices after the proposed move, we will accept with
probability,
</bodyText>
<equation confidence="0.993421375">
Paccept(x x
) = min
\x12
1,
P(c)P(x)
P(c)P(x)
\x13
(5)
</equation>
<bodyText confidence="0.9241334">
We first consider the ratio of the table probabili-
ties, P(x|)
P(x|) . Recall that each column of x is drawn
from a Dirichlet process; appealing to exchangeabil-
ity, we can treat the row t as the last element drawn,
and compute the probabilities P(xt,i  |x(t,i), i),
with x(t,i) indicating the elements of the column i
excluding row t. This probability is given by Equa-
tion 2. For a swap of columns i and j, we compute
the ratio:
</bodyText>
<equation confidence="0.930506333333333">
P(xt,i  |x(t,j), j)P(xt,j  |x(t,i), i)
P(xt,i  |x(t,i), i)P(xt,j  |x(t,j), j)
(6)
Next we consider the ratio of the column proba-
bilities, P(c)
P(c) . Again we can apply exchangeabil-
ity, P(c) = P({cm : rm = t}  |{cm0 : rm0 6=
t})P({cm0 : rm0 6= t}). The second term P({cm0 :
rm0 6= t}) is unaffected by the move, and so is iden-
</equation>
<bodyText confidence="0.96952725">
tical in both the numerator and denominator of the
likelihood ratio; probabilities from columns other
than i and j also cancel in this way. The remaining
ratio can be simplified to,
</bodyText>
<equation confidence="0.9961682">
\x12
P(c = j  |ct, c)
P(c = i  |ct, c)
\x13N(r=tc=i)N(r=tc=j)
(7)
</equation>
<bodyText confidence="0.9927408">
where the counts N() are from the state of the sam-
pler before executing the proposed move. The prob-
ability P(c = i  |ct, c) is defined in Equation 3,
and the overall acceptance ratio for column swaps is
the product of (6) and (7).
</bodyText>
<subsectionHeader confidence="0.996637">
4.3 Hyperparameters
</subsectionHeader>
<bodyText confidence="0.997209217391304">
The concentration parameters r and c help to con-
trol the number of rows and columns in the ta-
ble, respectively. These parameters are updated to
their maximum likelihood values using gradient-
based optimization, so our overall inference pro-
cedure is a form of Monte Carlo Expectation-
Maximization (Wei and Tanner, 1990).
The concentration parameters j control the di-
versity of each column in the table: if j is low then
we expect a high degree of repetition, as with titles;
if j is high then we expect a high degree of diver-
sity. When the sampling procedure adds a new col-
umn, there is very little information for how to set
its concentration parameter, as the conditional like-
lihood will be flat. Consequently, greater care must
be taken to handle these priors appropriately.
We place a log-normal hyperprior on the col-
umn concentration parameters, log j N(, 2).
The parameters of the log-normal are shared across
columns, which provides additional information to
constrain the concentration parameters of newly-
created columns. We then use Metropolis-Hastings
to sample the values of each j, using the joint like-
</bodyText>
<equation confidence="0.9837993">
lihood,
P(j, x(j)
 |, 2
)
exp((log j )2
)
kj
j (j)
22(nj + j)
,
</equation>
<bodyText confidence="0.992145571428572">
where x(j) is column j of the inferred table, nj is
the number of specified entries in column j of the
table x and kj is the number of unique entries in
the column; see Rasmussen (2000) for a derivation.
After repeatedly sampling several values of j for
each column in the table, we update and 2 to their
maximum-likelihood estimates.
</bodyText>
<sectionHeader confidence="0.992122" genericHeader="method">
5 Temporal Prominence
</sectionHeader>
<bodyText confidence="0.9990434375">
Andy Warhol predicted, in the future, everyone will
be world-famous for fifteen minutes. A model of
temporal dynamics that accounts for the fleeting and
fickle nature of fame might yield better performance
for transient entities, like Joe the Plumber. Among
several alternatives for modeling temporal dynamics
in latent variable models, we choose a simple non-
parametric approach: the recurrent Chinese restau-
rant process (RCRP; Ahmed and Xing, 2008). The
core idea of the RCRP is that time is partitioned into
epochs, with a unique Chinese restaurant process in
each epoch. Each CRP has a prior which takes the
form of pseudo-counts computed from the counts in
previous epochs. We employ the simplest version of
the RCRP, a first-order Markov model in which the
prior for epoch t is equal to the vector of counts for
</bodyText>
<equation confidence="0.933355357142857">
epoch t 1:
6
\x0cP(r(t)
m = i|r
(t)
1...m1, r(t1)
, r)
(
N(r
(t)
1...m1 = i) + N(r(t1)
= i), if &gt; 0;
r, otherwise.
(8)
</equation>
<bodyText confidence="0.651374">
The count of row i in epoch t 1 is written
</bodyText>
<equation confidence="0.99607725">
N(r(t1) = i); the count in epoch t for mentions
1 to m 1 is written N(r
(t)
1...m1 = i). As before,
</equation>
<bodyText confidence="0.974583333333333">
we can apply exchangeability to treat each mention
as the last in the epoch, so during inference we can
replace this with the count N(r
</bodyText>
<listItem confidence="0.877921">
(t)
m). Note that there
is zero probability of drawing an entity that has no
counts in epochs t or t 1 but exists in some other
epoch; the probability mass r is reserved for draw-
ing a new entity, and the chance of this matching
some existing entity from another epoch is vanish-
ingly small.
During Gibbs sampling, we also need to consider
the effect of r
(t)
</listItem>
<bodyText confidence="0.883449">
m on the subsequent epoch t + 1.
While space does not permit a derivation, the result-
ing probability is proportional to
</bodyText>
<equation confidence="0.999252727272727">
P(r(t+1)
|r
(t)
m, r(t)
m = i, r)
1 if N(r(t+1)
= i) = 0,
N(r(t+1)
=i)
r
if N(r
(t)
m = i) = 0,
1 + N(r(t+1)
=i)
N(r
(t)
m=i)
if N(r
(t)
m = i) &gt; 0.
(9)
</equation>
<bodyText confidence="0.9944119">
This favors entities which are frequent in epoch
t + 1 but infrequent in epoch t.
The move to a recurrent Chinese restaurant pro-
cess does not affect the sampling equations for the
columns c, nor the concentration parameters of the
table, . The only part of the inference procedure
that needs to be changed is the optimization of the
hyperparameter r; the log-likelihood is now the
sum across all epochs, and each epoch makes a con-
tribution to the gradient.
</bodyText>
<sectionHeader confidence="0.976455" genericHeader="method">
6 Evaluation Setup
</sectionHeader>
<bodyText confidence="0.997007446808511">
Our model jointly performs three tasks: identifying
a set of entities, discovering the set of fields, and
matching mention strings with the entities and fields
to which they refer. We are aware of no prior work
that performs these tasks jointly, nor any dataset that
is annotated for all three tasks.2 Consequently, we
focus our quantitative evaluation on what we take to
be the most important subtask: identifying the enti-
ties which are mentioned in raw text. We annotate
a new dataset of blog text for this purpose, and de-
sign precision and recall metrics to reward systems
that recover as much of the reference set as possi-
ble, while avoiding spurious entities and fields. We
also perform a qualitative analysis, noting the areas
where our method outperforms string matching ap-
proaches, and where there is need for further im-
provement.
Data Evaluation was performed on a corpus
of blogs describing United States politics in
2008 (Eisenstein and Xing, 2010). We ran the Stan-
ford Named Entity Recognition system (Finkel et
al., 2005) to obtain a set of 25,000 candidate men-
tions which the system judged to be names of peo-
ple. We then pruned strings that appeared fewer than
four times and eliminated strings with more than
seven tokens (these were usually errors). The result-
ing dataset has 19,247 mentions comprising 45,466
word tokens, and 813 unique mention strings.
Gold standard We develop a reference set of 100
entities for evaluation. This set was created by sort-
ing the unique name strings in the training set by fre-
quency, and manually merging strings that reference
the same entity. We also manually discarded strings
from the reference set if they resulted from errors in
the preprocessing pipeline (tokenization and named
entity recognition). Each entity is represented by
the set of all word tokens that appear in its refer-
ences; there are a total of 231 tokens for the 100 en-
tities. Most entities only include first and last names,
though the most frequent entities have many more:
for example, the entity Barack Obama has known
names: {Barack, Obama, Sen., Mr.}.
Metrics We evaluate the recall and precision of
a systems response set by matching against the
reference set. The first step is to create a bipar-
tite matching between response and reference enti-
ties.3 Using a cost function that quantifies the sim-
</bodyText>
<page confidence="0.957613">
2
</page>
<bodyText confidence="0.957451">
Recent work exploiting Wikipedia disambiguation pages
for evaluating cross-document coreference suggests an appeal-
ing alternative for future work (Singh et al., 2011).
</bodyText>
<page confidence="0.973347">
3
</page>
<bodyText confidence="0.7884525">
Bipartite matchings are typical in information extraction
evaluation metrics (e.g., Doddington et al., 2004).
</bodyText>
<page confidence="0.993315">
7
</page>
<bodyText confidence="0.996494446808511">
\x0cilarity of response and reference entities, we opti-
mize the matching using the Kuhn-Munkres algo-
rithm (Kuhn, 1955). For recall, the cost function
counts the number of shared word tokens, divided
by the number of word tokens in the reference enti-
ties; the recall is one minus the average cost of the
best matching (with a cost of one for reference enti-
ties that are not matched, and no cost for unmatched
response entities). Precision is computed identically,
but we normalize by the number of word tokens in
the response entity. Precision assigns a penalty of
one to unmatched response entities and no penalty
for unmatched reference entities.
Note that this metric grossly underrates the preci-
sion of all systems: the reference set is limited to 100
entities, but it is clear that our text mentions many
other people. This is harsh but fair: all systems are
penalized equally for identifying entities that are not
present in the reference set, and the ideal system will
recover the fifty reference entities (thus maximizing
recall) while keeping the table as compact as possi-
ble (thus maximizing precision). However, the raw
precision values have little meaning outside the con-
text of a direct comparison under identical experi-
mental conditions.
Systems The initial seed set for our system con-
sists of a partial annotation of five entities (Table 1)
larger seed sets did not improve performance. We
run the inference procedure described in the previ-
ous section for 20,000 iterations, and then obtain a
final database by taking the intersection of the in-
ferred tables x obtained at every 100 iterations, start-
ing with iteration 15,000. To account for variance
across Markov chains, we perform three different
runs. We evaluate a non-temporal version of our
model (as described in Sections 3 and 4), and a tem-
poral version with 5 epochs. For the non-temporal
version, a non-parallel C implementation had a wall
clock sampling time of roughly 16 hours; the tem-
poral version required 24 hours.
We compare against a baseline that incrementally
clusters strings into entities using a string edit dis-
tance metric, based on the work of Elmacioglu et
al. (2007). Starting from a configuration in which
each unique string forms its own cluster, we incre-
mentally merge clusters using the single-link crite-
rion, based on the minimum Jaccard edit distance
</bodyText>
<figure confidence="0.989107">
0.2 0.3 0.4 0.5 0.6 0.7
0
0.1
0.2
0.3
recall
precision
baseline
</figure>
<figureCaption confidence="0.687744333333333">
atemporal model
temporal model
Figure 2: The precision and recall of our models, as com-
</figureCaption>
<bodyText confidence="0.9804995">
pared to the curve defined by the incremental clustering
baseline. Each point indicates a unique sampling run.
</bodyText>
<table confidence="0.9422254">
Bill Clinton Benazir Bhutto
Nancy Pelosi Speaker
John Kerry Sen. Roberts
Martin King Dr. Jr. Luther
Bill Nelson
</table>
<tableCaption confidence="0.993546">
Table 2: A subset of the entity database discovered by
</tableCaption>
<bodyText confidence="0.986279444444445">
our model, hand selected to show highlight interesting
success and failure cases.
between each pair of clusters. This yields a series of
outputs that move along the precision-recall curve,
with precision increasing as the clusters encompass
more strings. There is prior work on heuristics for
selecting a stopping point, but we compare our re-
sults against the entire precision-recall curve (Man-
ning et al., 2008).
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.998663333333333">
The results of our evaluation are shown in Figure 2.
All sampling runs from our models lie well beyond
the precision-recall curve defined by the baseline
system, demonstrating the ability to achieve reason-
able recall with a far more compact database. The
baseline system can achieve nearly perfect recall by
creating one entity per unique string, but as it merges
strings to improve precision, its recall suffers sig-
nificantly. As noted above, perfect precision is not
possible on this task, because the reference set cov-
ers only a subset of the entities that appear in the
data. However, the numbers do measure the ability
to recover the reference entities in the most compact
table possible, allowing a quantitative comparison of
our models and the baseline approach.
</bodyText>
<page confidence="0.990847">
8
</page>
<bodyText confidence="0.99907856097561">
\x0cTable 2 shows a database identified by the atem-
poral version of our model. The most densely-
populated columns in the table correspond to well-
defined name parts: columns 1 and 2 are almost
exclusively populated with first and last names re-
spectively, and column 3 is mainly populated by ti-
tles. The remaining columns are more of a grab
bag. Column 4 correctly captures Jr. for Martin
Luther King; column 5 correctly captures Luther,
but mistakenly contains Roberts (thus merging the
John Kerry and John Roberts entities), and Bhutto
(thus helping to merge the Bill Clinton and Benazir
Bhutto entities).
The model successfully distinguishes some, but
not all, of the entities that share tokens. For example,
the model separates Bill Clinton from Bill Nelson;
it also separates John McCain from John Kerry
(whom it mistakenly merges with John Roberts).
The ability to distinguish individuals who share first
names is due in part to the model attributing a low
concentration parameter to first names, meaning that
some repetition in the first name column is expected.
The model correctly identifies several titles and al-
ternative names, including the rare title Speaker for
Nancy Pelosi; however, it misses others, such as the
Senator title for Bill Nelson. This may be due in
part to the sample merging procedure used to gener-
ate this table, which requires that a cell contain the
same value in at least 80% of the samples.
Many errors may be attributed to slow mixing.
After mistakenly merging Bhutto and Clinton at
an early stage, the Gibbs sampler which treats
each mention independently is unable to sep-
arate them. Given that several other mentions of
Bhutto are already in the row occupied by Clin-
ton, the overall likelihood would benefit little from
creating a new row for a single mention, though
moving all such mentions simultaneously would re-
sult in an improvement. Larger scale Metropolis-
Hastings moves, such as split-merge or type-based
sampling (Liang et al., 2010) may help.
</bodyText>
<sectionHeader confidence="0.9975" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.99181918">
Information Extraction A tradition of research
in information extraction focuses on processing raw
text to fill in the fields of manually-defined tem-
plates, thus populating databases of events or re-
lations (McNamee and Dang, 2009). While early
approaches focused on surface-level methods such
as wrapper induction (Kushmerick et al., 1997),
more recent work in this area includes Bayesian
nonparametrics to select the number of rows in the
database (Haghighi and Klein, 2010a). However,
even in such nonparametric work, the form of the
template and the number of slots are fixed in ad-
vance. Our approach differs in that the number of
fields and their meaning is learned from data. Recent
work by Chambers and Jurafsky (2011) approaches
a related problem, applying agglomerative cluster-
ing over sentences to detect events, and then clus-
tering syntactic constituents to induce the relevant
fields of each event entity. As described in Section 6,
our method performs well against an agglomerative
clustering baseline, though a more comprehensive
comparison of the two approaches is an important
step for future work.
Name Segmentation and Structure A related
stream of research focuses specifically on names:
identifying them in raw text, discovering their struc-
ture, and matching names that refer to the same en-
tity. We do not undertake the problem of named en-
tity recognition (Tjong Kim Sang, 2002), but rather
apply an existing NER system as a preprocessing
step (Finkel et al., 2005). Typical NER systems
do not attempt to discover the internal structure of
names or a database of canonical names, although
they often use prefabricated gazetteers of names
and name parts as features to improve performance
(Borthwick et al., 1998; Sarawagi and Cohen, 2005).
Charniak (2001) shows that it is possible to learn a
model of name structure, either by using coreference
information as labeled data, or by leveraging a small
set of hand-crafted constraints. Elsner et al. (2009)
develop a nonparametric Bayesian model of name
structure using adaptor grammars, which they use to
distinguish types of names (e.g., people, places, and
organizations). Li et al. (2004) use a set of manually-
crafted transformations of name parts to build a
model of how a name might be rendered in multi-
ple different ways. While each of these approaches
bears on one or more facets of the problem that we
consider here, none provides a holistic treatment of
name disambiguation and structure.
</bodyText>
<page confidence="0.940044">
9
</page>
<bodyText confidence="0.997638">
\x0cResolving Mentions to Entities The problem of
resolving mentions to entities has been approach
from a variety of different perspectives. There is
an extensive literature on probabilistic record link-
age, in which database records are compared to de-
termine if they are likely to have the same real-world
referents (e.g., Felligi and Sunter, 1969; Bilenko
et al., 2003). Most approaches focus on pairwise
assessments of whether two records are the same,
whereas our method attempts to infer a single coher-
ent model of the underlying relational data. Some
more recent work in record linkage has explicitly
formulated the task of inferring a latent relational
model of a set of observed datasets (e.g., Cohen
et al., 2000; Pasula et al., 2002; Bhattacharya and
Getoor, 2007); however, to our knowledge, these
prior models have all exploited some predefined
database schema (i.e., set of columns), which our
model does not require. Many of these prior mod-
els have been applied to bibliographic data, where
different conventions and abbreviations lead to im-
perfect matches in different references to the same
publication. In our task, we consider name mentions
in raw text; such mentions are short, and may not
offer as many redundant clues for linkage as biblio-
graphic references.
In natural language processing, coreference res-
olution is the task of grouping entity mentions
(strings), in one or more documents, based on their
common referents in the world. Although much of
coreference resolution has on the single document
setting, there has been some recent work on cross-
document coreference resolution (Li et al., 2004;
Haghighi and Klein, 2007; Poon and Domingos,
2008; Singh et al., 2011). The problem we consider
is related to cross-document coreference, although
we take on the additional challenge of providing
a canonicalized name for each referent (the corre-
sponding table row), and in inferring a structured
representation of entity names (the table columns).
For this reason, our evaluation focuses on the in-
duced table of entities, rather than the clustering of
mention strings. The best coreference systems de-
pend on carefully crafted, problem-specific linguis-
tic features (Bengtson and Roth, 2008) and exter-
nal knowledge (Haghighi and Klein, 2010b). Future
work might consider how to exploit such features for
the more holistic information extraction setting.
</bodyText>
<sectionHeader confidence="0.990898" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.988436307692308">
This paper presents a Bayesian nonparametric ap-
proach to recover structured records from text. Us-
ing only a small set of prototype records, we are able
to recover an accurate table that jointly identifies en-
tities and internal name structure. In our view, the
main advantage of a Bayesian approach compared
to more heuristic alternatives is that it facilitates in-
corporation of additional information sources when
available. In this paper, we have considered one
such additional source, incorporating temporal con-
text using the recurrent Chinese restaurant process.
We envision enhancing the model in several other
respects. One promising direction is the incorpo-
ration of name structure, which could be captured
using a first-order Markov model of the transitions
between name parts. In the nonparametric setting,
a transition matrix is unbounded along both dimen-
sions, and this can be handled by a hierarchical
Dirichlet process (HDP; Teh et al 2006).4 We en-
vision other potential applications of the HDP: for
example, learning topics of entities which tend to
appear together (i.e., given a mention of Mahmoud
Abbas in the American press, a mention of Ben-
jamin Netanyahu is likely), and handling document-
specific burstiness (i.e., given that an entity is men-
tioned once in a document, it is much more likely
to be mentioned again). Finally, we would like
to incorporate lexical context from the sentences in
which each entity is mentioned, which might help to
distinguish, say, computer science researchers who
share names with former defense secretaries or pro-
fessional basketball players.
Acknowledgments This research was enabled
by AFOSR FA95501010247, DARPA grant
N10AP20042, ONR N000140910758, NSF DBI-
0546594, IIS-0713379, IIS-0915187, IIS-0811562,
an Alfred P. Sloan Fellowship, and Googles support
of the Worldly Knowledge project at CMU. We
thank the reviewers for their thoughtful feedback.
</bodyText>
<page confidence="0.931755">
4
</page>
<bodyText confidence="0.9995595">
One of the reviewers proposed to draw entire column se-
quences from a Dirichlet process. Given the relatively small
number of columns and canonical name forms, this may be a
straightforward and effective alternative to the HDP.
</bodyText>
<page confidence="0.87264">
10
</page>
<bodyText confidence="0.894314636363637">
\x0cReferences
Amr Ahmed and Eric P. Xing. 2008. Dynamic non-
parametric mixture models and the recurrent Chinese
restaurant process with applications to evolutionary
clustering. In International Conference on Data Min-
ing.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007.
The SemEval-2007 WePS evaluation: establishing a
benchmark for the web people search task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations, SemEval 07, pages 6469. Associa-
</bodyText>
<reference confidence="0.965496421052632">
tion for Computational Linguistics.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 08,
pages 294303, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Indrajit Bhattacharya and Lise Getoor. 2007. Collec-
tive entity resolution in relational data. ACM Trans.
Knowl. Discov. Data, 1(1), March.
Mikhail Bilenko, William W. Cohen, Stephen Fien-
berg, Raymond J. Mooney, and Pradeep Ravikumar.
2003. Adaptive name-matching in information in-
tegration. IEEE Intelligent Systems, 18(5):1623,
September/October.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition. In Sixth
Workshop on Very Large Corpora New Brunswick,
New Jersey. Association for Computational Linguis-
tics.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of ACL.
Eugene Charniak. 2001. Unsupervised learning of name
structure from coreference data. In Proceedings of the
Second Meeting of the North American Chapter of the
Association for Computational Linguistics.
William W. Cohen, Henry Kautz, and David McAllester.
2000. Hardening soft information sources. In Pro-
ceedings of the Sixth International Conference on
Knowledge Discovery and Data Mining, pages 255
259.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) program: Tasks, data, and evaluation. In 4th
international conference on language resources and
evaluation (LREC04).
Jacob Eisenstein and Eric Xing. 2010. The CMU 2008
political blog corpus. Technical report, Carnegie Mel-
lon University.
Ergin Elmacioglu, Yee Fan Tan, Su Yan, Min-Yen Kan,
and Dongwon Lee. 2007. Psnus: Web people name
disambiguation by simple clustering with rich features.
In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 268
271, Prague, Czech Republic, June. Association for
Computational Linguistics.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsupervised
named-entity clustering. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 164172, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
I. P. Felligi and A. B. Sunter. 1969. A theory for record
linkage. Journal of the American Statistical Society,
64:11831210.
Jenny R. Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information
into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL 05,
pages 363370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
848855, Prague, Czech Republic, June. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2010a. An entity-level
approach to information extraction. In Proceedings
of the ACL 2010 Conference Short Papers, ACLShort
10, pages 291295, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Aria Haghighi and Dan Klein. 2010b. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385393, Los An-
geles, California, June. Association for Computational
Linguistics.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistic Quar-
terly, 2:8397.
Nicholas Kushmerick, Daniel S. Weld, and Robert
Doorenbos. 1997. Wrapper induction for information
extraction. In Proceedings of IJCAI.
Xin Li, Paul Morie, and Dan Roth. 2004. Identification
and tracing of ambiguous names: Discriminative and
generative approaches. In Proceedings of AAAI, pages
419424.
</reference>
<page confidence="0.991743">
11
</page>
<reference confidence="0.9960535">
\x0cPercy Liang, Michael I. Jordan, and Dan Klein. 2010.
Type-Based MCMC. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 573581, Los Angeles, California,
June. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schutze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, 1 edition, July.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track. In
Proceedings of the Text Analysis Conference (TAC).
Hanna Pasula, Bhaskara Marthi, Brian Milch, Stuart Rus-
sell, and Ilya Shpitser. 2002. Identity uncertainty and
citation matching. In Advances in Neural Processing
Systems 15, Vancouver, British Columbia. MIT Press.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 650
659, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Carl E. Rasmussen. 2000. The Infinite Gaussian Mixture
Model. In In Advances in Neural Information Process-
ing Systems 12, volume 12, pages 554560.
Sunita Sarawagi and William W. Cohen. 2005. Semi-
Markov conditional random fields for information ex-
traction. In Lawrence K. Saul, Yair Weiss, and Leon
Bottou, editors, Advances in Neural Information Pro-
cessing Systems 17, pages 11851192. MIT Press,
Cambridge, MA.
Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2011. Large-scale cross-
document coreference using distributed inference and
hierarchical models. In Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT).
Yee W. Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:15661581, December.
Erik F. Tjong Kim Sang. 2002. Introduction to
the CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of the Sixth
Conference on Natural Language Learning.
Greg C. G. Wei and Martin A. Tanner. 1990. A Monte
Carlo Implementation of the EM Algorithm and the
Poor Mans Data Augmentation Algorithms. Journal
of the American Statistical Association, 85(411):699
704.
</reference>
<page confidence="0.690948">
12
</page>
<figure confidence="0.464067">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.720093">
<note confidence="0.949764333333333">b&apos;Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 212, Edinburgh, Scotland, UK, July 2731, 2011. c 2011 Association for Computational Linguistics</note>
<title confidence="0.88294">Structured Databases of Named Entities from Bayesian Nonparametrics</title>
<author confidence="0.999907">Jacob Eisenstein Tae Yano William W Cohen Noah A Smith Eric P Xing</author>
<affiliation confidence="0.9997405">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.99341">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999788">jacobeis@cs.cmu.edu</email>
<email confidence="0.999788">taey@cs.cmu.edu</email>
<email confidence="0.999788">wcohen@cs.cmu.edu</email>
<email confidence="0.999788">nasmith@cs.cmu.edu</email>
<email confidence="0.999788">epxing@cs.cmu.edu</email>
<abstract confidence="0.996825142857143">We present a nonparametric Bayesian approach to extract a structured database of entities from text. Neither the number of entities nor the fields that characterize each entity are provided in advance; the only supervision is a set of five prototype examples. Our method jointly accomplishes three tasks: (i) identifying a set of canonical entities, (ii) inferring a schema for the fields that describe each entity, and (iii) matching entities to their references in raw text. Empirical evaluation shows that the approach learns an accurate database of entities and a sensible model of name structure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dan Roth</author>
</authors>
<title>tion for Computational Linguistics. Eric Bengtson</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP 08,</booktitle>
<pages>294303</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="33673" citStr="Roth, 2008" startWordPosition="5772" endWordPosition="5773">ion (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). Future work might consider how to exploit such features for the more holistic information extraction setting. 9 Conclusion This paper presents a Bayesian nonparametric approach to recover structured records from text. Using only a small set of prototype records, we are able to recover an accurate table that jointly identifies entities and internal name structure. In our view, the main advantage of a Bayesian approach compared to more heuristic alternatives is that it facilitates incorporation of additional information sources when available.</context>
</contexts>
<marker>Roth, 2008</marker>
<rawString>tion for Computational Linguistics. Eric Bengtson and Dan Roth. 2008. Understanding the value of features for coreference resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP 08, pages 294303, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Indrajit Bhattacharya</author>
<author>Lise Getoor</author>
</authors>
<title>Collective entity resolution in relational data.</title>
<date>2007</date>
<journal>ACM Trans. Knowl. Discov. Data,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="32234" citStr="Bhattacharya and Getoor, 2007" startWordPosition="5545" endWordPosition="5548">ere is an extensive literature on probabilistic record linkage, in which database records are compared to determine if they are likely to have the same real-world referents (e.g., Felligi and Sunter, 1969; Bilenko et al., 2003). Most approaches focus on pairwise assessments of whether two records are the same, whereas our method attempts to infer a single coherent model of the underlying relational data. Some more recent work in record linkage has explicitly formulated the task of inferring a latent relational model of a set of observed datasets (e.g., Cohen et al., 2000; Pasula et al., 2002; Bhattacharya and Getoor, 2007); however, to our knowledge, these prior models have all exploited some predefined database schema (i.e., set of columns), which our model does not require. Many of these prior models have been applied to bibliographic data, where different conventions and abbreviations lead to imperfect matches in different references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreference resolution is the task of grouping entity mentions</context>
</contexts>
<marker>Bhattacharya, Getoor, 2007</marker>
<rawString>Indrajit Bhattacharya and Lise Getoor. 2007. Collective entity resolution in relational data. ACM Trans. Knowl. Discov. Data, 1(1), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Bilenko</author>
<author>William W Cohen</author>
<author>Stephen Fienberg</author>
<author>Raymond J Mooney</author>
<author>Pradeep Ravikumar</author>
</authors>
<title>Adaptive name-matching in information integration.</title>
<date>2003</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>18</volume>
<issue>5</issue>
<pages>September/October.</pages>
<contexts>
<context position="31831" citStr="Bilenko et al., 2003" startWordPosition="5479" endWordPosition="5482">s to build a model of how a name might be rendered in multiple different ways. While each of these approaches bears on one or more facets of the problem that we consider here, none provides a holistic treatment of name disambiguation and structure. 9 \x0cResolving Mentions to Entities The problem of resolving mentions to entities has been approach from a variety of different perspectives. There is an extensive literature on probabilistic record linkage, in which database records are compared to determine if they are likely to have the same real-world referents (e.g., Felligi and Sunter, 1969; Bilenko et al., 2003). Most approaches focus on pairwise assessments of whether two records are the same, whereas our method attempts to infer a single coherent model of the underlying relational data. Some more recent work in record linkage has explicitly formulated the task of inferring a latent relational model of a set of observed datasets (e.g., Cohen et al., 2000; Pasula et al., 2002; Bhattacharya and Getoor, 2007); however, to our knowledge, these prior models have all exploited some predefined database schema (i.e., set of columns), which our model does not require. Many of these prior models have been app</context>
</contexts>
<marker>Bilenko, Cohen, Fienberg, Mooney, Ravikumar, 2003</marker>
<rawString>Mikhail Bilenko, William W. Cohen, Stephen Fienberg, Raymond J. Mooney, and Pradeep Ravikumar. 2003. Adaptive name-matching in information integration. IEEE Intelligent Systems, 18(5):1623, September/October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
<author>J Sterling</author>
<author>E Agichtein</author>
<author>R Grishman</author>
</authors>
<title>Exploiting diverse knowledge sources via maximum entropy in named entity recognition.</title>
<date>1998</date>
<booktitle>In Sixth Workshop on Very Large Corpora</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New Brunswick, New Jersey.</location>
<contexts>
<context position="30724" citStr="Borthwick et al., 1998" startWordPosition="5299" endWordPosition="5302">work. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated gazetteers of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted transformations of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches bear</context>
</contexts>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman. 1998. Exploiting diverse knowledge sources via maximum entropy in named entity recognition. In Sixth Workshop on Very Large Corpora New Brunswick, New Jersey. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Templatebased information extraction without the templates.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="29718" citStr="Chambers and Jurafsky (2011)" startWordPosition="5140" endWordPosition="5143">aw text to fill in the fields of manually-defined templates, thus populating databases of events or relations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approaches is an important step for future work. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Templatebased information extraction without the templates. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Unsupervised learning of name structure from coreference data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="30768" citStr="Charniak (2001)" startWordPosition="5307" endWordPosition="5308">eam of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated gazetteers of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted transformations of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches bears on one or more facets of the problem that </context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Unsupervised learning of name structure from coreference data. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Henry Kautz</author>
<author>David McAllester</author>
</authors>
<title>Hardening soft information sources.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>255--259</pages>
<contexts>
<context position="32181" citStr="Cohen et al., 2000" startWordPosition="5537" endWordPosition="5540">m a variety of different perspectives. There is an extensive literature on probabilistic record linkage, in which database records are compared to determine if they are likely to have the same real-world referents (e.g., Felligi and Sunter, 1969; Bilenko et al., 2003). Most approaches focus on pairwise assessments of whether two records are the same, whereas our method attempts to infer a single coherent model of the underlying relational data. Some more recent work in record linkage has explicitly formulated the task of inferring a latent relational model of a set of observed datasets (e.g., Cohen et al., 2000; Pasula et al., 2002; Bhattacharya and Getoor, 2007); however, to our knowledge, these prior models have all exploited some predefined database schema (i.e., set of columns), which our model does not require. Many of these prior models have been applied to bibliographic data, where different conventions and abbreviations lead to imperfect matches in different references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreferen</context>
</contexts>
<marker>Cohen, Kautz, McAllester, 2000</marker>
<rawString>William W. Cohen, Henry Kautz, and David McAllester. 2000. Hardening soft information sources. In Proceedings of the Sixth International Conference on Knowledge Discovery and Data Mining, pages 255 259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark Przybocki</author>
<author>Lance Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph Weischedel</author>
</authors>
<title>The automatic content extraction (ace) program: Tasks, data, and evaluation.</title>
<date>2004</date>
<booktitle>In 4th international conference on language resources and evaluation (LREC04).</booktitle>
<contexts>
<context position="23013" citStr="Doddington et al., 2004" startWordPosition="4034" endWordPosition="4037">have many more: for example, the entity Barack Obama has known names: {Barack, Obama, Sen., Mr.}. Metrics We evaluate the recall and precision of a systems response set by matching against the reference set. The first step is to create a bipartite matching between response and reference entities.3 Using a cost function that quantifies the sim2 Recent work exploiting Wikipedia disambiguation pages for evaluating cross-document coreference suggests an appealing alternative for future work (Singh et al., 2011). 3 Bipartite matchings are typical in information extraction evaluation metrics (e.g., Doddington et al., 2004). 7 \x0cilarity of response and reference entities, we optimize the matching using the Kuhn-Munkres algorithm (Kuhn, 1955). For recall, the cost function counts the number of shared word tokens, divided by the number of word tokens in the reference entities; the recall is one minus the average cost of the best matching (with a cost of one for reference entities that are not matched, and no cost for unmatched response entities). Precision is computed identically, but we normalize by the number of word tokens in the response entity. Precision assigns a penalty of one to unmatched response entiti</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The automatic content extraction (ace) program: Tasks, data, and evaluation. In 4th international conference on language resources and evaluation (LREC04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Eric Xing</author>
</authors>
<title>political blog corpus.</title>
<date>2010</date>
<booktitle>The CMU</booktitle>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="21366" citStr="Eisenstein and Xing, 2010" startWordPosition="3764" endWordPosition="3767">titative evaluation on what we take to be the most important subtask: identifying the entities which are mentioned in raw text. We annotate a new dataset of blog text for this purpose, and design precision and recall metrics to reward systems that recover as much of the reference set as possible, while avoiding spurious entities and fields. We also perform a qualitative analysis, noting the areas where our method outperforms string matching approaches, and where there is need for further improvement. Data Evaluation was performed on a corpus of blogs describing United States politics in 2008 (Eisenstein and Xing, 2010). We ran the Stanford Named Entity Recognition system (Finkel et al., 2005) to obtain a set of 25,000 candidate mentions which the system judged to be names of people. We then pruned strings that appeared fewer than four times and eliminated strings with more than seven tokens (these were usually errors). The resulting dataset has 19,247 mentions comprising 45,466 word tokens, and 813 unique mention strings. Gold standard We develop a reference set of 100 entities for evaluation. This set was created by sorting the unique name strings in the training set by frequency, and manually merging stri</context>
</contexts>
<marker>Eisenstein, Xing, 2010</marker>
<rawString>Jacob Eisenstein and Eric Xing. 2010. The CMU 2008 political blog corpus. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergin Elmacioglu</author>
<author>Yee Fan Tan</author>
<author>Su Yan</author>
<author>Min-Yen Kan</author>
<author>Dongwon Lee</author>
</authors>
<title>Psnus: Web people name disambiguation by simple clustering with rich features.</title>
<date>2007</date>
<contexts>
<context position="4679" citStr="Elmacioglu et al., 2007" startWordPosition="740" endWordPosition="743">s that appear in names (such as titles and first names). We are aware of no existing system that performs all three of these tasks jointly. We evaluate on a dataset of political blogs, measuring our systems ability to discover a set of reference entities (recall) while maintaining a compact number of rows and columns (precision). With as few as five partially-complete prototype examples, our approach gives accurate tables that match well against a manually-annotated reference list. Our method outperforms a baseline singlelink clustering approach inspired by one of the most successful entries (Elmacioglu et al., 2007) in the SEMEVAL Web People Search shared task (Artiles et al., 2007). 2 Task Definition In this work, we assume that a bag of M mentions in text have been identified. The mth mention wm is a sequence of contiguous word tokens (its length is denoted Nm) understood to refer to a real-world entity. The entities (and the mapping of mentions to entities) are not known in advance. While our focus in this paper is names of people, the task is defined in a more generic way. Formally, the task is to construct a table x where rows correspond to entities and columns to functional fields. The number of en</context>
<context position="25174" citStr="Elmacioglu et al. (2007)" startWordPosition="4394" endWordPosition="4397">he intersection of the inferred tables x obtained at every 100 iterations, starting with iteration 15,000. To account for variance across Markov chains, we perform three different runs. We evaluate a non-temporal version of our model (as described in Sections 3 and 4), and a temporal version with 5 epochs. For the non-temporal version, a non-parallel C implementation had a wall clock sampling time of roughly 16 hours; the temporal version required 24 hours. We compare against a baseline that incrementally clusters strings into entities using a string edit distance metric, based on the work of Elmacioglu et al. (2007). Starting from a configuration in which each unique string forms its own cluster, we incrementally merge clusters using the single-link criterion, based on the minimum Jaccard edit distance 0.2 0.3 0.4 0.5 0.6 0.7 0 0.1 0.2 0.3 recall precision baseline atemporal model temporal model Figure 2: The precision and recall of our models, as compared to the curve defined by the incremental clustering baseline. Each point indicates a unique sampling run. Bill Clinton Benazir Bhutto Nancy Pelosi Speaker John Kerry Sen. Roberts Martin King Dr. Jr. Luther Bill Nelson Table 2: A subset of the entity dat</context>
</contexts>
<marker>Elmacioglu, Tan, Yan, Kan, Lee, 2007</marker>
<rawString>Ergin Elmacioglu, Yee Fan Tan, Su Yan, Min-Yen Kan, and Dongwon Lee. 2007. Psnus: Web people name disambiguation by simple clustering with rich features.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>268--271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker></marker>
<rawString>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 268 271, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Structured generative models for unsupervised named-entity clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>164172</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="30966" citStr="Elsner et al. (2009)" startWordPosition="5338" endWordPosition="5341">med entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated gazetteers of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted transformations of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches bears on one or more facets of the problem that we consider here, none provides a holistic treatment of name disambiguation and structure. 9 \x0cResolving Mentions to Entities The problem of resolving mentions to entities has been approach from a</context>
</contexts>
<marker>Elsner, Charniak, Johnson, 2009</marker>
<rawString>Micha Elsner, Eugene Charniak, and Mark Johnson. 2009. Structured generative models for unsupervised named-entity clustering. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 164172, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I P Felligi</author>
<author>A B Sunter</author>
</authors>
<title>A theory for record linkage.</title>
<date>1969</date>
<journal>Journal of the American Statistical Society,</journal>
<pages>64--11831210</pages>
<contexts>
<context position="31808" citStr="Felligi and Sunter, 1969" startWordPosition="5475" endWordPosition="5478">ansformations of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches bears on one or more facets of the problem that we consider here, none provides a holistic treatment of name disambiguation and structure. 9 \x0cResolving Mentions to Entities The problem of resolving mentions to entities has been approach from a variety of different perspectives. There is an extensive literature on probabilistic record linkage, in which database records are compared to determine if they are likely to have the same real-world referents (e.g., Felligi and Sunter, 1969; Bilenko et al., 2003). Most approaches focus on pairwise assessments of whether two records are the same, whereas our method attempts to infer a single coherent model of the underlying relational data. Some more recent work in record linkage has explicitly formulated the task of inferring a latent relational model of a set of observed datasets (e.g., Cohen et al., 2000; Pasula et al., 2002; Bhattacharya and Getoor, 2007); however, to our knowledge, these prior models have all exploited some predefined database schema (i.e., set of columns), which our model does not require. Many of these pri</context>
</contexts>
<marker>Felligi, Sunter, 1969</marker>
<rawString>I. P. Felligi and A. B. Sunter. 1969. A theory for record linkage. Journal of the American Statistical Society, 64:11831210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL 05,</booktitle>
<pages>363370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="21441" citStr="Finkel et al., 2005" startWordPosition="3777" endWordPosition="3780"> the entities which are mentioned in raw text. We annotate a new dataset of blog text for this purpose, and design precision and recall metrics to reward systems that recover as much of the reference set as possible, while avoiding spurious entities and fields. We also perform a qualitative analysis, noting the areas where our method outperforms string matching approaches, and where there is need for further improvement. Data Evaluation was performed on a corpus of blogs describing United States politics in 2008 (Eisenstein and Xing, 2010). We ran the Stanford Named Entity Recognition system (Finkel et al., 2005) to obtain a set of 25,000 candidate mentions which the system judged to be names of people. We then pruned strings that appeared fewer than four times and eliminated strings with more than seven tokens (these were usually errors). The resulting dataset has 19,247 mentions comprising 45,466 word tokens, and 813 unique mention strings. Gold standard We develop a reference set of 100 entities for evaluation. This set was created by sorting the unique name strings in the training set by frequency, and manually merging strings that reference the same entity. We also manually discarded strings from</context>
<context position="30478" citStr="Finkel et al., 2005" startWordPosition="5260" endWordPosition="5263"> to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approaches is an important step for future work. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated gazetteers of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny R. Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL 05, pages 363370, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Unsupervised coreference resolution in a nonparametric bayesian model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>848855</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="33108" citStr="Haghighi and Klein, 2007" startWordPosition="5684" endWordPosition="5687">ns and abbreviations lead to imperfect matches in different references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreference resolution is the task of grouping entity mentions (strings), in one or more documents, based on their common referents in the world. Although much of coreference resolution has on the single document setting, there has been some recent work on crossdocument coreference resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi a</context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>Aria Haghighi and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric bayesian model. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 848855, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>An entity-level approach to information extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort 10,</booktitle>
<pages>291295</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="29468" citStr="Haghighi and Klein, 2010" startWordPosition="5096" endWordPosition="5099">lt in an improvement. Larger scale MetropolisHastings moves, such as split-merge or type-based sampling (Liang et al., 2010) may help. 8 Related Work Information Extraction A tradition of research in information extraction focuses on processing raw text to fill in the fields of manually-defined templates, thus populating databases of events or relations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approache</context>
<context position="33722" citStr="Haghighi and Klein, 2010" startWordPosition="5778" endWordPosition="5781">lein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). Future work might consider how to exploit such features for the more holistic information extraction setting. 9 Conclusion This paper presents a Bayesian nonparametric approach to recover structured records from text. Using only a small set of prototype records, we are able to recover an accurate table that jointly identifies entities and internal name structure. In our view, the main advantage of a Bayesian approach compared to more heuristic alternatives is that it facilitates incorporation of additional information sources when available. In this paper, we have considered one such addit</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010a. An entity-level approach to information extraction. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort 10, pages 291295, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>385393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="29468" citStr="Haghighi and Klein, 2010" startWordPosition="5096" endWordPosition="5099">lt in an improvement. Larger scale MetropolisHastings moves, such as split-merge or type-based sampling (Liang et al., 2010) may help. 8 Related Work Information Extraction A tradition of research in information extraction focuses on processing raw text to fill in the fields of manually-defined templates, thus populating databases of events or relations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approache</context>
<context position="33722" citStr="Haghighi and Klein, 2010" startWordPosition="5778" endWordPosition="5781">lein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). Future work might consider how to exploit such features for the more holistic information extraction setting. 9 Conclusion This paper presents a Bayesian nonparametric approach to recover structured records from text. Using only a small set of prototype records, we are able to recover an accurate table that jointly identifies entities and internal name structure. In our view, the main advantage of a Bayesian approach compared to more heuristic alternatives is that it facilitates incorporation of additional information sources when available. In this paper, we have considered one such addit</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010b. Coreference resolution in a modular, entity-centered model. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 385393, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold W Kuhn</author>
</authors>
<title>The Hungarian method for the assignment problem.</title>
<date>1955</date>
<journal>Naval Research Logistic Quarterly,</journal>
<pages>2--8397</pages>
<contexts>
<context position="23135" citStr="Kuhn, 1955" startWordPosition="4055" endWordPosition="4056">ecision of a systems response set by matching against the reference set. The first step is to create a bipartite matching between response and reference entities.3 Using a cost function that quantifies the sim2 Recent work exploiting Wikipedia disambiguation pages for evaluating cross-document coreference suggests an appealing alternative for future work (Singh et al., 2011). 3 Bipartite matchings are typical in information extraction evaluation metrics (e.g., Doddington et al., 2004). 7 \x0cilarity of response and reference entities, we optimize the matching using the Kuhn-Munkres algorithm (Kuhn, 1955). For recall, the cost function counts the number of shared word tokens, divided by the number of word tokens in the reference entities; the recall is one minus the average cost of the best matching (with a cost of one for reference entities that are not matched, and no cost for unmatched response entities). Precision is computed identically, but we normalize by the number of word tokens in the response entity. Precision assigns a penalty of one to unmatched response entities and no penalty for unmatched reference entities. Note that this metric grossly underrates the precision of all systems:</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>Harold W. Kuhn. 1955. The Hungarian method for the assignment problem. Naval Research Logistic Quarterly, 2:8397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Kushmerick</author>
<author>Daniel S Weld</author>
<author>Robert Doorenbos</author>
</authors>
<title>Wrapper induction for information extraction.</title>
<date>1997</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="29333" citStr="Kushmerick et al., 1997" startWordPosition="5074" endWordPosition="5077">likelihood would benefit little from creating a new row for a single mention, though moving all such mentions simultaneously would result in an improvement. Larger scale MetropolisHastings moves, such as split-merge or type-based sampling (Liang et al., 2010) may help. 8 Related Work Information Extraction A tradition of research in information extraction focuses on processing raw text to fill in the fields of manually-defined templates, thus populating databases of events or relations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Sectio</context>
</contexts>
<marker>Kushmerick, Weld, Doorenbos, 1997</marker>
<rawString>Nicholas Kushmerick, Daniel S. Weld, and Robert Doorenbos. 1997. Wrapper induction for information extraction. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Paul Morie</author>
<author>Dan Roth</author>
</authors>
<title>Identification and tracing of ambiguous names: Discriminative and generative approaches.</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>419424</pages>
<contexts>
<context position="31152" citStr="Li et al. (2004)" startWordPosition="5366" endWordPosition="5369">nal structure of names or a database of canonical names, although they often use prefabricated gazetteers of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted transformations of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches bears on one or more facets of the problem that we consider here, none provides a holistic treatment of name disambiguation and structure. 9 \x0cResolving Mentions to Entities The problem of resolving mentions to entities has been approach from a variety of different perspectives. There is an extensive literature on probabilistic record linkage, in which database records are compared to determine if they are likely to have the s</context>
<context position="33082" citStr="Li et al., 2004" startWordPosition="5680" endWordPosition="5683">fferent conventions and abbreviations lead to imperfect matches in different references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreference resolution is the task of grouping entity mentions (strings), in one or more documents, based on their common referents in the world. Although much of coreference resolution has on the single document setting, there has been some recent work on crossdocument coreference resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and exte</context>
</contexts>
<marker>Li, Morie, Roth, 2004</marker>
<rawString>Xin Li, Paul Morie, and Dan Roth. 2004. Identification and tracing of ambiguous names: Discriminative and generative approaches. In Proceedings of AAAI, pages 419424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cPercy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<date>2010</date>
<contexts>
<context position="28968" citStr="Liang et al., 2010" startWordPosition="5019" endWordPosition="5022">at a cell contain the same value in at least 80% of the samples. Many errors may be attributed to slow mixing. After mistakenly merging Bhutto and Clinton at an early stage, the Gibbs sampler which treats each mention independently is unable to separate them. Given that several other mentions of Bhutto are already in the row occupied by Clinton, the overall likelihood would benefit little from creating a new row for a single mention, though moving all such mentions simultaneously would result in an improvement. Larger scale MetropolisHastings moves, such as split-merge or type-based sampling (Liang et al., 2010) may help. 8 Related Work Information Extraction A tradition of research in information extraction focuses on processing raw text to fill in the fields of manually-defined templates, thus populating databases of events or relations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are f</context>
</contexts>
<marker>Liang, Jordan, Klein, 2010</marker>
<rawString>\x0cPercy Liang, Michael I. Jordan, and Dan Klein. 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Type-Based MCMC</author>
</authors>
<title>In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>573581</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<marker>MCMC, 2010</marker>
<rawString>Type-Based MCMC. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 573581, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Schutze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<volume>1</volume>
<pages>edition,</pages>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="26206" citStr="Manning et al., 2008" startWordPosition="4561" endWordPosition="4565">ch point indicates a unique sampling run. Bill Clinton Benazir Bhutto Nancy Pelosi Speaker John Kerry Sen. Roberts Martin King Dr. Jr. Luther Bill Nelson Table 2: A subset of the entity database discovered by our model, hand selected to show highlight interesting success and failure cases. between each pair of clusters. This yields a series of outputs that move along the precision-recall curve, with precision increasing as the clusters encompass more strings. There is prior work on heuristics for selecting a stopping point, but we compare our results against the entire precision-recall curve (Manning et al., 2008). 7 Results The results of our evaluation are shown in Figure 2. All sampling runs from our models lie well beyond the precision-recall curve defined by the baseline system, demonstrating the ability to achieve reasonable recall with a far more compact database. The baseline system can achieve nearly perfect recall by creating one entity per unique string, but as it merges strings to improve precision, its recall suffers significantly. As noted above, perfect precision is not possible on this task, because the reference set covers only a subset of the entities that appear in the data. However,</context>
</contexts>
<marker>Manning, Raghavan, Schutze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schutze. 2008. Introduction to Information Retrieval. Cambridge University Press, 1 edition, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Hoa Trang Dang</author>
</authors>
<title>knowledge base population track.</title>
<date>2009</date>
<journal>Overview of the TAC</journal>
<booktitle>In Proceedings of the Text Analysis Conference (TAC).</booktitle>
<contexts>
<context position="29224" citStr="McNamee and Dang, 2009" startWordPosition="5059" endWordPosition="5062">e them. Given that several other mentions of Bhutto are already in the row occupied by Clinton, the overall likelihood would benefit little from creating a new row for a single mention, though moving all such mentions simultaneously would result in an improvement. Larger scale MetropolisHastings moves, such as split-merge or type-based sampling (Liang et al., 2010) may help. 8 Related Work Information Extraction A tradition of research in information extraction focuses on processing raw text to fill in the fields of manually-defined templates, thus populating databases of events or relations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then</context>
</contexts>
<marker>McNamee, Dang, 2009</marker>
<rawString>Paul McNamee and Hoa Trang Dang. 2009. Overview of the TAC 2009 knowledge base population track. In Proceedings of the Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Pasula</author>
<author>Bhaskara Marthi</author>
<author>Brian Milch</author>
<author>Stuart Russell</author>
<author>Ilya Shpitser</author>
</authors>
<title>Identity uncertainty and citation matching.</title>
<date>2002</date>
<booktitle>In Advances in Neural Processing Systems 15,</booktitle>
<publisher>MIT Press.</publisher>
<location>Vancouver, British Columbia.</location>
<contexts>
<context position="32202" citStr="Pasula et al., 2002" startWordPosition="5541" endWordPosition="5544">rent perspectives. There is an extensive literature on probabilistic record linkage, in which database records are compared to determine if they are likely to have the same real-world referents (e.g., Felligi and Sunter, 1969; Bilenko et al., 2003). Most approaches focus on pairwise assessments of whether two records are the same, whereas our method attempts to infer a single coherent model of the underlying relational data. Some more recent work in record linkage has explicitly formulated the task of inferring a latent relational model of a set of observed datasets (e.g., Cohen et al., 2000; Pasula et al., 2002; Bhattacharya and Getoor, 2007); however, to our knowledge, these prior models have all exploited some predefined database schema (i.e., set of columns), which our model does not require. Many of these prior models have been applied to bibliographic data, where different conventions and abbreviations lead to imperfect matches in different references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreference resolution is the </context>
</contexts>
<marker>Pasula, Marthi, Milch, Russell, Shpitser, 2002</marker>
<rawString>Hanna Pasula, Bhaskara Marthi, Brian Milch, Stuart Russell, and Ilya Shpitser. 2002. Identity uncertainty and citation matching. In Advances in Neural Processing Systems 15, Vancouver, British Columbia. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with Markov Logic.</title>
<date>2008</date>
<contexts>
<context position="33133" citStr="Poon and Domingos, 2008" startWordPosition="5688" endWordPosition="5691">to imperfect matches in different references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreference resolution is the task of grouping entity mentions (strings), in one or more documents, based on their common referents in the world. Although much of coreference resolution has on the single document setting, there has been some recent work on crossdocument coreference resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). Future </context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008. Joint unsupervised coreference resolution with Markov Logic.</rawString>
</citation>
<citation valid="true">
<title>Association for Computational Linguistics.</title>
<date></date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>650--659</pages>
<location>Honolulu, Hawaii,</location>
<marker></marker>
<rawString>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 650 659, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl E Rasmussen</author>
</authors>
<title>The Infinite Gaussian Mixture Model. In</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems 12,</booktitle>
<volume>12</volume>
<pages>554560</pages>
<contexts>
<context position="17913" citStr="Rasmussen (2000)" startWordPosition="3138" endWordPosition="3139">dle these priors appropriately. We place a log-normal hyperprior on the column concentration parameters, log j N(, 2). The parameters of the log-normal are shared across columns, which provides additional information to constrain the concentration parameters of newlycreated columns. We then use Metropolis-Hastings to sample the values of each j, using the joint likelihood, P(j, x(j) |, 2 ) exp((log j )2 ) kj j (j) 22(nj + j) , where x(j) is column j of the inferred table, nj is the number of specified entries in column j of the table x and kj is the number of unique entries in the column; see Rasmussen (2000) for a derivation. After repeatedly sampling several values of j for each column in the table, we update and 2 to their maximum-likelihood estimates. 5 Temporal Prominence Andy Warhol predicted, in the future, everyone will be world-famous for fifteen minutes. A model of temporal dynamics that accounts for the fleeting and fickle nature of fame might yield better performance for transient entities, like Joe the Plumber. Among several alternatives for modeling temporal dynamics in latent variable models, we choose a simple nonparametric approach: the recurrent Chinese restaurant process (RCRP; </context>
</contexts>
<marker>Rasmussen, 2000</marker>
<rawString>Carl E. Rasmussen. 2000. The Infinite Gaussian Mixture Model. In In Advances in Neural Information Processing Systems 12, volume 12, pages 554560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>SemiMarkov conditional random fields for information extraction. In</title>
<date>2005</date>
<booktitle>Advances in Neural Information Processing Systems 17,</booktitle>
<pages>11851192</pages>
<editor>Lawrence K. Saul, Yair Weiss, and Leon Bottou, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="30751" citStr="Sarawagi and Cohen, 2005" startWordPosition="5303" endWordPosition="5306">and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated gazetteers of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted transformations of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches bears on one or more facets of </context>
</contexts>
<marker>Sarawagi, Cohen, 2005</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2005. SemiMarkov conditional random fields for information extraction. In Lawrence K. Saul, Yair Weiss, and Leon Bottou, editors, Advances in Neural Information Processing Systems 17, pages 11851192. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Amarnag Subramanya</author>
<author>Fernando Pereira</author>
<author>Andrew McCallum</author>
</authors>
<title>Large-scale crossdocument coreference using distributed inference and hierarchical models.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics: Human Language Technologies (ACL HLT).</booktitle>
<contexts>
<context position="22901" citStr="Singh et al., 2011" startWordPosition="4019" endWordPosition="4022">s for the 100 entities. Most entities only include first and last names, though the most frequent entities have many more: for example, the entity Barack Obama has known names: {Barack, Obama, Sen., Mr.}. Metrics We evaluate the recall and precision of a systems response set by matching against the reference set. The first step is to create a bipartite matching between response and reference entities.3 Using a cost function that quantifies the sim2 Recent work exploiting Wikipedia disambiguation pages for evaluating cross-document coreference suggests an appealing alternative for future work (Singh et al., 2011). 3 Bipartite matchings are typical in information extraction evaluation metrics (e.g., Doddington et al., 2004). 7 \x0cilarity of response and reference entities, we optimize the matching using the Kuhn-Munkres algorithm (Kuhn, 1955). For recall, the cost function counts the number of shared word tokens, divided by the number of word tokens in the reference entities; the recall is one minus the average cost of the best matching (with a cost of one for reference entities that are not matched, and no cost for unmatched response entities). Precision is computed identically, but we normalize by t</context>
<context position="33154" citStr="Singh et al., 2011" startWordPosition="5692" endWordPosition="5695">ifferent references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreference resolution is the task of grouping entity mentions (strings), in one or more documents, based on their common referents in the world. Although much of coreference resolution has on the single document setting, there has been some recent work on crossdocument coreference resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). Future work might consider h</context>
</contexts>
<marker>Singh, Subramanya, Pereira, McCallum, 2011</marker>
<rawString>Sameer Singh, Amarnag Subramanya, Fernando Pereira, and Andrew McCallum. 2011. Large-scale crossdocument coreference using distributed inference and hierarchical models. In Association for Computational Linguistics: Human Language Technologies (ACL HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee W Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>101--15661581</pages>
<contexts>
<context position="34801" citStr="Teh et al 2006" startWordPosition="5945" endWordPosition="5948"> is that it facilitates incorporation of additional information sources when available. In this paper, we have considered one such additional source, incorporating temporal context using the recurrent Chinese restaurant process. We envision enhancing the model in several other respects. One promising direction is the incorporation of name structure, which could be captured using a first-order Markov model of the transitions between name parts. In the nonparametric setting, a transition matrix is unbounded along both dimensions, and this can be handled by a hierarchical Dirichlet process (HDP; Teh et al 2006).4 We envision other potential applications of the HDP: for example, learning topics of entities which tend to appear together (i.e., given a mention of Mahmoud Abbas in the American press, a mention of Benjamin Netanyahu is likely), and handling documentspecific burstiness (i.e., given that an entity is mentioned once in a document, it is much more likely to be mentioned again). Finally, we would like to incorporate lexical context from the sentences in which each entity is mentioned, which might help to distinguish, say, computer science researchers who share names with former defense secret</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee W. Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical dirichlet processes. Journal of the American Statistical Association, 101:15661581, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="30391" citStr="Sang, 2002" startWordPosition="5247" endWordPosition="5248">ng over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approaches is an important step for future work. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated gazetteers of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric </context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition. In Proceedings of the Sixth Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg C G Wei</author>
<author>Martin A Tanner</author>
</authors>
<title>A Monte Carlo Implementation of the EM Algorithm and the Poor Mans Data Augmentation Algorithms.</title>
<date>1990</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>85</volume>
<issue>411</issue>
<pages>704</pages>
<contexts>
<context position="16870" citStr="Wei and Tanner, 1990" startWordPosition="2948" endWordPosition="2951">= j |ct, c) P(c = i |ct, c) \x13N(r=tc=i)N(r=tc=j) (7) where the counts N() are from the state of the sampler before executing the proposed move. The probability P(c = i |ct, c) is defined in Equation 3, and the overall acceptance ratio for column swaps is the product of (6) and (7). 4.3 Hyperparameters The concentration parameters r and c help to control the number of rows and columns in the table, respectively. These parameters are updated to their maximum likelihood values using gradientbased optimization, so our overall inference procedure is a form of Monte Carlo ExpectationMaximization (Wei and Tanner, 1990). The concentration parameters j control the diversity of each column in the table: if j is low then we expect a high degree of repetition, as with titles; if j is high then we expect a high degree of diversity. When the sampling procedure adds a new column, there is very little information for how to set its concentration parameter, as the conditional likelihood will be flat. Consequently, greater care must be taken to handle these priors appropriately. We place a log-normal hyperprior on the column concentration parameters, log j N(, 2). The parameters of the log-normal are shared across col</context>
</contexts>
<marker>Wei, Tanner, 1990</marker>
<rawString>Greg C. G. Wei and Martin A. Tanner. 1990. A Monte Carlo Implementation of the EM Algorithm and the Poor Mans Data Augmentation Algorithms. Journal of the American Statistical Association, 85(411):699 704.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>