 While it was initially believed that lexicalization of PCFG parsers (CITATION; CITATION) is crucial for obtaining good parsing results, CITATION demonstrated that the lexicalized Model-1 parser of Collins (1997) does not benefit from bilexical information when tested on a new text domain, and only marginally benefits from such information when tested on the same text domain as the training corpora,,
 This was followed by CITATION who showed that bilexical-information is used in only 1,,
 CITATION bridged the gap between lexicalized and unlexicalized parsing performance, providing a competitive unlexicalized parsing model, relying on lexical information for only a few closed-class lexical items,,
 This was recently followed by (CITATION; CITATION) who introduce state-of-the-art nearly unl,,
 For these results, the C parameter was tuned on a development set using Brents 1-dimension minimization method CITATION,,
 While early approaches to the NP-chunking task CITATION relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks,,
 Indeed, all the better-performing systems in the CoNLL shared tasks competitions for Chunking CITATION and Named Entity Recognition (Tjong Kim CITATION; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information,,
 The feature representation of CITATION could be a step in that direction,,
 While it was initially believed that lexicalization of PCFG parsers (CITATION; CITATION) is crucial for obtaining good parsing results, CITATION demonstrated that the lexicalized Model-1 parser of Collins (1997) does not benefit from bilexical information when tested on a new text domain, and only marginally benefits from such information when tested on the same text domain as the training corpora,,
 This was followed by CITATION who showed that bilexical-information is used in only 1,,
 The feature representation of CITATION could be a step in that direction,,
 While it was initially believed that lexicalization of PCFG parsers (CITATION; CITATION) is crucial for obtaining good parsing results, CITATION demonstrated that the lexicalized Model-1 parser of Collins (1997) does not benefit from bilexical information when tested on a new text domain, and only marginally benefits from such information when tested on the same text domain as the training corpora,,
 This was followed by CITATION who showed that bilexical-information is used in only 1,,
 The feature representation of CITATION could be a step in that direction,,
 While it was initially believed that lexicalization of PCFG parsers (CITATION; CITATION) is crucial for obtaining good parsing results, CITATION demonstrated that the lexicalized Model-1 parser of Collins (1997) does not benefit from bilexical information when tested on a new text do,,
 bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while CITATION demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy,,
 As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (CITATION; CITATION; CITATION; CITATION),,
 The feature representation of CITATION could be a step in that direction,,
 While it was initially believed that lexicalization of PCFG parsers (CITATION; CITATION) is crucial for obtaining good parsing results, CITATION demonstrated that the lexicalized Model-1 parser of Collins (1997) does not benefit from bilexical information when tested on a new text domain, and only marginally benefits from such information when tested on the same text domain as the training corpora,,
 This was followed by CITATION who showed that bilexical-information is used in only 1,,
 CITATION bridged the gap between lex,,
 In this work, we focus on learning with Support Vector Machines (SVMs) CITATION,,
 (CITATION; CITATION)),,
 However, in CITATION, we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations,,
 1142 \x0cWe show that by using a variant of SVM Anchored SVM Learning CITATION with a polynomial kernel, one can learn accurate models for English NP-chunking CITATION, base-phrase chunking (CoNLL 2000), and Dutch Named Entity Recognition (CoNLL 2002), on a heavily pruned feature space,,
ith a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (CITATION; CITATION; CITATION; CITATION),,
 We use instead the Anchored Learning heuristic, introduced in CITATION,,
 3 Learning Method SVM are discriminative, max-margin, linear classifiers CITATION, which can be kernelized,,
 For the formulation of SVMs in the context of NLP applications, see CITATION,,
 SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (CITATION; CITATION; CITATION; CITATION),,
 We use instead the Anchored Learning heuristic, introduced in CITATION,,
 3 Learning Method SVM are discriminative, max-margin, linear classifiers CITATION, which can be kernelized,,
 For the formulation of SVMs in the context of NLP applications, see CITATION,,
 SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (CITATION; CITATION; CITATION; CITATION),,
 We use instead the Anchored Learning heuristic, introduced in CITATION,,
 This was recently followed by (CITATION; CITATION) who introduce state-of-the-art nearly unlexicalized PCFG parsers,,
 Similarly for discriminative dependency parsing, state-of-the-art parsers (CITATION; CITATION) are highly lexicalized,,
 However, the model analysis in CITATION reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while CITATION demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy,,
 As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (CITATION; CITATION; CITATION; CITATION),,
al for obtaining good parsing results, CITATION demonstrated that the lexicalized Model-1 parser of Collins (1997) does not benefit from bilexical information when tested on a new text domain, and only marginally benefits from such information when tested on the same text domain as the training corpora,,
 This was followed by CITATION who showed that bilexical-information is used in only 1,,
 CITATION bridged the gap between lexicalized and unlexicalized parsing performance, providing a competitive unlexicalized parsing model, relying on lexical information for only a few closed-class lexical items,,
 This was recently followed by (CITATION; CITATION) who introduce state-of-the-art nearly unlexicalized PCFG parsers,,
 Similarly for discriminative dependency parsing, state-of-the-art parsers (CITATION; CITATION) are highly lexicalized,,
 However, the model analysis in CITATION reveals that bilexical features hardly contribute to the performance of ,,
 This is equivalent to maximizing the dual problem: PM i=1 i 1 2 P i,j ijyiyjK(xi, xj) Another variant is L2-penalty SVM CITATION, in which there is a quadratic penalty for misclassified instances,,
 In this work, we focus on learning with Support Vector Machines (SVMs) CITATION,,
 (CITATION; CITATION)),,
 However, in CITATION, we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations,,
 3 Learning Method SVM are discriminative, max-margin, linear classifiers CITATION, which can be kernelized,,
 For the formulation of SVMs in the context of NLP applications, see CITATION,,
 SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (CITATION; CITATION; CITATION; CITATION),,
 We use instead the Anchored Learning heuristic, introduced in CITATION,,
such syntactic tasks as Chunking and NER, as demonstrated by the many systems described in (CITATION; Tjong Kim CITATION),,
 This setting is shown to produce good results for sequence labeling tasks in previous work CITATION, and is what most end-users of SVM classifiers are likely to use,,
 3 Learning Method SVM are discriminative, max-margin, linear classifiers CITATION, which can be kernelized,,
 For the formulation of SVMs in the context of NLP applications, see CITATION,,
 SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (CITATION; CITATION; CITATION; CITATION),,
 However, in CITATION, we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations,,
 1142 \x0cWe show that by using a variant of SVM Anchored SVM Learning CITATION with a polynomial kernel, one can learn accurate models for English NP-chunking CITATION, base-phrase chunking (CoNLL 2000), and Dutch Named Entity Recognition (CoNLL 2002), on a heavily pruned feature space,,
2 NP Chunking The goal of this task CITATION is the identification of non-recursive NPs,,
 This was followed by CITATION who showed that bilexical-information is used in only 1,,
 CITATION bridged the gap between lexicalized and unlexicalized parsing performance, providing a competitive unlexicalized parsing model, relying on lexical information for only a few closed-class lexical items,,
 This was recently followed by (CITATION; CITATION) who introduce state-of-the-art nearly unlexicalized PCFG parsers,,
 Similarly for discriminative dependency parsing, state-of-the-art parsers (CITATION; CITATION) are highly lexicalized,,
 However, the model analysis in CITATION reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while CITATION demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy,,
 CITATION bridged the gap between lexicalized and unlexicalized parsing performance, providing a competitive unlexicalized parsing model, relying on lexical information for only a few closed-class lexical items,,
 This was recently followed by (CITATION; CITATION) who introduce state-of-the-art nearly unlexicalized PCFG parsers,,
 Similarly for discriminative dependency parsing, state-of-the-art parsers (CITATION; CITATION) are highly lexicalized,,
 However, the model analysis in CITATION reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while CITATION demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy,,
 In this work, we focus on learning with Support Vector Machines (SVMs) CITATION,,
 (CITATION; CITATION)),,
 However, in CITATION, we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations,,
 1142 \x0cWe show that by using a variant of SVM Anchored SVM Learning CITATION with a polyn,,
 CITATION bridged the gap between lexicalized and unlexicalized parsing performance, providing a competitive unlexicalized parsing model, relying on lexical information for only a few closed-class lexical items,,
 This was recently followed by (CITATION; CITATION) who introduce state-of-the-art nearly unlexicalized PCFG parsers,,
 Similarly for discriminative dependency parsing, state-of-the-art parsers (CITATION; CITATION) are highly lexicalized,,
 However, the model analysis in CITATION reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while CITATION demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy,,
 3 Learning Method SVM are discriminative, max-margin, linear classifiers CITATION, which can be kernelized,,
 For the formulation of SVMs in the context of NLP applications, see CITATION,,
 SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (CITATION; CITATION; CITATION; CITATION),,
 We use instead the Anchored Learning heuristic, introduced in CITATION,,
 This was followed by CITATION who showed that bilexical-information is used in only 1,,
 CITATION bridged the gap between lexicalized and unlexicalized parsing performance, providing a competitive unlexicalized parsing model, relying on lexical information for only a few closed-class lexical items,,
 This was recently followed by (CITATION; CITATION) who introduce state-of-the-art nearly unlexicalized PCFG parsers,,
 Similarly for discriminative dependency parsing, state-of-the-art parsers (CITATION; CITATION) are highly lexicalized,,
 However, the model analysis in CITATION reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while CITATION demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy,,
ardly contribute to the performance of a discriminative MSTbased dependency parser, while CITATION demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy,,
 As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (CITATION; CITATION; CITATION; CITATION),,
 While early approaches to the NP-chunking task CITATION relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks,,
 Indeed, all the better-performing systems in the CoNLL shared tasks competitions for Chunking CITATION and Named Entity Recognition (Tjong Kim CITATION; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information,,
 Syntactic structure information as captured by pairs of POS-tags and Word-POS pairs is certainly important for such syntactic tasks as Chunking and NER, as demonstrated by the many systems described in (CITATION; Tjong Kim CITATION),,
 This setting is shown to produce good results for sequence labeling tasks in previous work CITATION, and is what most end-users of SVM classifiers are likely to use,,
3 Chunking The goal of the Chunking task CITATION is the identification of an assortment of linguistic base-phrases,,
ald, 2006) reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while CITATION demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy,,
 As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (CITATION; CITATION; CITATION; CITATION),,
 While early approaches to the NP-chunking task CITATION relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks,,
 Indeed, all the better-performing systems in the CoNLL shared tasks competitions for Chunking CITATION and Named Entity Recognition (Tjong Kim CITATION; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information,,
 In this work, we focus on learning with Support Vector Machines (SVMs) CITATION,,
 Syntactic structure information as captured by pairs of POS-tags and Word-POS pairs is certainly important for such syntactic tasks as Chunking and NER, as demonstrated by the many systems described in (CITATION; Tjong Kim CITATION),,
 This setting is shown to produce good results for sequence labeling tasks in previous work CITATION, and is what most end-users of SVM classifiers are likely to use,,
1 Named Entity Recognition (NER) We use the Dutch data set from the CoNLL 2002 shared task (Tjong Kim CITATION),,
d Named Entity Recognition (Tjong Kim CITATION; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information,,
 In this work, we focus on learning with Support Vector Machines (SVMs) CITATION,,
 (CITATION; CITATION)),,
 However, in CITATION, we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations,,
 3 Learning Method SVM are discriminative, max-margin, linear classifiers CITATION, which can be kernelized,,
 For the formulation of SVMs in the context of NLP applications, see CITATION,,
 SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (CITATION; CITATION; CITATION; CITATION),,
l analysis in CITATION reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while CITATION demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy,,
 As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (CITATION; CITATION; CITATION; CITATION),,
