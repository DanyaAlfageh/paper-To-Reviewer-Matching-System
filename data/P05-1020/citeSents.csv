159 \x0cDecision tree learners CITATION, CITATION, CITATION, Learning (C4.5/C5/CART) CITATION, CITATION, CITATION algorithm RIPPER CITATIONb) Maximum entropy CITATION, CITATION, CITATION Instance McCarthy and Lehnerts CITATION, CITATION creation Soon et al.s CITATION, CITATION, CITATION method Ng and Cardies CITATIONb) Feature Soon et al.s CITATION set Ng and Cardies CITATIONb) Clustering Closest-first CITATION, Strube et al,,
In our experiments, we use the training texts to acquire coreference classifiers and evaluate the resulting systems on the test texts with respect to two commonly-used coreference scoring programs: the MUC scorer CITATION and the B-CUBED scorer CITATION,,
We employ as our baseline systems two existing coreference resolvers: our duplication of the CITATION system and the CITATIONb) system,,
approaches (e.g., CITATION, CITATION),,
For instance, CITATION and CITATION have explored the use of SVM, voted perceptron, and logistic regression for training coreference classifiers,,
CITATION and CITATION have employed graph-based partitioning algorithms such as correlation clustering CITATION,,
Finally, CITATION and CITATION have proposed new edit-distance-based string-matching features and centering-based features, respectively,,
3.1 Selecting Coreference Systems A learning-based coreference system can be defined by four elements: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set 2 Examples of such scoring functions include the DempsterShafer rule (see CITATION and CITATION) and its variants (see CITATION and CITATION),,
Previous attempts on bootstrapping coreference classifiers have only been mildly successful (e.g., CITATION), and this is also an area that deserves further research,,
To expand our set of candidate partitions, we can potentially incorporate more high-performing coreference systems into our framework, which is flexible enough to accommodate even those that adopt knowledge-based (e.g., CITATION) and unsupervised approaches (e.g., CITATION, CITATION),,
For instance, CITATION and CITATION have explored the use of SVM, voted perceptron, and logistic regression for training coreference classifiers,,
CITATION and CITATION have employed graph-based partitioning algorithms such as correlation clustering CITATION,,
Finally, CITATION and Iida et al,,
Details can be found in CITATION,,
See CITATIONb) for details,,
We consider three learning algorithms, namely, the C4.5 decision tree induction system CITATION, the RIPPER rule learning algorithm CITATION, and maximum entropy classification CITATION,,
Previous attempts on bootstrapping coreference classifiers have only been mildly successful (e.g., CITATION), and this is also an area that deserves further research,,
To expand our set of candidate partitions, we can potentially incorporate more high-performing coreference systems into our framework, which is flexible enough to accommodate even those that adopt knowledge-based (e.g., CITATION) and unsupervised approaches (e.g., CITATION, CITATION),,
For instance, CITATION and CITATION have explored the use of SVM, voted perceptron, and logistic regression for training coreference classifiers,,
CITATION and CITATION have employed graph-based partitioning algorithms such as correlation clustering CITATION,,
Details can be found in CITATION,,
See CITATIONb) for details,,
We consider three learning algorithms, namely, the C4.5 decision tree induction system CITATION, the RIPPER rule learning algorithm CITATION, and maximum entropy classification CITATION,,
by means of CITATION SVM \x02\x01\x04\x03\x06\x05 package, with all the parameters set to their default values,,
Following previous work on using global features of candidate structures to learn a ranking model CITATION, the global (i.e., partition-based) features we consider here are simple functions of the local features that capture the relationship between NP pairs,,
ranking model using global features of an NP partition, there is some related work on partition ranking where the score of a partition is computed via a heuristic function of the probabilities of its NP pairs being coreferent.2 For instance, CITATION introduce a greedy algorithm for finding the highest-scored partition by performing a beam search in the space of possible partitions,,
CITATIONa) attempt to optimize their rulebased coreference classifier for clustering-level accuracy, essentially by finding a subset of the learned rules that performs the best on held-out data with respect to the target coreference scoring program,,
CITATION propose a similar ide,,
3.1 Selecting Coreference Systems A learning-based coreference system can be defined by four elements: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set 2 Examples of such scoring functions include the DempsterShafer rule (see CITATION and CITATION) and its variants (see CITATION and CITATION),,
Previous attempts on bootstrapping coreference classifiers have only been mildly successful (e.g., CITATION), and this is also an area that deserves further research,,
To expand our set of candidate partitions, we can potentially incorporate more high-performing coreference systems into our framework, which is flexible enough to accommodate even those that adopt knowledge-based (e.g., CITATION) and unsupervised approaches (e.g., CITATION, CITATION),,
For instance, CITATION and CITATION have explored the use of SVM, voted perceptron, and logistic regression for training coreference classifiers,,
CITATION and CITATION have employed graph-based partitioning algorithms such as corre,,
159 \x0cDecision tree learners CITATION, CITATION, CITATION, Learning (C4.5/C5/CART) CITATION, CITATION, CITATION algorithm RIPPER CITATIONb) Maximum entropy CITATION, CITATION, CITATION Instance McCarthy and Lehnerts CITATION, CITATION creation Soon et al.s CITATION, CITATION, CITATION method Ng and Cardies CITATIONb) Feature Soon et al.s CITATION set Ng and Cardies CITATIONb) Clustering Closest-first CITATION, CITATION algorithm Best-first CITATION, CITATIONb), CITATION Aggressive-merge CITATION Table 1: Summary of the previous work on coreference resolution that employs the learning algorithms, the clustering algorithms, the feature sets, and the training instance creation methods discussed in Section 3.1,,
To expand our set of candidate partitions, we can potentially incorporate more high-performing coreference systems into our framework, which is flexible enough to accommodate even those that adopt knowledge-based (e.g., CITATION) and unsupervised approaches (e.g., CITATION, CITATION),,
For instance, CITATION and CITATION have explored the use of SVM, voted perceptron, and logistic regression for training coreference classifiers,,
CITATION and CITATION have employed graph-based partitioning algorithms such as correlation clustering CITATION,,
Finally, CITATION and CITATION have proposed new edit-distance-based string-matching features and centering-based features, respectively,,
 et al.s CITATION set Ng and Cardies CITATIONb) Clustering Closest-first CITATION, CITATION algorithm Best-first CITATION, CITATIONb), CITATION Aggressive-merge CITATION Table 1: Summary of the previous work on coreference resolution that employs the learning algorithms, the clustering algorithms, the feature sets, and the training instance creation methods discussed in Section 3.1,,
3.2 Learning to Rank Candidate Partitions We train an SVM-based ranker for ranking candidate partitions by means of CITATION SVM \x02\x01\x04\x03\x06\x05 package, with all the parameters set to their default values,,
Following previous work on using global features of candidate structures to learn a ranking model CITATION, the global ,,
3.1 Selecting Coreference Systems A learning-based coreference system can be defined by four elements: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set 2 Examples of such scoring functions include the DempsterShafer rule (see CITATION and CITATION) and its variants (see CITATION and CITATION),,
159 \x0cDecision tree learners CITATION, CITATION, CITATION, Learning (C4.5/C5/CART) CITATION, CITATION, CITATION algorithm RIPPER CITATIONb) Maximum entropy CITATION, CITATION, CITATION Instance McCarthy and Lehnerts CITATION, CITATION creation Soon et al.s CITATION, CITATION, CITATION method Ng and Cardies CITATIONb) Feature Soon et al.s CITATION set Ng and Cardies CITATIONb) Clustering Closest-first CITATION, CITATION algorithm Best-first CITATION, CITATIONb), CITATION Aggressive-merge CITATION Table 1: Summary of the previous work on coreference resolution that emplo,,
1 Introduction Recent research in coreference resolution the problem of determining which noun phrases (NPs) in a text or dialogue refer to which real-world entity has exhibited a shift from knowledgebased approaches to data-driven approaches, yielding learning-based coreference systems that rival their hand-crafted counterparts in performance (e.g., CITATION, CITATIONb), CITATION, CITATION, CITATION),,
3.1 Selecting Coreference Systems A learning-based coreference system can be defined by four elements: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set 2 Examples of such scoring functions include the DempsterShafer rule (see CITATION and CITATION) and its variants (see CITATION and CITATION),,
159 \x0cDecision tree learners CITATION, CITATION, CITATION, Learning (C4.5/C5/CART) CITATION, CITATION, CITATION algorithm RIPPER CITATIONb) Maximum entropy CITATION, CITATION, CITATION Instance McCarthy and Lehnerts CITATION, CITATION creation Soon et al.s CITATION, CITATION, CITATION method Ng and Cardies CITATIONb) Feature Soon et al.s CITATION set Ng and Cardies CITATIONb) Clustering Closest-first CITATION, CITATION algorithm Best-first CITATION, CITATIONb), CITATION Aggressive-merge CITATION Table 1: Summary of the previous work on coreference resolution that employs the learning algorithms, the cl,,
amework, which is flexible enough to accommodate even those that adopt knowledge-based (e.g., CITATION) and unsupervised approaches (e.g., CITATION, CITATION),,
For instance, CITATION and CITATION have explored the use of SVM, voted perceptron, and logistic regression for training coreference classifiers,,
CITATION and CITATION have employed graph-based partitioning algorithms such as correlation clustering CITATION,,
Finally, CITATION and CITATION have proposed new edit-distance-based string-matching features and centering-based features, respectively,,
159 \x0cDecision tree learners CITATION, CITATION, CITATION, Learning (C4.5/C5/CART) CITATION, CITATION, CITATION algorithm RIPPER CITATIONb) Maximum entropy CITATION, CITATION, CITATION Instance McCarthy and Lehnerts CITATION, CITATION creation Soon et al.s CITATION, CITATION, CITATION method Ng and Cardies CITATIONb) Feature Soon et al.s CITATION set Ng and Cardies CITATIONb) Clustering Closest-first CITATION, CITATION algorithm Best-first Aone ,,
159 \x0cDecision tree learners CITATION, CITATION, CITATION, Learning (C4.5/C5/CART) CITATION, CITATION, CITATION algorithm RIPPER CITATIONb) Maximum entropy CITATION, CITATION, CITATION Instance McCarthy and Lehnerts CITATION, CITATION creation Soon et al.s CITATION, CITATION, CITATION method Ng and Cardies CITATIONb) Feature Soon et al.s CITATION set Ng and Cardies CITATIONb) Clustering Closest-first CITATION, CITATION algorithm Best-first CITATION, CITATIONb), CITATION Aggressive-merge CITATION Table 1: Summary of the previous work on coreference resolution that employs the learning,,
Previous attempts on bootstrapping coreference classifiers have only been mildly successful (e.g., CITATION), and this is also an area that deserves further research,,
To expand our set of candidate partitions, we can potentially incorporate more high-performing coreference systems into our framework, which is flexible enough to accommodate even those that adopt knowledge-based (e.g., CITATION) and unsupervised approaches (e.g., CITATION, CITATION),,
1 Introduction Recent research in coreference resolution the problem of determining which noun phrases (NPs) in a text or dialogue refer to which real-world entity has exhibited a shift from knowledgebased approaches to data-driven approaches, yielding learning-based coreference systems that rival their hand-crafted counterparts in performance (e.g., CITATION, CITATIONb), CITATION, CITATION, CITATION),,
ranking model using global features of an NP partition, there is some related work on partition ranking where the score of a partition is computed via a heuristic function of the probabilities of its NP pairs being coreferent.2 For instance, CITATION introduce a greedy algorithm for finding the highest-scored partition by performing a beam search in the space of possible partitions,,
CITATIONa) attempt to optimize their rulebased coreference classifier for clustering-level accuracy, essentially by finding a subset of the learned rules that performs the best on held-out data with respect to the target coreference scoring program,,
CITATION propose a similar idea, but aim instead at finding a subset of the available features with which the resulting coreference classifier yields the best clustering-level accuracy on held-out data,,
Details can be found in CITATION,,
See CITATIONb) for details,,
We consider three learning algorithms, namely, the C4.5 decision tree induction system CITATION, the RIPPER rule learning algorithm CITATION, and maximum entropy classification CITATION,,
159 \x0cDecision tree learners CITATION, CITATION, CITATION, Learning (C4.5/C5/CART) CITATION, CITATION, CITATION algorithm RIPPER CITATIONb) Maximum entropy CITATION, CITATION, CITATION Instance McCarthy and Lehnerts CITATION, CITATION creation Soon et al.s CITATION, CITATION, CITATION method Ng and Cardies CITATIONb) Feature Soon et al.s CITATION set Ng and Cardies CITATIONb) Clustering Closest-first CITATION, CITATION algorithm Best-first CITATION, CITATIONb), CITATION Aggressive-merge CITATION Table 1: Summary of the previous work on c,,
In our experiments, we use the training texts to acquire coreference classifiers and evaluate the resulting systems on the test texts with respect to two commonly-used coreference scoring programs: the MUC scorer CITATION and the B-CUBED scorer CITATION,,
We employ as our baseline systems two existing coreference resolvers: our duplication of the CITATION system and the CITATIONb) system,,
1 Introduction Recent research in coreference resolution the problem of determining which noun phrases (NPs) in a text or dialogue refer to which real-world entity has exhibited a shift from knowledgebased approaches to data-driven approaches, yielding learning-based coreference systems that rival their hand-crafted counterparts in performance (e.g., CITATION, CITATIONb), CITATION, CITATION, CITATION),,
ranking model using global features of an NP partition, there is some related work on partition ranking where the score of a partition is computed via a heuristic function of the probabilities of its NP pairs being coreferent.2 For instance, CITATION introduce a greedy algorithm for finding the highest-scored partition by performing a beam search in the space of possible partitions,,
CITATIONa) attempt to optimize their rulebased coreference classifier for clustering-level accuracy, essentially by finding a subset of the learned rules that performs the best on held-out data with respect to the target coreference scoring program,,
CITATION propose a similar idea, but aim instead at finding a subset of the available features with which the resulting coreference classifier yields the best clustering-level accuracy on held-out data,,
Details can be found in CITATION,,
See CITATIONb) for details,,
We consider three learning algorithms, namely, the C4.5 decision tree induction system CITATION, the RIPPER rule learning algorithm CITATION, and maximum entropy classification CITATION,,
159 \x0cDecision tree learners CITATION, CITATION, CITATION, Learning (C4.5/C5/CART) CITATION, CITATION, CITATION algorithm RIPPER CITATIONb) Maximum entropy CITATION, CITATION, CITATION Instance McCarthy and Lehnerts CITATION, CITATION creation Soon et al.s CITATION, CITATION, CITATION method Ng and Cardies CITATIONb) Feature Soon et al.s CITATION set Ng and Cardies CITATIONb) Clustering Closest-first CITATION, CITATION algorithm Best-first CITATION, CITATIONb), CITATION Aggressive-merge CITATION Table 1: Summary of the previous work on c,,
In our experiments, we use the training texts to acquire coreference classifiers and evaluate the resulting systems on the test texts with respect to two commonly-used coreference scoring programs: the MUC scorer CITATION and the B-CUBED scorer CITATION,,
We employ as our baseline systems two existing coreference resolvers: our duplication of the CITATION system and the CITATIONb) system,,
Details can be found in CITATION,,
See CITATIONb) for details,,
We consider three learning algorithms, namely, the C4.5 decision tree induction system CITATION, the RIPPER rule learning algorithm CITATION, and maximum entropy classification CITATION,,
1 Introduction Recent research in coreference resolution the problem of determining which noun phrases (NPs) in a text or dialogue refer to which real-world entity has exhibited a shift from knowledgebased approaches to data-driven approaches, yielding learning-based coreference systems that rival their hand-crafted counterparts in performance (e.g., CITATION, CITATIONb), CITATION, CITATION, CITATION),,
Details can be found in CITATION,,
See CITATIONb) for details,,
We consider three learning algorithms, namely, the C4.5 decision tree induction system CITATION, the RIPPER rule learning algorithm CITATION, and maximum entropy classification CITATION,,
159 \x0cDecision tree learners CITATION, CITATION, CITATION, Learning (C4.5/C5/CART) CITATION, CITATION, CITATION algorithm RIPPER CITATIONb) Maximum entropy CITATION, CITATION, CITATION Instance McCarthy and Lehnerts CITATION, CITATION creation Soon et al.s CITATION, CITATION, CITATION method Ng and Cardies CITATIONb) Feature Soon et al.s CITATION set Ng and Cardies CITATIONb) Clustering Closest-first CITATION, CITATION algorithm Best-first CITATION, ,,
In our experiments, we use the training texts to acquire coreference classifiers and evaluate the resulting systems on the test texts with respect to two commonly-used coreference scoring programs: the MUC scorer CITATION and the B-CUBED scorer CITATION,,
We employ as our baseline systems two existing coreference resolvers: our duplication of the CITATION system and the CITATIONb) system,,
ferent.2 For instance, CITATION introduce a greedy algorithm for finding the highest-scored partition by performing a beam search in the space of possible partitions,,
CITATIONa) attempt to optimize their rulebased coreference classifier for clustering-level accuracy, essentially by finding a subset of the learned rules that performs the best on held-out data with respect to the target coreference scoring program,,
CITATION propose a similar idea, but aim instead at finding a subset of the available features with which the resulting coreference classifier yields the best clustering-level accuracy on held-out data,,
159 \x0cDecision tree learners CITATION, CITATION, CITATION, Learning (C4.5/C5/CART) CITATION, CITATION, CITATION algorithm RIPPER CITATIONb) Maximum entropy CITATION, CITATION, CITATION Instance McCarthy and Lehnerts CITATION, CITATION creation Soon et al.s CITATION, CITATION, CITATION method Ng and Cardies CITATIONb) Feature Soon et al.s CITATION set Ng and Cardies CITATIONb) Clustering Closest-first CITATION, CITATION algorithm Best-first CITATION, CITATIONb), CITATION Aggressive-merge McCarthy and ,,
1 Introduction Recent research in coreference resolution the problem of determining which noun phrases (NPs) in a text or dialogue refer to which real-world entity has exhibited a shift from knowledgebased approaches to data-driven approaches, yielding learning-based coreference systems that rival their hand-crafted counterparts in performance (e.g., CITATION, CITATIONb), CITATION, CITATION, CITATION),,
159 \x0cDecision tree learners CITATION, CITATION, CITATION, Learning (C4.5/C5/CART) CITATION, CITATION, CITATION algorithm RIPPER CITATIONb) Maximum entropy CITATION, CITATION, CITATION Instance McCarthy and Lehnerts CITATION, CITATION creation Soon et al.s CITATION, CITATION, CITATION method Ng and Cardies CITATIONb) Feature Soon et al.s CITATION set Ng and Cardies CITATIONb) Clustering Closest-first CITATION, CITATION algorithm Best-first CITATION, CITATIONb), CITATION Aggr,,
gstaff (1999), CITATION),,
For instance, CITATION and CITATION have explored the use of SVM, voted perceptron, and logistic regression for training coreference classifiers,,
CITATION and CITATION have employed graph-based partitioning algorithms such as correlation clustering CITATION,,
Finally, CITATION and CITATION have proposed new edit-distance-based string-matching features and centering-based features, respectively,,
In our experiments, we use the training texts to acquire coreference classifiers and evaluate the resulting systems on the test texts with respect to two commonly-used coreference scoring programs: the MUC scorer CITATION and the B-CUBED scorer CITATION,,
We employ as our baseline systems two existing coreference resolvers: our duplication of the CITATION system and the CITATIONb) system,,
1 Introduction Recent research in coreference resolution the problem of determining which noun phrases (NPs) in a text or dialogue refer to which real-world entity has exhibited a shift from knowledgebased approaches to data-driven approaches, yielding learning-based coreference systems that rival their hand-crafted counterparts in performance (e.g., CITATION, CITATIONb), CITATION, CITATION, CITATION),,
159 \x0cDecision tree learners CITATION, CITATION, CITATION, Learning (C4.5/C5/CART) CITATION, CITATION, CITATION algorithm RIPPER CITATIONb) Maximum entropy CITATION, CITATION, CITATION Instance McCarthy and Lehnerts CITATION, CITATION creation Soon et al.s CITATION, CITATION, CITATION method Ng and Cardies CITATIONb) Feature Soon et al.s CITATION set Ng and Cardies CITATIONb) Clustering Closest-first CITATION, CITATION algorithm Best-first CITATION, CITATIONb), CITATION Aggressive-merge CITATION Table,,
To expand our set of candidate partitions, we can potentially incorporate more high-performing coreference systems into our framework, which is flexible enough to accommodate even those that adopt knowledge-based (e.g., CITATION) and unsupervised approaches (e.g., CITATION, CITATION),,
For instance, CITATION and CITATION have explored the use of SVM, voted perceptron, and logistic regression for training coreference classifiers,,
CITATION and CITATION have employed graph-based partitioning algorithms such as correlation clustering CITATION,,
Finally, CITATION and CITATION have proposed new edit-distance-based string-matching features and centering-based features, respectively,,
