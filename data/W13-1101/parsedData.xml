<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.446936">
b&amp;apos;Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 110,
Atlanta, Georgia, June 13 2013. c
</bodyText>
<sectionHeader confidence="0.350624" genericHeader="abstract">
2013 Association for Computational Linguistics
Does Size Matter?
</sectionHeader>
<title confidence="0.750286">
Text and Grammar Revision for Parsing Social Media Data
</title>
<author confidence="0.945751">
Mohammad Khan
</author>
<affiliation confidence="0.998598">
Indiana University
</affiliation>
<address confidence="0.817119">
Bloomington, IN USA
</address>
<email confidence="0.995392">
khanms@indiana.edu
</email>
<author confidence="0.974241">
Markus Dickinson
</author>
<affiliation confidence="0.999239">
Indiana University
</affiliation>
<address confidence="0.816015">
Bloomington, IN USA
</address>
<email confidence="0.987729">
md7@indiana.edu
</email>
<author confidence="0.983604">
Sandra Kubler
</author>
<affiliation confidence="0.999581">
Indiana University
</affiliation>
<address confidence="0.816896">
Bloomington, IN USA
</address>
<email confidence="0.995958">
skuebler@indiana.edu
</email>
<sectionHeader confidence="0.990796" genericHeader="introduction">
Abstract
</sectionHeader>
<bodyText confidence="0.999667076923077">
We explore improving parsing social media
and other web data by altering the input data,
namely by normalizing web text, and by revis-
ing output parses. We find that text normal-
ization improves performance, though spell
checking has more of a mixed impact. We also
find that a very simple tree reviser based on
grammar comparisons performs slightly but
significantly better than the baseline and well
outperforms a machine learning model. The
results also demonstrate that, more than the
size of the training data, the goodness of fit
of the data has a great impact on the parser.
</bodyText>
<sectionHeader confidence="0.980348" genericHeader="method">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.997925266666667">
Parsing data from social media data, as well as other
data from the web, is notoriously difficult, as parsers
are generally trained on news data (Petrov and Mc-
Donald, 2012), which is not a good fit for social me-
dia data. The language used in social media does not
follow standard conventions (e.g., containing many
sentence fragments), is largely unedited, and tends
to be on different topics than standard NLP technol-
ogy is trained for. At the same time, there is a clear
need to develop even basic NLP technology for a
variety of types of social media and contexts (e.g.,
Twitter, Facebook, YouTube comments, discussion
forums, blogs, etc.). To perform tasks such as sen-
timent analysis (Nakagawa et al., 2010) or informa-
tion extraction (McClosky et al., 2011), it helps to
perform tagging and parsing, with an eye towards
providing a shallow semantic analysis.
We advance this line of research by investigating
adapting parsing to social media and other web data.
Specifically, we focus on two areas: 1) We compare
the impact of various text normalization techniques
on parsing web data; and 2) we explore parse revi-
sion techniques for dependency parsing web data to
improve the fit of the grammar learned by the parser.
One of the major problems in processing social
media data is the common usage of non-standard
terms (e.g., kawaii, a Japanese-borrowed net term
for cute), ungrammatical and (intentionally) mis-
spelled text (e.g., cuttie), emoticons, and short posts
with little contextual information, as exemplified in
</bodyText>
<page confidence="0.878309">
(1).1
</page>
<listItem confidence="0.691903">
(1) Awww cuttie little kitten, so Kawaii &lt;3
</listItem>
<bodyText confidence="0.9959981875">
To process such data, with its non-standard words,
we first develop techniques for normalizing the text,
so as to be able to accommodate the wide range of
realizations of a given token, e.g., all the different
spellings and intentional misspellings of cute. While
previous research has shown the benefit of text nor-
malization (Foster et al., 2011; Gadde et al., 2011;
Foster, 2010), it has not teased apart which parts
of the normalization are beneficial under which cir-
cumstances.
A second problem with parsing social media data
is the data situation: parsers can be trained on the
standard training set, the Penn Treebank (Marcus
et al., 1993), which has a sufficient size for train-
ing a statistical parser, but has the distinct down-
side of modeling language that is very dissimilar
</bodyText>
<figure confidence="0.304637">
1
Taken from: http://www.youtube.com/watch?
v=eHSpHCprXLA
</figure>
<page confidence="0.555405">
1
</page>
<bodyText confidence="0.983164625">
\x0cfrom the target. Or one can train parsers on the En-
glish Web Treebank (Bies et al., 2012), which cov-
ers web language, including social media data, but
is rather small. Our focus on improving parsing for
such data is on exploring parse revision techniques
for dependency parsers. As far as we know, de-
spite being efficient and trainable on a small amount
of data, parse revision (Henestroza Anguiano and
Candito, 2011; Cetinoglu et al., 2011; Attardi and
DellOrletta, 2009; Attardi and Ciaramita, 2007)
has not been used for web data, or more generally
for adapting a parser to out-of-domain data; an in-
vestigation of its strengths and weaknesses is thus
needed.
We describe the data sets used in our experiments
in section 2 and the process of normalization in sec-
tion 3 before turning to the main task of parsing in
section 4. Within this section, we discuss our main
parser as well as two different parse revision meth-
ods (sections 4.2 and 4.3). In the evaluation in sec-
tion 5, we will find that normalization has a positive
impact, although spell checking has mixed results,
and that a simple tree anomaly detection method
(Dickinson and Smith, 2011) outperforms a machine
learning reviser (Attardi and Ciaramita, 2007), espe-
cially when integrated with confidence scores from
the parser itself. In addition to the machine learner
requiring a weak baseline parser, some of the main
differences include the higher recall of the simple
method at positing revisions and the fact that it de-
tects odd structures, which parser confidence can
then sort out as incorrect or not.
</bodyText>
<sectionHeader confidence="0.997037" genericHeader="method">
2 Data
</sectionHeader>
<bodyText confidence="0.994498909090909">
For our experiments, we use two main resources, the
Wall Street Journal (WSJ) portion of the Penn Tree-
bank (PTB) (Marcus et al., 1993) and the English
Web Treebank (EWT) (Bies et al., 2012). The two
corpora were converted from PTB constituency trees
into dependency trees using the Stanford depen-
dency converter (de Marneffe and Manning, 2008).2
The EWT is comprised of approximately 16,000
sentences from weblogs, newsgroups, emails, re-
views, and question-answers. Instead of examining
each group individually, we chose to treat all web
</bodyText>
<page confidence="0.628429">
2
</page>
<figure confidence="0.95399052">
http://nlp.stanford.edu/software/
stanford-dependencies.shtml
1 &lt;&lt;_ -LRB--LRB-_ 2 punct _ _
2 File _ NN NN _ 0 root _ _
3 : _ : : _ 2 punct _ _
4 220b _ GW GW _ 11 dep _ _
5 -_ GW GW _ 11 dep _ _
6 dg _ GW GW _ 11 dep _ _
7 -_ GW GW _ 11 dep _ _
8 Agreement _ GW GW _ 11 dep _ _
9 for _ GW GW _ 11 dep _ _
10 Recruiting _ GW GW _ 11 dep _ _
11 Services.doc _ NN NN _ 2 dep _ _
12 &gt;&gt;_ -RRB--RRB-_ 2 punct _ _
13 &lt;&lt;_ -LRB--LRB-_ 14 punct _ _
14 File _ NN NN _ 2 dep _ _
15 : _ : : _ 14 punct _ _
16 220a _ GW GW _ 22 dep _ _
17 DG _ GW GW _ 22 dep _ _
18 -_ GW GW _ 22 dep _ _
19 Agreement _ GW GW _ 22 dep _ _
20 for _ GW GW _ 22 dep _ _
21 Contract _ GW GW _ 22 dep _ _
22 Services.DOC _ NN NN _ 14 dep _ _
23 &gt;&gt;_ -RRB--RRB-_ 14 punct _ _
</figure>
<figureCaption confidence="0.999415">
Figure 1: A sentence with GW POS tags.
</figureCaption>
<bodyText confidence="0.977789608695652">
data equally, pulling from each type of data in the
training/testing split.
Additionally, for our experiments, we deleted the
212 sentences from EWT that contain the POS tags
AFX and GW tags. EWT uses the POS tag AFX for
cases where a prefix is written as a separate word
from its root, e.g., semi/AFX automatic/JJ. Such
segmentation and tagging would interfere with our
normalization process. The POS tag GW is used for
other non-standard words, such as document names.
Such sentences are often difficult to analyze and
do not correspond to phenomena found in the PTB
(cf., figure 1).
To create training and test sets, we broke the data
into the following sets:
WSJ training: sections 02-22 (42,009 sen-
tences)
WSJ testing: section 23 (2,416 sentences)
EWT training: 80% of the data, taking the first
four out of every five sentences (13,130 sen-
tences)
EWT testing: 20% of the data, taking every
fifth sentence (3,282 sentences)
</bodyText>
<page confidence="0.934133">
2
</page>
<sectionHeader confidence="0.44278" genericHeader="method">
\x0c3 Text normalization
</sectionHeader>
<bodyText confidence="0.9996310625">
Previous work has shown that accounting for vari-
ability in form (e.g., misspellings) on the web, e.g.,
by mapping each form to a normalized form (Fos-
ter, 2010; Gadde et al., 2011) or by delexicaliz-
ing the parser to reduce the impact of unknown
words (vrelid and Skjrholt, 2012), leads to some
parser or tagger improvement. Foster (2010), for
example, lists adapting the parsers unknown word
model to handle capitalization and misspellings of
function words as a possibility for improvement.
Gadde et al. (2011) find that a model which posits
a corrected sentence and then is POS-taggedtheir
tagging after correction (TAC) modeloutperforms
one which cleans POS tags in a postprocessing step.
We follow this line of inquiry by developing text
normalization techniques prior to parsing.
</bodyText>
<subsectionHeader confidence="0.997834">
3.1 Basic text normalization
</subsectionHeader>
<bodyText confidence="0.99619075">
Machine learning algorithms and parsers are sensi-
tive to the surface form of words, and different forms
of a word can mislead the learner/parser. Our ba-
sic text normalization is centered around the idea
that reducing unnecessary variation will lead to im-
proved parsing performance.
For basic text normalization, we reduce all web
URLs to a single token, i.e., each web URL is re-
placed with a uniform place-holder in the entire
EWT, marking it as a URL. Similarly, all emoticons
are replaced by a single marker indicating an emoti-
con. Repeated use of punctuation, e.g., !!!, is re-
duced to a single punctuation token.
We also have a module to shorten words with con-
secutive sequences of the same character: Any char-
acter that occurs more than twice in sequence will
be shortened to one character, unless they appear in
a dictionary, including the internet and slang dictio-
naries discussed below, in which case they map to
the dictionary form. Thus, the word Awww in ex-
ample (1) is shortened to Aw, and cooool maps to
the dictionary form cool. However, since we use
gold POS tags for our experiments, this module is
not used in the experiments reported here.
</bodyText>
<subsectionHeader confidence="0.999492">
3.2 Spell checking
</subsectionHeader>
<bodyText confidence="0.999303787234043">
Next, we run a spell checker to normalize mis-
spellings, as online data often contains spelling
errors (e.g. cuttie in example (1)). Various sys-
tems for parsing web data (e.g., from the SANCL
shared task) have thus also explored spelling cor-
rection; McClosky et al. (2012), for example, used
1,057 autocorrect rules, thoughsince these did
not make many changesthe system was not ex-
plored after that. Spell checking web data, such as
YouTube comments or blog data, is a challenge be-
cause it contains non-standard orthography, as well
as acronyms and other short-hand forms unknown
to a standard spelling dictionary. Therefore, be-
fore mapping to a corrected spelling, it is vital to
differentiate between a misspelled word and a non-
standard one.
We use Aspell3 as our spell checker to recognize
and correct misspelled words. If asked to correct
non-standard words, the spell checker would choose
the closest standard English word, inappropriate to
the context. For example, Aspell suggests Lil for
lol. Thus, before correcting, we first check whether
a word is an instance of internet speech, i.e., an ab-
breviation or a slang term.
We use a list of more than 3,000 acronyms to
identify acronyms and other abbreviations not used
commonly in formal registers of language. The list
was obtained from NetLingo, restricted to the en-
tries listed as chat acronyms and text message short-
hand.4 To identify slang terminology, we use the
Urban Dictionary5. In a last step, we combine both
lists with the list of words extracted from the WSJ.
If a word is not found in these lists, Aspell is used
to suggest a correct spelling. In order to restrict As-
pell from suggesting spellings that are too different
from the word in question, we use Levenshtein dis-
tance (Levenshtein, 1966) to measure the degree of
similarity between the original form and the sug-
gested spelling; only words with small distances
are accepted as spelling corrections. Since we have
words of varying length, the Levenshtein distance is
normalized by the length of the suggested spelling
(i.e., number of characters). In non-exhaustive tests
on a subset of the test set, we found that a normal-
ized score of 0.301, i.e., a relatively low score ac-
cepting only conservative changes, achieves the best
results when used as a threshold for accepting a sug-
</bodyText>
<page confidence="0.754448">
3
</page>
<figure confidence="0.7605666">
www.aspell.net
4
http://www.netlingo.com/acronyms.php
5
www.urbandictionary.com
</figure>
<page confidence="0.950289">
3
</page>
<bodyText confidence="0.982271428571429">
\x0cgested spelling. The utilization of the threshold re-
stricts Aspell from suggesting wrong spellings for
a majority of the cases. For example, for the word
mujahidin, Aspell suggested Mukden, which has a
score of 1.0 and is thus rejected. Since we do not
consider context or any other information besides
edit distance, spell checking is not perfect and is
subject to making errors, but the number of errors
is considerably smaller than the number of correct
revisions. For example, lol would be changed into
Lil if it were not listed in the extended lexicon. Ad-
ditionally, since the errors are consistent throughout
the data, they result in normalization even when the
spelling is wrong.
</bodyText>
<sectionHeader confidence="0.985374" genericHeader="method">
4 Parser revision
</sectionHeader>
<bodyText confidence="0.999365714285714">
We use a state of the art dependency parser, MST-
Parser (McDonald and Pereira, 2006), as our main
parser; and we use two parse revision methods: a
machine learning model and a simple tree anomaly
model. The goal is to be able to learn where the
parser errs and to adjust the parses to be more appro-
priate given the target domain of social media texts.
</bodyText>
<subsectionHeader confidence="0.999296">
4.1 Basic parser
</subsectionHeader>
<bodyText confidence="0.99778455">
MSTParser (McDonald and Pereira, 2006)6 is a
freely available parser which reaches state-of-the-art
accuracy in dependency parsing for English. MST is
a graph-based parser which optimizes its parse tree
globally (McDonald et al., 2005), using a variety of
feature sets, i.e., edge, sibling, context, and non-
local features, employing information from words
and POS tags. We use its default settings for all ex-
periments.
We use MST as our base parser, training it in dif-
ferent conditions on the WSJ and the EWT. Also,
MST offers the possibility to retrieve confidence
scores for each dependency edge: We use the KD-
Fix edge confidence scores discussed by Mejer and
Crammer (2012) to assist in parse revision. As de-
scribed in section 4.4, the scores are used to limit
which dependencies are candidates for revision: if
a dependency has a low confidence score, it may be
revised, while high confidence dependencies are not
considered for revision.
</bodyText>
<page confidence="0.706187">
6
</page>
<table confidence="0.1916605">
http://sourceforge.net/projects/
mstparser/
4.2 Reviser #1: machine learning model
We use DeSR (Attardi and Ciaramita, 2007) as a ma-
</table>
<bodyText confidence="0.991968666666666">
chine learning model of parse revision. DeSR uses a
tree revision method based on decomposing revision
actions into basic graph movements and learning se-
quences of such movements, referred to as a revision
rule. For example, the rule -1u indicates that the
reviser should change a dependents head one word
to the left (-1) and then up one element in the tree
(u). Note that DeSR only changes the heads of de-
pendencies, but not their labels. Such revision rules
are learned for a base parser by comparing the base
parser output and the gold-standard of some unseen
data, based on a maximum entropy model.
In experiments, DeSR generally only considers
the most frequent rules (e.g., 20), as these cover
most of the errors. For best results, the reviser
should: a) be trained on extra data other than the
data the base parser is trained on, and b) begin with
a relatively poor base parsing model. As we will see,
using a fairly strong base parser presents difficulties
for DeSR.
4.3 Reviser #2: simple tree anomaly model
Another method we use for building parse revisions
is based on a method to detect anomalies in parse
structures (APS) using n-gram sequences of depen-
dency structures (Dickinson and Smith, 2011; Dick-
inson, 2010). The method checks whether the same
head category (e.g., verb) has a set of dependents
similar to others of the same category (Dickinson,
2010).
To see this, consider the partial tree in figure 2,
from the dependency-converted EWT.7 This tree is
converted to a rule as in (2), where all dependents of
a head are realized.
</bodyText>
<figure confidence="0.6736935">
... DT NN IN ...
dobj
det prep
Figure 2: A sketch of a basic dependency tree
(2) dobj det:DT NN prep:IN
7
DT/det=determiner, NN=noun, IN/prep=preposition,
dobj=direct object
</figure>
<page confidence="0.882007">
4
</page>
<bodyText confidence="0.985415">
\x0cThis rule is then broken down into its component
n-grams and compared to other rules, using the for-
mula for scoring an element (ei) in (3). N-gram
counts (C(ngrm)) come from a training corpus; an
instantiation for this rule is in (4).
</bodyText>
<equation confidence="0.5395586">
(3) s(ei) =
P
ngrm:eingrmn3
C(ngrm)
(4) s(prep:IN) = C(det:DT NN prep:IN)
</equation>
<table confidence="0.95556725">
+ C(NN prep:IN END)
+ C(START det:DT NN prep:IN)
+ C(det:DT NN prep:IN END)
+ C(START det:DT NN prep:IN END)
</table>
<bodyText confidence="0.996510807692308">
We modify the scoring slightly, incorporating bi-
grams (n 2), but weighing them as 0.01 of a count
(C(ngrm)); this handles the issue that bigrams are
not very informative, yet having some bigrams is
better than none (Dickinson and Smith, 2011).
The method detects non-standard parses which
may result from parser error or because the text
is unusual in some other way, e.g., ungrammatical
(Dickinson, 2011). The structures deemed atypical
depend upon the corpus used for obtaining the gram-
mar that parser output is compared to.
With a method of scoring the quality of individual
dependents in a tree, one can compare the score of
a dependent to the score obtaining by hypothesizing
a revision. For error detection, this ameliorates the
effect of odd structures for which no better parse is
available. The revision checking algorithm in Dick-
inson and Smith (2011) posits new labelings and
attachmentsmaintaining projectivity and acyclic-
ity, to consider only reasonable candidates8and
checks whether any have a higher score.9 If so, the
token is flagged as having a better revision and is
more likely to be an error.
In other words, the method checks revisions for
error detection. With a simple modification of the
code,10 one can also keep track of the best revision
</bodyText>
<page confidence="0.983012">
8
</page>
<bodyText confidence="0.9997535">
We remove the cyclicity check, in order to be able to detect
errors where the head and dependent are flipped.
</bodyText>
<page confidence="0.963795">
9
</page>
<bodyText confidence="0.9997412">
We actually check whether a new score is greater than or
equal to twice the original score, to account for meaningless
differences for large values, e.g., 1001 vs. 1000. We do not
expect our minor modifications to have a huge impact, though
more robust testing is surely required.
</bodyText>
<page confidence="0.904933">
10
</page>
<equation confidence="0.8207275">
http://cl.indiana.edu/ md7/papers/
dickinson-smith11.html
</equation>
<bodyText confidence="0.997776833333333">
for each token and actually change the tree structure.
This is precisely what we do. Because the method
relies upon very coarse scores, it can suggest too
many revisions; in tandem with parser confidence,
though, this can filter the set of revisions to a rea-
sonable amount, as discussed next.
</bodyText>
<subsectionHeader confidence="0.999106">
4.4 Pinpointing erroneous parses
</subsectionHeader>
<bodyText confidence="0.99648775">
The parse revision methods rely both on being able
to detect errors and on being able to correct them.
We can assist the methods by using MST confidence
scores (Mejer and Crammer, 2012) to pinpoint can-
didates for revision, and only pass these candidates
on to the parse revisers. For example, since APS
(anomaly detection) detects atypical structures (sec-
tion 4.3), some of which may not be errors, it will
find many strange parses and revise many positions
on its own, though some be questionable revisions.
By using a confidence filter, though, we only con-
sider ones flagged below a certain MST confidence
score. We follow Mejer and Crammer (2012) and
use confidence0.5 as our threshold for identifying
errors. Non-exhaustive tests on a subset of the test
set show good performance with this threshold.
In the experiments reported in section 5, if we use
the revision methods to revise everything, we refer
to this as the DeSR and the APS models; if we fil-
ter out high confidence cases and restrict revisions
to low confidence scoring cases, we refer to this as
DeSR restricted and APS restricted.
Before using the MST confidence scores as part
of the revision process, then, we first report on using
the scores for error detection at the 0.5 threshold,
as shown in table 1. As we can see, using confi-
dence scores allows us to pinpoint errors with high
precision. With a recall around 4050%, we find er-
rors with upwards of 90% precision, meaning that
these cases are in need of revision. Interestingly, the
highest error detection precision comes with WSJ
as part of the training data and EWT as the test-
ing. This could be related to the great difference be-
tween the WSJ and EWT grammatical models and
the greater number of unknown words in this ex-
periment, though more investigation is needed. Al-
though data sets are hard to compare, the precision
seems to outperform that of more generic (i.e., non-
parser-specific) error detection methods (Dickinson
and Smith, 2011).
</bodyText>
<page confidence="0.964357">
5
</page>
<table confidence="0.994746444444444">
\x0cNormalization Attach. Label. Total
Train Test (on test) Tokens Errors Errors Errors Precision Recall
WSJ WSJ none 4,621 2,452 1,297 3,749 0.81 0.40
WSJ EWT none 5,855 3,621 2,169 5,790 0.99 0.38
WSJ EWT full 5,617 3,484 1,959 5,443 0.97 0.37
EWT EWT none 7,268 4,083 2,202 6,285 0.86 0.51
EWT EWT full 7,131 3,905 2,147 6,052 0.85 0.50
WSJ+EWT EWT none 5,622 3,338 1,849 5,187 0.92 0.40
WSJ+EWT EWT full 5,640 3,379 1,862 5,241 0.93 0.41
</table>
<tableCaption confidence="0.998706">
Table 1: Error detection results for MST confidence scores ( 0.5) for different conditions and normalization settings.
</tableCaption>
<bodyText confidence="0.647946">
Number of tokens and errors below the threshold are reported.
</bodyText>
<sectionHeader confidence="0.997819" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999831777777778">
We report three major sets of experiments: the first
set compares the two parse revision strategies; the
second looks into text normalization strategies; and
the third set investigates whether the size of the
training set or its similarity to the target domain is
more important. Since we are interested in parsing
in these experiments, we use gold POS tags as in-
put for the parser, in order to exclude any unwanted
interaction between POS tagging and parsing.
</bodyText>
<subsectionHeader confidence="0.995817">
5.1 Parser revision
</subsectionHeader>
<bodyText confidence="0.997523410714286">
In this experiment, we are interested in comparing a
machine learning method to a simple n-gram revi-
sion model. For all experiments, we use the original
version of the EWT data, without any normalization.
The results of this set of experiments are shown
in table 2. The first row reports MSTs performance
on the standard WSJ data split, giving an idea of an
upper bound for these experiments. The second part
shows MSTs performance on the EWT data, when
trained on WSJ or the combination of the WSJ and
EWT training sets. Note that there is considerable
decrease for both settings in terms of unlabeled ac-
curacy (UAS) and labeled accuracy (LAS), of ap-
proximately 8% when trained on WSJ and 5.5% on
WSJ+EWT. This drop in score is consistent with
previous work on non-canonical data, e.g., web data
(Foster et al., 2011) and learner language (Krivanek
and Meurers, 2011). It is difficult to compare these
results, due to different training and testing condi-
tions, but MST (without any modifications) reaches
results that are in the mid-high range of results re-
ported by Petrov and McDonald (2012, table 4) in
their overview of the SANCL shared task using the
EWT data: 80.1087.62% UAS; 71.04%83.46%
LAS.
Next, we look at the performance of the two re-
visers on the same data sets. Note that since DeSR
requires training data for the revision part that is dif-
ferent from the training set of the base parser, we
conduct parsing and revision in DeSR with two dif-
ferent data sets. Thus, for the WSJ experiment, we
split the WSJ training set into two parts, WSJ02-
11 and WSJ12-2, instead of training on the whole
WSJ. For the EWT training set, we split this set into
two parts and use 25% of it for training the parser
(EWTs) and the rest for training the reviser (EWTr).
In contrast, APS does not need extra data for train-
ing and thus was trained on the same data as the
base parser. While this means that the base parser
for DeSR has a smaller training set, note that DeSR
works best with a weak base parser (Attardi, p.c.).
The results show that DeSRs performance is be-
low MSTs on the same data. In other words,
adding DeSRs revisions decreases accuracy. APS
also shows a deterioration in the results, but the dif-
ference is much smaller. Also, training on a combi-
nation of WSJ and EWT data increases the perfor-
mance of both revisers by 2-3% over training solely
on WSJ.
Since these results show that the revisions are
harmful, we decided to restrict the revisions further
by using MSTs KD-Fix edge confidence scores, as
described in section 4.4. We apply the revisions only
if MSTs confidence in this dependency is low (i.e.,
below or equal to 0.5). The results of this experiment
are shown in the last section of table 2. We can see
</bodyText>
<page confidence="0.990641">
6
</page>
<table confidence="0.9956793">
\x0cMethod Parser Train Reviser Train Test UAS LAS
MST WSJ n/a WSJ 89.94 87.24
MST WSJ n/a EWT 81.98 78.65
MST WSJ+EWT n/a EWT 84.50 81.61
DeSR WSJ02-11 WSJ12-22 EWT 80.63 77.33
DeSR WSJ+EWTs EWTr EWT 82.68 79.77
APS WSJ WSJ EWT 81.96 78.40
APS WSJ+EWT WSJ+EWT EWT 84.45 81.29
DeSR restricted WSJ+EWTs EWTr EWT 84.40 81.50
APS restricted WSJ+EWT WSJ+EWT EWT 84.53 *81.66
</table>
<tableCaption confidence="0.673408">
Table 2: Results of comparing a machine learning reviser (DeSR) with a tree anomaly model (APS), with base parser
MST (* = sig. at the 0.05 level, as compared to row 2).
</tableCaption>
<bodyText confidence="0.986351166666667">
that both revisers improve over their non-restricted
versions. However, while DeSRs results are still
below MSTs baseline results, APS shows slight im-
provements over the MST baseline, significant in the
LAS. Significance was tested using the CoNLL-X
evaluation script in combination with Dan Bikels
</bodyText>
<subsectionHeader confidence="0.575404">
Randomized Parsing Evaluation Comparator, which
</subsectionHeader>
<bodyText confidence="0.998305111111111">
is based on sampling.11
For the original experiment, APS changes 1,402
labels and 272 attachments of the MST output. In
the restricted version, label changes are reduced to
610, and attachment to 167. In contrast, DeSR
changes 1,509 attachments but only 303 in the re-
stricted version. The small numbers, given that
we have more than 3,000 sentences in the test set,
show that finding reliable revisions is a difficult task.
Since both revisers are used more or less off the
shelf, there is much room to improve.
Based on these results and other results based on
different settings, which, for DeSR, resulted in low
accuracy, we decided to concentrate on APS in the
following experiments, and more specifically focus
on the restricted version of APS to see whether there
are significant improvements under different data
conditions.
</bodyText>
<subsectionHeader confidence="0.998522">
5.2 Text normalization
</subsectionHeader>
<bodyText confidence="0.9966122">
In this set of experiments, we investigate the influ-
ence of the text normalization strategies presented
in section 3 on parsing and more specifically on our
parse revision strategy. Thus, we first apply a par-
tial normalization, using only the basic text normal-
</bodyText>
<page confidence="0.998222">
11
</page>
<bodyText confidence="0.9846452">
http://ilk.uvt.nl/conll/software.html
ization. For the full normalization, we combine the
basic text normalization with the spell checker. For
these experiments, we use the restricted APS reviser
and the EWT treebank for training and testing.
The results are shown in table 3. Note that since
we also normalize the training set, MST will also
profit from the normalizations. For this reason, we
present MST and APS (restricted) results for each
type of normalization. The first part of the table
shows the results for MST and APS without any nor-
malization; the numbers here are higher than in ta-
ble 2 because we now train only on EWTan issue
we take up in section 5.3. The second part shows the
results for partial normalization. These results show
that both approaches profit from the normalization
to the same degree: both UAS and LAS increase by
approximately 0.25 percent points. When we look at
the full normalization, including spell checking, we
can see that it does not have a positive effect on MST
but that APSs results increase, especially unlabeled
accuracy. Note that all APS versions significantly
outperform the MST versions but also that both nor-
malized MST versions significantly outperform the
non-normalized MST.
</bodyText>
<subsectionHeader confidence="0.967476">
5.3 WSJ versus domain data
</subsectionHeader>
<bodyText confidence="0.999642428571429">
In these experiments, we are interested in which type
of training data allows us to reach the highest accu-
racy in parsing. Is it more useful to use a large, out-
of-domain training set (WSJ in our case), a small,
in-domain training set, or a combination of both?
Our assumption was that the largest data set, con-
sisting of the WSJ and the EWT training sets, would
</bodyText>
<page confidence="0.997066">
7
</page>
<table confidence="0.989733714285714">
\x0cNorm. Method UAS LAS
Train:no; Test:no MST 84.87 82.21
Train:no; Test:no APS restr. **84.90 *82.23
Train:part; Test:part MST *85.12 *82.45
Train:part; Test:part APS restr. **85.18 *82.50
Train:full; Test:full MST **85.20 *82.45
Train:full; Test:full APS restr. **85.24 **82.52
</table>
<tableCaption confidence="0.832723">
Table 3: Results of comparing different types of text normalization, training and testing on EWT sets. (Significance
tested for APS versions as compared to the corresponding MST version and for each MST with the non-normalized
</tableCaption>
<bodyText confidence="0.996120205882353">
MST: * = sig. at the 0.05 level, ** = significance at the 0.01 level).
give the best results. For these experiments, we use
the EWT test set and different combinations of text
normalization, and the results are shown in table 4.
The first three sections in the table show the re-
sults of training on the WSJ and testing on the EWT.
The results show that both MST and APS profit from
text normalization. Surprisingly, the best results are
gained by using the partial normalization; adding the
spell checker (for full normalization) is detrimental,
because the spell checker introduces additional er-
rors that result in extra, non-standard words in EWT.
Such additional variation in words is not present in
the original training model of the base parser.
For the experiments with the EWT and the com-
bined WSJ+EWT training sets, spell checking does
help, and we report only the results with full normal-
ization since this setting gave us the best results. To
our surprise, results with only the EWT as training
set surpass those of using the full WSJ+EWT train-
ing sets (a UAS of 85.24% and a LAS of 82.52% for
EWT vs. a UAS of 82.34% and a LAS of 79.31%).
Note, however, that when we reduce the size of the
WSJ data such that it matches the size of the EWT
data, performance increases to the highest results,
a UAS of 86.41% and a LAS of 83.67%. Taken
together, these results seem to indicate that quality
(i.e., in-domain data) is more important than mere
(out-of-domain) quantity, but also that more out-of-
domain data can help if it does not overwhelm the
in-domain data. It is also obvious that MST per
se profits the most from normalization, but that the
APS consistently provides small but significant im-
provements over the MST baseline.
</bodyText>
<sectionHeader confidence="0.983662" genericHeader="conclusions">
6 Summary and Outlook
</sectionHeader>
<bodyText confidence="0.999644764705882">
We examined ways to improve parsing social me-
dia and other web data by altering the input data,
namely by normalizing such texts, and by revis-
ing output parses. We found that normalization im-
proves performance, though spell checking has more
of a mixed impact. We also found that a very sim-
ple tree reviser based on grammar comparisons per-
forms slightly but significantly better than the base-
line, across different experimental conditions, and
well outperforms a machine learning model. The re-
sults also demonstrated that, more than the size of
the training data, the goodness of fit of the data has
a great impact on the parser. Perhaps surprisingly,
adding the entire WSJ training data to web training
data leads to a deteriment in performance, whereas
balancing it with web data has the best performance.
There are many ways to take this work in the
future. The small, significant improvements from
the APS restricted reviser indicate that there is po-
tential for improvement in pursuing such grammar-
corrective models for parse revision. The model we
use relies on a simplistic notion of revisions, nei-
ther checking the resulting well-formedness of the
tree nor how one correction influences other cor-
rections. One could also, for example, treat gram-
mars from different domains in different ways to
improve scoring and revision. Another possibility
would be to apply the parse revisions also to the out-
of-domain training data, to make it more similar to
the in-domain data.
For text normalization, the module could benefit
from a few different improvements. For example,
non-contracted words such as well to mean well
require a more complicated normalization step, in-
</bodyText>
<page confidence="0.97856">
8
</page>
<table confidence="0.993520428571429">
\x0cTrain Test Normalization Method UAS LAS
WSJ EWT train:no; test:no MST 81.98 78.65
WSJ EWT train:no; test:no APS 81.96 78.40
WSJ EWT train:no; test:no APS restr 82.02 **78.71
WSJ EWT train:no; test:part MST 82.31 79.27
WSJ EWT train:no; test:part APS restr. *82.36 *79.32
WSJ EWT train:no; test:full MST 82.30 79.26
WSJ EWT train:no; test:full APS restr. 82.34 *79.31
EWT EWT train:full; test:full MST 85.20 82.45
EWT EWT train:full; test:full APS restr. **85.24 **82.52
WSJ+EWT EWT train:full; test:full MST 84.59 81.68
WSJ+EWT EWT train:full; test:full APS restr. **84.63 *81.73
Balanced WSJ+EWT EWT train:full; test:full MST 86.38 83.62
Balanced WSJ+EWT EWT train:full; test:full APS restr. *86.41 **83.67
</table>
<tableCaption confidence="0.832075">
Table 4: Results of different training data sets and normalization patterns on parsing the EWT test data. (Significance
tested for APS versions as compared to the corresponding MST: * = sig. at the 0.05 level, ** = sig. at the 0.01 level)
</tableCaption>
<bodyText confidence="0.979481">
volving machine learning or n-gram language mod-
els. In general, language models could be used for
more context-sensitive spelling correction. Given
the preponderance of terms on the web, using a
named entity recognizer (e.g., Finkel et al., 2005)
for preprocessing may also provide benefits.
</bodyText>
<sectionHeader confidence="0.966914" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99551475">
We would like to thank Giuseppe Attardi for his help
in using DeSR; Can Liu, Shoshana Berleant, and the
IU CL discussion group for discussion; and the three
anonymous reviewers for their helpful comments.
</bodyText>
<sectionHeader confidence="0.985872" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999352476190476">
Giuseppe Attardi and Massimiliano Ciaramita.
2007. Tree revision learning for dependency pars-
ing. In Proceedings of HLT-NAACL-07, pages
388395. Rochester, NY.
Giuseppe Attardi and Felice DellOrletta. 2009. Re-
verse revision and linear tree combination for
dependency parsing. In Proceedings of HLT-
NAACL-09, Short Papers, pages 261264. Boul-
der, CO.
Ann Bies, Justin Mott, Colin Warner, and Seth
Kulick. 2012. English Web Treebank. Linguis-
tic Data Consortium, Philadelphia, PA.
Ozlem Cetinoglu, Anton Bryl, Jennifer Foster, and
Josef Van Genabith. 2011. Improving dependency
label accuracy using statistical post-editing: A
cross-framework study. In Proceedings of the In-
ternational Conference on Dependency Linguis-
tics, pages 300309. Barcelona, Spain.
Marie-Catherine de Marneffe and Christopher D.
Manning. 2008. The Stanford typed dependencies
representation. In COLING 2008 Workshop on
Cross-framework and Cross-domain Parser Eval-
uation. Manchester, England.
Markus Dickinson. 2010. Detecting errors in
automatically-parsed dependency relations. In
Proceedings of ACL-10. Uppsala, Sweden.
Markus Dickinson. 2011. Detecting ad hoc rules for
treebank development. Linguistic Issues in Lan-
guage Technology, 4(3).
Markus Dickinson and Amber Smith. 2011. De-
tecting dependency parse errors with minimal re-
sources. In Proceedings of IWPT-11, pages 241
252. Dublin, Ireland.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of ACL05, pages 363
370. Ann Arbor, MI.
Jennifer Foster. 2010. cba to check the spelling:
Investigating parser performance on discussion
forum posts. In Proceedings of NAACL-HLT
2010, pages 381384. Los Angeles, CA.
</reference>
<page confidence="0.763682">
9
</page>
<reference confidence="0.995953078125">
\x0cJennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan,
and Josef van Genabith. 2011. From news to com-
ment: Resources and benchmarks for parsing the
language of web 2.0. In Proceedings of IJCNLP-
11, pages 893901. Chiang Mai, Thailand.
Phani Gadde, L. V. Subramaniam, and Tanveer A.
Faruquie. 2011. Adapting a WSJ trained part-of-
speech tagger to noisy text: Preliminary results.
In Proceedings of Joint Workshop on Multilingual
OCR and Analytics for Noisy Unstructured Text
Data. Beijing, China.
Enrique Henestroza Anguiano and Marie Candito.
2011. Parse correction with specialized models
for difficult attachment types. In Proceedings of
EMNLP-11, pages 12221233. Edinburgh, UK.
Julia Krivanek and Detmar Meurers. 2011. Compar-
ing rule-based and data-driven dependency pars-
ing of learner language. In Proceedings of the Int.
Conference on Dependency Linguistics (Depling
2011), pages 310317. Barcelona.
Vladimir I. Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions, and reversals.
Cybernetics and Control Theory, 10(8):707710.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Compu-
tational Linguistics, 19(2):313330.
David McClosky, Wanxiang Che, Marta Recasens,
Mengqiu Wang, Richard Socher, and Christopher
Manning. 2012. Stanfords system for parsing the
English web. In Workshop on the Syntactic Anal-
ysis of Non-Canonical Language (SANCL 2012).
Montreal, Canada.
David McClosky, Mihai Surdeanu, and Christopher
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of ACL-HLT-11, pages
16261635. Portland, OR.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of ACL-05,
pages 9198. Ann Arbor, MI.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing
algorithms. In Proceedings of EACL-06. Trento,
Italy.
Avihai Mejer and Koby Crammer. 2012. Are you
sure? Confidence in prediction of dependency
tree edges. In Proceedings of the NAACL-HTL
2012, pages 573576. Montreal, Canada.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kuro-
hashi. 2010. Dependency tree-based sentiment
classification using CRFs with hidden variables.
In Proceedings of NAACL-HLT 2010, pages 786
794. Los Angeles, CA.
Lilja vrelid and Arne Skjrholt. 2012. Lexical
categories for improved parsing of web data. In
Proceedings of the 24th International Conference
on Computational Linguistics (COLING 2012),
pages 903912. Mumbai, India.
Slav Petrov and Ryan McDonald. 2012. Overview
of the 2012 shared task on parsing the web.
In Workshop on the Syntactic Analysis of Non-
Canonical Language (SANCL 2012). Montreal,
</reference>
<figure confidence="0.506491">
Canada.
10
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.671955">
<note confidence="0.933672">b&amp;apos;Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 110, Atlanta, Georgia, June 13 2013. c 2013 Association for Computational Linguistics</note>
<title confidence="0.940158">Does Size Matter? Text and Grammar Revision for Parsing Social Media Data</title>
<author confidence="0.998655">Mohammad Khan</author>
<affiliation confidence="0.999999">Indiana University</affiliation>
<address confidence="0.99501">Bloomington, IN USA</address>
<email confidence="0.99975">khanms@indiana.edu</email>
<author confidence="0.997725">Markus Dickinson</author>
<affiliation confidence="0.999999">Indiana University</affiliation>
<address confidence="0.937978">Bloomington, IN USA</address>
<email confidence="0.996462">md7@indiana.edu</email>
<author confidence="0.9993">Sandra Kubler</author>
<affiliation confidence="0.999999">Indiana University</affiliation>
<address confidence="0.994461">Bloomington, IN USA</address>
<email confidence="0.999879">skuebler@indiana.edu</email>
<abstract confidence="0.999356857142857">We explore improving parsing social media and other web data by altering the input data, namely by normalizing web text, and by revising output parses. We find that text normalization improves performance, though spell checking has more of a mixed impact. We also find that a very simple tree reviser based on grammar comparisons performs slightly but significantly better than the baseline and well outperforms a machine learning model. The results also demonstrate that, more than the size of the training data, the goodness of fit of the data has a great impact on the parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
<author>Massimiliano Ciaramita</author>
</authors>
<title>Tree revision learning for dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL-07,</booktitle>
<pages>388395</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="3999" citStr="Attardi and Ciaramita, 2007" startWordPosition="639" endWordPosition="642">distinct downside of modeling language that is very dissimilar 1 Taken from: http://www.youtube.com/watch? v=eHSpHCprXLA 1 \x0cfrom the target. Or one can train parsers on the English Web Treebank (Bies et al., 2012), which covers web language, including social media data, but is rather small. Our focus on improving parsing for such data is on exploring parse revision techniques for dependency parsers. As far as we know, despite being efficient and trainable on a small amount of data, parse revision (Henestroza Anguiano and Candito, 2011; Cetinoglu et al., 2011; Attardi and DellOrletta, 2009; Attardi and Ciaramita, 2007) has not been used for web data, or more generally for adapting a parser to out-of-domain data; an investigation of its strengths and weaknesses is thus needed. We describe the data sets used in our experiments in section 2 and the process of normalization in section 3 before turning to the main task of parsing in section 4. Within this section, we discuss our main parser as well as two different parse revision methods (sections 4.2 and 4.3). In the evaluation in section 5, we will find that normalization has a positive impact, although spell checking has mixed results, and that a simple tree </context>
<context position="13876" citStr="Attardi and Ciaramita, 2007" startWordPosition="2376" endWordPosition="2379"> parser, training it in different conditions on the WSJ and the EWT. Also, MST offers the possibility to retrieve confidence scores for each dependency edge: We use the KDFix edge confidence scores discussed by Mejer and Crammer (2012) to assist in parse revision. As described in section 4.4, the scores are used to limit which dependencies are candidates for revision: if a dependency has a low confidence score, it may be revised, while high confidence dependencies are not considered for revision. 6 http://sourceforge.net/projects/ mstparser/ 4.2 Reviser #1: machine learning model We use DeSR (Attardi and Ciaramita, 2007) as a machine learning model of parse revision. DeSR uses a tree revision method based on decomposing revision actions into basic graph movements and learning sequences of such movements, referred to as a revision rule. For example, the rule -1u indicates that the reviser should change a dependents head one word to the left (-1) and then up one element in the tree (u). Note that DeSR only changes the heads of dependencies, but not their labels. Such revision rules are learned for a base parser by comparing the base parser output and the gold-standard of some unseen data, based on a maximum ent</context>
</contexts>
<marker>Attardi, Ciaramita, 2007</marker>
<rawString>Giuseppe Attardi and Massimiliano Ciaramita. 2007. Tree revision learning for dependency parsing. In Proceedings of HLT-NAACL-07, pages 388395. Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
<author>Felice DellOrletta</author>
</authors>
<title>Reverse revision and linear tree combination for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of HLTNAACL-09, Short Papers,</booktitle>
<pages>261264</pages>
<location>Boulder, CO.</location>
<contexts>
<context position="3969" citStr="Attardi and DellOrletta, 2009" startWordPosition="635" endWordPosition="638">tatistical parser, but has the distinct downside of modeling language that is very dissimilar 1 Taken from: http://www.youtube.com/watch? v=eHSpHCprXLA 1 \x0cfrom the target. Or one can train parsers on the English Web Treebank (Bies et al., 2012), which covers web language, including social media data, but is rather small. Our focus on improving parsing for such data is on exploring parse revision techniques for dependency parsers. As far as we know, despite being efficient and trainable on a small amount of data, parse revision (Henestroza Anguiano and Candito, 2011; Cetinoglu et al., 2011; Attardi and DellOrletta, 2009; Attardi and Ciaramita, 2007) has not been used for web data, or more generally for adapting a parser to out-of-domain data; an investigation of its strengths and weaknesses is thus needed. We describe the data sets used in our experiments in section 2 and the process of normalization in section 3 before turning to the main task of parsing in section 4. Within this section, we discuss our main parser as well as two different parse revision methods (sections 4.2 and 4.3). In the evaluation in section 5, we will find that normalization has a positive impact, although spell checking has mixed re</context>
</contexts>
<marker>Attardi, DellOrletta, 2009</marker>
<rawString>Giuseppe Attardi and Felice DellOrletta. 2009. Reverse revision and linear tree combination for dependency parsing. In Proceedings of HLTNAACL-09, Short Papers, pages 261264. Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Justin Mott</author>
<author>Colin Warner</author>
<author>Seth Kulick</author>
</authors>
<title>English Web Treebank. Linguistic Data Consortium,</title>
<date>2012</date>
<location>Philadelphia, PA.</location>
<contexts>
<context position="3587" citStr="Bies et al., 2012" startWordPosition="573" endWordPosition="576">ation (Foster et al., 2011; Gadde et al., 2011; Foster, 2010), it has not teased apart which parts of the normalization are beneficial under which circumstances. A second problem with parsing social media data is the data situation: parsers can be trained on the standard training set, the Penn Treebank (Marcus et al., 1993), which has a sufficient size for training a statistical parser, but has the distinct downside of modeling language that is very dissimilar 1 Taken from: http://www.youtube.com/watch? v=eHSpHCprXLA 1 \x0cfrom the target. Or one can train parsers on the English Web Treebank (Bies et al., 2012), which covers web language, including social media data, but is rather small. Our focus on improving parsing for such data is on exploring parse revision techniques for dependency parsers. As far as we know, despite being efficient and trainable on a small amount of data, parse revision (Henestroza Anguiano and Candito, 2011; Cetinoglu et al., 2011; Attardi and DellOrletta, 2009; Attardi and Ciaramita, 2007) has not been used for web data, or more generally for adapting a parser to out-of-domain data; an investigation of its strengths and weaknesses is thus needed. We describe the data sets u</context>
<context position="5267" citStr="Bies et al., 2012" startWordPosition="857" endWordPosition="860">1) outperforms a machine learning reviser (Attardi and Ciaramita, 2007), especially when integrated with confidence scores from the parser itself. In addition to the machine learner requiring a weak baseline parser, some of the main differences include the higher recall of the simple method at positing revisions and the fact that it detects odd structures, which parser confidence can then sort out as incorrect or not. 2 Data For our experiments, we use two main resources, the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993) and the English Web Treebank (EWT) (Bies et al., 2012). The two corpora were converted from PTB constituency trees into dependency trees using the Stanford dependency converter (de Marneffe and Manning, 2008).2 The EWT is comprised of approximately 16,000 sentences from weblogs, newsgroups, emails, reviews, and question-answers. Instead of examining each group individually, we chose to treat all web 2 http://nlp.stanford.edu/software/ stanford-dependencies.shtml 1 &lt;&lt;_ -LRB--LRB-_ 2 punct _ _ 2 File _ NN NN _ 0 root _ _ 3 : _ : : _ 2 punct _ _ 4 220b _ GW GW _ 11 dep _ _ 5 -_ GW GW _ 11 dep _ _ 6 dg _ GW GW _ 11 dep _ _ 7 -_ GW GW _ 11 dep _ _ 8 A</context>
</contexts>
<marker>Bies, Mott, Warner, Kulick, 2012</marker>
<rawString>Ann Bies, Justin Mott, Colin Warner, and Seth Kulick. 2012. English Web Treebank. Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozlem Cetinoglu</author>
<author>Anton Bryl</author>
<author>Jennifer Foster</author>
<author>Josef Van Genabith</author>
</authors>
<title>Improving dependency label accuracy using statistical post-editing: A cross-framework study.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Dependency Linguistics,</booktitle>
<pages>300309</pages>
<location>Barcelona,</location>
<marker>Cetinoglu, Bryl, Foster, Van Genabith, 2011</marker>
<rawString>Ozlem Cetinoglu, Anton Bryl, Jennifer Foster, and Josef Van Genabith. 2011. Improving dependency label accuracy using statistical post-editing: A cross-framework study. In Proceedings of the International Conference on Dependency Linguistics, pages 300309. Barcelona, Spain. Marie-Catherine de Marneffe and Christopher D.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In COLING 2008 Workshop on Cross-framework and Cross-domain Parser Evaluation.</booktitle>
<location>Manchester, England.</location>
<contexts>
<context position="5421" citStr="Manning, 2008" startWordPosition="882" endWordPosition="883"> to the machine learner requiring a weak baseline parser, some of the main differences include the higher recall of the simple method at positing revisions and the fact that it detects odd structures, which parser confidence can then sort out as incorrect or not. 2 Data For our experiments, we use two main resources, the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993) and the English Web Treebank (EWT) (Bies et al., 2012). The two corpora were converted from PTB constituency trees into dependency trees using the Stanford dependency converter (de Marneffe and Manning, 2008).2 The EWT is comprised of approximately 16,000 sentences from weblogs, newsgroups, emails, reviews, and question-answers. Instead of examining each group individually, we chose to treat all web 2 http://nlp.stanford.edu/software/ stanford-dependencies.shtml 1 &lt;&lt;_ -LRB--LRB-_ 2 punct _ _ 2 File _ NN NN _ 0 root _ _ 3 : _ : : _ 2 punct _ _ 4 220b _ GW GW _ 11 dep _ _ 5 -_ GW GW _ 11 dep _ _ 6 dg _ GW GW _ 11 dep _ _ 7 -_ GW GW _ 11 dep _ _ 8 Agreement _ GW GW _ 11 dep _ _ 9 for _ GW GW _ 11 dep _ _ 10 Recruiting _ GW GW _ 11 dep _ _ 11 Services.doc _ NN NN _ 2 dep _ _ 12 &gt;&gt;_ -RRB--RRB-_ 2 punct</context>
</contexts>
<marker>Manning, 2008</marker>
<rawString>Manning. 2008. The Stanford typed dependencies representation. In COLING 2008 Workshop on Cross-framework and Cross-domain Parser Evaluation. Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
</authors>
<title>Detecting errors in automatically-parsed dependency relations.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL-10.</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="15112" citStr="Dickinson, 2010" startWordPosition="2591" endWordPosition="2593">iments, DeSR generally only considers the most frequent rules (e.g., 20), as these cover most of the errors. For best results, the reviser should: a) be trained on extra data other than the data the base parser is trained on, and b) begin with a relatively poor base parsing model. As we will see, using a fairly strong base parser presents difficulties for DeSR. 4.3 Reviser #2: simple tree anomaly model Another method we use for building parse revisions is based on a method to detect anomalies in parse structures (APS) using n-gram sequences of dependency structures (Dickinson and Smith, 2011; Dickinson, 2010). The method checks whether the same head category (e.g., verb) has a set of dependents similar to others of the same category (Dickinson, 2010). To see this, consider the partial tree in figure 2, from the dependency-converted EWT.7 This tree is converted to a rule as in (2), where all dependents of a head are realized. ... DT NN IN ... dobj det prep Figure 2: A sketch of a basic dependency tree (2) dobj det:DT NN prep:IN 7 DT/det=determiner, NN=noun, IN/prep=preposition, dobj=direct object 4 \x0cThis rule is then broken down into its component n-grams and compared to other rules, using the f</context>
</contexts>
<marker>Dickinson, 2010</marker>
<rawString>Markus Dickinson. 2010. Detecting errors in automatically-parsed dependency relations. In Proceedings of ACL-10. Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
</authors>
<title>Detecting ad hoc rules for treebank development.</title>
<date>2011</date>
<journal>Linguistic Issues in Language Technology,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="16438" citStr="Dickinson, 2011" startWordPosition="2817" endWordPosition="2818">on for this rule is in (4). (3) s(ei) = P ngrm:eingrmn3 C(ngrm) (4) s(prep:IN) = C(det:DT NN prep:IN) + C(NN prep:IN END) + C(START det:DT NN prep:IN) + C(det:DT NN prep:IN END) + C(START det:DT NN prep:IN END) We modify the scoring slightly, incorporating bigrams (n 2), but weighing them as 0.01 of a count (C(ngrm)); this handles the issue that bigrams are not very informative, yet having some bigrams is better than none (Dickinson and Smith, 2011). The method detects non-standard parses which may result from parser error or because the text is unusual in some other way, e.g., ungrammatical (Dickinson, 2011). The structures deemed atypical depend upon the corpus used for obtaining the grammar that parser output is compared to. With a method of scoring the quality of individual dependents in a tree, one can compare the score of a dependent to the score obtaining by hypothesizing a revision. For error detection, this ameliorates the effect of odd structures for which no better parse is available. The revision checking algorithm in Dickinson and Smith (2011) posits new labelings and attachmentsmaintaining projectivity and acyclicity, to consider only reasonable candidates8and checks whether any have</context>
</contexts>
<marker>Dickinson, 2011</marker>
<rawString>Markus Dickinson. 2011. Detecting ad hoc rules for treebank development. Linguistic Issues in Language Technology, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
<author>Amber Smith</author>
</authors>
<title>Detecting dependency parse errors with minimal resources.</title>
<date>2011</date>
<booktitle>In Proceedings of IWPT-11,</booktitle>
<pages>241--252</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="4651" citStr="Dickinson and Smith, 2011" startWordPosition="754" endWordPosition="757"> data, or more generally for adapting a parser to out-of-domain data; an investigation of its strengths and weaknesses is thus needed. We describe the data sets used in our experiments in section 2 and the process of normalization in section 3 before turning to the main task of parsing in section 4. Within this section, we discuss our main parser as well as two different parse revision methods (sections 4.2 and 4.3). In the evaluation in section 5, we will find that normalization has a positive impact, although spell checking has mixed results, and that a simple tree anomaly detection method (Dickinson and Smith, 2011) outperforms a machine learning reviser (Attardi and Ciaramita, 2007), especially when integrated with confidence scores from the parser itself. In addition to the machine learner requiring a weak baseline parser, some of the main differences include the higher recall of the simple method at positing revisions and the fact that it detects odd structures, which parser confidence can then sort out as incorrect or not. 2 Data For our experiments, we use two main resources, the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993) and the English Web Treebank (EWT) (Bi</context>
<context position="15094" citStr="Dickinson and Smith, 2011" startWordPosition="2587" endWordPosition="2590">mum entropy model. In experiments, DeSR generally only considers the most frequent rules (e.g., 20), as these cover most of the errors. For best results, the reviser should: a) be trained on extra data other than the data the base parser is trained on, and b) begin with a relatively poor base parsing model. As we will see, using a fairly strong base parser presents difficulties for DeSR. 4.3 Reviser #2: simple tree anomaly model Another method we use for building parse revisions is based on a method to detect anomalies in parse structures (APS) using n-gram sequences of dependency structures (Dickinson and Smith, 2011; Dickinson, 2010). The method checks whether the same head category (e.g., verb) has a set of dependents similar to others of the same category (Dickinson, 2010). To see this, consider the partial tree in figure 2, from the dependency-converted EWT.7 This tree is converted to a rule as in (2), where all dependents of a head are realized. ... DT NN IN ... dobj det prep Figure 2: A sketch of a basic dependency tree (2) dobj det:DT NN prep:IN 7 DT/det=determiner, NN=noun, IN/prep=preposition, dobj=direct object 4 \x0cThis rule is then broken down into its component n-grams and compared to other </context>
<context position="16894" citStr="Dickinson and Smith (2011)" startWordPosition="2890" endWordPosition="2894">, 2011). The method detects non-standard parses which may result from parser error or because the text is unusual in some other way, e.g., ungrammatical (Dickinson, 2011). The structures deemed atypical depend upon the corpus used for obtaining the grammar that parser output is compared to. With a method of scoring the quality of individual dependents in a tree, one can compare the score of a dependent to the score obtaining by hypothesizing a revision. For error detection, this ameliorates the effect of odd structures for which no better parse is available. The revision checking algorithm in Dickinson and Smith (2011) posits new labelings and attachmentsmaintaining projectivity and acyclicity, to consider only reasonable candidates8and checks whether any have a higher score.9 If so, the token is flagged as having a better revision and is more likely to be an error. In other words, the method checks revisions for error detection. With a simple modification of the code,10 one can also keep track of the best revision 8 We remove the cyclicity check, in order to be able to detect errors where the head and dependent are flipped. 9 We actually check whether a new score is greater than or equal to twice the origi</context>
<context position="20051" citStr="Dickinson and Smith, 2011" startWordPosition="3423" endWordPosition="3426"> with high precision. With a recall around 4050%, we find errors with upwards of 90% precision, meaning that these cases are in need of revision. Interestingly, the highest error detection precision comes with WSJ as part of the training data and EWT as the testing. This could be related to the great difference between the WSJ and EWT grammatical models and the greater number of unknown words in this experiment, though more investigation is needed. Although data sets are hard to compare, the precision seems to outperform that of more generic (i.e., nonparser-specific) error detection methods (Dickinson and Smith, 2011). 5 \x0cNormalization Attach. Label. Total Train Test (on test) Tokens Errors Errors Errors Precision Recall WSJ WSJ none 4,621 2,452 1,297 3,749 0.81 0.40 WSJ EWT none 5,855 3,621 2,169 5,790 0.99 0.38 WSJ EWT full 5,617 3,484 1,959 5,443 0.97 0.37 EWT EWT none 7,268 4,083 2,202 6,285 0.86 0.51 EWT EWT full 7,131 3,905 2,147 6,052 0.85 0.50 WSJ+EWT EWT none 5,622 3,338 1,849 5,187 0.92 0.40 WSJ+EWT EWT full 5,640 3,379 1,862 5,241 0.93 0.41 Table 1: Error detection results for MST confidence scores ( 0.5) for different conditions and normalization settings. Number of tokens and errors below t</context>
</contexts>
<marker>Dickinson, Smith, 2011</marker>
<rawString>Markus Dickinson and Amber Smith. 2011. Detecting dependency parse errors with minimal resources. In Proceedings of IWPT-11, pages 241 252. Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL05,</booktitle>
<pages>363--370</pages>
<location>Ann Arbor, MI.</location>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of ACL05, pages 363 370. Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
</authors>
<title>cba to check the spelling: Investigating parser performance on discussion forum posts.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT 2010,</booktitle>
<pages>381384</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="3030" citStr="Foster, 2010" startWordPosition="483" endWordPosition="484">anese-borrowed net term for cute), ungrammatical and (intentionally) misspelled text (e.g., cuttie), emoticons, and short posts with little contextual information, as exemplified in (1).1 (1) Awww cuttie little kitten, so Kawaii &lt;3 To process such data, with its non-standard words, we first develop techniques for normalizing the text, so as to be able to accommodate the wide range of realizations of a given token, e.g., all the different spellings and intentional misspellings of cute. While previous research has shown the benefit of text normalization (Foster et al., 2011; Gadde et al., 2011; Foster, 2010), it has not teased apart which parts of the normalization are beneficial under which circumstances. A second problem with parsing social media data is the data situation: parsers can be trained on the standard training set, the Penn Treebank (Marcus et al., 1993), which has a sufficient size for training a statistical parser, but has the distinct downside of modeling language that is very dissimilar 1 Taken from: http://www.youtube.com/watch? v=eHSpHCprXLA 1 \x0cfrom the target. Or one can train parsers on the English Web Treebank (Bies et al., 2012), which covers web language, including soci</context>
<context position="7510" citStr="Foster, 2010" startWordPosition="1323" endWordPosition="1325">ult to analyze and do not correspond to phenomena found in the PTB (cf., figure 1). To create training and test sets, we broke the data into the following sets: WSJ training: sections 02-22 (42,009 sentences) WSJ testing: section 23 (2,416 sentences) EWT training: 80% of the data, taking the first four out of every five sentences (13,130 sentences) EWT testing: 20% of the data, taking every fifth sentence (3,282 sentences) 2 \x0c3 Text normalization Previous work has shown that accounting for variability in form (e.g., misspellings) on the web, e.g., by mapping each form to a normalized form (Foster, 2010; Gadde et al., 2011) or by delexicalizing the parser to reduce the impact of unknown words (vrelid and Skjrholt, 2012), leads to some parser or tagger improvement. Foster (2010), for example, lists adapting the parsers unknown word model to handle capitalization and misspellings of function words as a possibility for improvement. Gadde et al. (2011) find that a model which posits a corrected sentence and then is POS-taggedtheir tagging after correction (TAC) modeloutperforms one which cleans POS tags in a postprocessing step. We follow this line of inquiry by developing text normalization tec</context>
</contexts>
<marker>Foster, 2010</marker>
<rawString>Jennifer Foster. 2010. cba to check the spelling: Investigating parser performance on discussion forum posts. In Proceedings of NAACL-HLT 2010, pages 381384. Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cJennifer Foster</author>
<author>Ozlem Cetinoglu</author>
<author>Joachim Wagner</author>
<author>Joseph Le Roux</author>
<author>Joakim Nivre</author>
<author>Deirdre Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>From news to comment: Resources and benchmarks for parsing the language of web 2.0.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP11,</booktitle>
<pages>893901</pages>
<institution>Chiang Mai, Thailand. Phani</institution>
<marker>Foster, Cetinoglu, Wagner, Le Roux, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>\x0cJennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and Josef van Genabith. 2011. From news to comment: Resources and benchmarks for parsing the language of web 2.0. In Proceedings of IJCNLP11, pages 893901. Chiang Mai, Thailand. Phani Gadde, L. V. Subramaniam, and Tanveer A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Faruquie</author>
</authors>
<title>Adapting a WSJ trained part-ofspeech tagger to noisy text: Preliminary results.</title>
<date>2011</date>
<booktitle>In Proceedings of Joint Workshop on Multilingual OCR and Analytics for Noisy Unstructured Text Data.</booktitle>
<location>Beijing, China.</location>
<marker>Faruquie, 2011</marker>
<rawString>Faruquie. 2011. Adapting a WSJ trained part-ofspeech tagger to noisy text: Preliminary results. In Proceedings of Joint Workshop on Multilingual OCR and Analytics for Noisy Unstructured Text Data. Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Henestroza Anguiano</author>
<author>Marie Candito</author>
</authors>
<title>Parse correction with specialized models for difficult attachment types.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP-11,</booktitle>
<pages>12221233</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="3914" citStr="Anguiano and Candito, 2011" startWordPosition="627" endWordPosition="630"> 1993), which has a sufficient size for training a statistical parser, but has the distinct downside of modeling language that is very dissimilar 1 Taken from: http://www.youtube.com/watch? v=eHSpHCprXLA 1 \x0cfrom the target. Or one can train parsers on the English Web Treebank (Bies et al., 2012), which covers web language, including social media data, but is rather small. Our focus on improving parsing for such data is on exploring parse revision techniques for dependency parsers. As far as we know, despite being efficient and trainable on a small amount of data, parse revision (Henestroza Anguiano and Candito, 2011; Cetinoglu et al., 2011; Attardi and DellOrletta, 2009; Attardi and Ciaramita, 2007) has not been used for web data, or more generally for adapting a parser to out-of-domain data; an investigation of its strengths and weaknesses is thus needed. We describe the data sets used in our experiments in section 2 and the process of normalization in section 3 before turning to the main task of parsing in section 4. Within this section, we discuss our main parser as well as two different parse revision methods (sections 4.2 and 4.3). In the evaluation in section 5, we will find that normalization has </context>
</contexts>
<marker>Anguiano, Candito, 2011</marker>
<rawString>Enrique Henestroza Anguiano and Marie Candito. 2011. Parse correction with specialized models for difficult attachment types. In Proceedings of EMNLP-11, pages 12221233. Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Krivanek</author>
<author>Detmar Meurers</author>
</authors>
<title>Comparing rule-based and data-driven dependency parsing of learner language.</title>
<date>2011</date>
<booktitle>In Proceedings of the Int.</booktitle>
<contexts>
<context position="22041" citStr="Krivanek and Meurers, 2011" startWordPosition="3760" endWordPosition="3763">ts are shown in table 2. The first row reports MSTs performance on the standard WSJ data split, giving an idea of an upper bound for these experiments. The second part shows MSTs performance on the EWT data, when trained on WSJ or the combination of the WSJ and EWT training sets. Note that there is considerable decrease for both settings in terms of unlabeled accuracy (UAS) and labeled accuracy (LAS), of approximately 8% when trained on WSJ and 5.5% on WSJ+EWT. This drop in score is consistent with previous work on non-canonical data, e.g., web data (Foster et al., 2011) and learner language (Krivanek and Meurers, 2011). It is difficult to compare these results, due to different training and testing conditions, but MST (without any modifications) reaches results that are in the mid-high range of results reported by Petrov and McDonald (2012, table 4) in their overview of the SANCL shared task using the EWT data: 80.1087.62% UAS; 71.04%83.46% LAS. Next, we look at the performance of the two revisers on the same data sets. Note that since DeSR requires training data for the revision part that is different from the training set of the base parser, we conduct parsing and revision in DeSR with two different data </context>
</contexts>
<marker>Krivanek, Meurers, 2011</marker>
<rawString>Julia Krivanek and Detmar Meurers. 2011. Comparing rule-based and data-driven dependency parsing of learner language. In Proceedings of the Int.</rawString>
</citation>
<citation valid="true">
<title>Conference on Dependency Linguistics (Depling</title>
<date>2011</date>
<pages>310317</pages>
<location>Barcelona.</location>
<contexts>
<context position="7862" citStr="(2011)" startWordPosition="1381" endWordPosition="1381">ting: 20% of the data, taking every fifth sentence (3,282 sentences) 2 \x0c3 Text normalization Previous work has shown that accounting for variability in form (e.g., misspellings) on the web, e.g., by mapping each form to a normalized form (Foster, 2010; Gadde et al., 2011) or by delexicalizing the parser to reduce the impact of unknown words (vrelid and Skjrholt, 2012), leads to some parser or tagger improvement. Foster (2010), for example, lists adapting the parsers unknown word model to handle capitalization and misspellings of function words as a possibility for improvement. Gadde et al. (2011) find that a model which posits a corrected sentence and then is POS-taggedtheir tagging after correction (TAC) modeloutperforms one which cleans POS tags in a postprocessing step. We follow this line of inquiry by developing text normalization techniques prior to parsing. 3.1 Basic text normalization Machine learning algorithms and parsers are sensitive to the surface form of words, and different forms of a word can mislead the learner/parser. Our basic text normalization is centered around the idea that reducing unnecessary variation will lead to improved parsing performance. For basic text </context>
<context position="16894" citStr="(2011)" startWordPosition="2894" endWordPosition="2894">detects non-standard parses which may result from parser error or because the text is unusual in some other way, e.g., ungrammatical (Dickinson, 2011). The structures deemed atypical depend upon the corpus used for obtaining the grammar that parser output is compared to. With a method of scoring the quality of individual dependents in a tree, one can compare the score of a dependent to the score obtaining by hypothesizing a revision. For error detection, this ameliorates the effect of odd structures for which no better parse is available. The revision checking algorithm in Dickinson and Smith (2011) posits new labelings and attachmentsmaintaining projectivity and acyclicity, to consider only reasonable candidates8and checks whether any have a higher score.9 If so, the token is flagged as having a better revision and is more likely to be an error. In other words, the method checks revisions for error detection. With a simple modification of the code,10 one can also keep track of the best revision 8 We remove the cyclicity check, in order to be able to detect errors where the head and dependent are flipped. 9 We actually check whether a new score is greater than or equal to twice the origi</context>
</contexts>
<marker>2011</marker>
<rawString>Conference on Dependency Linguistics (Depling 2011), pages 310317. Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<booktitle>Cybernetics and Control Theory,</booktitle>
<pages>10--8</pages>
<contexts>
<context position="11105" citStr="Levenshtein, 1966" startWordPosition="1925" endWordPosition="1926">ore than 3,000 acronyms to identify acronyms and other abbreviations not used commonly in formal registers of language. The list was obtained from NetLingo, restricted to the entries listed as chat acronyms and text message shorthand.4 To identify slang terminology, we use the Urban Dictionary5. In a last step, we combine both lists with the list of words extracted from the WSJ. If a word is not found in these lists, Aspell is used to suggest a correct spelling. In order to restrict Aspell from suggesting spellings that are too different from the word in question, we use Levenshtein distance (Levenshtein, 1966) to measure the degree of similarity between the original form and the suggested spelling; only words with small distances are accepted as spelling corrections. Since we have words of varying length, the Levenshtein distance is normalized by the length of the suggested spelling (i.e., number of characters). In non-exhaustive tests on a subset of the test set, we found that a normalized score of 0.301, i.e., a relatively low score accepting only conservative changes, achieves the best results when used as a threshold for accepting a sug3 www.aspell.net 4 http://www.netlingo.com/acronyms.php 5 w</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Cybernetics and Control Theory, 10(8):707710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="3294" citStr="Marcus et al., 1993" startWordPosition="525" endWordPosition="528">h its non-standard words, we first develop techniques for normalizing the text, so as to be able to accommodate the wide range of realizations of a given token, e.g., all the different spellings and intentional misspellings of cute. While previous research has shown the benefit of text normalization (Foster et al., 2011; Gadde et al., 2011; Foster, 2010), it has not teased apart which parts of the normalization are beneficial under which circumstances. A second problem with parsing social media data is the data situation: parsers can be trained on the standard training set, the Penn Treebank (Marcus et al., 1993), which has a sufficient size for training a statistical parser, but has the distinct downside of modeling language that is very dissimilar 1 Taken from: http://www.youtube.com/watch? v=eHSpHCprXLA 1 \x0cfrom the target. Or one can train parsers on the English Web Treebank (Bies et al., 2012), which covers web language, including social media data, but is rather small. Our focus on improving parsing for such data is on exploring parse revision techniques for dependency parsers. As far as we know, despite being efficient and trainable on a small amount of data, parse revision (Henestroza Anguia</context>
<context position="5212" citStr="Marcus et al., 1993" startWordPosition="847" endWordPosition="850">e tree anomaly detection method (Dickinson and Smith, 2011) outperforms a machine learning reviser (Attardi and Ciaramita, 2007), especially when integrated with confidence scores from the parser itself. In addition to the machine learner requiring a weak baseline parser, some of the main differences include the higher recall of the simple method at positing revisions and the fact that it detects odd structures, which parser confidence can then sort out as incorrect or not. 2 Data For our experiments, we use two main resources, the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993) and the English Web Treebank (EWT) (Bies et al., 2012). The two corpora were converted from PTB constituency trees into dependency trees using the Stanford dependency converter (de Marneffe and Manning, 2008).2 The EWT is comprised of approximately 16,000 sentences from weblogs, newsgroups, emails, reviews, and question-answers. Instead of examining each group individually, we chose to treat all web 2 http://nlp.stanford.edu/software/ stanford-dependencies.shtml 1 &lt;&lt;_ -LRB--LRB-_ 2 punct _ _ 2 File _ NN NN _ 0 root _ _ 3 : _ : : _ 2 punct _ _ 4 220b _ GW GW _ 11 dep _ _ 5 -_ GW GW _ 11 dep _ </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Wanxiang Che</author>
<author>Marta Recasens</author>
<author>Mengqiu Wang</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>Stanfords system for parsing the English web.</title>
<date>2012</date>
<booktitle>In Workshop on the Syntactic Analysis of Non-Canonical Language (SANCL 2012).</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="9611" citStr="McClosky et al. (2012)" startWordPosition="1673" endWordPosition="1676">onary, including the internet and slang dictionaries discussed below, in which case they map to the dictionary form. Thus, the word Awww in example (1) is shortened to Aw, and cooool maps to the dictionary form cool. However, since we use gold POS tags for our experiments, this module is not used in the experiments reported here. 3.2 Spell checking Next, we run a spell checker to normalize misspellings, as online data often contains spelling errors (e.g. cuttie in example (1)). Various systems for parsing web data (e.g., from the SANCL shared task) have thus also explored spelling correction; McClosky et al. (2012), for example, used 1,057 autocorrect rules, thoughsince these did not make many changesthe system was not explored after that. Spell checking web data, such as YouTube comments or blog data, is a challenge because it contains non-standard orthography, as well as acronyms and other short-hand forms unknown to a standard spelling dictionary. Therefore, before mapping to a corrected spelling, it is vital to differentiate between a misspelled word and a nonstandard one. We use Aspell3 as our spell checker to recognize and correct misspelled words. If asked to correct non-standard words, the spell</context>
</contexts>
<marker>McClosky, Che, Recasens, Wang, Socher, Manning, 2012</marker>
<rawString>David McClosky, Wanxiang Che, Marta Recasens, Mengqiu Wang, Richard Socher, and Christopher Manning. 2012. Stanfords system for parsing the English web. In Workshop on the Syntactic Analysis of Non-Canonical Language (SANCL 2012). Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Mihai Surdeanu</author>
<author>Christopher Manning</author>
</authors>
<title>Event extraction as dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT-11,</booktitle>
<pages>16261635</pages>
<location>Portland, OR.</location>
<contexts>
<context position="1835" citStr="McClosky et al., 2011" startWordPosition="288" endWordPosition="291"> data (Petrov and McDonald, 2012), which is not a good fit for social media data. The language used in social media does not follow standard conventions (e.g., containing many sentence fragments), is largely unedited, and tends to be on different topics than standard NLP technology is trained for. At the same time, there is a clear need to develop even basic NLP technology for a variety of types of social media and contexts (e.g., Twitter, Facebook, YouTube comments, discussion forums, blogs, etc.). To perform tasks such as sentiment analysis (Nakagawa et al., 2010) or information extraction (McClosky et al., 2011), it helps to perform tagging and parsing, with an eye towards providing a shallow semantic analysis. We advance this line of research by investigating adapting parsing to social media and other web data. Specifically, we focus on two areas: 1) We compare the impact of various text normalization techniques on parsing web data; and 2) we explore parse revision techniques for dependency parsing web data to improve the fit of the grammar learned by the parser. One of the major problems in processing social media data is the common usage of non-standard terms (e.g., kawaii, a Japanese-borrowed net</context>
</contexts>
<marker>McClosky, Surdeanu, Manning, 2011</marker>
<rawString>David McClosky, Mihai Surdeanu, and Christopher Manning. 2011. Event extraction as dependency parsing. In Proceedings of ACL-HLT-11, pages 16261635. Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05,</booktitle>
<pages>9198</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="13042" citStr="McDonald et al., 2005" startWordPosition="2240" endWordPosition="2243"> 4 Parser revision We use a state of the art dependency parser, MSTParser (McDonald and Pereira, 2006), as our main parser; and we use two parse revision methods: a machine learning model and a simple tree anomaly model. The goal is to be able to learn where the parser errs and to adjust the parses to be more appropriate given the target domain of social media texts. 4.1 Basic parser MSTParser (McDonald and Pereira, 2006)6 is a freely available parser which reaches state-of-the-art accuracy in dependency parsing for English. MST is a graph-based parser which optimizes its parse tree globally (McDonald et al., 2005), using a variety of feature sets, i.e., edge, sibling, context, and nonlocal features, employing information from words and POS tags. We use its default settings for all experiments. We use MST as our base parser, training it in different conditions on the WSJ and the EWT. Also, MST offers the possibility to retrieve confidence scores for each dependency edge: We use the KDFix edge confidence scores discussed by Mejer and Crammer (2012) to assist in parse revision. As described in section 4.4, the scores are used to limit which dependencies are candidates for revision: if a dependency has a l</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of ACL-05, pages 9198. Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL-06.</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="12522" citStr="McDonald and Pereira, 2006" startWordPosition="2152" endWordPosition="2155">in, Aspell suggested Mukden, which has a score of 1.0 and is thus rejected. Since we do not consider context or any other information besides edit distance, spell checking is not perfect and is subject to making errors, but the number of errors is considerably smaller than the number of correct revisions. For example, lol would be changed into Lil if it were not listed in the extended lexicon. Additionally, since the errors are consistent throughout the data, they result in normalization even when the spelling is wrong. 4 Parser revision We use a state of the art dependency parser, MSTParser (McDonald and Pereira, 2006), as our main parser; and we use two parse revision methods: a machine learning model and a simple tree anomaly model. The goal is to be able to learn where the parser errs and to adjust the parses to be more appropriate given the target domain of social media texts. 4.1 Basic parser MSTParser (McDonald and Pereira, 2006)6 is a freely available parser which reaches state-of-the-art accuracy in dependency parsing for English. MST is a graph-based parser which optimizes its parse tree globally (McDonald et al., 2005), using a variety of feature sets, i.e., edge, sibling, context, and nonlocal fe</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL-06. Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avihai Mejer</author>
<author>Koby Crammer</author>
</authors>
<title>Are you sure? Confidence in prediction of dependency tree edges.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HTL 2012,</booktitle>
<pages>573576</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="13483" citStr="Mejer and Crammer (2012)" startWordPosition="2316" endWordPosition="2319">available parser which reaches state-of-the-art accuracy in dependency parsing for English. MST is a graph-based parser which optimizes its parse tree globally (McDonald et al., 2005), using a variety of feature sets, i.e., edge, sibling, context, and nonlocal features, employing information from words and POS tags. We use its default settings for all experiments. We use MST as our base parser, training it in different conditions on the WSJ and the EWT. Also, MST offers the possibility to retrieve confidence scores for each dependency edge: We use the KDFix edge confidence scores discussed by Mejer and Crammer (2012) to assist in parse revision. As described in section 4.4, the scores are used to limit which dependencies are candidates for revision: if a dependency has a low confidence score, it may be revised, while high confidence dependencies are not considered for revision. 6 http://sourceforge.net/projects/ mstparser/ 4.2 Reviser #1: machine learning model We use DeSR (Attardi and Ciaramita, 2007) as a machine learning model of parse revision. DeSR uses a tree revision method based on decomposing revision actions into basic graph movements and learning sequences of such movements, referred to as a re</context>
<context position="18266" citStr="Mejer and Crammer, 2012" startWordPosition="3118" endWordPosition="3121">act, though more robust testing is surely required. 10 http://cl.indiana.edu/ md7/papers/ dickinson-smith11.html for each token and actually change the tree structure. This is precisely what we do. Because the method relies upon very coarse scores, it can suggest too many revisions; in tandem with parser confidence, though, this can filter the set of revisions to a reasonable amount, as discussed next. 4.4 Pinpointing erroneous parses The parse revision methods rely both on being able to detect errors and on being able to correct them. We can assist the methods by using MST confidence scores (Mejer and Crammer, 2012) to pinpoint candidates for revision, and only pass these candidates on to the parse revisers. For example, since APS (anomaly detection) detects atypical structures (section 4.3), some of which may not be errors, it will find many strange parses and revise many positions on its own, though some be questionable revisions. By using a confidence filter, though, we only consider ones flagged below a certain MST confidence score. We follow Mejer and Crammer (2012) and use confidence0.5 as our threshold for identifying errors. Non-exhaustive tests on a subset of the test set show good performance w</context>
</contexts>
<marker>Mejer, Crammer, 2012</marker>
<rawString>Avihai Mejer and Koby Crammer. 2012. Are you sure? Confidence in prediction of dependency tree edges. In Proceedings of the NAACL-HTL 2012, pages 573576. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using CRFs with hidden variables.</title>
<date>2010</date>
<contexts>
<context position="1785" citStr="Nakagawa et al., 2010" startWordPosition="280" endWordPosition="283">ifficult, as parsers are generally trained on news data (Petrov and McDonald, 2012), which is not a good fit for social media data. The language used in social media does not follow standard conventions (e.g., containing many sentence fragments), is largely unedited, and tends to be on different topics than standard NLP technology is trained for. At the same time, there is a clear need to develop even basic NLP technology for a variety of types of social media and contexts (e.g., Twitter, Facebook, YouTube comments, discussion forums, blogs, etc.). To perform tasks such as sentiment analysis (Nakagawa et al., 2010) or information extraction (McClosky et al., 2011), it helps to perform tagging and parsing, with an eye towards providing a shallow semantic analysis. We advance this line of research by investigating adapting parsing to social media and other web data. Specifically, we focus on two areas: 1) We compare the impact of various text normalization techniques on parsing web data; and 2) we explore parse revision techniques for dependency parsing web data to improve the fit of the grammar learned by the parser. One of the major problems in processing social media data is the common usage of non-sta</context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 2010. Dependency tree-based sentiment classification using CRFs with hidden variables.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of NAACL-HLT 2010,</booktitle>
<pages>786--794</pages>
<location>Los Angeles, CA.</location>
<marker></marker>
<rawString>In Proceedings of NAACL-HLT 2010, pages 786 794. Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lilja vrelid</author>
<author>Arne Skjrholt</author>
</authors>
<title>Lexical categories for improved parsing of web data.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<pages>903912</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="7629" citStr="vrelid and Skjrholt, 2012" startWordPosition="1343" endWordPosition="1346">d test sets, we broke the data into the following sets: WSJ training: sections 02-22 (42,009 sentences) WSJ testing: section 23 (2,416 sentences) EWT training: 80% of the data, taking the first four out of every five sentences (13,130 sentences) EWT testing: 20% of the data, taking every fifth sentence (3,282 sentences) 2 \x0c3 Text normalization Previous work has shown that accounting for variability in form (e.g., misspellings) on the web, e.g., by mapping each form to a normalized form (Foster, 2010; Gadde et al., 2011) or by delexicalizing the parser to reduce the impact of unknown words (vrelid and Skjrholt, 2012), leads to some parser or tagger improvement. Foster (2010), for example, lists adapting the parsers unknown word model to handle capitalization and misspellings of function words as a possibility for improvement. Gadde et al. (2011) find that a model which posits a corrected sentence and then is POS-taggedtheir tagging after correction (TAC) modeloutperforms one which cleans POS tags in a postprocessing step. We follow this line of inquiry by developing text normalization techniques prior to parsing. 3.1 Basic text normalization Machine learning algorithms and parsers are sensitive to the sur</context>
</contexts>
<marker>vrelid, Skjrholt, 2012</marker>
<rawString>Lilja vrelid and Arne Skjrholt. 2012. Lexical categories for improved parsing of web data. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 903912. Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<title>Overview of the 2012 shared task on parsing the web.</title>
<date>2012</date>
<contexts>
<context position="1246" citStr="Petrov and McDonald, 2012" startWordPosition="188" endWordPosition="192">ind that text normalization improves performance, though spell checking has more of a mixed impact. We also find that a very simple tree reviser based on grammar comparisons performs slightly but significantly better than the baseline and well outperforms a machine learning model. The results also demonstrate that, more than the size of the training data, the goodness of fit of the data has a great impact on the parser. 1 Introduction and Motivation Parsing data from social media data, as well as other data from the web, is notoriously difficult, as parsers are generally trained on news data (Petrov and McDonald, 2012), which is not a good fit for social media data. The language used in social media does not follow standard conventions (e.g., containing many sentence fragments), is largely unedited, and tends to be on different topics than standard NLP technology is trained for. At the same time, there is a clear need to develop even basic NLP technology for a variety of types of social media and contexts (e.g., Twitter, Facebook, YouTube comments, discussion forums, blogs, etc.). To perform tasks such as sentiment analysis (Nakagawa et al., 2010) or information extraction (McClosky et al., 2011), it helps </context>
<context position="22266" citStr="Petrov and McDonald (2012" startWordPosition="3797" endWordPosition="3800"> or the combination of the WSJ and EWT training sets. Note that there is considerable decrease for both settings in terms of unlabeled accuracy (UAS) and labeled accuracy (LAS), of approximately 8% when trained on WSJ and 5.5% on WSJ+EWT. This drop in score is consistent with previous work on non-canonical data, e.g., web data (Foster et al., 2011) and learner language (Krivanek and Meurers, 2011). It is difficult to compare these results, due to different training and testing conditions, but MST (without any modifications) reaches results that are in the mid-high range of results reported by Petrov and McDonald (2012, table 4) in their overview of the SANCL shared task using the EWT data: 80.1087.62% UAS; 71.04%83.46% LAS. Next, we look at the performance of the two revisers on the same data sets. Note that since DeSR requires training data for the revision part that is different from the training set of the base parser, we conduct parsing and revision in DeSR with two different data sets. Thus, for the WSJ experiment, we split the WSJ training set into two parts, WSJ02- 11 and WSJ12-2, instead of training on the whole WSJ. For the EWT training set, we split this set into two parts and use 25% of it for t</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web.</rawString>
</citation>
<citation valid="false">
<booktitle>In Workshop on the Syntactic Analysis of NonCanonical Language (SANCL 2012).</booktitle>
<location>Montreal,</location>
<marker></marker>
<rawString>In Workshop on the Syntactic Analysis of NonCanonical Language (SANCL 2012). Montreal,</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>