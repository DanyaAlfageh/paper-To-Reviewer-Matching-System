In some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, CITATION and CITATION),,
In contrast, the Nitrogen (CITATIONa, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection,,
We use the evaluation metrics employed in published evaluations of HALogen, FUF/SURGE, and FERGUS (e.g., CITATION), although our results are for ordering only,,
In addition to processing time per input, we apply four other metrics: exact match, NIST simple string accuracy (the complement of the familiar word error rate), the IBM Bleu score CITATION, and the intra-constituent edit distance metric introduced earlier,,
With this extension, a model of Markov 3 A Markov grammar is a model of constituent structure that starts at the root of the tree and assigns probability to the expansion of a non-terminal one daughter at a time, rather than as entire productions (CITATION & 2000),,
We refer the reader to CITATION for details and evaluations of several headdriven models (the missing T3, T4, and T6 in this discussion),,
Label heads, according to Charniaks head labeling rules CITATION C,,
3 Estimation We estimate a models distributions with probabilistic decision trees (DTs).4 We build decision trees using the WinMine toolkit CITATION,,
itrogen, the HALogen system (CITATION; CITATIONa, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent,,
Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (CITATION; CITATIONa, 2002b; CITATION),,
CITATION; CITATIONa, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent,,
Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (CITATION; CITATIONa, 2002b; CITATION),,
CITATION; CITATIONa, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent,,
Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (CITATION; CITATIONa, 2002b; CITATION),,
( ) ( ) ( ) 1 , , , , , , i j j j i j j j i k k k i k P yes d P D d P yes d = = = = (8) This turns out to be the decision tree analogue of a Maximum Entropy Markov Model (MEMM) CITATION, which we can refer to as a DTMM,,
For a given sentence in our training set, we begin by analyzing the sentence as a surface syntax tree and an abstract predicate argument structure using the NLPWin system CITATION,,
In contrast, the Nitrogen (CITATIONa, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection,,
Like Nitrogen, the HALogen system (CITATION; CITATIONa, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent,,
Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (CITATION; Gamon et al., 2,,
, the Nitrogen (CITATIONa, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection,,
Like Nitrogen, the HALogen system (CITATION; CITATIONa, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent,,
Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (CITATION; CITATIONa, 2002b; Smets et a,,
, the Nitrogen (CITATIONa, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection,,
Like Nitrogen, the HALogen system (CITATION; CITATIONa, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent,,
Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (CITATION; CITATIONa, 2002b; Smets et a,,
In some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, CITATION and CITATION),,
In contrast, the Nitrogen (CITATIONa, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection,,
Like Nitrogen, the HALogen system (CITATION; CITATIONa, 2002b) uses word n,,
In some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, CITATION and CITATION),,
In contrast, the Nitrogen (CITATIONa, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection,,
Like Nitrogen, the HALogen system (CITATION; CITATIONa, 2002b) uses word n,,
( ) ( ) ( ) 1 , , , , , , i j j j i j j j i k k k i k P yes d P D d P yes d = = = = (8) This turns out to be the decision tree analogue of a Maximum Entropy Markov Model (MEMM) CITATION, which we can refer to as a DTMM,,
For a given sentence in our training set, we begin by analyzing the sentence as a surface syntax tree and an abstract predicate argument structure using the NLPWin system CITATION,,
We use the evaluation metrics employed in published evaluations of HALogen, FUF/SURGE, and FERGUS (e.g., CITATION), although our results are for ordering only,,
In addition to processing time per input, we apply four other metrics: exact match, NIST simple string accuracy (the complement of the familiar word error rate), the IBM Bleu score CITATION, and the intra-constituent edit distance metric introduced earlier,,
In some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, CITATION and CITATION),,
In contrast, the Nitrogen (CITATIONa, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection,,
1 For an overview of some of the issues in determining word and constituent order in German and French, see CITATION,,
cture that starts at the root of the tree and assigns probability to the expansion of a non-terminal one daughter at a time, rather than as entire productions (CITATION & 2000),,
We refer the reader to CITATION for details and evaluations of several headdriven models (the missing T3, T4, and T6 in this discussion),,
One experiment using interpolated language modeling techniques is described in CITATION 4 Search 4.1 Exhaustive search Given an unordered tree and a model of constituent structure O of any of the types already presented, we search for the best ordered tree that maximizes ( ) O P or ( ) O P , as appropriate, with the context varying according to the complexity of the model,,
(For a discussion of the impact of modeling German verb position, please consult CITATION.) \x0cBaseline (random) Greedy, IOCC Greedy DP, IOCC DP Total Sentences 2416 2416 2416 2416 2416 Mean Tokens/Sentence 23.59 23.59 23.59 23.59 23.59 Time/Input (sec.) n/a 0.01 0.01 0.39 0.43 Exact Match 0.424% 33.14% 27.53% 33.53% 35.72% Coverage 100% 100% 100% 100% 100% Mean Per-Const,,
Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (CITATION; CITATIONa, 2002b; CITATION),,
