<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.468714">
b&apos;Discriminative Training Methods for Hidden Markov Models:
Theory and Experiments with Perceptron Algorithms
</title>
<author confidence="0.538872">
Michael Collins
</author>
<affiliation confidence="0.175629">
AT&amp;T Labs-Research, Florham Park, New Jersey.
</affiliation>
<email confidence="0.955204">
mcollins@research.att.com
</email>
<sectionHeader confidence="0.98902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9988714">
We describe new algorithms for train-
ing tagging models, as an alternative
to maximum-entropy models or condi-
tional random \x0celds (CRFs). The al-
gorithms rely on Viterbi decoding of
training examples, combined with sim-
ple additive updates. We describe the-
ory justifying the algorithms through
a modi\x0ccation of the proof of conver-
gence of the perceptron algorithm for
classi\x0ccation problems. We give exper-
imental results on part-of-speech tag-
gingandbasenounphrasechunking,in
both cases showing improvements over
results for a maximum-entropy tagger.
</bodyText>
<sectionHeader confidence="0.994393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996164537037037">
Maximum-entropy (ME) models are justi\x0cably
a very popular choice for tagging problems in
Natural Language Processing: for example see
(Ratnaparkhi 96) for their use on part-of-speech
tagging, and (McCallum et al. 2000) for their
use on a FAQ segmentation task. ME models
have the advantage of being quite
exible in the
features that can be incorporated in the model.
However, recenttheoreticalandexperimentalre-
sults in (La\x0berty et al. 2001) have highlighted
problemswiththeparameterestimationmethod
for ME models. In response to these problems,
they describe alternative parameter estimation
methods based on Conditional Markov Random
Fields(CRFs). (La\x0berty et al. 2001) give exper-
imental results suggesting that CRFs can per-
form signi\x0ccantly better than ME models.
In this paper we describe parameter estima-
tionalgorithmswhicharenaturalalternativesto
CRFs. The algorithms are based on the percep-
tron algorithm (Rosenblatt 58), and the voted
or averaged versions of the perceptron described
in (Freund &amp; Schapire 99). These algorithms
have been shown by (Freund &amp; Schapire 99) to
be competitive withmodern learningalgorithms
such as support vector machines; however, they
have previously been applied mainly to classi\x0c-
cation tasks, and it is not entirely clear how the
algorithms can be carried across to NLP tasks
such as tagging or parsing.
This paper describes variants of the percep-
tron algorithm for tagging problems. The al-
gorithms rely on Viterbi decoding of training
examples, combined with simple additive up-
dates. We describe theory justifying the algo-
rithmthroughamodi\x0ccationoftheproofofcon-
vergence of the perceptron algorithm for classi-
\x0ccation problems. We give experimental results
on part-of-speech tagging and base noun phrase
chunking, in both cases showing improvements
over results for a maximum-entropy tagger (a
11.9% relative reduction in error for POS tag-
ging, a 5.1% relative reduction in error for NP
chunking). Although we concentrate on tagging
problems in this paper, the theoretical frame-
work and algorithm described in section 3 of
this paper should be applicable to a wide va-
riety of models where Viterbi-style algorithms
can be used for decoding: examples are Proba-
bilistic Context-Free Grammars, or ME models
for parsing. See (Collinsand Du\x0by 2001; Collins
and Du\x0by 2002; Collins 2002) for other applica-
tions of the voted perceptronto NLP problems.1
</bodyText>
<sectionHeader confidence="0.965248" genericHeader="method">
2 Parameter Estimation
</sectionHeader>
<subsectionHeader confidence="0.644779">
2.1 HMM Taggers
</subsectionHeader>
<bodyText confidence="0.7955755">
In this section, as a motivating example, we de-
scribe a special case of the algorithm in this
paper: the algorithm applied to a trigram tag-
ger. In a trigram HMM tagger, each trigram
1The theorems in section 3, and the proofs in sec-
tion 5, apply directly to the work in these other papers.
Association for Computational Linguistics.
Language Processing (EMNLP), Philadelphia, July 2002, pp. 1-8.
Proceedings of the Conference on Empirical Methods in Natural
\x0cof tags and each tag/word pair have associated
parameters. We write the parameter associated
with a trigram hx;y;zi as \x0bx;y;z, and the param-
eter associated with a tag/word pair (t;w) as
\x0bt;w. A common approach is to take the param-
etersto beestimatesofconditionalprobabilities:
\x0bx;y;z = logP(z j x;y), \x0bt;w = logP(w j t).
For convenience we will use w[1:n] as short-
hand for a sequence of words [w1;w2 :::wn],
and t[1:n] as shorthand for a taq sequence
[t1;t2 :::tn]. In a trigram tagger the score for
a tagged sequence t[1:n] paired with a word se-
quence w[1:n] is2 Pn
</bodyText>
<equation confidence="0.9056415">
i=1 \x0bti 2;ti 1;ti + Pn
i=1 \x0bti;wi.
</equation>
<bodyText confidence="0.975008333333333">
When the parameters are conditional probabil-
ities as above this \\score&quot; is an estimate of the
log of the joint probability P(w[1:n];t[1:n]). The
Viterbialgorithmcanbeusedto \x0cndthe highest
scoring tagged sequence under this score.
As an alternative to maximum{likelihood pa-
rameter estimates, this paper will propose the
following estimation algorithm. Say the train-
ing set consists of n tagged sentences, the i\&apos;th
sentence being of length ni. We will write these
examples as (wi
[1:ni];ti
[1:ni]) for i = 1:::n. Then
the training algorithm is as follows:
\x0f Choose a parameter T de\x0cning the number
of iterations over the training set.3
\x0f Initially set all parameters \x0bx;y;z and \x0bt;w
to be zero.
\x0f For t = 1:::T;i = 1:::n: Use the Viterbi
algorithm to \x0cnd the best tagged sequence for
sentence wi
[1:ni] under the current parameter
settings: we call this tagged sequence z[1:ni].
For every tag trigram hx;y;zi seen c1 times in
</bodyText>
<equation confidence="0.912516">
ti
[1:ni] and c2 times in z[1:ni] where c1 6= c2 set
\x0bx;y;z = \x0bx;y;z + c1 c2. For every tag/word
pair ht;wi seen c1 times in (wi
[1:ni];ti
[1:ni]) and
c2 times in (wi
[1:ni];z[1:ni]) where c1 6= c2 set
\x0bt;w = \x0bt;w +c1 c2.
</equation>
<bodyText confidence="0.913023">
As an example, say the i\&apos;th tagged sentence
</bodyText>
<equation confidence="0.6893235">
(wi
[1:ni];ti
[1:ni]) in training data is
the/D man/N saw/V the/D dog/N
</equation>
<bodyText confidence="0.6707105">
and under the current parameter settings the
highest scoring tag sequence (wi
</bodyText>
<footnote confidence="0.982023666666667">
[1:ni];z[1:ni]) is
2We take t 1 and t 2 to be special NULL tag symbols.
3T is usually chosen by tuning on a development set.
</footnote>
<bodyText confidence="0.921389923076923">
the/D man/N saw/N the/D dog/N
Then the parameter update will add 1 to the
parameters \x0bD;N;V , \x0bN;V;D, \x0bV;D;N, \x0bV;saw and
subtract 1 from the parameters \x0bD;N;N, \x0bN;N;D,
\x0bN;D;N, \x0bN;saw. Intuitively this has the ef-
fect of increasing the parameter values for fea-
tures which were \\missing&quot; from the proposed
sequence z[1:ni], and downweighting parameter
values for \\incorrect&quot; features in the sequence
z[1:ni]. Note that if z[1:ni] = ti
[1:ni]  |i.e., the
proposed tag sequence is correct  |no changes
are made to the parameter values.
</bodyText>
<subsectionHeader confidence="0.8881265">
2.2 Local and Global Feature Vectors
Wenowdescribehowtogeneralizethealgorithm
</subsectionHeader>
<bodyText confidence="0.994805333333333">
to more general representations of tagged se-
quences. In this section we describe the feature-
vector representations whichare commonlyused
in maximum-entropy models for tagging, and
which are also used in this paper.
In maximum-entropy taggers (Ratnaparkhi
96; McCallum et al. 2000), the tagging prob-
lem is decomposed into sequence of decisions in
tagging the problem in left-to-right fashion. At
each point there is a \\history&quot; { the context in
which a tagging decision is made { and the task
is to predict the tag given the history. Formally,
a history is a 4-tuple ht 1;t 2;w[1:n];ii where
t 1;t 2 are the previoustwo tags, w[1:n] isan ar-
ray specifyingthe n words in the inputsentence,
and i is the index of the word being tagged. We
use H to denote the set of all possible histories.
Maximum-entropy models represent the tag-
gingtaskthroughafeature-vector representation
ofhistory-tagpairs. Afeaturevector representa-
tion \x1e : H\x02T ! Rd is a function \x1e that maps a
history{tag pair to a d-dimensional feature vec-
tor. Each component \x1es(h;t) for s = 1:::d
could be an arbitrary function of (h;t). It is
common (e.g., see (Ratnaparkhi 96)) for each
feature \x1es to be an indicator function. For ex-
ample, one such feature might be
</bodyText>
<equation confidence="0.946449444444444">
\x1e1000(h;t) =
8
&gt;
&lt;
&gt;
:
1 if current word wi is the
and t = DT
0 otherwise
</equation>
<bodyText confidence="0.999069136363636">
Similar features might be de\x0cned for every
word/tag pair seen in training data. Another
\x0cfeature type might track trigramsof tags, for ex-
ample \x1e1001(h;t) = 1 if ht 2;t 1;ti = hD, N, Vi
and 0 otherwise. Similar features would be de-
\x0cned for all trigrams of tags seen in training. A
real advantage of these models comes from the
freedom in de\x0cning these features: for example,
(Ratnaparkhi 96; McCallum et al. 2000) both
describe feature sets which would be dicult to
incorporate in a generative model.
In addition to feature vector representations
of history/tag pairs, we will \x0cnd it convenient
to de\x0cne feature vectors of (w[1:n];t[1:n]) pairs
where w[1:n] is a sequence of n words, and t[1:n]
is an entire tag sequence. We use \x08 to de-
note a function from (w[1:n];t[1:n]) pairs to d-
dimensional feature vectors. We will often refer
to \x08 as a \\global&quot; representation, in contrast
to \x1e as a \\local&quot; representation. The particular
global representations considered in this paper
are simple functions of local representations:
</bodyText>
<equation confidence="0.9999026">
\x08s(w[1:n];t[1:n]) =
n
X
i=1
\x1es(hi;ti) (1)
</equation>
<bodyText confidence="0.996451111111111">
where hi = hti 1;ti 2;w[1:n];ii. Each global
feature \x08s(w[1:n];t[1:n]) is simply the value for
the local representation \x1es summed over all his-
tory/tag pairs in (w[1:n];t[1:n]). If the local fea-
turesareindicatorfunctions,thentheglobalfea-
tures will typically be \\counts&quot;. For example,
with \x1e1000 de\x0cned as above, \x081000(w[1:n];t[1:n])
is the number of times the is seen tagged as DT
in the pair of sequences (w[1:n];t[1:n]).
</bodyText>
<subsectionHeader confidence="0.986077">
2.3 Maximum-Entropy Taggers
</subsectionHeader>
<bodyText confidence="0.9056326">
In maximum-entropy taggers the feature vectors
\x1e together with a parameter vector \x16
\x0b 2 Rd are
used to de\x0cne a conditional probability distri-
bution over tags given a history as
</bodyText>
<equation confidence="0.992060916666667">
P(t j h; \x16
\x0b) = e
P
s
\x0bs\x1es(h;t)
Z(h; \x16
\x0b)
where Z(h; \x16
\x0b) = P
l2T e
P
s
</equation>
<bodyText confidence="0.7684515">
\x0bs\x1es(h;l). The log of
this probability has the form logp(t j h; \x16
</bodyText>
<equation confidence="0.998856666666667">
\x0b) =
Pd
s=1 \x0bs\x1es(h;t) logZ(h; \x16
</equation>
<bodyText confidence="0.8854185">
\x0b), and hence the log
probability for a (w[1:n];t[1:n]) pair will be
</bodyText>
<equation confidence="0.9989976">
X
i
d
X
s=1
\x0bs\x1es(hi;ti)
X
i
logZ(hi; \x16
\x0b) (2)
</equation>
<bodyText confidence="0.992457066666666">
where hi = hti 1;ti 2;w[1:n];ii. Given parame-
ter values \x16
\x0b, and an input sentence w[1:n], the
highest probability tagged sequence under the
formula in Eq. 2 can be found eciently using
the Viterbi algorithm.
The parameter vector \x16
\x0b is estimated from a
training set of sentence/tagged-sequence pairs.
Maximum-likelihood parameter values can be
estimated using Generalized Iterative Scaling
(Ratnaparkhi 96), or gradient descent methods.
In some cases it may be preferable to apply a
bayesian approach which includes a prior over
parameter values.
</bodyText>
<subsectionHeader confidence="0.989993">
2.4 A New Estimation Method
</subsectionHeader>
<bodyText confidence="0.9992608">
We now describe an alternative method for es-
timating parameters of the model. Given a se-
quence of words w[1:n] and a sequence of part of
speech tags, t[1:n], we will take the \\score&quot; of a
tagged sequence to be
</bodyText>
<equation confidence="0.995149909090909">
n
X
i=1
d
X
s=1
\x0bs\x1es(hi;ti) =
d
X
s=1
\x0bs\x08s(w[1:n];t[1:n]) :
</equation>
<bodyText confidence="0.992845461538462">
where hi is again hti 1;ti 2;w[1:n];ii. Note that
this is almost identical to Eq. 2, but without the
local normalization terms logZ(hi; \x16
\x0b). Under
this method for assigning scores to tagged se-
quences, the highest scoring sequence of tags for
an inputsentence can befoundusingthe Viterbi
algorithm. (We can use an almost identical de-
coding algorithm to that for maximum-entropy
taggers, the di\x0berence being that local normal-
ization terms do not need to be calculated.)
We then propose the trainingalgorithm in\x0cg-
ure 1. The algorithm takes T passes over the
training sample. All parameters are initially set
to be zero. Each sentence in turn is decoded us-
ing the current parameter settings. If the high-
est scoring sequence under the current model is
not correct, the parameters \x0bs are updated in a
simple additive fashion.
Note that if the local features \x1es are indica-
tor functions, then the global features \x08s willbe
counts. In this case the update will add cs ds
to each parameter \x0bs, where cs is the number
of times the s\&apos;th feature occurred in the cor-
rect tag sequence, and ds is the number of times
\x0cInputs: A training set of tagged sentences,
</bodyText>
<equation confidence="0.857883">
(wi
[1:ni];ti
</equation>
<bodyText confidence="0.809697181818182">
[1:ni]) for i = 1:::n. A parameter T
specifying number of iterations over the training set. A
\\local representation&quot; \x1e which is a function that maps
history/tag pairs to d-dimensional feature vectors. The
global representation \x08 is de\x0cned through \x1e as in Eq. 1.
Initialization: Set parameter vector \x16
\x0b = 0.
Algorithm:
For t = 1:::T;i = 1:::n
\x0f Use the Viterbi algorithm to \x0cnd the output of the
model on the i\&apos;th training sentence with the current pa-
</bodyText>
<equation confidence="0.9045692">
rameter settings, i.e.,
z[1:ni] = argmaxu[1:ni]2Tni
P
s \x0bs\x08s(wi
[1:ni];u[1:ni])
</equation>
<bodyText confidence="0.604724">
where T ni is the set of all tag sequences of length ni.
</bodyText>
<figure confidence="0.42842025">
\x0f If z[1:ni] 6= ti
[1:ni] then update the parameters
\x0bs = \x0bs +\x08s(wi
[1:ni];ti
[1:ni]) \x08s(wi
[1:ni];z[1:ni])
Output: Parameter vector \x16
\x0b.
</figure>
<figureCaption confidence="0.99972">
Figure 1: The training algorithm for tagging.
</figureCaption>
<bodyText confidence="0.9350958">
it occurs in highest scoring sequence under the
current model. For example, if the features \x1es
are indicator functions tracking all trigrams and
word/tag pairs, then the training algorithm is
identical to that given in section 2.1.
</bodyText>
<subsectionHeader confidence="0.987123">
2.5 Averaging Parameters
</subsectionHeader>
<bodyText confidence="0.99727">
There is a simple re\x0cnement to the algorithm
in \x0cgure 1, called the \\averaged parameters&quot;
method. De\x0cne \x0bt;i
s to be the value for the s\&apos;th
parameter after the i\&apos;th training example has
been processed in pass t over the training data.
Then the \\averaged parameters&quot; are de\x0cned as
</bodyText>
<equation confidence="0.97989">
s = P
t=1:::T;i=1:::n \x0bt;i
</equation>
<bodyText confidence="0.93618125">
s =nT for all s = 1:::d.
It is simple to modify the algorithm to store
this additional set of parameters. Experiments
in section 4 show that the averaged parameters
perform signi\x0ccantly better than the \x0cnal pa-
rameters \x0bT;n
s . The theory in the next section
gives justi\x0ccation for the averaging method.
</bodyText>
<sectionHeader confidence="0.82075" genericHeader="method">
3 Theory Justifying the Algorithm
</sectionHeader>
<bodyText confidence="0.999041888888889">
In this section we give a general algorithm for
problems such as tagging and parsing, and give
theorems justifyingthe algorithm. We alsoshow
how the tagging algorithm in \x0cgure 1 is a spe-
cial case of this algorithm. Convergence theo-
rems for the perceptron applied to classi\x0ccation
problems appear in (Freund &amp; Schapire 99) {
the results in this section, and the proofs in sec-
tion 5, show how the classi\x0ccationresults can be
</bodyText>
<table confidence="0.575582416666667">
Inputs: Training examples (xi;yi)
Initialization: Set \x16
\x0b = 0
Algorithm:
For t = 1:::T, i = 1:::n
Calculate zi = argmaxz2GEN(xi) \x08(xi;z)\x01 \x16
\x0b
If(zi 6= yi) then \x16
\x0b = \x16
\x0b +\x08(xi;yi) \x08(xi;zi)
Output: Parameters \x16
\x0b
</table>
<figureCaption confidence="0.980402">
Figure 2: A variant of the perceptron algorithm.
</figureCaption>
<bodyText confidence="0.9454204">
carried over to problems such as tagging.
The task is to learn a mapping from inputs
x 2 X to outputs y 2 Y. For example, X might
be a set of sentences, with Y being a set of pos-
sible tag sequences. We assume:
</bodyText>
<table confidence="0.7252106">
\x0f Training examples (xi;yi) for i = 1:::n.
\x0f A function GEN which enumerates a set of
candidates GEN(x) for an input x.
\x0f Arepresentation \x08mappingeach(x;y) 2
X \x02Y to a feature vector \x08(x;y) 2 Rd.
\x0f A parameter vector \x16
\x0b 2 Rd.
ThecomponentsGEN;\x08and \x16
\x0b de\x0cneamap-
ping from an input x to an output F(x) through
</table>
<equation confidence="0.961728571428572">
F(x) = arg max
y2GEN(x)
\x08(x;y)\x01 \x16
\x0b
where \x08(x;y) \x01 \x16
\x0b is the inner product
P
</equation>
<bodyText confidence="0.989558">
s \x0bs\x08s(x;y). The learning task is to set the
parameter values \x16
\x0b using the training examples
as evidence.
The tagging problem in section 2 can be
mapped to this setting as follows:
</bodyText>
<equation confidence="0.895653">
\x0f Thetrainingexamplesaresentence/tagged-
sequence pairs: xi = wi
[1:ni] and yi = ti
[1:ni]
for i = 1:::n.
\x0f Given a set of possible tags T , we de\x0cne
GEN(w[1:n]) = T n, i.e., the function GEN
</equation>
<bodyText confidence="0.4341694">
maps an input sentence w[1:n] to the set of
all tag sequences of length n.
\x0f The representation \x08(x;y) =
\x08(w[1:n];t[1:n]) is de\x0cned through local
feature vectors \x1e(h;t) where (h;t) is a
</bodyText>
<figureCaption confidence="0.84271925">
history/tag pair. (See Eq. 1.)
Figure 2 shows an algorithm for setting the
weights \x16
\x0b. It can be veri\x0ced that the training
</figureCaption>
<bodyText confidence="0.984193769230769">
\x0calgorithm for taggers in \x0cgure 1 is a special case
of this algorithm, if we de\x0cne (xi;yi);GEN and
\x08 as just described.
We will now give a \x0crst theorem regarding
the convergence of this algorithm. This theorem
therefore also describes conditions under which
the algorithm in \x0cgure 1 converges. First, we
need the following de\x0cnition:
De\x0cnition 1 Let GEN(xi) = GEN(xi) fyig. In
other words GEN(xi) is the set of incorrect candidates
for an example xi. We will say that a training sequence
(xi;yi) for i = 1:::n is separable with margin &gt; 0
if there exists some vector U with jjUjj = 1 such that
</bodyText>
<equation confidence="0.9057488">
8i;8z 2 GEN(xi); U \x01\x08(xi;yi) U \x01\x08(xi;z) \x15 (3)
(jjUjj is the 2-norm of U, i.e., jjUjj =
pP
s U2
s.)
</equation>
<bodyText confidence="0.933727166666667">
We can then state the following theorem (see
section 5 for a proof):
Theorem 1 For any training sequence (xi;yi) which is
separable with margin , then for the perceptron algorithm
in \x0cgure 2
Number of mistakes \x14 R2
</bodyText>
<page confidence="0.903172">
2
</page>
<bodyText confidence="0.98821462962963">
where R is a constant such that 8i;8z 2
GEN(xi) jj\x08(xi;yi) \x08(xi;z)jj \x14 R.
This theorem implies that if there is a param-
eter vector U which makes zero errors on the
training set, then after a \x0cnite number of itera-
tions the training algorithm will have converged
to parameter values with zero training error. A
crucialpointisthatthenumberofmistakesisin-
dependent of the number of candidates for each
example (i.e. the size of GEN(xi) for each i),
dependingonly on the separation of the training
data, where separation is de\x0cned above. This
is important because in many NLP problems
GEN(x) can be exponential in the size of the
inputs. All of the convergence and generaliza-
tion results in this paper depend on notions of
separability rather than the size of GEN.
Two questions come to mind. First, are there
guarantees for the algorithm if the training data
is not separable? Second, performance on a
training sample is all very well, but what does
this guarantee about how well the algorithm
generalizes to newly drawn test examples? (Fre-
und &amp; Schapire 99) discuss how the theory can
beextendedtodealwithbothofthesequestions.
The next sections describe how these results can
be applied to the algorithms in this paper.
</bodyText>
<subsectionHeader confidence="0.963873">
3.1 Theory for inseparable data
</subsectionHeader>
<bodyText confidence="0.88886425">
Inthissection we give boundswhichapplywhen
the data is not separable. First, we need the
following de\x0cnition:
De\x0cnition 2 Given a sequence (xi;yi), for a U, pair
</bodyText>
<equation confidence="0.9968652">
de\x0cne mi = U\x01\x08(xi;yi) maxz2GEN(xi) U\x01\x08(xi;z) and
\x0fi = maxf0; mig. Finally, de\x0cne DU; =
pPn
i=1 \x0f2
i .
</equation>
<bodyText confidence="0.97014425">
The value DU; is a measure of how close U
is to separating the training data with margin .
DU; is 0 if the vector U separates the data with
at least margin . If U separates almost all of
the examples with margin , but a few examples
are incorrectly tagged or have margin less than
, then DU; will take a relatively small value.
The following theorem then applies (see sec-
tion 5 for a proof):
Theorem 2 For any training sequence (xi;yi), for the
\x0crst pass over the training set of the perceptron algorithm
in \x0cgure 2,
</bodyText>
<figure confidence="0.457837">
Number of mistakes \x14 min
U;
(R +DU;)2
</figure>
<page confidence="0.596797">
2
</page>
<bodyText confidence="0.953528181818182">
where R is a constant such that 8i;8z 2
GEN(xi) jj\x08(xi;yi) \x08(xi;z)jj \x14 R, and the
min is taken over &gt; 0, jjUjj = 1.
This theorem implies that if the training data
is \\close&quot; to being separable with margin {
i.e., there exists some U such that DU; is rela-
tivelysmall{thenthealgorithmwillagainmake
a small number of mistakes. Thus theorem 2
shows that the perceptron algorithm can be ro-
bust to some training data examples being dif-
\x0ccult or impossible to tag correctly.
</bodyText>
<subsectionHeader confidence="0.998362">
3.2 Generalization results
</subsectionHeader>
<bodyText confidence="0.93019605">
Theorems 1 and 2 give results bounding the
number of errors on training samples, but the
question we are really interested in concerns
guarantees of how well the method generalizes
to new test examples. Fortunately, there are
several theoretical results suggesting that if the
perceptron algorithm makes a relatively small
number of mistakes on a training sample then it
islikelyto generalize wellto newexamples. This
section describes some of these results, which
originally appeared in (Freund &amp; Schapire 99),
and are derived directly from results in (Helm-
bold and Warmuth 95).
\x0cFirst we de\x0cne a modi\x0ccation of the percep-
tron algorithm, the voted perceptron. We can
consider the \x0crst pass of the perceptron algo-
rithm to build a sequence of parameter set-
tings \x16
\x0b1;i for i = 1:::n. For a given test ex-
ample x, each of these will de\x0cne an output
</bodyText>
<equation confidence="0.949049">
vi = argmaxz2GEN(x) \x16
\x0b1;i \x01 \x08(x;z). The voted
</equation>
<bodyText confidence="0.979123346153846">
perceptron takes the most frequently occurring
output in the set fv1 :::vng. Thus the voted
perceptron is a method where each of the pa-
rameter settings \x16
\x0b1;i for i = 1:::n get a sin-
gle vote for the output, and the majority wins.
The averaged algorithm in section 2.5 can be
considered to be an approximation of the voted
method, withthe advantage that a single decod-
ing with the averaged parameters can be per-
formed, rather than n decodings with each of
the n parameter settings.
In analyzing the voted perceptron the one as-
sumption we will make is that there is some
unknown distribution P(x;y) over the set X \x02
Y, and that both training and test examples
are drawn independently, identically distributed
(i.i.d.) from this distribution. Corollary 1 of
(Freund &amp; Schapire 99) then states:
Theorem 3 (Freund &amp; Schapire 99) Assume all ex-
amples are generated i.i.d. at random. Let
h(x1;y1)i:::(xn;yn)i be a sequence of training examples
and let (xn+1;yn+1) be a test example. Then the prob-
ability (over the choice of all n + 1 examples) that the
voted-perceptron algorithm does not predict yn+1 on in-
put xn+1 is at most
</bodyText>
<figure confidence="0.8337285">
2
n +1En+1
\x14
min
U;
(R +DU;)2
2
\x15
</figure>
<bodyText confidence="0.670289">
where En+1[] is an expected value taken over n + 1 ex-
amples, R and DU; are as de\x0cned above, and the min is
taken over &gt; 0, jjUjj = 1.
</bodyText>
<sectionHeader confidence="0.996272" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.992177">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.9884664">
We ran experiments on two data sets: part-of-
speech tagging on the Penn Wall Street Journal
treebank (Marcus et al. 93), and base noun-
phrase recognition on the data sets originallyin-
troduced by (Ramshaw and Marcus 95). In each
casewehadatraining,developmentandtestset.
For part-of-speech tagging the training set was
sections 0{18 of the treebank, the development
set was sections 19{21 and the \x0cnal test set was
sections 22-24. In NP chunking the training set
</bodyText>
<table confidence="0.949866476190476">
Current word wi &amp; ti
Previous word wi 1 &amp; ti
Word two back wi 2 &amp; ti
Next word wi+1 &amp; ti
Word two ahead wi+2 &amp; ti
Bigram features wi 2; wi 1 &amp; ti
wi 1; wi &amp; ti
wi; wi+1 &amp; ti
wi+1; wi+2 &amp; ti
Current tag pi &amp; ti
Previous tag pi 1 &amp; ti
Tag two back pi 2 &amp; ti
Next tag pi+1 &amp; ti
Tag two ahead pi+2 &amp; ti
Bigram tag features pi 2; pi 1 &amp; ti
pi 1; pi &amp; ti
pi; pi+1 &amp; ti
pi+1; pi+2 &amp; ti
Trigram tag features pi 2; pi 1; pi &amp; ti
pi 1; pi; pi+1 &amp; ti
pi; pi+1; pi+2 &amp; ti
</table>
<figureCaption confidence="0.994862">
Figure 3: Feature templates used in the NP chunking
</figureCaption>
<bodyText confidence="0.9882961">
experiments. wi is the current word, and w1 :::wn is the
entire sentence. pi is POS tag for the current word, and
p1 :::pn is the POS sequence for the sentence. ti is the
chunking tag assigned to the i\&apos;th word.
was taken from section 15{18, the development
set was section 21, and the test set was section
20. For POS tagging we report the percentage
of correct tags on a test set. For chunking we
report F-measure in recovering bracketings cor-
responding to base NP chunks.
</bodyText>
<subsectionHeader confidence="0.783717">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.996008090909091">
For POS tagging we used identical features to
those of (Ratnaparkhi 96), the only di\x0berence
being that we did not make the rare word dis-
tinction in table 1 of (Ratnaparkhi 96) (i.e.,
spelling features were included for all words in
training data, and the word itself was used as a
featureregardlessofwhetherthewordwasrare).
The feature set takes into account the previous
tag and previous pairs of tags in the history, as
well as the word being tagged, spelling features
of the words being tagged, and various features
of the words surroundingthe word beingtagged.
In the chunking experiments the input \\sen-
tences&quot;includedwordsaswellasparts-of-speech
forthosewordsfromthetagger in(Brill95). Ta-
ble3 showsthefeatures usedinthe experiments.
The chunking problem is represented as a three-
tag task, where the tags are B, I, O for words
beginning a chunk, continuing a chunk, and be-
ing outside a chunk respectively. All chunks be-
gin with a B symbol, regardless of whether the
previous word is tagged O or I.
</bodyText>
<table confidence="0.9828804375">
\x0cNP Chunking Results
Method F-Measure Numits
Perc, avg, cc=0 93.53 13
Perc, noavg, cc=0 93.04 35
Perc, avg, cc=5 93.33 9
Perc, noavg, cc=5 91.88 39
ME, cc=0 92.34 900
ME, cc=5 92.65 200
POS Tagging Results
Method Error rate/% Numits
Perc, avg, cc=0 2.93 10
Perc, noavg, cc=0 3.68 20
Perc, avg, cc=5 3.03 6
Perc, noavg, cc=5 4.04 17
ME, cc=0 3.4 100
ME, cc=5 3.28 200
</table>
<figureCaption confidence="0.996314">
Figure 4: Results for various methods on the part-of-
</figureCaption>
<bodyText confidence="0.979786875">
speech tagging and chunking tasks on development data.
All scores are error percentages. Numits is the number
of training iterations at which the best score is achieved.
Perc is the perceptron algorithm, ME is the maximum
entropy method. Avg/noavg is the perceptron with or
without averaged parameter vectors. cc=5 means only
features occurring 5 times or more in training are in-
cluded, cc=0 means all features in training are included.
</bodyText>
<subsectionHeader confidence="0.90724">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.996175320754717">
We applied both maximum-entropy models and
the perceptron algorithm to the two tagging
problems. We tested several variants for each
algorithm on the development set, to gain some
understanding of how the algorithms\&apos; perfor-
mance varied with various parameter settings,
and to allow optimization of free parameters so
that the comparison on the \x0cnal test set is a fair
one. For both methods, we tried the algorithms
with feature count cut-o\x0bs set at 0 and 5 (i.e.,
we ran experiments with all features in training
data included, or with all features occurring 5
times or more included { (Ratnaparkhi 96) uses
a count cut-o\x0b of 5). In the perceptron algo-
rithm, the numberof iterations T over the train-
ing set was varied, and the method was tested
with both averaged and unaveraged parameter
vectors (i.e., with \x0bT;n
s and
T;n
s , as de\x0cned in
section 2.5, for a variety of values for T). In
the maximum entropy model the number of it-
erations of training using Generalized Iterative
Scaling was varied.
Figure 4 shows results on development data
on the two tasks. The trends are fairly clear:
averaging improves results signi\x0ccantly for the
perceptron method, as does including all fea-
tures rather than imposing a count cut-o\x0b of 5.
In contrast, the ME models\&apos; performance su\x0bers
when all features are included. The best percep-
tron con\x0cguration gives improvements over the
maximum-entropy models in both cases: an im-
provement in F-measure from 92:65% to 93:53%
in chunking, and a reduction from 3:28% to
2:93% error rate in POS tagging. In looking
at the results for di\x0berent numbers of iterations
on development data we found that averaging
not only improves the best result, but also gives
much greater stability of the tagger (the non-
averaged variant has much greater variance in
its scores).
As a \x0cnal test, the perceptron and ME tag-
gers were applied to the test sets, with the op-
timal parameter settings on development data.
On POS tagging the perceptron algorithm gave
2.89% error compared to 3.28% error for the
maximum-entropy model (a 11.9% relative re-
duction in error). In NP chunking the percep-
tronalgorithmachieves anF-measureof93.63%,
in contrast to an F-measure of 93.29% for the
ME model (a 5.1% relative reduction in error).
</bodyText>
<sectionHeader confidence="0.602756" genericHeader="method">
5 Proofs of the Theorems
</sectionHeader>
<bodyText confidence="0.965073">
This section gives proofs of theorems 1 and 2.
The proofs are adapted from proofs for the clas-
</bodyText>
<table confidence="0.780118147058824">
si\x0ccation case in (Freund &amp; Schapire 99).
Proof of Theorem 1: Let \x16
\x0bk be the weights
before the k\&apos;th mistake is made. It follows that
\x16
\x0b1 = 0. Suppose the k\&apos;th mistake is made at
the i\&apos;th example. Take z to the output proposed
at this example, z = argmaxy2GEN(xi) \x08(xi;y)\x01
\x16
\x0bk. It follows from the algorithm updates that
\x16
\x0bk+1 = \x16
\x0bk +\x08(xi;yi) \x08(xi;z). We take inner
products of both sides with the vector U:
U \x01 \x16
\x0bk+1 = U \x01 \x16
\x0bk
+U \x01\x08(xi;yi) U \x01\x08(xi;z)
\x15 U \x01 \x16
\x0bk
+
wheretheinequalityfollowsbecauseofthe prop-
erty of U assumed in Eq. 3. Because \x16
\x0b1 = 0,
and therefore U \x01 \x16
\x0b1 = 0, it follows by induc-
tion on k that for all k, U \x01 \x16
\x0bk+1 \x15 k. Be-
cause U \x01 \x16
\x0bk+1 \x14 jjUjj jj\x16
\x0bk+1jj, it follows that
jj\x16
\x0bk+1jj \x15 k.
We also derive an upper bound for jj\x16
</table>
<figure confidence="0.891603210526316">
\x0bk+1jj2:
jj\x16
\x0bk+1jj2 = jj\x16
\x0bk
jj2 +jj\x08(xi;yi) \x08(xi;z)jj2
+2\x16
\x0bk
\x01(\x08(xi;yi) \x08(xi;z))
\x14 jj\x16
\x0bk
jj2 +R2
\x0cwhere the inequality follows because
jj\x08(xi;yi) \x08(xi;z)jj2 \x14 R2 by assump-
tion, and \x16
\x0bk \x01 (\x08(xi;yi) \x08(xi;z)) \x14 0 because
z is the highest scoring candidate for xi under
the parameters \x16
\x0bk. It follows by induction that
jj\x16
</figure>
<table confidence="0.841633833333333">
\x0bk+1jj2 \x14 kR2.
Combining the bounds jj\x16
\x0bk+1jj \x15 k and
jj\x16
\x0bk+1jj2 \x14 kR2 gives the result for all k that
k22 \x14 jj\x16
\x0bk+1jj2 \x14 kR2 ) k \x14 R2=2
Proof of Theorem 2: We transformtherep-
resentation \x08(x;y) 2 Rd to a new representation
\x16
\x08(x;y) 2 Rd+n as follows. For i = 1:::d de-
\x0cne \x16
</table>
<equation confidence="0.878534333333333">
\x08i(x;y) = \x08i(x;y). For j = 1:::n de\x0cne
\x16
\x08d+j(x;y) = \x01 if (x;y) = (xj;yj), 0 otherwise,
</equation>
<bodyText confidence="0.987182666666667">
where \x01 is a parameter which is greater than 0.
Similary, say we are given a U; pair, and cor-
responding values for \x0fi as de\x0cned above. We
</bodyText>
<figure confidence="0.959284647058824">
de\x0cne a modi\x0ced parameter vector \x16
U 2 Rd+n
with \x16
Ui = Ui for i = 1:::d and \x16
Ud+j = \x0fj=\x01
for j = 1:::n. Under these de\x0cnitions it can be
veri\x0ced that
8i;8z 2 GEN(xi); \x16
U \x01 \x16
\x08(xi;yi) \x16
U \x01 \x16
\x08(xi;z) \x15
8i;8z 2 GEN(xi); jj\x16
\x08(xi;yi) \x16
\x08(xi;z)jj2 \x14 R2 +\x012
jj\x16
Ujj2 = jjUjj2 +P
i \x0f2
i =\x012 = 1+D2
U;=\x012
It can be seen that the vector \x16
U=jj\x16
Ujj separates
the data withmargin=
q
1+D2
U;=\x012. By the-
orem 1, this means that the \x0crst pass of the per-
ceptron algorithm with representation \x16
\x08 makes
at most kmax(\x01) = 1
2 (R2 +\x012)(1+ D2
U;
\x012 ) mis-
</figure>
<bodyText confidence="0.924420714285714">
takes. But the \x0crst pass of the original algo-
rithm with representation \x08 is identical to the
\x0crst pass of the algorithm with representation
\x16
\x08, because the parameter weights for the addi-
tional features \x16
\x08d+j for j = 1:::n each a\x0bect a
singleexampleoftrainingdata, anddonota\x0bect
the classi\x0ccation of test data examples. Thus
the original perceptron algorithm also makes at
most kmax(\x01) mistakes on its \x0crst pass over the
trainingdata. Finally,wecanminimizekmax(\x01)
with respect to \x01, giving \x01 = p
RDU;, and
</bodyText>
<equation confidence="0.3137755">
kmax(p
RDU;) = (R2 +D2
</equation>
<bodyText confidence="0.6889065">
U;)=2, implying the
bound in the theorem.
</bodyText>
<sectionHeader confidence="0.995916" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9980755">
We have described new algorithms for tagging,
whose performance guarantees depend on a no-
tion of \\separability&quot; of training data exam-
ples. The generic algorithm in \x0cgure 2, and
the theorems describing its convergence prop-
erties, could be applied to several other models
in the NLP literature. For example, a weighted
context-free grammar can also be conceptual-
ized as a way of de\x0cning GEN, \x08 and \x16
\x0b, so the
weights for generative models such as PCFGs
could be trained using this method.
</bodyText>
<sectionHeader confidence="0.963563" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.978283666666667">
Thanksto NigelDu\x0by, Rob SchapireandYoram
Singer for many useful discussions regarding
the algorithms in this paper, and to Fernando
Pereira for pointers to the NP chunking data
set, and for suggestions regarding the features
used in the experiments.
</bodyText>
<sectionHeader confidence="0.992152" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9965235">
Brill, E. (1995). Transformation-Based Error-Driven
Learning and Natural Language Processing: A Case
Study in Part of Speech Tagging. Computational Lin-
guistics.
Collins, M., and Du\x0by, N. (2001). Convolution Kernels
for Natural Language. In Proceedings of Neural Infor-
mation Processing Systems (NIPS 14).
Collins, M., and Du\x0by, N. (2002). New Ranking Algo-
rithms for Parsing and Tagging: Kernels over Discrete
Structures, and the Voted Perceptron. In Proceedings
of ACL 2002.
Collins, M. (2002). Ranking Algorithms for Named{
Entity Extraction: Boosting and the Voted Percep-
tron. In Proceedings of ACL 2002.
Freund, Y. &amp; Schapire, R. (1999). Large Margin Classi-
\x0ccation using the Perceptron Algorithm. In Machine
Learning, 37(3):277{296.
Helmbold,D., andWarmuth,M. Onweaklearning. Jour-
nal of Computer and System Sciences, 50(3):551-573,
June 1995.
La\x0berty, J., McCallum, A., and Pereira, F. (2001). Con-
ditional random \x0celds: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
ICML 2001.
McCallum, A., Freitag, D., and Pereira, F. (2000) Max-
imum entropy markov models for information extrac-
tion and segmentation. In Proceedings of ICML 2000.
Marcus, M., Santorini, B., &amp; Marcinkiewicz, M. (1993).
Building a large annotated corpus of english: The
Penn treebank. Computational Linguistics, 19.
Ramshaw, L., and Marcus, M. P. (1995). Text Chunking
Using Transformation-Based Learning. In Proceedings
of the Third ACL Workshop on Very Large Corpora,
Association for Computational Linguistics, 1995.
Ratnaparkhi, A. (1996). A maximum entropy part-of-
speech tagger. In Proceedings of the empirical methods
in natural language processing conference.
Rosenblatt, F. 1958. The Perceptron: A Probabilistic
ModelforInformationStorageandOrganizationinthe
Brain. Psychological Review, 65, 386{408. (Reprinted
in Neurocomputing (MIT Press, 1998).)
\x0c&apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.949635">
<title confidence="0.9996015">b&apos;Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</title>
<author confidence="0.999854">Michael Collins</author>
<affiliation confidence="0.995964">AT&amp;T Labs-Research, Florham Park, New Jersey.</affiliation>
<email confidence="0.999891">mcollins@research.att.com</email>
<abstract confidence="0.996416">We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random \x0celds (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modi\x0ccation of the proof of convergence of the perceptron algorithm for classi\x0ccation problems. We give experimental results on part-of-speech taggingandbasenounphrasechunking,in both cases showing improvements over results for a maximum-entropy tagger.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging. Computational Linguistics.</title>
<date>1995</date>
<marker>Brill, 1995</marker>
<rawString>Brill, E. (1995). Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Du\x0by</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>In Proceedings of Neural Information Processing Systems (NIPS 14).</booktitle>
<marker>Collins, Du\x0by, 2001</marker>
<rawString>Collins, M., and Du\x0by, N. (2001). Convolution Kernels for Natural Language. In Proceedings of Neural Information Processing Systems (NIPS 14).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Du\x0by</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Collins, Du\x0by, 2002</marker>
<rawString>Collins, M., and Du\x0by, N. (2002). New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. In Proceedings of ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Ranking Algorithms for Named{ Entity Extraction: Boosting and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="3121" citStr="Collins 2002" startWordPosition="445" endWordPosition="446">ch tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger (a 11.9% relative reduction in error for POS tagging, a 5.1% relative reduction in error for NP chunking). Although we concentrate on tagging problems in this paper, the theoretical framework and algorithm described in section 3 of this paper should be applicable to a wide variety of models where Viterbi-style algorithms can be used for decoding: examples are Probabilistic Context-Free Grammars, or ME models for parsing. See (Collinsand Du\x0by 2001; Collins and Du\x0by 2002; Collins 2002) for other applications of the voted perceptronto NLP problems.1 2 Parameter Estimation 2.1 HMM Taggers In this section, as a motivating example, we describe a special case of the algorithm in this paper: the algorithm applied to a trigram tagger. In a trigram HMM tagger, each trigram 1The theorems in section 3, and the proofs in section 5, apply directly to the work in these other papers. Association for Computational Linguistics. Language Processing (EMNLP), Philadelphia, July 2002, pp. 1-8. Proceedings of the Conference on Empirical Methods in Natural \x0cof tags and each tag/word pair have</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, M. (2002). Ranking Algorithms for Named{ Entity Extraction: Boosting and the Voted Perceptron. In Proceedings of ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>Large Margin Classi\x0ccation using the Perceptron Algorithm.</title>
<date>1999</date>
<booktitle>In Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<marker>Freund, Schapire, 1999</marker>
<rawString>Freund, Y. &amp; Schapire, R. (1999). Large Margin Classi\x0ccation using the Perceptron Algorithm. In Machine Learning, 37(3):277{296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Helmbold</author>
<author>M Onweaklearning andWarmuth</author>
</authors>
<date>1995</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>50--3</pages>
<marker>Helmbold, andWarmuth, 1995</marker>
<rawString>Helmbold,D., andWarmuth,M. Onweaklearning. Journal of Computer and System Sciences, 50(3):551-573, June 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J La\x0berty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random \x0celds: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML</booktitle>
<marker>La\x0berty, McCallum, Pereira, 2001</marker>
<rawString>La\x0berty, J., McCallum, A., and Pereira, F. (2001). Conditional random \x0celds: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="998" citStr="McCallum et al. 2000" startWordPosition="132" endWordPosition="135">decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modi\x0ccation of the proof of convergence of the perceptron algorithm for classi\x0ccation problems. We give experimental results on part-of-speech taggingandbasenounphrasechunking,in both cases showing improvements over results for a maximum-entropy tagger. 1 Introduction Maximum-entropy (ME) models are justi\x0cably a very popular choice for tagging problems in Natural Language Processing: for example see (Ratnaparkhi 96) for their use on part-of-speech tagging, and (McCallum et al. 2000) for their use on a FAQ segmentation task. ME models have the advantage of being quite exible in the features that can be incorporated in the model. However, recenttheoreticalandexperimentalresults in (La\x0berty et al. 2001) have highlighted problemswiththeparameterestimationmethod for ME models. In response to these problems, they describe alternative parameter estimation methods based on Conditional Markov Random Fields(CRFs). (La\x0berty et al. 2001) give experimental results suggesting that CRFs can perform signi\x0ccantly better than ME models. In this paper we describe parameter estimat</context>
<context position="6723" citStr="McCallum et al. 2000" startWordPosition="1028" endWordPosition="1031">h were \\missing&quot; from the proposed sequence z[1:ni], and downweighting parameter values for \\incorrect&quot; features in the sequence z[1:ni]. Note that if z[1:ni] = ti [1:ni] |i.e., the proposed tag sequence is correct |no changes are made to the parameter values. 2.2 Local and Global Feature Vectors Wenowdescribehowtogeneralizethealgorithm to more general representations of tagged sequences. In this section we describe the featurevector representations whichare commonlyused in maximum-entropy models for tagging, and which are also used in this paper. In maximum-entropy taggers (Ratnaparkhi 96; McCallum et al. 2000), the tagging problem is decomposed into sequence of decisions in tagging the problem in left-to-right fashion. At each point there is a \\history&quot; { the context in which a tagging decision is made { and the task is to predict the tag given the history. Formally, a history is a 4-tuple ht 1;t 2;w[1:n];ii where t 1;t 2 are the previoustwo tags, w[1:n] isan array specifyingthe n words in the inputsentence, and i is the index of the word being tagged. We use H to denote the set of all possible histories. Maximum-entropy models represent the taggingtaskthroughafeature-vector representation ofhisto</context>
<context position="8186" citStr="McCallum et al. 2000" startWordPosition="1284" endWordPosition="1287">g., see (Ratnaparkhi 96)) for each feature \x1es to be an indicator function. For example, one such feature might be \x1e1000(h;t) = 8 &gt; &lt; &gt; : 1 if current word wi is the and t = DT 0 otherwise Similar features might be de\x0cned for every word/tag pair seen in training data. Another \x0cfeature type might track trigramsof tags, for example \x1e1001(h;t) = 1 if ht 2;t 1;ti = hD, N, Vi and 0 otherwise. Similar features would be de\x0cned for all trigrams of tags seen in training. A real advantage of these models comes from the freedom in de\x0cning these features: for example, (Ratnaparkhi 96; McCallum et al. 2000) both describe feature sets which would be dicult to incorporate in a generative model. In addition to feature vector representations of history/tag pairs, we will \x0cnd it convenient to de\x0cne feature vectors of (w[1:n];t[1:n]) pairs where w[1:n] is a sequence of n words, and t[1:n] is an entire tag sequence. We use \x08 to denote a function from (w[1:n];t[1:n]) pairs to ddimensional feature vectors. We will often refer to \x08 as a \\global&quot; representation, in contrast to \x1e as a \\local&quot; representation. The particular global representations considered in this paper are simple functions</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>McCallum, A., Freitag, D., and Pereira, F. (2000) Maximum entropy markov models for information extraction and segmentation. In Proceedings of ICML 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M., Santorini, B., &amp; Marcinkiewicz, M. (1993). Building a large annotated corpus of english: The Penn treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text Chunking Using Transformation-Based Learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third ACL Workshop on Very Large Corpora, Association for Computational Linguistics,</booktitle>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Ramshaw, L., and Marcus, M. P. (1995). Text Chunking Using Transformation-Based Learning. In Proceedings of the Third ACL Workshop on Very Large Corpora, Association for Computational Linguistics, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-ofspeech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the empirical methods in natural language processing conference.</booktitle>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi, A. (1996). A maximum entropy part-ofspeech tagger. In Proceedings of the empirical methods in natural language processing conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rosenblatt</author>
</authors>
<title>The Perceptron: A Probabilistic ModelforInformationStorageandOrganizationinthe Brain.</title>
<date>1958</date>
<journal>Psychological Review,</journal>
<volume>65</volume>
<pages>386--408</pages>
<publisher>MIT Press,</publisher>
<note>Reprinted in Neurocomputing</note>
<marker>Rosenblatt, 1958</marker>
<rawString>Rosenblatt, F. 1958. The Perceptron: A Probabilistic ModelforInformationStorageandOrganizationinthe Brain. Psychological Review, 65, 386{408. (Reprinted in Neurocomputing (MIT Press, 1998).) \x0c&apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>