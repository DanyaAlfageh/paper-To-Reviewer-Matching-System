<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.453292">
b&amp;apos;Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 770778,
</bodyText>
<figure confidence="0.8705525">
Beijing, August 2010
Recognising Entailment within Discourse
Shachar Mirkin
, Jonathan Berant
, Ido Dagan
, Eyal Shnarch
</figure>
<affiliation confidence="0.5031145">
Computer Science Department, Bar-Ilan University
The Blavatnik School of Computer Science, Tel-Aviv University
</affiliation>
<sectionHeader confidence="0.947347" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999054647058824">
Texts are commonly interpreted based on
the entire discourse in which they are sit-
uated. Discourse processing has been
shown useful for inference-based applica-
tion; yet, most systems for textual entail-
ment a generic paradigm for applied in-
ference have only addressed discourse
considerations via off-the-shelf corefer-
ence resolvers. In this paper we explore
various discourse aspects in entailment in-
ference, suggest initial solutions for them
and investigate their impact on entailment
performance. Our experiments suggest
that discourse provides useful informa-
tion, which significantly improves entail-
ment inference, and should be better ad-
dressed by future entailment systems.
</bodyText>
<sectionHeader confidence="0.998155" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997893877192982">
This paper investigates the problem of recognising
textual entailment within discourse. Textual En-
tailment (TE) is a generic framework for applied
semantic inference (Dagan et al., 2009). Under
TE, the relationship between a text (T) and a tex-
tual assertion (hypothesis, H) is defined such that
T entails H if humans reading T would infer that
H is most likely true (Dagan et al., 2006).
TE has been successfully applied to a variety of
natural language processing applications, includ-
ing information extraction (Romano et al., 2006)
and question answering (Harabagiu and Hickl,
2006). Yet, most entailment systems have thus
far paid little attention to discourse aspects of in-
ference. In part, this is the result of the unavail-
ability of adept tools for handling the kind of dis-
course processing required for inference. In addi-
tion in the main TE benchmarks, the Recognising
Textual Entailment (RTE) challenges, discourse
played little role. This state of affairs has started
to change with the recent introduction of the RTE
Pilot Search task (Bentivogli et al., 2009b), in
which assessed texts are situated within complete
documents. In this setting, texts need to be inter-
preted based on their entire discourse (Bentivogli
et al., 2009a), hence attending to discourse issues
becomes essential. Consider the following exam-
ple from the tasks dataset:
(T) The seven men on board were said to have
as little as 24 hours of air.
For the interpretation of T, e.g. the identity and
whereabouts of the seven men, one must consider
Ts discourse. The preceding sentence T, for in-
stance, provides useful information to that aim:
(T) The Russian navy worked desperately to
save a small military submarine.
This example demonstrates a common situation in
texts, and is also applicable to the RTE Search
tasks setting. Still, little was done by the tasks
participants to consider discourse, and sentences
were mostly processed independently.
Analyzing the Search tasks development set,
we identified several key discourse aspects that af-
fect entailment in a discourse-dependent setting.
First, we observed that the coverage of available
coreference resolution tools is considerably lim-
ited. To partly address this problem, we extend the
set of coreference relations to phrase pairs with
a certain degree of lexical overlap, as long as no
semantic incompatibility is found between them.
Second, many bridging relations (Clark, 1975) are
realized in the form of global information per-
ceived as known for entire documents. As bridg-
ing falls completely out of the scope of available
resolvers, we address this phenomenon by iden-
tifying and weighting prominent document terms
and allowing their incorporation in inference even
</bodyText>
<page confidence="0.976562">
770
</page>
<bodyText confidence="0.998893368421053">
\x0cwhen they are not explicitly mentioned in a sen-
tence. Finally, we observed a coherence-related
discourse phenomenon, namely inter-relations be-
tween entailing sentences in the discourse, such
as the tendency of entailing sentences to be ad-
jacent to one another. To that end, we apply a
two-phase classification scheme, where a second-
phase meta-classifier is applied, extracting dis-
course and document-level features based on the
classification of each sentence on its own.
Our results show that, even when simple so-
lutions are employed, the reliance on discourse-
based information is helpful and achieves a sig-
nificant improvement of results. We analyze the
contribution of each component and suggest some
future work to better attend to discourse in entail-
ment systems. To our knowledge, this is the most
extensive effort thus far to empirically explore the
effect of discourse on entailment systems.
</bodyText>
<sectionHeader confidence="0.994636" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999306136363637">
Discourse plays a key role in text understanding
applications such as question answering or infor-
mation extraction. Yet, such applications typically
only handle a narrow aspect of discourse, address-
ing coreference by term substitution (Dali et al.,
2009; Li et al., 2009). The limited coverage and
scope of existing tools for coreference resolution
and the unavailability of tools for addressing other
discourse aspects also contribute to this situation.
For instance, VP anaphora and bridging relations
are usually not handled at all by such resolvers. A
similar situation is seen in the TE research field.
The prominent benchmark for entailment sys-
tems evaluation is the series of RTE challenges.
The main task in these challenges has tradition-
ally been to determine, given a text-hypothesis
pair (T,H), whether T entails H. Discourse played
no role in the first two RTE challenges as
Ts were constructed of short simplified texts.
In RTE-3 (Giampiccolo et al., 2007), where
some paragraph-long texts were included, inter-
sentential relations became relevant for correct in-
ference. Yet the texts in the task were manually
modified to ensure they are self-contained. Con-
sequently, little effort was invested by the chal-
lenges participants to address discourse issues
beyond the standard substitution of coreferring
nominal phrases, using publicly available tools
such as JavaRap (Qiu et al., 2004) or OpenNLP1,
e.g. (Bar-Haim et al., 2008).
A major step in the RTE challenges towards a
more practical setting of text processing applica-
tions occurred with the introduction of the Search
task in the Fifth RTE challenge (RTE-5). In this
task entailing sentences are situated within doc-
uments and depend on other sentences for their
correct interpretation. Thus, discourse becomes
a substantial factor impacting inference. Surpris-
ingly, discourse hardly received any treatment in
this task beyond the standard use of coreference
resolution (Castillo, 2009; Litkowski, 2009), and
an attempt to address globally-known information
by removing from H words that appear in docu-
ment headlines (Clark and Harrison, 2009).
</bodyText>
<sectionHeader confidence="0.998412" genericHeader="method">
3 The RTE Search Task
</sectionHeader>
<bodyText confidence="0.998688">
The RTE-5 Search task was derived from the
TAC Summarization task2. The dataset consists
of several corpora, each comprised of news arti-
cles concerning a specific topic, such as the im-
pact of global warming on the Arctic or the Lon-
don terrorist attacks in 2005. Hypotheses were
manually generated based on Summary Content
Units (Nenkova et al., 2007), clause-long state-
ments taken from manual summaries of the cor-
pora. Texts are unmodified sentences in the arti-
cles. Given a topic and a hypothesis, entailment
systems are required to identify all sentences in
the topics corpus that entail the hypothesis.
Each sentence-hypothesis pair in both the de-
velopment and test sets was annotated, judging
whether the sentence entails the hypothesis. Out
of 20,104 annotations in the development set, only
810 were judged as positive. This small ratio (4%)
of positive examples, in comparison to 50% in tra-
ditional RTE tasks, better corresponds to the natu-
ral distribution of entailing texts in a corpus, thus
better simulates practical settings.
The task may seem as a variant of information
retrieval (IR), as it requires finding specific texts
in a corpus. Yet, it is fundamentally different from
IR for two reasons. First, the target output is a set
</bodyText>
<footnote confidence="0.6005145">
1
http://opennlp.sourceforge.net
2
http://www.nist.gov/tac/2009/Summarization/
</footnote>
<page confidence="0.998389">
771
</page>
<bodyText confidence="0.9996558125">
\x0cof sentences, each evaluated independently, rather
than a set of documents. Second, the decision cri-
terion is entailment rather than relevance.
Despite the above, apparently, IR techniques
provided hard-to-beat baselines for the RTE
Search task (MacKinlay and Baldwin, 2009), out-
performing every other system that relied on in-
ference without IR-based pre-filtering. At the cur-
rent state of performance of entailment systems, it
seems that lexical coverage largely overshadows
any other approach in this task. Still, most (6 out
of 8) participants in the challenge applied their en-
tailment systems to the entire dataset without a
prior retrieval of candidate sentences. F1 scores
for such systems vary between 10% and 33%, in
comparison to over 40% of the IR-based methods.
</bodyText>
<sectionHeader confidence="0.996705" genericHeader="method">
4 The Baseline RTE System
</sectionHeader>
<bodyText confidence="0.998160264705882">
In this work we used BIUTEE, Bar-Ilan Univer-
sity Textual Entailment Engine (Bar-Haim et al.,
2008; Bar-Haim et al., 2009), a state of the art
RTE system, as a baseline and as a basis for our
discourse-based enhancements. This section de-
scribes this systems architecture; the methods by
which it was augmented to address discourse are
presented in Section 5.
To determine entailment, BIUTEE performs the
following main steps:
Preprocessing First, all documents are parsed
and processed with standard tools for named en-
tity recognition (Finkel et al., 2005) and corefer-
ence resolution. For the latter purpose, we use
OpenNLP and enable the substitution of corefer-
ring terms. This is the only way by which BIUTEE
addresses discourse, representing the state of the
art in entailment systems.
Entailment-based transformations Given a
T-H pair (both represented as dependency
parse trees), the system applies a sequence of
knowledge-based entailment transformations over
T, generating a set of texts which are entailed by
it. The goal is to obtain consequent texts which
are more similar to H. Based on preliminary re-
sults on the development set, in our experiments
(Section 6) we use WordNet (Fellbaum, 1998) as
the systems only knowledge resource, using its
synonymy, hyponymy and derivation relations.
Classification A supervised classifier, trained
on the development set, is applied to determine
entailment of each pair based on a set of syntactic
and lexical syntactic features assessing the degree
by which T and its consequents cover H.
</bodyText>
<sectionHeader confidence="0.975358" genericHeader="method">
5 Addressing Discourse
</sectionHeader>
<bodyText confidence="0.999723142857143">
In the following subsections we describe the
prominent discourse phenomena that affect infer-
ence, which we have identified in an analysis of
the development set and addressed in our imple-
mentation. As mentioned, these phenomena are
poorly addressed by available reference resolvers
or fall completely out of their scope.
</bodyText>
<subsectionHeader confidence="0.971758">
5.1 Augmented coreference set
</subsectionHeader>
<bodyText confidence="0.998901285714286">
A large number of coreference relations are com-
prised of terms which share lexical elements, (e.g.
airlinerss first flight and Airbus A380s first
flight). Although common in coreference rela-
tions, standard resolvers miss many of these cases.
For the purpose of identifying additional corefer-
ring terms, we consider two noun phrases in the
same document as coreferring if: (i) their heads
are identical and (ii) no semantic incompatibil-
ity is found between their modifiers. The types
of incompatibility we handle are: (a) mismatch-
ing numbers, (b) antonymy and (c) co-hyponymy
(coordinate terms), as specified by WordNet. For
example, two nodes of the noun distance would
be considered incompatible if one is modified by
short and the second by its antonym long. Simi-
larly, two modifier co-hyponyms of distance, such
as walking and running would also result such
an incompatibility. Adding more incompatibility
types (e.g. first vs. second flight) may further im-
prove the precision of this method.
</bodyText>
<subsectionHeader confidence="0.99482">
5.2 Global information
</subsectionHeader>
<bodyText confidence="0.999195888888889">
Key terms or prominent pieces of information that
appear in the document, typically at the title or the
first few sentences, are many times perceived as
globally known throughout the document. For
example, the geographic location of the document
theme, mentioned at the beginning of the docu-
ment, is assumed to be known from that point on,
and will often not be mentioned explicitly in fur-
ther sentences. This is a bridging phenomenon
</bodyText>
<page confidence="0.991531">
772
</page>
<bodyText confidence="0.983539545454545">
\x0cthat is typically not addressed by available dis-
course processing tools. To compensate for that,
we identify key terms for each document based
on tf-idf scores and consider them as global in-
formation for that document. For example, global
terms for the topic discussing the ice melting in
the Arctic, typically contain a location such as
Arctic or Antarctica and terms referring to ice, like
permafrost or iceshelf.
We use a variant of tf-idf, where term frequency
is computed as follows: tf(ti,j) = ni,j +~
</bodyText>
<equation confidence="0.825082">
&gt;
~
</equation>
<bodyText confidence="0.98442955882353">
fi,j.
Here, ni,j is the frequency of term i in document j
(ti,j), which is incremented by additional positive
weights (~
) for a set of features ( ~
fi,j) of the term.
Based on our analysis, we defined the following
features, which correlated mostly with global in-
formation: (i) does the term appear in the title?
(ii) is it a proper name? (iii) is it a location? The
weights for these features are set empirically.
The documents top-n global terms are added
to each of its sentences. As a result, a global term
that occurs in the hypothesis is matched in each
sentence of the document, regardless of whether
the term explicitly appears in the sentence.
Considering the previous sentence Another
method for addressing missing coreference and
bridging relations is based on the assumption that
adjacent sentences often refer to the same entities
and events. Thus, when extracting classification
features for a given sentence, in addition to the
features extracted from the parse tree of the sen-
tence itself, we extract the same set of features
from the current and previous sentences together.
Recall the example presented in Section 1. T is
annotated as entailing the hypothesis The AS-28
mini-submarine was trapped underwater, but the
word submarine, e.g., appears only in its preced-
ing sentence T. Thus, considering both sentences
together when classifying T increases its coverage
of the hypothesis. Indeed, a bridging reference re-
lates on board in T with submarine in T, justify-
ing our assumption in this case.
</bodyText>
<subsectionHeader confidence="0.957528">
5.3 Document-level classification
</subsectionHeader>
<bodyText confidence="0.998072212121212">
Beyond discourse references addressed above,
further information concerning discourse and doc-
ument structure is available in the Search setting
and may contribute to entailment classification.
We observed that entailing sentences tend to come
in bulks. This reflects a common coherence as-
pect, where the discussion of a specific topic is
typically continuous rather than scattered across
the entire document. This locality phenomenon
may be useful for entailment classification since
knowing that a sentence entails the hypothesis in-
creases the probability that adjacent sentences en-
tail the hypothesis as well.
To capture this phenomenon, we use a two-
phase meta-classification scheme, in which a
meta-classifier utilizes entailment classifications
of the first classification phase to extract meta-
features and determine the final classification de-
cision. This scheme also provides a convenient
way to combine scores from multiple classifiers
used in the first classification phase. We refer
to these as base-classifiers. This scheme and the
meta-features we used are detailed hereunder.
Let us write (s, h) for a sentence-hypothesis
pair. We denote the set of pairs in the development
(training) set as D and in the test set as T . We split
D into two halves, D1 and D2. We make use of n
base-classifiers, C1, . . . , Cn, among which C? is
a designated classifier with additional roles in the
process, as described below. Classifiers may dif-
fer, for example, in their classification algorithm.
An additional meta-classifier is denoted CM . The
classification scheme is shown as Algorithm 1.
</bodyText>
<table confidence="0.952844933333333">
Algorithm 1 Meta-classification
Training
1: Extract features for every (s, h) in D
2: Train C1, . . . , Cn on D1
3: Classify D2, using C1, . . . , Cn
4: Extract meta-features for D2 using the
classification of C1, . . . , Cn
5: Train CM on D2
Classification
6: Extract features for every (s, h) in T
7: Classify T using C1, . . . , Cn
8: Extract meta-features for T
9: Classify T using CM
At Step 1, features are extracted for every (s, h)
pair in the training set, as in the baseline system.
</table>
<page confidence="0.918894">
773
</page>
<bodyText confidence="0.981599315789474">
\x0cIn Steps 2 and 3 we split the training set into two
halves (taking half of each topic), train n different
classifiers on the first half and then classify the
second half using each of the n classifiers. Given
the classification scores of the n base-classifiers
to the (s, h) pairs in the second half of the train-
ing set, D2, we add in Step 4 the meta-features
described in Section 5.3.1.
After adding the meta-features, we train
(Step 5) a meta-classifier on this new set of fea-
tures. Test sentences then go through the same
process: features are extracted for them and they
are classified by the already trained n classifiers
(Steps 6 and 7), meta-features are extracted in
Step 8, and a final classification decision is made
by the meta-classifier in Step 9.
A retrieval step may precede the actual en-
tailment classification, allowing the processing of
fewer and potentially better candidates.
</bodyText>
<subsubsectionHeader confidence="0.931371">
5.3.1 Meta-features
</subsubsectionHeader>
<bodyText confidence="0.967722461538462">
The following features are extracted in our
meta-classification scheme:
Classification scores The classification score of
each of the n base-classifiers.
Title entailment In many texts, and in news ar-
ticles in particular, the title and the first few sen-
tences often represent the entire documents con-
tent. Thus, knowing whether these sentences en-
tail the hypothesis may be an indicator to the gen-
eral potential of the document to include entailing
sentences. Two binary features are added accord-
ing to the classification of C? indicating whether
the title entails the hypothesis and whether the first
sentence entails it.
Second-closest entailment Considering the lo-
cality phenomenon described above, we add a fea-
ture assigning higher scores to sentences in the
vicinity of an entailment environment. This fea-
ture is computed as the distance to the second-
closest entailing sentence in the document (count-
ing the sentence itself as well), according to the
classification of C?. Formally, let i be the index of
the current sentence and J be the set of indices of
entailing sentences in the document according to
C?. For each j J we compute di,j = |ij|, and
choose the second smallest di,j as di. The idea is
</bodyText>
<figure confidence="0.940653083333334">
Ent?
#
11
10
9
8
7
6
5
4
3
2
1
NO
NO
YES
YES
YES
YES
NO
NO
NO
NO
NO
d
2nd closest
8
8
8
7 or 9
6 or 8
7
7
7
7
7
7
2
3
1
1
1
1
2
3
4
5
6
d
Closest
8
8
8
7 or 9
6 or 8
7
6
6
6
6
6
1
2
1
1
1
1
1
2
3
4
5
</figure>
<figureCaption confidence="0.999835">
Figure 1: Comparison of the closest and second-closest
</figureCaption>
<bodyText confidence="0.98434790909091">
schemes when applied to a bulk of entailing sentences (in
white) situated within a non-entailing environment (in gray).
Unlike the closest one, the second-closest scheme assigns
larger distance values to non-entailing sentences located on
the edge of the bulk (5 and 10) than to entailing ones.
that if entailing sentences indeed always come in
bulks, then di = 1 for all entailing sentences, but
di &gt; 1 for all non-entailing ones. Figure 1 illus-
trates such a case, comparing the second-closest
distance with the distance to the closest entailing
sentence. In the closest scheme we do not count
the sentence as closest to itself since it would dis-
regard the environment of the sentence altogether,
eliminating the desired effect. We scale the dis-
tance and add the feature score: log(di).
Smoothed entailment This feature addressed
the locality phenomenon by smoothing the
classification score of sentence i with the scores
of adjacent sentences, weighted by their distance
from the current sentence i. Let s(i) be the
score assigned by C? to sentence i. We add the
Smoothed Entailment feature score:
</bodyText>
<equation confidence="0.9980802">
SE(i) =
P
w(b|w|s(i+w))
P
w(b|w|)
</equation>
<bodyText confidence="0.9881">
where 0 &lt; b &lt; 1 is the decay parameter and w is
an integer bounded between N and N, denoting
the distance from sentence i.
1st sentence entailing title Bensley and Hickl
(2008) showed that the first sentence in a news ar-
ticle typically entails the articles title. We there-
fore assume that in each document, s1 s0,
where s1 and s0 are the documents first sentence
and title respectively. Hence, under entailment
transitivity, if s0 h then s1 h. The cor-
responding binary feature states whether the sen-
tence being classified is the documents first sen-
tence and the title entails h according to C?.
</bodyText>
<page confidence="0.997975">
774
</page>
<table confidence="0.991868333333333">
\x0cP (%) R (%) F1 (%)
BIU-BL 14.53 55.25 23.00
BIU-DISC 20.82 57.25 30.53
BIU-BL3 14.86 59.00 23.74
BIU-DISCnoloc 22.35 57.12 32.13
All-yes baseline 4.6 100.0 8.9
</table>
<tableCaption confidence="0.99855">
Table 1: Micro-average results.
</tableCaption>
<bodyText confidence="0.9987038">
Note that the above locality-based features rely
on high accuracy of the base classifier C?. Oth-
erwise, it will provide misleading information to
the features computation. We analyze the effect
of this accuracy in Section 6.
</bodyText>
<sectionHeader confidence="0.997496" genericHeader="evaluation">
6 Results and Analysis
</sectionHeader>
<bodyText confidence="0.986995941176471">
Using the RTE-5 Search data, we compare
BIUTEE in its baseline configuration (cf. Sec-
tion 4), denoted BIU-BL, with its discourse-aware
enhancement (BIU-DISC) which uses all the com-
ponents described in Section 5. To alleviate the
strong IR effect described in Section 3, both sys-
tems are applied to the complete datasets (both
training and test), without candidates pre-filtering.
BIU-DISC uses three base-classifiers (n = 3):
SVMperf (Joachims, 2006), and Nave Bayes and
Logistic Regression from the WEKA package
(Witten and Frank, 2005). The first among these
is set as our designated classifier C?, which is
used for the computation of the document-level
features. SVMperf is also used for the meta-
classifier. For the smoothed entailment score (cf.
Section 5.3), we used b = 0.9 and N = 3. Global
information is added by enriching each sentence
with the highest-ranking term in the document, ac-
cording to tf-idf scores (cf. Section 5.2), where
document frequencies were computed based on
about half a million documents from the TIP-
STER corpus (Harman, 1992). The set of weights
~
equals {2, 1, 4} for title terms, proper names and
locations, respectively. All parameters were tuned
based on a 10-fold cross-validation on the devel-
opment set, optimizing the micro-averaged F1.
The results are presented in Table 1. As can be
seen in the table, BIU-DISC outperforms BIU-BL in
every measure, showing the impact of addressing
discourse in this setting. To rule out the option that
the improvement is simply due to the fact that we
use three classifiers for BIU-DISC and a single one
</bodyText>
<table confidence="0.986451888888889">
P (%) R (%) F1 (%)
By Topic
BIU-BL 16.54 55.62 25.50
BIU-DISC 22.69 57.96 32.62
All-yes baseline 4.85 100.00 9.25
By Hypothesis
BIU-BL 22.87 59.62 33.06
BIU-DISC 27.81 61.97 38.39
All-yes baseline 4.96 100.00 9.46
</table>
<tableCaption confidence="0.996802">
Table 2: Macro-average results.
</tableCaption>
<bodyText confidence="0.996186378378378">
for BIU-BL, we show (BIU-BL3) the results when
the baseline system is applied in the same meta-
classification configuration as BIU-DISC, with the
same three classifiers. Apparently, without the
discourse information this configurations contri-
bution is limited.
As mentioned in Section 5.3, the benefit from
the locality features rely directly on the perfor-
mance of the base classifiers. Hence, considering
the low precision scores obtained here, we applied
BIU-DISC to the data in the meta-classification
scheme, but with locality features removed. The
results, shown as BIU-DISCnoloc in the Table, in-
dicate that indeed performance increases without
these features. The last line of the table shows the
results obtained by a nave baseline where all test-
set pairs are considered entailing.
For completeness, Table 2 shows the macro-
averaged results, when averaged over the topics or
over the hypotheses. Although we tuned our sys-
tem to maximize micro-averaged F1, these figures
comply with the ones shown in Table 1.
Analysis of locality As discussed in Section 5,
determining whether a sentence entails a hypothe-
sis should take into account whether adjacent sen-
tences also entail the hypothesis. In the above ex-
periment we were unable to show the contribution
of our systems component that attempts to cap-
ture this information; on the contrary, the results
show it had a negative impact on performance.
Still, we claim that this information can be use-
ful when used within a more accurate system. We
try to validate this conjecture by understanding
how performance of the locality features varies as
the systems becomes more accurate. We do so via
the following simulation.
When classifying a certain sentence, the classi-
</bodyText>
<page confidence="0.973369">
775
</page>
<figure confidence="0.8381365">
\x0c20
25
30
35
40
0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
p
F1
</figure>
<figureCaption confidence="0.971977">
Figure 2: F1 performance of BIU-DISC as a function of
</figureCaption>
<bodyText confidence="0.996330818181818">
the accuracy in classifying adjacent sentences.
fications of its adjacent sentences are given by an
oracle classifier that provides the correct answer
with probability p. The system is applied using
two locality features: the 1st sentence entailing
title feature and a close variant of the smoothed
entailment feature, which calculates the weighted
average of adjacent sentences, but disregards the
score of the currently evaluated sentence.3 Thus
we supply information about adjacent sentences
and test whether overall performance increases
with the accuracy of this information.
We performed this experiment for p in a range
of [0.5-1.0]. Figure 2 shows the results of this sim-
ulation, based on the average F1 of five runs for
each p. Since performance, from a certain point,
increases with the accuracy of the oracle classi-
fier, we can conclude that indeed precise infor-
mation about adjacent sentences improves perfor-
mance on the current sentence, and that locality is
a true phenomenon in the data. We note, however,
that performance improves only when accuracy is
very high, suggesting the currently limited prac-
tical potential of this information, at least in the
way locality was represented in this work.
Ablation tests Table 3 presents the results of the
ablation tests performed to evaluate the contribu-
tion of each component. Based on the result re-
ported in Table 1 and the above discussion, the
tests were performed relative to BIU-DISCnoloc,
the optimal configuration. As seen in the table,
the removal of each component causes a drop
in results. For global information we see a mi-
</bodyText>
<page confidence="0.985379">
3
</page>
<bodyText confidence="0.980537333333333">
The second-closest entailment feature was not used as it
considers the oracles decision for the current sentence, while
we wish to use only information about adjacent sentences.
</bodyText>
<table confidence="0.989121">
Component removed F1 (%) F1 (%)
Previous sent. features 28.55 3.58
Augmented coref. 26.73 5.40
Global information 31.76 0.37
</table>
<tableCaption confidence="0.999083">
Table 3: Results of ablation tests relative to
</tableCaption>
<bodyText confidence="0.997483027027027">
BIU-DISCnoloc. The columns specify the compo-
nent removed, the micro-averaged F1 score achieved without
it, and the marginal contribution of the component.
nor difference, which is not surprising considering
the conservative approach we took, using a sin-
gle global term for each sentence. Possibly, this
information is also included in the other compo-
nents, thus proving no marginal contribution rel-
ative to them. Under the conditions of an over-
whelming majority of negative examples, this is
a risky method to use, and should be considered
when the ratio of positive examples is higher. For
future work, we intend to use this information via
classification features (e.g. the coverage obtained
with and without global information), rather than
the crude addition of the term to the sentence.
Analysis of augmented coreferences We an-
alyzed the performance of the component for
augmenting coreference relations relative to the
OpenNLP resolver. Recall that our component
works on top of the resolvers output and can add
or remove coreference relations. As a complete
annotation of coreference chains in the dataset is
unavailable, we performed the following evalua-
tion. Recall is computed based on the number
of identified pairs from a sample of 100 intra-
document coreference and bridging relations from
the annotated dataset described in (Mirkin et al.,
2010). Precision is computed based on 50 pairs
sampled from the output of each method, equally
distributed over topics. The results, shown in Ta-
ble 4, indicate the much higher recall obtained
by our component at some cost in precision. Al-
though rather simple, the ablation test of this com-
ponent shows its usefulness. Still, both methods
achieve low absolute recall, suggesting the need
for more robust tools for this task.
</bodyText>
<table confidence="0.879244333333333">
P (%) R (%) F1 (%)
OpenNLP 74 16 26.3
Augmented coref. 60 28 38.2
</table>
<tableCaption confidence="0.999231">
Table 4: Performance of coreference methods.
</tableCaption>
<page confidence="0.943968">
776
</page>
<figure confidence="0.996404470588235">
\x0c0
5
10
15
20
25
30
35
40
45
50
0 10 20 30 40 50 60 70 80 90 100
k
F1
BIU-BL
BIU-DISC
Lucene
</figure>
<figureCaption confidence="0.790961">
Figure 3: F1 performance as a function of the number of
retrieved candidates.
</figureCaption>
<bodyText confidence="0.998528967741935">
Candidate retrieval setting As mentioned in
Section 3, best performance of RTE systems in the
task was obtained when applying a first step of IR-
based candidate filtering. We therefore compare
the performance of BIU-DISC with that of BIU-BL
under this setting as well.4 For candidate retrieval
we used Lucene, a state of the art search engine5,
in a range of top-k retrieved candidates. The re-
sults are shown in Figure 3. For reference, the fig-
ure also shows the performance along this range
of Lucene as-is, when no further inference is ap-
plied to the retrieved candidates.
While BIU-DISC does not outperform BIU-BL at
every point, the area under the curve is clearly
larger for BIU-DISC. The figure also indicates that
BIU-DISC is far more robust, maintaining a stable
F1 and enabling a stable tradeoff between recall
and precision along the whole range (recall ranges
between 42% and 55% for k [15 100], with
corresponding precision range of 51% to 33%).
Finally, Table 5 shows the results of the best
systems as determined in our first experiment.
We performed a single experiment to compare
BIU-DISCnoloc and BIU-BL3 under a candidate re-
trieval setting, using k = 20, where both systems
highly perform. We compare these results to the
highest score obtained by Lucene, as well as to the
two best submissions to the RTE-5 Search task6.
BIU-DISCnoloc outperforms all other methods and
its result is significantly better than BIU-BL3 with
p &lt; 0.01 according to McNemars test.
</bodyText>
<page confidence="0.9143">
4
</page>
<bodyText confidence="0.952959">
This time, for global information, the documents three
highest ranking terms were added to each sentence.
</bodyText>
<figure confidence="0.452712">
5
http://lucene.apache.org
</figure>
<page confidence="0.84305">
6
</page>
<bodyText confidence="0.871595">
The best one is an earlier version of this work (Mirkin et
al., 2009); the second is MacKinlay and Baldwins (2009).
</bodyText>
<table confidence="0.980188666666667">
P (%) R (%) F1 (%)
BIU-DISCnoloc 50.77 45.12 47.78
BIU-BL3 51.68 40.38 45.33
Lucene, top-15 35.93 52.50 42.66
RTE-5 best 40.98 51.38 45.59
RTE-5 second-best 42.94 38.00 40.32
</table>
<tableCaption confidence="0.999134">
Table 5: Performance of best configurations.
</tableCaption>
<sectionHeader confidence="0.997709" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999818615384615">
While it is generally assumed that discourse inter-
acts with semantic entailment inference, the con-
crete impacts of discourse on such inference have
been hardly explored. This paper presented a first
empirical investigation of discourse processing
aspects related to entailment. We argue that avail-
able discourse processing tools should be substan-
tially improved towards this end, both in terms of
the phenomena they address today, namely nom-
inal coreference, and with respect to the cover-
ing of additional phenomena, such as bridging
anaphora. Our experiments show that even rather
simple methods for addressing discourse can have
a substantial positive impact on the performance
of entailment inference. Concerning the local-
ity phenomenon stemming from discourse coher-
ence, we learned that it does carry potentially use-
ful information, which might become beneficial
in the future when better-performing entailment
systems become available. Until then, integrating
this information with entailment confidence may
be useful. Overall, we suggest that entailment sys-
tems should extensively incorporate discourse in-
formation, while developing sound algorithms for
addressing various discourse phenomena, includ-
ing the ones described in this paper.
</bodyText>
<sectionHeader confidence="0.962618" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.947219">
The authors are thankful to Asher Stern and Ilya
Kogan for their help in implementing and evalu-
ating the augmented coreference component, and
to Roy Bar-Haim for useful advice concerning
this paper. This work was partially supported
by the Israel Science Foundation grant 1112/08
and the PASCAL-2 Network of Excellence of the
European Community FP7-ICT-2007-1-216886.
Jonathan Berant is grateful to the Azrieli Foun-
dation for the award of an Azrieli Fellowship.
</bodyText>
<page confidence="0.962233">
777
</page>
<reference confidence="0.996783552083333">
\x0cReferences
Bar-Haim, Roy, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, Eyal Shnarch, and Idan
Szpektor. 2008. Efficient semantic deduction and
approximate matching over compact parse forests.
In Proc. of Text Analysis Conference (TAC).
Bar-Haim, Roy, Jonathan Berant, and Ido Dagan.
2009. A compact forest for scalable inference
over entailment and paraphrase rules. In Proc. of
EMNLP.
Bensley, Jeremy and Andrew Hickl. 2008. Unsuper-
vised resource creation for textual inference appli-
cations. In Proc. of LREC.
Bentivogli, Luisa, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, Medea Lo Leggio, and Bernardo
Magnini. 2009a. Considering discourse refer-
ences in textual entailment annotation. In Proc. of
the 5th International Conference on Generative Ap-
proaches to the Lexicon (GL2009).
Bentivogli, Luisa, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009b. The
fifth PASCAL recognizing textual entailment chal-
lenge. In Proc. of TAC.
Castillo, Julio J. 2009. Sagan in TAC2009: Using
support vector machines in recognizing textual en-
tailment and TE search pilot task. In Proc. of TAC.
Clark, Peter and Phil Harrison. 2009. An inference-
based approach to recognizing entailment. In Proc.
of TAC.
Clark, Herbert H. 1975. Bridging. In Schank, R. C.
and B. L. Nash-Webber, editors, Theoretical issues
in natural language processing, pages 169174. As-
sociation of Computing Machinery.
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges, vol-
ume 3944 of Lecture Notes in Computer Science,
pages 177190. Springer.
Dagan, Ido, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, pages 15(4):117.
Dali, Lorand, Delia Rusu, Blaz Fortuna, Dunja
Mladenic, and Marko Grobelnik. 2009. Question
answering based on semantic graphs. In Proc. of the
Workshop on Semantic Search (SemSearch 2009).
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Lexical Database (Language, Speech,
and Communication). The MIT Press.
Finkel, Jenny Rose, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proc. of ACL.
Giampiccolo, Danilo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recog-
nizing textual entailment challenge. In Proc. of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
Harabagiu, Sanda and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain ques-
tion answering. In Proc. of ACL.
Harman, Donna. 1992. The DARPA TIPSTER
project. SIGIR Forum, 26(2):2628.
Joachims, Thorsten. 2006. Training linear SVMs in
linear time. In Proc. of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Li, Fangtao, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with ran-
dom walks on graphs. In Proc. of ACL-IJCNLP.
Litkowski, Ken. 2009. Overlap analysis in textual en-
tailment recognition. In Proc. of TAC.
MacKinlay, Andrew and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Proc.
of TAC.
Mirkin, Shachar, Roy Bar-Haim, Jonathan Berant, Ido
Dagan Eyal Shnarch, Asher Stern, and Idan Szpek-
tor. 2009. Addressing discourse and document
structure in the RTE search task. In Proc. of TAC.
Mirkin, Shachar, Ido Dagan, and Sebastian Pado.
2010. Assessing the role of discourse references in
entailment inference. In Proc. of ACL.
Nenkova, Ani, Rebecca Passonneau, and Kathleen
Mckeown. 2007. The pyramid method: incorpo-
rating human content selection variation in summa-
rization evaluation. In ACM Transactions on Speech
and Language Processing.
Qiu, Long, Min-Yen Kan, and Tat-Seng Chua. 2004.
A public reference implementation of the RAP
anaphora resolution algorithm. In Proc. of LREC.
Romano, Lorenza, Milen Kouylekov, Idan Szpektor,
and Ido Dagan. 2006. Investigating a generic
paraphrase-based approach for relation extraction.
In Proc. of EACL.
Witten, Ian H. and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques,
2nd Edition. Morgan Kaufmann, San Francisco.
</reference>
<page confidence="0.974059">
778
</page>
<figure confidence="0.250265">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.429745">
<note confidence="0.8967955">b&amp;apos;Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 770778, Beijing, August 2010</note>
<title confidence="0.99668">Recognising Entailment within Discourse</title>
<author confidence="0.828556666666667">Jonathan Berant</author>
<affiliation confidence="0.993887666666667">Eyal Shnarch Computer Science Department, Bar-Ilan University The Blavatnik School of Computer Science, Tel-Aviv University</affiliation>
<abstract confidence="0.997069722222222">Texts are commonly interpreted based on the entire discourse in which they are situated. Discourse processing has been shown useful for inference-based application; yet, most systems for textual entailment a generic paradigm for applied inference have only addressed discourse considerations via off-the-shelf coreference resolvers. In this paper we explore various discourse aspects in entailment inference, suggest initial solutions for them and investigate their impact on entailment performance. Our experiments suggest that discourse provides useful information, which significantly improves entailment inference, and should be better addressed by future entailment systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences Bar-Haim</author>
<author>Jonathan Berant Roy</author>
</authors>
<title>Ido Dagan, Iddo Greental, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor.</title>
<date>2008</date>
<booktitle>In Proc. of Text Analysis Conference (TAC).</booktitle>
<marker>Bar-Haim, Roy, 2008</marker>
<rawString>\x0cReferences Bar-Haim, Roy, Jonathan Berant, Ido Dagan, Iddo Greental, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor. 2008. Efficient semantic deduction and approximate matching over compact parse forests. In Proc. of Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
</authors>
<title>A compact forest for scalable inference over entailment and paraphrase rules.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="9064" citStr="Bar-Haim et al., 2009" startWordPosition="1400" endWordPosition="1403">her system that relied on inference without IR-based pre-filtering. At the current state of performance of entailment systems, it seems that lexical coverage largely overshadows any other approach in this task. Still, most (6 out of 8) participants in the challenge applied their entailment systems to the entire dataset without a prior retrieval of candidate sentences. F1 scores for such systems vary between 10% and 33%, in comparison to over 40% of the IR-based methods. 4 The Baseline RTE System In this work we used BIUTEE, Bar-Ilan University Textual Entailment Engine (Bar-Haim et al., 2008; Bar-Haim et al., 2009), a state of the art RTE system, as a baseline and as a basis for our discourse-based enhancements. This section describes this systems architecture; the methods by which it was augmented to address discourse are presented in Section 5. To determine entailment, BIUTEE performs the following main steps: Preprocessing First, all documents are parsed and processed with standard tools for named entity recognition (Finkel et al., 2005) and coreference resolution. For the latter purpose, we use OpenNLP and enable the substitution of coreferring terms. This is the only way by which BIUTEE addresses d</context>
</contexts>
<marker>Bar-Haim, Berant, Dagan, 2009</marker>
<rawString>Bar-Haim, Roy, Jonathan Berant, and Ido Dagan. 2009. A compact forest for scalable inference over entailment and paraphrase rules. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy Bensley</author>
<author>Andrew Hickl</author>
</authors>
<title>Unsupervised resource creation for textual inference applications.</title>
<date>2008</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="20159" citStr="Bensley and Hickl (2008)" startWordPosition="3273" endWordPosition="3276">altogether, eliminating the desired effect. We scale the distance and add the feature score: log(di). Smoothed entailment This feature addressed the locality phenomenon by smoothing the classification score of sentence i with the scores of adjacent sentences, weighted by their distance from the current sentence i. Let s(i) be the score assigned by C? to sentence i. We add the Smoothed Entailment feature score: SE(i) = P w(b|w|s(i+w)) P w(b|w|) where 0 &lt; b &lt; 1 is the decay parameter and w is an integer bounded between N and N, denoting the distance from sentence i. 1st sentence entailing title Bensley and Hickl (2008) showed that the first sentence in a news article typically entails the articles title. We therefore assume that in each document, s1 s0, where s1 and s0 are the documents first sentence and title respectively. Hence, under entailment transitivity, if s0 h then s1 h. The corresponding binary feature states whether the sentence being classified is the documents first sentence and the title entails h according to C?. 774 \x0cP (%) R (%) F1 (%) BIU-BL 14.53 55.25 23.00 BIU-DISC 20.82 57.25 30.53 BIU-BL3 14.86 59.00 23.74 BIU-DISCnoloc 22.35 57.12 32.13 All-yes baseline 4.6 100.0 8.9 Table 1: Micr</context>
</contexts>
<marker>Bensley, Hickl, 2008</marker>
<rawString>Bensley, Jeremy and Andrew Hickl. 2008. Unsupervised resource creation for textual inference applications. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Ido Dagan</author>
</authors>
<title>Hoa Trang Dang, Danilo Giampiccolo,</title>
<date>2009</date>
<booktitle>In Proc. of the 5th International Conference on Generative Approaches to the Lexicon (GL2009).</booktitle>
<location>Medea</location>
<marker>Bentivogli, Dagan, 2009</marker>
<rawString>Bentivogli, Luisa, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, Medea Lo Leggio, and Bernardo Magnini. 2009a. Considering discourse references in textual entailment annotation. In Proc. of the 5th International Conference on Generative Approaches to the Lexicon (GL2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
</authors>
<title>The fifth PASCAL recognizing textual entailment challenge.</title>
<date>2009</date>
<booktitle>In Proc. of TAC.</booktitle>
<contexts>
<context position="2114" citStr="Bentivogli et al., 2009" startWordPosition="314" endWordPosition="317">anguage processing applications, including information extraction (Romano et al., 2006) and question answering (Harabagiu and Hickl, 2006). Yet, most entailment systems have thus far paid little attention to discourse aspects of inference. In part, this is the result of the unavailability of adept tools for handling the kind of discourse processing required for inference. In addition in the main TE benchmarks, the Recognising Textual Entailment (RTE) challenges, discourse played little role. This state of affairs has started to change with the recent introduction of the RTE Pilot Search task (Bentivogli et al., 2009b), in which assessed texts are situated within complete documents. In this setting, texts need to be interpreted based on their entire discourse (Bentivogli et al., 2009a), hence attending to discourse issues becomes essential. Consider the following example from the tasks dataset: (T) The seven men on board were said to have as little as 24 hours of air. For the interpretation of T, e.g. the identity and whereabouts of the seven men, one must consider Ts discourse. The preceding sentence T, for instance, provides useful information to that aim: (T) The Russian navy worked desperately to save</context>
</contexts>
<marker>Bentivogli, Dagan, Dang, Giampiccolo, Magnini, 2009</marker>
<rawString>Bentivogli, Luisa, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009b. The fifth PASCAL recognizing textual entailment challenge. In Proc. of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio J Castillo</author>
</authors>
<title>Sagan in TAC2009: Using support vector machines in recognizing textual entailment and TE search pilot task.</title>
<date>2009</date>
<booktitle>In Proc. of TAC.</booktitle>
<contexts>
<context position="6630" citStr="Castillo, 2009" startWordPosition="1018" endWordPosition="1019">publicly available tools such as JavaRap (Qiu et al., 2004) or OpenNLP1, e.g. (Bar-Haim et al., 2008). A major step in the RTE challenges towards a more practical setting of text processing applications occurred with the introduction of the Search task in the Fifth RTE challenge (RTE-5). In this task entailing sentences are situated within documents and depend on other sentences for their correct interpretation. Thus, discourse becomes a substantial factor impacting inference. Surprisingly, discourse hardly received any treatment in this task beyond the standard use of coreference resolution (Castillo, 2009; Litkowski, 2009), and an attempt to address globally-known information by removing from H words that appear in document headlines (Clark and Harrison, 2009). 3 The RTE Search Task The RTE-5 Search task was derived from the TAC Summarization task2. The dataset consists of several corpora, each comprised of news articles concerning a specific topic, such as the impact of global warming on the Arctic or the London terrorist attacks in 2005. Hypotheses were manually generated based on Summary Content Units (Nenkova et al., 2007), clause-long statements taken from manual summaries of the corpora.</context>
</contexts>
<marker>Castillo, 2009</marker>
<rawString>Castillo, Julio J. 2009. Sagan in TAC2009: Using support vector machines in recognizing textual entailment and TE search pilot task. In Proc. of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Clark</author>
<author>Phil Harrison</author>
</authors>
<title>An inferencebased approach to recognizing entailment.</title>
<date>2009</date>
<booktitle>In Proc. of TAC.</booktitle>
<contexts>
<context position="6788" citStr="Clark and Harrison, 2009" startWordPosition="1040" endWordPosition="1043"> more practical setting of text processing applications occurred with the introduction of the Search task in the Fifth RTE challenge (RTE-5). In this task entailing sentences are situated within documents and depend on other sentences for their correct interpretation. Thus, discourse becomes a substantial factor impacting inference. Surprisingly, discourse hardly received any treatment in this task beyond the standard use of coreference resolution (Castillo, 2009; Litkowski, 2009), and an attempt to address globally-known information by removing from H words that appear in document headlines (Clark and Harrison, 2009). 3 The RTE Search Task The RTE-5 Search task was derived from the TAC Summarization task2. The dataset consists of several corpora, each comprised of news articles concerning a specific topic, such as the impact of global warming on the Arctic or the London terrorist attacks in 2005. Hypotheses were manually generated based on Summary Content Units (Nenkova et al., 2007), clause-long statements taken from manual summaries of the corpora. Texts are unmodified sentences in the articles. Given a topic and a hypothesis, entailment systems are required to identify all sentences in the topics corpu</context>
</contexts>
<marker>Clark, Harrison, 2009</marker>
<rawString>Clark, Peter and Phil Harrison. 2009. An inferencebased approach to recognizing entailment. In Proc. of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<date>1975</date>
<journal>Association of Computing Machinery.</journal>
<booktitle>Theoretical issues in natural language processing,</booktitle>
<pages>169174</pages>
<editor>Bridging. In Schank, R. C. and B. L. Nash-Webber, editors,</editor>
<contexts>
<context position="3466" citStr="Clark, 1975" startWordPosition="528" endWordPosition="529">Still, little was done by the tasks participants to consider discourse, and sentences were mostly processed independently. Analyzing the Search tasks development set, we identified several key discourse aspects that affect entailment in a discourse-dependent setting. First, we observed that the coverage of available coreference resolution tools is considerably limited. To partly address this problem, we extend the set of coreference relations to phrase pairs with a certain degree of lexical overlap, as long as no semantic incompatibility is found between them. Second, many bridging relations (Clark, 1975) are realized in the form of global information perceived as known for entire documents. As bridging falls completely out of the scope of available resolvers, we address this phenomenon by identifying and weighting prominent document terms and allowing their incorporation in inference even 770 \x0cwhen they are not explicitly mentioned in a sentence. Finally, we observed a coherence-related discourse phenomenon, namely inter-relations between entailing sentences in the discourse, such as the tendency of entailing sentences to be adjacent to one another. To that end, we apply a two-phase classi</context>
</contexts>
<marker>Clark, 1975</marker>
<rawString>Clark, Herbert H. 1975. Bridging. In Schank, R. C. and B. L. Nash-Webber, editors, Theoretical issues in natural language processing, pages 169174. Association of Computing Machinery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Machine Learning Challenges,</booktitle>
<volume>3944</volume>
<pages>177190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1431" citStr="Dagan et al., 2006" startWordPosition="206" endWordPosition="209">eir impact on entailment performance. Our experiments suggest that discourse provides useful information, which significantly improves entailment inference, and should be better addressed by future entailment systems. 1 Introduction This paper investigates the problem of recognising textual entailment within discourse. Textual Entailment (TE) is a generic framework for applied semantic inference (Dagan et al., 2009). Under TE, the relationship between a text (T) and a textual assertion (hypothesis, H) is defined such that T entails H if humans reading T would infer that H is most likely true (Dagan et al., 2006). TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al., 2006) and question answering (Harabagiu and Hickl, 2006). Yet, most entailment systems have thus far paid little attention to discourse aspects of inference. In part, this is the result of the unavailability of adept tools for handling the kind of discourse processing required for inference. In addition in the main TE benchmarks, the Recognising Textual Entailment (RTE) challenges, discourse played little role. This state of affairs has started to change</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Dagan, Ido, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges, volume 3944 of Lecture Notes in Computer Science, pages 177190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Bernardo Magnini</author>
<author>Dan Roth</author>
</authors>
<title>Recognizing textual entailment: Rational, evaluation and approaches. Natural Language Engineering,</title>
<date>2009</date>
<pages>15--4</pages>
<contexts>
<context position="1231" citStr="Dagan et al., 2009" startWordPosition="168" endWordPosition="171">ssed discourse considerations via off-the-shelf coreference resolvers. In this paper we explore various discourse aspects in entailment inference, suggest initial solutions for them and investigate their impact on entailment performance. Our experiments suggest that discourse provides useful information, which significantly improves entailment inference, and should be better addressed by future entailment systems. 1 Introduction This paper investigates the problem of recognising textual entailment within discourse. Textual Entailment (TE) is a generic framework for applied semantic inference (Dagan et al., 2009). Under TE, the relationship between a text (T) and a textual assertion (hypothesis, H) is defined such that T entails H if humans reading T would infer that H is most likely true (Dagan et al., 2006). TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al., 2006) and question answering (Harabagiu and Hickl, 2006). Yet, most entailment systems have thus far paid little attention to discourse aspects of inference. In part, this is the result of the unavailability of adept tools for handling the kind of discourse </context>
</contexts>
<marker>Dagan, Dolan, Magnini, Roth, 2009</marker>
<rawString>Dagan, Ido, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entailment: Rational, evaluation and approaches. Natural Language Engineering, pages 15(4):117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorand Dali</author>
<author>Delia Rusu</author>
<author>Blaz Fortuna</author>
<author>Dunja Mladenic</author>
<author>Marko Grobelnik</author>
</authors>
<title>Question answering based on semantic graphs.</title>
<date>2009</date>
<booktitle>In Proc. of the Workshop on Semantic Search (SemSearch</booktitle>
<contexts>
<context position="4931" citStr="Dali et al., 2009" startWordPosition="754" endWordPosition="757">on discoursebased information is helpful and achieves a significant improvement of results. We analyze the contribution of each component and suggest some future work to better attend to discourse in entailment systems. To our knowledge, this is the most extensive effort thus far to empirically explore the effect of discourse on entailment systems. 2 Background Discourse plays a key role in text understanding applications such as question answering or information extraction. Yet, such applications typically only handle a narrow aspect of discourse, addressing coreference by term substitution (Dali et al., 2009; Li et al., 2009). The limited coverage and scope of existing tools for coreference resolution and the unavailability of tools for addressing other discourse aspects also contribute to this situation. For instance, VP anaphora and bridging relations are usually not handled at all by such resolvers. A similar situation is seen in the TE research field. The prominent benchmark for entailment systems evaluation is the series of RTE challenges. The main task in these challenges has traditionally been to determine, given a text-hypothesis pair (T,H), whether T entails H. Discourse played no role i</context>
</contexts>
<marker>Dali, Rusu, Fortuna, Mladenic, Grobelnik, 2009</marker>
<rawString>Dali, Lorand, Delia Rusu, Blaz Fortuna, Dunja Mladenic, and Marko Grobelnik. 2009. Question answering based on semantic graphs. In Proc. of the Workshop on Semantic Search (SemSearch 2009).</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database (Language, Speech, and Communication).</title>
<date>1998</date>
<editor>Fellbaum, Christiane, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Fellbaum, Christiane, editor. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="9498" citStr="Finkel et al., 2005" startWordPosition="1469" endWordPosition="1472">n to over 40% of the IR-based methods. 4 The Baseline RTE System In this work we used BIUTEE, Bar-Ilan University Textual Entailment Engine (Bar-Haim et al., 2008; Bar-Haim et al., 2009), a state of the art RTE system, as a baseline and as a basis for our discourse-based enhancements. This section describes this systems architecture; the methods by which it was augmented to address discourse are presented in Section 5. To determine entailment, BIUTEE performs the following main steps: Preprocessing First, all documents are parsed and processed with standard tools for named entity recognition (Finkel et al., 2005) and coreference resolution. For the latter purpose, we use OpenNLP and enable the substitution of coreferring terms. This is the only way by which BIUTEE addresses discourse, representing the state of the art in entailment systems. Entailment-based transformations Given a T-H pair (both represented as dependency parse trees), the system applies a sequence of knowledge-based entailment transformations over T, generating a set of texts which are entailed by it. The goal is to obtain consequent texts which are more similar to H. Based on preliminary results on the development set, in our experim</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Finkel, Jenny Rose, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The third PASCAL recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proc. of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</booktitle>
<contexts>
<context position="5647" citStr="Giampiccolo et al., 2007" startWordPosition="869" endWordPosition="872">tion and the unavailability of tools for addressing other discourse aspects also contribute to this situation. For instance, VP anaphora and bridging relations are usually not handled at all by such resolvers. A similar situation is seen in the TE research field. The prominent benchmark for entailment systems evaluation is the series of RTE challenges. The main task in these challenges has traditionally been to determine, given a text-hypothesis pair (T,H), whether T entails H. Discourse played no role in the first two RTE challenges as Ts were constructed of short simplified texts. In RTE-3 (Giampiccolo et al., 2007), where some paragraph-long texts were included, intersentential relations became relevant for correct inference. Yet the texts in the task were manually modified to ensure they are self-contained. Consequently, little effort was invested by the challenges participants to address discourse issues beyond the standard substitution of coreferring nominal phrases, using publicly available tools such as JavaRap (Qiu et al., 2004) or OpenNLP1, e.g. (Bar-Haim et al., 2008). A major step in the RTE challenges towards a more practical setting of text processing applications occurred with the introducti</context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>Giampiccolo, Danilo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proc. of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Andrew Hickl</author>
</authors>
<title>Methods for using textual entailment in open-domain question answering.</title>
<date>2006</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1629" citStr="Harabagiu and Hickl, 2006" startWordPosition="234" endWordPosition="237">future entailment systems. 1 Introduction This paper investigates the problem of recognising textual entailment within discourse. Textual Entailment (TE) is a generic framework for applied semantic inference (Dagan et al., 2009). Under TE, the relationship between a text (T) and a textual assertion (hypothesis, H) is defined such that T entails H if humans reading T would infer that H is most likely true (Dagan et al., 2006). TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al., 2006) and question answering (Harabagiu and Hickl, 2006). Yet, most entailment systems have thus far paid little attention to discourse aspects of inference. In part, this is the result of the unavailability of adept tools for handling the kind of discourse processing required for inference. In addition in the main TE benchmarks, the Recognising Textual Entailment (RTE) challenges, discourse played little role. This state of affairs has started to change with the recent introduction of the RTE Pilot Search task (Bentivogli et al., 2009b), in which assessed texts are situated within complete documents. In this setting, texts need to be interpreted b</context>
</contexts>
<marker>Harabagiu, Hickl, 2006</marker>
<rawString>Harabagiu, Sanda and Andrew Hickl. 2006. Methods for using textual entailment in open-domain question answering. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Harman</author>
</authors>
<title>The DARPA TIPSTER project.</title>
<date>1992</date>
<journal>SIGIR Forum,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="22085" citStr="Harman, 1992" startWordPosition="3591" endWordPosition="3592">006), and Nave Bayes and Logistic Regression from the WEKA package (Witten and Frank, 2005). The first among these is set as our designated classifier C?, which is used for the computation of the document-level features. SVMperf is also used for the metaclassifier. For the smoothed entailment score (cf. Section 5.3), we used b = 0.9 and N = 3. Global information is added by enriching each sentence with the highest-ranking term in the document, according to tf-idf scores (cf. Section 5.2), where document frequencies were computed based on about half a million documents from the TIPSTER corpus (Harman, 1992). The set of weights ~ equals {2, 1, 4} for title terms, proper names and locations, respectively. All parameters were tuned based on a 10-fold cross-validation on the development set, optimizing the micro-averaged F1. The results are presented in Table 1. As can be seen in the table, BIU-DISC outperforms BIU-BL in every measure, showing the impact of addressing discourse in this setting. To rule out the option that the improvement is simply due to the fact that we use three classifiers for BIU-DISC and a single one P (%) R (%) F1 (%) By Topic BIU-BL 16.54 55.62 25.50 BIU-DISC 22.69 57.96 32.6</context>
</contexts>
<marker>Harman, 1992</marker>
<rawString>Harman, Donna. 1992. The DARPA TIPSTER project. SIGIR Forum, 26(2):2628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear SVMs in linear time.</title>
<date>2006</date>
<booktitle>In Proc. of the ACM Conference on Knowledge Discovery and Data Mining (KDD).</booktitle>
<contexts>
<context position="21476" citStr="Joachims, 2006" startWordPosition="3489" endWordPosition="3490">ifier C?. Otherwise, it will provide misleading information to the features computation. We analyze the effect of this accuracy in Section 6. 6 Results and Analysis Using the RTE-5 Search data, we compare BIUTEE in its baseline configuration (cf. Section 4), denoted BIU-BL, with its discourse-aware enhancement (BIU-DISC) which uses all the components described in Section 5. To alleviate the strong IR effect described in Section 3, both systems are applied to the complete datasets (both training and test), without candidates pre-filtering. BIU-DISC uses three base-classifiers (n = 3): SVMperf (Joachims, 2006), and Nave Bayes and Logistic Regression from the WEKA package (Witten and Frank, 2005). The first among these is set as our designated classifier C?, which is used for the computation of the document-level features. SVMperf is also used for the metaclassifier. For the smoothed entailment score (cf. Section 5.3), we used b = 0.9 and N = 3. Global information is added by enriching each sentence with the highest-ranking term in the document, according to tf-idf scores (cf. Section 5.2), where document frequencies were computed based on about half a million documents from the TIPSTER corpus (Harm</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Joachims, Thorsten. 2006. Training linear SVMs in linear time. In Proc. of the ACM Conference on Knowledge Discovery and Data Mining (KDD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Yang Tang</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Answering opinion questions with random walks on graphs.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<contexts>
<context position="4949" citStr="Li et al., 2009" startWordPosition="758" endWordPosition="761">nformation is helpful and achieves a significant improvement of results. We analyze the contribution of each component and suggest some future work to better attend to discourse in entailment systems. To our knowledge, this is the most extensive effort thus far to empirically explore the effect of discourse on entailment systems. 2 Background Discourse plays a key role in text understanding applications such as question answering or information extraction. Yet, such applications typically only handle a narrow aspect of discourse, addressing coreference by term substitution (Dali et al., 2009; Li et al., 2009). The limited coverage and scope of existing tools for coreference resolution and the unavailability of tools for addressing other discourse aspects also contribute to this situation. For instance, VP anaphora and bridging relations are usually not handled at all by such resolvers. A similar situation is seen in the TE research field. The prominent benchmark for entailment systems evaluation is the series of RTE challenges. The main task in these challenges has traditionally been to determine, given a text-hypothesis pair (T,H), whether T entails H. Discourse played no role in the first two RT</context>
</contexts>
<marker>Li, Tang, Huang, Zhu, 2009</marker>
<rawString>Li, Fangtao, Yang Tang, Minlie Huang, and Xiaoyan Zhu. 2009. Answering opinion questions with random walks on graphs. In Proc. of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Litkowski</author>
</authors>
<title>Overlap analysis in textual entailment recognition.</title>
<date>2009</date>
<booktitle>In Proc. of TAC.</booktitle>
<contexts>
<context position="6648" citStr="Litkowski, 2009" startWordPosition="1020" endWordPosition="1021">le tools such as JavaRap (Qiu et al., 2004) or OpenNLP1, e.g. (Bar-Haim et al., 2008). A major step in the RTE challenges towards a more practical setting of text processing applications occurred with the introduction of the Search task in the Fifth RTE challenge (RTE-5). In this task entailing sentences are situated within documents and depend on other sentences for their correct interpretation. Thus, discourse becomes a substantial factor impacting inference. Surprisingly, discourse hardly received any treatment in this task beyond the standard use of coreference resolution (Castillo, 2009; Litkowski, 2009), and an attempt to address globally-known information by removing from H words that appear in document headlines (Clark and Harrison, 2009). 3 The RTE Search Task The RTE-5 Search task was derived from the TAC Summarization task2. The dataset consists of several corpora, each comprised of news articles concerning a specific topic, such as the impact of global warming on the Arctic or the London terrorist attacks in 2005. Hypotheses were manually generated based on Summary Content Units (Nenkova et al., 2007), clause-long statements taken from manual summaries of the corpora. Texts are unmodif</context>
</contexts>
<marker>Litkowski, 2009</marker>
<rawString>Litkowski, Ken. 2009. Overlap analysis in textual entailment recognition. In Proc. of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew MacKinlay</author>
<author>Timothy Baldwin</author>
</authors>
<title>A baseline approach to the RTE5 search pilot.</title>
<date>2009</date>
<booktitle>In Proc. of TAC.</booktitle>
<contexts>
<context position="8418" citStr="MacKinlay and Baldwin, 2009" startWordPosition="1294" endWordPosition="1297">exts in a corpus, thus better simulates practical settings. The task may seem as a variant of information retrieval (IR), as it requires finding specific texts in a corpus. Yet, it is fundamentally different from IR for two reasons. First, the target output is a set 1 http://opennlp.sourceforge.net 2 http://www.nist.gov/tac/2009/Summarization/ 771 \x0cof sentences, each evaluated independently, rather than a set of documents. Second, the decision criterion is entailment rather than relevance. Despite the above, apparently, IR techniques provided hard-to-beat baselines for the RTE Search task (MacKinlay and Baldwin, 2009), outperforming every other system that relied on inference without IR-based pre-filtering. At the current state of performance of entailment systems, it seems that lexical coverage largely overshadows any other approach in this task. Still, most (6 out of 8) participants in the challenge applied their entailment systems to the entire dataset without a prior retrieval of candidate sentences. F1 scores for such systems vary between 10% and 33%, in comparison to over 40% of the IR-based methods. 4 The Baseline RTE System In this work we used BIUTEE, Bar-Ilan University Textual Entailment Engine </context>
</contexts>
<marker>MacKinlay, Baldwin, 2009</marker>
<rawString>MacKinlay, Andrew and Timothy Baldwin. 2009. A baseline approach to the RTE5 search pilot. In Proc. of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Roy Bar-Haim</author>
<author>Jonathan Berant</author>
</authors>
<title>Ido Dagan Eyal Shnarch, Asher Stern, and Idan Szpektor.</title>
<date>2009</date>
<booktitle>In Proc. of TAC.</booktitle>
<contexts>
<context position="30404" citStr="Mirkin et al., 2009" startWordPosition="4961" endWordPosition="4964"> a single experiment to compare BIU-DISCnoloc and BIU-BL3 under a candidate retrieval setting, using k = 20, where both systems highly perform. We compare these results to the highest score obtained by Lucene, as well as to the two best submissions to the RTE-5 Search task6. BIU-DISCnoloc outperforms all other methods and its result is significantly better than BIU-BL3 with p &lt; 0.01 according to McNemars test. 4 This time, for global information, the documents three highest ranking terms were added to each sentence. 5 http://lucene.apache.org 6 The best one is an earlier version of this work (Mirkin et al., 2009); the second is MacKinlay and Baldwins (2009). P (%) R (%) F1 (%) BIU-DISCnoloc 50.77 45.12 47.78 BIU-BL3 51.68 40.38 45.33 Lucene, top-15 35.93 52.50 42.66 RTE-5 best 40.98 51.38 45.59 RTE-5 second-best 42.94 38.00 40.32 Table 5: Performance of best configurations. 7 Conclusions While it is generally assumed that discourse interacts with semantic entailment inference, the concrete impacts of discourse on such inference have been hardly explored. This paper presented a first empirical investigation of discourse processing aspects related to entailment. We argue that available discourse process</context>
</contexts>
<marker>Mirkin, Bar-Haim, Berant, 2009</marker>
<rawString>Mirkin, Shachar, Roy Bar-Haim, Jonathan Berant, Ido Dagan Eyal Shnarch, Asher Stern, and Idan Szpektor. 2009. Addressing discourse and document structure in the RTE search task. In Proc. of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Ido Dagan</author>
<author>Sebastian Pado</author>
</authors>
<title>Assessing the role of discourse references in entailment inference.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="28013" citStr="Mirkin et al., 2010" startWordPosition="4547" endWordPosition="4550"> than the crude addition of the term to the sentence. Analysis of augmented coreferences We analyzed the performance of the component for augmenting coreference relations relative to the OpenNLP resolver. Recall that our component works on top of the resolvers output and can add or remove coreference relations. As a complete annotation of coreference chains in the dataset is unavailable, we performed the following evaluation. Recall is computed based on the number of identified pairs from a sample of 100 intradocument coreference and bridging relations from the annotated dataset described in (Mirkin et al., 2010). Precision is computed based on 50 pairs sampled from the output of each method, equally distributed over topics. The results, shown in Table 4, indicate the much higher recall obtained by our component at some cost in precision. Although rather simple, the ablation test of this component shows its usefulness. Still, both methods achieve low absolute recall, suggesting the need for more robust tools for this task. P (%) R (%) F1 (%) OpenNLP 74 16 26.3 Augmented coref. 60 28 38.2 Table 4: Performance of coreference methods. 776 \x0c0 5 10 15 20 25 30 35 40 45 50 0 10 20 30 40 50 60 70 80 90 10</context>
</contexts>
<marker>Mirkin, Dagan, Pado, 2010</marker>
<rawString>Mirkin, Shachar, Ido Dagan, and Sebastian Pado. 2010. Assessing the role of discourse references in entailment inference. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
<author>Kathleen Mckeown</author>
</authors>
<title>The pyramid method: incorporating human content selection variation in summarization evaluation.</title>
<date>2007</date>
<booktitle>In ACM Transactions on Speech and Language Processing.</booktitle>
<contexts>
<context position="7162" citStr="Nenkova et al., 2007" startWordPosition="1104" endWordPosition="1107"> treatment in this task beyond the standard use of coreference resolution (Castillo, 2009; Litkowski, 2009), and an attempt to address globally-known information by removing from H words that appear in document headlines (Clark and Harrison, 2009). 3 The RTE Search Task The RTE-5 Search task was derived from the TAC Summarization task2. The dataset consists of several corpora, each comprised of news articles concerning a specific topic, such as the impact of global warming on the Arctic or the London terrorist attacks in 2005. Hypotheses were manually generated based on Summary Content Units (Nenkova et al., 2007), clause-long statements taken from manual summaries of the corpora. Texts are unmodified sentences in the articles. Given a topic and a hypothesis, entailment systems are required to identify all sentences in the topics corpus that entail the hypothesis. Each sentence-hypothesis pair in both the development and test sets was annotated, judging whether the sentence entails the hypothesis. Out of 20,104 annotations in the development set, only 810 were judged as positive. This small ratio (4%) of positive examples, in comparison to 50% in traditional RTE tasks, better corresponds to the natural</context>
</contexts>
<marker>Nenkova, Passonneau, Mckeown, 2007</marker>
<rawString>Nenkova, Ani, Rebecca Passonneau, and Kathleen Mckeown. 2007. The pyramid method: incorporating human content selection variation in summarization evaluation. In ACM Transactions on Speech and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Qiu</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>A public reference implementation of the RAP anaphora resolution algorithm.</title>
<date>2004</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="6075" citStr="Qiu et al., 2004" startWordPosition="932" endWordPosition="935">xt-hypothesis pair (T,H), whether T entails H. Discourse played no role in the first two RTE challenges as Ts were constructed of short simplified texts. In RTE-3 (Giampiccolo et al., 2007), where some paragraph-long texts were included, intersentential relations became relevant for correct inference. Yet the texts in the task were manually modified to ensure they are self-contained. Consequently, little effort was invested by the challenges participants to address discourse issues beyond the standard substitution of coreferring nominal phrases, using publicly available tools such as JavaRap (Qiu et al., 2004) or OpenNLP1, e.g. (Bar-Haim et al., 2008). A major step in the RTE challenges towards a more practical setting of text processing applications occurred with the introduction of the Search task in the Fifth RTE challenge (RTE-5). In this task entailing sentences are situated within documents and depend on other sentences for their correct interpretation. Thus, discourse becomes a substantial factor impacting inference. Surprisingly, discourse hardly received any treatment in this task beyond the standard use of coreference resolution (Castillo, 2009; Litkowski, 2009), and an attempt to address</context>
</contexts>
<marker>Qiu, Kan, Chua, 2004</marker>
<rawString>Qiu, Long, Min-Yen Kan, and Tat-Seng Chua. 2004. A public reference implementation of the RAP anaphora resolution algorithm. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorenza Romano</author>
<author>Milen Kouylekov</author>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Investigating a generic paraphrase-based approach for relation extraction.</title>
<date>2006</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="1578" citStr="Romano et al., 2006" startWordPosition="227" endWordPosition="230">inference, and should be better addressed by future entailment systems. 1 Introduction This paper investigates the problem of recognising textual entailment within discourse. Textual Entailment (TE) is a generic framework for applied semantic inference (Dagan et al., 2009). Under TE, the relationship between a text (T) and a textual assertion (hypothesis, H) is defined such that T entails H if humans reading T would infer that H is most likely true (Dagan et al., 2006). TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al., 2006) and question answering (Harabagiu and Hickl, 2006). Yet, most entailment systems have thus far paid little attention to discourse aspects of inference. In part, this is the result of the unavailability of adept tools for handling the kind of discourse processing required for inference. In addition in the main TE benchmarks, the Recognising Textual Entailment (RTE) challenges, discourse played little role. This state of affairs has started to change with the recent introduction of the RTE Pilot Search task (Bentivogli et al., 2009b), in which assessed texts are situated within complete documen</context>
</contexts>
<marker>Romano, Kouylekov, Szpektor, Dagan, 2006</marker>
<rawString>Romano, Lorenza, Milen Kouylekov, Idan Szpektor, and Ido Dagan. 2006. Investigating a generic paraphrase-based approach for relation extraction. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques, 2nd Edition.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="21563" citStr="Witten and Frank, 2005" startWordPosition="3501" endWordPosition="3504">putation. We analyze the effect of this accuracy in Section 6. 6 Results and Analysis Using the RTE-5 Search data, we compare BIUTEE in its baseline configuration (cf. Section 4), denoted BIU-BL, with its discourse-aware enhancement (BIU-DISC) which uses all the components described in Section 5. To alleviate the strong IR effect described in Section 3, both systems are applied to the complete datasets (both training and test), without candidates pre-filtering. BIU-DISC uses three base-classifiers (n = 3): SVMperf (Joachims, 2006), and Nave Bayes and Logistic Regression from the WEKA package (Witten and Frank, 2005). The first among these is set as our designated classifier C?, which is used for the computation of the document-level features. SVMperf is also used for the metaclassifier. For the smoothed entailment score (cf. Section 5.3), we used b = 0.9 and N = 3. Global information is added by enriching each sentence with the highest-ranking term in the document, according to tf-idf scores (cf. Section 5.2), where document frequencies were computed based on about half a million documents from the TIPSTER corpus (Harman, 1992). The set of weights ~ equals {2, 1, 4} for title terms, proper names and loca</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Witten, Ian H. and Eibe Frank. 2005. Data Mining: Practical machine learning tools and techniques, 2nd Edition. Morgan Kaufmann, San Francisco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>