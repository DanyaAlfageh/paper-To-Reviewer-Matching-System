 Head word As the head word of an entity plays an important role in information extraction (CITATIONa; CITATION), it is in6 http://www,,
 NEs are objects that can be referred by names CITATION, such as people, organizations, and locations,,
 A NER sys901 \x0ctem often builds some generative/discriminative model, then, either uses only one classifier CITATION or combines many classifiers using some heuristics CITATION,,
 To the best of our knowledge, reranking has not been applied to NER except for the reranking algorithms defined in (CITATIONb; CITATIONa), which only targeted the entity detection (and not entity classification) task,,
 (CITATIONb; CITATIONa) but the employed algorithm just exploited arbitrary information regarding word types and linguistic patterns,,
 In such cases, neither global features CITATION nor aggregated contexts CITATION can help,,
 In such cases, neither global features CITATION nor aggregated contexts CITATION can help,,
 An example is the tree kernel CITATION, where the objects are syntactic trees that encode grammatical derivations and the kernel function computes the number of common subtrees,,
 Similarly, sequence kernels CITATION count the number of common subsequences shared by two input strings,,
 NEs are objects that can be referred by names CITATION, such as people, organizations, and locations,,
 A NER sys901 \x0ctem often builds some generative/discriminative model, then, either uses only one classifier CITATION or combines many classifiers using some heuristics CITATION,,
 Various algorithms have been applied for reranking in NLP applications (CITATION; CITATION; CITATIONb; CITATION), including parsing, name tagging and machine translation,,
 Reranking appears extremely interesting if coupled with kernel methods (CITATION; CITATION; CITATION), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies,,
 An example is the tree kernel CITATION, where the objects are syntactic trees that encode grammatical derivations and the kernel function computes the,,
 The parse tree kernel CITATION and string kernel CITATION are examples of the well-known convolution kernels used in various NLP tasks,,
 At testing time the hypothesis receiving the highest score is selected CITATION,,
 Various algorithms have been applied for reranking in NLP applications (CITATION; CITATION; CITATIONb; CITATION), including parsing, name tagging and machine translation,,
 Reranking appears extremely interesting if coupled with kernel methods (CITATION; CITATION; CITATION), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies,,
 Various algorithms have been applied for reranking in NLP applications (CITATION; CITATION; CITATIONb; CITATION), including parsing, name tagging and machine translation,,
 Reranking appears extremely interesting if coupled with kernel methods (CITATION; CITATION; CITATION), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies,,
 A NER sys901 \x0ctem often builds some generative/discriminative model, then, either uses only one classifier CITATION or combines many classifiers using some heuristics CITATION,,
 To the best of our knowledge, reranking has not been applied to NER except for the reranking algorithms defined in (CITATIONb; CITATIONa), which only targeted the entity detection (and not entity classification) task,,
3 Global features Mixed n-grams features In previous works, some global features have been used (CITATIONb; CITATIONa) but the employed algorithm just exploited arbitrary information regarding word types and linguistic patterns,,
 Various algorithms have been applied for reranking in NLP applications (CITATION; CITATION; CITATIONb; CITATION), including parsing, name tagging and machine translation,,
 Reranking appears extremely interesting if coupled with kernel methods (CITATION; CITATION; CITATION), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies,,
 A NER sys901 \x0ctem often builds some generative/discriminative model, then, either uses only one classifier CITATION or combines many classifiers using some heuristics CITATION,,
 To the best of our knowledge, reranking has not been applied to NER except for the reranking algorithms defined in (CITATIONb; CITATIONa), which only targeted the entity detection (and not entity classification) task,,
3 Global features Mixed n-grams features In previous works, some global features have been used (CITATIONb; CITATIONa) but the employed algorithm just exploited arbitrary information regarding word types and linguistic patterns,,
 Various algorithms have been applied for reranking in NLP applications (CITATION; CITATION; CITATIONb; CITATION), including parsing, name tagging and machine translation,,
 Reranking appears extremely interesting if coupled with kernel methods (CITATION; CITATION; CITATION), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies,,
 An example is the tree kernel CITATION, where the objects are syntactic trees that encode grammatical deriv,,
 A NER sys901 \x0ctem often builds some generative/discriminative model, then, either uses only one classifier CITATION or combines many classifiers using some heuristics CITATION,,
 To the best of our knowledge, reranking has not been applied to NER except for the reranking algorithms defined in (CITATIONb; CITATIONa), which only targeted the entity detection (and not entity classification) task,,
 Various algorithms have been applied for reranking in NLP applications (CITATION; CITATION; CITATIONb; CITATION), including parsing, name tagging and machine translation,,
 Reranking appears extremely interesting if coupled with kernel methods (CITATION; CITATION; CITATION), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies,,
2 The baseline algorithm We selected Conditional Random Fields CITATION as the baseline model,,
 An example is the tree kernel CITATION, where the objects are syntactic trees that encode grammatical derivations and the kernel function computes the number of common subtrees,,
 Similarly, sequence kernels CITATION count the number of common subsequences shared by two input strings,,
 NEs are objects that can be referred by names CITATION, such as people, organizations, and locations,,
 The parse tree kernel CITATION and string kernel CITATION are examples of the well-known convolution kernels used in various NLP tasks,,
 The EVALITA 2009 Italian dataset is based on I-CAB, the Italian Content Annotation Bank CITATION, annotated with four entity types: Person (PER), Organization (ORG), Geo-Political Entity (GPE) and Location (LOC),,
 Various algorithms have been applied for reranking in NLP applications (CITATION; CITATION; CITATIONb; CITATION), including parsing, name tagging and machine translation,,
 Reranking appears extremely interesting if coupled with kernel methods (CITATION; CITATION; CITATION), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies,,
 An example is the tree kernel CITATION, where the objects are syntactic trees that encode grammatical derivations and the ke,,
 For sake of space, we do not report the mathematical description of them, which is available in CITATION, CITATION and CITATION, respectively,,
 The distinction is that an SST must be generated by applying the same grammatical rule set which generated the original tree, as pointed out in CITATION,,
 positive and negative examples, SVMs find a separating hyperplane H(~ x) = ~ ~ x + b = 0 where Rn and b R are learned by applying the Structural Risk Minimization principle CITATION,,
 by means of the one-vs-all method CITATION,,
4 Kernel methods Kernel methods CITATION are an attractive alternative to feature-based methods since the applied learning algorithm only needs to compute a product between a pair of objects (by means of kernel functions), avoiding the explicit feature representation,,
 positive and negative examples, SVMs find a separating hyperplane H(~ x) = ~ ~ x + b = 0 where Rn and b R are learned by applying the Structural Risk Minimization principle CITATION,,
 by means of the one-vs-all method CITATION,,
4 Kernel methods Kernel methods CITATION are an attractive alternative to feature-based methods since the applied learning algorithm only needs to compute a product between a pair of objects (by means of kernel functions), avoiding the explicit feature representation,,
 Various algorithms have been applied for reranking in NLP applications (CITATION; CITATION; CITATIONb; CITATION), including parsing, name tagging and machine translation,,
 Reranking appears extremely interesting if coupled with kernel methods (CITATION; CITATION; CITATION), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies,,
 Head word As the head word of an entity plays an important role in information extraction (CITATIONa; CITATION), it is in6 http://www,,
 positive and negative examples, SVMs find a separating hyperplane H(~ x) = ~ ~ x + b = 0 where Rn and b R are learned by applying the Structural Risk Minimization principle CITATION,,
 by means of the one-vs-all method CITATION,,
4 Kernel methods Kernel methods CITATION are an attractive alternative to feature-based methods since the applied learning algorithm only needs to compute a product between a pair of objects (by means of kernel functions), avoiding the e,,
 For sake of space, we do not report the mathematical description of them, which is available in CITATION, CITATION and CITATION, respectively,,
 The distinction is that an SST must be generated by applying the same grammatical rule set which generated the original tree, as pointed out in CITATION,,
33 CITATION 84,,
 To better collocate our results with previous work, we report the best NER outcome on the Italian CITATION and the English (Ratinov and Roth, ) datasets, in the last row (in italic) of each table,,
