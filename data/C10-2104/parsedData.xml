<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.275049">
b&amp;apos;Coling 2010: Poster Volume, pages 901909,
</note>
<address confidence="0.517862">
Beijing, August 2010
</address>
<title confidence="0.8205">
Kernel-based Reranking for Named-Entity Extraction
</title>
<author confidence="0.873865">
Truc-Vien T. Nguyen and Alessandro Moschitti and Giuseppe Riccardi
</author>
<affiliation confidence="0.993252">
Department of Information Engineering and Computer Science
University of Trento
</affiliation>
<email confidence="0.988438">
nguyenthi,moschitti,riccardi@disi.unitn.it
</email>
<sectionHeader confidence="0.990521" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999200904761905">
We present novel kernels based on struc-
tured and unstructured features for rerank-
ing the N-best hypotheses of conditional
random fields (CRFs) applied to entity ex-
traction. The former features are gener-
ated by a polynomial kernel encoding en-
tity features whereas tree kernels are used
to model dependencies amongst tagged
candidate examples. The experiments on
two standard corpora in two languages,
i.e. the Italian EVALITA 2009 and the En-
glish CoNLL 2003 datasets, show a large
improvement on CRFs in F-measure, i.e.
from 80.34% to 84.33% and from 84.86%
to 88.16%, respectively. Our analysis re-
veals that both kernels provide a compara-
ble improvement over the CRFs baseline.
Additionally, their combination improves
CRFs much more than the sum of the indi-
vidual contributions, suggesting an inter-
esting kernel synergy.
</bodyText>
<sectionHeader confidence="0.997887" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998656625">
Reranking is a promising computational frame-
work, which has drawn special attention in the
Natural Language Processing (NLP) community.
Basically, this method first employs a probabilis-
tic model to generate a list of top-n candidates and
then reranks this n-best list with additional fea-
tures. One appeal of this approach is its flexibility
of incorporating arbitrary features into a model.
These features help in discriminating good from
bad hypotheses and consequently their automatic
learning. Various algorithms have been applied
for reranking in NLP applications (Huang, 2008;
Shen et al., 2004; Collins, 2002b; Collins and
Koo, 2000), including parsing, name tagging and
machine translation. This work has exploited the
disciminative property as one of the key criterion
of the reranking algorithm.
Reranking appears extremely interesting if cou-
pled with kernel methods (Dinarelli et al., 2009;
Moschitti, 2004; Collins and Duffy, 2001), as the
latter allow for extracting from the ranking hy-
potheses a huge amount of features along with
their dependencies. Indeed, while feature-based
learning algorithms involve only the dot-product
between feature vectors, kernel methods allow
for a higher generalization by replacing the dot-
product with a function between pairs of linguis-
tic objects. Such functions are a kind of similarity
measure satisfying certain properties. An exam-
ple is the tree kernel (Collins and Duffy, 2001),
where the objects are syntactic trees that encode
grammatical derivations and the kernel function
computes the number of common subtrees. Simi-
larly, sequence kernels (Lodhi et al., 2002) count
the number of common subsequences shared by
two input strings.
Named-entities (NEs) are essential for defin-
ing the semantics of a document. NEs are ob-
jects that can be referred by names (Chinchor and
Robinson, 1998), such as people, organizations,
and locations. The research on NER has been
promoted by the Message Understanding Con-
ferences (MUCs, 1987-1998), the shared task of
the Conference on Natural Language Learning
(CoNLL, 2002-2003), and the Automatic Content
Extraction program (ACE, 2002-2005). In the lit-
erature, there exist various learning approaches
to extract named-entities from text. A NER sys-
</bodyText>
<page confidence="0.991033">
901
</page>
<bodyText confidence="0.996640871794872">
\x0ctem often builds some generative/discriminative
model, then, either uses only one classifier (Car-
reras et al., 2002) or combines many classifiers us-
ing some heuristics (Florian et al., 2003).
To the best of our knowledge, reranking has
not been applied to NER except for the rerank-
ing algorithms defined in (Collins, 2002b; Collins,
2002a), which only targeted the entity detection
(and not entity classification) task. Besides, since
kernel methods offer a natural way to exploit lin-
guistic properties, applying kernels for NE rerank-
ing is worthwhile.
In this paper, we describe how kernel methods
can be applied for reranking, i.e. detection and
classification of named-entities, in standard cor-
pora for Italian and English. The key aspect of
our reranking approach is how structured and flat
features can be employed in discriminating candi-
date tagged sequences. For this purpose, we apply
tree kernels to a tree structure encoding NE tags of
a sentence and combined them with a polynomial
kernel, which efficiently exploits global features.
Our main contribution is to show that (a) tree
kernels can be used to define general features (not
merely syntactic) and (b) using appropriate al-
gorithms and features, reranking can be very ef-
fective for named-entity recognition. Our study
demonstrates that the composite kernel is very
effective for reranking named-entity sequences.
Without the need of producing and heuristically
combining learning models like previous work on
NER, the composite kernel not only captures most
of the flat features but also efficiently exploits
structured features. More interestingly, this kernel
yields significant improvement when applied to
two corpora of two different languages. The eval-
uation in the Italian corpus shows that our method
outperforms the best reported methods whereas on
the English data it reaches the state-of-the-art.
</bodyText>
<sectionHeader confidence="0.987342" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.947257">
2.1 The data
</subsectionHeader>
<bodyText confidence="0.998383451612903">
Different languages exhibit different linguistic
phenomena and challenges. A robust NER sys-
tem is expected to be well-adapted to multiple
domains and languages. Therefore, we experi-
mented with two datasets: the EVALITA 2009
Italian corpus and the well-known CoNLL 2003
English shared task corpus.
The EVALITA 2009 Italian dataset is based
on I-CAB, the Italian Content Annotation
Bank (Magnini et al., 2006), annotated with four
entity types: Person (PER), Organization (ORG),
Geo-Political Entity (GPE) and Location (LOC).
The training data, taken from the local newspa-
per LAdige, consists of 525 news stories which
belong to five categories: News Stories, Cultural
News, Economic News, Sports News and Local
News. Test data, on the other hand, consist of
completely new data, taken from the same news-
paper and consists of 180 news stories.
The CoNLL 2003 English dataset is created
within the shared task of CoNLL-2003 (Sang
and Meulder, 2003). It is a collection of news
wire articles from the Reuters Corpus, annotated
with four entity types: Person (PER), Location
(LOC), Organization (ORG) and Miscellaneous
name (MISC). The training and the development
datasets are news feeds from August 1996, while
the test set contains news feeds from December
1996. Accordingly, the named entities in the test
dataset are considerably different from those that
appear in the training or the development set.
</bodyText>
<table confidence="0.995322882352941">
Italian GPE LOC ORG PER
Train
2813 362 3658 4577
24.65% 3.17% 32.06% 40.11%
Test
1143 156 1289 2378
23.02% 3.14% 25.96% 47.89%
English LOC MISC ORG PER
Train
7140 3438 6321 6600
30.38% 14.63% 26.90% 28.09%
Dev
1837 922 1341 1842
30.92% 15.52% 22.57% 31.00%
Test
1668 702 1661 1617
29.53% 12.43% 29.41% 28.63%
</table>
<tableCaption confidence="0.980912">
Table 1: Statistics on the Italian EVALITA 2009
</tableCaption>
<table confidence="0.246537">
and English CoNLL 2003 corpora.
</table>
<subsectionHeader confidence="0.992383">
2.2 The baseline algorithm
</subsectionHeader>
<bodyText confidence="0.997156">
We selected Conditional Random Fields (Lafferty
et al., 2001) as the baseline model. Conditional
</bodyText>
<page confidence="0.995294">
902
</page>
<bodyText confidence="0.998798263157895">
\x0crandom fields (CRFs) are a probabilistic frame-
work for labeling and segmenting sequence data.
They present several advantages over other purely
generative models such as Hidden Markov models
(HMMs) by relaxing the independence assump-
tions required by HMMs. Besides, HMMs and
other discriminative Markov models are prone to
the label bias problem, which is effectively solved
by CRFs.
The named-entity recognition (NER) task is
framed as assigning label sequences to a set of
observation sequences. We follow the IOB nota-
tion where the NE tags have the format B-TYPE,
I-TYPE or O, which mean that the word is a be-
ginning, a continuation of an entity, or not part of
an entity at all. For example, consider the sentence
with their corresponding NE tags, each word is la-
beled with a tag indicating its appropriate named-
entity, resulting in annotated text, such as:
</bodyText>
<equation confidence="0.6728815">
Il/O presidente/O della/O Fifa/B-ORG Sepp/B-PER
Blatter/I-PER affermando/O che/O il/O torneo/O era/O
stato/O ottimo/O (FIFA president Sepp Blatter says that the
tournament was excellent)
</equation>
<bodyText confidence="0.986701333333333">
For our experiments, we used CRF++ 1 to build
our recognizer, which is a model trained discrim-
inatively with the unigram and bigram features.
These are extracted from a window at k words
centered in the target word w (i.e. the one we want
to classify with the B, O, I tags). More in detail
such features are:
The word itself, its prefixes, suffixes, and
part-of-speech
Orthographic/Word features. These are
binary and mutually exclusive features that
test whether a word contains all upper-cased,
initial letter upper-cased, all lower-cased,
roman-number, dots, hyphens, acronym,
lonely initial, punctuation mark, single-char,
and functional-word.
Gazetteer features. Class (geographical,
first name, surname, organization prefix, lo-
cation prefix) of words in the window.
Left Predictions. The predicted tags on the
left of the word in the current classification.
</bodyText>
<page confidence="0.584341">
1
</page>
<bodyText confidence="0.951970909090909">
http://crfpp.sourceforge.net
The gazetteer lists are built with names im-
ported from different sources. For English, the
geographic features are imported from NIMAs
GEOnet Names Server (GNS)2, The Alexandria
Digital Library (ADL) gazetteer3. The company
data is included with all the publicly traded com-
panies listed in Google directory4, the European
business directory5. For Italian, the generic proper
nouns are extracted from Wikipedia and various
Italian sites.
</bodyText>
<subsectionHeader confidence="0.999601">
2.3 Support Vector Machines (SVMs)
</subsectionHeader>
<bodyText confidence="0.941042833333333">
Support Vector Machines refer to a supervised
machine learning technique based on the latest re-
sults of the statistical learning theory. Given a
vector space and a set of training points, i.e. posi-
tive and negative examples, SVMs find a separat-
ing hyperplane H(~
</bodyText>
<equation confidence="0.997373666666667">
x) = ~
~
x + b = 0 where
</equation>
<bodyText confidence="0.966798">
Rn and b R are learned by applying the
Structural Risk Minimization principle (Vapnik,
1998). SVMs are a binary classifier, but they can
be easily extended to multi-class classifier, e.g. by
means of the one-vs-all method (Rifkin and Pog-
gio, 2002).
One strong point of SVMs is the possibility to
apply kernel methods to implicitly map data in
a new space where the examples are more easily
separable as described in the next section.
</bodyText>
<subsectionHeader confidence="0.982914">
2.4 Kernel methods
</subsectionHeader>
<bodyText confidence="0.980133846153846">
Kernel methods (Scholkopf and Smola, 2001) are
an attractive alternative to feature-based methods
since the applied learning algorithm only needs
to compute a product between a pair of objects
(by means of kernel functions), avoiding the ex-
plicit feature representation. A kernel function
is a scalar product in a possibly unknown feature
space. More precisely, The object o is mapped in
~
x with a feature function : O &lt;n, where O
is the set of the objects.
The kernel trick allows us to rewrite the deci-
sion hyperplane as:
</bodyText>
<equation confidence="0.962879555555556">
H(~
x) =
\x10 X
i=1..l
yii~
xi
\x11
~
x + b =
</equation>
<page confidence="0.375999">
2
</page>
<figure confidence="0.872889">
http://www.nima.mil/gns/html
3
http://www.alexandria.ucsb.edu
4
http://directory.google.com/Top/Business
5
http://www.europages.net
</figure>
<page confidence="0.978997">
903
</page>
<equation confidence="0.99704">
\x0cX
i=1..l
yii~
xi ~
x + b =
X
i=1..l
yii(oi) (o) + b,
</equation>
<bodyText confidence="0.928536352941177">
where yi is equal to 1 for positive and -1 for
negative examples, i &lt; with i 0, oi
i {1, .., l} are the training instances and the
product K(oi, o) = h(oi) (o)i is the kernel
function associated with the mapping .
Kernel engineering can be carried out by com-
bining basic kernels with additive or multiplica-
tive operators or by designing specific data objects
(vectors, sequences and tree structures) for the tar-
get tasks.
Regarding NLP applications, kernel methods
have attracted much interest due to the ability of
implicitly exploring huge amounts of structural
features. The parse tree kernel (Collins and Duffy,
2001) and string kernel (Lodhi et al., 2002) are
examples of the well-known convolution kernels
used in various NLP tasks.
</bodyText>
<subsectionHeader confidence="0.896767">
2.5 Tree Kernels
</subsectionHeader>
<bodyText confidence="0.99766362962963">
Tree kernels represent trees in terms of their sub-
structures (called tree fragments). Such fragments
form a feature space which, in turn, is mapped into
a vector space. Tree kernels measure the similar-
ity between pair of trees by counting the number
of fragments in common. There are three impor-
tant characterizations of fragment type: the Sub-
Trees (ST), the SubSet Trees (SST) and the Partial
Trees (PT). For sake of space, we do not report the
mathematical description of them, which is avail-
able in (Vishwanathan and Smola, 2002), (Collins
and Duffy, 2001) and (Moschitti, 2006), respec-
tively. In contrast, we report some descriptions in
terms of feature space that may be useful to un-
derstand the new engineered kernels.
In principle, a SubTree (ST) is defined by tak-
ing any node along with its descendants. A SubSet
Tree (SST) is a more general structure which does
not necessarily include all the descendants. The
distinction is that an SST must be generated by ap-
plying the same grammatical rule set which gen-
erated the original tree, as pointed out in (Collins
and Duffy, 2001). A Partial Tree (PT) is a more
general form of sub-structures obtained by relax-
ing constraints over the SSTs. Figure 1 shows the
overall fragment set of the ST, SST and PT kernels
for the syntactic parse tree of the sentence frag-
</bodyText>
<figureCaption confidence="0.77492">
Figure 1: Three kinds of tree kernels.
ment: gives a talk .
</figureCaption>
<bodyText confidence="0.99952525">
In the next section, we will define new struc-
tures for tagged sequences of NEs which along
with the application of the PT kernel produce in-
novative tagging kernels for reranking.
</bodyText>
<sectionHeader confidence="0.97167" genericHeader="method">
3 Reranking Method
</sectionHeader>
<subsectionHeader confidence="0.997196">
3.1 Reranking Strategy
</subsectionHeader>
<bodyText confidence="0.966167">
As a baseline we trained the CRFs model to gen-
erate 10-best candidates per sentence, along with
their probabilities. Each candidate was then rep-
resented by a semantic tree together with a feature
vector. We consider our reranking task as a binary
classification problem where examples are pairs
of hypotheses &lt; Hi, Hj &amp;gt;.
Given a sentence South African Breweries Ltd
bought stakes in the Lech and Tychy brewers and three
of its candidate tagged sequences:
</bodyText>
<table confidence="0.9845558">
H1 B-ORG I-ORG I-ORG I-ORG O O O O B-ORG O
B-ORG O (the correct sequence)
H2 B-MISC I-MISC B-ORG I-ORG O O O O B-ORG
I-ORG I-ORG O
H3 B-ORG I-ORG I-ORG I-ORG O O O O B-ORG O
</table>
<sectionHeader confidence="0.696551" genericHeader="method">
B-LOC O
</sectionHeader>
<bodyText confidence="0.997422666666667">
where B-ORG, I-ORG, B-LOC, O are the gen-
erated NE tags according to IOB notation as de-
scribed in Section 3.2.
With the above data (an original sentence to-
gether with a list of candidate tagged sequences),
the following pairs of hypotheses will be gener-
</bodyText>
<page confidence="0.977528">
904
</page>
<equation confidence="0.527648">
\x0cated &lt; H1, H2 &amp;gt;, &lt; H1, H3 &amp;gt;,&lt; H2, H1 &amp;gt; and
&lt; H3, H1 &amp;gt;, where the first two pairs are positive
</equation>
<bodyText confidence="0.9986895">
and the latter pairs are negative instances. Then a
binary classifier based on SVMs and kernel meth-
ods can be trained to discriminate between the
best hypothesis, i.e. &lt; H1 &amp;gt; and the others. At
testing time the hypothesis receiving the highest
score is selected (Collins and Duffy, 2001).
</bodyText>
<subsectionHeader confidence="0.8634645">
3.2 Representation of Tagged Sequences in
Semantic Trees
</subsectionHeader>
<bodyText confidence="0.999491540540541">
We now consider the representation that exploits
the most discriminative aspects of candidate struc-
tures. As in the case of NER, an input can-
didate is a sequence of word/tag pairs x =
{w1/t1...wn/tn} where wi is the i0th word and
ti is the i0th NE tag for that word. The first repre-
sentation we consider is the tree structure. See fig-
ure 2 as an example of candidate tagged sequence
and its semantic tree.
With the sentence South African Breweries Ltd
bought stakes in the Lech and Tychy brewers and three
of its candidate tagged sequences in the previous
section, the training algorithm considers to con-
struct a tree for each sequence, with the named-
entity tags as pre-terminals and the words as
leaves. See figure 2 for an example of the seman-
tic tree for the first tagged sequence.
With this tree representation, for a word wi, the
target NE tag would be set at parent and the fea-
tures for this word are at child nodes. This allows
us to best exploit the inner product between com-
peting candidates. Indeed, in the kernel space,
the inner product counts the number of common
subtrees thus sequences with similar NE tags are
likely to have higher score. For example, the sim-
ilarity between H1 and H3 will be higher than the
similarity of the previous hypotheses with H2; this
is reasonable since these two also have higher F1.
It is worth noting that another useful modifica-
tion is the flexibility of incorporate diverse, ar-
bitrary features into this tree structure by adding
children to the parent node that contains entity tag.
These characteristics can be exploited efficiently
with the PT kernel, which relaxes constraints of
production rules. The inner product can implicitly
include these features and deal better with sparse
data.
</bodyText>
<subsectionHeader confidence="0.766548">
3.3 Global features
Mixed n-grams features
</subsectionHeader>
<bodyText confidence="0.991615971428571">
In previous works, some global features have
been used (Collins, 2002b; Collins, 2002a) but the
employed algorithm just exploited arbitrary infor-
mation regarding word types and linguistic pat-
terns. In contrast, we define and study diverse
features by also considering n-grams patterns pre-
ceding, and following the target entity.
Complementary context
In supervised learning, NER systems often suf-
fer from low recall, which is caused by lack of
both resource and context. For example, a word
like Arkansas may not appear in the training set
and in the test set, there may not be enough con-
text to infer its NE tag. In such cases, neither
global features (Chieu and Ng, 2002) nor aggre-
gated contexts (Chieu and Ng, 2003) can help.
To overcome this deficiency, we employed the
following unsupervised procedure: first, the base-
line NER is applied to the target un-annotated cor-
pus. Second, we associate each word of the corpus
with the most frequent NE category assigned in
the previous step. Finally, the above tags are used
as features during the training of the improved
NER and also for building the feature represen-
tation for a new classification instance.
This way, for any unknown word w of the test
set, we can rely on the most probable NE category
as feature. The advantage is that we derived it by
using the average over many possible contexts of
w, which are in the different instances of the un-
nanotated corpus.
The unlabeled corpus for Italian was collected
from La Repubblica 6 and it contains over 20 mil-
lions words. Whereas the unlabeled corpus for
English was collected mainly from The New York
</bodyText>
<figureCaption confidence="0.60364">
Times 7 and BBC news stories 8 with more than
</figureCaption>
<figure confidence="0.465763272727273">
35 millions words.
Head word
As the head word of an entity plays an impor-
tant role in information extraction (Bunescu and
Mooney, 2005a; Surdeanu et al., 2003), it is in-
6
http://www.repubblica.it/
7
http://www.nytimes.com/
8
http://news.bbc.co.uk/
</figure>
<page confidence="0.923682">
905
</page>
<footnote confidence="0.7837038">
\x0cFigure 2: Semantic structure of the first sequence
cluded in the global set together with its ortho-
graphic feature. We now describe some primitives
for our global feature framework.
1. wi for i = 1 . . . n is the i0th word
2. ti is the NE tag of wi
3. gi is the gazetteer feature of the word wi
4. fi is the most frequent NE tag seen in a large
corpus of wi
5. hi is the head word of the entity. We nor-
</footnote>
<bodyText confidence="0.986774476190476">
mally set the head word of an entity as its last
word. However, when a preposition exists in
the entity string, its head word is set as the
last word before the preposition. For exam-
ple, the head word of the entity University
of Pennsylvania is University.
6. Mixed n-grams features of the words and
their gazetteers/frequent-tag before/after the
start/end of an entity. In addition to the
normal n-grams solely based on words, we
mixed words with gazetteers/frequent-tag
seen from a large corpus and create mixed
n-grams features.
Table 2 shows the full set of global features in
our reranking framework. Features are anchored
to each entity instance and adapted to entity types.
This helps to discriminate different entities with
the same surface forms. Moreover, they can be
combined with n-grams patterns to learn and ex-
plicitly push the score of the correct sequence
above the score of competing sequences.
</bodyText>
<subsectionHeader confidence="0.998449">
3.4 Reranking with Composite Kernel
</subsectionHeader>
<bodyText confidence="0.970397153846154">
In this section we describe our novel tagging ker-
nels based on diverse global features as well as
semantic trees for reranking candidate tagged se-
quences. As mentioned in the previous section,
we can engineer kernels by combining tree and
entity kernels. Thus we focus on the problem to
define structure embedding the desired relational
information among tagged sequences.
The Partial Tree Kernel
Let F = f1, f2, . . . , f|F |be a tree fragment
space of type PTs and let the indicator function
Ii(n) be equal to 1 if the target f1 is rooted at node
n and 0 otherwise, we define the PT kernel as:
</bodyText>
<equation confidence="0.983756333333333">
K(T1, T2) =
X
n1NT1
X
n2NT2
(n1, n2)
</equation>
<bodyText confidence="0.918043555555556">
where NT1 and NT2 are the set of nodes
in T1 and T2 respectively and (n1, n2) =
P|F|
i=1 Ii(n1)Ii(n2), i.e. the number of common
fragments rooted at the n1 and n2 nodes of the
type shown in Figure 1.c.
The Polynomial Kernel
The polynomial kernel between two candidate
tagged sequences is defined as:
</bodyText>
<equation confidence="0.982581166666667">
K(x, y) = (1 + ~
x1 ~
x2)2
,
where ~
x1 and ~
</equation>
<bodyText confidence="0.955404625">
x2 are two feature vectors extracted
from the two sequences with the global feature
template.
The Tagging Kernels
In our reranking framework, we incorporate the
probability from the original model with the tree
structure as well as the feature vectors. Let us con-
sider the following notations:
</bodyText>
<page confidence="0.993309">
906
</page>
<bodyText confidence="0.923922764705882">
\x0cFeature Description
ws ws+1 . . . we Entity string
gs gs+1 . . . ge The gazetteer feature within the entity
fs fs+1 . . . fe The most frequent NE tag feature (seen from a
large corpus) within the entity
hw The head word of the entity
lhw Indicates whether the head word is lower-cased
ws1 ws; ws1 gs; gs1 ws; gs1 gs Mixed bigrams of the words/gazetteer features
before/after the start of the entity
we we+1; we ge+1; ge we+1; ge ge+1 Mixed bigrams of the words/gazetteer features
before/after the end of the entity
ws1 ws; ws1 fs; fs1 ws; fs1 fs Mixed bigrams of the words/frequent-tag fea-
tures before/after the start of the entity
we we+1; we fe+1; fe we+1; fe fe+1 Mixed bigrams of the words/frequent-tag fea-
tures before/after the end of the entity
ws2 ws1 ws; ws1 ws ws+1; we1 we we+1; we2 we1 we Trigram features of the words before/after the
start/end of the entity
</bodyText>
<equation confidence="0.98186175">
ws2 ws1 gs; ws2 gs1 ws; ws2 gs1 gs;
gs2 ws1 ws; gs2 ws1 gs; gs2 gs1 ws; gs2 gs1 gs;
ws1 ws gs+1; ws1 gs ws+1; ws1 gs gs+1;
gs1 ws ws+1; gs1 ws gs+1; gs1 gs ws+1; gs1 gs gs+1
</equation>
<bodyText confidence="0.976136">
Mixed trigrams of the words/gazetteer features
before/after the start of the entity
</bodyText>
<equation confidence="0.83455475">
we1 we ge+1; we1 ge we+1; we1 ge ge+1;
ge1 we we+1; ge1 we ge+1; ge1 ge we+1; ge1 ge ge+1;
we2 we1 ge; we2 ge1 we; we2 ge1 ge;
ge2 we1 we; ge2 we1 ge; ge2 ge1 we; ge2 ge1 ge
</equation>
<bodyText confidence="0.982608">
Mixed trigrams of the words/gazetteer features
before/after the end of the entity
</bodyText>
<equation confidence="0.909252">
ws2 ws1 fs; ws2 fs1 ws; ws2 fs1 fs;
fs2 ws1 ws; fs2 ws1 fs; fs2 fs1 ws; fs2 fs1 fs;
ws1 ws fs+1; ws1 fs ws+1; ws1 fs fs+1;
fs1 ws ws+1; fs1 ws fs+1; fs1 fs ws+1; fs1 fs fs+1
</equation>
<bodyText confidence="0.9149975">
Mixed trigrams of the words/frequent-tag fea-
tures before/after the start of the entity
</bodyText>
<equation confidence="0.58982125">
we1 we fe+1; we1 fe we+1; we1 fe fe+1;
fe1 we we+1; fe1 we fe+1; fe1 fe we+1; fe1 fe fe+1;
we2 we1 fe; we2 fe1 we; we2 fe1 fe;
fe2 we1 we; fe2 we1 fe; fe2 fe1 we; fe2 fe1 fe
</equation>
<bodyText confidence="0.8971981875">
Mixed trigrams of the words/frequent-tag fea-
tures before/after the end of the entity
Table 2: Global features in the entity kernel for reranking. These features are anchored for each entity
instance and adapted to entity categories. For example, the entity string (first feature) of the entity
United Nations with entity type ORG is ORG United Nations.
K(x, y) = L(x) L(y) is the basic kernel
where L(x) is the log probability of a can-
didate tagged sequence x under the original
probability model.
TK(x, y) = t(x)t(y) is the partial tree ker-
nel under the structure representation
FK(x, y) = f(x) f(y) is the polynomial
kernel under the global features
The tagging kernels between two tagged se-
quences are defined in the following combina-
tions:
</bodyText>
<listItem confidence="0.998651333333333">
1. CTK = K + (1 ) TK
2. CFK = K + (1 ) FK
3. CTFK = K + (1 ) (TK + FK)
</listItem>
<bodyText confidence="0.9965">
where , , are parameters weighting the two
participating terms. Experiments on the validation
set showed that these combinations yield the best
performance with = 0.2 for both languages,
= 0.4 for English and = 0.3 for and Italian,
= 0.24 for English and = 0.2 for Italian.
</bodyText>
<sectionHeader confidence="0.988233" genericHeader="evaluation">
4 Experimens and Results
</sectionHeader>
<subsectionHeader confidence="0.920144">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99869325">
As a baseline we trained the CRFs classifier on
the full training portion (11,227 sentences in the
Italian and 14,987 sentences in the English cor-
pus). In developing a reranking strategy for both
English and Italian, the training data was split into
5 sections, and in each case the baseline classifier
was trained on 4/5 of the data, then used to decode
the remaining 1/5.
</bodyText>
<page confidence="0.959804">
907
</page>
<bodyText confidence="0.913733555555555">
\x0cThe top 10 hypotheses together with their log
probabilities were recovered for each training sen-
tence. Similarly, a model trained on the whole
training data was used to produce 10 hypotheses
for each sentence in the development set. For the
reranking experiments, we applied different ker-
nel setups to the two corpora described in Section
2.1. The three kernels were trained on the training
portion.
</bodyText>
<table confidence="0.972966333333333">
Italian Test P R F
CRFs 83.43 77.48 80.34
CTK 84.97 78.03 81.35
CFK 84.93 79.13 81.93
CTFK 85.99 82.73 84.33
(Zanoli et al., 2009) 84.07 80.02 82.00
English Test P R F
CRFs 85.37 84.35 84.86
CTK 87.19 84.79 85.97
CFK 86.53 86.75 86.64
CTFK 88.07 88.25 88.16
(Ratinov and Roth, ) N/A N/A 90.57
</table>
<tableCaption confidence="0.8364045">
Table 3: Reranking results of the three tagging
kernels on the Italian and English testset.
</tableCaption>
<subsectionHeader confidence="0.945076">
4.2 Discussion
</subsectionHeader>
<bodyText confidence="0.9967294">
Table 3 presents the reranking results on the test
data of both corpora. The results show a 20.29%
relative improvement in F-measure for Italian and
21.79% for English.
CFK based on unstructured features achieves
higher accuracy than CTK based on structured
features. However, the huge amount of subtrees
generated by the PT kernel may limit the expres-
sivity of some structural features, e.g. many frag-
ments may only generate noise. This problem is
less important with the polynomial kernel where
global features are tailored for individual entities.
In any case, the experiments demonstrate that
both tagging kernels CTK and CFK give im-
provement over the CRFs baseline in both lan-
guages. This suggests that structured and unstruc-
tured features are effective in discriminating be-
tween competing NE annotations.
Furthermore, the combination of the two tag-
ging kernels on both standard corpora shows a
large improvement in F-measure from 80.34% to
84.33% for Italian and from 84.86% to 88.16%
for English data. This suggests that these two ker-
nels, corresponding to two kinds of feature, com-
plement each other.
To better collocate our results with previous
work, we report the best NER outcome on the
Italian (Zanoli et al., 2009) and the English (Rati-
nov and Roth, ) datasets, in the last row (in italic)
of each table. This shows that our model outper-
forms the best Italian NER system and it is close
to the state-of-art model for English, which ex-
ploits many complex features9. Also note that we
are very close to the F1 achieved by the best sys-
tem of CoNLL 2003, i.e. 88.8.
</bodyText>
<sectionHeader confidence="0.997205" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999192388888889">
We analyzed the impact of kernel-based ap-
proaches for modeling dependencies between
tagged sequences for NER. Our study illustrates
that each individual kernel, either with structured
or with flat features clearly gives improvement to
the base model. Most interestingly, as we showed,
these contributions are independent and, the ap-
proaches can be used together to yield better re-
sults. The composite kernel, which combines both
kinds of features, can outperform the state-of-the-
art.
In the future, it will be very interesting to
use syntactic/semantic kernels, as for example in
(Basili et al., 2005; Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b). An-
other promising direction is the use of syntactic
trees, feature sequences and pairs of instances,
e.g. (Nguyen et al., 2009; Moschitti, 2008).
</bodyText>
<sectionHeader confidence="0.966959" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99463025">
We would like to thank Roberto Zanoli and
Marco Dinarelli for helpful explanation
about their work. This work has been par-
tially funded by the LiveMemories project
</bodyText>
<figure confidence="0.86218">
(http://www.livememories.org/) and Expert
System (http://www.expertsystem.net/) research
grant.
</figure>
<page confidence="0.857459">
9
</page>
<bodyText confidence="0.984411">
In the future we will be able to integrate them with the
authors collaboration.
</bodyText>
<page confidence="0.990159">
908
</page>
<reference confidence="0.998125452631579">
\x0cReferences
Basili, Roberto, Marco Cammisa, and Alessandro
Moschitti. 2005. Effective use of WordNet seman-
tics via kernel-based learning. In CoNLL.
Bloehdorn, Stephan and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In ECIR.
Bloehdorn, Stephan and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In CIKM.
Bunescu, Razvan C. and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation ex-
traction. In EMNLP.
Carreras, Xavier, Llus Marques, and Llus Padro.
2002. Named entity extraction using Adaboost. In
CoNLL.
Chieu, Hai Leong and Hwee Tou Ng. 2002. Named
entity recognition: A maximum entropy approach
using global information. In COLING.
Chieu, Hai Leong and Hwee Tou Ng. 2003. Named
entity recognition with a maximum entropy ap-
proach. In CoNLL.
Chinchor, Nancy and Patricia Robinson. 1998. Muc-7
named entity task definition. In the MUC.
Collins, Michael and Nigel Duffy. 2001. Convolution
kernels for natural language. In NIPS.
Collins, Michael and Terry Koo. 2000. Discriminative
reranking for natural language parsing. In ICML.
Collins, Michael. 2002a. New ranking algorithms for
parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL.
Collins, Michael. 2002b. Ranking algorithms for
named-entity extraction boosting and the voted per-
ceptron. In ACL.
Dinarelli, Marco, Alessandro Moschitti, and Giuseppe
Riccardi. 2009. Re-ranking models based on small
training data for spoken language understanding. In
EMNLP.
Florian, Radu, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In CoNLL.
Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL-HLT.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Lodhi, Huma, Craig Saunders, John Shawe Taylor,
Nello Cristianini, , and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, pages 419444.
Magnini, Bernardo, Emmanuele Pianta, Christian Gi-
rardi, Matteo Negri, Lorenza Romano, Manuela
Speranza, Valentina Bartalesi Lenzi, and Rachele
Sprugnoli. 2006. I-CAB: the italian content anno-
tation bank. In LREC.
Moschitti, Alessandro. 2004. A study on convolution
kernels for shallow semantic parsing. In ACL.
Moschitti, Alessandro. 2006. Efficient convolution
kernels for dependency and constituent syntactic
trees. In ICML.
Moschitti, Alessandro. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Nguyen, Truc-Vien T., Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures
for relation extraction. In EMNLP.
Ratinov, Lev and Dan Roth. Design challenges and
misconceptions in named entity recognition. In
CoNLL.
Rifkin, Ryan Michael and Tomaso Poggio. 2002. Ev-
erything old is new again: a fresh look at historical
approaches in machine learning. PhD thesis, MIT.
Sang, Erik F. Tjong Kim and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition.
In CoNLL.
Scholkopf, Bernhard and Alexander J. Smola. 2001.
Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. MIT
Press, Cambridge, MA, USA.
Shen, Libin, Anoop Sarkar, and Franz Josef Och.
2004. Discriminative reranking for machine transla-
tion. In HLT-NAACL, Boston, Massachusetts, USA.
Surdeanu, Mihai, Sanda Harabagiu, John Williams,
and Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In ACL.
Vapnik, Vladimir N. 1998. Statistical Learning The-
ory. John Wiley and Sons, New York.
Vishwanathan, S.V.N. and Alexander J. Smola. 2002.
Fast kernels on strings and trees. In NIPS.
Zanoli, Roberto, Emanuele Pianta, and Claudio Giu-
liano. 2009. Named entity recognition through re-
dundancy driven classifiers. In EVALITA.
</reference>
<page confidence="0.975505">
909
</page>
<figure confidence="0.254092">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.572933">
<note confidence="0.779805">b&amp;apos;Coling 2010: Poster Volume, pages 901909, Beijing, August 2010</note>
<title confidence="0.981375">Kernel-based Reranking for Named-Entity Extraction</title>
<author confidence="0.999871">Truc-Vien T Nguyen</author>
<author confidence="0.999871">Alessandro Moschitti</author>
<author confidence="0.999871">Giuseppe Riccardi</author>
<affiliation confidence="0.999875">Department of Information Engineering and Computer Science University of Trento</affiliation>
<email confidence="0.994229">nguyenthi,moschitti,riccardi@disi.unitn.it</email>
<abstract confidence="0.996396045454545">We present novel kernels based on structured and unstructured features for reranking the N-best hypotheses of conditional random fields (CRFs) applied to entity extraction. The former features are generated by a polynomial kernel encoding entity features whereas tree kernels are used to model dependencies amongst tagged candidate examples. The experiments on two standard corpora in two languages, i.e. the Italian EVALITA 2009 and the English CoNLL 2003 datasets, show a large improvement on CRFs in F-measure, i.e. from 80.34% to 84.33% and from 84.86% to 88.16%, respectively. Our analysis reveals that both kernels provide a comparable improvement over the CRFs baseline. Additionally, their combination improves CRFs much more than the sum of the individual contributions, suggesting an interesting kernel synergy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences Basili</author>
<author>Marco Cammisa Roberto</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Effective use of WordNet semantics via kernel-based learning.</title>
<date>2005</date>
<booktitle>In CoNLL.</booktitle>
<marker>Basili, Roberto, Moschitti, 2005</marker>
<rawString>\x0cReferences Basili, Roberto, Marco Cammisa, and Alessandro Moschitti. 2005. Effective use of WordNet semantics via kernel-based learning. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Combined syntactic and semantic kernels for text classification.</title>
<date>2007</date>
<booktitle>In ECIR.</booktitle>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Bloehdorn, Stephan and Alessandro Moschitti. 2007a. Combined syntactic and semantic kernels for text classification. In ECIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structure and semantics for expressive text kernels.</title>
<date>2007</date>
<booktitle>In CIKM.</booktitle>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Bloehdorn, Stephan and Alessandro Moschitti. 2007b. Structure and semantics for expressive text kernels. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="18403" citStr="Bunescu and Mooney, 2005" startWordPosition="2994" endWordPosition="2997">ay, for any unknown word w of the test set, we can rely on the most probable NE category as feature. The advantage is that we derived it by using the average over many possible contexts of w, which are in the different instances of the unnanotated corpus. The unlabeled corpus for Italian was collected from La Repubblica 6 and it contains over 20 millions words. Whereas the unlabeled corpus for English was collected mainly from The New York Times 7 and BBC news stories 8 with more than 35 millions words. Head word As the head word of an entity plays an important role in information extraction (Bunescu and Mooney, 2005a; Surdeanu et al., 2003), it is in6 http://www.repubblica.it/ 7 http://www.nytimes.com/ 8 http://news.bbc.co.uk/ 905 \x0cFigure 2: Semantic structure of the first sequence cluded in the global set together with its orthographic feature. We now describe some primitives for our global feature framework. 1. wi for i = 1 . . . n is the i0th word 2. ti is the NE tag of wi 3. gi is the gazetteer feature of the word wi 4. fi is the most frequent NE tag seen in a large corpus of wi 5. hi is the head word of the entity. We normally set the head word of an entity as its last word. However, when a prepo</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Bunescu, Razvan C. and Raymond J. Mooney. 2005a. A shortest path dependency kernel for relation extraction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llus Marques</author>
<author>Llus Padro</author>
</authors>
<title>Named entity extraction using Adaboost. In CoNLL.</title>
<date>2002</date>
<contexts>
<context position="3513" citStr="Carreras et al., 2002" startWordPosition="518" endWordPosition="522"> semantics of a document. NEs are objects that can be referred by names (Chinchor and Robinson, 1998), such as people, organizations, and locations. The research on NER has been promoted by the Message Understanding Conferences (MUCs, 1987-1998), the shared task of the Conference on Natural Language Learning (CoNLL, 2002-2003), and the Automatic Content Extraction program (ACE, 2002-2005). In the literature, there exist various learning approaches to extract named-entities from text. A NER sys901 \x0ctem often builds some generative/discriminative model, then, either uses only one classifier (Carreras et al., 2002) or combines many classifiers using some heuristics (Florian et al., 2003). To the best of our knowledge, reranking has not been applied to NER except for the reranking algorithms defined in (Collins, 2002b; Collins, 2002a), which only targeted the entity detection (and not entity classification) task. Besides, since kernel methods offer a natural way to exploit linguistic properties, applying kernels for NE reranking is worthwhile. In this paper, we describe how kernel methods can be applied for reranking, i.e. detection and classification of named-entities, in standard corpora for Italian an</context>
</contexts>
<marker>Carreras, Marques, Padro, 2002</marker>
<rawString>Carreras, Xavier, Llus Marques, and Llus Padro. 2002. Named entity extraction using Adaboost. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Named entity recognition: A maximum entropy approach using global information.</title>
<date>2002</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="17288" citStr="Chieu and Ng, 2002" startWordPosition="2798" endWordPosition="2801"> (Collins, 2002b; Collins, 2002a) but the employed algorithm just exploited arbitrary information regarding word types and linguistic patterns. In contrast, we define and study diverse features by also considering n-grams patterns preceding, and following the target entity. Complementary context In supervised learning, NER systems often suffer from low recall, which is caused by lack of both resource and context. For example, a word like Arkansas may not appear in the training set and in the test set, there may not be enough context to infer its NE tag. In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help. To overcome this deficiency, we employed the following unsupervised procedure: first, the baseline NER is applied to the target un-annotated corpus. Second, we associate each word of the corpus with the most frequent NE category assigned in the previous step. Finally, the above tags are used as features during the training of the improved NER and also for building the feature representation for a new classification instance. This way, for any unknown word w of the test set, we can rely on the most probable NE category as feature. The adva</context>
</contexts>
<marker>Chieu, Ng, 2002</marker>
<rawString>Chieu, Hai Leong and Hwee Tou Ng. 2002. Named entity recognition: A maximum entropy approach using global information. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Named entity recognition with a maximum entropy approach.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="17333" citStr="Chieu and Ng, 2003" startWordPosition="2806" endWordPosition="2809">loyed algorithm just exploited arbitrary information regarding word types and linguistic patterns. In contrast, we define and study diverse features by also considering n-grams patterns preceding, and following the target entity. Complementary context In supervised learning, NER systems often suffer from low recall, which is caused by lack of both resource and context. For example, a word like Arkansas may not appear in the training set and in the test set, there may not be enough context to infer its NE tag. In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help. To overcome this deficiency, we employed the following unsupervised procedure: first, the baseline NER is applied to the target un-annotated corpus. Second, we associate each word of the corpus with the most frequent NE category assigned in the previous step. Finally, the above tags are used as features during the training of the improved NER and also for building the feature representation for a new classification instance. This way, for any unknown word w of the test set, we can rely on the most probable NE category as feature. The advantage is that we derived it by using the aver</context>
</contexts>
<marker>Chieu, Ng, 2003</marker>
<rawString>Chieu, Hai Leong and Hwee Tou Ng. 2003. Named entity recognition with a maximum entropy approach. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
<author>Patricia Robinson</author>
</authors>
<title>Muc-7 named entity task definition.</title>
<date>1998</date>
<booktitle>In the MUC.</booktitle>
<contexts>
<context position="2992" citStr="Chinchor and Robinson, 1998" startWordPosition="443" endWordPosition="446">on by replacing the dotproduct with a function between pairs of linguistic objects. Such functions are a kind of similarity measure satisfying certain properties. An example is the tree kernel (Collins and Duffy, 2001), where the objects are syntactic trees that encode grammatical derivations and the kernel function computes the number of common subtrees. Similarly, sequence kernels (Lodhi et al., 2002) count the number of common subsequences shared by two input strings. Named-entities (NEs) are essential for defining the semantics of a document. NEs are objects that can be referred by names (Chinchor and Robinson, 1998), such as people, organizations, and locations. The research on NER has been promoted by the Message Understanding Conferences (MUCs, 1987-1998), the shared task of the Conference on Natural Language Learning (CoNLL, 2002-2003), and the Automatic Content Extraction program (ACE, 2002-2005). In the literature, there exist various learning approaches to extract named-entities from text. A NER sys901 \x0ctem often builds some generative/discriminative model, then, either uses only one classifier (Carreras et al., 2002) or combines many classifiers using some heuristics (Florian et al., 2003). To </context>
</contexts>
<marker>Chinchor, Robinson, 1998</marker>
<rawString>Chinchor, Nancy and Patricia Robinson. 1998. Muc-7 named entity task definition. In the MUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language. In NIPS.</title>
<date>2001</date>
<contexts>
<context position="2094" citStr="Collins and Duffy, 2001" startWordPosition="303" endWordPosition="306">ach is its flexibility of incorporating arbitrary features into a model. These features help in discriminating good from bad hypotheses and consequently their automatic learning. Various algorithms have been applied for reranking in NLP applications (Huang, 2008; Shen et al., 2004; Collins, 2002b; Collins and Koo, 2000), including parsing, name tagging and machine translation. This work has exploited the disciminative property as one of the key criterion of the reranking algorithm. Reranking appears extremely interesting if coupled with kernel methods (Dinarelli et al., 2009; Moschitti, 2004; Collins and Duffy, 2001), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies. Indeed, while feature-based learning algorithms involve only the dot-product between feature vectors, kernel methods allow for a higher generalization by replacing the dotproduct with a function between pairs of linguistic objects. Such functions are a kind of similarity measure satisfying certain properties. An example is the tree kernel (Collins and Duffy, 2001), where the objects are syntactic trees that encode grammatical derivations and the kernel function computes the</context>
<context position="11765" citStr="Collins and Duffy, 2001" startWordPosition="1835" endWordPosition="1838">b, where yi is equal to 1 for positive and -1 for negative examples, i &lt; with i 0, oi i {1, .., l} are the training instances and the product K(oi, o) = h(oi) (o)i is the kernel function associated with the mapping . Kernel engineering can be carried out by combining basic kernels with additive or multiplicative operators or by designing specific data objects (vectors, sequences and tree structures) for the target tasks. Regarding NLP applications, kernel methods have attracted much interest due to the ability of implicitly exploring huge amounts of structural features. The parse tree kernel (Collins and Duffy, 2001) and string kernel (Lodhi et al., 2002) are examples of the well-known convolution kernels used in various NLP tasks. 2.5 Tree Kernels Tree kernels represent trees in terms of their substructures (called tree fragments). Such fragments form a feature space which, in turn, is mapped into a vector space. Tree kernels measure the similarity between pair of trees by counting the number of fragments in common. There are three important characterizations of fragment type: the SubTrees (ST), the SubSet Trees (SST) and the Partial Trees (PT). For sake of space, we do not report the mathematical descri</context>
<context position="14774" citStr="Collins and Duffy, 2001" startWordPosition="2370" endWordPosition="2373">G, B-LOC, O are the generated NE tags according to IOB notation as described in Section 3.2. With the above data (an original sentence together with a list of candidate tagged sequences), the following pairs of hypotheses will be gener904 \x0cated &lt; H1, H2 &amp;gt;, &lt; H1, H3 &amp;gt;,&lt; H2, H1 &amp;gt; and &lt; H3, H1 &amp;gt;, where the first two pairs are positive and the latter pairs are negative instances. Then a binary classifier based on SVMs and kernel methods can be trained to discriminate between the best hypothesis, i.e. &lt; H1 &amp;gt; and the others. At testing time the hypothesis receiving the highest score is selected (Collins and Duffy, 2001). 3.2 Representation of Tagged Sequences in Semantic Trees We now consider the representation that exploits the most discriminative aspects of candidate structures. As in the case of NER, an input candidate is a sequence of word/tag pairs x = {w1/t1...wn/tn} where wi is the i0th word and ti is the i0th NE tag for that word. The first representation we consider is the tree structure. See figure 2 as an example of candidate tagged sequence and its semantic tree. With the sentence South African Breweries Ltd bought stakes in the Lech and Tychy brewers and three of its candidate tagged sequences i</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Collins, Michael and Nigel Duffy. 2001. Convolution kernels for natural language. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1791" citStr="Collins and Koo, 2000" startWordPosition="259" endWordPosition="262">omising computational framework, which has drawn special attention in the Natural Language Processing (NLP) community. Basically, this method first employs a probabilistic model to generate a list of top-n candidates and then reranks this n-best list with additional features. One appeal of this approach is its flexibility of incorporating arbitrary features into a model. These features help in discriminating good from bad hypotheses and consequently their automatic learning. Various algorithms have been applied for reranking in NLP applications (Huang, 2008; Shen et al., 2004; Collins, 2002b; Collins and Koo, 2000), including parsing, name tagging and machine translation. This work has exploited the disciminative property as one of the key criterion of the reranking algorithm. Reranking appears extremely interesting if coupled with kernel methods (Dinarelli et al., 2009; Moschitti, 2004; Collins and Duffy, 2001), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies. Indeed, while feature-based learning algorithms involve only the dot-product between feature vectors, kernel methods allow for a higher generalization by replacing the dotprod</context>
</contexts>
<marker>Collins, Koo, 2000</marker>
<rawString>Collins, Michael and Terry Koo. 2000. Discriminative reranking for natural language parsing. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1766" citStr="Collins, 2002" startWordPosition="257" endWordPosition="258">eranking is a promising computational framework, which has drawn special attention in the Natural Language Processing (NLP) community. Basically, this method first employs a probabilistic model to generate a list of top-n candidates and then reranks this n-best list with additional features. One appeal of this approach is its flexibility of incorporating arbitrary features into a model. These features help in discriminating good from bad hypotheses and consequently their automatic learning. Various algorithms have been applied for reranking in NLP applications (Huang, 2008; Shen et al., 2004; Collins, 2002b; Collins and Koo, 2000), including parsing, name tagging and machine translation. This work has exploited the disciminative property as one of the key criterion of the reranking algorithm. Reranking appears extremely interesting if coupled with kernel methods (Dinarelli et al., 2009; Moschitti, 2004; Collins and Duffy, 2001), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies. Indeed, while feature-based learning algorithms involve only the dot-product between feature vectors, kernel methods allow for a higher generalization</context>
<context position="3718" citStr="Collins, 2002" startWordPosition="556" endWordPosition="557">g Conferences (MUCs, 1987-1998), the shared task of the Conference on Natural Language Learning (CoNLL, 2002-2003), and the Automatic Content Extraction program (ACE, 2002-2005). In the literature, there exist various learning approaches to extract named-entities from text. A NER sys901 \x0ctem often builds some generative/discriminative model, then, either uses only one classifier (Carreras et al., 2002) or combines many classifiers using some heuristics (Florian et al., 2003). To the best of our knowledge, reranking has not been applied to NER except for the reranking algorithms defined in (Collins, 2002b; Collins, 2002a), which only targeted the entity detection (and not entity classification) task. Besides, since kernel methods offer a natural way to exploit linguistic properties, applying kernels for NE reranking is worthwhile. In this paper, we describe how kernel methods can be applied for reranking, i.e. detection and classification of named-entities, in standard corpora for Italian and English. The key aspect of our reranking approach is how structured and flat features can be employed in discriminating candidate tagged sequences. For this purpose, we apply tree kernels to a tree struc</context>
<context position="16684" citStr="Collins, 2002" startWordPosition="2699" endWordPosition="2700"> the previous hypotheses with H2; this is reasonable since these two also have higher F1. It is worth noting that another useful modification is the flexibility of incorporate diverse, arbitrary features into this tree structure by adding children to the parent node that contains entity tag. These characteristics can be exploited efficiently with the PT kernel, which relaxes constraints of production rules. The inner product can implicitly include these features and deal better with sparse data. 3.3 Global features Mixed n-grams features In previous works, some global features have been used (Collins, 2002b; Collins, 2002a) but the employed algorithm just exploited arbitrary information regarding word types and linguistic patterns. In contrast, we define and study diverse features by also considering n-grams patterns preceding, and following the target entity. Complementary context In supervised learning, NER systems often suffer from low recall, which is caused by lack of both resource and context. For example, a word like Arkansas may not appear in the training set and in the test set, there may not be enough context to infer its NE tag. In such cases, neither global features (Chieu and Ng, 2</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, Michael. 2002a. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking algorithms for named-entity extraction boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1766" citStr="Collins, 2002" startWordPosition="257" endWordPosition="258">eranking is a promising computational framework, which has drawn special attention in the Natural Language Processing (NLP) community. Basically, this method first employs a probabilistic model to generate a list of top-n candidates and then reranks this n-best list with additional features. One appeal of this approach is its flexibility of incorporating arbitrary features into a model. These features help in discriminating good from bad hypotheses and consequently their automatic learning. Various algorithms have been applied for reranking in NLP applications (Huang, 2008; Shen et al., 2004; Collins, 2002b; Collins and Koo, 2000), including parsing, name tagging and machine translation. This work has exploited the disciminative property as one of the key criterion of the reranking algorithm. Reranking appears extremely interesting if coupled with kernel methods (Dinarelli et al., 2009; Moschitti, 2004; Collins and Duffy, 2001), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies. Indeed, while feature-based learning algorithms involve only the dot-product between feature vectors, kernel methods allow for a higher generalization</context>
<context position="3718" citStr="Collins, 2002" startWordPosition="556" endWordPosition="557">g Conferences (MUCs, 1987-1998), the shared task of the Conference on Natural Language Learning (CoNLL, 2002-2003), and the Automatic Content Extraction program (ACE, 2002-2005). In the literature, there exist various learning approaches to extract named-entities from text. A NER sys901 \x0ctem often builds some generative/discriminative model, then, either uses only one classifier (Carreras et al., 2002) or combines many classifiers using some heuristics (Florian et al., 2003). To the best of our knowledge, reranking has not been applied to NER except for the reranking algorithms defined in (Collins, 2002b; Collins, 2002a), which only targeted the entity detection (and not entity classification) task. Besides, since kernel methods offer a natural way to exploit linguistic properties, applying kernels for NE reranking is worthwhile. In this paper, we describe how kernel methods can be applied for reranking, i.e. detection and classification of named-entities, in standard corpora for Italian and English. The key aspect of our reranking approach is how structured and flat features can be employed in discriminating candidate tagged sequences. For this purpose, we apply tree kernels to a tree struc</context>
<context position="16684" citStr="Collins, 2002" startWordPosition="2699" endWordPosition="2700"> the previous hypotheses with H2; this is reasonable since these two also have higher F1. It is worth noting that another useful modification is the flexibility of incorporate diverse, arbitrary features into this tree structure by adding children to the parent node that contains entity tag. These characteristics can be exploited efficiently with the PT kernel, which relaxes constraints of production rules. The inner product can implicitly include these features and deal better with sparse data. 3.3 Global features Mixed n-grams features In previous works, some global features have been used (Collins, 2002b; Collins, 2002a) but the employed algorithm just exploited arbitrary information regarding word types and linguistic patterns. In contrast, we define and study diverse features by also considering n-grams patterns preceding, and following the target entity. Complementary context In supervised learning, NER systems often suffer from low recall, which is caused by lack of both resource and context. For example, a word like Arkansas may not appear in the training set and in the test set, there may not be enough context to infer its NE tag. In such cases, neither global features (Chieu and Ng, 2</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, Michael. 2002b. Ranking algorithms for named-entity extraction boosting and the voted perceptron. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Dinarelli</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Re-ranking models based on small training data for spoken language understanding.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2051" citStr="Dinarelli et al., 2009" startWordPosition="297" endWordPosition="300">tional features. One appeal of this approach is its flexibility of incorporating arbitrary features into a model. These features help in discriminating good from bad hypotheses and consequently their automatic learning. Various algorithms have been applied for reranking in NLP applications (Huang, 2008; Shen et al., 2004; Collins, 2002b; Collins and Koo, 2000), including parsing, name tagging and machine translation. This work has exploited the disciminative property as one of the key criterion of the reranking algorithm. Reranking appears extremely interesting if coupled with kernel methods (Dinarelli et al., 2009; Moschitti, 2004; Collins and Duffy, 2001), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies. Indeed, while feature-based learning algorithms involve only the dot-product between feature vectors, kernel methods allow for a higher generalization by replacing the dotproduct with a function between pairs of linguistic objects. Such functions are a kind of similarity measure satisfying certain properties. An example is the tree kernel (Collins and Duffy, 2001), where the objects are syntactic trees that encode grammatical deriv</context>
</contexts>
<marker>Dinarelli, Moschitti, Riccardi, 2009</marker>
<rawString>Dinarelli, Marco, Alessandro Moschitti, and Giuseppe Riccardi. 2009. Re-ranking models based on small training data for spoken language understanding. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Tong Zhang</author>
</authors>
<title>Named entity recognition through classifier combination.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="3587" citStr="Florian et al., 2003" startWordPosition="531" endWordPosition="534">inchor and Robinson, 1998), such as people, organizations, and locations. The research on NER has been promoted by the Message Understanding Conferences (MUCs, 1987-1998), the shared task of the Conference on Natural Language Learning (CoNLL, 2002-2003), and the Automatic Content Extraction program (ACE, 2002-2005). In the literature, there exist various learning approaches to extract named-entities from text. A NER sys901 \x0ctem often builds some generative/discriminative model, then, either uses only one classifier (Carreras et al., 2002) or combines many classifiers using some heuristics (Florian et al., 2003). To the best of our knowledge, reranking has not been applied to NER except for the reranking algorithms defined in (Collins, 2002b; Collins, 2002a), which only targeted the entity detection (and not entity classification) task. Besides, since kernel methods offer a natural way to exploit linguistic properties, applying kernels for NE reranking is worthwhile. In this paper, we describe how kernel methods can be applied for reranking, i.e. detection and classification of named-entities, in standard corpora for Italian and English. The key aspect of our reranking approach is how structured and </context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>Florian, Radu, Abe Ittycheriah, Hongyan Jing, and Tong Zhang. 2003. Named entity recognition through classifier combination. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In ACL-HLT.</booktitle>
<contexts>
<context position="1732" citStr="Huang, 2008" startWordPosition="251" endWordPosition="252">kernel synergy. 1 Introduction Reranking is a promising computational framework, which has drawn special attention in the Natural Language Processing (NLP) community. Basically, this method first employs a probabilistic model to generate a list of top-n candidates and then reranks this n-best list with additional features. One appeal of this approach is its flexibility of incorporating arbitrary features into a model. These features help in discriminating good from bad hypotheses and consequently their automatic learning. Various algorithms have been applied for reranking in NLP applications (Huang, 2008; Shen et al., 2004; Collins, 2002b; Collins and Koo, 2000), including parsing, name tagging and machine translation. This work has exploited the disciminative property as one of the key criterion of the reranking algorithm. Reranking appears extremely interesting if coupled with kernel methods (Dinarelli et al., 2009; Moschitti, 2004; Collins and Duffy, 2001), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies. Indeed, while feature-based learning algorithms involve only the dot-product between feature vectors, kernel methods</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Huang, Liang. 2008. Forest reranking: Discriminative parsing with non-local features. In ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="7177" citStr="Lafferty et al., 2001" startWordPosition="1095" endWordPosition="1098">6. Accordingly, the named entities in the test dataset are considerably different from those that appear in the training or the development set. Italian GPE LOC ORG PER Train 2813 362 3658 4577 24.65% 3.17% 32.06% 40.11% Test 1143 156 1289 2378 23.02% 3.14% 25.96% 47.89% English LOC MISC ORG PER Train 7140 3438 6321 6600 30.38% 14.63% 26.90% 28.09% Dev 1837 922 1341 1842 30.92% 15.52% 22.57% 31.00% Test 1668 702 1661 1617 29.53% 12.43% 29.41% 28.63% Table 1: Statistics on the Italian EVALITA 2009 and English CoNLL 2003 corpora. 2.2 The baseline algorithm We selected Conditional Random Fields (Lafferty et al., 2001) as the baseline model. Conditional 902 \x0crandom fields (CRFs) are a probabilistic framework for labeling and segmenting sequence data. They present several advantages over other purely generative models such as Hidden Markov models (HMMs) by relaxing the independence assumptions required by HMMs. Besides, HMMs and other discriminative Markov models are prone to the label bias problem, which is effectively solved by CRFs. The named-entity recognition (NER) task is framed as assigning label sequences to a set of observation sequences. We follow the IOB notation where the NE tags have the form</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, John, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>419444</pages>
<contexts>
<context position="2770" citStr="Lodhi et al., 2002" startWordPosition="406" endWordPosition="409">otheses a huge amount of features along with their dependencies. Indeed, while feature-based learning algorithms involve only the dot-product between feature vectors, kernel methods allow for a higher generalization by replacing the dotproduct with a function between pairs of linguistic objects. Such functions are a kind of similarity measure satisfying certain properties. An example is the tree kernel (Collins and Duffy, 2001), where the objects are syntactic trees that encode grammatical derivations and the kernel function computes the number of common subtrees. Similarly, sequence kernels (Lodhi et al., 2002) count the number of common subsequences shared by two input strings. Named-entities (NEs) are essential for defining the semantics of a document. NEs are objects that can be referred by names (Chinchor and Robinson, 1998), such as people, organizations, and locations. The research on NER has been promoted by the Message Understanding Conferences (MUCs, 1987-1998), the shared task of the Conference on Natural Language Learning (CoNLL, 2002-2003), and the Automatic Content Extraction program (ACE, 2002-2005). In the literature, there exist various learning approaches to extract named-entities f</context>
<context position="11804" citStr="Lodhi et al., 2002" startWordPosition="1842" endWordPosition="1845">1 for negative examples, i &lt; with i 0, oi i {1, .., l} are the training instances and the product K(oi, o) = h(oi) (o)i is the kernel function associated with the mapping . Kernel engineering can be carried out by combining basic kernels with additive or multiplicative operators or by designing specific data objects (vectors, sequences and tree structures) for the target tasks. Regarding NLP applications, kernel methods have attracted much interest due to the ability of implicitly exploring huge amounts of structural features. The parse tree kernel (Collins and Duffy, 2001) and string kernel (Lodhi et al., 2002) are examples of the well-known convolution kernels used in various NLP tasks. 2.5 Tree Kernels Tree kernels represent trees in terms of their substructures (called tree fragments). Such fragments form a feature space which, in turn, is mapped into a vector space. Tree kernels measure the similarity between pair of trees by counting the number of fragments in common. There are three important characterizations of fragment type: the SubTrees (ST), the SubSet Trees (SST) and the Partial Trees (PT). For sake of space, we do not report the mathematical description of them, which is available in (V</context>
</contexts>
<marker>Lodhi, Saunders, Taylor, Cristianini, 2002</marker>
<rawString>Lodhi, Huma, Craig Saunders, John Shawe Taylor, Nello Cristianini, , and Chris Watkins. 2002. Text classification using string kernels. Journal of Machine Learning Research, pages 419444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Emmanuele Pianta</author>
<author>Christian Girardi</author>
<author>Matteo Negri</author>
<author>Lorenza Romano</author>
<author>Manuela Speranza</author>
<author>Valentina Bartalesi Lenzi</author>
<author>Rachele Sprugnoli</author>
</authors>
<title>I-CAB: the italian content annotation bank.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="5704" citStr="Magnini et al., 2006" startWordPosition="857" endWordPosition="860">a of two different languages. The evaluation in the Italian corpus shows that our method outperforms the best reported methods whereas on the English data it reaches the state-of-the-art. 2 Background 2.1 The data Different languages exhibit different linguistic phenomena and challenges. A robust NER system is expected to be well-adapted to multiple domains and languages. Therefore, we experimented with two datasets: the EVALITA 2009 Italian corpus and the well-known CoNLL 2003 English shared task corpus. The EVALITA 2009 Italian dataset is based on I-CAB, the Italian Content Annotation Bank (Magnini et al., 2006), annotated with four entity types: Person (PER), Organization (ORG), Geo-Political Entity (GPE) and Location (LOC). The training data, taken from the local newspaper LAdige, consists of 525 news stories which belong to five categories: News Stories, Cultural News, Economic News, Sports News and Local News. Test data, on the other hand, consist of completely new data, taken from the same newspaper and consists of 180 news stories. The CoNLL 2003 English dataset is created within the shared task of CoNLL-2003 (Sang and Meulder, 2003). It is a collection of news wire articles from the Reuters Co</context>
</contexts>
<marker>Magnini, Pianta, Girardi, Negri, Romano, Speranza, Lenzi, Sprugnoli, 2006</marker>
<rawString>Magnini, Bernardo, Emmanuele Pianta, Christian Girardi, Matteo Negri, Lorenza Romano, Manuela Speranza, Valentina Bartalesi Lenzi, and Rachele Sprugnoli. 2006. I-CAB: the italian content annotation bank. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow semantic parsing.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2068" citStr="Moschitti, 2004" startWordPosition="301" endWordPosition="302">eal of this approach is its flexibility of incorporating arbitrary features into a model. These features help in discriminating good from bad hypotheses and consequently their automatic learning. Various algorithms have been applied for reranking in NLP applications (Huang, 2008; Shen et al., 2004; Collins, 2002b; Collins and Koo, 2000), including parsing, name tagging and machine translation. This work has exploited the disciminative property as one of the key criterion of the reranking algorithm. Reranking appears extremely interesting if coupled with kernel methods (Dinarelli et al., 2009; Moschitti, 2004; Collins and Duffy, 2001), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies. Indeed, while feature-based learning algorithms involve only the dot-product between feature vectors, kernel methods allow for a higher generalization by replacing the dotproduct with a function between pairs of linguistic objects. Such functions are a kind of similarity measure satisfying certain properties. An example is the tree kernel (Collins and Duffy, 2001), where the objects are syntactic trees that encode grammatical derivations and the ke</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Moschitti, Alessandro. 2004. A study on convolution kernels for shallow semantic parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="12481" citStr="Moschitti, 2006" startWordPosition="1956" endWordPosition="1957">arious NLP tasks. 2.5 Tree Kernels Tree kernels represent trees in terms of their substructures (called tree fragments). Such fragments form a feature space which, in turn, is mapped into a vector space. Tree kernels measure the similarity between pair of trees by counting the number of fragments in common. There are three important characterizations of fragment type: the SubTrees (ST), the SubSet Trees (SST) and the Partial Trees (PT). For sake of space, we do not report the mathematical description of them, which is available in (Vishwanathan and Smola, 2002), (Collins and Duffy, 2001) and (Moschitti, 2006), respectively. In contrast, we report some descriptions in terms of feature space that may be useful to understand the new engineered kernels. In principle, a SubTree (ST) is defined by taking any node along with its descendants. A SubSet Tree (SST) is a more general structure which does not necessarily include all the descendants. The distinction is that an SST must be generated by applying the same grammatical rule set which generated the original tree, as pointed out in (Collins and Duffy, 2001). A Partial Tree (PT) is a more general form of sub-structures obtained by relaxing constraints </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Moschitti, Alessandro. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Kernel methods, syntax and semantics for relational text categorization.</title>
<date>2008</date>
<booktitle>In CIKM.</booktitle>
<marker>Moschitti, 2008</marker>
<rawString>Moschitti, Alessandro. 2008. Kernel methods, syntax and semantics for relational text categorization. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Convolution kernels on constituent, dependency and sequential structures for relation extraction.</title>
<date>2009</date>
<booktitle>In EMNLP. Ratinov, Lev and</booktitle>
<marker>Nguyen, Moschitti, Riccardi, 2009</marker>
<rawString>Nguyen, Truc-Vien T., Alessandro Moschitti, and Giuseppe Riccardi. 2009. Convolution kernels on constituent, dependency and sequential structures for relation extraction. In EMNLP. Ratinov, Lev and Dan Roth. Design challenges and misconceptions in named entity recognition. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Michael Rifkin</author>
<author>Tomaso Poggio</author>
</authors>
<title>Everything old is new again: a fresh look at historical approaches in machine learning.</title>
<date>2002</date>
<tech>PhD thesis,</tech>
<institution>MIT.</institution>
<contexts>
<context position="10172" citStr="Rifkin and Poggio, 2002" startWordPosition="1564" endWordPosition="1568">e extracted from Wikipedia and various Italian sites. 2.3 Support Vector Machines (SVMs) Support Vector Machines refer to a supervised machine learning technique based on the latest results of the statistical learning theory. Given a vector space and a set of training points, i.e. positive and negative examples, SVMs find a separating hyperplane H(~ x) = ~ ~ x + b = 0 where Rn and b R are learned by applying the Structural Risk Minimization principle (Vapnik, 1998). SVMs are a binary classifier, but they can be easily extended to multi-class classifier, e.g. by means of the one-vs-all method (Rifkin and Poggio, 2002). One strong point of SVMs is the possibility to apply kernel methods to implicitly map data in a new space where the examples are more easily separable as described in the next section. 2.4 Kernel methods Kernel methods (Scholkopf and Smola, 2001) are an attractive alternative to feature-based methods since the applied learning algorithm only needs to compute a product between a pair of objects (by means of kernel functions), avoiding the explicit feature representation. A kernel function is a scalar product in a possibly unknown feature space. More precisely, The object o is mapped in ~ x wi</context>
</contexts>
<marker>Rifkin, Poggio, 2002</marker>
<rawString>Rifkin, Ryan Michael and Tomaso Poggio. 2002. Everything old is new again: a fresh look at historical approaches in machine learning. PhD thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Fien De Meulder</author>
</authors>
<title>Introduction to the conll-2003 shared task: Language-independent named entity recognition.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<marker>Sang, De Meulder, 2003</marker>
<rawString>Sang, Erik F. Tjong Kim and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Scholkopf</author>
<author>Alexander J Smola</author>
</authors>
<title>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond.</title>
<date>2001</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="10420" citStr="Scholkopf and Smola, 2001" startWordPosition="1607" endWordPosition="1610"> and a set of training points, i.e. positive and negative examples, SVMs find a separating hyperplane H(~ x) = ~ ~ x + b = 0 where Rn and b R are learned by applying the Structural Risk Minimization principle (Vapnik, 1998). SVMs are a binary classifier, but they can be easily extended to multi-class classifier, e.g. by means of the one-vs-all method (Rifkin and Poggio, 2002). One strong point of SVMs is the possibility to apply kernel methods to implicitly map data in a new space where the examples are more easily separable as described in the next section. 2.4 Kernel methods Kernel methods (Scholkopf and Smola, 2001) are an attractive alternative to feature-based methods since the applied learning algorithm only needs to compute a product between a pair of objects (by means of kernel functions), avoiding the explicit feature representation. A kernel function is a scalar product in a possibly unknown feature space. More precisely, The object o is mapped in ~ x with a feature function : O &lt;n, where O is the set of the objects. The kernel trick allows us to rewrite the decision hyperplane as: H(~ x) = \x10 X i=1..l yii~ xi \x11 ~ x + b = 2 http://www.nima.mil/gns/html 3 http://www.alexandria.ucsb.edu 4 http:</context>
</contexts>
<marker>Scholkopf, Smola, 2001</marker>
<rawString>Scholkopf, Bernhard and Alexander J. Smola. 2001. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Franz Josef Och</author>
</authors>
<title>Discriminative reranking for machine translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="1751" citStr="Shen et al., 2004" startWordPosition="253" endWordPosition="256">y. 1 Introduction Reranking is a promising computational framework, which has drawn special attention in the Natural Language Processing (NLP) community. Basically, this method first employs a probabilistic model to generate a list of top-n candidates and then reranks this n-best list with additional features. One appeal of this approach is its flexibility of incorporating arbitrary features into a model. These features help in discriminating good from bad hypotheses and consequently their automatic learning. Various algorithms have been applied for reranking in NLP applications (Huang, 2008; Shen et al., 2004; Collins, 2002b; Collins and Koo, 2000), including parsing, name tagging and machine translation. This work has exploited the disciminative property as one of the key criterion of the reranking algorithm. Reranking appears extremely interesting if coupled with kernel methods (Dinarelli et al., 2009; Moschitti, 2004; Collins and Duffy, 2001), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies. Indeed, while feature-based learning algorithms involve only the dot-product between feature vectors, kernel methods allow for a higher</context>
</contexts>
<marker>Shen, Sarkar, Och, 2004</marker>
<rawString>Shen, Libin, Anoop Sarkar, and Franz Josef Och. 2004. Discriminative reranking for machine translation. In HLT-NAACL, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="18428" citStr="Surdeanu et al., 2003" startWordPosition="2998" endWordPosition="3001">of the test set, we can rely on the most probable NE category as feature. The advantage is that we derived it by using the average over many possible contexts of w, which are in the different instances of the unnanotated corpus. The unlabeled corpus for Italian was collected from La Repubblica 6 and it contains over 20 millions words. Whereas the unlabeled corpus for English was collected mainly from The New York Times 7 and BBC news stories 8 with more than 35 millions words. Head word As the head word of an entity plays an important role in information extraction (Bunescu and Mooney, 2005a; Surdeanu et al., 2003), it is in6 http://www.repubblica.it/ 7 http://www.nytimes.com/ 8 http://news.bbc.co.uk/ 905 \x0cFigure 2: Semantic structure of the first sequence cluded in the global set together with its orthographic feature. We now describe some primitives for our global feature framework. 1. wi for i = 1 . . . n is the i0th word 2. ti is the NE tag of wi 3. gi is the gazetteer feature of the word wi 4. fi is the most frequent NE tag seen in a large corpus of wi 5. hi is the head word of the entity. We normally set the head word of an entity as its last word. However, when a preposition exists in the enti</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Surdeanu, Mihai, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate-argument structures for information extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>John Wiley and Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="10017" citStr="Vapnik, 1998" startWordPosition="1541" endWordPosition="1542">d with all the publicly traded companies listed in Google directory4, the European business directory5. For Italian, the generic proper nouns are extracted from Wikipedia and various Italian sites. 2.3 Support Vector Machines (SVMs) Support Vector Machines refer to a supervised machine learning technique based on the latest results of the statistical learning theory. Given a vector space and a set of training points, i.e. positive and negative examples, SVMs find a separating hyperplane H(~ x) = ~ ~ x + b = 0 where Rn and b R are learned by applying the Structural Risk Minimization principle (Vapnik, 1998). SVMs are a binary classifier, but they can be easily extended to multi-class classifier, e.g. by means of the one-vs-all method (Rifkin and Poggio, 2002). One strong point of SVMs is the possibility to apply kernel methods to implicitly map data in a new space where the examples are more easily separable as described in the next section. 2.4 Kernel methods Kernel methods (Scholkopf and Smola, 2001) are an attractive alternative to feature-based methods since the applied learning algorithm only needs to compute a product between a pair of objects (by means of kernel functions), avoiding the e</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vapnik, Vladimir N. 1998. Statistical Learning Theory. John Wiley and Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V N Vishwanathan</author>
<author>Alexander J Smola</author>
</authors>
<title>Fast kernels on strings and trees.</title>
<date>2002</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="12432" citStr="Vishwanathan and Smola, 2002" startWordPosition="1947" endWordPosition="1950">) are examples of the well-known convolution kernels used in various NLP tasks. 2.5 Tree Kernels Tree kernels represent trees in terms of their substructures (called tree fragments). Such fragments form a feature space which, in turn, is mapped into a vector space. Tree kernels measure the similarity between pair of trees by counting the number of fragments in common. There are three important characterizations of fragment type: the SubTrees (ST), the SubSet Trees (SST) and the Partial Trees (PT). For sake of space, we do not report the mathematical description of them, which is available in (Vishwanathan and Smola, 2002), (Collins and Duffy, 2001) and (Moschitti, 2006), respectively. In contrast, we report some descriptions in terms of feature space that may be useful to understand the new engineered kernels. In principle, a SubTree (ST) is defined by taking any node along with its descendants. A SubSet Tree (SST) is a more general structure which does not necessarily include all the descendants. The distinction is that an SST must be generated by applying the same grammatical rule set which generated the original tree, as pointed out in (Collins and Duffy, 2001). A Partial Tree (PT) is a more general form of</context>
</contexts>
<marker>Vishwanathan, Smola, 2002</marker>
<rawString>Vishwanathan, S.V.N. and Alexander J. Smola. 2002. Fast kernels on strings and trees. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Zanoli</author>
<author>Emanuele Pianta</author>
<author>Claudio Giuliano</author>
</authors>
<title>Named entity recognition through redundancy driven classifiers.</title>
<date>2009</date>
<booktitle>In EVALITA.</booktitle>
<contexts>
<context position="25025" citStr="Zanoli et al., 2009" startWordPosition="4215" endWordPosition="4218">classifier was trained on 4/5 of the data, then used to decode the remaining 1/5. 907 \x0cThe top 10 hypotheses together with their log probabilities were recovered for each training sentence. Similarly, a model trained on the whole training data was used to produce 10 hypotheses for each sentence in the development set. For the reranking experiments, we applied different kernel setups to the two corpora described in Section 2.1. The three kernels were trained on the training portion. Italian Test P R F CRFs 83.43 77.48 80.34 CTK 84.97 78.03 81.35 CFK 84.93 79.13 81.93 CTFK 85.99 82.73 84.33 (Zanoli et al., 2009) 84.07 80.02 82.00 English Test P R F CRFs 85.37 84.35 84.86 CTK 87.19 84.79 85.97 CFK 86.53 86.75 86.64 CTFK 88.07 88.25 88.16 (Ratinov and Roth, ) N/A N/A 90.57 Table 3: Reranking results of the three tagging kernels on the Italian and English testset. 4.2 Discussion Table 3 presents the reranking results on the test data of both corpora. The results show a 20.29% relative improvement in F-measure for Italian and 21.79% for English. CFK based on unstructured features achieves higher accuracy than CTK based on structured features. However, the huge amount of subtrees generated by the PT kerne</context>
<context position="26523" citStr="Zanoli et al., 2009" startWordPosition="4462" endWordPosition="4465">g kernels CTK and CFK give improvement over the CRFs baseline in both languages. This suggests that structured and unstructured features are effective in discriminating between competing NE annotations. Furthermore, the combination of the two tagging kernels on both standard corpora shows a large improvement in F-measure from 80.34% to 84.33% for Italian and from 84.86% to 88.16% for English data. This suggests that these two kernels, corresponding to two kinds of feature, complement each other. To better collocate our results with previous work, we report the best NER outcome on the Italian (Zanoli et al., 2009) and the English (Ratinov and Roth, ) datasets, in the last row (in italic) of each table. This shows that our model outperforms the best Italian NER system and it is close to the state-of-art model for English, which exploits many complex features9. Also note that we are very close to the F1 achieved by the best system of CoNLL 2003, i.e. 88.8. 5 Conclusion We analyzed the impact of kernel-based approaches for modeling dependencies between tagged sequences for NER. Our study illustrates that each individual kernel, either with structured or with flat features clearly gives improvement to the </context>
</contexts>
<marker>Zanoli, Pianta, Giuliano, 2009</marker>
<rawString>Zanoli, Roberto, Emanuele Pianta, and Claudio Giuliano. 2009. Named entity recognition through redundancy driven classifiers. In EVALITA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>