In addition we include 40K tokens of annotated IRC chat data CITATION, which is similar in style,,
This baseline obtained a 0.9 accuracy on the Brown corpus CITATION,,
The stateof-the-art Stanford POS tagger CITATION im,,
In addition there has been been work on SkipChain CRFs (CITATION; CITATION) which enforce consistency when classifying multiple occurrences of an entity within a document,,
In contrast to previous weakly supervised approaches to Named Entity Classification, for example the CoTraining and Nave Bayes (EM) models of CITATION, LabeledLDA models each entity string as a mixture of types rather than using a single hidden variable to represent the type of each mention,,
Assumes segmentation is given as in (Collins and Singer, 1999), and CITATION,,
Recent work (CITATION; CITATION) has proposed lexical normalization of tweets which may be useful as a preprocessing step for the upstream tasks like POS tagging and NER,,
Previous work on Semantic Bootstrapping has taken a weakly-supervised approach to classifying named entities based on large amounts of unlabeled text (CITATION; CITATION; CITATION; CITATION; CITATION),,
CITATION train a classifier to recognize named entities based on annotated Twitter data, handling the types PERSON, LOCATION, and ORGANIZATION,,
Yet tweets provide a unique compilation of information that is more upto-date and inclusive than news articles, due to the low-barrier to tweeting, and the proliferation of mobile devices.1 The corpus of tweets already exceeds 1 See the trending topics displayed on twitter.com the size of the Library of Congress CITATION and is growing far more rapidly,,
These clusters are often effective in capturing lexical variations, for ex4 Using MMAX2 CITATION for annotation,,
In particular, we perform hierarchical clustering using Jcluster CITATION on 52 million tweets; each word is uniquely represented by a bit string based on the path from the root of the resulting hierarchy to the words leaf,,
Following CITATION, CITATION and CITATION, we treat classification and segmentation of named entities as separate tasks,,
ification, for example the CoTraining and Nave Bayes (EM) models of CITATION, LabeledLDA models each entity string as a mixture of types rather than using a single hidden variable to represent the type of each mention,,
2.2 Shallow Parsing Shallow parsing, or chunking is the task of identifying non-recursive phrases, such as noun phrases, 5 We use MALLET CITATION,,
As in standard LDA CITATION, each bag of words is associated with a distribution over topics, Multinom,,
Distant Supervision with Topic Models: To model unlabeled entities and their possible types, we apply LabeledLDA CITATION, constraining each entitys distribution over topics based on its set of possible types according to Freebase,,
To address these issues we propose a distantly supervised approach which applies LabeledLDA CITATION to leverage large amounts of unlabeled data in addition to large dictionaries of entities gathered from Freebase, and combines information about an entitys context across its mentions,,
Compared with the state-of-the-art newstrained Stanford Named Entity Recognizer CITATION, T-SEG obtains a 52% increase in F1 score,,
2.3 Capitalization A key orthographic feature for recognizing named entities is capitalization (CITATION; CITATION),,
As in standard LDA CITATION, each bag of words is associated with a distribution over topics, Multinomial(e), and each topic is associated with a distribution over words, Multinomial(t),,
Developed in parallel to our work, CITATION investigate NER on the same 3 types, in addition to PRODUCTs and present a semi1531 \x0csupervised approach using k-nearest neighbor,,
We use the set of shallow parsing features described by CITATION, in addition to the Brown clusters mentioned above,,
Recent work (CITATION; CITATION) ,,
To address both these issues we have presented and evaluated a distantly supervised approach based on LabeledLDA, which obtains a 25% increase in F1 score over the co-training approach to Named Entity Classification suggested by CITATION when applied to Twitter,,
In order to make predictions, for each entity we use an informative Dirichlet prior based on train e and perform 100 iterations of Gibbs Sampling holding the hidden topic variables in the training data fixed CITATION,,
Additionally we compare against the co-training algorithm of CITATION which also leverages unlabeled data and uses our Freebase type lists; for seed rules we use the unambiguous Freebase entities,,
Like SMS CITATION, tweets are particularly terse and difficult (See Table 1),,
We introduce a novel approach to distant supervision CITATION using Topic Models,,
The stateof-the-art Stanford POS tagger CITATION improves on the baseline, obtaining an accuracy of 0.8,,
end for end for To infer values for the hidden variables, we apply Collapsed Gibbs sampling CITATION, where parameters are integrated out, and the ze,is are sampled directly,,
us CITATION,,
To help address the issue of OOV words and lexical variations, we perform clustering to group together words which are distributionally similar (CITATION; CITATION),,
This approach increases F1 score by 25% relative to co-training (CITATION; CITATION) on the task of classifying named entities in Tweets,,
While it might be beneficial to jointly model segmentation and (distantly supervised) classification using a joint sequence labeling and topic model similar to that proposed by CITATION, we leave this for potential future work,,
without any prior knowledge, there is not enough context to determine what type of entity KKTNY refers to, however by exploiting redundancy in the data CITATION, we can determine it is likely a reference to a television show since it of1528 \x0cten co-occurs with words such as watching and premieres in other contexts.9 In order to handle the problem of many infrequent types, we leverage large lists of entities and their types gathered from an open-domain ontology (Freebase) as a source of distant supervision, allowing use of large amounts of unlabeled data in learning,,
