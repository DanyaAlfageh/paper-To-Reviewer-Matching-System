<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.3093405">
b&amp;apos;Proceedings of the SIGDIAL 2013 Conference, pages 6169,
Metz, France, 22-24 August 2013. c
</note>
<title confidence="0.6075015">
2013 Association for Computational Linguistics
Stance Classification in Online Debates by Recognizing Users Intentions
</title>
<author confidence="0.819427">
Sarvesh Ranade, Rajeev Sangal, Radhika Mamidi
</author>
<affiliation confidence="0.773776666666667">
Language Technologies Research Centre
International Institute of Information Technology
Hyderabad, India
</affiliation>
<email confidence="0.881185">
sarvesh.ranade@research.iiit.ac.in, {sangal, radhika.mamidi}@iiit.ac.in
</email>
<sectionHeader confidence="0.990204" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998859764705882">
Online debate forums provide a rich col-
lection of differing opinions on vari-
ous topics. In dual-sided debates, users
present their opinions or judge others
opinions to support their stance. In this
paper, we examine the use of users inten-
tions and debate structure for stance clas-
sification of the debate posts. We propose
a domain independent approach to capture
users intent at sentence level using its de-
pendency parse and sentiWordNet and to
build the intention structure of the post to
identify its stance. To aid the task of clas-
sification, we define the health of the de-
bate structure and show that maximizing
its value leads to better stance classifica-
tion accuracies.
</bodyText>
<sectionHeader confidence="0.99811" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995881360655738">
Online debate forums provide Internet users a plat-
form to discuss popular ideological debates. De-
bate in essence is a method of interactive and rep-
resentational arguments. In an online debate, users
make assertions with superior content to support
their stance. Factual accuracy and emotional ap-
peal are important features used to persuade the
readers. It is easy to observe that personal opin-
ions are important in ideological stance taking
(Somasundaran and Wiebe, 2009). Because of the
availability of Internet resources and time, people
intelligently use the factual data to support their
opinions.
Online debates differ from public debates in
terms of logical consistency. In online debates,
users assert their opinion towards either side,
sometimes ignoring discourse coherence required
for logical soundness of the post. Generally they
use strong degree of sentiment words including in-
sulting or sarcastic remarks for greater emphasis
of their point. Apart from supporting/opposing a
side, users make factual statements such as Stan
lee once said Superman is superior than batman in
all areas. to strengthen their stance.
We collected debate posts from an online site
called convinceme.net which allows users to in-
stantiate debates on questions of their choice. The
debates are held between two topics. To gener-
alize the debate scenarios, we refer to these top-
ics as Topic A and B. When users participate
in the debate, they support their stance by post-
ing on the appropriate side, thus self-labeling their
stance. Users stance is determined by the debate
topic they are supporting and we refer to each in-
stance of users opinion as a post. Each post can
have multiple utterances which are the smallest
discourse units. This site has an option to rebut
another post, thus enabling users to comment on
others opinion.
A post with most of its utterances supporting a
debate topic most likely supports that topic. This
shows that users intentions play an important role
in supporting their stance. In this paper, we em-
ploy topic directed sentiment analysis based ap-
proach to capture utterance level intentions. We
have designed debate specific utterance level in-
tentions which denote users attitude of support-
ing/opposing a specific debate topic or stating a
known fact.
Message level intention is denoted by the stance
users are taking in the debate. We build posts in-
tention structure and calculate posts debate topics
related sentiment scores to classify their stance in
ideological debates. Intuitively, posts by same au-
thor support same stance and rebutting posts have
opposite stances. This inter-post information pre-
sented by debates structure is also used to revise
posts stance. As mentioned earlier, we use the de-
bate data collected from convinceme.net to eval-
uate our approach on stance classification task and
it beats baseline systems and a previous approach
</bodyText>
<page confidence="0.997987">
61
</page>
<bodyText confidence="0.998909894736842">
\x0cby significant margin and achieves overall accu-
racy of 74.4%.
The rest of the paper is organized as follows:
Section 2 presents previous approaches to stance
classification and sentiment analysis. Section 3
highlights the importance of users intentions in
ideological debates and presents our algorithm to
capture utterance intentions using topic directed
sentiment analysis. In Section 4, we describe the
use the utterance level intentions to capture inten-
tion of the entire post. We explain our stance clas-
sification method using post content features and
post intention structure in this section. Section 5
describes the use of the dialogue structure of the
debate and presents a gradient ascent method for
re-evaluating posts stance. We present experi-
ments and results on capturing users intentions
and stance classification in Section 6. This is fol-
lowed by conclusions in Section 7.
</bodyText>
<sectionHeader confidence="0.999587" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999452605633803">
To classify posts stance in dual-sided debates,
previous approaches have used probabilistic (So-
masundaran and Wiebe, 2009) as well as machine
learning techniques (Anand et al., 2011; Somasun-
daran and Wiebe, 2010). Some approaches exten-
sively used the dialogue structure to identify posts
stance (Walker et al., 2012) whereas others consid-
ered opinion expressions and their targets essen-
tial to capture sentiment in the posts towards de-
bate topics (Somasundaran and Wiebe, 2009; So-
masundaran and Wiebe, 2010).
Pure machine learning approaches (Anand et
al., 2011) have extracted lexical and contextual
features of debate posts to classify their stance.
Walker et al. (2012) partitioned the debate posts
based on the dialogue structure of the debate and
assigned stance to a partition using lexical features
of candidate posts. This approach has a disadvan-
tage that it loses posts individuality because it as-
signs stance based on the entire partitions whereas
our approach treats each post individually.
To extract opinion expressions, Somasundaran
and Wiebe (2009) used the Subjectivity lexicon
and Somasundaran and Wiebe (2010) used the
MPQA opinion corpus (Wiebe et al., 2005). These
opinion expressions were attached to the target
words using different techniques. Somasundaran
and Wiebe (2009) attached opinion expressions to
all plausible sentence words whereas Somasun-
daran and Wiebe (2010) attached opinion expres-
sions to the debate topic closest to them. Proba-
bilistic association learning of target-opinion pair
and the debate topic was used by Somasundaran
and Wiebe (2010) as an integer linear program-
ming problem to classify posts stance. Even
though opinions might not be directed towards de-
bate topics, these approaches attach the opinions
to debate topics based only on their context co-
occurrence. Our approach finds the target word
for an opinion expression by analyzing the full de-
pendency parse of the utterance.
There has also been a lot of work done in social
media on target directed sentiment analysis (Agar-
wal et al., 2011; OHare et al., 2009; Mukher-
jee and Bhattacharyya, 2012) which we incorpo-
rate for capturing users intentions. Agarwal et
al. (2011) used syntactic features as target de-
pendent features to differentiate sentiments ef-
fect on different targets in a tweet. OHare et al.
(2009) employed a word-based approach to deter-
mine sentiments directed towards companies and
their stocks from financial blogs. Mukherjee and
Bhattacharyya (2012) applied clustering to extract
feature specific opinions and calculated the overall
feature sentiment using subjectivity lexicon.
Discourse markers cues were used by Sood
(2013) to prioritize the conversational sentences
and by Yelati and Sangal (2011) to identify users
intentions in help desk emails. Most of the dis-
course analysis theories defined their own dis-
course segment tagging schema to understand
the meaning of each utterance. Yelati and San-
gal (2011) devised a help desk specific tagging
schema to capture the queries and build email
structure in help desk emails. Lampert et al.
(2006) used verbal response modes to understand
the client/therapist conversations. We incorporate
target directed sentiment analysis to capture utter-
ance level intentions using a sentiment lexicon and
dependency parses as described in the following
section.
</bodyText>
<sectionHeader confidence="0.95975" genericHeader="method">
3 Capturing User Intentions
</sectionHeader>
<bodyText confidence="0.995961125">
Users intention at the utterance level plays a vital
role in overall stance taking. We define a set of
intentions each utterance can hold. The proposed
topic directed sentiment analysis based approach
will automatically identify users intention behind
each utterance.
Because of the unstructured and noisy nature of
social media, we need to pre-process the debate
</bodyText>
<page confidence="0.997384">
62
</page>
<bodyText confidence="0.906825">
\x0cdata before analyzing it further for users inten-
tions.
</bodyText>
<subsectionHeader confidence="0.999456">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.991028846153846">
The posts data is split into utterances, i.e. smallest
discourse units, based on sentence ending mark-
ers and a few specific Penn Discourse Tree Bank
(PDTB) (Prasad et al., 2008) discourse markers
listed in Table 2. Merged words like mindbog-
gling, super-awesome, etc. are split based on
the default Unix dictionary and special charac-
ter delimiters. Once the debate posts are broken
into utterances, we identify the intention behind
each utterance in the post to compute entire posts
stance.
Table 1 presents the statistics of the debate data
collected from convinceme.net.
</bodyText>
<table confidence="0.747185">
Debates Posts Author P/A Utterances
28 2040 1333 1.53 12438
</table>
<tableCaption confidence="0.986407">
Table 1: Debate Data Statistics
</tableCaption>
<subsectionHeader confidence="0.993517">
3.2 Debate Intention Tagging Schema
</subsectionHeader>
<bodyText confidence="0.981384571428571">
Based on the intent each utterance can have, we
have devised a debate specific intention tagging
schema. In debates, users either express their
opinion or state a known fact.
For a dual-sided debate between topic A and
topic B, our tagging schema includes the follow-
ing intention tags:
</bodyText>
<listItem confidence="0.848748">
1. A+ and B+ : These tags capture users in-
</listItem>
<bodyText confidence="0.759612705882353">
tent to support topic A or B. For example,
in utterance Superman is very nearly inde-
structible. the user is supporting Supermans
indestructibility in the debate between Super-
man and Batman.
2. A and B : These tags capture users in-
tent to oppose topic A or B. For example,
Superman is a stupid person who has an ob-
vious weakness, like cyclops. the user is op-
posing Superman by pointing out his weak-
ness.
3. NI: This category includes utterances which
hold no sentiment towards the debate topics
or can utter about non-debate topic entities,
In utterance We are voting for who will win
in a battle between these two. is neither
praising nor opposing either of the sides.
</bodyText>
<subsectionHeader confidence="0.344242">
Type Discourse Connectives
</subsectionHeader>
<bodyText confidence="0.948450866666667">
Contrast but, by comparison, by con-
trast, conversely, even though,
in contrast, in fact, instead, nev-
ertheless, on the contrary, on
the other hand, rather, whereas,
even if, however, because, as
Reason because, as
Result as a result, so, thus, therefore,
thereby
Elaboration for instance, in particular, in-
deed
Conjunction and, also, further, furthermore,
in addition, in fact, similarly,
indeed, meanwhile, more ever,
while
</bodyText>
<tableCaption confidence="0.778287">
Table 2: PDTB Discourse Markers List
</tableCaption>
<bodyText confidence="0.959277444444445">
Evaluation data was created by five linguists
who were provided with a complete set of instruc-
tions along with the sample annotated data. Each
utterance was annotated with its intention tag by 2
linguists and the inter-annotator agreement for the
evaluation data was 81.4%.
Table 3 gives a quantitative overview of the an-
notations in the corpus. There are total 12438 ut-
terances spread over 2420 debate posts.
</bodyText>
<table confidence="0.838271">
Tag A+ A B+ B NI
Corpus% 20.8 18.4 16.7 21.8 22.3
</table>
<tableCaption confidence="0.992041">
Table 3: Utterance Annotation Statistics
</tableCaption>
<subsectionHeader confidence="0.997295">
3.3 Topic Directed Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.999375">
To identify intetion behind each utterance, we cal-
culate debate topic directed sentiment. In topic di-
rected sentiment analysis, the sentiment score is
calculated using dependency parses of utterances
and the sentiment lexicon sentiWordNet (Bac-
cianella et al., 2010). sentiWordNet is a lexical
corpus used for opinion mining. It stores positive
and negative sentiment scores for every sense of
the word present in the wordNet (Fellbaum, 2010).
First, pronoun referencing is resolved using the
Stanford co-reference resolution system (Lee et
al., 2011). Using the Stanford dependency parser
(De Marneffe et al., 2006), utterances are repre-
sented in a tree format where each node represents
an utterance word storing its sentiment score and
the edges represents dependency relations. Each
</bodyText>
<page confidence="0.982251">
63
</page>
<equation confidence="0.981161">
\x0cparentScore = sign(parentScore) (|parentScore |+ updateScore(childScore)) (1)
</equation>
<bodyText confidence="0.961707833333333">
utterance word is looked in the sentiWordNet and
the sentiment score calculated using Algorithm
1 is stored in the words tree node. For words
missing from sentiWordNet, average of sentiment
scores of its synset member words is stored in the
words tree node, otherwise a zero sentiment score
</bodyText>
<listItem confidence="0.790999909090909">
is stored. If words are modified by negation words
like {never,not,nonetheless,etc.}, their senti-
ment scores are negated.
Algorithm 1 Word Sentiment Score
1: S Senses of word W
2: wordScore 0
3: for all s S do
4: sscore = sposScore snegscore
5: wordScore = wordScore + sscore
6: end for
7: wordScore = wordScore
</listItem>
<equation confidence="0.402005">
|S|
</equation>
<bodyText confidence="0.999499416666667">
In noun phrases such as great warrior, cruel
person, etc., the first word being the adjective of
the latter, it influences its sentiment score. Thus,
based on the semantic significance of the depen-
dency relation each edge holds, sentiment score of
parent nodes are updated with that of child nodes
using Equation 1. Tree structure and recursive na-
ture of Equation 1 ensures that sentiment scores
of child nodes are updated before updating their
parents sentiment scores. Table 4 lists the seman-
tically significant dependency relations used to up-
date parent node scores.
</bodyText>
<subsectionHeader confidence="0.76455">
Type Dependency Relations
</subsectionHeader>
<bodyText confidence="0.978047075">
Noun Modifying nn, amod, appos, abbrev,
infmod, poss, rcmod, rel,
prep
Verb Modifying advmod, acomp, ad-
vcl, ccomp, prt, purpcl,
xcomp, parataxis, prep
Table 4: List of Dependency Relations
In a sentence, Batman killed a bad guy., sen-
timent score of word Batman is affected by ac-
tion kill and thus for verb-predicate relations
like nsubj,dobj,cobj,iobj,etc., predicate sen-
timent scores are updated with that of verb scores
using Equation 1.
Extended targets (extendedTargets) are the en-
tities closely related to debate topics. For exam-
ple, Joker,Clarke Kent are related to Batman
and Darth Vader, Yoda to Star Wars. To ex-
tract the extended targets, we capture named enti-
ties (NE) from the Wikipedia page of the debate
topic (fetched using jsoup java library) using the
Stanford Named Entity Recognizer (Finkel et al.,
2005) and sort them based on their page occur-
rence count. Out of top-k (k = 20) NEs, some can
belong to both of the debate topics. For example,
DC Comics is common between Superman and
Batman. We remove these NEs from individual
lists and the remaining NEs are treated as extended
targets (extendedTargets) of the debate topics.
Debate topic directed sentiment scores are cal-
culated by adding the sentiment scores of the utter-
ance words which belong to the extended targets
list of each debate topic. We refer to these scores
as AScore and BScore representing scores directed
towards topics A and B. We also count the oc-
currences of each debate topic in the utterance by
checking word with topics extended targets.
We use these topic sentiment scores along with
utterance lexical features mentioned in Table 5 to
classify utterance intentions into one of the pro-
posed 5 intention tags.
</bodyText>
<table confidence="0.688501733333333">
Set Description/Example
Unigrams,
Bigrams
Word and word pair frequen-
cies
Cue Words Sentece beginning unigrams
and bigrams
Verb Frame Opinion, action or statement
verb
Sentiment
Count
count of subjective adjectives
and adverbs
topic Count count of words representing
debate topics
</table>
<tableCaption confidence="0.959289">
Table 5: Lexical Features for Intention Capturing
</tableCaption>
<bodyText confidence="0.9738214">
We analyze the experiments and results on cap-
turing user intention in Subsection 6.1. User inten-
tions are used in building the intention structure,
thus to calculate the sentiment score of the entire
post.
</bodyText>
<page confidence="0.773129">
64
</page>
<figure confidence="0.6487278">
\x0cPost Sentiment Score =
X
A
(A Score) where A Argument Structure (2)
4 Argument Structure and Post
</figure>
<subsectionHeader confidence="0.502439">
Sentiment Score
</subsectionHeader>
<bodyText confidence="0.967609411764706">
Arguments are the basis of persuasive communi-
cation. An argument is a set of statements of
which one (conclusion) is supported by others
(premises). In our debate data, the implicit con-
clusion is to support/oppose the debate topics and
premises are users opinion/knowledge about the
topics. Thus, neighboring utterances with same in-
tentions are merged into single argument forming
the argument structure for debate posts. Argument
structure, also referred to as Intention Structure,
may contain multiple arguments with different in-
tentions. But to identify the intention behind the
entire post, we need to consider sentiment strength
and correlation of each argument.
Sentiment Strength: Sentiment strength of ar-
guments with different intentions are compared to
compute intention behind entire post. Algorithm
</bodyText>
<figure confidence="0.3971569">
2 the computes sentiment strength of an argument
from its constituent utterances.
Algorithm 2 Argument Sentiment Score
1: U Argument Utterances
2: for all u U do
3: uscore = uAScore uBscore
4: end for
5: Argument Score =
P
u U (uscore)
</figure>
<bodyText confidence="0.969847285714286">
First example in Table 6 shows two utterances
one of which praising Superman and other prais-
ing Batman. Our argument structure has two ar-
guments containing an utterance each. Comparing
the sentiment strength of the 2 arguments, we can
conclude that author supports Batman in this ex-
ample.
</bodyText>
<table confidence="0.427965">
Debate Post Score
A1 Superman is a good person. 0.34
A2 Batman is the best hero ever. 0.62
A1 Superman has high speed,
agility and awesome strength.
1.23
A2 But, Batman is a better hero. 0.42
</table>
<tableCaption confidence="0.834276">
Table 6: Argument Structure Examples
</tableCaption>
<bodyText confidence="0.909234">
Correlation Between Arguments: The Second
example in Table 6 shows that though the first ar-
gument has a higher sentiment strength, the con-
trasting discourse marker but nullifies it, result-
ing in an overall stance supporting Batman. Dis-
course markers listed in the first row of Table 2 are
used to identify contrast between two utterances
out of which sentiment strength of the former ut-
terance is nullified.
Algorithm 3 Utterance Level Sentiment Score
</bodyText>
<listItem confidence="0.97940925">
1: U Post Utterances
2: postScore 0
3: for u U do
4: uscore = uAScore uBscore
5: uweight =  ||U|
2 uposition |
6: postScore = postScore+uscoreuweight
7: end for
</listItem>
<subsectionHeader confidence="0.997023">
4.1 Calculating Post Sentiment Score
</subsectionHeader>
<bodyText confidence="0.914880458333333">
To calculate sentiment score of the entire post,
three different approaches mentioned below are
tried out:
1. uttrScore (Utterance Level): Given two utter-
ances connected by a contrasting discourse
markers from Table 2, sentiment score of
the former is nullified. The posts sentiment
scores are calculated using Algorithm 3. In
this algorithm, the utterance score is multi-
plied by function of its position (line 5) which
gives more importance to the initial and end-
ing utterances than to those in the middle.
2. argScore (Argument Level): First, the sen-
timent score of each argument is calculated
using Algorithm 2. As in the above method,
sentiment score of the former argument con-
nected with contrast discourse marker is nul-
lified and then posts sentiment scores are
calculated using Equation 2.
3. argSpanScore (Argument Level with Span):
For each argument in the posts, argument
score is multiplied by its span i.e., the number
of utterances in an argument. We use Equa-
tion 2 to calculate posts sentiment score.
</bodyText>
<page confidence="0.995806">
65
</page>
<bodyText confidence="0.9898594">
\x0cCount of each debate topic entities in the posts
and of each intention type are used as post features
along with the posts sentiment scores to classify
their stance, the results of which are discussed in
Subsection 6.2.
</bodyText>
<sectionHeader confidence="0.960854" genericHeader="method">
5 Gradient Ascent Method
</sectionHeader>
<bodyText confidence="0.996310956521739">
In the previous section, sentiment score and inten-
tion of the utterances were used to calculate the
posts sentiment scores. In this section, we focus
on the use of the dialogue structure of the debate
to improve stance classification. convinceme.net
stores user information for posts and also provides
an option to rebut another posts. Intuitively, the
rebutting posts should have opposite stances and
same author posts should support the same stance.
Walker et al. (2012) uses the same intuition to split
the debate posts in two partitions using a max-cut
algorithm. This approach loses the posts indi-
viduality because it assigns the same stance to all
posts belonging to a partition. Our approach de-
scribed below uses the debate structure to refine
posts sentiment scores, calculated in the previous
section, thus maintaining post individuality.
If two posts by same author P1 and P2 have
sentiment scores 0.1 and 0.7, the previous ap-
proach would classify post P1 as supporting topic
B and P2 as supporting A, even if they are by
same author and supporting the same stance. What
if an error crept in while calculating post senti-
ment or utterance sentiment score? Can we use the
debate structure to refine posts sentiment scores
such that same author posts support same stance
and rebuttal author posts support opposite stance?
We use a gradient ascent method to accomplish
this task.
Gradient ascent is a greedy, hill-climbing ap-
proach used to find the local maxima of a function.
It maximizes a health/cost function by taking steps
proportional to gradient of the health function at a
given point. In our case, the dialogue structure of
the debate is represented by a Graph G(V, E) us-
ing rebuttal and same author links. Nodes (v V )
of graph represents debate posts with their senti-
ment score, and edges (e E) represent the di-
alogue information between two posts with value
1 denoting same author posts and 1 rebutting
participant posts.
We formulate the health function H(G) which
measures the health of the given graph G(V, E)
in Algorithm 4. This health function signifies the
health or correctness of each edge in the debate
structure.
</bodyText>
<figure confidence="0.976012285714286">
Algorithm 4 Debate Health Function
Require: Debate Graph G(V,E)
1: H(G) 0
2: for all eij E do
3: if eij = 1 then
4: if Vi Vj &amp;gt; 0 then
5: H(G) = H(G) + 1
6: else
7: H(G) = H(G) + (1
|ViVj|
2 )
8: end if
9: else
10: if Vi Vj &lt; 0 then
11: H(G) = H(G) + 1
12: else
13: H(G) = H(G) + |Vi Vj|
14: end if
15: end if
16: end for
Return H(G)
</figure>
<bodyText confidence="0.979270321428571">
It calculates health of each edge based on dia-
logue information it holds and participating nodes
scores. A perfect score of 1 is assigned to each
edge if participating nodes satisfy edge criteria
(line 45, 1011). If not, difference of nodes sen-
timent scores are used to calculate edges health.
(line 67, 1213)
For an imperfect edge, updating sentiment
scores of its connecting nodes will increase its
health thus improving health of the graph. Thus
we aim to increase the health of the graph by grad-
ually modifying posts sentiment scores.
Gradient Ascent algorithm (Algorithm 5) is fed
with parameters set to (EPOCH = 1000, =
0.01 and = 0.05). For each iteration, let
G(V, E) represent the current state of the graph
and H its health. For each node, the sentiment
score is updated by adding partial derivative of
health function with respect to given node at the
current state (line 9). Partial derivative of the
Health function with respect to current node is de-
fined in line 8. This continues (line 1 15) untill
there is no such node which improves the graphs
health or till the number of iterations reach epoch.
These refined post sentiment scores along with
post features (topic Count and intention type
count) are used to classify posts stance. We dis-
cuss the results in Subsection 6.2.
</bodyText>
<page confidence="0.891207">
66
</page>
<table confidence="0.808398666666667">
\x0cAlgorithm 5 Gradient Ascent Approach
Require: Debate Graph G(V,E) and H(G) Health
Function
</table>
<listItem confidence="0.668373588235294">
1: for iteraton = 1 EPOCH do
2: H Health(G(V, E))
3: newH H
4: for all vi V do
5: V 0 V
6: v0
i v0
i +
7: H0 Health(G0(V 0, E))
8: PDi (H0H)
9: vi vi + PDi
10: newH = max(newH, H0)
11: end for
12: if newH = H then
13: Break
14: end if
15: end for
</listItem>
<bodyText confidence="0.936014666666667">
Figure 1 gives a working example of our ap-
proach. It clearly shows improving health of the
graph using the gradient ascent method helps in
</bodyText>
<figureCaption confidence="0.843737">
rectifying post P1s stance.
Figure 1: Working Example of Gradient Ascent
</figureCaption>
<sectionHeader confidence="0.972068" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9982625">
This section highlights experiments, results, ad-
vantages and shortcomings of our approach on in-
tention capturing and posts stance classification
tasks.
</bodyText>
<subsectionHeader confidence="0.999783">
6.1 Capturing User Intentions
</subsectionHeader>
<bodyText confidence="0.99291778125">
Experiments on debate posts from following de-
bates are carried out: Superman vs Batman, Fire-
fox vs Internet Explorer, Cats vs Dogs, Ninja vs
Pirates and Star Wars vs Lord Of The Rings. Our
experimental data for utterances intention captur-
ing includes 1928 posts and 9015 utterances from
5 debates with equal intention class distribution
for each domain. Thus our data has 1803 correctly
annotated utterances belonging to each intention
class. The first task focuses on classifying each
utterance into one of the proposed intention tags.
Our first baseline is a Unigram system which
uses unigram content information of the utter-
ances. Unigram systems are proved reliable in
sentiment analysis (Mullen and Collier, 2004;
Pang and Lee, 2004). The second baseline sys-
tem LexFeatures uses the lexical features (Table
5). This baseline system is a strong baseline for the
evaluation because it captures sentiment as well as
pragmatic information of the utterances. We con-
struct two systems to capture intentions: a Topic-
Score system which uses the topic directed sen-
timent scores (described in Subsection 3.3) and
topic occurrence counts to capture utterance in-
tentions, and a TopicScore+LexFeatures system
which uses topic sentiment scores (described in
Subsection 3.3) along with lexical features in Ta-
ble 5. All systems are implemented using the
Weka toolkit with its standard SVM implementa-
tion. Table 7 shows the accuracies of classifying
utterance intentions for each of described baseline
and proposed systems.
</bodyText>
<table confidence="0.972719714285714">
Accuracy Total A+ A B+ B NI
Unigram 64.2 63.2 65.4 60.3 66.5 65.6
LexFeatures 62.7 64.3 60.7 64.2 61.9 62.4
TopicScore 68.4 68.1 68.7 67.2 68.7 69.3
TopicScore+
LexFeatures
74.3 73.9 74.8 75.1 73.6 74.1
</table>
<tableCaption confidence="0.7289305">
Table 7: Accuracy of Utterance Intention Classifi-
cation
</tableCaption>
<bodyText confidence="0.989796136363637">
Overall we notice that the proposed approaches
perform better than baseline systems, with Top-
icScore+LexFeatures outperforming all systems.
This shows that topic directed sentiment score
helps in capturing users intentions better than the
word level sentiment analysis. We can also con-
clude that the Unigram system achieves higher ac-
curacies than the lexFeatures system, showing that
what the user says is a better indicator of users
intentions than his sentiments and thus confirm-
ing previous research results (Somasundaran and
Wiebe, 2010; Pang and Lee, 2008). TopicScore
performs lower in capturing NI tag than the base-
line systems, denoting that TopicScore is not cap-
turing debate topics and their sentiments correctly.
Thus it assigns non-NI tagged utterances an NI
tag, lowering its accuracy.
We run the same approach but comparing utter-
ance words only with the debate topics in calculat-
ing topic directed sentiment score and not with the
lists of extended targets. This produces an accu-
racy of 70.8% clearly highlighting the importance
</bodyText>
<page confidence="0.992654">
67
</page>
<bodyText confidence="0.976652">
\x0cof extended targets in calculating debate topic di-
rected sentiment analysis.
</bodyText>
<subsectionHeader confidence="0.998644">
6.2 Post Stance Classification
</subsectionHeader>
<bodyText confidence="0.997653034482759">
Experiment data covers 2040 posts with equal
topic stance distribution from each of the follow-
ing domains: Superman vs Batman, Firefox vs
Internet Explorer, Cats vs Dogs, Ninja vs Pirates
and Star Wars vs Lord Of The Rings. Two base-
line systems are designed for this task of debate
posts stance classification. The first baseline,
sentVicinity, assigns each words sentiment score
to the closest debate topic entity. Then, the senti-
ment score of the debate topics over an entire post
is compared to classify post stance. The second
baseline, subjTopic, counts the number of subjec-
tive words in each utterance of the post and assigns
them to debate topic related entity if present in
the utterance. It compares overall subjective pos-
itivity of debate topics to assign post stance. We
also compared our approach with the (Arg+Sent)
method proposed by Somasundaran and Wiebe
(2010).
Three systems described in Subsection 4.1 are
used to compute posts sentiment score by ana-
lyzing its content namely, uttrScore, argScore and
argSpanScore. Post sentiment scores from these
three techniques along with post features (topic
Count and intention type count) are used to clas-
sify post stance and results are compared in Table
8. Table 8 shows that the second approach of cal-
culating posts sentiment scores using their argu-
ment structure outperforms the other approaches.
</bodyText>
<table confidence="0.721645285714286">
System Accuracy
sentVicinity 61.6%
subjTopic 58.1%
Arg+Sent 63.9%
uttrScore 67.4%
argScore 70.3%
argSpanScore 69.2%
</table>
<tableCaption confidence="0.99606">
Table 8: Stance Classification Using Post Content
</tableCaption>
<bodyText confidence="0.996158">
Our approach perform better than Somasun-
daran and Wiebe (2010)s approach signifying the
importance of identifying target-opinion depen-
dency relation as opposed to assigning the opin-
ion words to each content word in the sentence.
It is important to notice that the argSpanScore
method which multiplies argument score by its
span doesnt perform as well as argScore alone.
This shows the utterance sentiment strength mat-
ters more than neighboring same intention utter-
ance. This supports our hypothesis that online de-
bate posts focus more on sentiments rather than
discourse coherence.
We experiment with gradient ascent approach
and study how refining posts sentiment scores
based on the dialogue structure of the debate helps
improving stance classification. Table 9 gives the
classification accuracies between argScore tech-
nique and gradient ascent method.
</bodyText>
<table confidence="0.852111571428571">
System
Accuracy
Total Dialogue Non-dialogue
argScore 70.3% 70.5% 70.1%
argScore + gra-
dientAscent
74.4% 80.1% 70.1%
</table>
<tableCaption confidence="0.995875">
Table 9: Stance Classification: Dialogue Structure
</tableCaption>
<bodyText confidence="0.999748142857143">
The dialogue column in Table 9 shows accura-
cies for posts participating in dialogue structure
i.e., those linked to other post with same author or
rebutting links. It shows a remarkable improve-
ment (10% gain) which clearly signifies impor-
tance of the dialogue structure. The non-dialogue
column shows the accuracies for posts not in-
volved in dialogue structure. As health function
for debate graph is a function of dialogue partici-
pating posts, it does not affect stance classification
accuracy for non-dialogue participating posts. Di-
alogue participating posts cover 41% of the exper-
iment data giving 4% accuracy improvement over
argScore system on complete dataset.
</bodyText>
<sectionHeader confidence="0.998612" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999737">
In this paper, We designed debate specific utter-
ance level intention tags and described a topic
directed sentiment analysis approach to capture
these intentions. We proposed a novel approach to
capture the posts intention structure. Our results
validate our hypothesis that capturing user inten-
tions and post intention structure helps in classi-
fying posts stance. It also emphasizes the impor-
tance of building the intention structure rather than
just aggregating utterances sentiment scores.
This is the first application of Gradient Ascent
method for stance classification. Results show
re-modifying the posts sentiment scores by tak-
ing the debates structure into account highly im-
proves stance classification accuracies over inten-
tion based method. We aim to apply topic directed
sentiment scores along with lexical features for de-
bate summarization in our future work.
</bodyText>
<page confidence="0.993172">
68
</page>
<reference confidence="0.99718412745098">
\x0cReferences
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of twitter data. In Proceedings of the Work-
shop on Languages in Social Media, pages 3038.
Association for Computational Linguistics.
P. Anand, M. Walker, R. Abbott, J.E.F. Tree, R. Bow-
mani, and M. Minor. 2011. Cats rule and dogs
drool!: Classifying stance in online debate. In Pro-
ceedings of the 2nd Workshop on Computational
Approaches to Subjectivity and Sentiment Analysis
(WASSA 2.011), pages 19.
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Sen-
tiwordnet 3.0: An enhanced lexical resource for sen-
timent analysis and opinion mining. In Proceed-
ings of the Seventh conference on International Lan-
guage Resources and Evaluation (LREC10), Val-
letta, Malta, May. European Language Resources
Association (ELRA).
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC,
volume 6, pages 449454.
Christiane Fellbaum. 2010. WordNet. Springer.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363370. Association for Computational Lin-
guistics.
A. Lampert, R. Dale, and C. Paris. 2006. Classifying
speech acts using verbal response modes. In Aus-
tralasian Language Technology Workshop, page 34.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanfords multi-pass sieve coref-
erence resolution system at the conll-2011 shared
task. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 2834. Association for Computational
Linguistics.
Subhabrata Mukherjee and Pushpak Bhattacharyya.
2012. Feature specific sentiment analysis for prod-
uct reviews. In Computational Linguistics and In-
telligent Text Processing, pages 475487. Springer.
Tony Mullen and Nigel Collier. 2004. Sentiment anal-
ysis using support vector machines with diverse in-
formation sources. In Proceedings of EMNLP, vol-
ume 4, pages 412418.
Neil OHare, Michael Davy, Adam Bermingham,
Paul Ferguson, Paraic Sheridan, Cathal Gurrin, and
Alan F Smeaton. 2009. Topic-dependent sentiment
analysis of financial blogs. In Proceedings of the 1st
international CIKM workshop on Topic-sentiment
analysis for mass opinion, pages 916. ACM.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd annual meeting on Association for Compu-
tational Linguistics, page 271. Association for Com-
putational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1135.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The penn discourse treebank
2.0. In LREC. Citeseer.
S. Somasundaran and J. Wiebe. 2009. Recognizing
stances in online debates. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1-Volume 1, pages 226234. Association for
Computational Linguistics.
S. Somasundaran and J. Wiebe. 2010. Recognizing
stances in ideological on-line debates. In Proceed-
ings of the NAACL HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation
of Emotion in Text, pages 116124. Association for
Computational Linguistics.
Arpit Sood, Thanvir P Mohamed, and Vasudeva Varma.
2013. Topic-focused summarization of chat conver-
sations. In Advances in Information Retrieval, pages
800803. Springer.
M.A. Walker, P. Anand, R. Abbott, and R. Grant.
2012. Stance classification using dialogic proper-
ties of persuasion. Proceedings of the 2012 Con-
ference of the North American Chapter of the As-
sociaotion for Computational Linguistics: Human
Language Technologies, pages 592596.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, 39(2):165
210.
S. Yelati and R. Sangal. 2011. Novel approach for
tagging of discourse segments in help-desk e-mails.
In Proceedings of the 2011 IEEE/WIC/ACM Inter-
national Conferences on Web Intelligence and Intel-
ligent Agent Technology-Volume 03, pages 369372.
IEEE Computer Society.
</reference>
<page confidence="0.920464">
69
</page>
<figure confidence="0.298326">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.796164">
<note confidence="0.933408666666667">b&amp;apos;Proceedings of the SIGDIAL 2013 Conference, pages 6169, Metz, France, 22-24 August 2013. c 2013 Association for Computational Linguistics</note>
<title confidence="0.991652">Stance Classification in Online Debates by Recognizing Users Intentions</title>
<author confidence="0.994993">Sarvesh Ranade</author>
<author confidence="0.994993">Rajeev Sangal</author>
<author confidence="0.994993">Radhika Mamidi</author>
<affiliation confidence="0.997184">Language Technologies Research Centre International Institute of Information Technology</affiliation>
<address confidence="0.969424">Hyderabad, India</address>
<email confidence="0.990989">sarvesh.ranade@research.iiit.ac.in,{sangal,radhika.mamidi}@iiit.ac.in</email>
<abstract confidence="0.998652055555555">Online debate forums provide a rich collection of differing opinions on various topics. In dual-sided debates, users present their opinions or judge others opinions to support their stance. In this paper, we examine the use of users intentions and debate structure for stance classification of the debate posts. We propose a domain independent approach to capture users intent at sentence level using its dependency parse and sentiWordNet and to build the intention structure of the post to identify its stance. To aid the task of classification, we define the health of the debate structure and show that maximizing its value leads to better stance classification accuracies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment analysis of twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media,</booktitle>
<pages>3038</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6965" citStr="Agarwal et al., 2011" startWordPosition="1080" endWordPosition="1084"> to the debate topic closest to them. Probabilistic association learning of target-opinion pair and the debate topic was used by Somasundaran and Wiebe (2010) as an integer linear programming problem to classify posts stance. Even though opinions might not be directed towards debate topics, these approaches attach the opinions to debate topics based only on their context cooccurrence. Our approach finds the target word for an opinion expression by analyzing the full dependency parse of the utterance. There has also been a lot of work done in social media on target directed sentiment analysis (Agarwal et al., 2011; OHare et al., 2009; Mukherjee and Bhattacharyya, 2012) which we incorporate for capturing users intentions. Agarwal et al. (2011) used syntactic features as target dependent features to differentiate sentiments effect on different targets in a tweet. OHare et al. (2009) employed a word-based approach to determine sentiments directed towards companies and their stocks from financial blogs. Mukherjee and Bhattacharyya (2012) applied clustering to extract feature specific opinions and calculated the overall feature sentiment using subjectivity lexicon. Discourse markers cues were used by Sood (</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>\x0cReferences Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment analysis of twitter data. In Proceedings of the Workshop on Languages in Social Media, pages 3038. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Anand</author>
<author>M Walker</author>
<author>R Abbott</author>
<author>J E F Tree</author>
<author>R Bowmani</author>
<author>M Minor</author>
</authors>
<title>Cats rule and dogs drool!: Classifying stance in online debate.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA</booktitle>
<pages>19</pages>
<contexts>
<context position="5102" citStr="Anand et al., 2011" startWordPosition="789" endWordPosition="792">e entire post. We explain our stance classification method using post content features and post intention structure in this section. Section 5 describes the use of the dialogue structure of the debate and presents a gradient ascent method for re-evaluating posts stance. We present experiments and results on capturing users intentions and stance classification in Section 6. This is followed by conclusions in Section 7. 2 Related Work To classify posts stance in dual-sided debates, previous approaches have used probabilistic (Somasundaran and Wiebe, 2009) as well as machine learning techniques (Anand et al., 2011; Somasundaran and Wiebe, 2010). Some approaches extensively used the dialogue structure to identify posts stance (Walker et al., 2012) whereas others considered opinion expressions and their targets essential to capture sentiment in the posts towards debate topics (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). Pure machine learning approaches (Anand et al., 2011) have extracted lexical and contextual features of debate posts to classify their stance. Walker et al. (2012) partitioned the debate posts based on the dialogue structure of the debate and assigned stance to a partitio</context>
</contexts>
<marker>Anand, Walker, Abbott, Tree, Bowmani, Minor, 2011</marker>
<rawString>P. Anand, M. Walker, R. Abbott, J.E.F. Tree, R. Bowmani, and M. Minor. 2011. Cats rule and dogs drool!: Classifying stance in online debate. In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2.011), pages 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Baccianella</author>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC10),</booktitle>
<location>Valletta, Malta,</location>
<contexts>
<context position="11707" citStr="Baccianella et al., 2010" startWordPosition="1835" endWordPosition="1839">ention tag by 2 linguists and the inter-annotator agreement for the evaluation data was 81.4%. Table 3 gives a quantitative overview of the annotations in the corpus. There are total 12438 utterances spread over 2420 debate posts. Tag A+ A B+ B NI Corpus% 20.8 18.4 16.7 21.8 22.3 Table 3: Utterance Annotation Statistics 3.3 Topic Directed Sentiment Analysis To identify intetion behind each utterance, we calculate debate topic directed sentiment. In topic directed sentiment analysis, the sentiment score is calculated using dependency parses of utterances and the sentiment lexicon sentiWordNet (Baccianella et al., 2010). sentiWordNet is a lexical corpus used for opinion mining. It stores positive and negative sentiment scores for every sense of the word present in the wordNet (Fellbaum, 2010). First, pronoun referencing is resolved using the Stanford co-reference resolution system (Lee et al., 2011). Using the Stanford dependency parser (De Marneffe et al., 2006), utterances are represented in a tree format where each node represents an utterance word storing its sentiment score and the edges represents dependency relations. Each 63 \x0cparentScore = sign(parentScore) (|parentScore |+ updateScore(childScore)</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC10), Valletta, Malta, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C De Marneffe</author>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>449454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>M.C. De Marneffe, B. MacCartney, and C.D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>2010</date>
<publisher>WordNet. Springer.</publisher>
<contexts>
<context position="11883" citStr="Fellbaum, 2010" startWordPosition="1866" endWordPosition="1867">38 utterances spread over 2420 debate posts. Tag A+ A B+ B NI Corpus% 20.8 18.4 16.7 21.8 22.3 Table 3: Utterance Annotation Statistics 3.3 Topic Directed Sentiment Analysis To identify intetion behind each utterance, we calculate debate topic directed sentiment. In topic directed sentiment analysis, the sentiment score is calculated using dependency parses of utterances and the sentiment lexicon sentiWordNet (Baccianella et al., 2010). sentiWordNet is a lexical corpus used for opinion mining. It stores positive and negative sentiment scores for every sense of the word present in the wordNet (Fellbaum, 2010). First, pronoun referencing is resolved using the Stanford co-reference resolution system (Lee et al., 2011). Using the Stanford dependency parser (De Marneffe et al., 2006), utterances are represented in a tree format where each node represents an utterance word storing its sentiment score and the edges represents dependency relations. Each 63 \x0cparentScore = sign(parentScore) (|parentScore |+ updateScore(childScore)) (1) utterance word is looked in the sentiWordNet and the sentiment score calculated using Algorithm 1 is stored in the words tree node. For words missing from sentiWordNet, a</context>
</contexts>
<marker>Fellbaum, 2010</marker>
<rawString>Christiane Fellbaum. 2010. WordNet. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14338" citStr="Finkel et al., 2005" startWordPosition="2256" endWordPosition="2259">, Batman killed a bad guy., sentiment score of word Batman is affected by action kill and thus for verb-predicate relations like nsubj,dobj,cobj,iobj,etc., predicate sentiment scores are updated with that of verb scores using Equation 1. Extended targets (extendedTargets) are the entities closely related to debate topics. For example, Joker,Clarke Kent are related to Batman and Darth Vader, Yoda to Star Wars. To extract the extended targets, we capture named entities (NE) from the Wikipedia page of the debate topic (fetched using jsoup java library) using the Stanford Named Entity Recognizer (Finkel et al., 2005) and sort them based on their page occurrence count. Out of top-k (k = 20) NEs, some can belong to both of the debate topics. For example, DC Comics is common between Superman and Batman. We remove these NEs from individual lists and the remaining NEs are treated as extended targets (extendedTargets) of the debate topics. Debate topic directed sentiment scores are calculated by adding the sentiment scores of the utterance words which belong to the extended targets list of each debate topic. We refer to these scores as AScore and BScore representing scores directed towards topics A and B. We al</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lampert</author>
<author>R Dale</author>
<author>C Paris</author>
</authors>
<title>Classifying speech acts using verbal response modes.</title>
<date>2006</date>
<booktitle>In Australasian Language Technology Workshop,</booktitle>
<pages>34</pages>
<contexts>
<context position="7992" citStr="Lampert et al. (2006)" startWordPosition="1239" endWordPosition="1242">d Bhattacharyya (2012) applied clustering to extract feature specific opinions and calculated the overall feature sentiment using subjectivity lexicon. Discourse markers cues were used by Sood (2013) to prioritize the conversational sentences and by Yelati and Sangal (2011) to identify users intentions in help desk emails. Most of the discourse analysis theories defined their own discourse segment tagging schema to understand the meaning of each utterance. Yelati and Sangal (2011) devised a help desk specific tagging schema to capture the queries and build email structure in help desk emails. Lampert et al. (2006) used verbal response modes to understand the client/therapist conversations. We incorporate target directed sentiment analysis to capture utterance level intentions using a sentiment lexicon and dependency parses as described in the following section. 3 Capturing User Intentions Users intention at the utterance level plays a vital role in overall stance taking. We define a set of intentions each utterance can hold. The proposed topic directed sentiment analysis based approach will automatically identify users intention behind each utterance. Because of the unstructured and noisy nature of soc</context>
</contexts>
<marker>Lampert, Dale, Paris, 2006</marker>
<rawString>A. Lampert, R. Dale, and C. Paris. 2006. Classifying speech acts using verbal response modes. In Australasian Language Technology Workshop, page 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanfords multi-pass sieve coreference resolution system at the conll-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>2834</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11992" citStr="Lee et al., 2011" startWordPosition="1879" endWordPosition="1882">erance Annotation Statistics 3.3 Topic Directed Sentiment Analysis To identify intetion behind each utterance, we calculate debate topic directed sentiment. In topic directed sentiment analysis, the sentiment score is calculated using dependency parses of utterances and the sentiment lexicon sentiWordNet (Baccianella et al., 2010). sentiWordNet is a lexical corpus used for opinion mining. It stores positive and negative sentiment scores for every sense of the word present in the wordNet (Fellbaum, 2010). First, pronoun referencing is resolved using the Stanford co-reference resolution system (Lee et al., 2011). Using the Stanford dependency parser (De Marneffe et al., 2006), utterances are represented in a tree format where each node represents an utterance word storing its sentiment score and the edges represents dependency relations. Each 63 \x0cparentScore = sign(parentScore) (|parentScore |+ updateScore(childScore)) (1) utterance word is looked in the sentiWordNet and the sentiment score calculated using Algorithm 1 is stored in the words tree node. For words missing from sentiWordNet, average of sentiment scores of its synset member words is stored in the words tree node, otherwise a zero sent</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanfords multi-pass sieve coreference resolution system at the conll-2011 shared task. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 2834. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Subhabrata Mukherjee</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Feature specific sentiment analysis for product reviews.</title>
<date>2012</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>475487</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="7021" citStr="Mukherjee and Bhattacharyya, 2012" startWordPosition="1089" endWordPosition="1093">abilistic association learning of target-opinion pair and the debate topic was used by Somasundaran and Wiebe (2010) as an integer linear programming problem to classify posts stance. Even though opinions might not be directed towards debate topics, these approaches attach the opinions to debate topics based only on their context cooccurrence. Our approach finds the target word for an opinion expression by analyzing the full dependency parse of the utterance. There has also been a lot of work done in social media on target directed sentiment analysis (Agarwal et al., 2011; OHare et al., 2009; Mukherjee and Bhattacharyya, 2012) which we incorporate for capturing users intentions. Agarwal et al. (2011) used syntactic features as target dependent features to differentiate sentiments effect on different targets in a tweet. OHare et al. (2009) employed a word-based approach to determine sentiments directed towards companies and their stocks from financial blogs. Mukherjee and Bhattacharyya (2012) applied clustering to extract feature specific opinions and calculated the overall feature sentiment using subjectivity lexicon. Discourse markers cues were used by Sood (2013) to prioritize the conversational sentences and by </context>
</contexts>
<marker>Mukherjee, Bhattacharyya, 2012</marker>
<rawString>Subhabrata Mukherjee and Pushpak Bhattacharyya. 2012. Feature specific sentiment analysis for product reviews. In Computational Linguistics and Intelligent Text Processing, pages 475487. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Mullen</author>
<author>Nigel Collier</author>
</authors>
<title>Sentiment analysis using support vector machines with diverse information sources.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<volume>4</volume>
<pages>412418</pages>
<contexts>
<context position="24606" citStr="Mullen and Collier, 2004" startWordPosition="4007" endWordPosition="4010">ox vs Internet Explorer, Cats vs Dogs, Ninja vs Pirates and Star Wars vs Lord Of The Rings. Our experimental data for utterances intention capturing includes 1928 posts and 9015 utterances from 5 debates with equal intention class distribution for each domain. Thus our data has 1803 correctly annotated utterances belonging to each intention class. The first task focuses on classifying each utterance into one of the proposed intention tags. Our first baseline is a Unigram system which uses unigram content information of the utterances. Unigram systems are proved reliable in sentiment analysis (Mullen and Collier, 2004; Pang and Lee, 2004). The second baseline system LexFeatures uses the lexical features (Table 5). This baseline system is a strong baseline for the evaluation because it captures sentiment as well as pragmatic information of the utterances. We construct two systems to capture intentions: a TopicScore system which uses the topic directed sentiment scores (described in Subsection 3.3) and topic occurrence counts to capture utterance intentions, and a TopicScore+LexFeatures system which uses topic sentiment scores (described in Subsection 3.3) along with lexical features in Table 5. All systems </context>
</contexts>
<marker>Mullen, Collier, 2004</marker>
<rawString>Tony Mullen and Nigel Collier. 2004. Sentiment analysis using support vector machines with diverse information sources. In Proceedings of EMNLP, volume 4, pages 412418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil OHare</author>
<author>Michael Davy</author>
<author>Adam Bermingham</author>
<author>Paul Ferguson</author>
<author>Paraic Sheridan</author>
<author>Cathal Gurrin</author>
<author>Alan F Smeaton</author>
</authors>
<title>Topic-dependent sentiment analysis of financial blogs.</title>
<date>2009</date>
<booktitle>In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion,</booktitle>
<pages>916</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6985" citStr="OHare et al., 2009" startWordPosition="1085" endWordPosition="1088">losest to them. Probabilistic association learning of target-opinion pair and the debate topic was used by Somasundaran and Wiebe (2010) as an integer linear programming problem to classify posts stance. Even though opinions might not be directed towards debate topics, these approaches attach the opinions to debate topics based only on their context cooccurrence. Our approach finds the target word for an opinion expression by analyzing the full dependency parse of the utterance. There has also been a lot of work done in social media on target directed sentiment analysis (Agarwal et al., 2011; OHare et al., 2009; Mukherjee and Bhattacharyya, 2012) which we incorporate for capturing users intentions. Agarwal et al. (2011) used syntactic features as target dependent features to differentiate sentiments effect on different targets in a tweet. OHare et al. (2009) employed a word-based approach to determine sentiments directed towards companies and their stocks from financial blogs. Mukherjee and Bhattacharyya (2012) applied clustering to extract feature specific opinions and calculated the overall feature sentiment using subjectivity lexicon. Discourse markers cues were used by Sood (2013) to prioritize </context>
</contexts>
<marker>OHare, Davy, Bermingham, Ferguson, Sheridan, Gurrin, Smeaton, 2009</marker>
<rawString>Neil OHare, Michael Davy, Adam Bermingham, Paul Ferguson, Paraic Sheridan, Cathal Gurrin, and Alan F Smeaton. 2009. Topic-dependent sentiment analysis of financial blogs. In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion, pages 916. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24627" citStr="Pang and Lee, 2004" startWordPosition="4011" endWordPosition="4014">ats vs Dogs, Ninja vs Pirates and Star Wars vs Lord Of The Rings. Our experimental data for utterances intention capturing includes 1928 posts and 9015 utterances from 5 debates with equal intention class distribution for each domain. Thus our data has 1803 correctly annotated utterances belonging to each intention class. The first task focuses on classifying each utterance into one of the proposed intention tags. Our first baseline is a Unigram system which uses unigram content information of the utterances. Unigram systems are proved reliable in sentiment analysis (Mullen and Collier, 2004; Pang and Lee, 2004). The second baseline system LexFeatures uses the lexical features (Table 5). This baseline system is a strong baseline for the evaluation because it captures sentiment as well as pragmatic information of the utterances. We construct two systems to capture intentions: a TopicScore system which uses the topic directed sentiment scores (described in Subsection 3.3) and topic occurrence counts to capture utterance intentions, and a TopicScore+LexFeatures system which uses topic sentiment scores (described in Subsection 3.3) along with lexical features in Table 5. All systems are implemented using</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and trends in information retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="26221" citStr="Pang and Lee, 2008" startWordPosition="4259" endWordPosition="4262">1 Table 7: Accuracy of Utterance Intention Classification Overall we notice that the proposed approaches perform better than baseline systems, with TopicScore+LexFeatures outperforming all systems. This shows that topic directed sentiment score helps in capturing users intentions better than the word level sentiment analysis. We can also conclude that the Unigram system achieves higher accuracies than the lexFeatures system, showing that what the user says is a better indicator of users intentions than his sentiments and thus confirming previous research results (Somasundaran and Wiebe, 2010; Pang and Lee, 2008). TopicScore performs lower in capturing NI tag than the baseline systems, denoting that TopicScore is not capturing debate topics and their sentiments correctly. Thus it assigns non-NI tagged utterances an NI tag, lowering its accuracy. We run the same approach but comparing utterance words only with the debate topics in calculating topic directed sentiment score and not with the lists of extended targets. This produces an accuracy of 70.8% clearly highlighting the importance 67 \x0cof extended targets in calculating debate topic directed sentiment analysis. 6.2 Post Stance Classification Exp</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind K Joshi</author>
<author>Bonnie L Webber</author>
</authors>
<title>The penn discourse treebank 2.0. In LREC.</title>
<date>2008</date>
<publisher>Citeseer.</publisher>
<contexts>
<context position="8894" citStr="Prasad et al., 2008" startWordPosition="1375" endWordPosition="1378">sers intention at the utterance level plays a vital role in overall stance taking. We define a set of intentions each utterance can hold. The proposed topic directed sentiment analysis based approach will automatically identify users intention behind each utterance. Because of the unstructured and noisy nature of social media, we need to pre-process the debate 62 \x0cdata before analyzing it further for users intentions. 3.1 Preprocessing The posts data is split into utterances, i.e. smallest discourse units, based on sentence ending markers and a few specific Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008) discourse markers listed in Table 2. Merged words like mindboggling, super-awesome, etc. are split based on the default Unix dictionary and special character delimiters. Once the debate posts are broken into utterances, we identify the intention behind each utterance in the post to compute entire posts stance. Table 1 presents the statistics of the debate data collected from convinceme.net. Debates Posts Author P/A Utterances 28 2040 1333 1.53 12438 Table 1: Debate Data Statistics 3.2 Debate Intention Tagging Schema Based on the intent each utterance can have, we have devised a debate specifi</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L Webber. 2008. The penn discourse treebank 2.0. In LREC. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>226234</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1607" citStr="Somasundaran and Wiebe, 2009" startWordPosition="235" endWordPosition="238">ssification, we define the health of the debate structure and show that maximizing its value leads to better stance classification accuracies. 1 Introduction Online debate forums provide Internet users a platform to discuss popular ideological debates. Debate in essence is a method of interactive and representational arguments. In an online debate, users make assertions with superior content to support their stance. Factual accuracy and emotional appeal are important features used to persuade the readers. It is easy to observe that personal opinions are important in ideological stance taking (Somasundaran and Wiebe, 2009). Because of the availability of Internet resources and time, people intelligently use the factual data to support their opinions. Online debates differ from public debates in terms of logical consistency. In online debates, users assert their opinion towards either side, sometimes ignoring discourse coherence required for logical soundness of the post. Generally they use strong degree of sentiment words including insulting or sarcastic remarks for greater emphasis of their point. Apart from supporting/opposing a side, users make factual statements such as Stan lee once said Superman is superi</context>
<context position="5043" citStr="Somasundaran and Wiebe, 2009" startWordPosition="778" endWordPosition="782">ribe the use the utterance level intentions to capture intention of the entire post. We explain our stance classification method using post content features and post intention structure in this section. Section 5 describes the use of the dialogue structure of the debate and presents a gradient ascent method for re-evaluating posts stance. We present experiments and results on capturing users intentions and stance classification in Section 6. This is followed by conclusions in Section 7. 2 Related Work To classify posts stance in dual-sided debates, previous approaches have used probabilistic (Somasundaran and Wiebe, 2009) as well as machine learning techniques (Anand et al., 2011; Somasundaran and Wiebe, 2010). Some approaches extensively used the dialogue structure to identify posts stance (Walker et al., 2012) whereas others considered opinion expressions and their targets essential to capture sentiment in the posts towards debate topics (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). Pure machine learning approaches (Anand et al., 2011) have extracted lexical and contextual features of debate posts to classify their stance. Walker et al. (2012) partitioned the debate posts based on the dialogu</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>S. Somasundaran and J. Wiebe. 2009. Recognizing stances in online debates. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 226234. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Wiebe</author>
</authors>
<title>Recognizing stances in ideological on-line debates.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>116124</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5133" citStr="Somasundaran and Wiebe, 2010" startWordPosition="793" endWordPosition="797">plain our stance classification method using post content features and post intention structure in this section. Section 5 describes the use of the dialogue structure of the debate and presents a gradient ascent method for re-evaluating posts stance. We present experiments and results on capturing users intentions and stance classification in Section 6. This is followed by conclusions in Section 7. 2 Related Work To classify posts stance in dual-sided debates, previous approaches have used probabilistic (Somasundaran and Wiebe, 2009) as well as machine learning techniques (Anand et al., 2011; Somasundaran and Wiebe, 2010). Some approaches extensively used the dialogue structure to identify posts stance (Walker et al., 2012) whereas others considered opinion expressions and their targets essential to capture sentiment in the posts towards debate topics (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). Pure machine learning approaches (Anand et al., 2011) have extracted lexical and contextual features of debate posts to classify their stance. Walker et al. (2012) partitioned the debate posts based on the dialogue structure of the debate and assigned stance to a partition using lexical features of can</context>
<context position="6503" citStr="Somasundaran and Wiebe (2010)" startWordPosition="1002" endWordPosition="1005">reas our approach treats each post individually. To extract opinion expressions, Somasundaran and Wiebe (2009) used the Subjectivity lexicon and Somasundaran and Wiebe (2010) used the MPQA opinion corpus (Wiebe et al., 2005). These opinion expressions were attached to the target words using different techniques. Somasundaran and Wiebe (2009) attached opinion expressions to all plausible sentence words whereas Somasundaran and Wiebe (2010) attached opinion expressions to the debate topic closest to them. Probabilistic association learning of target-opinion pair and the debate topic was used by Somasundaran and Wiebe (2010) as an integer linear programming problem to classify posts stance. Even though opinions might not be directed towards debate topics, these approaches attach the opinions to debate topics based only on their context cooccurrence. Our approach finds the target word for an opinion expression by analyzing the full dependency parse of the utterance. There has also been a lot of work done in social media on target directed sentiment analysis (Agarwal et al., 2011; OHare et al., 2009; Mukherjee and Bhattacharyya, 2012) which we incorporate for capturing users intentions. Agarwal et al. (2011) used s</context>
<context position="26200" citStr="Somasundaran and Wiebe, 2010" startWordPosition="4255" endWordPosition="4258">s 74.3 73.9 74.8 75.1 73.6 74.1 Table 7: Accuracy of Utterance Intention Classification Overall we notice that the proposed approaches perform better than baseline systems, with TopicScore+LexFeatures outperforming all systems. This shows that topic directed sentiment score helps in capturing users intentions better than the word level sentiment analysis. We can also conclude that the Unigram system achieves higher accuracies than the lexFeatures system, showing that what the user says is a better indicator of users intentions than his sentiments and thus confirming previous research results (Somasundaran and Wiebe, 2010; Pang and Lee, 2008). TopicScore performs lower in capturing NI tag than the baseline systems, denoting that TopicScore is not capturing debate topics and their sentiments correctly. Thus it assigns non-NI tagged utterances an NI tag, lowering its accuracy. We run the same approach but comparing utterance words only with the debate topics in calculating topic directed sentiment score and not with the lists of extended targets. This produces an accuracy of 70.8% clearly highlighting the importance 67 \x0cof extended targets in calculating debate topic directed sentiment analysis. 6.2 Post Stan</context>
<context position="27695" citStr="Somasundaran and Wiebe (2010)" startWordPosition="4497" endWordPosition="4500"> designed for this task of debate posts stance classification. The first baseline, sentVicinity, assigns each words sentiment score to the closest debate topic entity. Then, the sentiment score of the debate topics over an entire post is compared to classify post stance. The second baseline, subjTopic, counts the number of subjective words in each utterance of the post and assigns them to debate topic related entity if present in the utterance. It compares overall subjective positivity of debate topics to assign post stance. We also compared our approach with the (Arg+Sent) method proposed by Somasundaran and Wiebe (2010). Three systems described in Subsection 4.1 are used to compute posts sentiment score by analyzing its content namely, uttrScore, argScore and argSpanScore. Post sentiment scores from these three techniques along with post features (topic Count and intention type count) are used to classify post stance and results are compared in Table 8. Table 8 shows that the second approach of calculating posts sentiment scores using their argument structure outperforms the other approaches. System Accuracy sentVicinity 61.6% subjTopic 58.1% Arg+Sent 63.9% uttrScore 67.4% argScore 70.3% argSpanScore 69.2% T</context>
</contexts>
<marker>Somasundaran, Wiebe, 2010</marker>
<rawString>S. Somasundaran and J. Wiebe. 2010. Recognizing stances in ideological on-line debates. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 116124. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arpit Sood</author>
<author>Thanvir P Mohamed</author>
<author>Vasudeva Varma</author>
</authors>
<title>Topic-focused summarization of chat conversations.</title>
<date>2013</date>
<booktitle>In Advances in Information Retrieval,</booktitle>
<pages>800803</pages>
<publisher>Springer.</publisher>
<marker>Sood, Mohamed, Varma, 2013</marker>
<rawString>Arpit Sood, Thanvir P Mohamed, and Vasudeva Varma. 2013. Topic-focused summarization of chat conversations. In Advances in Information Retrieval, pages 800803. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>P Anand</author>
<author>R Abbott</author>
<author>R Grant</author>
</authors>
<title>Stance classification using dialogic properties of persuasion.</title>
<date>2012</date>
<booktitle>Proceedings of the 2012 Conference of the North American Chapter of the Associaotion for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>592596</pages>
<contexts>
<context position="5237" citStr="Walker et al., 2012" startWordPosition="810" endWordPosition="813">Section 5 describes the use of the dialogue structure of the debate and presents a gradient ascent method for re-evaluating posts stance. We present experiments and results on capturing users intentions and stance classification in Section 6. This is followed by conclusions in Section 7. 2 Related Work To classify posts stance in dual-sided debates, previous approaches have used probabilistic (Somasundaran and Wiebe, 2009) as well as machine learning techniques (Anand et al., 2011; Somasundaran and Wiebe, 2010). Some approaches extensively used the dialogue structure to identify posts stance (Walker et al., 2012) whereas others considered opinion expressions and their targets essential to capture sentiment in the posts towards debate topics (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). Pure machine learning approaches (Anand et al., 2011) have extracted lexical and contextual features of debate posts to classify their stance. Walker et al. (2012) partitioned the debate posts based on the dialogue structure of the debate and assigned stance to a partition using lexical features of candidate posts. This approach has a disadvantage that it loses posts individuality because it assigns stan</context>
<context position="19828" citStr="Walker et al. (2012)" startWordPosition="3155" endWordPosition="3158">ures along with the posts sentiment scores to classify their stance, the results of which are discussed in Subsection 6.2. 5 Gradient Ascent Method In the previous section, sentiment score and intention of the utterances were used to calculate the posts sentiment scores. In this section, we focus on the use of the dialogue structure of the debate to improve stance classification. convinceme.net stores user information for posts and also provides an option to rebut another posts. Intuitively, the rebutting posts should have opposite stances and same author posts should support the same stance. Walker et al. (2012) uses the same intuition to split the debate posts in two partitions using a max-cut algorithm. This approach loses the posts individuality because it assigns the same stance to all posts belonging to a partition. Our approach described below uses the debate structure to refine posts sentiment scores, calculated in the previous section, thus maintaining post individuality. If two posts by same author P1 and P2 have sentiment scores 0.1 and 0.7, the previous approach would classify post P1 as supporting topic B and P2 as supporting A, even if they are by same author and supporting the same stan</context>
</contexts>
<marker>Walker, Anand, Abbott, Grant, 2012</marker>
<rawString>M.A. Walker, P. Anand, R. Abbott, and R. Grant. 2012. Stance classification using dialogic properties of persuasion. Proceedings of the 2012 Conference of the North American Chapter of the Associaotion for Computational Linguistics: Human Language Technologies, pages 592596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<volume>39</volume>
<issue>2</issue>
<pages>210</pages>
<contexts>
<context position="6098" citStr="Wiebe et al., 2005" startWordPosition="943" endWordPosition="946">011) have extracted lexical and contextual features of debate posts to classify their stance. Walker et al. (2012) partitioned the debate posts based on the dialogue structure of the debate and assigned stance to a partition using lexical features of candidate posts. This approach has a disadvantage that it loses posts individuality because it assigns stance based on the entire partitions whereas our approach treats each post individually. To extract opinion expressions, Somasundaran and Wiebe (2009) used the Subjectivity lexicon and Somasundaran and Wiebe (2010) used the MPQA opinion corpus (Wiebe et al., 2005). These opinion expressions were attached to the target words using different techniques. Somasundaran and Wiebe (2009) attached opinion expressions to all plausible sentence words whereas Somasundaran and Wiebe (2010) attached opinion expressions to the debate topic closest to them. Probabilistic association learning of target-opinion pair and the debate topic was used by Somasundaran and Wiebe (2010) as an integer linear programming problem to classify posts stance. Even though opinions might not be directed towards debate topics, these approaches attach the opinions to debate topics based o</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2):165 210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Yelati</author>
<author>R Sangal</author>
</authors>
<title>Novel approach for tagging of discourse segments in help-desk e-mails.</title>
<date>2011</date>
<contexts>
<context position="7645" citStr="Yelati and Sangal (2011)" startWordPosition="1181" endWordPosition="1184"> which we incorporate for capturing users intentions. Agarwal et al. (2011) used syntactic features as target dependent features to differentiate sentiments effect on different targets in a tweet. OHare et al. (2009) employed a word-based approach to determine sentiments directed towards companies and their stocks from financial blogs. Mukherjee and Bhattacharyya (2012) applied clustering to extract feature specific opinions and calculated the overall feature sentiment using subjectivity lexicon. Discourse markers cues were used by Sood (2013) to prioritize the conversational sentences and by Yelati and Sangal (2011) to identify users intentions in help desk emails. Most of the discourse analysis theories defined their own discourse segment tagging schema to understand the meaning of each utterance. Yelati and Sangal (2011) devised a help desk specific tagging schema to capture the queries and build email structure in help desk emails. Lampert et al. (2006) used verbal response modes to understand the client/therapist conversations. We incorporate target directed sentiment analysis to capture utterance level intentions using a sentiment lexicon and dependency parses as described in the following section. </context>
</contexts>
<marker>Yelati, Sangal, 2011</marker>
<rawString>S. Yelati and R. Sangal. 2011. Novel approach for tagging of discourse segments in help-desk e-mails.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology-Volume 03,</booktitle>
<pages>369372</pages>
<publisher>IEEE Computer Society.</publisher>
<marker></marker>
<rawString>In Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology-Volume 03, pages 369372. IEEE Computer Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>