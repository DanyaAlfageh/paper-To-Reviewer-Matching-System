<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.596046">
b&amp;apos;Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 1019,
Avignon, France, April 23 - 27 2012. c
</bodyText>
<sectionHeader confidence="0.402449" genericHeader="abstract">
2012 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.88177">
Measuring Comparability of Documents in Non-Parallel Corpora for
Efficient Extraction of (Semi-)Parallel Translation Equivalents
</title>
<author confidence="0.966883">
Fangzhong Su
</author>
<affiliation confidence="0.9889095">
Centre for Translation Studies
University Of Leeds
</affiliation>
<address confidence="0.996638">
LS2 9JT, Leeds, UK
</address>
<email confidence="0.994524">
smlfs@leeds.ac.uk
</email>
<author confidence="0.934119">
Bogdan Babych
</author>
<affiliation confidence="0.9879775">
Centre for Translation Studies
University Of Leeds
</affiliation>
<address confidence="0.996706">
LS2 9JT, Leeds, UK
</address>
<email confidence="0.996634">
b.babych@leeds.ac.uk
</email>
<sectionHeader confidence="0.988696" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.98957125">
In this paper we present and evaluate three
approaches to measure comparability of
documents in non-parallel corpora. We de-
velop a task-oriented definition of compa-
rability, based on the performance of auto-
matic extraction of translation equivalents
from the documents aligned by the pro-
posed metrics, which formalises intuitive
definitions of comparability for machine
translation research. We demonstrate ap-
plication of our metrics for the task of
automatic extraction of parallel and semi-
parallel translation equivalents and discuss
how these resources can be used in the
frameworks of statistical and rule-based
machine translation.
</bodyText>
<sectionHeader confidence="0.994569" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998509327272727">
Parallel corpora have been extensively exploited
in different ways in machine translation (MT)
both in Statistical (SMT) and more recently,
in Rule-Based (RBMT) architectures: in SMT
aligned parallel resources are used for building
translation phrase tables and calculating transla-
tion probabilities; and in RBMT, they are used
for automatically building bilingual dictionaries
of translation equivalents and automatically deriv-
ing bilingual mappings for frequent structural pat-
terns. However, large parallel resources are not
always available, especially for under-resourced
languages or narrow domains. Therefore, in re-
cent years, the use of cross-lingual comparable
corpora has attracted considerable attention in
the MT community (Sharoff et al., 2006; Fung
and Cheung, 2004a; Munteanu and Marcu, 2005;
Babych et al., 2008).
Most of the applications of comparable cor-
pora focus on discovering translation equivalents
to support machine translation, such as bilingual
lexicon extraction (Rapp, 1995; Rapp, 1999;
Morin et al., 2007; Yu and Tsujii, 2009; Li and
Gaussier, 2010; Prachasson and Fung, 2011), par-
allel phrase extraction (Munteanu and Marcu,
2006), and parallel sentence extraction (Fung and
Cheung, 2004b; Munteanu and Marcu, 2005;
Munteanu et al., 2004; Smith et al., 2010).
Comparability between documents is often un-
derstood as belonging to the same subject domain,
genre or text type, so this definition relies on these
vague linguistic concepts. The problem with this
definition then is that it cannot be exactly bench-
marked, since it becomes hard to relate automated
measures of comparability to such inexact and un-
measurable linguistic concepts. Research on com-
parable corpora needs not only good measures for
comparability, but also a clearer, technologically-
grounded and quantifiable definition of compara-
bility in the first place.
In this paper we relate comparability to use-
fulness of comparable texts for MT. In particu-
lar, we propose a performance-based definition of
comparability, as the possibility to extract parallel
or quasi-parallel translation equivalents words,
phrases and sentences which are translations of
each other. This definition directly relates compa-
rability to texts potential to improve the quality
of MT by adding extracted phrases to phrase ta-
bles, training corpus or dictionaries. It also can be
quantified as the rate of successful extraction of
translation equivalents by automated tools, such
as proposed in Munteanu and Marcu (2006).
Still, successful detection of translation equiv-
alents from comparable corpora very much de-
</bodyText>
<page confidence="0.996679">
10
</page>
<bodyText confidence="0.993355785714286">
\x0cpends on the quality of these corpora, specifically
on the degree of their textual equivalence and suc-
cessful alignment on various text units. There-
fore, the goal of this work is to provide compa-
rability metrics which can reliably identify cross-
lingual comparable documents from raw corpora
crawled from the Web, and characterize the de-
gree of their similarity, which enriches compara-
ble corpora with the document alignment infor-
mation, filters out documents that are not useful
and eventually leads to extraction of good-quality
translation equivalents from the corpora.
To achieve this goal, we need to define a
scale to assess comparability qualitatively, met-
rics to measure comparability quantitatively, and
the sources to get comparable corpora from. In
this work, we directly characterize comparability
by how useful comparable corpora are for the task
of detecting translation equivalents in them, and
ultimately to machine translation. We focus on
document-level comparability, and use three cat-
egories for qualitative definition of comparability
levels, defined in terms of granularity for possible
alignment:
Parallel: Traditional parallel texts that are
translations of each other or approximate
translations with minor variations, which can
be aligned on the sentence level.
Strongly-comparable: Texts that talk about
the same event or subject, but in different
languages. For example, international news
about oil spill in the Gulf of Mexico, or
linked articles in Wikipedia about the same
topic. These documents can be aligned on
the document level on the basis of their ori-
gin.
Weakly-comparable: Texts in the same sub-
ject domain which describe different events.
For example, customer reviews about hotel
and restaurant in London. These documents
do not have an independent alignment across
languages, but sets of texts can be aligned
on the basis of belonging to the same subject
domain or sub-domain.
In this paper, we present three different ap-
proaches to measure the comparability of cross-
lingual (especially under-resourced languages)
comparable documents: a lexical mapping based
approach, a keyword based approach, and a ma-
chine translation based approach. The experimen-
tal results show that all of them can effectively
predict the comparability levels of the compared
document pairs. We then further investigate the
applicability of the proposed metrics by measur-
ing their impact on the task of parallel phrase ex-
traction from comparable corpora. It turns out
that, higher comparability level predicted by the
metrics consistently lead to more number of paral-
lel phrase extracted from comparable documents.
Thus, the metrics can help select more compara-
ble document pairs to improve the performance of
parallel phrase extraction.
The remainder of this paper is organized as fol-
lows. Section 2 discusses previous work. Section
3 introduces our comparability metrics. Section
4 presents the experimental results and evaluation.
Section 5 describes the application of the metrics.
Section 6 discusses the pros and cons of the pro-
posed metrics, followed by conclusions and future
work in Section 7.
</bodyText>
<sectionHeader confidence="0.999621" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998784222222222">
The term comparability, which is the key con-
cept in this work, applies to the level of corpora,
documents and sub-document units. However, so
far there is no widely accepted definition of com-
parability. For example, there is no agreement on
the degree of similarity that documents in com-
parable corpora should have or on the criteria for
measuring comparability. Also, most of the work
that performs translation equivalent extraction in
comparable corpora usually assumes that the cor-
pora they use are reliably comparable and focuses
on the design of efficient extraction algorithms.
Therefore, there has been very little literature dis-
cussing the characteristics of comparable corpora
(Maia, 2003). In this section, we introduce some
representative work which tackles comparability
metrics.
Some studies (Sharoff, 2007; Maia, 2003;
McEnery and Xiao, 2007) analyse comparability
by assessing corpus composition, such as struc-
tural criteria (e.g., format and size), and linguistic
criteria (e.g., topic, domain, and genre). Kilgarriff
and Rose (1998) measure similarity and homo-
geneity between monolingual corpora. They gen-
erate word frequency list from each corpus and
then apply 2 statistic on the most frequent n (e.g.,
500) words of the compared corpora.
</bodyText>
<page confidence="0.999444">
11
</page>
<bodyText confidence="0.994135486486487">
\x0cThe work which deals with comparability
measures in cross-lingual comparable corpora is
closer to our work. Saralegi et al. (2008) measure
the degree of comparability of comparable cor-
pora (English and Basque) according to the dis-
tribution of topics and publication dates of docu-
ments. They compute content similarity for all the
document pairs between two corpora. These sim-
ilarity scores are then input as parameters for the
EMD (Earth Movers Distance) distance measure,
which is employed to calculate the global com-
patibility of the corpora. Munteanu and Marcu
(2005; 2006) select more comparable document
pairs in a cross-lingual information retrieval based
manner by using a toolkit called Lemur1. The
retrieved document pairs then serve as input for
the tasks of parallel sentence and sub-sentence ex-
traction. Smith et al. (2010) treat Wikipedia as
a comparable corpus and use interwiki links to
identify aligned comparable document pairs for
the task of parallel sentence extraction. Li and
Gaussier (2010) propose a comparability met-
ric which can be applied at both document level
and corpus level and use it as a measure to se-
lect more comparable texts from other external
sources into the original corpora for bilingual lex-
icon extraction. The metric measures the propor-
tion of words in the source language corpus trans-
lated in the target language corpus by looking up
a bilingual dictionary. They evaluate the met-
ric on the rich-resourced English-French language
pair, thus good dictionary resources are available.
However, this is not the case for under-resourced
languages in which reliable language resources
such as machine-readable bilingual dictionaries
with broad word coverage or word lemmatizers
might be not publicly available.
</bodyText>
<sectionHeader confidence="0.993722" genericHeader="method">
3 Comparability Metrics
</sectionHeader>
<bodyText confidence="0.9978905">
To measure the comparability degree of document
pairs in different languages, we need to translate
the texts or map lexical items from the source lan-
guage into the target languages so that we can
compare them within the same language. Usually
this can be done by using bilingual dictionaries
(Rapp, 1999; Li and Gaussier, 2010; Prachasson
and Fung, 2011) or existing machine translation
tools. Based on this process, in this section we
present three different approaches to measure the
</bodyText>
<page confidence="0.595447">
1
</page>
<construct confidence="0.2804305">
Available at http://www.lemurproject.org/
comparability of comparable documents.
</construct>
<subsectionHeader confidence="0.995856">
3.1 Lexical mapping based metric
</subsectionHeader>
<bodyText confidence="0.999477523809524">
It is straightforward that we expect a bilingual dic-
tionary can be used for lexical mapping between a
language pair. However, unlike the language pairs
in which both languages are rich-resourced (e.g.,
English-French, or English-Spanish) and dictio-
nary resources are relatively easy to obtain, it is
likely that bilingual dictionaries with good word
coverage are not publicly available for under-
resourced languages (e.g., English-Slovenian, or
English-Lithuanian). In order to address this
problem, we automatically construct dictionaries
by using word alignment on large-scale parallel
corpora (e.g., Europarl and JRC-Acquis2).
Specifically, GIZA++ toolkit (Och and Ney,
2000) with default setting is used for word align-
ment on the JRC-Acquis parallel corpora (Stein-
berger et al., 2006). The aligned word pairs to-
gether with the alignment probabilities are then
converted into dictionary entries. For example,
in Estonian-English language pair, the alignment
example kompanii company 0.625 in the word
alignment table means the Estonian word kom-
panii can be translated as (or aligned with) the
English candidate word company with a prob-
ability of 0.625. In the dictionary, the transla-
tion candidates are ranked by translation proba-
bility in descending order. Note that the dictio-
nary collects inflectional form of words, but not
only base form of words. This is because the dic-
tionary is directly generated from the word align-
ment results and no further word lemmatization is
applied.
Using the resulting dictionary, we then per-
form lexical mapping in a word-for-word map-
ping strategy. We scan each word in the source
language texts to check if it occurs in the dic-
tionary entries. If so, the first translation candi-
date are recorded as the corresponding mapping
word. If there are more than one translation can-
didate, the second candidate will also be kept as
the mapping result if its translation probability is
higher than 0.33. For non-English and English
</bodyText>
<page confidence="0.972947">
2
</page>
<bodyText confidence="0.997762333333333">
The JRC-Acquis covers 22 European languages and
provides large-scale parallel corpora for all the 231 language
pairs.
</bodyText>
<page confidence="0.989545">
3
</page>
<bodyText confidence="0.982999">
From the manual inspection on the word alignment re-
sults, we find that if the alignment probability is higher than
0.3, it is more reliable.
</bodyText>
<page confidence="0.996118">
12
</page>
<bodyText confidence="0.998494714285714">
\x0clanguage pair, the non-English texts are mapped
into English. If both languages are non-English
(e.g., Greek-Romanian), we use English as a pivot
langauge and map both the source and target
language texts into English4. Due to the lack
of reliable linguistic resources in non-English
languages, mapping texts from non-English lan-
guage into English can avoid language process-
ing in non-English texts and allows us to make
use of the rich resources in English for further
text processing, such as stop-word filtering and
word lemmatization5. Finally, cosine similarity
measure is applied to compute the comparability
strength of the compared document pairs.
</bodyText>
<subsectionHeader confidence="0.993837">
3.2 Keyword based metric
</subsectionHeader>
<bodyText confidence="0.999657142857143">
The lexical mapping based metric takes all the
words in the text into account for comparability
measure, but if we only retain a small number of
representative words (keywords) and discard all
the other less informative words in each docu-
ment, can we judge the comparability of a doc-
ument pair by comparing these words? Our in-
tuition is that, if two document share more key-
words, they should be more comparable. To
validate this, we then perform keyword extrac-
tion by using a simple TFIDF based approach,
which has been shown effective for keyword or
keyphrase extraction from the texts (Frank et al.,
1999; Hulth, 2003; Liu et al., 2009).
More specifically, the keyword based metric
can be described as below. First, similar to the
lexical mapping based metric, bilingual dictionar-
ies are used to map non-English texts into En-
glish. Thus, only the English resources are ap-
plied for stop-word filtering and word lemmatiza-
tion, which are useful text preprocessing steps for
keyword extraction. We then use TFIDF to mea-
sure the weight of words in the document and rank
the words by their TFIDF weights in descending
order. The top n (e.g., 30) words are extracted
as keywords to represent the document. Finally,
the comparability of each document pair is deter-
mined by applying cosine similarity to their key-
</bodyText>
<page confidence="0.986117">
4
</page>
<bodyText confidence="0.992741666666667">
Generally in JRC-Acquis, the size of parallel corpora
for most of non-English langauge pairs is much smaller than
that of language pairs which contain English. Therefore, the
resulting bilingual dictionaries which contain English have
better word coverage as they have many more dictionary en-
tries.
</bodyText>
<page confidence="0.972101">
5
</page>
<bodyText confidence="0.974687666666667">
We use WordNet (Fellbaum, 1998) for word lemmatiza-
tion.
word lists.
</bodyText>
<subsectionHeader confidence="0.944399">
3.3 Machine translation based metrics
</subsectionHeader>
<bodyText confidence="0.991608566666667">
Bilingual dictionary is used for word-for-word
translation in the lexical mapping based metric
and words which do not occur in the dictionary
will be omitted. Thus, the mapping result is like
a list of isolated words and information such as
word order, syntactic structure and named entities
can not be preserved. Therefore, in order to im-
prove the text translation quality, we turn to the
state-of-the-art SMT systems.
In practice, we use Microsoft translation API6
to translate texts in under-resourced languages
(e.g, Lithuanian and Slovenian) into English and
then explore several features for comparability
metric design, which are listed as below.
Lexical feature: Lemmatized bag-of-word
representation of each document after stop-
word filtering. Lexical similarity (denoted
by WL) of each document pair is then ob-
tained by applying cosine measure to the lex-
ical feature.
Structure feature: We approximate it by
the number of content words (adjectives, ad-
verbs, nouns, verbs and proper nouns) and
the number of sentences in each document,
denoted by CD and SD respectively. The in-
tuition is that, if two documents are highly
comparable, their number of content words
and their document length should be similar.
The structure similarity (denoted by WS) of
two documents D1 and D2 is defined as bel-
</bodyText>
<equation confidence="0.658502">
low.
WS = 0.5 (CD1/CD2) + 0.5 (SD1/SD2)
</equation>
<bodyText confidence="0.948759666666667">
suppose that CD1&amp;lt;=CD2, and SD1&amp;lt;=SD2.
Keyword feature: Top-20 words (ranked by
TFIDF weight) of each document. keyword
similarity (denoted by WK) of two docu-
ments is also measured by cosine.
Named entity feature: Named entities of
each document. If more named entities co-
occur in two documents, they are very likely
to talk about the same event or subject and
</bodyText>
<page confidence="0.77763">
6
</page>
<footnote confidence="0.5128465">
Available at http://code.google.com/p/microsoft-
translator-java-api/
</footnote>
<page confidence="0.998186">
13
</page>
<bodyText confidence="0.998649285714286">
\x0cthus should be more comparable. We use
Stanford named entity recognizer7 to extract
named entities from the texts (Finkel et al.,
2005). Again, cosine is then applied to mea-
sure the similarity of named entities (denoted
by WN ) between a document pair.
We then combine these four different types of
score in an ensemble manner. Specifically, a
weighted average strategy is applied: each indi-
vidual score is associated with a constant weight,
indicating the relative confidence (importance) of
the corresponding type of score. The overall com-
parability score (denoted by SC) of a document
pair is thus computed as below:
</bodyText>
<equation confidence="0.760706">
SC = WL + WS + WK + WN
</equation>
<bodyText confidence="0.909546">
where , , , and [0, 1], and + + + =
</bodyText>
<listItem confidence="0.769392">
1. SC should be a value between 0 and 1, and
larger SC value indicates higher comparability
level.
4 Experiment and Evaluation
</listItem>
<subsectionHeader confidence="0.978932">
4.1 Data source
</subsectionHeader>
<bodyText confidence="0.90594075">
To investigate the reliability of the proposed
comparability metrics, we perform experiments
for 6 language pairs which contain under-
resoured languages: German-English (DE-EN),
</bodyText>
<table confidence="0.859865">
Estonian-English (ET-EN), Lithuanian-English
(LT-EN), Latvian-English (LV-EN), Slovenian-
English (SL-EN) and Greek-Romanian (EL-RO).
</table>
<bodyText confidence="0.996671090909091">
A comparable corpus is collected for each lan-
guage pair. Based on the definition of compa-
rability levels (see Section 1), human annota-
tors fluent in both languages then manually anno-
tated the comparability degree (parallel, strongly-
comparable, and weakly-comparable) at the doc-
ument level. Hence, these bilingual comparable
corpora are used as gold standard for experiments.
The data distribution for each language pair, i.e.,
number of document pairs in each comparability
level, is given in Table 1.
</bodyText>
<subsectionHeader confidence="0.92742">
4.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.99890725">
We adopt a simple method for evaluation. For
each language pair, we compute the average
scores for all the document pairs in the same com-
parability level, and compare them to the gold
</bodyText>
<page confidence="0.916696">
7
</page>
<figure confidence="0.9031053">
Available at http://nlp.stanford.edu/software/CRF-
NER.shtml
Language
pair
#document
pair
parallel strongly-
comparable
weakly-
comparable
</figure>
<table confidence="0.990558166666667">
DE-EN 1286 531 715 40
ET-EN 1648 182 987 479
LT-EN 1177 347 509 321
LV-EN 1252 184 558 510
SL-EN 1795 532 302 961
EL-RO 485 38 365 82
</table>
<tableCaption confidence="0.999116">
Table 1: Data distribution of gold standard corpora
</tableCaption>
<bodyText confidence="0.986938315789473">
standard comparability labels. In addition, in or-
der to better reveal the relation between the scores
obtained from the proposed metrics and compara-
bility levels, we also measure the Pearson correla-
tion between them8. For the keyword based met-
ric, top 30 keywords are extracted from each text
for experiment. For the machine translation based
metric, we empirically set = 0.5, = = 0.2,
and = 0.1. This is based on the assumption
that, lexical feature can best characterize the com-
parability given the good translation quality pro-
vided by the powerful MT system, while keyword
and named entity features are also better indica-
tors of comparability than the simple document
length information.
The results for the lexical mapping based met-
ric, the keyword based metric and the machine
translation based metric are listed in Table 2, 3,
and 4, respectively.
</bodyText>
<table confidence="0.695112076923077">
Language
pair
parallel strongly-
comparable
weakly-
comparable
correlation
DE-EN 0.545 0.476 0.182 0.941
ET-EN 0.553 0.381 0.228 0.999
LT-EN 0.545 0.461 0.225 0.964
LV-EN 0.625 0.494 0.179 0.973
SL-EN 0.535 0.456 0.314 0.987
EL-RO 0.342 0.131 0.090 0.932
</table>
<tableCaption confidence="0.978411">
Table 2: Average comparability scores for lexical map-
</tableCaption>
<bodyText confidence="0.9486034">
ping based metric
Overall, from the average scores for each
comparability level presented in Table 2, 3,
and 4, we can see that, the scores obtained
from the three comparability metrics can reli-
</bodyText>
<page confidence="0.991909">
8
</page>
<bodyText confidence="0.997521428571429">
For correlation measure, we use numerical calibration
to different comparability degrees: Parallel, strongly-
comparable and weakly-comparable are converted as 3,
2, and 1, respectively. The correlation is then computed
between the numerical comparability levels and the cor-
responding average comparability scores automatically de-
rived from the metrics.
</bodyText>
<page confidence="0.926619">
14
</page>
<figure confidence="0.876574428571429">
\x0cLanguage
pair
parallel strongly-
comparable
weakly-
comparable
correlation
</figure>
<table confidence="0.998330333333333">
DE-EN 0.526 0.486 0.084 0.941
ET-EN 0.502 0.345 0.184 0.990
LT-EN 0.485 0.420 0.202 0.954
LV-EN 0.590 0.448 0.124 0.975
SL-EN 0.551 0.505 0.292 0.937
EL-RO 0.210 0.110 0.031 0.997
</table>
<tableCaption confidence="0.996769">
Table 3: Average comparability scores for keyword
</tableCaption>
<figure confidence="0.906900625">
based metric
Language
pair
parallel strongly-
comparable
weakly-
comparable
correlation
</figure>
<table confidence="0.998347333333333">
DE-EN 0.912 0.622 0.326 0.999
ET-EN 0.765 0.547 0.310 0.999
LT-EN 0.755 0.613 0.308 0.984
LV-EN 0.770 0.627 0.236 0.966
SL-EN 0.779 0.582 0.373 0.988
EL-RO 0.863 0.446 0.214 0.988
</table>
<tableCaption confidence="0.8140815">
Table 4: Average comparability scores for machine
translation based metric
</tableCaption>
<bodyText confidence="0.997435534883721">
ably reflect the comparability levels across dif-
ferent language pairs, as the average scores
for higher comparable levels are always sig-
nificantly larger than those of lower compara-
ble levels, namely SC(parallel)&amp;gt;SC(strongly-
comparable)&amp;gt;SC(weakly-comparable). In addi-
tion, in all the three metrics, the Pearson correla-
tion scores are very high (over 0.93) across dif-
ferent language pairs, which indicate that there
is strong correlation between the comparability
scores obtained from the metrics and the corre-
sponding comparability level.
Moreover, from the comparison of Table 2, 3,
and 4, we also have several other findings. Firstly,
the performance of keyword based metric (see
Table 3) is comparable to the lexical mapping
based metric (see Table 2) as their comparability
scores for the corresponding comparability levels
are similar. This means it is reasonable to deter-
mine the comparability level by only comparing a
small number of keywords of the texts. Secondly,
the scores obtained from the machine translation
based metric (see Table 4) are significantly higher
than those in both the lexical mapping based met-
ric and the keyword based metric. Clearly, this
is due to the advantages of using the state-of-the-
art MT system. In comparison to the approach
of using dictionary for word-for-word mapping,
it can provide much better text translation which
allows detecting more proportion of lexical over-
lapping and mining more useful features in the
translated texts. Thirdly, in the lexical mapping
based metric and keyword based metric, we can
also see that, although the average scores for EL-
RO (both under-resourced languages) conform to
the comparability levels, they are much lower than
those of the other 5 language pairs. The reason
is that, the size of the parallel corpora in JRC-
Acquis for these 5 language pairs are significantly
larger (over 1 million parallel sentences) than that
of EL-EN, RO-EN9, and EL-RO, thus the result-
ing dictionaries of these 5 language pairs also con-
tain many more dictionary entries.
</bodyText>
<sectionHeader confidence="0.987756" genericHeader="method">
5 Application
</sectionHeader>
<bodyText confidence="0.990442090909091">
The experiments in Section 4 confirm the reli-
ability of the proposed metrics. The compara-
bility metrics are thus useful for collecting high-
quality comparable corpora, as they can help filter
out weakly comparable or non-comparable doc-
ument pairs from the raw crawled corpora. But
are they also useful for other NLP tasks, such as
translation equivalent detection from comparable
corpora? In this section, we further measure the
impact of the metrics on parallel phrase extraction
(PPE) from comparable corpora. Our intuition is
that, if document pairs are assigned higher com-
parability scores by the metrics, they should be
more comparable and thus more parallel phrases
can be extracted from them.
The algorithm of parallel phrase extraction,
which develops the approached presented in
Munteanu and Marcu (2006), uses lexical over-
lap and structural matching measures (Ion, 2012).
Taking a list of bilingual comparable document
pairs as input, the extraction algorithm involves
the following steps.
</bodyText>
<listItem confidence="0.990550333333333">
1. Split the source and target language docu-
ments into phrases.
2. Compute the degree of parallelism for each
</listItem>
<bodyText confidence="0.7620868">
candidate pair of phrases by using the bilin-
gual dictionary generated from GIZA++
(base dictionary), and retain all the phrase
pairs with a score larger than a predefined
parallelism threshold.
</bodyText>
<page confidence="0.977817">
9
</page>
<bodyText confidence="0.9937055">
Remember that in our experiment, English is used as the
pivot language for non-English langauge pairs.
</bodyText>
<page confidence="0.982191">
15
</page>
<bodyText confidence="0.9375247">
\x0c3. Apply GIZA++ to the retained phrase pairs
to detect new dictionary entries and add them
to the base dictionary.
4. Repeat Step 2 and 3 for several times (empir-
ically set at 5) by using the augmented dic-
tionary, and output the detected phrase pairs.
Phrases which are extracted by this algorithm
are frequently not exact translation equivalents.
Below we give some English-German examples
of extracted equivalents with their corresponding
</bodyText>
<listItem confidence="0.59557">
alignment scores:
1. But a successful mission seiner uberaus
erfolgreichen Mission abgebremst
0.815501989333333
2. Former President Jimmy Carter Der
ehemalige US-Prasident Jimmy Carter
0.69708324976825
3. on the Korean Peninsula auf der koreanis-
chen Halbinsel 0.8677432145
4. across the Muslim world mit der muslim-
ischen Welt ermoglichen 0.893330864
5. to join the United Nations der Weg
in die Vereinten Nationen offensteht
</listItem>
<page confidence="0.863174">
0.397418711927629
</page>
<bodyText confidence="0.998634857142857">
Even though some of the extracted phrases are
not exact translation equivalents, they may still
be useful resources both for SMT and RBMT if
these phrases are passed through an extra pre-
processing stage, of if the engines are modified
specifically to work with semi-parallel translation
equivalents extracted from comparable texts. We
address this issue in the discussion section (see
Section 6).
For evaluation, we measure how the metrics af-
fect the performance of parallel phrase extraction
algorithm on 5 language pairs (DE-EN, ET-EN,
LT-EN, LV-EN, and SL-EN). A large raw compa-
rable corpus for each language pair was crawled
from the Web, and the metrics were then applied
to assign comparability scores to all the docu-
ment pairs in each corpus. For each language pair,
we set three different intervals based on the com-
parability score (SC) and randomly select 500
document pairs in each interval for evaluation.
For the MT based metric, the three intervals are
</bodyText>
<equation confidence="0.916498">
(1) 0.1&amp;lt;=SC&amp;lt;0.3, (2) 0.3&amp;lt;=SC&amp;lt;0.5, and (3)
</equation>
<bodyText confidence="0.98027575">
SC&amp;gt;=0.5. For the lexical mapping based metric
and keyword based metric, since their scores are
lower than those of the MT based metric for each
comparability level, we set three lower intervals at
</bodyText>
<equation confidence="0.908044">
(1) 0.1&amp;lt;=SC&amp;lt;0.2, (2) 0.2&amp;lt;=SC&amp;lt;0.4, and (3)
</equation>
<bodyText confidence="0.988831846153846">
SC&amp;gt;=0.4. The experiment focuses on counting
the number of extracted parallel phrases with par-
allelism score&amp;gt;=0.410, and computes the average
number of extracted phrases per 100000 words
(the sum of words in the source and target lan-
guage documents) for each interval. In addition,
the Pearson correlation measure is also applied to
measure the correlation between the interval11 of
comparability scores and the number of extracted
parallel phrases. The results which summarize the
impact of the three metrics to the performance of
parallel phrase extraction are listed in Table 5, 6,
and 7, respectively.
</bodyText>
<figure confidence="0.878268142857143">
Language
pair
0.1&amp;lt;=
SC&amp;lt;0.2
0.2&amp;lt;=
SC&amp;lt;0.4
SC&amp;gt;=0.4 correlation
</figure>
<table confidence="0.997001">
DE-EN 728 1434 2510 0.993
ET-EN 313 631 1166 0.989
LT-EN 258 419 894 0.962
LV-EN 470 859 1900 0.967
SL-EN 393 946 2220 0.975
</table>
<tableCaption confidence="0.7993765">
Table 5: Impact of the lexical mapping based metric to
parallel phrase extraction
</tableCaption>
<figure confidence="0.883473428571429">
Language
pair
0.1&amp;lt;=
SC&amp;lt;0.2
0.2&amp;lt;=
SC&amp;lt;0.4
SC&amp;gt;=0.4 correlation
</figure>
<table confidence="0.9933202">
DE-EN 1007 1340 2151 0.972
ET-EN 438 650 1050 0.984
LT-EN 306 442 765 0.973
LV-EN 600 966 1722 0.980
SL-EN 715 1026 1854 0.967
</table>
<tableCaption confidence="0.99709">
Table 6: Impact of the keyword based metric to parallel
</tableCaption>
<bodyText confidence="0.941640142857143">
phrase extraction
From Table 5, 6, and 7, we can see that
for all the 5 language pairs, based on the aver-
age number of extracted aligned phrases, clearly
we have interval (3)&amp;gt;(2)&amp;gt;(1). In other words, in
any of the three metrics, a higher comparability
level always leads to significantly more number
</bodyText>
<page confidence="0.99537">
10
</page>
<bodyText confidence="0.995871">
A manual evaluation of a small set of extracted data
shows that parallel phrases with parallelism score &amp;gt;=0.4 are
more reliable.
</bodyText>
<page confidence="0.998501">
11
</page>
<bodyText confidence="0.990394666666667">
For the purpose of correlation measure, the three inter-
vals are numerically calibrated as 1, 2, and 3, respec-
tively.
</bodyText>
<page confidence="0.782003">
16
</page>
<figure confidence="0.926130857142857">
\x0cLanguage
pair
0.1&amp;lt;=
SC&amp;lt;0.3
0.3&amp;lt;=
SC&amp;lt;0.5
SC&amp;gt;=0.5 correlation
</figure>
<table confidence="0.9963924">
DE-EN 861 1547 2552 0.996
ET-EN 448 883 1251 0.999
LT-EN 293 483 1070 0.959
LV-EN 589 1072 2037 0.982
SL-EN 560 1151 2421 0.979
</table>
<tableCaption confidence="0.996792">
Table 7: Impact of the machine translation based met-
</tableCaption>
<bodyText confidence="0.999396">
ric to parallel phrase extraction
of aligned phrases extracted from the comparable
documents. Moreover, although the lexical map-
ping based metric and the keyword based metric
produce lower comparability scores than the MT
based metric (see Section 4), they have similar
impact to the task of parallel phrase extraction.
This means, the comparability score itself does
not matter much, as long as the metrics are re-
liable and proper thresholds are set for different
metrics.
In all the three metrics, the Pearson correla-
tion scores are very close to 1 for all the language
pairs, which indicate that the intervals of compa-
rability scores obtained from the metrics are in
line with the performance of equivalent extrac-
tion algorithm. Therefore, in order to extract more
parallel phrases (or other translation equivalents)
from comparable corpora, we can try to improve
the corpus comparability by applying the compa-
rability metrics beforehand to add highly compa-
rable document pairs in the corpora.
</bodyText>
<sectionHeader confidence="0.998325" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9994968">
We have presented three different approaches to
measure comparability at the document level. In
this section, we will analyze the advantages and
limitations of the proposed metrics, and the feasi-
bility of using semi-parallel equivalents in MT.
</bodyText>
<subsectionHeader confidence="0.994309">
6.1 Pros and cons of the metrics
</subsectionHeader>
<bodyText confidence="0.999859351351351">
Using bilingual dictionary for lexical mapping is
simple and fast. However, as it adopts the word-
for-word mapping strategy and out-of-vocabulary
(OOV) words are omitted, the linguistic structure
of the original texts is badly hurt after mapping.
Thus, apart from lexical information, it is diffi-
cult to explore more useful features for the com-
parability metrics. The TFIDF based keyword ex-
traction approach allows us to select more repre-
sentative words and prune a large amount of less
informative words from the texts. The keywords
are usually relevant to subject and domain terms,
which is quite useful in judging the comparabil-
ity of two documents. Both the lexical mapping
based approach and the keyword based approach
use dictionary for lexical translation, thus rely on
the availability and completeness of the dictionary
resources or large scale parallel corpora.
For the machine translation based metric, it
provides much better text translation than the
dictionary-based approach so that the comparabil-
ity of two document can be better revealed from
the richer lexical information and other useful
features, such as named entities. However, the
text translation process is expensive, as it depends
on the availability of the powerful MT systems12
and takes much longer than the simple dictionary
based translation.
In addition, we use a translation strategy of
translating texts from under-resourced (or less-
resourced) languages into rich-resourced lan-
guage. In case that both languages are under-
resourced languages, English is used as the pivot
langauge for translation. This can compensate the
shortage of the linguistic resources in the under-
resourced languages and take advantages of vari-
ous resources in the rich-resourced languages.
</bodyText>
<subsectionHeader confidence="0.997946">
6.2 Using semi-parallel equivalents in MT
</subsectionHeader>
<bodyText confidence="0.9841891">
systems
We note that modern SMT and RBMT sys-
tems take maximal advantage of strictly parallel
phrases, but they still do not use full potential
of the semi-parallel translation equivalents, of the
type that is illustrated in the application section
(see Section 5). Such resources, even though they
are not exact equivalents contain useful informa-
tion which is not used by the systems.
In particular, the modern decoders do not work
with under-specified phrases in phrase tables, and
do not work with factored semantic features. For
example, the phrase:
But a successful mission seiner uberaus er-
folgreichen Mission abgebremst
The English side contains the word but, which
pre-supposes contrast, and on the Greman side
words uberaus erfolgreichen (generally success-
ful) and abgebremst (slowed down) which
taken together exemplify a contrast, since they
</bodyText>
<page confidence="0.987827">
12
</page>
<bodyText confidence="0.995987666666667">
Alternatively, we can also train MT systems for text
translation by using the available SMT toolkits (e.g., Moses)
on large scale parallel corpora.
</bodyText>
<page confidence="0.99216">
17
</page>
<bodyText confidence="0.998006571428571">
\x0chave different semantic prosodies. In this example
the semantic feature of contrast can be extracted
and reused in other contexts. However, this would
require the development of a new generation of
decoders or rule-based systems which can suc-
cessfully identify and reuse such subtle semantic
features.
</bodyText>
<sectionHeader confidence="0.984699" genericHeader="conclusions">
7 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.99980247826087">
The success of extracting good-quality translation
equivalents from comparable corpora to improve
machine translation performance highly depends
on how comparable the used corpora are. In this
paper, we propose three different comparability
measures at the document level. The experiments
show that all the three approaches can effectively
determine the comparability levels of comparable
document pairs. We also further investigate the
impact of the metrics on the task of parallel phrase
extraction from comparable corpora. It turns out
that higher comparability scores always lead to
significantly more parallel phrases extracted from
comparable documents. Since better quality of
comparable corpora should have better applica-
bility, our metrics can be applied to select highly
comparable document pairs for the tasks of trans-
lation equivalent extraction.
In the future work, we will conduct more com-
prehensive evaluation of the metrics by capturing
its impact to the performance of machine transla-
tion systems with extended phrase tables derived
from comparable corpora.
</bodyText>
<sectionHeader confidence="0.966977" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.977682142857143">
We thank Radu Ion at RACAI for providing us
the toolkit of parallel phrase extraction, and the
three anonymous reviewers for valuable com-
ments. This work is supported by the EU funded
ACCURAT project (FP7-ICT-2009-4-248347) at
the Centre for Translation Studies, University of
Leeds.
</bodyText>
<sectionHeader confidence="0.970032" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999667378787879">
Bogdan Babych, Serge Sharoff and Anthony Hartley.
2008. Generalising Lexical Translation Strategies
for MT Using Comparable Corpora. Proceedings
of LREC 2008, Marrakech, Morocco.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. Proceedings of
COLING 2002, Taipei, Taiwan.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA,
USA.
Jenny Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. Proceedings of ACL 2005, University of
Michigan, Ann Arbor, USA.
Eibe Frank, Gordon Paynter and Ian Witten. 1999.
Domain-specific keyphrase extraction. Proceedings
of IJCAI 1999, Stockholm, Sweden.
Pascale Fung and Percy Cheung. 2004a. Mining very
non-parallel corpora: Parallel sentence and lexicon
extraction via bootstrapping and EM. Proceedings
of EMNLP 2004, Barcelona, Spain.
Pascale Fung and Percy Cheung. 2004b. Multi-level
bootstrapping for extracting parallel sentences from
a quasicomparable corpus. Proceedings of COL-
ING 2004, Geneva, Switzerland.
Anette Hulth. 2003. Improved Automatic Keyword
Extraction Given More Linguistic Knowledge. Pro-
ceedings of EMNLP 2003, Sapporo, Japan.
Radu Ion. 2012. PEXACC: A Parallel Data Mining
Algorithm from Comparable Corpora. Proceedings
of LREC 2012, Istanbul, Turkey.
Adam Kilgarriff and Tony Rose. 1998. Measures for
corpus similarity and homogeneity. Proceedings of
EMNLP 1998, Granada, Spain.
Bo Li and Eric Gaussier. 2010. Improving cor-
pus comparability for bilingual lexicon extraction
from comparable corpora. Proceedings of COL-
ING 2010, Beijing, China.
Feifan Liu, Deana Pennell, Fei Liu and Yang Liu.
2009. Unsupervised Approaches for Automatic
Keyword Extraction Using Meeting Transcripts.
Proceedings of NAACL 2009, Boulder, Colorado,
USA.
Belinda Maia. 2003. What are comparable corpora?
Proceedings of the Corpus Linguistics workshop on
Multilingual Corpora: Linguistic requirements and
technical perspectives, 2003, Lancaster, U.K.
Anthony McEnery and Zhonghua Xiao. 2007. Par-
allel and comparable corpora? In Incorporating
Corpora: Translation and the Linguist. Translating
Europe. Multilingual Matters, Clevedon, UK.
Emmanuel Morin, Beatrice Daille, Korchi Takeuchi
and Kyo Kageura. 2007. Bilingual terminology
mining using brain, not brawn comparable cor-
pora. Proceedings of ACL 2007, Prague, Czech Re-
public.
Dragos Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. Proceedings of ACL 2006, Syn-
dey, Australia.
Dragos Munteanu and Daniel Marcu. 2005. Improv-
ing machine translation performance by exploiting
non-parallel corpora. Computational Linguistics,
31(4): 477-504.
</reference>
<page confidence="0.967932">
18
</page>
<reference confidence="0.999751711111111">
\x0cDragos Munteanu, Alexander Fraser and Daniel
Marcu. 2004. Improved machine translation
performance via parallel sentence extraction from
comparable corpora. Proceedings of HLT-NAACL
2004, Boston, USA.
Franz Och and Hermann Ney. 2000. Improved Statis-
tical Alignment Models. Proceedings of ACL 2000,
Hongkong, China.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
Word Translation Extraction from Aligned Compa-
rable Documents. Proceedings of ACL-HLT 2011,
Portland, USA.
Reinhard Rapp. 1995. Identifying Word Translation
in Non-Parallel Texts. Proceedings of ACL 1995,
Cambridge, Massachusetts, USA.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. Proceedings of ACL 1999, College
Park, Maryland, USA.
Xabier Saralegi, Inaki Vicente and Antton Gurrutxaga.
2008. Automatic Extraction of Bilingual Terms
from Comparable Corpora in a Popular Science
Domain. Proceedings of the Workshop on Compa-
rable Corpora, LREC 2008, Marrakech, Morocco.
Serge Sharoff. 2007. Classifying Web corpora into
domain and genre using automatic feature identifi-
cation. Proceedings of 3rd Web as Corpus Work-
shop, Louvain-la-Neuve, Belgium.
Serge Sharoff, Bogdan Babych and Anthony Hartley.
2006. Using Comparable Corpora to Solve Prob-
lems Difficult for Human Translators. Proceedings
of ACL 2006, Syndey, Australia.
Jason Smith, Chris Quirk and Kristina Toutanova.
2010. Extracting Parallel Sentences from Compa-
rable Corpora using Document Level Alignment.
Proceedings of NAACL 2010, Los Angeles, USA.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat and Dan Tufis. 2006. The JRC-
Acquis: A multilingual aligned parallel corpus
with 20+ languages. Proceedings of LREC 2006,
Genoa, Italy.
Kun Yu and Junichi Tsujii. 2009. Extracting bilingual
dictionary from comparable corpora with depen-
dency heterogeneity. Proceedings of HLT-NAACL
2009, Boulder, Colorado, USA.
</reference>
<page confidence="0.974652">
19
</page>
<figure confidence="0.258899">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.881485">
<note confidence="0.968706333333333">b&amp;apos;Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 1019, Avignon, France, April 23 - 27 2012. c 2012 Association for Computational Linguistics</note>
<title confidence="0.9971445">Measuring Comparability of Documents in Non-Parallel Corpora for Efficient Extraction of (Semi-)Parallel Translation Equivalents</title>
<author confidence="0.997312">Fangzhong Su</author>
<affiliation confidence="0.9995845">Centre for Translation Studies University Of Leeds</affiliation>
<address confidence="0.998256">LS2 9JT, Leeds, UK</address>
<email confidence="0.985771">smlfs@leeds.ac.uk</email>
<author confidence="0.988157">Bogdan Babych</author>
<affiliation confidence="0.9993165">Centre for Translation Studies University Of Leeds</affiliation>
<address confidence="0.998373">LS2 9JT, Leeds, UK</address>
<email confidence="0.998055">b.babych@leeds.ac.uk</email>
<abstract confidence="0.999364411764706">In this paper we present and evaluate three approaches to measure comparability of documents in non-parallel corpora. We develop a task-oriented definition of comparability, based on the performance of automatic extraction of translation equivalents from the documents aligned by the proposed metrics, which formalises intuitive definitions of comparability for machine translation research. We demonstrate application of our metrics for the task of automatic extraction of parallel and semiparallel translation equivalents and discuss how these resources can be used in the frameworks of statistical and rule-based machine translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bogdan Babych</author>
<author>Serge Sharoff</author>
<author>Anthony Hartley</author>
</authors>
<title>Generalising Lexical Translation Strategies for MT Using Comparable Corpora.</title>
<date>2008</date>
<booktitle>Proceedings of LREC 2008,</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="2032" citStr="Babych et al., 2008" startWordPosition="280" endWordPosition="283">es are used for building translation phrase tables and calculating translation probabilities; and in RBMT, they are used for automatically building bilingual dictionaries of translation equivalents and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relie</context>
</contexts>
<marker>Babych, Sharoff, Hartley, 2008</marker>
<rawString>Bogdan Babych, Serge Sharoff and Anthony Hartley. 2008. Generalising Lexical Translation Strategies for MT Using Comparable Corpora. Proceedings of LREC 2008, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Chuang Chiao</author>
<author>Pierre Zweigenbaum</author>
</authors>
<date>2002</date>
<marker>Chiao, Zweigenbaum, 2002</marker>
<rawString>Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.</rawString>
</citation>
<citation valid="false">
<title>Looking for candidate translational equivalents in specialized, comparable corpora.</title>
<booktitle>Proceedings of COLING 2002,</booktitle>
<location>Taipei, Taiwan.</location>
<marker></marker>
<rawString>Looking for candidate translational equivalents in specialized, comparable corpora. Proceedings of COLING 2002, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="15117" citStr="Fellbaum, 1998" startWordPosition="2321" endWordPosition="2322"> weight of words in the document and rank the words by their TFIDF weights in descending order. The top n (e.g., 30) words are extracted as keywords to represent the document. Finally, the comparability of each document pair is determined by applying cosine similarity to their key4 Generally in JRC-Acquis, the size of parallel corpora for most of non-English langauge pairs is much smaller than that of language pairs which contain English. Therefore, the resulting bilingual dictionaries which contain English have better word coverage as they have many more dictionary entries. 5 We use WordNet (Fellbaum, 1998) for word lemmatization. word lists. 3.3 Machine translation based metrics Bilingual dictionary is used for word-for-word translation in the lexical mapping based metric and words which do not occur in the dictionary will be omitted. Thus, the mapping result is like a list of isolated words and information such as word order, syntactic structure and named entities can not be preserved. Therefore, in order to improve the text translation quality, we turn to the state-of-the-art SMT systems. In practice, we use Microsoft translation API6 to translate texts in under-resourced languages (e.g, Lith</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>Proceedings of ACL</booktitle>
<institution>University of Michigan,</institution>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="17105" citStr="Finkel et al., 2005" startWordPosition="2633" endWordPosition="2636">ed as bellow. WS = 0.5 (CD1/CD2) + 0.5 (SD1/SD2) suppose that CD1&amp;lt;=CD2, and SD1&amp;lt;=SD2. Keyword feature: Top-20 words (ranked by TFIDF weight) of each document. keyword similarity (denoted by WK) of two documents is also measured by cosine. Named entity feature: Named entities of each document. If more named entities cooccur in two documents, they are very likely to talk about the same event or subject and 6 Available at http://code.google.com/p/microsofttranslator-java-api/ 13 \x0cthus should be more comparable. We use Stanford named entity recognizer7 to extract named entities from the texts (Finkel et al., 2005). Again, cosine is then applied to measure the similarity of named entities (denoted by WN ) between a document pair. We then combine these four different types of score in an ensemble manner. Specifically, a weighted average strategy is applied: each individual score is associated with a constant weight, indicating the relative confidence (importance) of the corresponding type of score. The overall comparability score (denoted by SC) of a document pair is thus computed as below: SC = WL + WS + WK + WN where , , , and [0, 1], and + + + = 1. SC should be a value between 0 and 1, and larger SC v</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of ACL 2005, University of Michigan, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Gordon Paynter</author>
<author>Ian Witten</author>
</authors>
<date>1999</date>
<contexts>
<context position="14083" citStr="Frank et al., 1999" startWordPosition="2150" endWordPosition="2153">ased metric The lexical mapping based metric takes all the words in the text into account for comparability measure, but if we only retain a small number of representative words (keywords) and discard all the other less informative words in each document, can we judge the comparability of a document pair by comparing these words? Our intuition is that, if two document share more keywords, they should be more comparable. To validate this, we then perform keyword extraction by using a simple TFIDF based approach, which has been shown effective for keyword or keyphrase extraction from the texts (Frank et al., 1999; Hulth, 2003; Liu et al., 2009). More specifically, the keyword based metric can be described as below. First, similar to the lexical mapping based metric, bilingual dictionaries are used to map non-English texts into English. Thus, only the English resources are applied for stop-word filtering and word lemmatization, which are useful text preprocessing steps for keyword extraction. We then use TFIDF to measure the weight of words in the document and rank the words by their TFIDF weights in descending order. The top n (e.g., 30) words are extracted as keywords to represent the document. Final</context>
</contexts>
<marker>Frank, Paynter, Witten, 1999</marker>
<rawString>Eibe Frank, Gordon Paynter and Ian Witten. 1999.</rawString>
</citation>
<citation valid="true">
<title>Domain-specific keyphrase extraction.</title>
<date></date>
<booktitle>Proceedings of IJCAI 1999,</booktitle>
<location>Stockholm,</location>
<marker></marker>
<rawString>Domain-specific keyphrase extraction. Proceedings of IJCAI 1999, Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Percy Cheung</author>
</authors>
<title>Mining very non-parallel corpora: Parallel sentence and lexicon extraction via bootstrapping and EM.</title>
<date>2004</date>
<booktitle>Proceedings of EMNLP 2004,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1983" citStr="Fung and Cheung, 2004" startWordPosition="272" endWordPosition="275">MT) architectures: in SMT aligned parallel resources are used for building translation phrase tables and calculating translation probabilities; and in RBMT, they are used for automatically building bilingual dictionaries of translation equivalents and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject dom</context>
</contexts>
<marker>Fung, Cheung, 2004</marker>
<rawString>Pascale Fung and Percy Cheung. 2004a. Mining very non-parallel corpora: Parallel sentence and lexicon extraction via bootstrapping and EM. Proceedings of EMNLP 2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Percy Cheung</author>
</authors>
<title>Multi-level bootstrapping for extracting parallel sentences from a quasicomparable corpus.</title>
<date>2004</date>
<booktitle>Proceedings of COLING 2004,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1983" citStr="Fung and Cheung, 2004" startWordPosition="272" endWordPosition="275">MT) architectures: in SMT aligned parallel resources are used for building translation phrase tables and calculating translation probabilities; and in RBMT, they are used for automatically building bilingual dictionaries of translation equivalents and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject dom</context>
</contexts>
<marker>Fung, Cheung, 2004</marker>
<rawString>Pascale Fung and Percy Cheung. 2004b. Multi-level bootstrapping for extracting parallel sentences from a quasicomparable corpus. Proceedings of COLING 2004, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Hulth</author>
</authors>
<title>Improved Automatic Keyword Extraction Given More Linguistic Knowledge.</title>
<date>2003</date>
<booktitle>Proceedings of EMNLP 2003,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="14096" citStr="Hulth, 2003" startWordPosition="2154" endWordPosition="2155">cal mapping based metric takes all the words in the text into account for comparability measure, but if we only retain a small number of representative words (keywords) and discard all the other less informative words in each document, can we judge the comparability of a document pair by comparing these words? Our intuition is that, if two document share more keywords, they should be more comparable. To validate this, we then perform keyword extraction by using a simple TFIDF based approach, which has been shown effective for keyword or keyphrase extraction from the texts (Frank et al., 1999; Hulth, 2003; Liu et al., 2009). More specifically, the keyword based metric can be described as below. First, similar to the lexical mapping based metric, bilingual dictionaries are used to map non-English texts into English. Thus, only the English resources are applied for stop-word filtering and word lemmatization, which are useful text preprocessing steps for keyword extraction. We then use TFIDF to measure the weight of words in the document and rank the words by their TFIDF weights in descending order. The top n (e.g., 30) words are extracted as keywords to represent the document. Finally, the compa</context>
</contexts>
<marker>Hulth, 2003</marker>
<rawString>Anette Hulth. 2003. Improved Automatic Keyword Extraction Given More Linguistic Knowledge. Proceedings of EMNLP 2003, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Ion</author>
</authors>
<title>PEXACC: A Parallel Data Mining Algorithm from Comparable Corpora.</title>
<date>2012</date>
<booktitle>Proceedings of LREC 2012,</booktitle>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="24411" citStr="Ion, 2012" startWordPosition="3785" endWordPosition="3786">rpora. But are they also useful for other NLP tasks, such as translation equivalent detection from comparable corpora? In this section, we further measure the impact of the metrics on parallel phrase extraction (PPE) from comparable corpora. Our intuition is that, if document pairs are assigned higher comparability scores by the metrics, they should be more comparable and thus more parallel phrases can be extracted from them. The algorithm of parallel phrase extraction, which develops the approached presented in Munteanu and Marcu (2006), uses lexical overlap and structural matching measures (Ion, 2012). Taking a list of bilingual comparable document pairs as input, the extraction algorithm involves the following steps. 1. Split the source and target language documents into phrases. 2. Compute the degree of parallelism for each candidate pair of phrases by using the bilingual dictionary generated from GIZA++ (base dictionary), and retain all the phrase pairs with a score larger than a predefined parallelism threshold. 9 Remember that in our experiment, English is used as the pivot language for non-English langauge pairs. 15 \x0c3. Apply GIZA++ to the retained phrase pairs to detect new dicti</context>
</contexts>
<marker>Ion, 2012</marker>
<rawString>Radu Ion. 2012. PEXACC: A Parallel Data Mining Algorithm from Comparable Corpora. Proceedings of LREC 2012, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Tony Rose</author>
</authors>
<title>Measures for corpus similarity and homogeneity.</title>
<date>1998</date>
<booktitle>Proceedings of EMNLP</booktitle>
<location>Granada,</location>
<contexts>
<context position="7974" citStr="Kilgarriff and Rose (1998)" startWordPosition="1186" endWordPosition="1189">extraction in comparable corpora usually assumes that the corpora they use are reliably comparable and focuses on the design of efficient extraction algorithms. Therefore, there has been very little literature discussing the characteristics of comparable corpora (Maia, 2003). In this section, we introduce some representative work which tackles comparability metrics. Some studies (Sharoff, 2007; Maia, 2003; McEnery and Xiao, 2007) analyse comparability by assessing corpus composition, such as structural criteria (e.g., format and size), and linguistic criteria (e.g., topic, domain, and genre). Kilgarriff and Rose (1998) measure similarity and homogeneity between monolingual corpora. They generate word frequency list from each corpus and then apply 2 statistic on the most frequent n (e.g., 500) words of the compared corpora. 11 \x0cThe work which deals with comparability measures in cross-lingual comparable corpora is closer to our work. Saralegi et al. (2008) measure the degree of comparability of comparable corpora (English and Basque) according to the distribution of topics and publication dates of documents. They compute content similarity for all the document pairs between two corpora. These similarity s</context>
</contexts>
<marker>Kilgarriff, Rose, 1998</marker>
<rawString>Adam Kilgarriff and Tony Rose. 1998. Measures for corpus similarity and homogeneity. Proceedings of EMNLP 1998, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Li</author>
<author>Eric Gaussier</author>
</authors>
<title>Improving corpus comparability for bilingual lexicon extraction from comparable corpora.</title>
<date>2010</date>
<booktitle>Proceedings of COLING 2010,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="2282" citStr="Li and Gaussier, 2010" startWordPosition="318" endWordPosition="321">requent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability to such inexact and unmeasurable linguistic concepts. Research o</context>
<context position="9203" citStr="Li and Gaussier (2010)" startWordPosition="1379" endWordPosition="1382">re then input as parameters for the EMD (Earth Movers Distance) distance measure, which is employed to calculate the global compatibility of the corpora. Munteanu and Marcu (2005; 2006) select more comparable document pairs in a cross-lingual information retrieval based manner by using a toolkit called Lemur1. The retrieved document pairs then serve as input for the tasks of parallel sentence and sub-sentence extraction. Smith et al. (2010) treat Wikipedia as a comparable corpus and use interwiki links to identify aligned comparable document pairs for the task of parallel sentence extraction. Li and Gaussier (2010) propose a comparability metric which can be applied at both document level and corpus level and use it as a measure to select more comparable texts from other external sources into the original corpora for bilingual lexicon extraction. The metric measures the proportion of words in the source language corpus translated in the target language corpus by looking up a bilingual dictionary. They evaluate the metric on the rich-resourced English-French language pair, thus good dictionary resources are available. However, this is not the case for under-resourced languages in which reliable language </context>
</contexts>
<marker>Li, Gaussier, 2010</marker>
<rawString>Bo Li and Eric Gaussier. 2010. Improving corpus comparability for bilingual lexicon extraction from comparable corpora. Proceedings of COLING 2010, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feifan Liu</author>
<author>Deana Pennell</author>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>Unsupervised Approaches for Automatic Keyword Extraction Using Meeting Transcripts.</title>
<date>2009</date>
<contexts>
<context position="14115" citStr="Liu et al., 2009" startWordPosition="2156" endWordPosition="2159">ased metric takes all the words in the text into account for comparability measure, but if we only retain a small number of representative words (keywords) and discard all the other less informative words in each document, can we judge the comparability of a document pair by comparing these words? Our intuition is that, if two document share more keywords, they should be more comparable. To validate this, we then perform keyword extraction by using a simple TFIDF based approach, which has been shown effective for keyword or keyphrase extraction from the texts (Frank et al., 1999; Hulth, 2003; Liu et al., 2009). More specifically, the keyword based metric can be described as below. First, similar to the lexical mapping based metric, bilingual dictionaries are used to map non-English texts into English. Thus, only the English resources are applied for stop-word filtering and word lemmatization, which are useful text preprocessing steps for keyword extraction. We then use TFIDF to measure the weight of words in the document and rank the words by their TFIDF weights in descending order. The top n (e.g., 30) words are extracted as keywords to represent the document. Finally, the comparability of each do</context>
</contexts>
<marker>Liu, Pennell, Liu, Liu, 2009</marker>
<rawString>Feifan Liu, Deana Pennell, Fei Liu and Yang Liu. 2009. Unsupervised Approaches for Automatic Keyword Extraction Using Meeting Transcripts.</rawString>
</citation>
<citation valid="false">
<booktitle>Proceedings of NAACL 2009,</booktitle>
<location>Boulder, Colorado, USA.</location>
<marker></marker>
<rawString>Proceedings of NAACL 2009, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Belinda Maia</author>
</authors>
<title>What are comparable corpora? Proceedings of the Corpus Linguistics workshop on Multilingual Corpora: Linguistic requirements and technical perspectives,</title>
<date>2003</date>
<location>Lancaster, U.K. Anthony McEnery</location>
<contexts>
<context position="7623" citStr="Maia, 2003" startWordPosition="1139" endWordPosition="1140">corpora, documents and sub-document units. However, so far there is no widely accepted definition of comparability. For example, there is no agreement on the degree of similarity that documents in comparable corpora should have or on the criteria for measuring comparability. Also, most of the work that performs translation equivalent extraction in comparable corpora usually assumes that the corpora they use are reliably comparable and focuses on the design of efficient extraction algorithms. Therefore, there has been very little literature discussing the characteristics of comparable corpora (Maia, 2003). In this section, we introduce some representative work which tackles comparability metrics. Some studies (Sharoff, 2007; Maia, 2003; McEnery and Xiao, 2007) analyse comparability by assessing corpus composition, such as structural criteria (e.g., format and size), and linguistic criteria (e.g., topic, domain, and genre). Kilgarriff and Rose (1998) measure similarity and homogeneity between monolingual corpora. They generate word frequency list from each corpus and then apply 2 statistic on the most frequent n (e.g., 500) words of the compared corpora. 11 \x0cThe work which deals with compara</context>
</contexts>
<marker>Maia, 2003</marker>
<rawString>Belinda Maia. 2003. What are comparable corpora? Proceedings of the Corpus Linguistics workshop on Multilingual Corpora: Linguistic requirements and technical perspectives, 2003, Lancaster, U.K. Anthony McEnery and Zhonghua Xiao. 2007. Parallel and comparable corpora? In Incorporating Corpora: Translation and the Linguist. Translating Europe. Multilingual Matters, Clevedon, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Morin</author>
<author>Beatrice Daille</author>
<author>Korchi Takeuchi</author>
<author>Kyo Kageura</author>
</authors>
<title>Bilingual terminology mining using brain, not brawn comparable corpora.</title>
<date>2007</date>
<booktitle>Proceedings of ACL 2007,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2238" citStr="Morin et al., 2007" startWordPosition="310" endWordPosition="313">tically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability to such inexact and </context>
</contexts>
<marker>Morin, Daille, Takeuchi, Kageura, 2007</marker>
<rawString>Emmanuel Morin, Beatrice Daille, Korchi Takeuchi and Kyo Kageura. 2007. Bilingual terminology mining using brain, not brawn comparable corpora. Proceedings of ACL 2007, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Extracting parallel sub-sentential fragments from nonparallel corpora.</title>
<date>2006</date>
<booktitle>Proceedings of ACL</booktitle>
<location>Syndey, Australia.</location>
<contexts>
<context position="2365" citStr="Munteanu and Marcu, 2006" startWordPosition="330" endWordPosition="333">vailable, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability to such inexact and unmeasurable linguistic concepts. Research on comparable corpora needs not only good measures for comparability, but also a cle</context>
<context position="3692" citStr="Munteanu and Marcu (2006)" startWordPosition="533" endWordPosition="536"> paper we relate comparability to usefulness of comparable texts for MT. In particular, we propose a performance-based definition of comparability, as the possibility to extract parallel or quasi-parallel translation equivalents words, phrases and sentences which are translations of each other. This definition directly relates comparability to texts potential to improve the quality of MT by adding extracted phrases to phrase tables, training corpus or dictionaries. It also can be quantified as the rate of successful extraction of translation equivalents by automated tools, such as proposed in Munteanu and Marcu (2006). Still, successful detection of translation equivalents from comparable corpora very much de10 \x0cpends on the quality of these corpora, specifically on the degree of their textual equivalence and successful alignment on various text units. Therefore, the goal of this work is to provide comparability metrics which can reliably identify crosslingual comparable documents from raw corpora crawled from the Web, and characterize the degree of their similarity, which enriches comparable corpora with the document alignment information, filters out documents that are not useful and eventually leads </context>
<context position="24344" citStr="Munteanu and Marcu (2006)" startWordPosition="3773" endWordPosition="3776">ter out weakly comparable or non-comparable document pairs from the raw crawled corpora. But are they also useful for other NLP tasks, such as translation equivalent detection from comparable corpora? In this section, we further measure the impact of the metrics on parallel phrase extraction (PPE) from comparable corpora. Our intuition is that, if document pairs are assigned higher comparability scores by the metrics, they should be more comparable and thus more parallel phrases can be extracted from them. The algorithm of parallel phrase extraction, which develops the approached presented in Munteanu and Marcu (2006), uses lexical overlap and structural matching measures (Ion, 2012). Taking a list of bilingual comparable document pairs as input, the extraction algorithm involves the following steps. 1. Split the source and target language documents into phrases. 2. Compute the degree of parallelism for each candidate pair of phrases by using the bilingual dictionary generated from GIZA++ (base dictionary), and retain all the phrase pairs with a score larger than a predefined parallelism threshold. 9 Remember that in our experiment, English is used as the pivot language for non-English langauge pairs. 15 \</context>
</contexts>
<marker>Munteanu, Marcu, 2006</marker>
<rawString>Dragos Munteanu and Daniel Marcu. 2006. Extracting parallel sub-sentential fragments from nonparallel corpora. Proceedings of ACL 2006, Syndey, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<pages>477--504</pages>
<contexts>
<context position="2010" citStr="Munteanu and Marcu, 2005" startWordPosition="276" endWordPosition="279">T aligned parallel resources are used for building translation phrase tables and calculating translation probabilities; and in RBMT, they are used for automatically building bilingual dictionaries of translation equivalents and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so</context>
<context position="8759" citStr="Munteanu and Marcu (2005" startWordPosition="1311" endWordPosition="1314">nt n (e.g., 500) words of the compared corpora. 11 \x0cThe work which deals with comparability measures in cross-lingual comparable corpora is closer to our work. Saralegi et al. (2008) measure the degree of comparability of comparable corpora (English and Basque) according to the distribution of topics and publication dates of documents. They compute content similarity for all the document pairs between two corpora. These similarity scores are then input as parameters for the EMD (Earth Movers Distance) distance measure, which is employed to calculate the global compatibility of the corpora. Munteanu and Marcu (2005; 2006) select more comparable document pairs in a cross-lingual information retrieval based manner by using a toolkit called Lemur1. The retrieved document pairs then serve as input for the tasks of parallel sentence and sub-sentence extraction. Smith et al. (2010) treat Wikipedia as a comparable corpus and use interwiki links to identify aligned comparable document pairs for the task of parallel sentence extraction. Li and Gaussier (2010) propose a comparability metric which can be applied at both document level and corpus level and use it as a measure to select more comparable texts from ot</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31(4): 477-504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cDragos Munteanu</author>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Improved machine translation performance via parallel sentence extraction from comparable corpora.</title>
<date>2004</date>
<booktitle>Proceedings of HLT-NAACL 2004,</booktitle>
<location>Boston, USA.</location>
<contexts>
<context position="2472" citStr="Munteanu et al., 2004" startWordPosition="346" endWordPosition="349">oss-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability to such inexact and unmeasurable linguistic concepts. Research on comparable corpora needs not only good measures for comparability, but also a clearer, technologicallygrounded and quantifiable definition of comparability in the first place. In this pape</context>
</contexts>
<marker>Munteanu, Fraser, Marcu, 2004</marker>
<rawString>\x0cDragos Munteanu, Alexander Fraser and Daniel Marcu. 2004. Improved machine translation performance via parallel sentence extraction from comparable corpora. Proceedings of HLT-NAACL 2004, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>Proceedings of ACL</booktitle>
<location>Hongkong, China.</location>
<contexts>
<context position="11242" citStr="Och and Ney, 2000" startWordPosition="1684" endWordPosition="1687"> can be used for lexical mapping between a language pair. However, unlike the language pairs in which both languages are rich-resourced (e.g., English-French, or English-Spanish) and dictionary resources are relatively easy to obtain, it is likely that bilingual dictionaries with good word coverage are not publicly available for underresourced languages (e.g., English-Slovenian, or English-Lithuanian). In order to address this problem, we automatically construct dictionaries by using word alignment on large-scale parallel corpora (e.g., Europarl and JRC-Acquis2). Specifically, GIZA++ toolkit (Och and Ney, 2000) with default setting is used for word alignment on the JRC-Acquis parallel corpora (Steinberger et al., 2006). The aligned word pairs together with the alignment probabilities are then converted into dictionary entries. For example, in Estonian-English language pair, the alignment example kompanii company 0.625 in the word alignment table means the Estonian word kompanii can be translated as (or aligned with) the English candidate word company with a probability of 0.625. In the dictionary, the translation candidates are ranked by translation probability in descending order. Note that the dic</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Och and Hermann Ney. 2000. Improved Statistical Alignment Models. Proceedings of ACL 2000, Hongkong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Prochasson</author>
<author>Pascale Fung</author>
</authors>
<title>Rare Word Translation Extraction from Aligned Comparable Documents.</title>
<date>2011</date>
<booktitle>Proceedings of ACL-HLT 2011,</booktitle>
<location>Portland, USA.</location>
<marker>Prochasson, Fung, 2011</marker>
<rawString>Emmanuel Prochasson and Pascale Fung. 2011. Rare Word Translation Extraction from Aligned Comparable Documents. Proceedings of ACL-HLT 2011, Portland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying Word Translation in Non-Parallel Texts.</title>
<date>1995</date>
<booktitle>Proceedings of ACL 1995,</booktitle>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="2206" citStr="Rapp, 1995" startWordPosition="306" endWordPosition="307">n equivalents and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of co</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying Word Translation in Non-Parallel Texts. Proceedings of ACL 1995, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>Proceedings of ACL 1999,</booktitle>
<location>College Park, Maryland, USA.</location>
<contexts>
<context position="2218" citStr="Rapp, 1999" startWordPosition="308" endWordPosition="309">s and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability </context>
<context position="10266" citStr="Rapp, 1999" startWordPosition="1550" endWordPosition="1551">anguage pair, thus good dictionary resources are available. However, this is not the case for under-resourced languages in which reliable language resources such as machine-readable bilingual dictionaries with broad word coverage or word lemmatizers might be not publicly available. 3 Comparability Metrics To measure the comparability degree of document pairs in different languages, we need to translate the texts or map lexical items from the source language into the target languages so that we can compare them within the same language. Usually this can be done by using bilingual dictionaries (Rapp, 1999; Li and Gaussier, 2010; Prachasson and Fung, 2011) or existing machine translation tools. Based on this process, in this section we present three different approaches to measure the 1 Available at http://www.lemurproject.org/ comparability of comparable documents. 3.1 Lexical mapping based metric It is straightforward that we expect a bilingual dictionary can be used for lexical mapping between a language pair. However, unlike the language pairs in which both languages are rich-resourced (e.g., English-French, or English-Spanish) and dictionary resources are relatively easy to obtain, it is l</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora. Proceedings of ACL 1999, College Park, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xabier Saralegi</author>
</authors>
<title>Inaki Vicente and Antton Gurrutxaga.</title>
<date>2008</date>
<booktitle>Proceedings of the Workshop on Comparable Corpora, LREC</booktitle>
<location>Marrakech, Morocco.</location>
<marker>Saralegi, 2008</marker>
<rawString>Xabier Saralegi, Inaki Vicente and Antton Gurrutxaga. 2008. Automatic Extraction of Bilingual Terms from Comparable Corpora in a Popular Science Domain. Proceedings of the Workshop on Comparable Corpora, LREC 2008, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serge Sharoff</author>
</authors>
<title>Classifying Web corpora into domain and genre using automatic feature identification.</title>
<date>2007</date>
<booktitle>Proceedings of 3rd Web as Corpus Workshop,</booktitle>
<location>Louvain-la-Neuve, Belgium.</location>
<contexts>
<context position="7744" citStr="Sharoff, 2007" startWordPosition="1155" endWordPosition="1156"> example, there is no agreement on the degree of similarity that documents in comparable corpora should have or on the criteria for measuring comparability. Also, most of the work that performs translation equivalent extraction in comparable corpora usually assumes that the corpora they use are reliably comparable and focuses on the design of efficient extraction algorithms. Therefore, there has been very little literature discussing the characteristics of comparable corpora (Maia, 2003). In this section, we introduce some representative work which tackles comparability metrics. Some studies (Sharoff, 2007; Maia, 2003; McEnery and Xiao, 2007) analyse comparability by assessing corpus composition, such as structural criteria (e.g., format and size), and linguistic criteria (e.g., topic, domain, and genre). Kilgarriff and Rose (1998) measure similarity and homogeneity between monolingual corpora. They generate word frequency list from each corpus and then apply 2 statistic on the most frequent n (e.g., 500) words of the compared corpora. 11 \x0cThe work which deals with comparability measures in cross-lingual comparable corpora is closer to our work. Saralegi et al. (2008) measure the degree of c</context>
</contexts>
<marker>Sharoff, 2007</marker>
<rawString>Serge Sharoff. 2007. Classifying Web corpora into domain and genre using automatic feature identification. Proceedings of 3rd Web as Corpus Workshop, Louvain-la-Neuve, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serge Sharoff</author>
<author>Bogdan Babych</author>
<author>Anthony Hartley</author>
</authors>
<title>Using Comparable Corpora to Solve Problems Difficult for Human Translators.</title>
<date>2006</date>
<booktitle>Proceedings of ACL</booktitle>
<location>Syndey, Australia.</location>
<contexts>
<context position="1960" citStr="Sharoff et al., 2006" startWordPosition="268" endWordPosition="271">tly, in Rule-Based (RBMT) architectures: in SMT aligned parallel resources are used for building translation phrase tables and calculating translation probabilities; and in RBMT, they are used for automatically building bilingual dictionaries of translation equivalents and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging </context>
</contexts>
<marker>Sharoff, Babych, Hartley, 2006</marker>
<rawString>Serge Sharoff, Bogdan Babych and Anthony Hartley. 2006. Using Comparable Corpora to Solve Problems Difficult for Human Translators. Proceedings of ACL 2006, Syndey, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Smith</author>
<author>Chris Quirk</author>
<author>Kristina Toutanova</author>
</authors>
<title>Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment.</title>
<date>2010</date>
<contexts>
<context position="2493" citStr="Smith et al., 2010" startWordPosition="350" endWordPosition="353">corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability to such inexact and unmeasurable linguistic concepts. Research on comparable corpora needs not only good measures for comparability, but also a clearer, technologicallygrounded and quantifiable definition of comparability in the first place. In this paper we relate comparabi</context>
<context position="9025" citStr="Smith et al. (2010)" startWordPosition="1352" endWordPosition="1355">ing to the distribution of topics and publication dates of documents. They compute content similarity for all the document pairs between two corpora. These similarity scores are then input as parameters for the EMD (Earth Movers Distance) distance measure, which is employed to calculate the global compatibility of the corpora. Munteanu and Marcu (2005; 2006) select more comparable document pairs in a cross-lingual information retrieval based manner by using a toolkit called Lemur1. The retrieved document pairs then serve as input for the tasks of parallel sentence and sub-sentence extraction. Smith et al. (2010) treat Wikipedia as a comparable corpus and use interwiki links to identify aligned comparable document pairs for the task of parallel sentence extraction. Li and Gaussier (2010) propose a comparability metric which can be applied at both document level and corpus level and use it as a measure to select more comparable texts from other external sources into the original corpora for bilingual lexicon extraction. The metric measures the proportion of words in the source language corpus translated in the target language corpus by looking up a bilingual dictionary. They evaluate the metric on the </context>
</contexts>
<marker>Smith, Quirk, Toutanova, 2010</marker>
<rawString>Jason Smith, Chris Quirk and Kristina Toutanova. 2010. Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment.</rawString>
</citation>
<citation valid="false">
<booktitle>Proceedings of NAACL 2010,</booktitle>
<location>Los Angeles, USA.</location>
<marker></marker>
<rawString>Proceedings of NAACL 2010, Los Angeles, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
<author>Bruno Pouliquen</author>
<author>Anna Widiger</author>
</authors>
<title>Camelia Ignat and Dan Tufis.</title>
<date>2006</date>
<booktitle>Proceedings of LREC 2006,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="11352" citStr="Steinberger et al., 2006" startWordPosition="1702" endWordPosition="1706">oth languages are rich-resourced (e.g., English-French, or English-Spanish) and dictionary resources are relatively easy to obtain, it is likely that bilingual dictionaries with good word coverage are not publicly available for underresourced languages (e.g., English-Slovenian, or English-Lithuanian). In order to address this problem, we automatically construct dictionaries by using word alignment on large-scale parallel corpora (e.g., Europarl and JRC-Acquis2). Specifically, GIZA++ toolkit (Och and Ney, 2000) with default setting is used for word alignment on the JRC-Acquis parallel corpora (Steinberger et al., 2006). The aligned word pairs together with the alignment probabilities are then converted into dictionary entries. For example, in Estonian-English language pair, the alignment example kompanii company 0.625 in the word alignment table means the Estonian word kompanii can be translated as (or aligned with) the English candidate word company with a probability of 0.625. In the dictionary, the translation candidates are ranked by translation probability in descending order. Note that the dictionary collects inflectional form of words, but not only base form of words. This is because the dictionary i</context>
</contexts>
<marker>Steinberger, Pouliquen, Widiger, 2006</marker>
<rawString>Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat and Dan Tufis. 2006. The JRCAcquis: A multilingual aligned parallel corpus with 20+ languages. Proceedings of LREC 2006, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Yu</author>
<author>Junichi Tsujii</author>
</authors>
<title>Extracting bilingual dictionary from comparable corpora with dependency heterogeneity.</title>
<date>2009</date>
<booktitle>Proceedings of HLT-NAACL 2009,</booktitle>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="2259" citStr="Yu and Tsujii, 2009" startWordPosition="314" endWordPosition="317">ingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability to such inexact and unmeasurable linguist</context>
</contexts>
<marker>Yu, Tsujii, 2009</marker>
<rawString>Kun Yu and Junichi Tsujii. 2009. Extracting bilingual dictionary from comparable corpora with dependency heterogeneity. Proceedings of HLT-NAACL 2009, Boulder, Colorado, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>