<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<figure confidence="0.899170833333333">
b&amp;apos;A Hybrid Japanese Parser with Hand-crafted Grammar and
Statistics
Hiroshi Kanayama
, Kentaro Torisawa
,
Yutaka Mitsuishi
</figure>
<author confidence="0.255548">
and Junichi Tsujii?
</author>
<affiliation confidence="0.686798">
Tokyo Research Laboratory, IBM Japan, Ltd.
</affiliation>
<address confidence="0.695718">
1623-14 Shimo-tsuruma, Yamato-shi, Kanagawa 242-8502, Japan
</address>
<affiliation confidence="0.947877">
Department of Information Science, Graduate School of Science, University of Tokyo
</affiliation>
<address confidence="0.836846">
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan
</address>
<category confidence="0.437257333333333">
Information and Human Behavior, PRESTO, Japan Science and Technology Corporation
Kawaguchi Hon-cho 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan
? CCL, UMIST, U.K.
</category>
<email confidence="0.830233">
{kanayama, torisawa, mitsuisi, tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.989472" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999808952380953">
This paper describes a hybrid parsing method for
Japanese which uses both a hand-crafted gram-
mar and a statistical technique. The key feature
of our system is that in order to estimate likeli-
hood for a parse tree, the system uses informa-
tion taken from alternative partial parse trees gen-
erated by the grammar. This utilization of alter-
native trees enables us to construct a new statis-
tical model called Triplet/Quadruplet Model. We
show that this model can capture a certain ten-
dency in Japanese syntactic structures and this point
contributes to improvement of parsing accuracy on
a shallow level. We report that, with an under-
specified HPSG-based grammar and a maximum en-
tropy estimation, our parser achieved high accuracy:
88.6% accuracy in dependency analysis of the EDR
annotated corpus, and that it outperformed other
purely statistical parsing methods on the same cor-
pus. This result suggests that proper treatment of
hand-crafted grammars can contribute to parsing ac-
curacy on a shallow level.
</bodyText>
<sectionHeader confidence="0.998341" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9984833125">
There have been many attempts to combine hand-
crafted high-level grammars, such as FB-LTAG,
HPSG and LFG, and statistical disambiguation
techniques to obtain precise linguistic structures
(Schabes, 1992; Abney, 1996; Carroll et al., 1998).
One evident advantage of this approach over purely
statistical parsing techniques is that grammars can
provide precise semantic representations. However,
considering that remarkable parsing accuracy in a
shallow level has been achieved by purely statisti-
cal techniques (e.g. Ratnaparkhi (1997)), it may be
thought more reasonable to use high-level grammars
just for postprocessing which maps results of shallow
syntactical analyses onto deep analyses.
This work was conducted while the first author was a
graduate student at Univ. of Tokyo.
</bodyText>
<equation confidence="0.534381333333333">
n h
M
NH H
</equation>
<figureCaption confidence="0.999255">
Figure 1: A tree M with a non-head daughter NH and
</figureCaption>
<bodyText confidence="0.980406583333333">
a head daughter H.
In this work we propose that hand-crafted high-
level grammars can be useful in shallow-level analy-
ses and statistical models. In our framework, gram-
mars are used to obtain precise features for probabil-
ity estimation, which are difficult to obtain without a
grammar, and we show that such features contribute
to high parsing accuracy on a shallow level.
In this paper, the most preferable parse trees are
chosen with a statistical model. In our method, the
likelihood value L(M) of a (partial) tree M in Fig-
ure 1 is defined as in (1):
</bodyText>
<equation confidence="0.886821333333333">
L(M)
def
= L(NH) L(H) P(n h) (1)
</equation>
<bodyText confidence="0.9199725">
where NH is Ms non-head daughter (whose lexical
head is n), H is the head-daughter (whose lexical
head is h), and P(n h) is the probability of n
being related to h. For a single lexical item W, L(W)
is defined as 1.0.
In most models already proposed, the probability
P(n h) is calculated with the conditional proba-
bility (2):
</bodyText>
<equation confidence="0.995289666666667">
P(n h)
def
= P(T  |n, h, n,h) (2)
</equation>
<bodyText confidence="0.996954555555556">
where T indicates that the dependency is true; n
and h are attributes of n and h, respectively. And
n,h, the distance between the two words, is widely
used, because this attribute is believed to strongly
affect whether those two words are going to be re-
lated.
In contrast, in the statistical model proposed in
this paper, P(n h) depends not only on the at-
tributes of the tree M, but also on alternative trees
</bodyText>
<equation confidence="0.861687454545455">
\x0cInput sentence : &lt; n h1 hi hl &amp;gt;
n h1
M1
NH1 H1
n hi
Mi
NHi Hi
n hl
Ml
NHl
Hl
</equation>
<figureCaption confidence="0.8451875">
Figure 2: Partial trees whose non-head daughters lexi-
cal head is n.
</figureCaption>
<bodyText confidence="0.999636444444444">
in the parse forest generated by the grammar. More
precisely, when P(n h) is calculated, we consider
partial trees whose non-head daughters lexical head
is n, as displayed in Figure 2. Here alternative pos-
sible hk (k = 1, , l) are taken into consideration,
and ordered according to their distance to n. We
call such set of hk modification candidates, and all
modification candidates are placed together in the
conditional part of the probability as in (3). Now
</bodyText>
<equation confidence="0.959395833333333">
assume h = hi.
P(n hi)
def
= P(i  |n, h1 , h2 , , hi , , hl
)
(3)
</equation>
<bodyText confidence="0.999435043478261">
where i indicates the ith candidate among the
modification candidates. Equation (3) shows two
important properties of our model. One point lies in
the new distance metric. (3) is the probability that n
chooses the ith candidate as the modifiee among the
modification candidates which are ordered according
to their distance to n. Thus, we no longer require
the distance metric n,h, instead we use the relative
position among the modification candidates, which
works as an attribute of the modification. The other
point is the use of the attributes of the alternative
parse trees, that is, attributes of the modifier and all
its modification candidates are considered simulta-
neously. We show that these techniques sophisticate
our model, by providing linguistic examples in Sec-
tion 3.2.
In practice, however, treating all candidates is not
feasible because of data-sparseness. We therefore
apply a strategy of restricting the modification can-
didates to at most three. The strategy and its justi-
fication are discussed in Section 3.1.
Applying the strategy to the equation (3), we ob-
tain equations (4) and (5):
</bodyText>
<equation confidence="0.998518571428571">
P(n hi)
def
= P(i  |n, h1 , h2 ) (i = 1, 2) (4)
P(n hi)
def
= P(i  |n, h1 , h2 , hl
) (i = 1, 2, l)(5)
</equation>
<bodyText confidence="0.975291125">
When there are only two candidates, equation (4)
is used; otherwise, equation (5) is used. Our statis-
tical model is called the Triplet/Quadruplet Model,
which was named after the number of constituents
in the conditional parts of the equations.
We report that our parsing framework achieved
high accuracy (88.6%) in dependency analysis of
Japanese with a combination of an underspecified
</bodyText>
<equation confidence="0.902352333333333">
l r
M
L R
l0
r0
?
</equation>
<figureCaption confidence="0.982138">
Figure 3: Transformation from a tree to a dependency.
</figureCaption>
<page confidence="0.84844">
l0
</page>
<bodyText confidence="0.9954714">
and r0
denote the bunsetsus l and r belong to, respec-
tively.
HPSG-based Japanese grammar, SLUNG (Mitsu-
ishi et al., 1998) and the maximum entropy method
(Berger et al., 1996). Moreover, the resulting parse
trees generated by our hybrid parser are legitimate
trees in terms of given hand-crafted grammars, and
we are expecting that we can enjoy advantages pro-
vided by high-level grammar formalisms, such as
construction of semantic structures.
In the above explanation, we used the notion of
lexical heads for the estimation of probabilities of
trees for the sake of simplicity. But, in the present
implementation, we use bunsetsus instead of lexical
heads, and a relation on a tree is converted to a
bunsetsu-dependency as shown in Figure 3. A bun-
setsu is a basic syntactic unit in Japanese. It consists
of a content word and some functional morphemes
such as a particle.
In Section 2, we describe some existing statisti-
cal parsers, and the Japanese grammar which we
adopted. Section 3 describes our statistical method
and its advantages in detail. We report experimental
results in Section 4.
</bodyText>
<sectionHeader confidence="0.993367" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.990079">
In this section, we describe several models for
Japanese dependency analysis and works on statisti-
cal approaches with grammars. Next, we introduce
SLUNG, the HPSG-based Japanese grammar which
is used in our hybrid parser.
</bodyText>
<subsectionHeader confidence="0.8991505">
2.1 Previous Dependency Analysis Models
of Japanese
</subsectionHeader>
<bodyText confidence="0.988571333333333">
Several statistical models for Japanese dependency
analysis which do not utilize a hand-crafted gram-
mar have been proposed. We evaluate the accuracy
of bunsetsu-dependencies as they do, thus here we
introduce them for comparison. All models intro-
duced below are based on the likelihood value of the
dependency between two bunsetsus. But they differ
from each other in the attributes or outputs which
are considered when a likelihood value is calculated.
There are some models which calculate the likeli-
hood values of a dependency between bunsetsu i and
j as in (6), such as a decision tree model (Haruno et
al., 1998), a maximum entropy model (Uchimoto et
al., 1999), a model based on distance and lexical in-
formation (Fujio and Matsumoto, 1998). Attributes
i and j consist of a part-of-speech (POS), a lexi-
cal item, presence of a comma, and so on. And i,j
\x0cis the number of intervening bunsetsus between i and
</bodyText>
<equation confidence="0.97212175">
j.
P(i j)
def
= P(T  |i, j, i,j) (6)
</equation>
<bodyText confidence="0.99016145">
However, these models fail to reflect contextual
information because attributes of the surrounding
bunsetsus are not considered.
Uchimoto et al. (2000) proposed a model us-
ing posterior context. The model utilizes not only
attributes about bunsetsus i, j but also attributes
about all bunsetsus (including j) which follow bun-
setsu i. That is, instead of learning two output val-
ues T(true) or F(false) for the dependency be-
tween two bunsetsus, three output values are used
for learning: the bunsetsu i is bynd (dependent on
a bunsetsu beyond j), dpnd (dependent on the
bunsetsu j) or btwn (dependent on a bunsetsu be-
tween i and j). The probability is calculated by
multiplying probabilities for all bunsetsus which fol-
low bunsetsu i as in (7). They report that this kind
of contextual information improves accuracy. How-
ever, the model has to assume the independency of
all the random variables, which may cause some er-
rors.
</bodyText>
<equation confidence="0.9995041">
P(i j)
def
=
Y
i&lt;k&lt;j
P(bynd  |i, k, i,k)
P (dpnd  |i, j, i,j)
Y
k&amp;gt;j
P(btwn  |i, k, i,k)(7)
</equation>
<bodyText confidence="0.999643">
The difference between our model and these pre-
vious models are discussed in Section 3.
</bodyText>
<subsectionHeader confidence="0.994586">
2.2 Statistical Approaches with a grammar
</subsectionHeader>
<bodyText confidence="0.9762658">
There have been many proposals for statistical
frameworks particularly designed for parsers with
hand-crafted grammars (Schabes, 1992; Briscoe and
Carroll, 1993; Abney, 1996; Inui et al., 1997). The
main issue in this type of research is how to assign
likelihoods to a single linguistic structure generated
by a grammar. Some of them (Briscoe and Carroll,
1993; Inui et al., 1997) treat information on contexts,
but the contextual information is derived only from
a structure to which the parser is trying to assign
a likelihood value. Then, the major difference be-
tween their method and ours is that we consider the
attributes of alternative linguistic structures gener-
ated by the grammar in order to determine the like-
lihood for linguistic structures.
</bodyText>
<subsectionHeader confidence="0.955521">
2.3 SLUNG : Japanese Grammar
</subsectionHeader>
<bodyText confidence="0.9746173125">
The Japanese grammar which we adopted, SLUNG
(Mitsuishi et al., 1998), is an HPSG-based under-
specified grammar. It consists of 8 rule schemata,
48 lexical templates for POSs and 105 lexical entries
for functional words. As can be seen from these fig-
ures, the grammar does not contain detailed lexical
information that needs intensive labor for develop-
ment. However, it is precise in the sense that it
achieves 83.7% dependency accuracy with a simple
heuristics2
for the EDR annotated corpus, and it
can produce at least one parse tree for 98.4% sen-
tences in the EDR annotated corpus. We use the
grammar for generating parse tree forests, and our
Triplet/Quadruplet Model is used for picking up a
single tree from a forest.
</bodyText>
<sectionHeader confidence="0.986201" genericHeader="method">
3 The Hybrid Parsing Method
</sectionHeader>
<bodyText confidence="0.999379">
This section describes the procedure of parsing with
the Triplet/Quadruplet Model. Our hybrid parsing
method proceeds as follows:
At the beginning, dependency structures are
obtained from trees generated by SLUNG. For
each bunsetsu, modification candidates are enu-
merated, and if there are four or more candi-
dates, they are restricted to three. The heuristic
used in this process is described in Section 3.1.
Then, with the Triplet/Quadruplet Model and
maximum entropy estimation, probabilities of
the dependencies are calculated. Section 3.2
discusses the characteristics and advantages of
the model.
Finally, the most preferable trees for the whole
sentence are selected.
</bodyText>
<subsectionHeader confidence="0.999645">
3.1 Restriction of Modification Candidates
</subsectionHeader>
<bodyText confidence="0.999777071428571">
Kanayama et al. (1999) report that when mod-
ification candidates are enumerated according to
SLUNG, 98.6% of the correct modifiees are in one of
the following three positions among the candidates:
the nearest one from the modifier, the second nearest
one, and the farthest one.
As a consequence, we can simplify the problem
by considering only these three candidates and dis-
carding the other candidates, with only 1.4% poten-
tial errors. We therefore assume that the number of
modification candidates is always three or less.
This idea is similar to that of Sekine (2000)s
study, which restricts the candidates to five, but in
his case, without a grammar.
</bodyText>
<subsectionHeader confidence="0.99097">
3.2 The Triplet/Quadruplet Model
</subsectionHeader>
<bodyText confidence="0.993535285714286">
The Triplet/Quadruplet Model calculates the like-
lihood of the dependency between bunsetsu i and
bunsetsu cn; P(i cn) with the formulas (8) and
(9), where cn denotes the nth candidate among bun-
setsu is candidates; i denotes some attributes of
i; and cn denotes attributes of cn (including at-
tributes between i and cn).
</bodyText>
<equation confidence="0.999303142857143">
P(i cn)
def
= P(n  |i, c1 , c2 ) (n = 1, 2) (8)
P(i cn)
def
= P(n  |i, c1 , c2 , cl
) (n = 1, 2, l)(9)
</equation>
<bodyText confidence="0.948342095238095">
2This heuristics is a Japanese version of a left-association
rule: see (Mitsuishi et al., 1998) for detail.
\x0cAs (8) and (9) suggest, the model considers at-
tributes of the modifier bunsetsu and attributes of all
modification candidates simultaneously in the condi-
tional parts of the probabilities. Moreover, what is
calculated is not the probability of whether the de-
pendency is correct (T, see Formula(6)), but the
probability of which of the given candidates is cho-
sen as the modifiee (n =1, 2, or l). These charac-
teristics imply the following two advantages.
Advantage 1 A new distance metric. The correct
modifiee can be chosen by considering relative
position among grammatically licensed candi-
dates, instead of the absolute distance between
bunsetsus.
Advantage 2 Treating alternative trees. The can-
didates are taken into consideration simultane-
ously. But because the modification candidates
are restricted to at most three, we considerably
avoid data-sparseness problems.
Below we discuss these advantages in order. These
advantages clarify the differences from previous
models described in Section 2.1, and are empirically
confirmed through the experiments in Section 4.
3.2.1 Advantage 1 : A new distance metric
As discussed in Section 2.1, the distance metric i,j
used in previous statistical methods was obtained
simply by counting intervening words or bunsetsus
between i and j. On the other hand, we use the rel-
ative position among the modification candidates as
the distance metric. The following examples illus-
trate a difference between those two types of metric.
The correct modifiee of kare-ga is hashiru-no-wo in
both (10a) and (10b).
(10)a. kare-ga hashiru-no-wo mita koto
he-SUBJ run see fact
(the fact that I saw him run)
b. kare-ga yukkuri hashiru-no-wo mita koto
he-SUBJ slowly run see fact
(the fact that I saw him run slowly)
In previous models, (10a) and (10b) would yield,
</bodyText>
<equation confidence="0.6023075">
Pa(kare-ga hashiru-no-wo)=P(T|kare-ga, hashiru-no-wo,1)
Pb(kare-ga hashiru-no-wo)=P(T|kare-ga, hashiru-no-wo,2)
</equation>
<bodyText confidence="0.999058625">
respectively, where 1 = 1 and 2 = 2. Then, the
two probabilities above do not have the same value
in general.
Our grammar does not allow the dependency
kare-ga yukkuri for (10b). The modification
candidates of kare-ga are hashiru-no-wo and mita,
hence (8) gives the probabilities between kare-ga and
hashiru-no-wo as follows, in both examples.
</bodyText>
<equation confidence="0.971820666666667">
Pa(kare-ga hashiru-no-wo)
= Pb(kare-ga hashiru-no-wo)
= P(1|kare-ga, hashiru-no-wo, mita)
</equation>
<bodyText confidence="0.998969454545454">
Thus, P(kare-ga hashiru-no-wo) has the same value
for both examples. Our interpretation of this differ-
ence is summarized as follows. The word yukkuri is
an adverb modifying the verb hashiru. Our linguis-
tic intuition tells us that the presence of such adverb
should not affect the strength for the dependency
between kare-ga and hashiru-no-wo. According to
this intuition, the existence of the adverb should be
considered as a noise. Our model allows us to ignore
such a noise in learning from annotated corpus, while
previous models are affected by such noisy elements.
</bodyText>
<table confidence="0.956113909090909">
3.2.2 Advantage 2 : Treating alternative
trees or contextual information
Consider the following examples.
(11)a. Taro-no kawaii musume
NP Adj NP
Taro-POSS pretty daughter
(Taros pretty daughter)
b. Taro-no yuujin-no musume
NP NP NP
Taro-POSS friend-POSS daughter
(Taros friends daughter)
</table>
<bodyText confidence="0.903573111111111">
Contrary to the previous examples, Taro-no in
(11) modifies different modification candidates. In
example (11a), Taro-no musume is the correct
dependency while Taro-no musume is not cor-
rect in (11b). This difference is caused by the bun-
setsu between Taro-no and musume, kawaii (Adj)
in (11a) and yuujin-no (NP) in (11b). Actually, the
grammar allows Taro-no to depend on either of these
types of words. Thus, in our model,
</bodyText>
<equation confidence="0.9954405">
Pa(Taro-no musume)
= P(2|Taro-no, kawaii, musume)
Pb(Taro-no musume)
= P(2|Taro-no, yuujin-no, musume)
</equation>
<bodyText confidence="0.991943285714286">
Then, P(Taro-no musume) has different values
for the two examples. In the annotated corpus,
P(2|Taro-no, kawaii, musume) tends to have a high
value since kawaii is an adjective. However, since
yuujin-no is an NP, P(2|Taro-no, yuujin-no, musume)
tends to have a low value.
Now consider previous models.
</bodyText>
<equation confidence="0.99849">
Pa(Taro-no musume) = P(T|Taro-no, musume, 2)
Pb(Taro-no musume) = P(T|Taro-no, musume, 2)
</equation>
<bodyText confidence="0.876456">
Then, contrary to our model, P(Taro-no musume)
has exactly the same value for both examples. The
outcome is determined by
</bodyText>
<equation confidence="0.779842">
Pa(Taro-no kawaii) = P(T|Taro-no, kawaii, 1)
</equation>
<bodyText confidence="0.994289875">
In text corpora, P(T|Taro-no, yuujin-no, 1) tends
to be high, and consequently, P(T|Taro-no, musume,
2) is very small. These values will make the correct
prediction for (11b) as yuujin-no will be favored over
musume. However, for (11a), these models are likely
to incorrectly favor kawaii over musume. This is
\x0cbecause P(T|Taro-no, musume, 2), being very small, is
likely to be smaller than P(T|Taro-no, kawaii, 1).
</bodyText>
<sectionHeader confidence="0.997704" genericHeader="method">
4 Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.999293">
This section reports a series of parsing experiments
with our model, and gives some discussion.
</bodyText>
<subsectionHeader confidence="0.994805">
4.1 Environments
</subsectionHeader>
<bodyText confidence="0.974550538461538">
We used the EDR Japanese Corpus (EDR, 1996)
for training and evaluation of parsing accuracy. The
EDR Corpus is a Japanese treebank which consists
of 208,157 sentences from newspapers and maga-
zines. We used 192,778 sentences for training, 6,744
for pre-analysis (as reported in Section 3.1), and
3,372 for testing3
.
With triplets constituted of a modifiee and two
modification candidates extracted from the learn-
ing corpus the Triplet Model is constructed. With
the quadruplets constituted of a modifiee and three
candidates, the Quadruplet Model is constructed.
These models are estimated by the ChoiceMaker
Maximum Entropy Estimator (Borthwick, 1999).
The features for the estimation are listed in Ta-
ble 1. The values partially follow other researches
e.g. Uchimoto et al. (1999), and JUMANs outputs
are used for POS classification. Mainly the head of
the bunsetsu (the rightmost morpheme in a bunsetsu
except for whose major POS is peculiar, auxiliary
verb, particle, suffix or copula) and type of
the bunsetsu (the rightmost morpheme in a bunsetsu
except for whose major POS is peculiar) are used
as the attributes. We show the meaning of some
features below.
POS JUMANs minor POS (for both head and
type).
particle, adverb Frequent words: 26 particles and
69 adverbs.
head lex 294 lexical forms regardless of their POS.
type lex 70 suffixes or auxiliary verbs.
inflection 6 types of inflection : normal, adver-
bial, adnominal, te-form, ta-form, and
others.
The column variation in Table 1 denotes the
number of possible values for the feature. Valid
features indicates the number of features which ap-
peared three times or more in the training corpus.
</bodyText>
<sectionHeader confidence="0.638744" genericHeader="method">
4.2 Results
</sectionHeader>
<bodyText confidence="0.9833485">
With our model and the features described above,
the accuracy shown in Table 2 is achieved. We eval-
uate the following two types of accuracy:
35,263 sentences were removed because the order of the
words in the annotation differed from that in the original
sentences.
</bodyText>
<figure confidence="0.955400666666667">
ID Feature type Variation
Valid features
Trip. Quad.
1 Head POS of modifier 24 42 64
2 Type POS of modifier 34 66 99
3 Particle of modifier 27 47 73
4 Adverb of modifier 70 131 193
5 Type lex of modifier 71 110 225
6 Inflection of modifier 6 12 18
7 Whether modifier has a comma 2 4 6
8 Head POS of modifiee 24 70 158
9 Type POS of modifiee 34 96 231
10 Head lex of modifiee 295 1164 2597
11 Particle of modifiee 27 92 204
12 Type lex of modifiee 71 216 454
13 Inflection of modifiee 6 24 53
14 Whether modifiee has a comma 2 8 18
15 Whether modifiee has wa 2 8 18
16 Whether modifiee has to 2 6 17
17 # of commas between two bunsetsus 4 16 36
18 # of wa between two bunsetsus 3 12 27
</figure>
<table confidence="0.9560483">
19 2 8 816 1187 2727
20 2 7 14 136 380 870
21 3 10 7965 6465 13463
22 2 9 1156 1213 3108
23 3 11 729 618 1637
24 2 11 918 1025 2494
25 2 12 2414 1483 3514
26 2 3 7 8 132192 1331 3058
27 1 2 6 8 13 705024 6605 14700
Total - 22433 50063
</table>
<tableCaption confidence="0.999058">
Table 1: Used features : Features from 8 to 27 are re-
</tableCaption>
<bodyText confidence="0.973686333333333">
lated to the modifiee, thus they are considered for each
candidate. Features from 19 to 27 are combination fea-
tures.
</bodyText>
<table confidence="0.865973375">
In-coverage
sentences
Bunsetsu accuracy 88.55%(23078/26062)
Sentence accuracy 46.90% (1560/3326)
All
sentences
Bunsetsu accuracy 88.33% (23350/26436)
Sentence accuracy 46.35% (1563/3372)
</table>
<tableCaption confidence="0.997335">
Table 2: Results of parsing with the Triplet/Quadruplet
</tableCaption>
<bodyText confidence="0.951873529411765">
Model.
Bunsetsu accuracy The percentage of bunsetsus
whose modifiee is correctly identified. The de-
nominator includes all bunsetsus except for the
last bunsetsu of a sentence.
Sentence accuracy The percentage of sentences
whose dependencies are perfectly correct.
In-coverage sentences is the accuracy for the
sentences for which SLUNG could generate parse
trees. We give the accuracy for All sentences too,
by partially parsing sentences which SLUNG fail to
parse. The coverage of SLUNG is about 99%, thus
high accuracy is achieved even for All sentences.
Moreover, we conducted a series of experiments
in order to evaluate the contribution of each charac-
teristic in our parsing model. The parsing schemes
used are the four in Figure 3. Major differences
</bodyText>
<listItem confidence="0.6174815">
among them are (I) whether a grammar is used,
(II) whether modification candidates are restricted
to three, and (III) whether a previous pair model
with Formula (6) or the Triplet/Quadruplet Model
</listItem>
<table confidence="0.832802875">
with Formula (8),(9) was used.
W/O Grammar Model This model does not use
a grammar. Likelihood values for dependen-
\x0cG R F Bunsetsu accuracy
W/O Grammar P 86.70%(22594/26062)
W/O Restriction + P 87.37%(22770/26062)
Pair + + P 87.67%(22849/26062)
Triplet/Quadruplet + + T 88.55%(23078/26062)
</table>
<tableCaption confidence="0.997208">
Table 3: Bunsetsu accuracies for four models. Column
</tableCaption>
<bodyText confidence="0.988707076923077">
G indicates whether the grammar is used, R indi-
cates whether the modification candidates are restricted
to three, and F denotes the formula; P is the pair
formula (6), and T is the Triplet/Quadruplet formula
(8), (9).
cies are calculated for all bunsetsus that follow
a modifier bunsetsu. Formula (6) is used, and
as a distance metric i,j, the number of bun-
setsus between the modifier and the modifiee4
are combined with all features. In general lines,
this model corresponds to models such as (Fu-
jio and Matsumoto, 1998; Haruno et al., 1998;
Uchimoto et al., 1999).
</bodyText>
<sectionHeader confidence="0.762169" genericHeader="method">
W/O Restriction Model Modification candi-
</sectionHeader>
<bodyText confidence="0.972637677419355">
dates are restricted by SLUNG. The remaining
is the same as the W/O Grammar Model.
Pair Model Modification candidates are restricted
to three, in the way described in Section 3.1.
The remaining is the same as W/O Grammar
Model.
Triplet/Quadruplet Model This is the model
proposed in the paper. Modification candidates
are restricted to three, and Formula (8) or (9)
are used.
From the result shown in Table 3, we can say
our method contributes to the improvement of our
parser, because of the following reasons:
The Triplet/Quadruplet Model outperforms the
Pair Model by 0.9%. Both of them restricts
modification candidates to three, but the accu-
racy got higher when all candidates are consid-
ered simultaneously. It is because of the two
advantages described in Section 3.2.
The Pair Model outperforms the W/O Restric-
tion Model by 0.3%. Thus the restriction of
modification candidates does not reduce the ac-
curacy.
The W/O Restriction Model outperforms the
W/O Grammar Model by 0.7%. This means
that the use of a grammar as a preprocessor
works well to pick up possible modifiee.
We found that many structures similar to the
ones described in Section 3.2 appeared in the EDR
4Three values: 1, from 2 to 5, 6 or more are distin-
guished.
</bodyText>
<table confidence="0.60936725">
In-coverage
sentences
Bunsetsu accuracy 87.08% (8299/9530)
Sentence accuracy 44.70% (493/1103)
</table>
<tableCaption confidence="0.972986">
Table 4: Accuracy for Kyoto University Corpus
</tableCaption>
<bodyText confidence="0.986159125">
corpus. Our Triplet/Quadruplet model could treat
these structures precisely as we intended. This is the
main factor that contributed to the improvement of
the overall parsing accuracy.
Based on the above experiments, we can say that
our approach to use the grammar as a preprocessor
before the calculating of the probability is appropri-
ate for the improvement of parsing accuracy.
</bodyText>
<subsectionHeader confidence="0.990389">
4.3 Comparison to other models
</subsectionHeader>
<subsubsectionHeader confidence="0.738795">
4.3.1 Models using the EDR corpus
</subsubsectionHeader>
<bodyText confidence="0.998505875">
There are several works which use the EDR corpus
for evaluation. The decision tree model (Haruno et
al., 1998) achieves around 85%, the integrated model
of lexical/syntactic information (Shirai et al., 1998)
achieves around 86%, and the lexicalized statistical
model (Fujio and Matsumoto, 1999) achieves 86.8%
in bunsetsu accuracy. Our model outperforms all of
them by 2 or 3%.
</bodyText>
<subsubsectionHeader confidence="0.911809">
4.3.2 Models using the Kyoto corpus
</subsubsectionHeader>
<bodyText confidence="0.999628625">
Shirai et al. (1998) used the Kyoto University text
corpus (Kurohashi and Nagao, 1997) for evaluation
and achieved around 86%. Uchimoto et al. (2000)
also used the Kyoto corpus, and their accuracy was
87.9%. For comparison, we applied our method to
the same 1,246 sentences that Uchimoto et al. (2000)
used. The result is shown in Table 4.
Our result is worse than theirs. The reason is
thought to be as follows:
We use the EDR corpus for training. Although
we used around 24 times the amount of train-
ing data that Uchimoto et al. used, our training
data lead to errors in the analysis of the Kyoto
Corpus, because of differences in the annotation
schemes adopted.
Uchimoto et al. used the correct morphological
analyses, but we used JUMAN. Sometimes this
may cause errors.
The grammar SLUNG was designed for the
EDR corpus, and some types of structures in
the Kyoto Corpus are not allowed.
Clearly, our parser should be improved to overcome
these problems and compared with other works di-
rectly.
</bodyText>
<subsectionHeader confidence="0.991557">
4.4 Discussion and Future Work
</subsectionHeader>
<bodyText confidence="0.9990189375">
The following are some observations about the speed
of our parser. Existing statistical parsers are quite
efficient compared to grammar-based systems. Par-
ticularly, our system used an HPSG-based grammar,
\x0cwhose speed is said to be slow. However, recent ad-
vances in HPSG parsing (Torisawa et al., 2000) en-
abled us to obtain a unique parse tree with our sys-
tem in 0.5 sec. in average for sentences in the EDR
corpus.
Future work shall extend SLUNG so that semantic
representations are produced. Carroll et al. (1998)
discussed the precision of argument structures. We
believe that the focus of our study will shift from a
shallow level to such a deeper level for our final aim,
realization of intelligent natural language processing
systems.
</bodyText>
<sectionHeader confidence="0.998348" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999955294117647">
We presented a hybrid parsing scheme that uses a
hand-crafted grammar and a statistical technique.
As other hybrid parsing methods, the statistical
technique is used for picking up the most preferable
parse tree from the parse forest generated by the
grammar. The difference from other works is that
the precise contextual information needed to esti-
mate the likelihood of a parse tree is obtained from
alternative parse trees generated by the grammar,
and that such contextual information from alterna-
tive trees enables us to construct our new statisti-
cal model called the Triplet/Quadruplet model. We
have shown that these points contributed to substan-
tial improvement of parsing accuracy in Japanese de-
pendency analysis, through a series of experiments
using an HPSG-based Japanese grammar SLUNG
and the maximum entropy method.
</bodyText>
<sectionHeader confidence="0.992264" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99879112345679">
Steven Abney. 1996. Stochastic attribute-value
grammars. The Computation and Language E-
Print Archive, October.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent. J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):3971.
Andrew Borthwick. 1999. Choicemaker maximum
entropy estimator. ChoiceMaker Tech., Inc. Email
borthwic@cs.nyu.edu for information.
Ted Briscoe and John Carroll. 1993. Generalized
probabilistic LR parsing of natural language (cor-
pora) with unification-based grammars. Compu-
tational Linguistics, 19(1):2550.
John Carroll, Guido Minnen, and Ted Briscoe. 1998.
Can subcategorisation probabilities help a statis-
tical parser? In Proc. of the 6th ACL/SIGDAT
Workshop on Very Large Corpora, pages 118126.
EDR. 1996. EDR (Japan Electronic Dictionary Re-
search Institute, Ltd.) dictionary version 1.5 tech-
nical guide. Second edition is available via
http://www.iijnet.or.jp/edr/E_TG.html.
Masakazu Fujio and Yuji Matsumoto. 1998.
Japanese dependency structure analysis based on
lexicalized statistics. In Proc. of the 3rd Confer-
ence on Empirical Methods in Natural Language
Processing, pages 8896.
Masakazu Fujio and Yuuji Matsumoto. 1999. Sta-
tistical syntactic analysis based on co-occurrence
probability of words. In Proc. of 5th workshop
of Natural Language Processing, pages 7178. (in
Japanese).
Masahiko Haruno, Satoshi Shirai, and Yoshifumi
Ooyama. 1998. Using decision trees to construct
a practical parser. In Proc. COLINGACL 98,
pages 505511.
Kentaro Inui, Virach Sornlertlamvanich, Hozumi
Tanaka, and Takenobu Tokunaga. 1997. A new
probabilistic LR language model for statistical
parsing. Technical Report TR97-0005, Dept. of
Computer Science, Tokyo Institute of Technology.
Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mit-
suishi, and Junichi Tsujii. 1999. Statistical de-
pendency analysis with an HPSG-based Japanese
grammar. In Proc. 5th NLPRS, pages 138143.
Sadao Kurohashi and Makoto Nagao. 1997. Kyoto
University text corpus project. In Proc. of 3rd
Annual Meeting of Natural Language Processing,
pages 115118. (in Japanese).
Yutaka Mitsuishi, Kentaro Torisawa, and Junichi
Tsujii. 1998. HPSG-style underspecified Japanese
grammar with wide coverage. In Proc. COLING
ACL 98, pages 876880, August.
Adwait Ratnaparkhi. 1997. A linear observed time
statistical parser based on maximum entropy
models. In Proc. the Empirical Methods in Natu-
ral Language Processing Conference.
Yves Schabes. 1992. Stochastic lexicalized tree-
adjoining grammars. In Proc. 14th COLING,
pages 426432.
Satoshi Sekine. 2000. Japanese dependency analysis
using a deterministic finite state transducer. In
Proc. COLING 2000. (this proceedings).
Kiyoaki Shirai, Kentaro Inui, Takenobu Tokunaga,
and Hozumi Tanaka. 1998. A framework of inte-
grating syntactic and lexical statistics in statisti-
cal parsing. Journal of Natural Language Process-
ing, 5(3). (in Japanese).
Kentaro Torisawa, Kenji Nishida, Yusuke Miyao,
and Junichi Tsujii. 2000. An HPSG parser with
CFG filtering. Jounal of Natural Language Engi-
neering. (to appear).
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 1999. Japanese dependency structure anal-
ysis based on maximum entropy models. In Proc.
13th EACL, pages 196203.
Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine,
and Hitoshi Isahara. 2000. Dependency model us-
ing posterior context. In Proc. of Sixth Interna-
tional Workshop on Parsing Technologies.
\x0c&amp;apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.048446">
<title confidence="0.990121">b&amp;apos;A Hybrid Japanese Parser with Hand-crafted Grammar and Statistics</title>
<author confidence="0.616792">Hiroshi Kanayama</author>
<email confidence="0.413194">,</email>
<author confidence="0.922549">Yutaka Mitsuishi</author>
<affiliation confidence="0.910518">and Junichi Tsujii? Tokyo Research Laboratory, IBM Japan, Ltd.</affiliation>
<address confidence="0.99282">1623-14 Shimo-tsuruma, Yamato-shi, Kanagawa 242-8502, Japan</address>
<affiliation confidence="0.997948">Department of Information Science, Graduate School of Science, University of Tokyo</affiliation>
<address confidence="0.976604">7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan</address>
<title confidence="0.398113">Information and Human Behavior, PRESTO, Japan Science and Technology Corporation</title>
<author confidence="0.401736">Saitama Kawaguchi-shi</author>
<affiliation confidence="0.601421">CCL, UMIST, U.K.</affiliation>
<email confidence="0.965916">kanayama@is.s.u-tokyo.ac.jp</email>
<email confidence="0.965916">torisawa@is.s.u-tokyo.ac.jp</email>
<email confidence="0.965916">mitsuisi@is.s.u-tokyo.ac.jp</email>
<email confidence="0.965916">tsujii@is.s.u-tokyo.ac.jp</email>
<abstract confidence="0.998765636363636">This paper describes a hybrid parsing method for Japanese which uses both a hand-crafted grammar and a statistical technique. The key feature of our system is that in order to estimate likelihood for a parse tree, the system uses information taken from alternative partial parse trees generated by the grammar. This utilization of alternative trees enables us to construct a new statistical model called Triplet/Quadruplet Model. We show that this model can capture a certain tendency in Japanese syntactic structures and this point contributes to improvement of parsing accuracy on a shallow level. We report that, with an underspecified HPSG-based grammar and a maximum entropy estimation, our parser achieved high accuracy: 88.6% accuracy in dependency analysis of the EDR annotated corpus, and that it outperformed other purely statistical parsing methods on the same corpus. This result suggests that proper treatment of hand-crafted grammars can contribute to parsing accuracy on a shallow level.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Stochastic attribute-value grammars.</title>
<date>1996</date>
<booktitle>The Computation and Language EPrint Archive,</booktitle>
<contexts>
<context position="1838" citStr="Abney, 1996" startWordPosition="268" endWordPosition="269">an underspecified HPSG-based grammar and a maximum entropy estimation, our parser achieved high accuracy: 88.6% accuracy in dependency analysis of the EDR annotated corpus, and that it outperformed other purely statistical parsing methods on the same corpus. This result suggests that proper treatment of hand-crafted grammars can contribute to parsing accuracy on a shallow level. 1 Introduction There have been many attempts to combine handcrafted high-level grammars, such as FB-LTAG, HPSG and LFG, and statistical disambiguation techniques to obtain precise linguistic structures (Schabes, 1992; Abney, 1996; Carroll et al., 1998). One evident advantage of this approach over purely statistical parsing techniques is that grammars can provide precise semantic representations. However, considering that remarkable parsing accuracy in a shallow level has been achieved by purely statistical techniques (e.g. Ratnaparkhi (1997)), it may be thought more reasonable to use high-level grammars just for postprocessing which maps results of shallow syntactical analyses onto deep analyses. This work was conducted while the first author was a graduate student at Univ. of Tokyo. n h M NH H Figure 1: A tree M with</context>
<context position="9808" citStr="Abney, 1996" startWordPosition="1644" endWordPosition="1645">etsus which follow bunsetsu i as in (7). They report that this kind of contextual information improves accuracy. However, the model has to assume the independency of all the random variables, which may cause some errors. P(i j) def = Y i&lt;k&lt;j P(bynd |i, k, i,k) P (dpnd |i, j, i,j) Y k&amp;gt;j P(btwn |i, k, i,k)(7) The difference between our model and these previous models are discussed in Section 3. 2.2 Statistical Approaches with a grammar There have been many proposals for statistical frameworks particularly designed for parsers with hand-crafted grammars (Schabes, 1992; Briscoe and Carroll, 1993; Abney, 1996; Inui et al., 1997). The main issue in this type of research is how to assign likelihoods to a single linguistic structure generated by a grammar. Some of them (Briscoe and Carroll, 1993; Inui et al., 1997) treat information on contexts, but the contextual information is derived only from a structure to which the parser is trying to assign a likelihood value. Then, the major difference between their method and ours is that we consider the attributes of alternative linguistic structures generated by the grammar in order to determine the likelihood for linguistic structures. 2.3 SLUNG : Japanes</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Steven Abney. 1996. Stochastic attribute-value grammars. The Computation and Language EPrint Archive, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):3971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
</authors>
<title>Choicemaker maximum entropy estimator. ChoiceMaker Tech., Inc. Email borthwic@cs.nyu.edu for information.</title>
<date>1999</date>
<contexts>
<context position="18596" citStr="Borthwick, 1999" startWordPosition="3029" endWordPosition="3030">, 1996) for training and evaluation of parsing accuracy. The EDR Corpus is a Japanese treebank which consists of 208,157 sentences from newspapers and magazines. We used 192,778 sentences for training, 6,744 for pre-analysis (as reported in Section 3.1), and 3,372 for testing3 . With triplets constituted of a modifiee and two modification candidates extracted from the learning corpus the Triplet Model is constructed. With the quadruplets constituted of a modifiee and three candidates, the Quadruplet Model is constructed. These models are estimated by the ChoiceMaker Maximum Entropy Estimator (Borthwick, 1999). The features for the estimation are listed in Table 1. The values partially follow other researches e.g. Uchimoto et al. (1999), and JUMANs outputs are used for POS classification. Mainly the head of the bunsetsu (the rightmost morpheme in a bunsetsu except for whose major POS is peculiar, auxiliary verb, particle, suffix or copula) and type of the bunsetsu (the rightmost morpheme in a bunsetsu except for whose major POS is peculiar) are used as the attributes. We show the meaning of some features below. POS JUMANs minor POS (for both head and type). particle, adverb Frequent words: 26 parti</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Andrew Borthwick. 1999. Choicemaker maximum entropy estimator. ChoiceMaker Tech., Inc. Email borthwic@cs.nyu.edu for information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="9795" citStr="Briscoe and Carroll, 1993" startWordPosition="1640" endWordPosition="1643"> probabilities for all bunsetsus which follow bunsetsu i as in (7). They report that this kind of contextual information improves accuracy. However, the model has to assume the independency of all the random variables, which may cause some errors. P(i j) def = Y i&lt;k&lt;j P(bynd |i, k, i,k) P (dpnd |i, j, i,j) Y k&amp;gt;j P(btwn |i, k, i,k)(7) The difference between our model and these previous models are discussed in Section 3. 2.2 Statistical Approaches with a grammar There have been many proposals for statistical frameworks particularly designed for parsers with hand-crafted grammars (Schabes, 1992; Briscoe and Carroll, 1993; Abney, 1996; Inui et al., 1997). The main issue in this type of research is how to assign likelihoods to a single linguistic structure generated by a grammar. Some of them (Briscoe and Carroll, 1993; Inui et al., 1997) treat information on contexts, but the contextual information is derived only from a structure to which the parser is trying to assign a likelihood value. Then, the major difference between their method and ours is that we consider the attributes of alternative linguistic structures generated by the grammar in order to determine the likelihood for linguistic structures. 2.3 SL</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Ted Briscoe and John Carroll. 1993. Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1):2550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Ted Briscoe</author>
</authors>
<title>Can subcategorisation probabilities help a statistical parser?</title>
<date>1998</date>
<booktitle>In Proc. of the 6th ACL/SIGDAT Workshop on Very Large Corpora,</booktitle>
<pages>118126</pages>
<contexts>
<context position="1861" citStr="Carroll et al., 1998" startWordPosition="270" endWordPosition="273">fied HPSG-based grammar and a maximum entropy estimation, our parser achieved high accuracy: 88.6% accuracy in dependency analysis of the EDR annotated corpus, and that it outperformed other purely statistical parsing methods on the same corpus. This result suggests that proper treatment of hand-crafted grammars can contribute to parsing accuracy on a shallow level. 1 Introduction There have been many attempts to combine handcrafted high-level grammars, such as FB-LTAG, HPSG and LFG, and statistical disambiguation techniques to obtain precise linguistic structures (Schabes, 1992; Abney, 1996; Carroll et al., 1998). One evident advantage of this approach over purely statistical parsing techniques is that grammars can provide precise semantic representations. However, considering that remarkable parsing accuracy in a shallow level has been achieved by purely statistical techniques (e.g. Ratnaparkhi (1997)), it may be thought more reasonable to use high-level grammars just for postprocessing which maps results of shallow syntactical analyses onto deep analyses. This work was conducted while the first author was a graduate student at Univ. of Tokyo. n h M NH H Figure 1: A tree M with a non-head daughter NH</context>
<context position="26896" citStr="Carroll et al. (1998)" startWordPosition="4444" endWordPosition="4447">roved to overcome these problems and compared with other works directly. 4.4 Discussion and Future Work The following are some observations about the speed of our parser. Existing statistical parsers are quite efficient compared to grammar-based systems. Particularly, our system used an HPSG-based grammar, \x0cwhose speed is said to be slow. However, recent advances in HPSG parsing (Torisawa et al., 2000) enabled us to obtain a unique parse tree with our system in 0.5 sec. in average for sentences in the EDR corpus. Future work shall extend SLUNG so that semantic representations are produced. Carroll et al. (1998) discussed the precision of argument structures. We believe that the focus of our study will shift from a shallow level to such a deeper level for our final aim, realization of intelligent natural language processing systems. 5 Conclusion We presented a hybrid parsing scheme that uses a hand-crafted grammar and a statistical technique. As other hybrid parsing methods, the statistical technique is used for picking up the most preferable parse tree from the parse forest generated by the grammar. The difference from other works is that the precise contextual information needed to estimate the lik</context>
</contexts>
<marker>Carroll, Minnen, Briscoe, 1998</marker>
<rawString>John Carroll, Guido Minnen, and Ted Briscoe. 1998. Can subcategorisation probabilities help a statistical parser? In Proc. of the 6th ACL/SIGDAT Workshop on Very Large Corpora, pages 118126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>EDR</author>
</authors>
<title>EDR (Japan Electronic Dictionary Research Institute, Ltd.) dictionary version 1.5 technical guide. Second edition is available via http://www.iijnet.or.jp/edr/E_TG.html.</title>
<date>1996</date>
<contexts>
<context position="17987" citStr="EDR, 1996" startWordPosition="2938" endWordPosition="2939">i, 1) In text corpora, P(T|Taro-no, yuujin-no, 1) tends to be high, and consequently, P(T|Taro-no, musume, 2) is very small. These values will make the correct prediction for (11b) as yuujin-no will be favored over musume. However, for (11a), these models are likely to incorrectly favor kawaii over musume. This is \x0cbecause P(T|Taro-no, musume, 2), being very small, is likely to be smaller than P(T|Taro-no, kawaii, 1). 4 Experiments and Discussion This section reports a series of parsing experiments with our model, and gives some discussion. 4.1 Environments We used the EDR Japanese Corpus (EDR, 1996) for training and evaluation of parsing accuracy. The EDR Corpus is a Japanese treebank which consists of 208,157 sentences from newspapers and magazines. We used 192,778 sentences for training, 6,744 for pre-analysis (as reported in Section 3.1), and 3,372 for testing3 . With triplets constituted of a modifiee and two modification candidates extracted from the learning corpus the Triplet Model is constructed. With the quadruplets constituted of a modifiee and three candidates, the Quadruplet Model is constructed. These models are estimated by the ChoiceMaker Maximum Entropy Estimator (Borthwi</context>
</contexts>
<marker>EDR, 1996</marker>
<rawString>EDR. 1996. EDR (Japan Electronic Dictionary Research Institute, Ltd.) dictionary version 1.5 technical guide. Second edition is available via http://www.iijnet.or.jp/edr/E_TG.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masakazu Fujio</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency structure analysis based on lexicalized statistics.</title>
<date>1998</date>
<booktitle>In Proc. of the 3rd Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>8896</pages>
<contexts>
<context position="8291" citStr="Fujio and Matsumoto, 1998" startWordPosition="1383" endWordPosition="1386">sed. We evaluate the accuracy of bunsetsu-dependencies as they do, thus here we introduce them for comparison. All models introduced below are based on the likelihood value of the dependency between two bunsetsus. But they differ from each other in the attributes or outputs which are considered when a likelihood value is calculated. There are some models which calculate the likelihood values of a dependency between bunsetsu i and j as in (6), such as a decision tree model (Haruno et al., 1998), a maximum entropy model (Uchimoto et al., 1999), a model based on distance and lexical information (Fujio and Matsumoto, 1998). Attributes i and j consist of a part-of-speech (POS), a lexical item, presence of a comma, and so on. And i,j \x0cis the number of intervening bunsetsus between i and j. P(i j) def = P(T |i, j, i,j) (6) However, these models fail to reflect contextual information because attributes of the surrounding bunsetsus are not considered. Uchimoto et al. (2000) proposed a model using posterior context. The model utilizes not only attributes about bunsetsus i, j but also attributes about all bunsetsus (including j) which follow bunsetsu i. That is, instead of learning two output values T(true) or F(fa</context>
<context position="23028" citStr="Fujio and Matsumoto, 1998" startWordPosition="3805" endWordPosition="3809">26062) Triplet/Quadruplet + + T 88.55%(23078/26062) Table 3: Bunsetsu accuracies for four models. Column G indicates whether the grammar is used, R indicates whether the modification candidates are restricted to three, and F denotes the formula; P is the pair formula (6), and T is the Triplet/Quadruplet formula (8), (9). cies are calculated for all bunsetsus that follow a modifier bunsetsu. Formula (6) is used, and as a distance metric i,j, the number of bunsetsus between the modifier and the modifiee4 are combined with all features. In general lines, this model corresponds to models such as (Fujio and Matsumoto, 1998; Haruno et al., 1998; Uchimoto et al., 1999). W/O Restriction Model Modification candidates are restricted by SLUNG. The remaining is the same as the W/O Grammar Model. Pair Model Modification candidates are restricted to three, in the way described in Section 3.1. The remaining is the same as W/O Grammar Model. Triplet/Quadruplet Model This is the model proposed in the paper. Modification candidates are restricted to three, and Formula (8) or (9) are used. From the result shown in Table 3, we can say our method contributes to the improvement of our parser, because of the following reasons: T</context>
</contexts>
<marker>Fujio, Matsumoto, 1998</marker>
<rawString>Masakazu Fujio and Yuji Matsumoto. 1998. Japanese dependency structure analysis based on lexicalized statistics. In Proc. of the 3rd Conference on Empirical Methods in Natural Language Processing, pages 8896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masakazu Fujio</author>
<author>Yuuji Matsumoto</author>
</authors>
<title>Statistical syntactic analysis based on co-occurrence probability of words.</title>
<date>1999</date>
<booktitle>In Proc. of 5th workshop of Natural Language Processing,</booktitle>
<pages>7178</pages>
<note>(in Japanese).</note>
<contexts>
<context position="25231" citStr="Fujio and Matsumoto, 1999" startWordPosition="4160" endWordPosition="4163">r that contributed to the improvement of the overall parsing accuracy. Based on the above experiments, we can say that our approach to use the grammar as a preprocessor before the calculating of the probability is appropriate for the improvement of parsing accuracy. 4.3 Comparison to other models 4.3.1 Models using the EDR corpus There are several works which use the EDR corpus for evaluation. The decision tree model (Haruno et al., 1998) achieves around 85%, the integrated model of lexical/syntactic information (Shirai et al., 1998) achieves around 86%, and the lexicalized statistical model (Fujio and Matsumoto, 1999) achieves 86.8% in bunsetsu accuracy. Our model outperforms all of them by 2 or 3%. 4.3.2 Models using the Kyoto corpus Shirai et al. (1998) used the Kyoto University text corpus (Kurohashi and Nagao, 1997) for evaluation and achieved around 86%. Uchimoto et al. (2000) also used the Kyoto corpus, and their accuracy was 87.9%. For comparison, we applied our method to the same 1,246 sentences that Uchimoto et al. (2000) used. The result is shown in Table 4. Our result is worse than theirs. The reason is thought to be as follows: We use the EDR corpus for training. Although we used around 24 time</context>
</contexts>
<marker>Fujio, Matsumoto, 1999</marker>
<rawString>Masakazu Fujio and Yuuji Matsumoto. 1999. Statistical syntactic analysis based on co-occurrence probability of words. In Proc. of 5th workshop of Natural Language Processing, pages 7178. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masahiko Haruno</author>
<author>Satoshi Shirai</author>
<author>Yoshifumi Ooyama</author>
</authors>
<title>Using decision trees to construct a practical parser.</title>
<date>1998</date>
<booktitle>In Proc. COLINGACL 98,</booktitle>
<pages>505511</pages>
<contexts>
<context position="8163" citStr="Haruno et al., 1998" startWordPosition="1362" endWordPosition="1365">se Several statistical models for Japanese dependency analysis which do not utilize a hand-crafted grammar have been proposed. We evaluate the accuracy of bunsetsu-dependencies as they do, thus here we introduce them for comparison. All models introduced below are based on the likelihood value of the dependency between two bunsetsus. But they differ from each other in the attributes or outputs which are considered when a likelihood value is calculated. There are some models which calculate the likelihood values of a dependency between bunsetsu i and j as in (6), such as a decision tree model (Haruno et al., 1998), a maximum entropy model (Uchimoto et al., 1999), a model based on distance and lexical information (Fujio and Matsumoto, 1998). Attributes i and j consist of a part-of-speech (POS), a lexical item, presence of a comma, and so on. And i,j \x0cis the number of intervening bunsetsus between i and j. P(i j) def = P(T |i, j, i,j) (6) However, these models fail to reflect contextual information because attributes of the surrounding bunsetsus are not considered. Uchimoto et al. (2000) proposed a model using posterior context. The model utilizes not only attributes about bunsetsus i, j but also attr</context>
<context position="23049" citStr="Haruno et al., 1998" startWordPosition="3810" endWordPosition="3813"> + T 88.55%(23078/26062) Table 3: Bunsetsu accuracies for four models. Column G indicates whether the grammar is used, R indicates whether the modification candidates are restricted to three, and F denotes the formula; P is the pair formula (6), and T is the Triplet/Quadruplet formula (8), (9). cies are calculated for all bunsetsus that follow a modifier bunsetsu. Formula (6) is used, and as a distance metric i,j, the number of bunsetsus between the modifier and the modifiee4 are combined with all features. In general lines, this model corresponds to models such as (Fujio and Matsumoto, 1998; Haruno et al., 1998; Uchimoto et al., 1999). W/O Restriction Model Modification candidates are restricted by SLUNG. The remaining is the same as the W/O Grammar Model. Pair Model Modification candidates are restricted to three, in the way described in Section 3.1. The remaining is the same as W/O Grammar Model. Triplet/Quadruplet Model This is the model proposed in the paper. Modification candidates are restricted to three, and Formula (8) or (9) are used. From the result shown in Table 3, we can say our method contributes to the improvement of our parser, because of the following reasons: The Triplet/Quadruplet</context>
<context position="25047" citStr="Haruno et al., 1998" startWordPosition="4135" endWordPosition="4138">44.70% (493/1103) Table 4: Accuracy for Kyoto University Corpus corpus. Our Triplet/Quadruplet model could treat these structures precisely as we intended. This is the main factor that contributed to the improvement of the overall parsing accuracy. Based on the above experiments, we can say that our approach to use the grammar as a preprocessor before the calculating of the probability is appropriate for the improvement of parsing accuracy. 4.3 Comparison to other models 4.3.1 Models using the EDR corpus There are several works which use the EDR corpus for evaluation. The decision tree model (Haruno et al., 1998) achieves around 85%, the integrated model of lexical/syntactic information (Shirai et al., 1998) achieves around 86%, and the lexicalized statistical model (Fujio and Matsumoto, 1999) achieves 86.8% in bunsetsu accuracy. Our model outperforms all of them by 2 or 3%. 4.3.2 Models using the Kyoto corpus Shirai et al. (1998) used the Kyoto University text corpus (Kurohashi and Nagao, 1997) for evaluation and achieved around 86%. Uchimoto et al. (2000) also used the Kyoto corpus, and their accuracy was 87.9%. For comparison, we applied our method to the same 1,246 sentences that Uchimoto et al. (</context>
</contexts>
<marker>Haruno, Shirai, Ooyama, 1998</marker>
<rawString>Masahiko Haruno, Satoshi Shirai, and Yoshifumi Ooyama. 1998. Using decision trees to construct a practical parser. In Proc. COLINGACL 98, pages 505511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Inui</author>
<author>Virach Sornlertlamvanich</author>
<author>Hozumi Tanaka</author>
<author>Takenobu Tokunaga</author>
</authors>
<title>A new probabilistic LR language model for statistical parsing.</title>
<date>1997</date>
<tech>Technical Report TR97-0005,</tech>
<institution>Dept. of Computer Science, Tokyo Institute of Technology.</institution>
<contexts>
<context position="9828" citStr="Inui et al., 1997" startWordPosition="1646" endWordPosition="1649">ollow bunsetsu i as in (7). They report that this kind of contextual information improves accuracy. However, the model has to assume the independency of all the random variables, which may cause some errors. P(i j) def = Y i&lt;k&lt;j P(bynd |i, k, i,k) P (dpnd |i, j, i,j) Y k&amp;gt;j P(btwn |i, k, i,k)(7) The difference between our model and these previous models are discussed in Section 3. 2.2 Statistical Approaches with a grammar There have been many proposals for statistical frameworks particularly designed for parsers with hand-crafted grammars (Schabes, 1992; Briscoe and Carroll, 1993; Abney, 1996; Inui et al., 1997). The main issue in this type of research is how to assign likelihoods to a single linguistic structure generated by a grammar. Some of them (Briscoe and Carroll, 1993; Inui et al., 1997) treat information on contexts, but the contextual information is derived only from a structure to which the parser is trying to assign a likelihood value. Then, the major difference between their method and ours is that we consider the attributes of alternative linguistic structures generated by the grammar in order to determine the likelihood for linguistic structures. 2.3 SLUNG : Japanese Grammar The Japane</context>
</contexts>
<marker>Inui, Sornlertlamvanich, Tanaka, Tokunaga, 1997</marker>
<rawString>Kentaro Inui, Virach Sornlertlamvanich, Hozumi Tanaka, and Takenobu Tokunaga. 1997. A new probabilistic LR language model for statistical parsing. Technical Report TR97-0005, Dept. of Computer Science, Tokyo Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama</author>
<author>Kentaro Torisawa</author>
<author>Yutaka Mitsuishi</author>
<author>Junichi Tsujii</author>
</authors>
<title>Statistical dependency analysis with an HPSG-based Japanese grammar.</title>
<date>1999</date>
<booktitle>In Proc. 5th NLPRS,</booktitle>
<pages>138143</pages>
<contexts>
<context position="11908" citStr="Kanayama et al. (1999)" startWordPosition="1979" endWordPosition="1982"> as follows: At the beginning, dependency structures are obtained from trees generated by SLUNG. For each bunsetsu, modification candidates are enumerated, and if there are four or more candidates, they are restricted to three. The heuristic used in this process is described in Section 3.1. Then, with the Triplet/Quadruplet Model and maximum entropy estimation, probabilities of the dependencies are calculated. Section 3.2 discusses the characteristics and advantages of the model. Finally, the most preferable trees for the whole sentence are selected. 3.1 Restriction of Modification Candidates Kanayama et al. (1999) report that when modification candidates are enumerated according to SLUNG, 98.6% of the correct modifiees are in one of the following three positions among the candidates: the nearest one from the modifier, the second nearest one, and the farthest one. As a consequence, we can simplify the problem by considering only these three candidates and discarding the other candidates, with only 1.4% potential errors. We therefore assume that the number of modification candidates is always three or less. This idea is similar to that of Sekine (2000)s study, which restricts the candidates to five, but </context>
</contexts>
<marker>Kanayama, Torisawa, Mitsuishi, Tsujii, 1999</marker>
<rawString>Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsuishi, and Junichi Tsujii. 1999. Statistical dependency analysis with an HPSG-based Japanese grammar. In Proc. 5th NLPRS, pages 138143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Kyoto University text corpus project.</title>
<date>1997</date>
<booktitle>In Proc. of 3rd Annual Meeting of Natural Language Processing,</booktitle>
<pages>115118</pages>
<note>(in Japanese).</note>
<contexts>
<context position="25437" citStr="Kurohashi and Nagao, 1997" startWordPosition="4195" endWordPosition="4198">lity is appropriate for the improvement of parsing accuracy. 4.3 Comparison to other models 4.3.1 Models using the EDR corpus There are several works which use the EDR corpus for evaluation. The decision tree model (Haruno et al., 1998) achieves around 85%, the integrated model of lexical/syntactic information (Shirai et al., 1998) achieves around 86%, and the lexicalized statistical model (Fujio and Matsumoto, 1999) achieves 86.8% in bunsetsu accuracy. Our model outperforms all of them by 2 or 3%. 4.3.2 Models using the Kyoto corpus Shirai et al. (1998) used the Kyoto University text corpus (Kurohashi and Nagao, 1997) for evaluation and achieved around 86%. Uchimoto et al. (2000) also used the Kyoto corpus, and their accuracy was 87.9%. For comparison, we applied our method to the same 1,246 sentences that Uchimoto et al. (2000) used. The result is shown in Table 4. Our result is worse than theirs. The reason is thought to be as follows: We use the EDR corpus for training. Although we used around 24 times the amount of training data that Uchimoto et al. used, our training data lead to errors in the analysis of the Kyoto Corpus, because of differences in the annotation schemes adopted. Uchimoto et al. used </context>
</contexts>
<marker>Kurohashi, Nagao, 1997</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1997. Kyoto University text corpus project. In Proc. of 3rd Annual Meeting of Natural Language Processing, pages 115118. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yutaka Mitsuishi</author>
<author>Kentaro Torisawa</author>
<author>Junichi Tsujii</author>
</authors>
<title>HPSG-style underspecified Japanese grammar with wide coverage.</title>
<date>1998</date>
<booktitle>In Proc. COLING ACL 98,</booktitle>
<pages>876880</pages>
<contexts>
<context position="6286" citStr="Mitsuishi et al., 1998" startWordPosition="1057" endWordPosition="1061">P(i |n, h1 , h2 , hl ) (i = 1, 2, l)(5) When there are only two candidates, equation (4) is used; otherwise, equation (5) is used. Our statistical model is called the Triplet/Quadruplet Model, which was named after the number of constituents in the conditional parts of the equations. We report that our parsing framework achieved high accuracy (88.6%) in dependency analysis of Japanese with a combination of an underspecified l r M L R l0 r0 ? Figure 3: Transformation from a tree to a dependency. l0 and r0 denote the bunsetsus l and r belong to, respectively. HPSG-based Japanese grammar, SLUNG (Mitsuishi et al., 1998) and the maximum entropy method (Berger et al., 1996). Moreover, the resulting parse trees generated by our hybrid parser are legitimate trees in terms of given hand-crafted grammars, and we are expecting that we can enjoy advantages provided by high-level grammar formalisms, such as construction of semantic structures. In the above explanation, we used the notion of lexical heads for the estimation of probabilities of trees for the sake of simplicity. But, in the present implementation, we use bunsetsus instead of lexical heads, and a relation on a tree is converted to a bunsetsu-dependency a</context>
<context position="10487" citStr="Mitsuishi et al., 1998" startWordPosition="1755" endWordPosition="1758">earch is how to assign likelihoods to a single linguistic structure generated by a grammar. Some of them (Briscoe and Carroll, 1993; Inui et al., 1997) treat information on contexts, but the contextual information is derived only from a structure to which the parser is trying to assign a likelihood value. Then, the major difference between their method and ours is that we consider the attributes of alternative linguistic structures generated by the grammar in order to determine the likelihood for linguistic structures. 2.3 SLUNG : Japanese Grammar The Japanese grammar which we adopted, SLUNG (Mitsuishi et al., 1998), is an HPSG-based underspecified grammar. It consists of 8 rule schemata, 48 lexical templates for POSs and 105 lexical entries for functional words. As can be seen from these figures, the grammar does not contain detailed lexical information that needs intensive labor for development. However, it is precise in the sense that it achieves 83.7% dependency accuracy with a simple heuristics2 for the EDR annotated corpus, and it can produce at least one parse tree for 98.4% sentences in the EDR annotated corpus. We use the grammar for generating parse tree forests, and our Triplet/Quadruplet Mode</context>
<context position="13087" citStr="Mitsuishi et al., 1998" startWordPosition="2188" endWordPosition="2191">which restricts the candidates to five, but in his case, without a grammar. 3.2 The Triplet/Quadruplet Model The Triplet/Quadruplet Model calculates the likelihood of the dependency between bunsetsu i and bunsetsu cn; P(i cn) with the formulas (8) and (9), where cn denotes the nth candidate among bunsetsu is candidates; i denotes some attributes of i; and cn denotes attributes of cn (including attributes between i and cn). P(i cn) def = P(n |i, c1 , c2 ) (n = 1, 2) (8) P(i cn) def = P(n |i, c1 , c2 , cl ) (n = 1, 2, l)(9) 2This heuristics is a Japanese version of a left-association rule: see (Mitsuishi et al., 1998) for detail. \x0cAs (8) and (9) suggest, the model considers attributes of the modifier bunsetsu and attributes of all modification candidates simultaneously in the conditional parts of the probabilities. Moreover, what is calculated is not the probability of whether the dependency is correct (T, see Formula(6)), but the probability of which of the given candidates is chosen as the modifiee (n =1, 2, or l). These characteristics imply the following two advantages. Advantage 1 A new distance metric. The correct modifiee can be chosen by considering relative position among grammatically licensed</context>
</contexts>
<marker>Mitsuishi, Torisawa, Tsujii, 1998</marker>
<rawString>Yutaka Mitsuishi, Kentaro Torisawa, and Junichi Tsujii. 1998. HPSG-style underspecified Japanese grammar with wide coverage. In Proc. COLING ACL 98, pages 876880, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proc. the Empirical Methods in Natural Language Processing Conference.</booktitle>
<contexts>
<context position="2156" citStr="Ratnaparkhi (1997)" startWordPosition="312" endWordPosition="313">rafted grammars can contribute to parsing accuracy on a shallow level. 1 Introduction There have been many attempts to combine handcrafted high-level grammars, such as FB-LTAG, HPSG and LFG, and statistical disambiguation techniques to obtain precise linguistic structures (Schabes, 1992; Abney, 1996; Carroll et al., 1998). One evident advantage of this approach over purely statistical parsing techniques is that grammars can provide precise semantic representations. However, considering that remarkable parsing accuracy in a shallow level has been achieved by purely statistical techniques (e.g. Ratnaparkhi (1997)), it may be thought more reasonable to use high-level grammars just for postprocessing which maps results of shallow syntactical analyses onto deep analyses. This work was conducted while the first author was a graduate student at Univ. of Tokyo. n h M NH H Figure 1: A tree M with a non-head daughter NH and a head daughter H. In this work we propose that hand-crafted highlevel grammars can be useful in shallow-level analyses and statistical models. In our framework, grammars are used to obtain precise features for probability estimation, which are difficult to obtain without a grammar, and we</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proc. the Empirical Methods in Natural Language Processing Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Stochastic lexicalized treeadjoining grammars.</title>
<date>1992</date>
<booktitle>In Proc. 14th COLING,</booktitle>
<pages>426432</pages>
<contexts>
<context position="1825" citStr="Schabes, 1992" startWordPosition="266" endWordPosition="267">ort that, with an underspecified HPSG-based grammar and a maximum entropy estimation, our parser achieved high accuracy: 88.6% accuracy in dependency analysis of the EDR annotated corpus, and that it outperformed other purely statistical parsing methods on the same corpus. This result suggests that proper treatment of hand-crafted grammars can contribute to parsing accuracy on a shallow level. 1 Introduction There have been many attempts to combine handcrafted high-level grammars, such as FB-LTAG, HPSG and LFG, and statistical disambiguation techniques to obtain precise linguistic structures (Schabes, 1992; Abney, 1996; Carroll et al., 1998). One evident advantage of this approach over purely statistical parsing techniques is that grammars can provide precise semantic representations. However, considering that remarkable parsing accuracy in a shallow level has been achieved by purely statistical techniques (e.g. Ratnaparkhi (1997)), it may be thought more reasonable to use high-level grammars just for postprocessing which maps results of shallow syntactical analyses onto deep analyses. This work was conducted while the first author was a graduate student at Univ. of Tokyo. n h M NH H Figure 1: </context>
<context position="9768" citStr="Schabes, 1992" startWordPosition="1638" endWordPosition="1639"> by multiplying probabilities for all bunsetsus which follow bunsetsu i as in (7). They report that this kind of contextual information improves accuracy. However, the model has to assume the independency of all the random variables, which may cause some errors. P(i j) def = Y i&lt;k&lt;j P(bynd |i, k, i,k) P (dpnd |i, j, i,j) Y k&amp;gt;j P(btwn |i, k, i,k)(7) The difference between our model and these previous models are discussed in Section 3. 2.2 Statistical Approaches with a grammar There have been many proposals for statistical frameworks particularly designed for parsers with hand-crafted grammars (Schabes, 1992; Briscoe and Carroll, 1993; Abney, 1996; Inui et al., 1997). The main issue in this type of research is how to assign likelihoods to a single linguistic structure generated by a grammar. Some of them (Briscoe and Carroll, 1993; Inui et al., 1997) treat information on contexts, but the contextual information is derived only from a structure to which the parser is trying to assign a likelihood value. Then, the major difference between their method and ours is that we consider the attributes of alternative linguistic structures generated by the grammar in order to determine the likelihood for li</context>
</contexts>
<marker>Schabes, 1992</marker>
<rawString>Yves Schabes. 1992. Stochastic lexicalized treeadjoining grammars. In Proc. 14th COLING, pages 426432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>Japanese dependency analysis using a deterministic finite state transducer.</title>
<date>2000</date>
<booktitle>In Proc. COLING</booktitle>
<note>(this proceedings).</note>
<contexts>
<context position="12455" citStr="Sekine (2000)" startWordPosition="2071" endWordPosition="2072"> 3.1 Restriction of Modification Candidates Kanayama et al. (1999) report that when modification candidates are enumerated according to SLUNG, 98.6% of the correct modifiees are in one of the following three positions among the candidates: the nearest one from the modifier, the second nearest one, and the farthest one. As a consequence, we can simplify the problem by considering only these three candidates and discarding the other candidates, with only 1.4% potential errors. We therefore assume that the number of modification candidates is always three or less. This idea is similar to that of Sekine (2000)s study, which restricts the candidates to five, but in his case, without a grammar. 3.2 The Triplet/Quadruplet Model The Triplet/Quadruplet Model calculates the likelihood of the dependency between bunsetsu i and bunsetsu cn; P(i cn) with the formulas (8) and (9), where cn denotes the nth candidate among bunsetsu is candidates; i denotes some attributes of i; and cn denotes attributes of cn (including attributes between i and cn). P(i cn) def = P(n |i, c1 , c2 ) (n = 1, 2) (8) P(i cn) def = P(n |i, c1 , c2 , cl ) (n = 1, 2, l)(9) 2This heuristics is a Japanese version of a left-association ru</context>
</contexts>
<marker>Sekine, 2000</marker>
<rawString>Satoshi Sekine. 2000. Japanese dependency analysis using a deterministic finite state transducer. In Proc. COLING 2000. (this proceedings).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoaki Shirai</author>
<author>Kentaro Inui</author>
<author>Takenobu Tokunaga</author>
<author>Hozumi Tanaka</author>
</authors>
<title>A framework of integrating syntactic and lexical statistics in statistical parsing.</title>
<date>1998</date>
<journal>Journal of Natural Language Processing,</journal>
<volume>5</volume>
<issue>3</issue>
<note>(in Japanese).</note>
<contexts>
<context position="25144" citStr="Shirai et al., 1998" startWordPosition="4148" endWordPosition="4151">del could treat these structures precisely as we intended. This is the main factor that contributed to the improvement of the overall parsing accuracy. Based on the above experiments, we can say that our approach to use the grammar as a preprocessor before the calculating of the probability is appropriate for the improvement of parsing accuracy. 4.3 Comparison to other models 4.3.1 Models using the EDR corpus There are several works which use the EDR corpus for evaluation. The decision tree model (Haruno et al., 1998) achieves around 85%, the integrated model of lexical/syntactic information (Shirai et al., 1998) achieves around 86%, and the lexicalized statistical model (Fujio and Matsumoto, 1999) achieves 86.8% in bunsetsu accuracy. Our model outperforms all of them by 2 or 3%. 4.3.2 Models using the Kyoto corpus Shirai et al. (1998) used the Kyoto University text corpus (Kurohashi and Nagao, 1997) for evaluation and achieved around 86%. Uchimoto et al. (2000) also used the Kyoto corpus, and their accuracy was 87.9%. For comparison, we applied our method to the same 1,246 sentences that Uchimoto et al. (2000) used. The result is shown in Table 4. Our result is worse than theirs. The reason is though</context>
</contexts>
<marker>Shirai, Inui, Tokunaga, Tanaka, 1998</marker>
<rawString>Kiyoaki Shirai, Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka. 1998. A framework of integrating syntactic and lexical statistics in statistical parsing. Journal of Natural Language Processing, 5(3). (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Torisawa</author>
<author>Kenji Nishida</author>
<author>Yusuke Miyao</author>
<author>Junichi Tsujii</author>
</authors>
<title>An HPSG parser with CFG filtering.</title>
<date>2000</date>
<journal>Jounal of Natural Language Engineering.</journal>
<note>(to appear).</note>
<contexts>
<context position="26683" citStr="Torisawa et al., 2000" startWordPosition="4405" endWordPosition="4408">l analyses, but we used JUMAN. Sometimes this may cause errors. The grammar SLUNG was designed for the EDR corpus, and some types of structures in the Kyoto Corpus are not allowed. Clearly, our parser should be improved to overcome these problems and compared with other works directly. 4.4 Discussion and Future Work The following are some observations about the speed of our parser. Existing statistical parsers are quite efficient compared to grammar-based systems. Particularly, our system used an HPSG-based grammar, \x0cwhose speed is said to be slow. However, recent advances in HPSG parsing (Torisawa et al., 2000) enabled us to obtain a unique parse tree with our system in 0.5 sec. in average for sentences in the EDR corpus. Future work shall extend SLUNG so that semantic representations are produced. Carroll et al. (1998) discussed the precision of argument structures. We believe that the focus of our study will shift from a shallow level to such a deeper level for our final aim, realization of intelligent natural language processing systems. 5 Conclusion We presented a hybrid parsing scheme that uses a hand-crafted grammar and a statistical technique. As other hybrid parsing methods, the statistical </context>
</contexts>
<marker>Torisawa, Nishida, Miyao, Tsujii, 2000</marker>
<rawString>Kentaro Torisawa, Kenji Nishida, Yusuke Miyao, and Junichi Tsujii. 2000. An HPSG parser with CFG filtering. Jounal of Natural Language Engineering. (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Japanese dependency structure analysis based on maximum entropy models.</title>
<date>1999</date>
<booktitle>In Proc. 13th EACL,</booktitle>
<pages>196203</pages>
<contexts>
<context position="8212" citStr="Uchimoto et al., 1999" startWordPosition="1370" endWordPosition="1373">ndency analysis which do not utilize a hand-crafted grammar have been proposed. We evaluate the accuracy of bunsetsu-dependencies as they do, thus here we introduce them for comparison. All models introduced below are based on the likelihood value of the dependency between two bunsetsus. But they differ from each other in the attributes or outputs which are considered when a likelihood value is calculated. There are some models which calculate the likelihood values of a dependency between bunsetsu i and j as in (6), such as a decision tree model (Haruno et al., 1998), a maximum entropy model (Uchimoto et al., 1999), a model based on distance and lexical information (Fujio and Matsumoto, 1998). Attributes i and j consist of a part-of-speech (POS), a lexical item, presence of a comma, and so on. And i,j \x0cis the number of intervening bunsetsus between i and j. P(i j) def = P(T |i, j, i,j) (6) However, these models fail to reflect contextual information because attributes of the surrounding bunsetsus are not considered. Uchimoto et al. (2000) proposed a model using posterior context. The model utilizes not only attributes about bunsetsus i, j but also attributes about all bunsetsus (including j) which fo</context>
<context position="18725" citStr="Uchimoto et al. (1999)" startWordPosition="3049" endWordPosition="3052">ntences from newspapers and magazines. We used 192,778 sentences for training, 6,744 for pre-analysis (as reported in Section 3.1), and 3,372 for testing3 . With triplets constituted of a modifiee and two modification candidates extracted from the learning corpus the Triplet Model is constructed. With the quadruplets constituted of a modifiee and three candidates, the Quadruplet Model is constructed. These models are estimated by the ChoiceMaker Maximum Entropy Estimator (Borthwick, 1999). The features for the estimation are listed in Table 1. The values partially follow other researches e.g. Uchimoto et al. (1999), and JUMANs outputs are used for POS classification. Mainly the head of the bunsetsu (the rightmost morpheme in a bunsetsu except for whose major POS is peculiar, auxiliary verb, particle, suffix or copula) and type of the bunsetsu (the rightmost morpheme in a bunsetsu except for whose major POS is peculiar) are used as the attributes. We show the meaning of some features below. POS JUMANs minor POS (for both head and type). particle, adverb Frequent words: 26 particles and 69 adverbs. head lex 294 lexical forms regardless of their POS. type lex 70 suffixes or auxiliary verbs. inflection 6 ty</context>
<context position="23073" citStr="Uchimoto et al., 1999" startWordPosition="3814" endWordPosition="3817">62) Table 3: Bunsetsu accuracies for four models. Column G indicates whether the grammar is used, R indicates whether the modification candidates are restricted to three, and F denotes the formula; P is the pair formula (6), and T is the Triplet/Quadruplet formula (8), (9). cies are calculated for all bunsetsus that follow a modifier bunsetsu. Formula (6) is used, and as a distance metric i,j, the number of bunsetsus between the modifier and the modifiee4 are combined with all features. In general lines, this model corresponds to models such as (Fujio and Matsumoto, 1998; Haruno et al., 1998; Uchimoto et al., 1999). W/O Restriction Model Modification candidates are restricted by SLUNG. The remaining is the same as the W/O Grammar Model. Pair Model Modification candidates are restricted to three, in the way described in Section 3.1. The remaining is the same as W/O Grammar Model. Triplet/Quadruplet Model This is the model proposed in the paper. Modification candidates are restricted to three, and Formula (8) or (9) are used. From the result shown in Table 3, we can say our method contributes to the improvement of our parser, because of the following reasons: The Triplet/Quadruplet Model outperforms the P</context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 1999</marker>
<rawString>Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 1999. Japanese dependency structure analysis based on maximum entropy models. In Proc. 13th EACL, pages 196203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Masaki Murata</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Dependency model using posterior context.</title>
<date>2000</date>
<booktitle>In Proc. of Sixth International Workshop on Parsing Technologies. \x0c&amp;apos;</booktitle>
<contexts>
<context position="8647" citStr="Uchimoto et al. (2000)" startWordPosition="1445" endWordPosition="1448">ch calculate the likelihood values of a dependency between bunsetsu i and j as in (6), such as a decision tree model (Haruno et al., 1998), a maximum entropy model (Uchimoto et al., 1999), a model based on distance and lexical information (Fujio and Matsumoto, 1998). Attributes i and j consist of a part-of-speech (POS), a lexical item, presence of a comma, and so on. And i,j \x0cis the number of intervening bunsetsus between i and j. P(i j) def = P(T |i, j, i,j) (6) However, these models fail to reflect contextual information because attributes of the surrounding bunsetsus are not considered. Uchimoto et al. (2000) proposed a model using posterior context. The model utilizes not only attributes about bunsetsus i, j but also attributes about all bunsetsus (including j) which follow bunsetsu i. That is, instead of learning two output values T(true) or F(false) for the dependency between two bunsetsus, three output values are used for learning: the bunsetsu i is bynd (dependent on a bunsetsu beyond j), dpnd (dependent on the bunsetsu j) or btwn (dependent on a bunsetsu between i and j). The probability is calculated by multiplying probabilities for all bunsetsus which follow bunsetsu i as in (7). They repo</context>
<context position="25500" citStr="Uchimoto et al. (2000)" startWordPosition="4205" endWordPosition="4208">mparison to other models 4.3.1 Models using the EDR corpus There are several works which use the EDR corpus for evaluation. The decision tree model (Haruno et al., 1998) achieves around 85%, the integrated model of lexical/syntactic information (Shirai et al., 1998) achieves around 86%, and the lexicalized statistical model (Fujio and Matsumoto, 1999) achieves 86.8% in bunsetsu accuracy. Our model outperforms all of them by 2 or 3%. 4.3.2 Models using the Kyoto corpus Shirai et al. (1998) used the Kyoto University text corpus (Kurohashi and Nagao, 1997) for evaluation and achieved around 86%. Uchimoto et al. (2000) also used the Kyoto corpus, and their accuracy was 87.9%. For comparison, we applied our method to the same 1,246 sentences that Uchimoto et al. (2000) used. The result is shown in Table 4. Our result is worse than theirs. The reason is thought to be as follows: We use the EDR corpus for training. Although we used around 24 times the amount of training data that Uchimoto et al. used, our training data lead to errors in the analysis of the Kyoto Corpus, because of differences in the annotation schemes adopted. Uchimoto et al. used the correct morphological analyses, but we used JUMAN. Sometime</context>
</contexts>
<marker>Uchimoto, Murata, Sekine, Isahara, 2000</marker>
<rawString>Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine, and Hitoshi Isahara. 2000. Dependency model using posterior context. In Proc. of Sixth International Workshop on Parsing Technologies. \x0c&amp;apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>