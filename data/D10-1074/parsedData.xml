<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.7710425">
b&amp;quot;Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756766,
MIT, Massachusetts, USA, 9-11 October 2010. c
</bodyText>
<sectionHeader confidence="0.566562" genericHeader="abstract">
2010 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.95158">
Towards Conversation Entailment: An Empirical Investigation
</title>
<author confidence="0.998242">
Chen Zhang Joyce Y. Chai
</author>
<affiliation confidence="0.996591">
Department of Computer Science and Engineering
Michigan State University
</affiliation>
<address confidence="0.976546">
East Lansing, MI 48824, USA
</address>
<email confidence="0.987925">
{zhangch6, jchai}@cse.msu.edu
</email>
<sectionHeader confidence="0.990611" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999736818181818">
While a significant amount of research has
been devoted to textual entailment, automated
entailment from conversational scripts has re-
ceived less attention. To address this limi-
tation, this paper investigates the problem of
conversation entailment: automated inference
of hypotheses from conversation scripts. We
examine two levels of semantic representa-
tions: a basic representation based on syntac-
tic parsing from conversation utterances and
an augmented representation taking into con-
sideration of conversation structures. For each
of these levels, we further explore two ways of
capturing long distance relations between lan-
guage constituents: implicit modeling based
on the length of distance and explicit mod-
eling based on actual patterns of relations.
Our empirical findings have shown that the
augmented representation with conversation
structures is important, which achieves the
best performance when combined with ex-
plicit modeling of long distance relations.
</bodyText>
<sectionHeader confidence="0.998299" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99822468">
Textual entailment has received increasing attention
in recent years (Dagan et al., 2005; Bar-Haim et al.,
2006; Giampiccolo et al., 2007; Giampiccolo et al.,
2008; Bentivogli et al., 2009). Given a segment from
a textual document, the task of textual entailment is
to automatically determine whether a given hypoth-
esis can be entailed from the segment. The capa-
bility of such kind of inference can benefit many
text-based applications such as information extrac-
tion and question answering.
Textual entailment has mainly focused on infer-
ence from written text in monologue. Recent years
also observed an increasing amount of conversa-
tional data such as conversation scripts of meetings,
call center records, court proceedings, as well as on-
line chatting. Although conversation is a form of
language, it is different from monologue text with
several unique characteristics. The key distinctive
features include turn-taking between participants,
grounding between participants, different linguistic
phenomena of utterances, and conversation impli-
catures. Traditional approaches dealing with tex-
tual entailment were not designed to handle these
unique conversation behaviors and thus to support
automated entailment from conversation scripts.
</bodyText>
<subsectionHeader confidence="0.2157875">
Example 1:
Conversation Segment:
</subsectionHeader>
<bodyText confidence="0.9180383125">
B: My mother also was very very independent.
She had her own, still had her own little house
and still driving her own car,
A: Yeah.
B: at age eighty-three.
Hypothesis:
(1) Bs mother is eighty-three.
(2) B is eighty-three.
To address this limitation, our previous
work (Zhang and Chai, 2009) has initiated an
investigation on the problem of conversation en-
tailment. The problem was formulated as follows:
given a conversation discourse D and a hypothesis
H concerning its participant, the goal was to identify
whether D entails H. For instance, as in Example
1, the first hypothesis can be entailed from the
</bodyText>
<page confidence="0.997022">
756
</page>
<bodyText confidence="0.99913425">
\x0cconversation segment while the second hypothesis
cannot. While our previous work has provided
some interesting preliminary observations, it mostly
focused on data collection and initial experiments
and analysis using a small set of development data.
It is not clear whether the previous results are
generally applicable, how different components in
the entailment framework interact with each other,
and how different representations may influence the
entailment outcome.
To reach a better understanding of conversation
entailment, we conducted a further investigation
based on the larger set of test data collected in our
previous work (Zhang and Chai, 2009). We specifi-
cally examined two levels of representations: a basic
representation based on syntactic parsing from con-
versation utterances and an augmented representa-
tion taking into consideration of conversation struc-
tures. For each of these levels, we further explored
two ways of capturing long distance relations: (1)
implicit modeling based on the length of distance
and (2) explicit modeling based on actual patterns
of relations. Our empirical findings have shown that
augmented representation with conversation struc-
tures is important in conversation entailment. Com-
bining conversation structures with explicit model-
ing of long distance relations results in the best per-
formance.
</bodyText>
<sectionHeader confidence="0.999635" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999688676923077">
Our work here is related to recent advances in tex-
tual entailment, automated processing of conversa-
tion scripts, and our initial investigation on conver-
sation entailment.
There is a large body of work on textual en-
tailment initiated by the Pascal Recognizing Tex-
tual Entailment (RTE) Challenges (Dagan et al.,
2005; Bar-Haim et al., 2006; Giampiccolo et al.,
2007; Giampiccolo et al., 2008; Bentivogli et al.,
2009). Different approaches have been developed,
for example, based on logic proving (Tatu and
Moldovan, 2005; Bos and Markert, 2005; Raina et
al., 2005) and graph match (Haghighi et al., 2005;
de Salvo Braz et al., 2005; MacCartney et al., 2006).
Supervised learning approaches have also been ap-
plied to measure the similarities between training
and testing pairs (Zanzotto and Moschitti, 2006). In
the most recent RTE Challenge (Bentivogli et al.,
2009), the best system achieves 73.5% of accuracy,
while the median performance among all partici-
pants is 60.4%. These results indicate that, while
progress has been made, textual entailment remains
a challenging problem.
As more and more conversation data becomes
available, researchers have investigated automated
processing of conversation data to acquire useful
information, for example, related to opinions (So-
masundaran et al., 2007; Somasundaran et al.,
2008; Somasundaran et al., 2009), biographic at-
tributes (Garera and Yarowsky, 2009), social net-
works (Jing et al., 2007), and agreements and
disagreements between participants (Galley et al.,
2004). Recent studies have also developed ap-
proaches to summarize conversations (Murray and
Carenini, 2008) and to model conversation struc-
tures (dialogue acts) from online Twitter conversa-
tions (Ritter et al., 2010). Here we address a dif-
ferent angle regarding conversation scripts, namely
conversation entailment.
In our previous work (Zhang and Chai, 2009),
we started an initial investigation on conversation
entailment. We have collected a dataset of 875
instances. Each instance consists of a conversa-
tion segment and a hypothesis (as described in Sec-
tion 1). The hypotheses are statements about conver-
sation participants and are further categorized into
four types: about their profile information, their be-
liefs and opinions, their desires, and their commu-
nicative intentions. We developed an approach that
is motivated by previous work on textual entailment.
We use clauses in the logic-based approaches as the
underlying representation of our system. Based on
this representation, we apply a two stage entailment
process similar to MacCartney et al. (2006) devel-
oped for textual entailment: an alignment stage fol-
lowed by an entailment stage.
Building upon our previous work, in this paper,
we systematically examine different representations
of the conversation segment and different modeling
of long distance relations between language con-
stituents. We compare the roles of these different
representations on the performance of entailment
prediction using a larger testing dataset that was not
previously evaluated. This analysis allows better un-
derstanding of the problem and provides insight on
</bodyText>
<page confidence="0.997274">
757
</page>
<bodyText confidence="0.457545">
\x0cpotential solutions.
</bodyText>
<sectionHeader confidence="0.980667" genericHeader="method">
3 Overall Framework
</sectionHeader>
<bodyText confidence="0.992207307692308">
In our previous work (Zhang and Chai, 2009), con-
versation entailment is formulated as the follow-
ing: given a conversation segment D which is rep-
resented by a set of clauses D = d1 . . . dm,
and a hypothesis H represented by another set of
clauses H = h1 . . . hn, the prediction on
whether D entails H is determined by the product
of probabilities that each hypothesis clause hj is
entailed from all the conversation segment clauses
d1 . . . dm as follows. This is based on a simple as-
sumption that whether a clause is entailed from a
conversation segment is conditionally independent
from other clauses.
</bodyText>
<equation confidence="0.999672416666667">
P(D \x0f H|D, H)
= P(D \x0f h1, . . . , D \x0f hn|D, h1, . . . , hn)
=
n
Y
j=1
P(D \x0f hj|D = d1 . . . dm, hj)
=
n
Y
j=1
P(d1 . . . dm \x0f hj|d1, . . . , dm, hj) (1)
</equation>
<bodyText confidence="0.99656419047619">
A clause here is similar to a sentence in first-
order predicate calculus. It is made up by terms
and predicates. A term is either: 1) an entity
described by a noun phrase, e.g., John Lennon,
mother, or she; or 2) an action or event de-
scribed by a verb phrase, e.g., marry in John
married Eva in 1940. A predicate represents
either: 1) a property (i.e., unary) for a term,
e.g., Russian(company), or recently(visit);
or 2) a relation (i.e., binary) between two
terms, e.g., subj(visit, Prime Minister) and
obj(visit, Brazil) in Prime Minister recently vis-
ited Brazil.
Given the clause representation, we follow the
idea similar to MacCartney et al. (2006), and predict
the entailment decision in two stages of processing:
(1) an alignment model aligns terms in the hypothe-
sis to terms in the conversation segment; and (2) an
inference model predicts the entailment based on the
alignment between the hypothesis and the conversa-
tion segment.
</bodyText>
<subsectionHeader confidence="0.996144">
3.1 Alignment Model
</subsectionHeader>
<bodyText confidence="0.997074636363636">
An alignment is defined as a mapping function g
between a term x in the conversation segment and a
term y in the hypothesis. g(x, y) = 1 if x and y are
aligned; otherwise g(x, y) = 0. It is possible that
multiple terms from the segment are mapped to one
term in the hypothesis (g(x1, y) = g(x2, y) = 1),
or vice versa (g(x, y1) = g(x, y2) = 1). To predict
these alignments, the problem is formulated as bi-
nary classification: given any two terms x from the
conversation and y from the hypothesis, decide the
value of their alignment function g(x, y).
</bodyText>
<subsectionHeader confidence="0.940793">
3.2 Inference Model
</subsectionHeader>
<bodyText confidence="0.997993428571428">
Once an alignment between a hypothesis and a con-
versation segment is established, an inference model
is applied to predict whether the conversation seg-
ment entails the hypothesis given such alignment.
More specifically, as shown in Equation 1, given a
clause from the hypothesis hj, a set of clauses from
the conversation segment d1, . . . , dm, and an align-
ment g between them, the goal is to predict whether
d1, . . . , dm entails hj under the alignment g.
The prediction is treated differently according to
different types of clauses. If hj is a property clause
(i.e., takes one argument hj()), a property inference
model is applied; otherwise (i.e., relational clauses
with two arguments hj(, )), a relational inference
model is applied.
In this paper we follow the same framework.
However our focus here is on the new question that
how different levels of semantic representation and
different approaches of modeling long distance rela-
tionship affect the alignment and inference models
as well as the overall entailment performance.
</bodyText>
<sectionHeader confidence="0.993194" genericHeader="method">
4 Semantic Representation
</sectionHeader>
<bodyText confidence="0.999445111111111">
Given the clause representation described earlier,
an important question is what information from the
conversation segment should be captured and repre-
sented. To address this question, we examined two
levels of shallow semantic representation. The first
level is basic representation which only captures the
information from all the utterances in the conversa-
tion segment. The second representation includes
conversation structures (e.g., speakers and dialogue
</bodyText>
<page confidence="0.996078">
758
</page>
<bodyText confidence="0.8938395">
\x0cacts). Next we use Example 2 to illustrate these rep-
resentations.
</bodyText>
<table confidence="0.647920142857143">
Example 2:
Conversation Segment:
B: Have you seen Sleeping with the Enemy?
A: No. Ive heard thats really great, though.
B: You have to go see that one.
Hypothesis:
B suggests A to watch Sleeping with the Enemy.
</table>
<subsectionHeader confidence="0.997794">
4.1 Basic Representation
</subsectionHeader>
<bodyText confidence="0.999825529411765">
The first representation is based on the syntactic
parsing from conversation utterances and we call it
a basic representation. Figure 1(a) shows an exam-
ple of dependency structures for several utterances
that are derived from the Stanford parser (Klein and
Manning, 2003), and Figure 1(b) shows the corre-
sponding clause representation. In the dependency
structure, the vertices represent entities (e.g., x1) and
actions (e.g., x3) within an utterance. They corre-
spond to terms in the clause representation. An edge
between vertices captures a dependency relation and
is represented as predicates in the clause representa-
tion. For example, the edge between x1 and x3 indi-
cates x1 is the subject of x3, which is represented by
the clause representation subj(x3, x1). Similar rep-
resentation also applies to the hypothesis as shown
in Figure 1(c), 1(d).
</bodyText>
<subsectionHeader confidence="0.991309">
4.2 Augmented Representation
</subsectionHeader>
<bodyText confidence="0.981871888888889">
The second representation is built upon the basic
representation and incorporates conversation struc-
ture across turns and utterances. We call it an aug-
mented representation. Figure 2(a) shows the aug-
mented structures of the conversation segment and
Figure 2(b) shows the corresponding clause repre-
sentation. Compared to the basic representation,
there are two additional types of vertices (i.e., terms)
highlighted in the figures:
Vertices representing utterances (e.g.,
u1 . . . u4). Their corresponding terms capture
the dialogue acts for the utterances (e.g.
u1 = yes no question). To focus our effort,
currently we only apply annotated dialogue
acts provided in the Switchboard corpus (God-
frey and Holliman, 1997). Two edges are
added to connect different utterances. The
first edge connects each utterance vertex to
the head of the corresponding utterance to
indicate the specific content of the utterance
(e.g., content(u1, x3)). The second edge con-
nects an utterance to its succeeding utterance
to indicate the temporal progression of the
conversation (e.g., follow(u2, u1)).
Vertices representing speakers or participants
(e.g., sA, sB). One edge is added to
connect each utterance to its speaker (e.g.,
speaker(u1, sB)).
Note that since our clause representations are
mainly based on the dependency relations, they are
mostly syntactic-driven. However, it does capture
some shallow semantics such as who is the agent
(i.e., subject) or the patient (i.e., object) of an event.
The incorporation of speakers and dialogue acts in
our augmented representations provides additional
semantics of conversation discourse.
</bodyText>
<sectionHeader confidence="0.992403" genericHeader="method">
5 Modeling LDR
</sectionHeader>
<bodyText confidence="0.9959225">
A critical part in predicting entailment is to recog-
nize the semantic relationship between two language
constituents, especially when these two constituents
are not directly related. In Figure 2(a), for exam-
ple, we want to recognize that x9 (You) is the (log-
ical) subject of x11 (see). Here we experimented
two ways of modeling such long distance relations
(LDR).
</bodyText>
<subsectionHeader confidence="0.99757">
5.1 Implicit Modeling of LDR
</subsectionHeader>
<bodyText confidence="0.999830454545454">
The first method characterizes the relationship sim-
ply by the distance between two constituents in the
dependency structure (or augmented structure). For
example, in Figure 2(a) the distance between x11
and x9 is 3. We call this method an implicit mod-
eling of long distance relationship.
The advantage of implicit modeling is that it is
easy to implement based on the dependency struc-
ture. However, its limitation is that the distance mea-
sure does not capture sufficient information of se-
mantic relations between language constituents.
</bodyText>
<subsectionHeader confidence="0.999359">
5.2 Explicit Modeling of LDR
</subsectionHeader>
<bodyText confidence="0.998041">
The second way of modeling long distance relation-
ship is called explicit modeling. It uses a string to
</bodyText>
<page confidence="0.994474">
759
</page>
<bodyText confidence="0.922897">
\x0cB: Have you seen Sleeping with the Enemy?
A: No. I&amp;apos;ve heard that&amp;apos;s really great, though.
B: You have to go see that one.
</bodyText>
<figure confidence="0.88849034965035">
x9
x13
x12
x11
x10
x4 x1
x3
x2
x5
x8
x6
x7
obj(x3,x2)
subj(x3,x1)
aux(x3,x4)
x1=A
x2=Sleeping
with the Enemy
x3=seen, x4=have
obj(x11,x10)
obj(x12,x11)
obj(x13,x12)
subj(x13,x9)
x9=A, x10=one,
x11=see, x12=go,
x13=have
subj(x7,x6)
obj(x8,x7)
subj(x8,x5)
x5=A, x6=that
x7=is really great
x8=have heard
Clauses
Terms
(a) dependency structure of the conversation
utterances
B: Have you seen Sleeping with the Enemy?
A: No. I&amp;apos;ve heard that&amp;apos;s really great, though.
B: You have to go see that one.
x9
x13
x12
x11
x10
x4 x1
x3
x2
x5
x8
x6
x7
obj(x3,x2)
subj(x3,x1)
aux(x3,x4)
x1=A
x2=Sleeping
with the Enemy
x3=seen, x4=have
obj(x11,x10)
obj(x12,x11)
obj(x13,x12)
subj(x13,x9)
x9=A, x10=one,
x11=see, x12=go,
x13=have
subj(x7,x6)
obj(x8,x7)
subj(x8,x5)
x5=A, x6=that
x7=is really great
x8=have heard
Clauses
Terms
(b) basic representation of the conver-
sation segment
x1
x2
x5
x4
x3
B suggests A to watch Sleeping with the Enemy.
subj
subj
obj
obj
B: Have you seen Sleeping with the Enemy?
A: No. I&amp;apos;ve heard that&amp;apos;s really great, though.
B: You have to go see that one.
x9
x13
x12
x11
x10
x4 x1
x3
x2
x5
x8
x6
x7
(c) dependency structure of the hypothesis
obj(x3,x2), subj(x3,x1)
aux(x3,x4)
x1=A, x3=seen, x4=have
x2=Sleeping with the Enemy
obj(x11,x10), obj(x12,x11)
obj(x13,x12), subj(x13,x9)
x9=A, x10=one, x11=see
x12=go, x13=have
speaker(u4,sB)
content(u4,x13)
follow(u4,u3)
u4=viewpoint
subj(x7,x6), obj(x8,x7)
subj(x8,x5)
x5=A, x7=is really great
x6=that, x8=have heard
speaker(u3,sA)
content(u3.x8)
follow(u3,u2)
u3=statement
speaker(u2,sA)
follow(u2,u1)
u2=no_answer
speaker(u1,sB)
content(u1, x3)
sA, sB
u1=yes_no_question
Clauses
Terms
subj(x4,x2)
obj(x4,x3)
subj(x5,x1)
obj(x5,x4)
x1=B, x2=A
x3=Sleeping
with the Enemy
x4=watch
x5=suggests
Clauses
Terms
(d) representation of the hy-
pothesis
</figure>
<figureCaption confidence="0.999468">
Figure 1: The dependency structures and corresponding basic representation of Example 2
</figureCaption>
<figure confidence="0.937068602739726">
x1
x2
x5
x4
x3
B suggests A to watch Sleeping with the Enemy.
subj
subj
obj
obj
B: Have you seen Sleeping with the Enemy?
A: No. I&amp;apos;ve heard that&amp;apos;s really great, though.
B: You have to go see that one.
x9
x13
x12
x11
x10
x4 x1
x3
x2
x5
x8
x6
x7
u1
u2
u3
u4
sB
sA
(a) dependency and conversation structures of the conversation
segment
obj(x3,x2), subj(x3,x1)
aux(x3,x4)
x1=A, x3=seen, x4=have
x2=Sleeping with the Enemy
obj(x11,x10), obj(x12,x11)
obj(x13,x12), subj(x13,x9)
x9=A, x10=one, x11=see
x12=go, x13=have
speaker(u4,sB)
content(u4,x13)
follow(u4,u3)
u4=viewpoint
subj(x7,x6), obj(x8,x7)
subj(x8,x5)
x5=A, x7=is really great
x6=that, x8=have heard
speaker(u3,sA)
content(u3.x8)
follow(u3,u2)
u3=statement
speaker(u2,sA)
follow(u2,u1)
u2=no_answer
speaker(u1,sB)
content(u1, x3)
sA, sB
u1=yes_no_question
Clauses
Terms
s
o
s
o
x1=B, x2=A
x3=Sleeping
with the Enemy
x4=watch
x5=suggests
C
Terms
</figure>
<figureCaption confidence="0.910403333333333">
(b) augmented representation of the conversation seg-
ment
Figure 2: The dependency and conversation structures and corresponding augmented representation of Example 2
</figureCaption>
<page confidence="0.987673">
760
</page>
<bodyText confidence="0.988069947368421">
\x0cdescribe the path from one constituent to the other:
v1e1 . . . vl1el1vl, where v1, . . . , vl are the vertices
on the path and e1, . . . , el1 are the edges. Each vi
describes the type of the vertex in the dependency
structure, which is either a noun (N), a verb (V ),
or an utterance (U). Each ei describes whether the
edge is forward () or backward (). For ex-
ample, in Figure 2(a), the path from x11 to x9 is
V V V N.
This kind of string representation of paths in syn-
tactic parse is known as a way of modeling shal-
low semantics between any two constituents in a
language structure. It is largely used in other NLP
tasks such as semantic role labeling (Pradhan et al.,
2008). The difference here is our paths are extracted
from dependency parses as opposed to traditional
constituent parses, and our paths also incorporate the
representation of conversation structures (e.g., utter-
ances and speakers).
</bodyText>
<sectionHeader confidence="0.63471" genericHeader="method">
6 Applications in Entailment Models
</sectionHeader>
<bodyText confidence="0.992348">
In this section we describe how different representa-
tions and modeling of LDR are used in the alignment
and inference models.
</bodyText>
<subsectionHeader confidence="0.999763">
6.1 Applications in Alignment Model
</subsectionHeader>
<bodyText confidence="0.998971238095238">
Although a noun and a verb can potentially be
aligned, to simplify the problem, we restrict the
problem to the alignment between two nouns or two
verbs. We trained an alignment model for nouns and
one for verbs separately.
Table 1 summarizes a set of features used in the
alignment models. Most of these features are shared
by the model for noun alignment and the model for
verb alignment. These features include whether the
two strings are the same, two terms have the same
stem, the similarity between the two terms either
based on WordNet or distributional statistics (Lin,
1998).
To learn the alignment model for nouns, we anno-
tated the noun alignments for the development data
used in PASCAL RTE-3 Challenge (Giampiccolo et
al., 2007) and trained a logistic regression model
based on the features in Table 1. Cross-validation
on the same dataset shows relatively satisfying per-
formance (96.4% precision and 94.9% recall). In
this paper, we focus on the alignment between verbs
</bodyText>
<table confidence="0.980890727272727">
Noun Verb
Align. Align.
Verb be identification X
String equality X X
Stemmed equality X X
Acronym equality X
Named entity equality X
WordNet similarity X X
Distributional similarity X X
Subject consistency X
Object consistency X
</table>
<tableCaption confidence="0.915581">
Table 1: Features for alignment models
</tableCaption>
<bodyText confidence="0.9981625">
since it appears more difficult.
A major difference between noun alignment and
verb alignment is that, for verb alignment the con-
sistency of their arguments is also important. For
two events (described by two verbs) to be aligned, at
least their subjects (usually denoting the executers of
actions) and objects (usually denoting the receivers
of actions) should match to each other respectively.
Note that, although actions/events also depend on
other arguments or adjuncts, here we only consider
the subjects and objects and leave the consistency
check of other arguments/adjuncts to downstream
processes. Based on two different ways of model-
ing long distance relationship (as described in Sec-
tion 5), we explored two methods for modeling ar-
gument consistency (AC) in verb alignment models.
</bodyText>
<subsubsectionHeader confidence="0.67083">
6.1.1 Implicit Modeling of AC
</subsubsectionHeader>
<bodyText confidence="0.999360333333333">
The first approach models argument consistency
based on implicit modeling of the relationship be-
tween a verb and its aligned subject/object. Specif-
ically, given a pair of verb terms (x, y) where x is
from the conversation segment and y is from the hy-
pothesis, let sy be the subject of y and sx be the
aligned entity of sy in the conversation (in case of
multiple alignments, sx is the one closest to x). The
subject consistency of the verbs (x, y) is then mea-
sured by the distance between sx and x in the de-
pendency structure. Similarly, the distance between
a verb and its aligned object is used as a measure of
the object consistency.
In Example 2, to decide whether the conversa-
tion term see (x11 in Figure 1(a), 1(b), and 2) and
the hypothesis term watch (x4 in Figure 1(c), 1(d))
should be aligned, we first identify the subject of x4
in the hypothesis, which is x2 (A). We then look for
</bodyText>
<page confidence="0.979493">
761
</page>
<bodyText confidence="0.965355666666667">
\x0cx2s alignments in the conversation segment, among
which x9 (You) is the closest to x11 (see). In Fig-
ure 2(a), we find the distance between x11 and x9 is
</bodyText>
<page confidence="0.458465">
3.
</page>
<bodyText confidence="0.999856">
Using the implicit modeling of argument consis-
tency, we follow the same approach as in our pre-
vious work (Zhang and Chai, 2009) and trained a
logistic regression model to predict verb alignment
based on the features in Table 1.
</bodyText>
<subsubsectionHeader confidence="0.762016">
6.1.2 Explicit Modeling of AC
</subsubsectionHeader>
<bodyText confidence="0.983102806451613">
The second approach captures argument consis-
tency based on explicit modeling of the relationship
between a verb and its aligned subject (or object).
Given a pair of verb terms (x, y), let sy be the sub-
ject of y and sx be the aligned entity of sy in the
conversation closest to x, we use the string describ-
ing the path from x to sx as the feature to capture
subject consistency. For example, in Figure 2(a), the
path from x11 to x9 is V V V N.
This string representation of paths is used to cap-
ture both the subject consistency and the object con-
sistency. Since they are non-numerical features, and
the variability of their values can be extremely large,
so we applied an instance-based classification model
(e.g., k-nearest neighbor) to determine alignments
between verb terms. We measure the distance be-
tween two path features by their minimal string edit
distance, and then simply use the Euclidean distance
to measure the closeness between any two verbs.
Again this model is trained from our development
data described in Zhang and Chai (2009).
Figure 3 shows an example of alignment between
the conversation terms and hypothesis terms in Ex-
ample 2. Note that in this figure the alignment
between x5 = suggests from the hypothesis and
u4 = opinion from the conversation segment is a
pseudo alignment, which directly maps a verb term
in the hypothesis to an utterance term represented
by its dialogue act. This alignment is obtained by
following the same set of rules learned from the de-
velopment dataset as in (Zhang and Chai, 2009).
</bodyText>
<subsectionHeader confidence="0.99961">
6.2 Applications in Inference Model
</subsectionHeader>
<bodyText confidence="0.99863275">
As mentioned earlier, once an alignment is estab-
lished, the inference model is to predict whether
each clause in the hypothesis is entailed from the
conversation segment. Two separate models were
</bodyText>
<figure confidence="0.994400857142857">
x4=have
x5=A
x2=Sleeping
with the Enemy
x1=A
x7=is really great
x10=one
u4=opinion
Conversation Segment
x3=Sleeping
with the Enemy
x5=suggests
x2=A
x1=B
x4=watch
Hypothesis
x6=that
x11=see
x12=go
x13=have
x8=have heard
u3=statement
u2=no_answer
u1=yes_no_question
x3=seen
x9=A
sB
sA
</figure>
<figureCaption confidence="0.916087">
Figure 3: The alignment result for Example 2
used to handle the inference of property clauses
(hj(x)) and and the inference of relational clauses
(hj(x, y)). Property clauses involve less variables
</figureCaption>
<bodyText confidence="0.989065318181818">
and are relatively simple, so we used the same prop-
erty inference model as in (Zhang and Chai, 2009).
Here we focus on relational inference model and ex-
amine how different modeling of long distance rela-
tionship may affect relation inference.
For a relation h between x and y to be entailed
from a conversation segment, we need to find a same
or similar relation in the conversation segment be-
tween xs and ys counterparts (i.e., aligned entities
of x and y in the conversation segment).
More specifically, given a relational clause from
the hypothesis, hj(x, y), we find the sets of
terms X0 = {x0|x0 D, g(x0, x) = 1} and Y 0 =
{y0|y0 D, g(y0, y) = 1}, which are aligned with x
and y, respectively. We then find the closest re-
lation between these two sets of terms, (x, y),
such that the distance between x and y is the
smallest for any x X0 and y Y 0. For in-
stance, in the hypothesis of Example 2 there are
terms x5=suggests and x4=watch, and a relational
clause obj(x5, x4) describing an action-object rela-
tion between them. Their counterparts in the con-
</bodyText>
<page confidence="0.980988">
762
</page>
<bodyText confidence="0.987739428571428">
\x0cversation segment are X0 = {u4=viewpoint} and
Y 0 = {x3=seen, x11=see}. So the closest pair of
terms between these two sets is u4 and x11. Conse-
quently, whether the target relational clause hj(x, y)
is entailed is determined by the relationship between
x and y. Such relationship can be modeled either
implicitly or explicitly.
</bodyText>
<subsectionHeader confidence="0.998186">
6.3 Implicit modeling of relation inference
</subsectionHeader>
<bodyText confidence="0.989378666666667">
In this model we follow the simple idea that the
shorter a path is between two terms, the more likely
these two terms have a direct relationship. So we
predefine a threshold, L. We predict that hj(x, y) is
entailed if the distance between x and y is smaller
than L. However, as can be seen, this distance does
not reflect whether the type of relationship between
x and y is similar to the relationship that holds be-
tween x and y.
</bodyText>
<subsectionHeader confidence="0.993607">
6.4 Explicit modeling of relation inference
</subsectionHeader>
<bodyText confidence="0.979536545454545">
In order to capture more semantics from the rela-
tion between two terms, we use explicit modeling
of the relationship between terms x and y. In
the previous example, the relationship between u4
and x11 is modeled by the path from u4 to x11,
U V V V .
Given this characterization, the prediction of
whether hj(x, y) is entailed from the conversation
segment is formulated as a binary classification
problem, using a k-nearest neighbor classification
model with following features:
</bodyText>
<listItem confidence="0.612359555555556">
1. Explicit modeling of long distance relationship,
i.e., the path from x to y in the dependency
structure of the conversation segment;
2. The types (N, V, or U) of x, y, x, and y;
3. The type of relation between x and y, for ex-
ample, obj in obj(x, y);
4. The order (i.e., before or after) between x and
y, and between x and y;
5. The specific type of the hypothesis.
</listItem>
<sectionHeader confidence="0.701915" genericHeader="evaluation">
7 Evaluation and Analysis
</sectionHeader>
<bodyText confidence="0.974583333333333">
We evaluated different model configurations using
our data1. This dataset consists of 291 development
instances and 584 testing instances. The hypotheses
</bodyText>
<figure confidence="0.6898198">
1
The data is available for download at http:
//links.cse.msu.edu:8000/lair/projects/
conversationentailment_data.html.
(a) Based on basic representation
</figure>
<figureCaption confidence="0.9053045">
(b) Based on augmented representation
Figure 4: Evaluation of verb alignment
</figureCaption>
<bodyText confidence="0.9703358">
were categorized into four types: (1) fact: profile
and social relations of conversation participants (ac-
counted for 47% of the development data and 49%
of the testing data); (2) belief: participants beliefs
and opinions (34% and 35%); (3) desire: partici-
pants desire of certain actions or outcomes (11%
and 4%); (4) intent: communicative intent that cap-
tures some perlocutionary force from one participant
to the other (e.g. A stops B from doing something;
A disagreees with B on something, 8% and 12%)
Note that in our original work (Zhang and Chai,
2009), only development data were used to show
some initial observations. Here we trained our mod-
els on the development data and results shown are
from the testing data.
</bodyText>
<subsectionHeader confidence="0.999412">
7.1 Evaluation of Alignment Models
</subsectionHeader>
<bodyText confidence="0.9989094">
The evaluation of alignment models is based on pair-
wise decision. For each pair of terms (x, y), where
x is from a conversation segment and y is from
a hypothesis, we measure whether the model cor-
rectly predicts that the two terms should or should
not be aligned. Because the alignment classification
has extremely unbalanced classes, we use precision-
recall of true alignments as evaluation metrics.
Figure 4(a) and 4(b) shows the comparison (F-
measure) of two alignment models for verb align-
</bodyText>
<page confidence="0.993823">
763
</page>
<figureCaption confidence="0.810185">
\x0cFigure 5: Evaluation of inference models based on different representations
</figureCaption>
<bodyText confidence="0.9838853125">
ment, based on the basic representation and the aug-
mented representation, respectively. Note that we
cannot directly compare the results between these
two figures since they involve different number of
alignment instances2. Nevertheless, we can see the
overall trend within each figure: the explicit model
outperforms the implicit model. This suggests that
the explicit modeling of semantic relationship be-
tween verbs and arguments works better than the im-
plicit modeling used in previous work. Furthermore,
the improvement is most noticeable when hypothe-
ses are facts (24.8% with the basic representation
and 24.1% with the augmented representation), and
least when hypotheses are intents (12.2% with the
basic representation and 6.2% with the augmented
representation).
</bodyText>
<subsectionHeader confidence="0.998695">
7.2 Evaluation of Inference Models
</subsectionHeader>
<bodyText confidence="0.998844058823529">
In order to compare different inference models, in
this section (and this section only) we use gold-
standard alignment results. They are obtained from
manual annotation in our evaluation. We evaluated
two inference models, one with implicit modeling
of long distance relationship and one with explicit
modeling. Evaluations were conducted based on
both the basic representation and the augmented rep-
resentation. Figure 5 shows the four groups of eval-
uation results.
Overall speaking, the augmented representation
outperforms the basic representation for both im-
plicit modeling and explicit modeling of long dis-
tance relationship (McNemars tests, p &lt; 0.05). The
explicit model performs better than implicit model
only based on augmented representation (McNe-
mars test, p &lt; 0.05).
</bodyText>
<page confidence="0.880536">
2
</page>
<bodyText confidence="0.441659">
The alignment based on the augmented representation in
Figure 4(b) also includes pseudo alignments.
</bodyText>
<table confidence="0.954837">
Clause Rep- Relation modeling Improve-
resentation Implicit Explicit ment
Basic 53.9% 53.9% 0
Augmented 54.8% 58.7% 3.9%
</table>
<tableCaption confidence="0.859712">
Table 2: Entailment performance with different represen-
tations and LDR modeling
</tableCaption>
<bodyText confidence="0.9994802">
The results were further broken down by different
hypothesis types. For the fact type of hypotheses,
there is no difference between different represen-
tations and modeling of long distance relationship.
This is not surprising since most hypotheses about
partipants profiling information can be inferred di-
rectly from the utterances. The augmented repre-
sentation affects the intent type of hypothesis most
significantly, so does the explicit modeling of long
distance relationship.
</bodyText>
<subsectionHeader confidence="0.9556375">
7.3 Interaction between Clause
Representations and LDR Modeling
</subsectionHeader>
<bodyText confidence="0.999796529411765">
It was shown in previous sections that the aug-
mented representation helps entailment prediction
compared to the basic representation. Here we want
to study how they interact with other entailment
components and what is their effect in the enhanced
modeling of long distance relations. Specifically, we
test the performance of implicit and explicit mod-
eling of long distance relations under two different
representation settings: the basic representation and
the augmented representation.
Table 2 compares the performance (accuracy) of
entailment models with different relationship mod-
eling. We can see that the explicit model makes im-
provement over the implicit model for augmented
representation (McNemars test, p &lt; 0.05), while
no improvement is made for basic representation.
These evaluation results appear to suggest that there
</bodyText>
<page confidence="0.978427">
764
</page>
<bodyText confidence="0.999426444444444">
\x0cis an interaction between clause representations and
semantic modeling of long distance relations: the
modeling of long distance relations between lan-
guage constituents appears only effective when con-
versation structure is incorporated in the representa-
tion.
It is interesting to see the difference in the predic-
tion performances on fact hypotheses and intent hy-
potheses. For fact, the most benefit of incorporating
explicit modeling of long distance relationship ap-
pears at the alignment stage, but not much at the in-
ference stage. However, this situation is different for
intent, where the benefit of explicitly modeling long
distance relationship mostly happened at the infer-
ence stage. This observation suggests that the effects
of different types of modeling may vary for different
types of hypotheses, which indicates that hypothesis
type dependent models may be beneficial.
</bodyText>
<sectionHeader confidence="0.964982" genericHeader="discussions">
8 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999864785714286">
This paper presents an empirical investigation on
conversation entailment. We specifically examine
two levels of representation of conversation seg-
ments and two different ways of modeling long dis-
tance relations between language constituents. Our
findings indicate that, although traditional architec-
ture and approaches for textual entailment remain
important, additional representation and processing
that address conversation structures is critical. The
augmented representation with conversation struc-
tures, together with explicit modeling of semantic
relations between language constituents, results in
the best performance (58.7% accuracy).
The work here only represents an initial step to-
wards conversation entailment. Conversation phe-
nomena are rich and complex. Conversation entail-
ment is extremely difficult. Besides the same chal-
lenges faced by textual entailment, it is further com-
plicated by conversation implicature. Although our
current data enables us to start an initial investiga-
tion, its small size poses significant limitations on
technology development and evaluation. For ex-
ample, our studies have indicated hypothesis type-
dependent approaches may be beneficial, however
we do not have sufficient data to yield reasonable
models. A more systematical approach to collect
and create a larger set of data is crucial. Inno-
vative community-based approaches (e.g., through
web) for data collection and annotation can be pur-
sued in the future. As more techniques in semantic
processing (e.g., semantic role) become available,
future work should also capture deeper semantics,
address pragmatics, and incorporate richer world
knowledge.
Finally, as the technology in conversation entail-
ment is developed, its applications in NLP problems
should be explored. Example applications include
information extraction, question answering, summa-
rization from conversation scripts, and modeling of
conversation participants. These applications may
provide new insights on the nature of the conversa-
tion entailment problem and its potential solutions.
</bodyText>
<sectionHeader confidence="0.978345" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99902625">
This work was supported by grant IIS-0347548 from
the National Science Foundation. We thank the
anonymous reviewers for their valuable comments
and suggestions.
</bodyText>
<sectionHeader confidence="0.9919" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.966486148148148">
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment, Venice, Italy.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
pascal recognizing textual entailment challenge. In
Proceedings of the Second Text Analysis Conference
(TAC 2009).
Johan Bos and Katja Markert. 2005. Recognising textual
entailment with logical inference. In Proceedings of
HLT-EMNLP, pages 628635.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In PASCAL Challenges Workshop on Recognis-
ing Textual Entailment.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural lan-
guage. In Proceedings of AAAI.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agreement
and disagreement in conversational speech: Use of
bayesian networks to model pragmatic dependencies.
In Proceedings of ACL, pages 669676.
</reference>
<page confidence="0.946261">
765
</page>
<reference confidence="0.999785784090909">
\x0cNikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 710718, Suntec, Singapore, Au-
gust.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third pascal recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 19.
Danilo Giampiccolo, Hoa Trang Dang, Bernardog
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth pascal recognizing textual entail-
ment challenge. In Proceedings of the First Text Anal-
ysis Conference (TAC 2008).
John J. Godfrey and Edward Holliman. 1997.
Switchboard-1 Release 2. Linguistic Data Consor-
tium, Philadelphia.
Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching. In
Proceedings of HLT-EMNLP, pages 387394.
Hongyan Jing, Nanda Kambhatla, and Salim Roukos.
2007. Extracting social networks and biographical
facts from conversational speech transcripts. In Pro-
ceedings of ACL, pages 10401047.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL 03: Proceedings
of the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 423430, Morristown, NJ,
USA.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of International Confer-
ence on Machine Learning, pages 296304.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of HLT-NAACL,
pages 4148.
Gabriel Murray and Giuseppe Carenini. 2008. Summa-
rizing spoken and written conversations. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 773782, Hon-
olulu, Hawaii, October.
Sameer S. Pradhan, Wayne Ward, and James H. Martin.
2008. Towards robust semantic role labeling. Compu-
tational Linguistics, 34(2):289310.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI, pages
10991105.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 172180, Los An-
geles, California, June.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2007. Detecting arguing and sentiment in
meetings. In Proceedings of the 8th SIGdial Workshop
on Discourse and Dialogue, Antwerp, September.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 801808, Manchester, UK, August.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 170179, Singapore,
August.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Proceed-
ings of HLT-EMNLP, pages 371378.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments with
cross-pair similarities. In ACL-44: Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 401408,
Morristown, NJ, USA.
Chen Zhang and Joyce Chai. 2009. What do we know
about conversation participants: Experiments on con-
versation entailment. In Proceedings of the SIGDIAL
2009 Conference, pages 206215.
</reference>
<page confidence="0.987119">
766
</page>
<figure confidence="0.244711">
\x0c&amp;quot;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.788950">
<note confidence="0.934670666666667">b&amp;quot;Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756766, MIT, Massachusetts, USA, 9-11 October 2010. c 2010 Association for Computational Linguistics</note>
<title confidence="0.991356">Towards Conversation Entailment: An Empirical Investigation</title>
<author confidence="0.999991">Chen Zhang Joyce Y Chai</author>
<affiliation confidence="0.9992435">Department of Computer Science and Engineering Michigan State University</affiliation>
<address confidence="0.99984">East Lansing, MI 48824, USA</address>
<email confidence="0.999366">zhangch6@cse.msu.edu</email>
<email confidence="0.999366">jchai@cse.msu.edu</email>
<abstract confidence="0.998840391304348">While a significant amount of research has been devoted to textual entailment, automated entailment from conversational scripts has received less attention. To address this limitation, this paper investigates the problem of conversation entailment: automated inference of hypotheses from conversation scripts. We examine two levels of semantic representations: a basic representation based on syntactic parsing from conversation utterances and an augmented representation taking into consideration of conversation structures. For each of these levels, we further explore two ways of capturing long distance relations between language constituents: implicit modeling based on the length of distance and explicit modeling based on actual patterns of relations. Our empirical findings have shown that the augmented representation with conversation structures is important, which achieves the best performance when combined with explicit modeling of long distance relations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The second pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="1519" citStr="Bar-Haim et al., 2006" startWordPosition="207" endWordPosition="210">ng into consideration of conversation structures. For each of these levels, we further explore two ways of capturing long distance relations between language constituents: implicit modeling based on the length of distance and explicit modeling based on actual patterns of relations. Our empirical findings have shown that the augmented representation with conversation structures is important, which achieves the best performance when combined with explicit modeling of long distance relations. 1 Introduction Textual entailment has received increasing attention in recent years (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Given a segment from a textual document, the task of textual entailment is to automatically determine whether a given hypothesis can be entailed from the segment. The capability of such kind of inference can benefit many text-based applications such as information extraction and question answering. Textual entailment has mainly focused on inference from written text in monologue. Recent years also observed an increasing amount of conversational data such as conversation scripts of meetings, call center records, cou</context>
<context position="4994" citStr="Bar-Haim et al., 2006" startWordPosition="732" endWordPosition="735">rns of relations. Our empirical findings have shown that augmented representation with conversation structures is important in conversation entailment. Combining conversation structures with explicit modeling of long distance relations results in the best performance. 2 Related Work Our work here is related to recent advances in textual entailment, automated processing of conversation scripts, and our initial investigation on conversation entailment. There is a large body of work on textual entailment initiated by the Pascal Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Different approaches have been developed, for example, based on logic proving (Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005) and graph match (Haghighi et al., 2005; de Salvo Braz et al., 2005; MacCartney et al., 2006). Supervised learning approaches have also been applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performan</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
</authors>
<title>The fifth pascal recognizing textual entailment challenge.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second Text Analysis Conference (TAC</booktitle>
<contexts>
<context position="1597" citStr="Bentivogli et al., 2009" startWordPosition="219" endWordPosition="222"> we further explore two ways of capturing long distance relations between language constituents: implicit modeling based on the length of distance and explicit modeling based on actual patterns of relations. Our empirical findings have shown that the augmented representation with conversation structures is important, which achieves the best performance when combined with explicit modeling of long distance relations. 1 Introduction Textual entailment has received increasing attention in recent years (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Given a segment from a textual document, the task of textual entailment is to automatically determine whether a given hypothesis can be entailed from the segment. The capability of such kind of inference can benefit many text-based applications such as information extraction and question answering. Textual entailment has mainly focused on inference from written text in monologue. Recent years also observed an increasing amount of conversational data such as conversation scripts of meetings, call center records, court proceedings, as well as online chatting. Although conversation is a form of</context>
<context position="5072" citStr="Bentivogli et al., 2009" startWordPosition="744" endWordPosition="747">tation with conversation structures is important in conversation entailment. Combining conversation structures with explicit modeling of long distance relations results in the best performance. 2 Related Work Our work here is related to recent advances in textual entailment, automated processing of conversation scripts, and our initial investigation on conversation entailment. There is a large body of work on textual entailment initiated by the Pascal Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Different approaches have been developed, for example, based on logic proving (Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005) and graph match (Haghighi et al., 2005; de Salvo Braz et al., 2005; MacCartney et al., 2006). Supervised learning approaches have also been applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progres</context>
</contexts>
<marker>Bentivogli, Dagan, Dang, Giampiccolo, Magnini, 2009</marker>
<rawString>Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth pascal recognizing textual entailment challenge. In Proceedings of the Second Text Analysis Conference (TAC 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising textual entailment with logical inference.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>628635</pages>
<contexts>
<context position="5199" citStr="Bos and Markert, 2005" startWordPosition="763" endWordPosition="766">ing of long distance relations results in the best performance. 2 Related Work Our work here is related to recent advances in textual entailment, automated processing of conversation scripts, and our initial investigation on conversation entailment. There is a large body of work on textual entailment initiated by the Pascal Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Different approaches have been developed, for example, based on logic proving (Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005) and graph match (Haghighi et al., 2005; de Salvo Braz et al., 2005; MacCartney et al., 2006). Supervised learning approaches have also been applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, resear</context>
</contexts>
<marker>Bos, Markert, 2005</marker>
<rawString>Johan Bos and Katja Markert. 2005. Recognising textual entailment with logical inference. In Proceedings of HLT-EMNLP, pages 628635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="1496" citStr="Dagan et al., 2005" startWordPosition="203" endWordPosition="206"> representation taking into consideration of conversation structures. For each of these levels, we further explore two ways of capturing long distance relations between language constituents: implicit modeling based on the length of distance and explicit modeling based on actual patterns of relations. Our empirical findings have shown that the augmented representation with conversation structures is important, which achieves the best performance when combined with explicit modeling of long distance relations. 1 Introduction Textual entailment has received increasing attention in recent years (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Given a segment from a textual document, the task of textual entailment is to automatically determine whether a given hypothesis can be entailed from the segment. The capability of such kind of inference can benefit many text-based applications such as information extraction and question answering. Textual entailment has mainly focused on inference from written text in monologue. Recent years also observed an increasing amount of conversational data such as conversation scripts of meetings, c</context>
<context position="4971" citStr="Dagan et al., 2005" startWordPosition="728" endWordPosition="731">ased on actual patterns of relations. Our empirical findings have shown that augmented representation with conversation structures is important in conversation entailment. Combining conversation structures with explicit modeling of long distance relations results in the best performance. 2 Related Work Our work here is related to recent advances in textual entailment, automated processing of conversation scripts, and our initial investigation on conversation entailment. There is a large body of work on textual entailment initiated by the Pascal Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Different approaches have been developed, for example, based on logic proving (Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005) and graph match (Haghighi et al., 2005; de Salvo Braz et al., 2005; MacCartney et al., 2006). Supervised learning approaches have also been applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, whi</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodrigo de Salvo Braz</author>
<author>Roxana Girju</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Mark Sammons</author>
</authors>
<title>An inference model for semantic entailment in natural language.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="5287" citStr="Braz et al., 2005" startWordPosition="780" endWordPosition="783"> is related to recent advances in textual entailment, automated processing of conversation scripts, and our initial investigation on conversation entailment. There is a large body of work on textual entailment initiated by the Pascal Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Different approaches have been developed, for example, based on logic proving (Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005) and graph match (Haghighi et al., 2005; de Salvo Braz et al., 2005; MacCartney et al., 2006). Supervised learning approaches have also been applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation data to acquire useful info</context>
</contexts>
<marker>Braz, Girju, Punyakanok, Roth, Sammons, 2005</marker>
<rawString>Rodrigo de Salvo Braz, Roxana Girju, Vasin Punyakanok, Dan Roth, and Mark Sammons. 2005. An inference model for semantic entailment in natural language. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
<author>Julia Hirschberg</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: Use of bayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>669676</pages>
<contexts>
<context position="6175" citStr="Galley et al., 2004" startWordPosition="909" endWordPosition="912">of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about con</context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>Michel Galley, Kathleen McKeown, Julia Hirschberg, and Elizabeth Shriberg. 2004. Identifying agreement and disagreement in conversational speech: Use of bayesian networks to model pragmatic dependencies. In Proceedings of ACL, pages 669676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cNikesh Garera</author>
<author>David Yarowsky</author>
</authors>
<title>Modeling latent biographic attributes in conversational genres.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>710718</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="6061" citStr="Garera and Yarowsky, 2009" startWordPosition="892" endWordPosition="895">nzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consis</context>
</contexts>
<marker>Garera, Yarowsky, 2009</marker>
<rawString>\x0cNikesh Garera and David Yarowsky. 2009. Modeling latent biographic attributes in conversational genres. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 710718, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The third pascal recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>19</pages>
<contexts>
<context position="1545" citStr="Giampiccolo et al., 2007" startWordPosition="211" endWordPosition="214">f conversation structures. For each of these levels, we further explore two ways of capturing long distance relations between language constituents: implicit modeling based on the length of distance and explicit modeling based on actual patterns of relations. Our empirical findings have shown that the augmented representation with conversation structures is important, which achieves the best performance when combined with explicit modeling of long distance relations. 1 Introduction Textual entailment has received increasing attention in recent years (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Given a segment from a textual document, the task of textual entailment is to automatically determine whether a given hypothesis can be entailed from the segment. The capability of such kind of inference can benefit many text-based applications such as information extraction and question answering. Textual entailment has mainly focused on inference from written text in monologue. Recent years also observed an increasing amount of conversational data such as conversation scripts of meetings, call center records, court proceedings, as well as</context>
<context position="5020" citStr="Giampiccolo et al., 2007" startWordPosition="736" endWordPosition="739">mpirical findings have shown that augmented representation with conversation structures is important in conversation entailment. Combining conversation structures with explicit modeling of long distance relations results in the best performance. 2 Related Work Our work here is related to recent advances in textual entailment, automated processing of conversation scripts, and our initial investigation on conversation entailment. There is a large body of work on textual entailment initiated by the Pascal Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Different approaches have been developed, for example, based on logic proving (Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005) and graph match (Haghighi et al., 2005; de Salvo Braz et al., 2005; MacCartney et al., 2006). Supervised learning approaches have also been applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants </context>
<context position="20470" citStr="Giampiccolo et al., 2007" startWordPosition="3214" endWordPosition="3217">gnment between two nouns or two verbs. We trained an alignment model for nouns and one for verbs separately. Table 1 summarizes a set of features used in the alignment models. Most of these features are shared by the model for noun alignment and the model for verb alignment. These features include whether the two strings are the same, two terms have the same stem, the similarity between the two terms either based on WordNet or distributional statistics (Lin, 1998). To learn the alignment model for nouns, we annotated the noun alignments for the development data used in PASCAL RTE-3 Challenge (Giampiccolo et al., 2007) and trained a logistic regression model based on the features in Table 1. Cross-validation on the same dataset shows relatively satisfying performance (96.4% precision and 94.9% recall). In this paper, we focus on the alignment between verbs Noun Verb Align. Align. Verb be identification X String equality X X Stemmed equality X X Acronym equality X Named entity equality X WordNet similarity X X Distributional similarity X X Subject consistency X Object consistency X Table 1: Features for alignment models since it appears more difficult. A major difference between noun alignment and verb align</context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing, pages 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Hoa Trang Dang</author>
<author>Bernardog Magnini</author>
<author>Ido Dagan</author>
<author>Elena Cabrio</author>
<author>Bill Dolan</author>
</authors>
<title>The fourth pascal recognizing textual entailment challenge.</title>
<date>2008</date>
<booktitle>In Proceedings of the First Text Analysis Conference (TAC</booktitle>
<contexts>
<context position="1571" citStr="Giampiccolo et al., 2008" startWordPosition="215" endWordPosition="218"> For each of these levels, we further explore two ways of capturing long distance relations between language constituents: implicit modeling based on the length of distance and explicit modeling based on actual patterns of relations. Our empirical findings have shown that the augmented representation with conversation structures is important, which achieves the best performance when combined with explicit modeling of long distance relations. 1 Introduction Textual entailment has received increasing attention in recent years (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Given a segment from a textual document, the task of textual entailment is to automatically determine whether a given hypothesis can be entailed from the segment. The capability of such kind of inference can benefit many text-based applications such as information extraction and question answering. Textual entailment has mainly focused on inference from written text in monologue. Recent years also observed an increasing amount of conversational data such as conversation scripts of meetings, call center records, court proceedings, as well as online chatting. Although</context>
<context position="5046" citStr="Giampiccolo et al., 2008" startWordPosition="740" endWordPosition="743">wn that augmented representation with conversation structures is important in conversation entailment. Combining conversation structures with explicit modeling of long distance relations results in the best performance. 2 Related Work Our work here is related to recent advances in textual entailment, automated processing of conversation scripts, and our initial investigation on conversation entailment. There is a large body of work on textual entailment initiated by the Pascal Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Different approaches have been developed, for example, based on logic proving (Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005) and graph match (Haghighi et al., 2005; de Salvo Braz et al., 2005; MacCartney et al., 2006). Supervised learning approaches have also been applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results in</context>
</contexts>
<marker>Giampiccolo, Dang, Magnini, Dagan, Cabrio, Dolan, 2008</marker>
<rawString>Danilo Giampiccolo, Hoa Trang Dang, Bernardog Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan. 2008. The fourth pascal recognizing textual entailment challenge. In Proceedings of the First Text Analysis Conference (TAC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward Holliman</author>
</authors>
<date>1997</date>
<booktitle>Switchboard-1 Release 2. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="13562" citStr="Godfrey and Holliman, 1997" startWordPosition="2129" endWordPosition="2133">tion structure across turns and utterances. We call it an augmented representation. Figure 2(a) shows the augmented structures of the conversation segment and Figure 2(b) shows the corresponding clause representation. Compared to the basic representation, there are two additional types of vertices (i.e., terms) highlighted in the figures: Vertices representing utterances (e.g., u1 . . . u4). Their corresponding terms capture the dialogue acts for the utterances (e.g. u1 = yes no question). To focus our effort, currently we only apply annotated dialogue acts provided in the Switchboard corpus (Godfrey and Holliman, 1997). Two edges are added to connect different utterances. The first edge connects each utterance vertex to the head of the corresponding utterance to indicate the specific content of the utterance (e.g., content(u1, x3)). The second edge connects an utterance to its succeeding utterance to indicate the temporal progression of the conversation (e.g., follow(u2, u1)). Vertices representing speakers or participants (e.g., sA, sB). One edge is added to connect each utterance to its speaker (e.g., speaker(u1, sB)). Note that since our clause representations are mainly based on the dependency relations</context>
</contexts>
<marker>Godfrey, Holliman, 1997</marker>
<rawString>John J. Godfrey and Edward Holliman. 1997. Switchboard-1 Release 2. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Andrew Ng</author>
<author>Christopher Manning</author>
</authors>
<title>Robust textual inference via graph matching.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>387394</pages>
<contexts>
<context position="5259" citStr="Haghighi et al., 2005" startWordPosition="774" endWordPosition="777">ce. 2 Related Work Our work here is related to recent advances in textual entailment, automated processing of conversation scripts, and our initial investigation on conversation entailment. There is a large body of work on textual entailment initiated by the Pascal Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Different approaches have been developed, for example, based on logic proving (Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005) and graph match (Haghighi et al., 2005; de Salvo Braz et al., 2005; MacCartney et al., 2006). Supervised learning approaches have also been applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation</context>
</contexts>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>Aria Haghighi, Andrew Ng, and Christopher Manning. 2005. Robust textual inference via graph matching. In Proceedings of HLT-EMNLP, pages 387394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>Extracting social networks and biographical facts from conversational speech transcripts.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>10401047</pages>
<contexts>
<context position="6098" citStr="Jing et al., 2007" startWordPosition="899" endWordPosition="902">nt RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hy</context>
</contexts>
<marker>Jing, Kambhatla, Roukos, 2007</marker>
<rawString>Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2007. Extracting social networks and biographical facts from conversational speech transcripts. In Proceedings of ACL, pages 10401047.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL 03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>423430</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="12237" citStr="Klein and Manning, 2003" startWordPosition="1926" endWordPosition="1929">on structures (e.g., speakers and dialogue 758 \x0cacts). Next we use Example 2 to illustrate these representations. Example 2: Conversation Segment: B: Have you seen Sleeping with the Enemy? A: No. Ive heard thats really great, though. B: You have to go see that one. Hypothesis: B suggests A to watch Sleeping with the Enemy. 4.1 Basic Representation The first representation is based on the syntactic parsing from conversation utterances and we call it a basic representation. Figure 1(a) shows an example of dependency structures for several utterances that are derived from the Stanford parser (Klein and Manning, 2003), and Figure 1(b) shows the corresponding clause representation. In the dependency structure, the vertices represent entities (e.g., x1) and actions (e.g., x3) within an utterance. They correspond to terms in the clause representation. An edge between vertices captures a dependency relation and is represented as predicates in the clause representation. For example, the edge between x1 and x3 indicates x1 is the subject of x3, which is represented by the clause representation subj(x3, x1). Similar representation also applies to the hypothesis as shown in Figure 1(c), 1(d). 4.2 Augmented Represe</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In ACL 03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 423430, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of International Conference on Machine Learning,</booktitle>
<pages>296304</pages>
<contexts>
<context position="20313" citStr="Lin, 1998" startWordPosition="3190" endWordPosition="3191">ications in Alignment Model Although a noun and a verb can potentially be aligned, to simplify the problem, we restrict the problem to the alignment between two nouns or two verbs. We trained an alignment model for nouns and one for verbs separately. Table 1 summarizes a set of features used in the alignment models. Most of these features are shared by the model for noun alignment and the model for verb alignment. These features include whether the two strings are the same, two terms have the same stem, the similarity between the two terms either based on WordNet or distributional statistics (Lin, 1998). To learn the alignment model for nouns, we annotated the noun alignments for the development data used in PASCAL RTE-3 Challenge (Giampiccolo et al., 2007) and trained a logistic regression model based on the features in Table 1. Cross-validation on the same dataset shows relatively satisfying performance (96.4% precision and 94.9% recall). In this paper, we focus on the alignment between verbs Noun Verb Align. Align. Verb be identification X String equality X X Stemmed equality X X Acronym equality X Named entity equality X WordNet similarity X X Distributional similarity X X Subject consis</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of International Conference on Machine Learning, pages 296304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to recognize features of valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>4148</pages>
<marker>MacCartney, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D. Manning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of HLT-NAACL, pages 4148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Giuseppe Carenini</author>
</authors>
<title>Summarizing spoken and written conversations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>773782</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="6277" citStr="Murray and Carenini, 2008" startWordPosition="923" endWordPosition="926">e that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, t</context>
</contexts>
<marker>Murray, Carenini, 2008</marker>
<rawString>Gabriel Murray and Giuseppe Carenini. 2008. Summarizing spoken and written conversations. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 773782, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
</authors>
<title>Towards robust semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="19305" citStr="Pradhan et al., 2008" startWordPosition="3025" endWordPosition="3028">1 . . . vl1el1vl, where v1, . . . , vl are the vertices on the path and e1, . . . , el1 are the edges. Each vi describes the type of the vertex in the dependency structure, which is either a noun (N), a verb (V ), or an utterance (U). Each ei describes whether the edge is forward () or backward (). For example, in Figure 2(a), the path from x11 to x9 is V V V N. This kind of string representation of paths in syntactic parse is known as a way of modeling shallow semantics between any two constituents in a language structure. It is largely used in other NLP tasks such as semantic role labeling (Pradhan et al., 2008). The difference here is our paths are extracted from dependency parses as opposed to traditional constituent parses, and our paths also incorporate the representation of conversation structures (e.g., utterances and speakers). 6 Applications in Entailment Models In this section we describe how different representations and modeling of LDR are used in the alignment and inference models. 6.1 Applications in Alignment Model Although a noun and a verb can potentially be aligned, to simplify the problem, we restrict the problem to the alignment between two nouns or two verbs. We trained an alignme</context>
</contexts>
<marker>Pradhan, Ward, Martin, 2008</marker>
<rawString>Sameer S. Pradhan, Wayne Ward, and James H. Martin. 2008. Towards robust semantic role labeling. Computational Linguistics, 34(2):289310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Robust textual inference via learning and abductive reasoning.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>10991105</pages>
<contexts>
<context position="5220" citStr="Raina et al., 2005" startWordPosition="767" endWordPosition="770">lations results in the best performance. 2 Related Work Our work here is related to recent advances in textual entailment, automated processing of conversation scripts, and our initial investigation on conversation entailment. There is a large body of work on textual entailment initiated by the Pascal Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Different approaches have been developed, for example, based on logic proving (Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005) and graph match (Haghighi et al., 2005; de Salvo Braz et al., 2005; MacCartney et al., 2006). Supervised learning approaches have also been applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigat</context>
</contexts>
<marker>Raina, Ng, Manning, 2005</marker>
<rawString>Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005. Robust textual inference via learning and abductive reasoning. In Proceedings of AAAI, pages 10991105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of twitter conversations.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>172180</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="6386" citStr="Ritter et al., 2010" startWordPosition="940" endWordPosition="943">on data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions. We developed an approach that i</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 172180, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Josef Ruppenhofer</author>
<author>Janyce Wiebe</author>
</authors>
<title>Detecting arguing and sentiment in meetings.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Antwerp,</location>
<contexts>
<context position="5955" citStr="Somasundaran et al., 2007" startWordPosition="876" endWordPosition="880">ning approaches have also been applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial in</context>
</contexts>
<marker>Somasundaran, Ruppenhofer, Wiebe, 2007</marker>
<rawString>Swapna Somasundaran, Josef Ruppenhofer, and Janyce Wiebe. 2007. Detecting arguing and sentiment in meetings. In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue, Antwerp, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
<author>Josef Ruppenhofer</author>
</authors>
<title>Discourse level opinion interpretation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>801808</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="5982" citStr="Somasundaran et al., 2008" startWordPosition="881" endWordPosition="884">een applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation</context>
</contexts>
<marker>Somasundaran, Wiebe, Ruppenhofer, 2008</marker>
<rawString>Swapna Somasundaran, Janyce Wiebe, and Josef Ruppenhofer. 2008. Discourse level opinion interpretation. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 801808, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Galileo Namata</author>
<author>Janyce Wiebe</author>
<author>Lise Getoor</author>
</authors>
<title>Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>170179</pages>
<location>Singapore,</location>
<contexts>
<context position="6010" citStr="Somasundaran et al., 2009" startWordPosition="885" endWordPosition="888">similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collect</context>
</contexts>
<marker>Somasundaran, Namata, Wiebe, Getoor, 2009</marker>
<rawString>Swapna Somasundaran, Galileo Namata, Janyce Wiebe, and Lise Getoor. 2009. Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 170179, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Tatu</author>
<author>Dan Moldovan</author>
</authors>
<title>A semantic approach to recognizing textual entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>371378</pages>
<contexts>
<context position="5176" citStr="Tatu and Moldovan, 2005" startWordPosition="759" endWordPosition="762">tures with explicit modeling of long distance relations results in the best performance. 2 Related Work Our work here is related to recent advances in textual entailment, automated processing of conversation scripts, and our initial investigation on conversation entailment. There is a large body of work on textual entailment initiated by the Pascal Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Different approaches have been developed, for example, based on logic proving (Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005) and graph match (Haghighi et al., 2005; de Salvo Braz et al., 2005; MacCartney et al., 2006). Supervised learning approaches have also been applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data be</context>
</contexts>
<marker>Tatu, Moldovan, 2005</marker>
<rawString>Marta Tatu and Dan Moldovan. 2005. A semantic approach to recognizing textual entailment. In Proceedings of HLT-EMNLP, pages 371378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Automatic learning of textual entailments with cross-pair similarities.</title>
<date>2006</date>
<booktitle>In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>401408</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5462" citStr="Zanzotto and Moschitti, 2006" startWordPosition="805" endWordPosition="808"> is a large body of work on textual entailment initiated by the Pascal Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Different approaches have been developed, for example, based on logic proving (Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005) and graph match (Haghighi et al., 2005; de Salvo Braz et al., 2005; MacCartney et al., 2006). Supervised learning approaches have also been applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006). In the most recent RTE Challenge (Bentivogli et al., 2009), the best system achieves 73.5% of accuracy, while the median performance among all participants is 60.4%. These results indicate that, while progress has been made, textual entailment remains a challenging problem. As more and more conversation data becomes available, researchers have investigated automated processing of conversation data to acquire useful information, for example, related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009),</context>
</contexts>
<marker>Zanzotto, Moschitti, 2006</marker>
<rawString>Fabio Massimo Zanzotto and Alessandro Moschitti. 2006. Automatic learning of textual entailments with cross-pair similarities. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 401408, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Zhang</author>
<author>Joyce Chai</author>
</authors>
<title>What do we know about conversation participants: Experiments on conversation entailment.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL 2009 Conference,</booktitle>
<pages>206215</pages>
<contexts>
<context position="2972" citStr="Zhang and Chai, 2009" startWordPosition="425" endWordPosition="428"> grounding between participants, different linguistic phenomena of utterances, and conversation implicatures. Traditional approaches dealing with textual entailment were not designed to handle these unique conversation behaviors and thus to support automated entailment from conversation scripts. Example 1: Conversation Segment: B: My mother also was very very independent. She had her own, still had her own little house and still driving her own car, A: Yeah. B: at age eighty-three. Hypothesis: (1) Bs mother is eighty-three. (2) B is eighty-three. To address this limitation, our previous work (Zhang and Chai, 2009) has initiated an investigation on the problem of conversation entailment. The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H. For instance, as in Example 1, the first hypothesis can be entailed from the 756 \x0cconversation segment while the second hypothesis cannot. While our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data. It is not clear whether t</context>
<context position="6529" citStr="Zhang and Chai, 2009" startWordPosition="961" endWordPosition="964"> related to opinions (Somasundaran et al., 2007; Somasundaran et al., 2008; Somasundaran et al., 2009), biographic attributes (Garera and Yarowsky, 2009), social networks (Jing et al., 2007), and agreements and disagreements between participants (Galley et al., 2004). Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al., 2010). Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work (Zhang and Chai, 2009), we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions. We developed an approach that is motivated by previous work on textual entailment. We use clauses in the logic-based approaches as the underlying representation of our system</context>
<context position="7871" citStr="Zhang and Chai, 2009" startWordPosition="1162" endWordPosition="1165"> textual entailment: an alignment stage followed by an entailment stage. Building upon our previous work, in this paper, we systematically examine different representations of the conversation segment and different modeling of long distance relations between language constituents. We compare the roles of these different representations on the performance of entailment prediction using a larger testing dataset that was not previously evaluated. This analysis allows better understanding of the problem and provides insight on 757 \x0cpotential solutions. 3 Overall Framework In our previous work (Zhang and Chai, 2009), conversation entailment is formulated as the following: given a conversation segment D which is represented by a set of clauses D = d1 . . . dm, and a hypothesis H represented by another set of clauses H = h1 . . . hn, the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 . . . dm as follows. This is based on a simple assumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses. P(D \x0f H|D, H) = P(D \x0f h1, . . . </context>
<context position="22987" citStr="Zhang and Chai, 2009" startWordPosition="3644" endWordPosition="3647">rb and its aligned object is used as a measure of the object consistency. In Example 2, to decide whether the conversation term see (x11 in Figure 1(a), 1(b), and 2) and the hypothesis term watch (x4 in Figure 1(c), 1(d)) should be aligned, we first identify the subject of x4 in the hypothesis, which is x2 (A). We then look for 761 \x0cx2s alignments in the conversation segment, among which x9 (You) is the closest to x11 (see). In Figure 2(a), we find the distance between x11 and x9 is 3. Using the implicit modeling of argument consistency, we follow the same approach as in our previous work (Zhang and Chai, 2009) and trained a logistic regression model to predict verb alignment based on the features in Table 1. 6.1.2 Explicit Modeling of AC The second approach captures argument consistency based on explicit modeling of the relationship between a verb and its aligned subject (or object). Given a pair of verb terms (x, y), let sy be the subject of y and sx be the aligned entity of sy in the conversation closest to x, we use the string describing the path from x to sx as the feature to capture subject consistency. For example, in Figure 2(a), the path from x11 to x9 is V V V N. This string representation</context>
<context position="24653" citStr="Zhang and Chai, 2009" startWordPosition="3934" endWordPosition="3937">easure the closeness between any two verbs. Again this model is trained from our development data described in Zhang and Chai (2009). Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x5 = suggests from the hypothesis and u4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act. This alignment is obtained by following the same set of rules learned from the development dataset as in (Zhang and Chai, 2009). 6.2 Applications in Inference Model As mentioned earlier, once an alignment is established, the inference model is to predict whether each clause in the hypothesis is entailed from the conversation segment. Two separate models were x4=have x5=A x2=Sleeping with the Enemy x1=A x7=is really great x10=one u4=opinion Conversation Segment x3=Sleeping with the Enemy x5=suggests x2=A x1=B x4=watch Hypothesis x6=that x11=see x12=go x13=have x8=have heard u3=statement u2=no_answer u1=yes_no_question x3=seen x9=A sB sA Figure 3: The alignment result for Example 2 used to handle the inference of proper</context>
<context position="29092" citStr="Zhang and Chai, 2009" startWordPosition="4681" endWordPosition="4684">augmented representation Figure 4: Evaluation of verb alignment were categorized into four types: (1) fact: profile and social relations of conversation participants (accounted for 47% of the development data and 49% of the testing data); (2) belief: participants beliefs and opinions (34% and 35%); (3) desire: participants desire of certain actions or outcomes (11% and 4%); (4) intent: communicative intent that captures some perlocutionary force from one participant to the other (e.g. A stops B from doing something; A disagreees with B on something, 8% and 12%) Note that in our original work (Zhang and Chai, 2009), only development data were used to show some initial observations. Here we trained our models on the development data and results shown are from the testing data. 7.1 Evaluation of Alignment Models The evaluation of alignment models is based on pairwise decision. For each pair of terms (x, y), where x is from a conversation segment and y is from a hypothesis, we measure whether the model correctly predicts that the two terms should or should not be aligned. Because the alignment classification has extremely unbalanced classes, we use precisionrecall of true alignments as evaluation metrics. </context>
</contexts>
<marker>Zhang, Chai, 2009</marker>
<rawString>Chen Zhang and Joyce Chai. 2009. What do we know about conversation participants: Experiments on conversation entailment. In Proceedings of the SIGDIAL 2009 Conference, pages 206215.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>