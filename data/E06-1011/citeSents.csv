 Previous work has shown that conditioning on neighboring decisions can lead to significant improvements in accuracy (CITATION; CITATION),,
 The second-order model allows us to condition on the most recent parsing decision, that is, the last dependent picked up by a particular word, which is analogous to the the Markov conditioning of in the Charniak parser CITATION,,
2 Exact Projective Parsing For projective MST parsing, the first-order algorithm can be extended to the second-order case, as was noted by CITATION,,
 Though less common than trees, dependency graphs involving multiple parents are well established in the literature CITATION,,
 Unfortunately, the problem of finding the dependency structure with highest score in this setting is intractable CITATION,,
 proposed by CITATIONc),,
 This formulation leads to efficient parsing algorithms for both projective and non-projective dependency trees with the Eisner algorithm CITATION and the Chu-Liu-Edmonds algorithm (CITATION; CITATION) respectively,,
 This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs CITATION,,
 Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment CITATION, sequence analysis (DaumCITATION; CITATIONa) and phrase-structure parsing CITATION,,
2 Czech Results For the Czech data, we used the predefined training, development and testing split of the Prague Dependency Treebank CITATION, and the automatically generated POS tags supplied with the data, which we reduce to the POS tag set from CITATION,,
 CITATIONc) showed a substantial improvement in accuracy by modeling nonprojective edges in Czech, shown by the difference between two first-orde,,
 The algorithm is an extension of the Margin Infused Relaxed Algorithm (MIRA) CITATION to learning with structured outputs, in the present case dependency structures,,
 We use the common method of setting the final weight vector as the average of the weight vectors after each iteration CITATION, which has been shown to alleviate overfitting,,
 We parse this instance to obtain a predicted dependency graph, and find the smallest-norm update to the weight vector w that ensures that the training graph outscores the predicted graph by a margin proportional to the loss of the predicted graph relative to the training graph, which is the number of words with incorrect parents in the predicted tree (CITATIONb),,
est scoring tree (CITATIONb) or even all possible trees by using factored representations (CITATION; CITATIONc),,
 This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs CITATION,,
 Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment CITATION, sequence analysis (DaumCITATION; CITATIONa) and phrase-structure parsing CITATION,,
 4 Online Learning and Approximate Inference In this section, we review the work of CITATIONb) for online large-margin dependency 2 We are not concerned with violating the tree constraint,,
 The algorithm is an extension of the Margin Infused Relaxed Algorithm (MIRA) CITATION to learning with structured outputs, in the present case dependency structures,,
 We use the common method of setting the final weight vector as the average of the weight vectors after each iteration CITATION, which has been shown to alleviate overfitting,,
 This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs CITATION,,
 Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment CITATION, sequence analysis (DaumCITATION; CITATIONa) and phrase-structure parsing CITATION,,
 proposed by CITATIONc),,
 This formulation leads to efficient parsing algorithms for both projective and non-projective dependency trees with the Eisner algorithm CITATION and the Chu-Liu-Edmonds algorithm (CITATION; CITATION) respectively,,
 proposed by CITATIONc),,
 This formulation leads to efficient parsing algorithms for both projective and non-projective dependency trees with the Eisner algorithm CITATION and the Chu-Liu-Edmonds algorithm (CITATION; CITATION) respectively,,
 The second-order model allows us to condition on the most recent parsing decision, that is, the last dependent picked up by a particular word, which is analogous to the the Markov conditioning of in the Charniak parser CITATION,,
2 Exact Projective Parsing For projective MST parsing, the first-order algorithm can be extended to the second-order case, as was noted by CITATION,,
 By splitting up chart items into left and right components, the Eisner algorithm only requires 3 indices to be maintained at each step, as discussed in detail elsewhere (CITATION; CITATIONb),,
2 Czech Results For the Czech data, we used the predefined training, development and testing split of the Prague Dependency Treebank CITATION, and the automatically generated POS tags supplied with the data, which we reduce to the POS tag set from CITATION,,
 CITATIONc) showe,,
um spanning tree (MST) dependency parsing framework of CITATIONc) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word,,
 1 Introduction Dependency representations of sentences (CITATION; CITATION) model head-dependent syntactic relations as edges in a directed graph,,
 Though less common than trees, dependency graphs involving multiple parents are well established in the literature CITATION,,
 Unfortunately, the problem of finding the dependency structure with highest score in this setting is intractable CITATION,,
 3 Danish: Parsing Secondary Parents CITATION argued for a dependency formalism called Discontinuous Grammar and annotated a large set of Danish sentences using this formalism to create the Danish Dependency Treebank CITATION,,
 3 Danish: Parsing Secondary Parents CITATION argued for a dependency formalism called Discontinuous Grammar and annotated a large set of Danish sentences using this formalism to create the Danish Dependency Treebank CITATION,,
 Though trees are 1 Examples are drawn from CITATIONc),,
 more common, some formalisms allow for words to modify multiple parents CITATION,,
 Recently, CITATIONc) have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree (MST) in a graph yields efficient algorithms for both projective and non-projective trees,,
 proposed by CITATIONc),,
 This formulation leads to efficient parsing algorithms for both projective and non-projective dependency trees with the Eisner algorithm CITATION and the Chu-Liu-Edmonds algorithm (CITATION; CITATION) respectively,,
 By defining a graph in which the words in a sentence are the vertices and there is a directed edge between all words with a score as calculated above, CITATIONc) showed that dependency parsing is equivalent to finding the MST in this graph,,
parsing, the first-order algorithm can be extended to the second-order case, as was noted by CITATION,,
 By splitting up chart items into left and right components, the Eisner algorithm only requires 3 indices to be maintained at each step, as discussed in detail elsewhere (CITATION; CITATIONb),,
 4 Online Learning and Approximate Inference In this section, we review the work of CITATIONb) for online large-margin dependency 2 We are not concerned with violating the tree constraint,,
 The algorithm is an extension of the Margin Infused Relaxed Algorithm (MIRA) CITATION to learning with structured outputs, in the present case dependency structures,,
the common method of setting the final weight vector as the average of the weight vectors after each iteration CITATION, which has been shown to alleviate overfitting,,
 We parse this instance to obtain a predicted dependency graph, and find the smallest-norm update to the weight vector w that ensures that the training graph outscores the predicted graph by a margin proportional to the loss of the predicted graph relative to the training graph, which is the number of words with incorrect parents in the predicted tree (CITATIONb),,
 Past work on tree-structured outputs has used constraints for the k-best scoring tree (CITATIONb) or even all possible trees by using factored representations (CITATION; CITATIONc),,
 As noted earlier, this representation subsumes the first-order representation of CITATIONb), so we can incorporate all of their features as well as the new second-order features we now describe,,
Treebank CITATION, and the automatically generated POS tags supplied with the data, which we reduce to the POS tag set from CITATION,,
 CITATIONc) showed a substantial improvement in accuracy by modeling nonprojective edges in Czech, shown by the difference between two first-order models,,
 Though trees are 1 Examples are drawn from CITATIONc),,
 more common, some formalisms allow for words to modify multiple parents CITATION,,
 Recently, CITATIONc) have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree (MST) in a graph yields efficient algorithms for both projective and non-projective trees,,
 proposed by CITATIONc),,
 This formulation leads to efficient parsing algorithms for both projective and non-projective dependency trees with the Eisner algorithm CITATION and the Chu-Liu-Edmonds algorithm (CITATION; CITATION) respectively,,
 By defining a graph in which the words in a sentence are the vertices and there is a directed edge between all words with a score as calculated above, CITATIONc) showed that dependency parsing is equivalent to finding the MST in this graph,,
parsing, the first-order algorithm can be extended to the second-order case, as was noted by CITATION,,
 By splitting up chart items into left and right components, the Eisner algorithm only requires 3 indices to be maintained at each step, as discussed in detail elsewhere (CITATION; CITATIONb),,
 4 Online Learning and Approximate Inference In this section, we review the work of CITATIONb) for online large-margin dependency 2 We are not concerned with violating the tree constraint,,
 The algorithm is an extension of the Margin Infused Relaxed Algorithm (MIRA) CITATION to learning with structured outputs, in the present case dependency structures,,
the common method of setting the final weight vector as the average of the weight vectors after each iteration CITATION, which has been shown to alleviate overfitting,,
 We parse this instance to obtain a predicted dependency graph, and find the smallest-norm update to the weight vector w that ensures that the training graph outscores the predicted graph by a margin proportional to the loss of the predicted graph relative to the training graph, which is the number of words with incorrect parents in the predicted tree (CITATIONb),,
 Past work on tree-structured outputs has used constraints for the k-best scoring tree (CITATIONb) or even all possible trees by using factored representations (CITATION; CITATIONc),,
 As noted earlier, this representation subsumes the first-order representation of CITATIONb), so we can incorporate all of their features as well as the new second-order features we now describe,,
Treebank CITATION, and the automatically generated POS tags supplied with the data, which we reduce to the POS tag set from CITATION,,
 CITATIONc) showed a substantial improvement in accuracy by modeling nonprojective edges in Czech, shown by the difference between two first-order models,,
 Though trees are 1 Examples are drawn from CITATIONc),,
 more common, some formalisms allow for words to modify multiple parents CITATION,,
 Recently, CITATIONc) have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree (MST) in a graph yields efficient algorithms for both projective and non-projective trees,,
 proposed by CITATIONc),,
 This formulation leads to efficient parsing algorithms for both projective and non-projective dependency trees with the Eisner algorithm CITATION and the Chu-Liu-Edmonds algorithm (CITATION; CITATION) respectively,,
 By defining a graph in which the words in a sentence are the vertices and there is a directed edge between all words with a score as calculated above, CITATIONc) showed that dependency parsing is equivalent to finding the MST in this graph,,
parsing, the first-order algorithm can be extended to the second-order case, as was noted by CITATION,,
 By splitting up chart items into left and right components, the Eisner algorithm only requires 3 indices to be maintained at each step, as discussed in detail elsewhere (CITATION; CITATIONb),,
 4 Online Learning and Approximate Inference In this section, we review the work of CITATIONb) for online large-margin dependency 2 We are not concerned with violating the tree constraint,,
 The algorithm is an extension of the Margin Infused Relaxed Algorithm (MIRA) CITATION to learning with structured outputs, in the present case dependency structures,,
the common method of setting the final weight vector as the average of the weight vectors after each iteration CITATION, which has been shown to alleviate overfitting,,
 We parse this instance to obtain a predicted dependency graph, and find the smallest-norm update to the weight vector w that ensures that the training graph outscores the predicted graph by a margin proportional to the loss of the predicted graph relative to the training graph, which is the number of words with incorrect parents in the predicted tree (CITATIONb),,
 Past work on tree-structured outputs has used constraints for the k-best scoring tree (CITATIONb) or even all possible trees by using factored representations (CITATION; CITATIONc),,
 As noted earlier, this representation subsumes the first-order representation of CITATIONb), so we can incorporate all of their features as well as the new second-order features we now describe,,
Treebank CITATION, and the automatically generated POS tags supplied with the data, which we reduce to the POS tag set from CITATION,,
 CITATIONc) showed a substantial improvement in accuracy by modeling nonprojective edges in Czech, shown by the difference between two first-order models,,
ee (MST) dependency parsing framework of CITATIONc) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word,,
 1 Introduction Dependency representations of sentences (CITATION; CITATION) model head-dependent syntactic relations as edges in a directed graph,,
 This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs CITATION,,
 Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment CITATION, sequence analysis (DaumCITATION; CITATIONa) and phrase-structure parsing CITATION,,
 We can easily motivate this approximation by observing that even in non-projective languages like Czech and Danish, most trees are primarily projective with just a few non-projective edges CITATION,,
1 English Results To create data sets for English, we used the CITATION head rules to extract dependency trees from the WSJ, setting sections 2-21 as training, section 22 for development and section 23 for evaluation,,
 The models rely on part-of-speech tags as input and we used the CITATION tagger to provide these for the development and evaluation set,,
 the weight vector w that ensures that the training graph outscores the predicted graph by a margin proportional to the loss of the predicted graph relative to the training graph, which is the number of words with incorrect parents in the predicted tree (CITATIONb),,
 Past work on tree-structured outputs has used constraints for the k-best scoring tree (CITATIONb) or even all possible trees by using factored representations (CITATION; CITATIONc),,
 This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs CITATION,,
 Previous work has shown that conditioning on neighboring decisions can lead to significant improvements in accuracy (CITATION; CITATION),,
 CITATION showed that keeping a small amount of parsing history was crucial to improving parsing performance for their locally-trained shift-reduce SVM parser,,
1 English Results To create data sets for English, we used the CITATION head rules to extract dependency trees from the WSJ, setting sections 2-21 as training, section 22 for development and section 23 for evaluation,,
 The models rely on part-of-speech tags as input and we used the CITATION tagger to provide these for the development and evaluation set,,
