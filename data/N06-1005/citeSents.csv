For example, two high-accuracy systems are those described in CITATION, achieving 60.4% accuracy with no task-specific information, and CITATION, which achieves 61.2% task-dependent accuracy, i.e,,
The chief difference in our algorithm is that we generate the surface text search strings from the parsed logical forms using the generation capabilities of NLPWIN CITATION, and we verify that the syntactic relations in each discovered web snippet are isomorphic to those in the original candidate paraphrase template,,
xample, two high-accuracy systems are those described in CITATION, achieving 60.4% accuracy with no task-specific information, and CITATION, which achieves 61.2% task-dependent accuracy, i.e,,
aphrase 0.0053 0.0000 Lexical Similarity 0.0053 0.0000 Value Match 0.0017 0.0013 Acronym Match 0.0017 0.0013 Adjectival Form7 0.0000 0.0063 False Entailment Feature Dev Test Negation Mismatch 0.0106 0.0025 Argument Movement 0.0070 0.0250 Conditional Mismatch 0.0053 0.0037 Modal Mismatch 0.0035 0.0013 Superlative Mismatch 0.0035 -0.0025 Entity Mismatch 0.0018 0.0063 Table 3: Feature ablation study; quantity is the accuracy loss obtained by removal of single feature test set CWS of 0.6534 is higher than previouslyreported task-independent systems (however, the task-dependent system reported in CITATION achieves a CWS of 0.686),,
We would expect a higher CWS to result from learning a more appropriate confidence function; nonetheless our overall 6 As in CITATION we compute the confidenceweighted score (or average precision) over n examples {c1, c2, ..., cn} ranked in order of decreasing confidence as cws = 1 n Pn i=1 (#correct-up-to-rank-i) i Dev Set Test Set Task acc cws acc cws CD 0.8061 0.8357 0.7867 0.8261 RC 0.5534 0.5885 0.6429 0.6476 IR 0.6857 0.6954 0.6000 0.6571 MT 0.7037 0.7145 0.6000 0.6350 IE 0.5857 0.6008 0.5917 0.6275 QA 0.7111 0.7121 0.5308 0.5463 PP 0.7683 0.7470 0.5200 0.5333 All 0.6878 0.6888 0.6250 0.6534 Table 2: Summary of accuracies and confidenceweighted scores, by task Alignment Feature Dev Test Synonym Match 0.0106 0.0038 Der,,
We then incorporate paraphrase similarity within the lexical similarity model by allowing, for some unaligned node h Ph, where t Pt: sim(h, t) = max(MN(h, t), score(Ph, Pt)) 38 \x0cOur approach to paraphrase detection is most similar to the TE/ASE algorithm CITATION, and bears similarity to both DIRT CITATION and KnowItAll CITATION,,
5 Lexical similarity and paraphrase detection 5.1 Lexical similarity using MindNet In case none of the preceding heuristics for rejection are applicable, we back off to a lexical similarity model similar to that described in CITATION,,
These logical forms are generated using NLPWIN3, a robust system for natural language parsing and generation CITATION,,
To accomplish the task of node alignment we rely on the following heuristics: 3.1 WordNet synonym match As in CITATION and others, we align a node h H to any node t T that has both the same part of speech and belongs to the same synset in WordNet,,
3.6 Other heuristics for alignment In addition to these heuristics, we implemented a hyponym match heuristic similar to that discussed in CITATION, and a heuristic based on the string-edit distance of two lemmas; however, these heuristics yielded a decrease in our systems accuracy on the development set and were thus left out of our final system,,
Our overall test set accuracy of 62.50% represents a 2.1% absolute improvement over the task-independent system described in CITATION, and a 20.2% relative improvement in accuracy over their system with respect to an uninformed baseline accuracy of 50%,,
The RTE problem as presented in the PASCAL RTE dataset is particularly attractive in that it is a reasonably simple task for human annotators with high inter-annotator agreement (95.1% in one independent labeling CITATION), but an extremely challenging task for automated systems,,
2 CITATION suggest that the truth or falsehood of 48% of the entailment examples in the RTE test set could be correctly identified via syntax and a thesaurus alone; thus by random guessing on the rest of the examples one might hope for an accuracy level of 0.48 + 0.52 2 = 74%,,
For every content node h H 37 \x0cnot already aligned by one of the heuristics in Section 3, we obtain a similarity score MN(h, t) from a similarity database that is constructed automatically from the data contained in MindNet5 as described in CITATION,,
Many previous approaches have used a logical form representation of the text and hypothesis sentences, focusing on deriving a proof by which one can infer the hypothesis logical form from the text logical form (CITATION; CITATION; CITATION; CITATION),,
Attempts have been made to remedy this deficit through various techniques, including modelbuilding CITATION and the addition of semantic axioms CITATION,,
Our approach is inspired by an analysis of the RTE dataset that suggested a syntax-based approach should be approximately twice as effective at predicting false entailment as true entailment CITATION,,
We then compute the entailment score: score(H, T) = 1 |H| Y hH max tT sim(h, t) This approach is identical to that used in CITATION, except that we use alignment heuristics and MindNet similarity scores in place of their web-based estimation of lexical entailment probabilities, and we take as our score the geometric mean of the component entailm,,
