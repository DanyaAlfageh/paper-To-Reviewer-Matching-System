<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.672415">
b&apos;Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 181189, Prague, June 2007. c
</bodyText>
<sectionHeader confidence="0.472245" genericHeader="abstract">
2007 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.541802">
Improving Query Spelling Correction
Using Web Search Results
</title>
<author confidence="0.63557">
Qing Chen
</author>
<affiliation confidence="0.7403705">
Natural Language Processing Lab
Northeastern University
</affiliation>
<address confidence="0.970613">
Shenyang, Liaoning, China, 110004
</address>
<email confidence="0.989273">
chenqing@ics.neu.edu.cn
</email>
<author confidence="0.948001">
Mu Li
</author>
<affiliation confidence="0.970978">
Microsoft Research Asia
</affiliation>
<address confidence="0.898198">
5F Sigma Center
Zhichun Road, Haidian District
Beijing, China, 100080
</address>
<email confidence="0.984433">
muli@microsoft.com
</email>
<author confidence="0.941832">
Ming Zhou
</author>
<affiliation confidence="0.965896">
Microsoft Research Asia
</affiliation>
<address confidence="0.896796333333333">
5F Sigma Center
Zhichun Road, Haidian District
Beijing, China, 100080
</address>
<email confidence="0.994595">
mingzhou@microsoft.com
</email>
<sectionHeader confidence="0.99059" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999516576923077">
Traditional research on spelling correction
in natural language processing and infor-
mation retrieval literature mostly relies on
pre-defined lexicons to detect spelling er-
rors. But this method does not work well
for web query spelling correction, because
there is no lexicon that can cover the vast
amount of terms occurring across the web.
Recent work showed that using search
query logs helps to solve this problem to
some extent. However, such approaches
cannot deal with rarely-used query terms
well due to the data sparseness problem. In
this paper, a novel method is proposed for
use of web search results to improve the
existing query spelling correction models
solely based on query logs by leveraging
the rich information on the web related to
the query and its top-ranked candidate. Ex-
periments are performed based on real-
world queries randomly sampled from
search engines daily logs, and the results
show that our new method can achieve
16.9% relative F-measure improvement
and 35.4% overall error rate reduction in
comparison with the baseline method.
</bodyText>
<sectionHeader confidence="0.99829" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999321703703704">
Nowadays more and more people are using Inter-
net search engine to locate information on the web.
Search engines take text queries that users type as
input, and present users with information of ranked
web pages related to users queries. During this
process, one of the important factors that lead to
poor search results is misspelled query terms. Ac-
tually misspelled queries are rather commonly ob-
served in query logs, as shown in previous investi-
gations into the search engines log data that
around 10%~15% queries were misspelled (Cucer-
zan and Brill, 2004).
Sometimes misspellings are due to simple typo-
graphic errors such as teh for the. In many cases
the spelling errors are more complicated cognitive
errors such as camoflauge for camouflage. As a
matter of fact, correct spelling is not always an
easy task even many Americans cannot exactly
spell out California governors last name: Schwar-
zenegger. A spelling correction tool can help im-
prove users efficiency in the first case, but it is
more useful in the latter since the users cannot fig-
ure out the correct spelling by themselves.
There has been a long history of general-purpose
spelling correction research in natural language
processing and information retrieval literature
(Kukich, 1992), but its application to web search
</bodyText>
<page confidence="0.996253">
181
</page>
<bodyText confidence="0.994630571428571">
\x0cquery is still a new challenge. Although there are
some similarities in correction candidate genera-
tion and selection, these two settings are quite dif-
ferent in one fundamental problem: How to deter-
mine the validity of a search term. Traditionally,
the measure is mostly based on a pre-defined spel-
ling lexicon all character strings that cannot be
found in the lexicon are judged to be invalid. How-
ever, in the web search context, there is little hope
that we can construct such a lexicon with ideal
coverage of web search terms. For example, even
manually collecting a full list of car names and
company names will be a formidable task.
To obtain more accurate understanding of this
problem, we performed a detailed investigation
over one weeks MSN daily query logs, among
which found that 16.5% of search terms are out of
the scope of our spelling lexicon containing around
200,000 entries. In order to get more specific num-
bers, we also manually labeled a query data set that
contains 2,323 randomly sampled queries and
6,318 terms. In this data set, the ratio of out-of-
vocabulary (OOV) terms is 17.4%, which is very
similar to the overall distribution. However, only
25.3% of these OOV terms are identified to be
misspelled, which occupy 85% of the overall spel-
ling errors. All these statistics indicate that accu-
rate OOV term classification is of crucial impor-
tance to good query spelling correction perfor-
mance.
Cucerzan and Brill (2004) first investigated this
issue and proposed to use query logs to infer cor-
rect spellings of misspelled terms. Their principle
can be summarized as follows: given an input
query string q, finding a more probable query c
than q within a confusion set of q, in which the edit
distance between each element and q is less than a
given threshold. They reported good recall for
misspelled terms, but without detailed discussions
on accurate classification of valid out-of-
vocabulary terms and misspellings. In Lis work,
distributional similarity metrics estimated from
query logs were proposed to be used to discrimi-
nate high-frequent spelling errors such as massen-
ger from valid out-of-vocabulary terms such as
biocycle. But this method suffers from the data
sparseness problem: sufficient amounts of occur-
rences of every possible misspelling and valid
terms are required to make good estimation of dis-
tributional similarity metrics; thus this method
does not work well for rarely-used out-of-
vocabulary search terms and uncommon misspel-
lings.
In this paper we propose to use web search re-
sults to further improve the performance of query
spelling correction models. The key contribution of
our work is to identify that the dynamic online
search results can serve as additional evidence to
determine users intended spelling of a given term.
The information in web search results we used in-
cludes the number of pages matched for the query,
the term distribution in the web page snippets and
URLs. We studied two schemes to make use of the
returning results of a web search engine. The first
one only exploits indicators of the input querys
returning results, while the other also looks at other
potential correction candidates search results. We
performed extensive evaluations on a query set
randomly sampled from search engines daily
query logs, and experimental results show that we
can achieve 35.4% overall error rate reduction and
18.2% relative F-measure improvement on OOV
misspelled terms.
The rest of the paper is structured as follows.
Section 2 details other related work of spelling cor-
rection research. In section 3, we show the intuitive
motivations to use web search results for the query
spelling correction. After presenting the formal
statement of the query spelling correction problem
in Section 4, we describe our approaches that use
machine learning methods to integrate statistical
features from web search results in Section 5. We
present our evaluation methods for the proposed
methods and analyze their performance in Section
</bodyText>
<sectionHeader confidence="0.6969925" genericHeader="method">
6. Section 7 concludes the paper.
2 Related Work
</sectionHeader>
<bodyText confidence="0.999629357142857">
Spelling correction models in most previous work
were constructed based on conventional task set-
tings. Based on the focus of these task settings, two
lines of research have been applied to deal with
non-word errors and real-word errors respectively.
Non-word error spelling correction is focused on
the task of generating and ranking a list of possible
spelling corrections for each word not existing in a
spelling lexicon. Traditionally candidate ranking is
based on manually tuned scores such as assigning
alternative weights to different edit operations or
leveraging candidate frequencies (Damerau, 1964;
Levenshtein, 1966). In recent years, statistical
models have been widely used for the tasks of nat-
</bodyText>
<page confidence="0.998148">
182
</page>
<bodyText confidence="0.999121619047619">
\x0cural language processing, including spelling cor-
rection task. (Brill and Moore, 2000) presented an
improved error model over the one proposed by
(Kernighan et al., 1990) by allowing generic
string-to-string edit operations, which helps with
modeling major cognitive errors such as the confu-
sion between le and al. Via explicit modeling of
phonetic information of English words, (Toutanova
and Moore, 2002) further investigated this issue.
Both of them require misspelled/correct word pairs
for training, and the latter also needs a pronuncia-
tion lexicon, but recently (Ahmad and Kondrak,
2005) demonstrated that it is also possible to learn
such models automatically from query logs with
the EM algorithm, which is similar to work of
(Martin, 2004), learning from a very large corpus
of raw text for removing non-word spelling errors
in large corpus. All the work for non-word spelling
correction focused on the current word itself with-
out taking into account contextual information.
Real-word spelling correction is also referred to
be context sensitive spelling correction (CSSC),
which tries to detect incorrect usage of valid words
in certain contexts. Using a pre-defined confusion
set is a common strategy for this task, such as in
the work of (Golding and Roth, 1996) and (Mangu
and Brill, 1997). Opposite to non-word spelling
correction, in this direction only contextual evi-
dences were taken into account for modeling by
assuming all spelling similarities are equal.
The complexity of query spelling correction task
requires the combination of these types of evidence,
as done in (Cucerzan and Brill, 2004; Li et al.,
2006). One important contribution of our work is
that we use web search results as extended contex-
tual information beyond query strings by taking
advantage of application specific knowledge. Al-
though the information used in our methods can all
be accessed in a search engines web archive, such
a strategy involves web-scale data processing
which is a big engineering challenge, while our
method is a light-weight solution to this issue.
</bodyText>
<sectionHeader confidence="0.991188" genericHeader="method">
3 Motivation
</sectionHeader>
<bodyText confidence="0.999266809523809">
When a spelling correction model tries to make a
decision whether to make a suggestion c to a query
q, it generally needs to leverage two types of evi-
dence: the similarity between c and q, and the va-
lidity plausibility of c and q. All the previous work
estimated plausibility of a query based on the
query string itself typically it is represented as
the string probability, which is further decomposed
into production of consecutive n-gram probabilities.
For example, both the work of (Cucerzan and Brill,
2004; Li et al., 2006) used n-gram statistical lan-
guage models trained from search engines query
logs to estimate the query string probability.
In the following, we will show that the search
results for a query can serve as a feedback mechan-
ism to provide additional evidences to make better
spelling correction decisions. The usefulness of
web search results can be two-fold:
First, search results can be used to validate
query terms, especially those not popular enough
in query logs. One case is the validation for navi-
gational queries (Broder, 2004). Navigational que-
ries usually contain terms that are key parts of des-
tination URLs, which may be out-of-vocabulary
terms since there are millions of sites on the web.
Because some of these navigational terms are very
relatively rare in query logs, without knowledge of
the special navigational property of a term, a query
spelling correction model might confuse them with
other low-frequency misspellings. But such infor-
mation can be effectively obtained from the URLs
of retrieved web pages. Inferring navigational que-
ries through term-URL matching thus can help re-
duce the chance that the spelling correction model
changes an uncommon web site name into popular
search term, such as from innovet to innovate.
Another example is that search results can be used
in identifying acronyms or other abbreviations. We
can observe some clear text patterns that relate ab-
breviations to their full spellings in the search re-
sults as shown in Figure 1. But such mappings
cannot easily be obtained from query logs.
</bodyText>
<figureCaption confidence="0.671349">
Figure 1. Sample search results for SARS
</figureCaption>
<bodyText confidence="0.999114625">
Second, search results can help verify correction
candidates. The terms appearing in search results,
both in the web page titles and snippets, provide
additional evidences for users intention. For exam-
ple, if a user searches for a misspelled query vac-
cum cleaner on a search engine, it is very likely
that he will obtain some search results containing
the correct term vacuum as shown in Figure 2. This
</bodyText>
<page confidence="0.997517">
183
</page>
<bodyText confidence="0.9076912">
\x0ccan be attributed to the collective link text distribu-
tion on the web many links with misspelled text
point to sites with correct spellings. Such evi-
dences can boost the confidence of a spelling cor-
rection model to suggest vacuum as a correction.
</bodyText>
<figureCaption confidence="0.931375">
Figure 2. Sample search results
</figureCaption>
<bodyText confidence="0.9918274">
for vaccum cleaner
The number of matched pages can be used to
measure the popularity of a query on the web,
which is similar to term frequencies occurring in
query logs, but with broader coverage. Poor cor-
rection candidates can usually be verified by a
smaller number of matched web pages.
Another observation is that the documents re-
trieved with correctly-spelled query and misspelled
ones are similar to some extent in the view of term
distribution. Both the web retrieval results of va-
cuum and vaccum contain terms such as cleaner,
pump, bag or systems. We can take this similarity
as an evidence to verify the spelling correction re-
sults.
</bodyText>
<sectionHeader confidence="0.93677" genericHeader="method">
4 Problem Statement
</sectionHeader>
<bodyText confidence="0.9495275">
Given a query q, a spelling correction model is to
find a query string c that maximizes the posterior
probability of c given q within the confusion set of
q. Formally we can write this as follows:
</bodyText>
<equation confidence="0.989642">
c
= argmax
cC
Pr(c|q) (1)
</equation>
<bodyText confidence="0.995824125">
where C is the confusion set of q. Each query
string c in the confusion set is a correction candi-
date for q, which satisfies the constraint that the
spelling similarity between c and q is within given
threshold .
In this formulation, the error detection and cor-
rection are performed in a unified way. The query
q itself always belongs to its confusion set C, and
when the spelling correction model identifies a
more probable query string c in C which is differ-
ent from q, it claims a spelling error detected and
makes a correction suggestion c.
There are two tasks in this framework. One is
how to learn a statistical model to estimate the
conditional probability Pr c q , and the other is
how to generate confusion set C of a given query q
</bodyText>
<subsectionHeader confidence="0.9982495">
4.1 Maximum Entropy Model for Query
Spelling Correction
</subsectionHeader>
<bodyText confidence="0.99622825">
We take a feature-based approach to model the
posterior probability Pr c q . Specifically we use
the maximum entropy model (Berger et al., 1996)
for this task:
</bodyText>
<equation confidence="0.996218916666667">
Pr c q =
exp ifi c, q
N
i=1
exp( ifi(c, q)
N
i=1 )
c
(2)
where exp( ifi(c, q)
N
i=1 )
</equation>
<bodyText confidence="0.9736701875">
c is the normalization
factor; fi c, q is a feature function defined over
query q and correction candidate c , while i is the
corresponding feature weight. s can be optimized
using the numerical optimization algorithms such
as the Generalized Iterative Scaling (GIS) algo-
rithm (Darroch and Ratcliff, 1972) by maximizing
the posterior probability of the training set which
contains a manually labeled set of query-truth pairs:
= argmax c,q log Pr (c|q) (3)
The advantage of maximum entropy model is
that it provides a natural way and unified frame-
work to integrate all available information sources.
This property is well fit for our task in which we
are using a wide variety of evidences based on lex-
icon, query log and web search results.
</bodyText>
<subsectionHeader confidence="0.997642">
4.2 Correction Candidate Generation
</subsectionHeader>
<bodyText confidence="0.974505384615384">
Correction candidate generation for a query q can
be decomposed into two phases. In the first phase,
correction candidates are generated for each term
in the query from a term-base extracted from query
logs. This task can leverage conventional spelling
correction methods such as generating candidates
based on edit distance (Cucerzan and Brill, 2004)
or phonetic similarity (Philips, 1990). Then the
correction candidates of the entire query are gener-
ated by composing the correction candidates of
each individual term. Let q = w1 wn , and the
confusion set of wi is Cwi
, then the confusion set
</bodyText>
<figure confidence="0.812545125">
of q is Cw1
Cw2
Cwn
1. For example, for a
query q = w1w2 , w1 has candidates c11 and c12 ,
while w2 has candidates c21and c22, then the con-
fusion set C is {c11c21, c11c22, c12c21, c12c22}.
1
</figure>
<bodyText confidence="0.7497635">
For denotation simplicity, we do not cover compound and
composition errors here.
</bodyText>
<page confidence="0.997299">
184
</page>
<bodyText confidence="0.996937111111111">
\x0cThe problem of this method is the size of confu-
sion set C may be huge for multi-term queries. In
practice, one term may have hundreds of possible
candidates, then a query containing several terms
may have millions. This might lead to impractical
search and training using the maximum entropy
modeling method. Our solution to this problem is
to use candidate pruning. We first roughly rank the
candidates based on the statistical n-gram language
model estimated from query logs. Then we only
choose a subset of C that contains a specified
number of top-ranked (most probable) candidates
to present to the maximum entropy model for of-
fline training and online re-ranking, and the num-
ber of candidates is used as a parameter to balance
top-line performance and run-time efficiency. This
subset can be efficiently generated as shown in (Li
et al., 2006).
</bodyText>
<sectionHeader confidence="0.852466" genericHeader="method">
5 Web Search Results based Query Spel-
</sectionHeader>
<subsectionHeader confidence="0.872754">
ling Correction
</subsectionHeader>
<bodyText confidence="0.997026181818182">
In this section we will describe in detail the me-
thods for use of web search results in the query
spelling correction task. In our work we studied
two schemes. The first one only employs indicators
of the input querys search results, while the other
also looks at the most probable correction candi-
dates search results. For each scheme, we extract
additional scheme-specific features from the avail-
able search results, combine them with baseline
features and construct a new maximal model to
perform candidate ranking.
</bodyText>
<subsectionHeader confidence="0.869496">
5.1 Baseline model
</subsectionHeader>
<bodyText confidence="0.996308285714286">
We denote the maximum entropy model based on
baseline model feature set as M0 and the feature
set S0 derived from the latest state of the art works
of (Li et al., 2006), where S0 includes the features
mostly concerning the statistics of the query terms
and the similarities between query terms and their
correction candidates.
</bodyText>
<subsectionHeader confidence="0.963722">
5.2 Scheme 1: Using search results for input
</subsectionHeader>
<bodyText confidence="0.977966285714286">
query only
In this scheme we build more features for each cor-
rection candidate (including input query q itself)
by distilling more evidence from the search results
of the query. S1 denotes the augmented feature set,
and M1 denotes the maximum entropy model
based on S1. The features are listed as follows:
</bodyText>
<listItem confidence="0.890927">
1. Number of pages returned: the number of
web search pages retrieved by a web search
engine, which is used to estimate the popu-
larity of query. This feature is only for q.
2. URL string: Binary features indicating
whether the combination of terms of each
candidate is in the URLs of top retrieved
documents. This feature is for all candidates.
3. Frequency of correction candidate term:
the number of occurrences of modified
</listItem>
<bodyText confidence="0.7740414">
terms in the correction candidate found in
the title and snippet of top retrieved docu-
ments based on the observation that correc-
tion terms possibly co-occur with their
misspelled ones. This feature is invalid for q.
4. Frequency of query term: the number of
occurrences of each term of q found in the
title or snippet of the top retrieved docu-
ments, based on the observation that the cor-
rect terms always appear frequently in their
search results.
5. Abbreviation pattern: Binary features indi-
cating whether inputted query terms might
be abbreviations according to text patterns in
search results.
</bodyText>
<subsectionHeader confidence="0.735965">
5.3 Scheme 2: Using both search results of
</subsectionHeader>
<bodyText confidence="0.984920428571429">
input query and top-ranked candidate
In this scheme we extend the use of search results
both for query q and for top-ranked candidate c
other than q determined by M1. First we submit a
query to a search engine for the initial retrieval to
obtain one set of search results Rq, then use M1 to
find the best correction candidate c other than q.
Next we perform a second retrieval with c to ob-
tain another set of search results Rc. Finally addi-
tional features are generated for each candidate
based on Rc, then a new maximum entropy model
M2 is built to re-rank the candidates for a second
time. The entire process can be schematically
shown in Figure 3.
</bodyText>
<figureCaption confidence="0.998468">
Figure 3. Relations of models and features
</figureCaption>
<figure confidence="0.988656692307692">
Lexicon / query
Logs Spelling
Similarity
q Rq
c Rc
S0 features
S1 specific
features
S2 specific
features
Model M1
Model M2
Model M0
</figure>
<page confidence="0.996801">
185
</page>
<bodyText confidence="0.994384222222222">
\x0cwhere Rq is the web search results of query q; Rc is
the web search results of c which is the top-ranked
correction of q suggested by model M1.
The new feature set denoted with S2 is a set of
document similarities between Rq and Rc , which
includes different similarity estimations between
the query and its correction at the document level
using merely cosine measure based on term fre-
quency vectors of Rq and Rc.
</bodyText>
<sectionHeader confidence="0.994514" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.994216">
6.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.99133562962963">
In our work, we consider the following four types
of evaluation metrics:
Accuracy: The number of correct outputs
proposed by the spelling correction model di-
vided by the total number of queries in the test
set
Recall: The number of correct suggestions for
misspelled queries by the spelling correction
model divided by the total number of miss-
pelled queries in the test set
Precision: The number of correct suggestions
for misspelled queries proposed by the spel-
ling correction model divided by the total
number of suggestions made by the system
F-measure: Formula F = 2PR/(P + R) used
for calculating the f-measure, which is essen-
tially the harmonic mean of recall and preci-
sion
Any individual metric above might not be suffi-
cient to indicate the overall performance of a query
spelling correction model. For example, as in most
retrieval tasks, we can trade recall for precision or
vice versa. Although intuitively F might be in ac-
cordance with accuracy, there is no strict theoreti-
cal relation between these two numbers there are
conditions under which accuracy improves while
F-measure may drop or be unchanged.
</bodyText>
<subsectionHeader confidence="0.997552">
6.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.960875">
We used a manually constructed data set as gold
standard for evaluation. First we randomly sam-
pled 7,000 queries from search engines daily
query logs of different time periods, and had them
manually labeled by two annotators independently.
Each query is attached to a truth, which is either
the query itself for valid queries, or a spelling cor-
rection for misspelled ones. From the annotation
results that both annotators agreed with each other,
we extracted 2,323 query-truth pairs as training set
and 991 as test set. Table 1 shows the statistics of
the data sets, in which Eq denotes the error rate of
query and Et denotes the error rate of term.
</bodyText>
<table confidence="0.9969802">
# queries # terms q
E t
E
Training set 2,323 6,318 15.0% 5.6%
Test set 991 2,589 12.8% 5.2%
</table>
<tableCaption confidence="0.999087">
Table 1. Statistics of training set and test set
</tableCaption>
<bodyText confidence="0.999813111111111">
In the following experiments, at most 50 correc-
tion candidates were used in the maximum entropy
model for each query if there is no special explana-
tion. The web search results were fetched from
MSNs search engine. By default, top 100 re-
trieved items from the web retrieval results were
used to perform feature extraction. A set of query
log data spanning 9 months are used for collecting
statistics required by the baseline.
</bodyText>
<subsectionHeader confidence="0.996935">
6.3 Overall Results
</subsectionHeader>
<bodyText confidence="0.9349182">
Following the method as described in previous sec-
tions, we first ran a group of experiments to eva-
luate the performance of each model we discussed
with default settings. The detailed results are
shown in Table 2.
</bodyText>
<table confidence="0.98715475">
Model Accuracy Recall Precision F
M0 91.8% 60.6% 62.6% 0.616
M1 93.9% 64.6% 77.4% 0.704
M2 94.7% 66.9% 78.0% 0.720
</table>
<tableCaption confidence="0.995872">
Table 2. Overall Results
</tableCaption>
<bodyText confidence="0.988405384615385">
From the table we can observe significant per-
formance boosts on all evaluation metrics of M1
and M2 over M0.
We can achieve 25.6% error rate reduction and
23.6% improvement in precision, as well as 6.6%
relative improvement in recall, when adding S1 to
M1. Paired t-test gives p-value of 0.002, which is
significant to 0.01 level.
M2 can bring additional 13.1% error rate reduc-
tion and moderate improvement in precision, as
well as 3.6% improvement in recall over M1, with
paired t-test showing that the improvement is sig-
nificant to 0.01 level.
</bodyText>
<page confidence="0.997539">
186
</page>
<bodyText confidence="0.964849666666667">
\x0c6.4 Impact of Candidate number
Theoretically the number of correction candidates
in the confusion set determines the accuracy and
recall upper bounds for all models concerned in
this paper. Performance might be hurt if we use a
too small candidate number, which is because the
corrections are separated from the confusion sets.
These curves shown in Figure 4 and 5, include
both theoretical bound (oracle) and actual perfor-
mance of our described models. From the chart we
can see that our models perform best when Nt is
around 50, and when Nt &gt; 15 the oracle recall and
accuracy almost stay unchanged, thus the actual
models performance only benefits a little from
larger Nt values. The missing part of recall is
largely due to the fact that we are not able to gen-
erate truth candidates for some weird query terms
rather than insufficient size of confusion set.
</bodyText>
<figureCaption confidence="0.9998685">
Figure 4. Recall versus candidate number
Figure 5. Accuracy versus candidate number
</figureCaption>
<subsectionHeader confidence="0.936633">
6.5 Discussions
</subsectionHeader>
<bodyText confidence="0.9965376">
We also studied the performance difference be-
tween in-vocabulary (IV) and out-of-vocabulary
(OOV) terms when using different spelling correc-
tion models. The detailed results are shown in Ta-
ble 3 and Table 4.
</bodyText>
<table confidence="0.98892725">
Accuracy Precision Recall F
M0 88.2% 77.1% 67.3% 0.718
M1 92.4% 88.5% 77.3% 0.825
M2 93.2% 91.6% 79.1% 0.849
</table>
<tableCaption confidence="0.568324">
Table 3. OOV Term Results
</tableCaption>
<table confidence="0.99818675">
Accuracy Precision Recall F
M0 98.8% 44.0% 45.8% 0.449
M1 99.0% 62.5% 20.8% 0.313
M2 99.1% 75.0% 37.5% 0.500
</table>
<tableCaption confidence="0.993853">
Table 4. IV Term Results
</tableCaption>
<bodyText confidence="0.999519909090909">
The results show that M1 is very powerful to
identify and correct OOV spelling errors compared
with M0. Actually M1 is able to correct spelling
errors such as guiness, whose frequency in query
log is even higher than its truth spelling guinness.
Since most spelling errors are OOV terms, this ex-
plains why the model M1 can significantly outper-
form the baseline. But for IV terms things are dif-
ferent. Although the overall accuracy is better, the
F-measure of M1 is far lower than M0. M2 per-
forms best for the IV task in terms of both accura-
cy and F-measure. However, IV spelling errors is
so small a portion of the total misspelling (only
17.4% of total spelling errors in our test set) that
the room for improvement is very small. This helps
to explain why the performance gap between M1
and M0 is much larger than the one between M2
and M1, and shows the tendency that M1 prefer to
identify and correct OOV misspellings in compari-
son to IV ones, which causes F-measure drop from
M0 to M1; while by introducing more useful evi-
dence, M2 outperforms better for both OOV and
IV terms over M0 and M1.
Another set of statistics we collected from the
experiments is the performance data of low-
frequency terms when using the models proposed
in this paper, since we believe that our approach
would help make better classification of low-
frequency search terms. As a case study, we identi-
fied in the test set all terms whose frequencies in
our query logs are less than 800, and for these
terms we calculated the error reduction rate of
model M1 over the baseline model M0 at each in-
</bodyText>
<figure confidence="0.998995285714286">
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 45 50
Recall
Candidate number
M0 M1
M2 Oracle
86%
88%
90%
92%
94%
96%
98%
100%
1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 45 50
Accuracy
Candidate number
M0 M1
M2 Oracle
</figure>
<page confidence="0.989465">
187
</page>
<bodyText confidence="0.992558090909091">
\x0cterval of 50. The detailed results are shown in Fig-
ure 6. The clear trend can be observed that M1 can
achieve larger error rate reduction over baseline for
terms with lower frequencies. This is because the
performance of baseline model drops for these
terms when there are no reliable distributional si-
milarity estimations available due to data sparse-
ness in query logs, while M1 can use web data to
alleviate this problem.
Figure 6. Error rate reduction of M1 over baseline
for terms in different frequency ranges
</bodyText>
<sectionHeader confidence="0.994781" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999917904761905">
The task of query spelling correction is very differ-
ent from conventional spelling checkers, and poses
special research challenges. In this paper, we pre-
sented a novel method for use of web search re-
sults to improve existing query spelling correction
models.
We explored two schemes for taking advantage
of the information extracted from web search re-
sults. Experimental results show that our proposed
methods can achieve statistically significant im-
provements over the baseline model which only
relies on evidences of lexicon, spelling similarity
and statistics estimated from query logs.
There is still further potential useful information
that should be studied in this direction. For exam-
ple, we can work on page ranking information of
returning pages, because trusted or well-known
sites with high page rank generally contain few
wrong spellings. In addition, the term co-
occurrence statistics on the returned snippet text
are also worth deep investigation.
</bodyText>
<sectionHeader confidence="0.992078" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993867745098039">
Ahmad F. and Grzegorz Kondrak G. Learning a spelling
error model from search query logs. Proceedings of
EMNLP 2005, pages 955-962, 2005
Berger A. L., Della Pietra S. A., and Della Pietra V. J.
A maximum entropy approach to natural language
processing. Computation Linguistics, 22(1):39-72,
1996
Brill E. and Moore R. C. An improved error model for
noisy channel spelling correction. Proceedings of
38th annual meeting of the ACL, pages 286-293,
2000.
Broder, A. A taxonomy of web search. SIGIR Forum
Fall 2002, Volume 36 Number 2, 2002.
Church K. W. and Gale W. A. Probability scoring for
spelling correction. In Statistics and Computing, vo-
lume 1, pages 93-103, 1991.
Cucerzan S. and Brill E. Spelling correction as an itera-
tive process that exploits the collective knowledge of
web users. Proceedings of EMNLP04, pages 293-
300, 2004.
Damerau F. A technique for computer detection and
correction of spelling errors. Communication of the
ACM 7(3):659-664, 1964.
Darroch J. N. and Ratcliff D. Generalized iterative scal-
ing for long-linear models. Annals of Mathematical
Statistics, 43:1470-1480, 1972.
Efthimiadis, N.E., Query Expansion, In Annual Review
of Information Systems and Technology, Vol. 31, pp.
121-187 , 1996.
Golding A. R. and Roth D. Applying winnow to con-
text-sensitive spelling correction. Proceedings of
ICML 1996, pages 182-190, 1996.
J. Lafferty and C. Zhai. Document language models,
query models, and risk minimization for information
retrieval. In Proceedings of SIGIR2001, pages 111-
119, Sept 2001.
J. Xu and W. Croft. Query expansion using local and
global document analysis. In Proceedings of the SI-
GIR 1996, pages 4-11, 1996
Kernighan M. D., Church K. W. and Gale W. A. A spel-
ling correction program based on a noisy channel
model. Proceedings of COLING 1990, pages 205-
210, 1990.
Kukich K. Techniques for automatically correcting
words in text. ACM Computing Surveys. 24(4): 377-
439, 1992.
Levenshtein V. Binary codes capable of correcting dele-
tions, insertions and reversals. Soviet Physice Dok-
lady 10: 707-710, 1966.
Li M., Zhu M. H., Zhang Y. and Zhou M. Exploring
distributional similarity based models for query spel-
</reference>
<figure confidence="0.9955805">
5%
10%
15%
20%
25%
30%
35%
50 150 250 350 450 550 650 750
error
rate
reduction
term frequency
</figure>
<page confidence="0.9832">
188
</page>
<reference confidence="0.999010733333333">
\x0cling correction. Proceedings of COLING-ACL 2006,
pages 1025-1032, 2006
Mangu L. and Eric Brill E. Automatic rule acquisition
for spelling correction. Proceedings of ICML 1997,
pages 734-741, 1997.
Martin Reynaert. Text induced spelling correction. Pro-
ceedings of COLING 2004,pages 834-840, 2004.
Mayes E., Damerau F. and Mercer R. Context based
spelling correction. Information processing and
management, 27(5): 517-522, 1991.
Philips L. Hanging on the metaphone. Computer Lan-
guage Magazine, 7(12): 39, 1990.
Toutanova K. and Moore R. Pronunciation modeling for
improved spelling correction. Proceedings of the
40th annual meeting of ACL, pages 144-151, 2002.
</reference>
<page confidence="0.985938">
189
</page>
<figure confidence="0.250199">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.156765">
<note confidence="0.977316333333333">b&apos;Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 181189, Prague, June 2007. c 2007 Association for Computational Linguistics</note>
<title confidence="0.988563">Improving Query Spelling Correction Using Web Search Results</title>
<author confidence="0.988352">Qing Chen</author>
<affiliation confidence="0.993511">Natural Language Processing Lab Northeastern University</affiliation>
<address confidence="0.999933">Shenyang, Liaoning, China, 110004</address>
<email confidence="0.876738">chenqing@ics.neu.edu.cn</email>
<author confidence="0.998818">Mu Li</author>
<affiliation confidence="0.730706">Microsoft Research Asia 5F Sigma Center Zhichun Road, Haidian District</affiliation>
<address confidence="0.993464">Beijing, China, 100080</address>
<email confidence="0.998928">muli@microsoft.com</email>
<author confidence="0.999452">Ming Zhou</author>
<affiliation confidence="0.730681333333333">Microsoft Research Asia 5F Sigma Center Zhichun Road, Haidian District</affiliation>
<address confidence="0.993467">Beijing, China, 100080</address>
<email confidence="0.999454">mingzhou@microsoft.com</email>
<abstract confidence="0.998141333333333">Traditional research on spelling correction in natural language processing and information retrieval literature mostly relies on pre-defined lexicons to detect spelling errors. But this method does not work well for web query spelling correction, because there is no lexicon that can cover the vast amount of terms occurring across the web. Recent work showed that using search query logs helps to solve this problem to some extent. However, such approaches cannot deal with rarely-used query terms well due to the data sparseness problem. In this paper, a novel method is proposed for use of web search results to improve the existing query spelling correction models solely based on query logs by leveraging the rich information on the web related to the query and its top-ranked candidate. Experiments are performed based on realworld queries randomly sampled from search engines daily logs, and the results show that our new method can achieve 16.9% relative F-measure improvement and 35.4% overall error rate reduction in comparison with the baseline method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Ahmad</author>
<author>Grzegorz Kondrak G</author>
</authors>
<title>Learning a spelling error model from search query logs.</title>
<date>2005</date>
<booktitle>Proceedings of EMNLP</booktitle>
<pages>955--962</pages>
<marker>Ahmad, G, 2005</marker>
<rawString>Ahmad F. and Grzegorz Kondrak G. Learning a spelling error model from search query logs. Proceedings of EMNLP 2005, pages 955-962, 2005 Berger A. L., Della Pietra S. A., and Della Pietra V. J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>R C Moore</author>
</authors>
<title>A maximum entropy approach to natural language processing. Computation Linguistics,</title>
<date>2000</date>
<booktitle>Proceedings of 38th annual meeting of the ACL,</booktitle>
<pages>22--1</pages>
<contexts>
<context position="7870" citStr="Brill and Moore, 2000" startWordPosition="1241" endWordPosition="1244">applied to deal with non-word errors and real-word errors respectively. Non-word error spelling correction is focused on the task of generating and ranking a list of possible spelling corrections for each word not existing in a spelling lexicon. Traditionally candidate ranking is based on manually tuned scores such as assigning alternative weights to different edit operations or leveraging candidate frequencies (Damerau, 1964; Levenshtein, 1966). In recent years, statistical models have been widely used for the tasks of nat182 \x0cural language processing, including spelling correction task. (Brill and Moore, 2000) presented an improved error model over the one proposed by (Kernighan et al., 1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. Via explicit modeling of phonetic information of English words, (Toutanova and Moore, 2002) further investigated this issue. Both of them require misspelled/correct word pairs for training, and the latter also needs a pronunciation lexicon, but recently (Ahmad and Kondrak, 2005) demonstrated that it is also possible to learn such models automatically from query logs wi</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>A maximum entropy approach to natural language processing. Computation Linguistics, 22(1):39-72, Brill E. and Moore R. C. An improved error model for noisy channel spelling correction. Proceedings of 38th annual meeting of the ACL, pages 286-293, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Broder</author>
</authors>
<title>A taxonomy of web search.</title>
<date>2002</date>
<journal>SIGIR Forum Fall</journal>
<volume>36</volume>
<marker>Broder, 2002</marker>
<rawString>Broder, A. A taxonomy of web search. SIGIR Forum Fall 2002, Volume 36 Number 2, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>W A Gale</author>
</authors>
<title>Probability scoring for spelling correction.</title>
<date>1991</date>
<booktitle>In Statistics and Computing,</booktitle>
<volume>1</volume>
<pages>93--103</pages>
<marker>Church, Gale, 1991</marker>
<rawString>Church K. W. and Gale W. A. Probability scoring for spelling correction. In Statistics and Computing, volume 1, pages 93-103, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>E Brill</author>
</authors>
<title>Spelling correction as an iterative process that exploits the collective knowledge of web users.</title>
<date>2004</date>
<booktitle>Proceedings of EMNLP04,</booktitle>
<pages>293--300</pages>
<contexts>
<context position="2297" citStr="Cucerzan and Brill, 2004" startWordPosition="343" endWordPosition="347">e reduction in comparison with the baseline method. 1 Introduction Nowadays more and more people are using Internet search engine to locate information on the web. Search engines take text queries that users type as input, and present users with information of ranked web pages related to users queries. During this process, one of the important factors that lead to poor search results is misspelled query terms. Actually misspelled queries are rather commonly observed in query logs, as shown in previous investigations into the search engines log data that around 10%~15% queries were misspelled (Cucerzan and Brill, 2004). Sometimes misspellings are due to simple typographic errors such as teh for the. In many cases the spelling errors are more complicated cognitive errors such as camoflauge for camouflage. As a matter of fact, correct spelling is not always an easy task even many Americans cannot exactly spell out California governors last name: Schwarzenegger. A spelling correction tool can help improve users efficiency in the first case, but it is more useful in the latter since the users cannot figure out the correct spelling by themselves. There has been a long history of general-purpose spelling correcti</context>
<context position="4478" citStr="Cucerzan and Brill (2004)" startWordPosition="706" endWordPosition="709">ut of the scope of our spelling lexicon containing around 200,000 entries. In order to get more specific numbers, we also manually labeled a query data set that contains 2,323 randomly sampled queries and 6,318 terms. In this data set, the ratio of out-ofvocabulary (OOV) terms is 17.4%, which is very similar to the overall distribution. However, only 25.3% of these OOV terms are identified to be misspelled, which occupy 85% of the overall spelling errors. All these statistics indicate that accurate OOV term classification is of crucial importance to good query spelling correction performance. Cucerzan and Brill (2004) first investigated this issue and proposed to use query logs to infer correct spellings of misspelled terms. Their principle can be summarized as follows: given an input query string q, finding a more probable query c than q within a confusion set of q, in which the edit distance between each element and q is less than a given threshold. They reported good recall for misspelled terms, but without detailed discussions on accurate classification of valid out-ofvocabulary terms and misspellings. In Lis work, distributional similarity metrics estimated from query logs were proposed to be used to </context>
<context position="9399" citStr="Cucerzan and Brill, 2004" startWordPosition="1480" endWordPosition="1483">ord spelling correction is also referred to be context sensitive spelling correction (CSSC), which tries to detect incorrect usage of valid words in certain contexts. Using a pre-defined confusion set is a common strategy for this task, such as in the work of (Golding and Roth, 1996) and (Mangu and Brill, 1997). Opposite to non-word spelling correction, in this direction only contextual evidences were taken into account for modeling by assuming all spelling similarities are equal. The complexity of query spelling correction task requires the combination of these types of evidence, as done in (Cucerzan and Brill, 2004; Li et al., 2006). One important contribution of our work is that we use web search results as extended contextual information beyond query strings by taking advantage of application specific knowledge. Although the information used in our methods can all be accessed in a search engines web archive, such a strategy involves web-scale data processing which is a big engineering challenge, while our method is a light-weight solution to this issue. 3 Motivation When a spelling correction model tries to make a decision whether to make a suggestion c to a query q, it generally needs to leverage two</context>
<context position="15715" citStr="Cucerzan and Brill, 2004" startWordPosition="2559" endWordPosition="2562"> it provides a natural way and unified framework to integrate all available information sources. This property is well fit for our task in which we are using a wide variety of evidences based on lexicon, query log and web search results. 4.2 Correction Candidate Generation Correction candidate generation for a query q can be decomposed into two phases. In the first phase, correction candidates are generated for each term in the query from a term-base extracted from query logs. This task can leverage conventional spelling correction methods such as generating candidates based on edit distance (Cucerzan and Brill, 2004) or phonetic similarity (Philips, 1990). Then the correction candidates of the entire query are generated by composing the correction candidates of each individual term. Let q = w1 wn , and the confusion set of wi is Cwi , then the confusion set of q is Cw1 Cw2 Cwn 1. For example, for a query q = w1w2 , w1 has candidates c11 and c12 , while w2 has candidates c21and c22, then the confusion set C is {c11c21, c11c22, c12c21, c12c22}. 1 For denotation simplicity, we do not cover compound and composition errors here. 184 \x0cThe problem of this method is the size of confusion set C may be huge for </context>
</contexts>
<marker>Cucerzan, Brill, 2004</marker>
<rawString>Cucerzan S. and Brill E. Spelling correction as an iterative process that exploits the collective knowledge of web users. Proceedings of EMNLP04, pages 293-300, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Damerau</author>
</authors>
<title>A technique for computer detection and correction of spelling errors.</title>
<date>1964</date>
<journal>Communication of the ACM</journal>
<pages>7--3</pages>
<contexts>
<context position="7677" citStr="Damerau, 1964" startWordPosition="1214" endWordPosition="1215"> Work Spelling correction models in most previous work were constructed based on conventional task settings. Based on the focus of these task settings, two lines of research have been applied to deal with non-word errors and real-word errors respectively. Non-word error spelling correction is focused on the task of generating and ranking a list of possible spelling corrections for each word not existing in a spelling lexicon. Traditionally candidate ranking is based on manually tuned scores such as assigning alternative weights to different edit operations or leveraging candidate frequencies (Damerau, 1964; Levenshtein, 1966). In recent years, statistical models have been widely used for the tasks of nat182 \x0cural language processing, including spelling correction task. (Brill and Moore, 2000) presented an improved error model over the one proposed by (Kernighan et al., 1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. Via explicit modeling of phonetic information of English words, (Toutanova and Moore, 2002) further investigated this issue. Both of them require misspelled/correct word pairs fo</context>
</contexts>
<marker>Damerau, 1964</marker>
<rawString>Damerau F. A technique for computer detection and correction of spelling errors. Communication of the ACM 7(3):659-664, 1964.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for long-linear models.</title>
<date>1972</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>43--1470</pages>
<contexts>
<context position="14893" citStr="Darroch and Ratcliff, 1972" startWordPosition="2425" endWordPosition="2428"> of a given query q 4.1 Maximum Entropy Model for Query Spelling Correction We take a feature-based approach to model the posterior probability Pr c q . Specifically we use the maximum entropy model (Berger et al., 1996) for this task: Pr c q = exp ifi c, q N i=1 exp( ifi(c, q) N i=1 ) c (2) where exp( ifi(c, q) N i=1 ) c is the normalization factor; fi c, q is a feature function defined over query q and correction candidate c , while i is the corresponding feature weight. s can be optimized using the numerical optimization algorithms such as the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972) by maximizing the posterior probability of the training set which contains a manually labeled set of query-truth pairs: = argmax c,q log Pr (c|q) (3) The advantage of maximum entropy model is that it provides a natural way and unified framework to integrate all available information sources. This property is well fit for our task in which we are using a wide variety of evidences based on lexicon, query log and web search results. 4.2 Correction Candidate Generation Correction candidate generation for a query q can be decomposed into two phases. In the first phase, correction candidates are ge</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>Darroch J. N. and Ratcliff D. Generalized iterative scaling for long-linear models. Annals of Mathematical Statistics, 43:1470-1480, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N E Efthimiadis</author>
</authors>
<title>Query Expansion,</title>
<date>1996</date>
<booktitle>In Annual Review of Information Systems and Technology,</booktitle>
<volume>31</volume>
<pages>121--187</pages>
<marker>Efthimiadis, 1996</marker>
<rawString>Efthimiadis, N.E., Query Expansion, In Annual Review of Information Systems and Technology, Vol. 31, pp. 121-187 , 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>Applying winnow to context-sensitive spelling correction.</title>
<date>1996</date>
<booktitle>Proceedings of ICML</booktitle>
<pages>182--190</pages>
<contexts>
<context position="9059" citStr="Golding and Roth, 1996" startWordPosition="1428" endWordPosition="1431">s automatically from query logs with the EM algorithm, which is similar to work of (Martin, 2004), learning from a very large corpus of raw text for removing non-word spelling errors in large corpus. All the work for non-word spelling correction focused on the current word itself without taking into account contextual information. Real-word spelling correction is also referred to be context sensitive spelling correction (CSSC), which tries to detect incorrect usage of valid words in certain contexts. Using a pre-defined confusion set is a common strategy for this task, such as in the work of (Golding and Roth, 1996) and (Mangu and Brill, 1997). Opposite to non-word spelling correction, in this direction only contextual evidences were taken into account for modeling by assuming all spelling similarities are equal. The complexity of query spelling correction task requires the combination of these types of evidence, as done in (Cucerzan and Brill, 2004; Li et al., 2006). One important contribution of our work is that we use web search results as extended contextual information beyond query strings by taking advantage of application specific knowledge. Although the information used in our methods can all be </context>
</contexts>
<marker>Golding, Roth, 1996</marker>
<rawString>Golding A. R. and Roth D. Applying winnow to context-sensitive spelling correction. Proceedings of ICML 1996, pages 182-190, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>C Zhai</author>
</authors>
<title>Document language models, query models, and risk minimization for information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR2001,</booktitle>
<pages>111--119</pages>
<marker>Lafferty, Zhai, 2001</marker>
<rawString>J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In Proceedings of SIGIR2001, pages 111-119, Sept 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>W Croft</author>
</authors>
<title>Query expansion using local and global document analysis.</title>
<date>1996</date>
<booktitle>In Proceedings of the SIGIR</booktitle>
<pages>4--11</pages>
<marker>Xu, Croft, 1996</marker>
<rawString>J. Xu and W. Croft. Query expansion using local and global document analysis. In Proceedings of the SIGIR 1996, pages 4-11, 1996 Kernighan M. D., Church K. W. and Gale W. A. A spelling correction program based on a noisy channel model. Proceedings of COLING 1990, pages 205-210, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys.</journal>
<volume>24</volume>
<issue>4</issue>
<pages>377--439</pages>
<contexts>
<context position="2991" citStr="Kukich, 1992" startWordPosition="457" endWordPosition="458">. In many cases the spelling errors are more complicated cognitive errors such as camoflauge for camouflage. As a matter of fact, correct spelling is not always an easy task even many Americans cannot exactly spell out California governors last name: Schwarzenegger. A spelling correction tool can help improve users efficiency in the first case, but it is more useful in the latter since the users cannot figure out the correct spelling by themselves. There has been a long history of general-purpose spelling correction research in natural language processing and information retrieval literature (Kukich, 1992), but its application to web search 181 \x0cquery is still a new challenge. Although there are some similarities in correction candidate generation and selection, these two settings are quite different in one fundamental problem: How to determine the validity of a search term. Traditionally, the measure is mostly based on a pre-defined spelling lexicon all character strings that cannot be found in the lexicon are judged to be invalid. However, in the web search context, there is little hope that we can construct such a lexicon with ideal coverage of web search terms. For example, even manually</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Kukich K. Techniques for automatically correcting words in text. ACM Computing Surveys. 24(4): 377-439, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<journal>Soviet Physice Doklady</journal>
<volume>10</volume>
<pages>707--710</pages>
<contexts>
<context position="7697" citStr="Levenshtein, 1966" startWordPosition="1216" endWordPosition="1217">correction models in most previous work were constructed based on conventional task settings. Based on the focus of these task settings, two lines of research have been applied to deal with non-word errors and real-word errors respectively. Non-word error spelling correction is focused on the task of generating and ranking a list of possible spelling corrections for each word not existing in a spelling lexicon. Traditionally candidate ranking is based on manually tuned scores such as assigning alternative weights to different edit operations or leveraging candidate frequencies (Damerau, 1964; Levenshtein, 1966). In recent years, statistical models have been widely used for the tasks of nat182 \x0cural language processing, including spelling correction task. (Brill and Moore, 2000) presented an improved error model over the one proposed by (Kernighan et al., 1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. Via explicit modeling of phonetic information of English words, (Toutanova and Moore, 2002) further investigated this issue. Both of them require misspelled/correct word pairs for training, and the </context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Levenshtein V. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physice Doklady 10: 707-710, 1966.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Li</author>
<author>M H Zhu</author>
<author>Y Zhang</author>
<author>M Zhou</author>
</authors>
<title>Exploring distributional similarity based models for query spel\x0cling correction.</title>
<date>2006</date>
<booktitle>Proceedings of COLING-ACL</booktitle>
<pages>1025--1032</pages>
<contexts>
<context position="9417" citStr="Li et al., 2006" startWordPosition="1484" endWordPosition="1487"> also referred to be context sensitive spelling correction (CSSC), which tries to detect incorrect usage of valid words in certain contexts. Using a pre-defined confusion set is a common strategy for this task, such as in the work of (Golding and Roth, 1996) and (Mangu and Brill, 1997). Opposite to non-word spelling correction, in this direction only contextual evidences were taken into account for modeling by assuming all spelling similarities are equal. The complexity of query spelling correction task requires the combination of these types of evidence, as done in (Cucerzan and Brill, 2004; Li et al., 2006). One important contribution of our work is that we use web search results as extended contextual information beyond query strings by taking advantage of application specific knowledge. Although the information used in our methods can all be accessed in a search engines web archive, such a strategy involves web-scale data processing which is a big engineering challenge, while our method is a light-weight solution to this issue. 3 Motivation When a spelling correction model tries to make a decision whether to make a suggestion c to a query q, it generally needs to leverage two types of evidence</context>
<context position="17090" citStr="Li et al., 2006" startWordPosition="2799" endWordPosition="2802"> impractical search and training using the maximum entropy modeling method. Our solution to this problem is to use candidate pruning. We first roughly rank the candidates based on the statistical n-gram language model estimated from query logs. Then we only choose a subset of C that contains a specified number of top-ranked (most probable) candidates to present to the maximum entropy model for offline training and online re-ranking, and the number of candidates is used as a parameter to balance top-line performance and run-time efficiency. This subset can be efficiently generated as shown in (Li et al., 2006). 5 Web Search Results based Query Spelling Correction In this section we will describe in detail the methods for use of web search results in the query spelling correction task. In our work we studied two schemes. The first one only employs indicators of the input querys search results, while the other also looks at the most probable correction candidates search results. For each scheme, we extract additional scheme-specific features from the available search results, combine them with baseline features and construct a new maximal model to perform candidate ranking. 5.1 Baseline model We deno</context>
</contexts>
<marker>Li, Zhu, Zhang, Zhou, 2006</marker>
<rawString>Li M., Zhu M. H., Zhang Y. and Zhou M. Exploring distributional similarity based models for query spel\x0cling correction. Proceedings of COLING-ACL 2006, pages 1025-1032, 2006 Mangu L. and Eric Brill E. Automatic rule acquisition for spelling correction. Proceedings of ICML 1997, pages 734-741, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Reynaert</author>
</authors>
<title>Text induced spelling correction.</title>
<date>2004</date>
<booktitle>Proceedings of COLING 2004,pages 834-840,</booktitle>
<marker>Reynaert, 2004</marker>
<rawString>Martin Reynaert. Text induced spelling correction. Proceedings of COLING 2004,pages 834-840, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mayes</author>
<author>F Damerau</author>
<author>R Mercer</author>
</authors>
<title>Context based spelling correction. Information processing and management,</title>
<date>1991</date>
<volume>27</volume>
<issue>5</issue>
<pages>517--522</pages>
<marker>Mayes, Damerau, Mercer, 1991</marker>
<rawString>Mayes E., Damerau F. and Mercer R. Context based spelling correction. Information processing and management, 27(5): 517-522, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Philips</author>
</authors>
<title>Hanging on the metaphone.</title>
<date>1990</date>
<journal>Computer Language Magazine,</journal>
<volume>7</volume>
<issue>12</issue>
<contexts>
<context position="15754" citStr="Philips, 1990" startWordPosition="2566" endWordPosition="2567">o integrate all available information sources. This property is well fit for our task in which we are using a wide variety of evidences based on lexicon, query log and web search results. 4.2 Correction Candidate Generation Correction candidate generation for a query q can be decomposed into two phases. In the first phase, correction candidates are generated for each term in the query from a term-base extracted from query logs. This task can leverage conventional spelling correction methods such as generating candidates based on edit distance (Cucerzan and Brill, 2004) or phonetic similarity (Philips, 1990). Then the correction candidates of the entire query are generated by composing the correction candidates of each individual term. Let q = w1 wn , and the confusion set of wi is Cwi , then the confusion set of q is Cw1 Cw2 Cwn 1. For example, for a query q = w1w2 , w1 has candidates c11 and c12 , while w2 has candidates c21and c22, then the confusion set C is {c11c21, c11c22, c12c21, c12c22}. 1 For denotation simplicity, we do not cover compound and composition errors here. 184 \x0cThe problem of this method is the size of confusion set C may be huge for multi-term queries. In practice, one te</context>
</contexts>
<marker>Philips, 1990</marker>
<rawString>Philips L. Hanging on the metaphone. Computer Language Magazine, 7(12): 39, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>R Moore</author>
</authors>
<title>Pronunciation modeling for improved spelling correction.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th annual meeting of ACL,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="8190" citStr="Toutanova and Moore, 2002" startWordPosition="1290" endWordPosition="1293"> as assigning alternative weights to different edit operations or leveraging candidate frequencies (Damerau, 1964; Levenshtein, 1966). In recent years, statistical models have been widely used for the tasks of nat182 \x0cural language processing, including spelling correction task. (Brill and Moore, 2000) presented an improved error model over the one proposed by (Kernighan et al., 1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. Via explicit modeling of phonetic information of English words, (Toutanova and Moore, 2002) further investigated this issue. Both of them require misspelled/correct word pairs for training, and the latter also needs a pronunciation lexicon, but recently (Ahmad and Kondrak, 2005) demonstrated that it is also possible to learn such models automatically from query logs with the EM algorithm, which is similar to work of (Martin, 2004), learning from a very large corpus of raw text for removing non-word spelling errors in large corpus. All the work for non-word spelling correction focused on the current word itself without taking into account contextual information. Real-word spelling co</context>
</contexts>
<marker>Toutanova, Moore, 2002</marker>
<rawString>Toutanova K. and Moore R. Pronunciation modeling for improved spelling correction. Proceedings of the 40th annual meeting of ACL, pages 144-151, 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>