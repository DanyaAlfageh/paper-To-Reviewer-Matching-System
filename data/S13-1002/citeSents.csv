Of existing RTE approaches, the closest to ours is by CITATION, who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover,,
Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 CITATION,,
The best performer in 2012s competition was by CITATION, an ensemble system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics,,
Weighted inference, and combined structuraldistributional representations One approach to weighted inference in NLP is that of CITATION, who proposed viewing natural language interpretation as abductive inference,,
CITATION achieves 0.89 accuracy and 0.88 cws on the combined RTE-2 and RTE-3 dataset,,
The corpus itself was built by asking annotators from Amazon Mechanical Turk to describe very short video fragments CITATION,,
The organizers of the STS 2012 task CITATION sampled video descriptions and asked Turkers to assign similarity scores (ranging from 0 to 5) to pairs of sentences, without access to the video,,
Of existing RTE approaches, the closest to ours is by CITATION, who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover,,
Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 CITATION,,
The best performer in 2012s competition was by CITATION, an ensemble system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics,,
Weighted inference, and combined structuraldistributional representations One approach to weighted inference in NLP is that of CITATION, who proposed viewing natural language interpretation as abductive inference,,
Finally, we built an ensemble that combines the output of multiple systems using regression trained 17 \x0cMethod r AvgComb + no DIR 0.58 AvgComb + word DIR 0.60 AvgComb + phrase DIR 0.66 AvgComb w/o VarBind + no DIR 0.58 AvgComb w/o VarBind + word DIR 0.60 AvgComb w/o VarBind + phrase DIR 0.73 VS-Add 0.78 VS-Mul 0.58 VS-Pairwise 0.77 Ensemble (VS-Add + VS-Mul + VSPairwise) 0.83 Ensemble ([AvgComb + phrase DIR] + VS-Add + VS-Mul + VS-Pairwise) 0.85 Best MSR-Vid score in STS 2012 CITATION 0.87 Table 2: Results on the STS video dataset,,
This is the same technique used by CITATION except we used additive regression CITATION instead of linear regression since it gave better results,,
 the similarity of vectors representing the contexts in which they occur (CITATION; CITATION),,
The simplest models compute a phrase vector by adding the vectors for the individual words CITATION or by a component-wise product of word vectors (CITATION; CITATION),,
Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (CITATION; CITATION),,
Wide-coverage logic-based semantics Boxer CITATION is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures CITATION,,
It builds on the C&C CCG parser CITATION,,
Specifically, we utilize Markov Logic Networks (MLNs) CITATION, which employ weighted formulas in first-order logic to compactly ,,
CITATION conceptualize textual entailment as tree rewriting of syntactic graphs, where some rewriting rules are distributional inference rules,,
CITATION recognize paraphrases using a tree of vectors, a phrase structure tree in which each constituent is associated with a vector, and overall sentence similarity is computed by a classifier that integrates all pairwise similarities,,
(This is in contrast to approaches like CITATION and CITATION, who do not offer a proposal for using vectors at multiple levels in a syntactic tree simultaneously.) 3 MLN system Our system extends that of CITATION; 2013) to support larger-scale evaluation on standard benchmarks for both RTE and STS,,
First, we intend to incorporate logical form translations of existing distributional inference rule collections (e.g., (CITATION; Chan et al., 2011)),,
We plan to experiment with more sophisticated approaches to computing phrase vectors such as those recently presented by CITATION and CITATION,,
We plan to incorporate directed similarity measures such as those of CITATION and CITATION,,
We plan to explore coarser logical representations such as Minimal Recursion Semantics (MRS) CITATION,,
In particular, they perform worse than strict 16 \x0cMethod acc cws Chance 0.50 0.50 Bos & Markert, strict 0.52 0.55 Best system in RTE-1 challenge CITATION 0.59 0.62 VS-Add 0.49 0.53 VS-Mul 0.51 0.52 VS-Pairwise 0.50 0.50 AvgComb w/o VarBind + phrase DIR 0.52 0.53 Deterministic AND + phrase DIR 0.57 0.57 Table 1: Results on the RTE-1 Test Set,,
entailment from CITATION, a system that uses only logic,,
In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition CITATION or component-wise multiplication (CITATION; CITATION),,
Existing approaches that derive phrasal inference rules from distributional similarity (CITATIONa; CITATION; CITATION) precompile large lists of inference rules,,
to use a version of C&C that can produce the top-n parses together with parse re-ranking CITATION,,
First, we intend to incorporate logical form translations of existing distributional inference rule collections (e.g., (CITATION; Chan et al., 2011)),,
We plan to experiment with more sophisticated approaches to computing phrase vectors such as those recently presented by CITATION and CITATION,,
We plan to incorporate directed similarity measures such as those of CITATION and CITATION,,
Of existing RTE approaches, the closest to ours is by CITATION, who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover,,
Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 CITATION,,
The best performer in 2012s competition was by CITATION, an ensemble system that integrates many techniques including string similarity, n-,,
CITATION use this framework for RTE, deriving inference costs from WordNet similarity and properties of the syntactic parse,,
CITATION; 2013) proposed an approach to RTE that uses MLNs to combine traditional logical representations with distributional information in order to support probabilistic textual inference,,
This approach can be viewed as a bridge between CITATIONs purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators or entailment,,
CITATION conceptualize textual entailment as tree rewriting of syntactic graphs, where some rewriting rules are distributional inference rules,,
CITATION recognize paraphrases using a tree of vectors, a phrase structure tree in which each constituent is associated,,
4 Task 1: Recognizing Textual Entailment 4.1 Dataset In order to compare directly to the logic-based system of CITATION, we focus on the RTE-1 dataset CITATION, which includes 567 Text-Hypothesis (T-H) pairs in the development set and 800 pairs in the test set,,
The third scales pairwise words similarities to the sentence level using weighted average where weights are inverse document frequencies idf as suggested by CITATION (VSPairwise),,
For the RTE task, systems were evaluated using both accuracy and confidence-weighted score (cws) as used by CITATION and the RTE1 challenge CITATION,,
In particular, they perform worse than strict 16 \x0cMethod acc cws Chance 0.50 0.50 Bos & Markert, strict 0.52 0.55 Best system in RTE-1 challenge CITATION 0.59 0.62 VS-Add 0.49 0.53 VS-Mul 0.51 0.52 VS-Pairwise 0.50 0.50 AvgComb w/o VarBind + phrase DIR 0.52 0.53 Deterministic AND + phrase DIR 0.57 0.57 Table 1: Results on the RTE-1 Test Set,,
entailment from CITATION, a system that uses only logic,,
The simplest models compute a phrase vector by adding the vectors for the individual words CITATION or by a component-wise product of word vectors (CITATION; CITATION),,
Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (CITATION; CITATION),,
Wide-coverage logic-based semantics Boxer CITATION is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures CITATION,,
It builds on the C&C CCG parser CITATION,,
Specifically, we utilize Markov Logic Networks (MLNs) CITATION, which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models,,
CITATION achieves 0.89 accuracy and 0.88 cws on the combined RTE-2 and RTE-3 dataset,,
The corpus itself was built by asking annotators from Amazon Mechanical Turk to describe very short video fragments CITATION,,
The organizers of the STS 2012 task CITATION sampled video descriptions and asked Turkers to assign similarity scores (ranging from 0 to 5) to pairs of sentences, without access to the video,,
CITATION or by a component-wise product of word vectors (CITATION; CITATION),,
Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (CITATION; CITATION),,
Wide-coverage logic-based semantics Boxer CITATION is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures CITATION,,
It builds on the C&C CCG parser CITATION,,
Specifically, we utilize Markov Logic Networks (MLNs) CITATION, which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models,,
collections (e.g., (CITATION; Chan et al., 2011)),,
We plan to experiment with more sophisticated approaches to computing phrase vectors such as those recently presented by CITATION and CITATION,,
We plan to incorporate directed similarity measures such as those of CITATION and CITATION,,
We plan to explore coarser logical representations such as Minimal Recursion Semantics (MRS) CITATION,,
CITATION introduced a tractable subset of Markov Logic (TML) for which a future software release is planned,,
presented by CITATION and CITATION,,
We plan to incorporate directed similarity measures such as those of CITATION and CITATION,,
We plan to explore coarser logical representations such as Minimal Recursion Semantics (MRS) CITATION,,
CITATION introduced a tractable subset of Markov Logic (TML) for which a future software release is planned,,
4 Task 1: Recognizing Textual Entailment 4.1 Dataset In order to compare directly to the logic-based system of CITATION, we focus on the RTE-1 dataset CITATION, which includes 567 Text-Hypothesis (T-H) pairs in the development set and 800 pairs in the test set,,
The third scales pairwise words similarities to the sentence level using weighted average where weights are inverse document frequencies idf as suggested by CITATION (VSPairwise),,
For the RTE task, systems were evaluated using both accuracy and confidence-weighted score (cws) as used by CITATION and the RTE1 challenge CITATION,,
This is possible because MLNs constitute a flexible programming language based on probabilistic logic CITATION that can be easily adapted to support multiple types of linguistically useful inference,,
hods that compute phrase vectors from word vectors and tensors (CITATION; CITATION),,
Wide-coverage logic-based semantics Boxer CITATION is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures CITATION,,
It builds on the C&C CCG parser CITATION,,
Specifically, we utilize Markov Logic Networks (MLNs) CITATION, which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models,,
We plan to incorporate directed similarity measures such as those of CITATION and CITATION,,
We plan to explore coarser logical representations such as Minimal Recursion Semantics (MRS) CITATION,,
CITATION introduced a tractable subset of Markov Logic (TML) for which a future software release is planned,,
Logic-based representations (CITATION; CITATION) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms,,
Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els CITATION use contextual similarity to predict semantic similarity of words and phrases (CITATION; CITATION), and to model polysemy (CITATION; CITATION; CITATION),,
This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (CITATION; CITATION), which encourages developing new techniques to combine them,,
CITATION; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation,,
AvgComb + word DIR 0.60 AvgComb + phrase DIR 0.66 AvgComb w/o VarBind + no DIR 0.58 AvgComb w/o VarBind + word DIR 0.60 AvgComb w/o VarBind + phrase DIR 0.73 VS-Add 0.78 VS-Mul 0.58 VS-Pairwise 0.77 Ensemble (VS-Add + VS-Mul + VSPairwise) 0.83 Ensemble ([AvgComb + phrase DIR] + VS-Add + VS-Mul + VS-Pairwise) 0.85 Best MSR-Vid score in STS 2012 CITATION 0.87 Table 2: Results on the STS video dataset,,
This is the same technique used by CITATION except we used additive regression CITATION instead of linear regression since it gave better results,,
Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els CITATION use contextual similarity to predict semantic similarity of words and phrases (CITATION; CITATION), and to model polysemy (CITATION; CITATION; CITATION),,
This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (CITATION; CITATION), which encourages developing new techniques to combine them,,
CITATION; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation,,
Finally, Markov Logic Networks CITATION (MLNs) are used to perform weighted inference on the resulting knowledge base,,
Weighted inference, and combined structuraldistributional representations One approach to weighted inference in NLP is that of CITATION, who proposed viewing natural language interpretation as abductive inference,,
CITATION use this framework for RTE, deriving inference costs from WordNet similarity and properties of the syntactic parse,,
CITATION; 2013) proposed an approach to RTE that uses MLNs to combine traditional logical representations with distributional information in order to support probabilistic textual inference,,
This approach can be viewed as a bridge between CITATIONs purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators or entailment,,
CITATION conceptualize,,
CITATION recognize paraphrases using a tree of vectors, a phrase structure tree in which each constituent is associated with a vector, and overall sentence similarity is computed by a classifier that integrates all pairwise similarities,,
(This is in contrast to approaches like CITATION and CITATION, who do not offer a proposal for using vectors at multiple levels in a syntactic tree simultaneously.) 3 MLN system Our system extends that of CITATION; 2013) to support larger-scale evaluation on standard benchmarks for both RTE and STS,,
As discussed by CITATION), Boxers output is mapped to logical form and augmented with additional information to handle a variety of semantic phenomena,,
The distributional model we use contains all lemmas occurring at least 50 times in the Gigaword corpus CITATION except a list of stop words,,
Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els CITATION use contextual similarity to predict semantic similarity of words and phrases (CITATION; CITATION), and to model polysemy (CITATION; CITATION; CITATION),,
This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (CITATION; CITATION), which encourages developing new techniques to combine them,,
CITATION; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation,,
Finally, Markov Logic Networks CITATION (MLNs) are used to perform weighted inference on the resulting knowledge base,,
presenting the contexts in which they occur (CITATION; CITATION),,
The simplest models compute a phrase vector by adding the vectors for the individual words CITATION or by a component-wise product of word vectors (CITATION; CITATION),,
Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (CITATION; CITATION),,
Wide-coverage logic-based semantics Boxer CITATION is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures CITATION,,
It builds on the C&C CCG parser CITATION,,
Specifically, we utilize Markov Logic Networks (MLNs) CITATION, which employ weighted formulas in first-order logic to compactly encode complex undirected probabili,,
CITATION conceptualize textual entailment as tree rewriting of syntactic graphs, where some rewriting rules are distributional inference rules,,
CITATION recognize paraphrases using a tree of vectors, a phrase structure tree in which each constituent is associated with a vector, and overall sentence similarity is computed by a classifier that integrates all pairwise similarities,,
(This is in contrast to approaches like CITATION and CITATION, who do not offer a proposal for using vectors at multiple levels in a syntactic tree simultaneously.) 3 MLN system Our system extends that of CITATION; 2013) to support larger-scale evaluation on standard benchmarks for both RTE and STS,,
First, we intend to incorporate logical form translations of existing distributional inference rule collections (e.g., (CITATION; Chan et al., 2011)),,
We plan to experiment with more sophisticated approaches to computing phrase vectors such as those recently presented by CITATION and CITATION,,
We plan to incorporate directed similarity measures such as those of CITATION and CITATION,,
We plan to explore coarser logical representations such as Minimal Recursion Semantics (MRS) CITATION,,
CITATION achieves 0.89 accuracy and 0.88 cws on the combined RTE-2 and RTE-3 dataset,,
The corpus itself was built by asking annotators from Amazon Mechanical Turk to describe very short video fragments CITATION,,
The organizers of the STS 2012 task CITATION sampled video descriptions and asked Turkers to assign similarity scores (ranging from 0 to 5) to pairs of sentences, without ,,
Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 CITATION,,
The best performer in 2012s competition was by CITATION, an ensemble system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics,,
Weighted inference, and combined structuraldistributional representations One approach to weighted inference in NLP is that of CITATION, who proposed viewing natural language interpretation as abductive inference,,
CITATION use this framework for RTE, deriving inference costs from WordNet similarity and properties of the syntactic parse,,
CITATION; 2013) proposed an approach to RTE that uses MLNs to combine traditional logical representations with distributional information in order,,
Logic-based representations (CITATION; CITATION) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms,,
Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els CITATION use contextual similarity to predict semantic similarity of words and phrases (CITATION; CITATION), and to model polysemy (CITATION; CITATION; CITATION),,
e vector by adding the vectors for the individual words CITATION or by a component-wise product of word vectors (CITATION; CITATION),,
Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (CITATION; CITATION),,
Wide-coverage logic-based semantics Boxer CITATION is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures CITATION,,
It builds on the C&C CCG parser CITATION,,
Specifically, we utilize Markov Logic Networks (MLNs) CITATION, which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models,,
We use the open-source software package Alchemy CITATION for MLN inference, which allows computing the probability of a query literal given a set of weighted clauses as background knowledge and evidence,,
stributional inference rule collections (e.g., (CITATION; Chan et al., 2011)),,
We plan to experiment with more sophisticated approaches to computing phrase vectors such as those recently presented by CITATION and CITATION,,
We plan to incorporate directed similarity measures such as those of CITATION and CITATION,,
We plan to explore coarser logical representations such as Minimal Recursion Semantics (MRS) CITATION,,
CITATION introduced a tractable subset of Markov Logic (TML) for which a future software release is planned,,
Logic-based representations (CITATION; CITATION) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms,,
Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els CITATION use contextual similarity to predict semantic similarity of words and phrases (CITATION; CITATION), and to model polysemy (CITATION; CITATION; CITATION),,
This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (CITATION; CITATION), which encourages developing new techniques to combine them,,
CITATION; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation,,
2 Related work Distributional semantics Distributional models define the semantic relatedness of words as the similarity of vectors representing the contexts in which they occur (CITATION; CITATION),,
The simplest models compute a phrase vector by adding the vectors for the individual words CITATION or by a component-wise product of word vectors (CITATION; CITATION),,
Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (CITATION; CITATION),,
In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition CITATION or component-wise multiplication (CITATION; CITATION),,
Existing approaches that derive phrasal inference rules from distributional similarity (CITATIONa; CITATION; CITATION) precompile large lists of inference rules,,
In comparison to existing methods for creating textual inference rules (CITATIONb; CITATION), these rules are computed on the fly as needed, rather than pre-compiled,,
We replace deterministic conjunction by an average combiner, which encodes causal independence CITATION,,
We then treat similarity as degree of entailment, a move that has a long tradition (e.g., (CITATIONb; CITATION; CITATION)),,
In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition CITATION or component-wise multiplication (CITATION; CITATION),,
Existing approaches that derive phrasal inference rules from distributional similarity (CITATIONa; CITATION; CITATION) precompile large lists of inference rules,,
In comparison to existing methods for creating textual inference rules (CITATIONb; CITATION), these rules are computed on the fly as needed, rather than pre-compiled,,
We replace deterministic conjunction by an average combiner, which encodes causal independence CITATION,,
We then treat similarity as degree of entailment, a move that has a long tradition (e.g., (CITATIONb; CITATION; CITATION)),,
In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition CITATION or component-wise multiplication (CITATION; CITATION),,
Existing approaches that derive phrasal inference rules from distributional similarity (CITATIONa; CITATION; CITATION) precompile large lists of inference rules,,
For example, pizza slice and slice of pizza are both mapped to the 2 It is customary to transform raw counts in a way that captures association between target words and dimensions, for example through point-wise mutual information CITATION,,
2 Related work Distributional semantics Distributional models define the semantic relatedness of words as the similarity of vectors representing the contexts in which they occur (CITATION; CITATION),,
The simplest models compute a phrase vector by adding the vectors for the individual words CITATION or by a component-wise product of word vectors (CITATION; CITATION),,
Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (CITATION; CITATION),,
Wide-coverage logic-based semantics Boxer CITATION is a s,,
The third scales pairwise words similarities to the sentence level using weighted average where weights are inverse document frequencies idf as suggested by CITATION (VSPairwise),,
For the RTE task, systems were evaluated using both accuracy and confidence-weighted score (cws) as used by CITATION and the RTE1 challenge CITATION,,
2 Related work Distributional semantics Distributional models define the semantic relatedness of words as the similarity of vectors representing the contexts in which they occur (CITATION; CITATION),,
The simplest models compute a phrase vector by adding the vectors for the individual words CITATION or by a component-wise product of word vectors (CITATION; CITATION),,
Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (CITATION; CITATION),,
Wide-coverage logic-based semantics Boxer CITATION is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures CITATION,,
It builds on the C&C CCG parser CITATION,,
In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition CITATION or component-wise multiplication (CITATION; CITATION),,
Existing approaches that derive phrasal inference rules from distributional similarity (CITATIONa; CITATION; CITATION) precompile large lists of inference rules,,
Logic-based representations (CITATION; CITATION) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms,,
Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els CITATION use contextual similarity to predict semantic similarity of words and phrases (CITATION; CITATION), and to model polysemy (CITATION; CITATION; CITATION),,
This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (CITATION; CITATION), which encourages developing new techniques to combine them,,
CITATION; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation,,
2 Related work Distributional semantics Distributional models define the semantic relatedness of words as the similarity of vectors representing the contexts in which they occur (CITATION; CITATION),,
The simplest models compute a phrase vector by adding the vectors for the individual words CITATION or by a component-wise product of word vectors (CITATION; CITATION),,
Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (CITATION; CITATION),,
Wide-coverage logic-based semantics Boxer CITATION is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures CITATION,,
It builds on the C&C CCG parser CITATION,,
In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition CITATION or component-wise multiplication (CITATION; CITATION),,
Existing approaches that derive phrasal inference rules from distributional similarity (CITATIONa; CITATION; CITATION) precompile large lists of inference rules,,
Logic-based representations (CITATION; CITATION) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms,,
Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els CITATION use contextual similarity to predict semantic similarity of words and phrases (CITATION; CITATION), and to model polysemy (CITATION; CITATION; CITATION),,
In comparison to existing methods for creating textual inference rules (CITATIONb; CITATION), these rules are computed on the fly as needed, rather than pre-compiled,,
We replace deterministic conjunction by an average combiner, which encodes causal independence CITATION,,
We chose to use the average evidence combiner for MLNs introduced by CITATION,,
To reduce the impact of misparsing, we plan to use a version of C&C that can produce the top-n parses together with parse re-ranking CITATION,,
First, we intend to incorporate logical form translations of existing distributional inference rule collections (e.g., (CITATION; Chan et al., 2011)),,
Weighted inference, and combined structuraldistributional representations One approach to weighted inference in NLP is that of CITATION, who proposed viewing natural language interpretation as abductive inference,,
CITATION use this framework for RTE, deriving inference costs from WordNet similarity and properties of the syntactic parse,,
CITATION; 2013) proposed an approach to RTE that uses MLNs to combine traditional logical representations with distributional information in order to support probabilistic textual inference,,
This approach can be viewed as a bridge between CITATIONs purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators,,
We then treat similarity as degree of entailment, a move that has a long tradition (e.g., (CITATIONb; CITATION; CITATION)),,
This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (CITATION; CITATION), which encourages developing new techniques to combine them,,
CITATION; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation,,
Finally, Markov Logic Networks CITATION (MLNs) are used to perform weighted inference on the resulting knowledge base,,
Logic-based representations (CITATION; CITATION) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms,,
Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els CITATION use contextual similarity to predict semantic similarity of words and phrases (CITATION; CITATION), and to model polysemy (CITATION; CITATION; CITATION),,
This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (CITATION; CITATION), which encourages developing new techniques to combine them,,
CITATION; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation,,
This approach can be viewed as a bridge between CITATIONs purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators or entailment,,
CITATION conceptualize textual entailment as tree rewriting of syntactic graphs, where some rewriting rules are distributional inference rules,,
CITATION recognize paraphrases using a tree of vectors, a phrase structure tree in which each constituent is associated with a vector, and overall sentence similarity is computed by a classifier that integrates all pairwise similarities,,
(This is in contrast to approaches like CITATION and CITATION, who do not offer a proposal for using vectors at multiple levels in a syntactic tree simultaneously.) 3 MLN system Our system extends that of CITATION; 2013) to support larger-scale evaluation on standard benchmarks for both RTE and STS,,
CITATION; 2013) proposed an approach to RTE that uses MLNs to combine traditional logical representations with distributional information in order to support probabilistic textual inference,,
This approach can be viewed as a bridge between CITATIONs purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators or entailment,,
CITATION conceptualize textual entailment as tree rewriting of syntactic graphs, where some rewriting rules are distributional inference rules,,
CITATION recognize paraphrases using a tree of vectors, a phrase structure tree in which each constituent is associated with a vector, and overall sentence similarity is computed by a classifier that integrates all pairwise similarities,,
(This is in contrast to approaches like CITATION and CITATION, who do not offer a proposal for using vectors at multiple levels in a syntactic tree simultaneously.) 3 ML,,
In comparison to existing methods for creating textual inference rules (CITATIONb; CITATION), these rules are computed on the fly as needed, rather than pre-compiled,,
We replace deterministic conjunction by an average combiner, which encodes causal independence CITATION,,
We then treat similarity as degree of entailment, a move that has a long tradition (e.g., (CITATIONb; CITATION; CITATION)),,
In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition CITATION or component-wise multiplication (CITATION; CITATION),,
Existing approaches that derive phrasal inference rules from distributional similarity (CITATIONa; CITATION; CITATION) precompile large lists of inference rules,,
Logic-based representations (CITATION; CITATION) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms,,
Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els CITATION use contextual similarity to predict semantic similarity of words and phrases (CITATION; CITATION), and to model polysemy (CITATION; CITATION; CITATION),,
This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (CITATION; CITATION), which encourages developing new techniques to combine them,,
CITATION; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation,,
Logic-based representations (CITATION; CITATION) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms,,
Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els CITATION use contextual similarity to predict semantic similarity of words and phrases (CITATION; CITATION), and to model polysemy (CITATION; CITATION; CITATION),,
This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (CITATION; CITATION), which encourages developing new techniques to combine them,,
CITATION; 2013) propose a framework for combining logic and distributional models in which logical form is the prima,,
