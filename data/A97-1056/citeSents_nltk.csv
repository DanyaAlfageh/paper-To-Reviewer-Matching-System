This paper considers two significance tests, the exact conditional test CITATION and the Log-likelihood ratio statistic G2 CITATION, and two information criteria, Akaike\'s Information Criterion (AIC) CITATION and the Bayesian Information Criterion (BIC) CITATION.,,
Because their joint distributions have such closed-form expressions, the parameters can be estimated directly from the training data without the need for an iterative fitting procedure (as is required, for example, to estimate the parameters of maximum entropy models; CITATION).,,
Although there are far fewer decomposable models than log-linear models for a given set of feature variables, it has been shown that they have substantially the same expressive power CITATION.,,
In order to utilize models with more complicated interactions among feature variables, (CITATIONb) introduce the use of sequential model selection and decomposable models for word-sense disambiguation.,,
~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., CITATION, CITATION, and CITATION present techniques for identifying the optimal feature to use in disambiguation).,,
Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., CITATION), but within this framework no systematic study of interactions has been proposed.,,
CITATION and CITATION) but, while it is a type of model selection, the models are not parametric.,,
2 Decomposable Models Decomposable models are a subset of the class of graphical models CITATION which are in turn a subset of the class of log-linear models CITATION.,,
They are characterized by the following properties (CITATIONb): 1.,,
This paper considers two significance tests, the exact conditional test CITATION and the Log-likelihood ratio statistic G2 CITATION, and two information criteria, Akaike\'s Information Criterion (AIC) CITATION and the Bayesian Information Criterion (BIC) CITATION.,,
~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., CITATION, CITATION, and CITATION present techniques for identifying the optimal feature to use in disambiguation).,,
Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., CITATION), but within this framework no systematic study of interactions has been proposed.,,
CITATION and CITATION) but, while it is a type of model selection, the models are not parametric.,,
However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (CITATIONa), CITATION, CITATION, and CITATION).,,
In order to utilize models with more complicated interactions among feature variables, (CITATIONb) introduce the use of sequential model selection and decomposable models for word-sense disambiguation.,,
~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., CITATION, CITATION, and CITATION present techniques for identifying the optimal feature to use in disambiguation).,,
Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., CITATION), but within this framework no systematic study of interactions has been proposed.,,
CITATION and CITATION) but, while it is a type of model selection, the models are not parametric.,,
2 Decomposable Models Decomposable models are a subset of the class of graphical models CITATION which are in turn a subset of the class of log-linear models CITATION.,,
They are characterized by the following properties (CITATIONb): 1.,,
However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (CITATIONa), CITATION, CITATION, and CITATION).,,
In order to utilize models with more complicated interactions among feature variables, (CITATIONb) introduce the use of sequential model selection and decomposable models for word-sense disambiguation.,,
~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., CITATION, CITATION, and CITATION present techniques for identifying the optimal feature to use in disambiguation).,,
2 Decomposable Models Decomposable models are a subset of the class of graphical models CITATION which are in turn a subset of the class of log-linear models CITATION.,,
They are characterized by the following properties (CITATIONb): 1.,,
However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (CITATIONa), CITATION, CITATION, and CITATION).,,
In order to utilize models with more complicated interactions among feature variables, (CITATIONb) introduce the use of sequential model selection and decomposable models for word-sense disambiguation.,,
~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., CITATION, CITATION, and CITATION present techniques for identifying the optimal feature to use in disambiguation).,,
5 Experimental Data The sense-tagged text and feature set used in these experiments are the same as in CITATION.,,
However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (CITATIONa), CITATION, CITATION, and CITATION).,,
In order to utilize models with more complicated interactions among feature variables, (CITATIONb) introduce the use of sequential model selection and decomposable models for word-sense disambiguation.,,
~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., CITATION, CITATION, and CITATION present techniques for identifying the optimal feature to use in disambiguation).,,
Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., CITATION), but within this framework no systematic study of interactions has been proposed.,,
CITATION and CITATION) but, while it is a type of model selection, the models are not parametric.,,
CITATION and CITATION).,,
However, if the data sample can be adequately characterized by a less complex model, i.e., a model in which there are fewer interactions between variables, then more reliable parameter estimates can be obtained: In the case of decomposable models (CITATION; see below), the parameters of a less complex model are parameters of marginal distributions, so the MLEs involve frequencies of combinations of values of only subsets of the variables in the model.,,
However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (CITATIONa), CITATION, CITATION, and CITATION).,,
In order to utilize models with more complicated interactions among feature variables, (CITATIONb) introduce the use of sequential model selection and decomposable models for word-sense disambiguation.,,
~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., CITATION, CITATION, and CITATION present techniques for identifying the optimal feature to use in disambiguation).,,
This paper considers two significance tests, the exact conditional test CITATION and the Log-likelihood ratio statistic G2 CITATION, and two information criteria, Akaike\'s Information Criterion (AIC) CITATION and the Bayesian Information Criterion (BIC) CITATION.,,
However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (CITATIONa), CITATION, CITATION, and CITATION).,,
In order to utilize models with more complicated interactions among feature variables, (CITATIONb) introduce the use of sequential model selection and decomposable models for word-sense disambiguation.,,
~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., CITATION, CITATION, and CITATION present techniques for identifying the optimal feature to use in disambiguation).,,
However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (CITATIONa), CITATION, CITATION, and CITATION).,,
In order to utilize models with more complicated interactions among feature variables, (CITATIONb) introduce the use of sequential model selection and decomposable models for word-sense disambiguation.,,
~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., CITATION, CITATION, and CITATION present techniques for identifying the optimal feature to use in disambiguation).,,
2An alternative feature set for this data is utilized with an exemplar-based learning algorithm in CITATION.,,
CITATION and CITATION).,,
However, if the data sample can be adequately characterized by a less complex model, i.e., a model in which there are fewer interactions between variables, then more reliable parameter estimates can be obtained: In the case of decomposable models (CITATION; see below), the parameters of a less complex model are parameters of marginal distributions, so the MLEs involve frequencies of combinations of values of only subsets of the variables in the model.,,
The significance of G 2 based on the exact conditional distribution does not rely on an asymptotic approximation and is accurate for sparse and skewed data samples CITATION 4.2 Information criteria The family of model evaluation criteria known as information criteria have the following expression: IC,~ = G 2 - ~ x dof (3) where G~ and dof are defined above.,,
This paper considers two significance tests, the exact conditional test CITATION and the Log-likelihood ratio statistic G2 CITATION, and two information criteria, Akaike\'s Information Criterion (AIC) CITATION and the Bayesian Information Criterion (BIC) CITATION.,,
2 Decomposable Models Decomposable models are a subset of the class of graphical models CITATION which are in turn a subset of the class of log-linear models CITATION.,,
They are characterized by the following properties (CITATIONb): 1.,,
Because their joint distributions have such closed-form expressions, the parameters can be estimated directly from the training data without the need for an iterative fitting procedure (as is required, for example, to estimate the parameters of maximum entropy models; CITATION).,,
Although there are far fewer decomposable models than log-linear models for a given set of feature variables, it has been shown that they have substantially the same expressive power CITATION.,,
es classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (CITATIONa), CITATION, CITATION, and CITATION).,,
In order to utilize models with more complicated interactions among feature variables, (CITATIONb) introduce the use of sequential model selection and decomposable models for word-sense disambiguation.,,
~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., CITATION, CITATION, and CITATION present techniques for identifying the optimal feature to use in disambiguation).,,
Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., CITATION), but within this framework no systematic study of interactions has been proposed.,,
CITATION and CITATION) but, while it is a type of model selection, the models are not parametric.,,
CITATION and CITATION).,,
However, if the data sample can be adequately characterized by a less complex model, i.e., a model in which there are fewer interactions between variables, then more reliable parameter estimates can be obtained: In the case of decomposable models (CITATION; see below), the parameters of a less complex model are parameters of marginal distributions, so the MLEs involve frequencies of combinations of values of only subsets of the variables in the model.,,
