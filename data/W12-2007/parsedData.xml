<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<copyright confidence="0.119953">
b&amp;apos;The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 6372,
</copyright>
<note confidence="0.237434">
Montreal, Canada, June 3-8, 2012. c
</note>
<title confidence="0.7115225">
2012 Association for Computational Linguistics
Measuring the Use of Factual Information in Test-Taker Essays
</title>
<author confidence="0.708212">
Beata Beigman Klebanov
</author>
<affiliation confidence="0.734026">
Educational Testing Service
</affiliation>
<address confidence="0.9050145">
660 Rosedale Road
Princeton, NJ 08541, USA
</address>
<email confidence="0.967692">
bbeigmanklebanov@ets.org
</email>
<author confidence="0.954462">
Derrick Higgins
</author>
<affiliation confidence="0.917681">
Educational Testing Service
</affiliation>
<address confidence="0.942006">
660 Rosedale Road
Princeton, NJ 08541, USA
</address>
<email confidence="0.990602">
dhiggins@ets.org
</email>
<sectionHeader confidence="0.990509" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993425">
We describe a study aimed at measuring the
use of factual information in test-taker essays
and assessing its effectiveness for predicting
essay scores. We found medium correlations
with the proposed measures, that remained
significant after the effect of essay length was
factored out. The correlations did not dif-
fer substantionally between a simple, rela-
tively robust measure vs a more sophisticated
measure with better construct validity. Impli-
cations for development of automated essay
scoring systems are discussed.
</bodyText>
<sectionHeader confidence="0.998183" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990753313725491">
Automated scoring of essays deals with various as-
pects of writing, such as grammar, usage, mecha-
nics, as well as organization and content (Attali
and Burstein, 2006). For assessment of content,
the focus is traditionally on topical appropriateness
of the vocabulary (Attali and Burstein, 2006; Lan-
dauer et al., 2003; Louis and Higgins, 2010; Chen
et al., 2010; De and Kopparapu, 2011; Higgins et
al., 2006; Ishioka and Kameda, 2006; Kakkonen et
al., 2005; Kakkonen and Sutinen, 2004; Lemaire
and Dessus, 2001; Rose et al., 2003; Larkey, 1998),
although recently other aspects, such as detection
of sentiment or figurative language, have started to
attract attention (Beigman Klebanov et al., 2012;
Chang et al., 2006).
The nature of factual information used in an es-
say has not so far been addressed, to our knowledge;
yet a misleading premise, insufficient factual basis,
or an example that flies in the face of the readers
knowledge clearly detract from an essays quality.
This paper presents a study on assessing the use
of factual knowledge in argumentative essays on ge-
neral topics written for a graduate school entrance
exam. We propose a definition of fact, and an opera-
tionalization thereof. We find that the proposed mea-
sure has positive medium-strength correlation with
essay grade, which remains significant after the im-
pact of essay length is factored out. In order to
quantify which aspects of the measure drive the ob-
served correlations, we gradually relax the measure-
ment procedure, down to a simple and robust proxy
measure. Surprisingly, we find that the correlations
do not change throughout the relaxation process. We
discuss the findings in the context of validity vs re-
liability of measurement, and point out implications
for automated essay scoring.
2 What is a Fact?
To help articulate the notion of fact, we use the fol-
lowing definition from a seminal text in argumenta-
tion theory: ... in the context of argumentation, the
notion of fact is uniquely characterized by the idea
that is held of agreements of a certain type relating
to certain data, those which refer to an objective rea-
lity, and, in Poincares words, designate essentially
what is common to several thinking beings, and
could be common to all (Perelman and Olbrechts-
Tyteca, 1969, 67). Factuality is thus a matter of se-
lecting certain kinds of data and securing a certain
type of agreement over those data.
Of the different statements that refer to objec-
tive reality, the term facts is used to designate ob-
</bodyText>
<page confidence="0.998521">
63
</page>
<bodyText confidence="0.997521903225807">
\x0cjects of precise, limited agreement (Perelman and
Olbrechts-Tyteca, 1969, 69). These are contrasted
with presumptions statements connected to what
is normal and likely (ibid.). We suggest that the dis-
tinctions in the scope of the required agreement can
be related to the referential device used in a state-
ment: If the reference is more rigid (Kripke, 1980),
that is, less prone to change in time and to inde-
terminacy of the boundaries, the scope of the ne-
cessary agreement is likely to be more precise and
limited. With proper names prototypically being the
most rigid designators, we will focus our efforts on
statements about named entities.1
Perhaps the simplest model of the universal au-
dience is an encyclopedia a body of knowledge
that is verified by experts, and is, therefore, com-
mon to several thinking beings, and could be com-
mon to all by virtue of the authority of the experts
and the wide availability of the resource. However,
many facts known to various groups of people that
could be known to all are absent from any encyclo-
pedia. The knowledge contained in the WWW at
large, reaching not only statements explicitly con-
tributed to an encyclopedia but also those made by
people on their blogs is perhaps as close as it gets
to a working model of the universal audience.
Recent developments in Open Information Ex-
traction make it possible to tap into this vast know-
ledge resource. Indeed, fact-checking is one of the
applications the developers of OpenIE have in mind
for their emergent technology (Etzioni et al., 2008).
</bodyText>
<sectionHeader confidence="0.994377" genericHeader="method">
3 Open Information Extraction
</sectionHeader>
<bodyText confidence="0.991560625">
Traditionally, the goal of an information extrac-
tion system is automated population of structured
databases of events or concepts of interest and their
properties by analyzing large corpora of text (Chin-
chor et al., 1993; Onyshkevych, 1993; Grishman and
Sundheim, 1995; Ravichandran and Hovy, 2002;
Agichtein and Gravano, 2000; Davidov and Rap-
poport, 2009).
</bodyText>
<page confidence="0.937418">
1
</page>
<bodyText confidence="0.999382526315789">
For example, Barack Obama picks out precisely one per-
son, and the same one in 2010 as it did in 1990. In contrast, the
current US president picks out different people every 4-8 years.
For indeteminacy of boundaries, consider a statement like US
officials are wealthy. To determine its truth, one must first se-
cure agreement on acceptable referents of US officials.
In contrast, the recently proposed Open Informa-
tion Extraction paradigm aims to detect related pairs
of entities without knowing in advance what kinds of
relations exist between entities in the source data and
without any seeding (Banko and Etzioni, 2008). The
possibility of such extraction in English is attributed
by the authors to a small number of syntactic pat-
terns that realize binary relations between entities.
In particular, they found that almost 40% of such re-
lations are realized by the argument-verb-argument
pattern (henceforth, AVA) (see Table 1 in Banko and
Etzioni (2008)).
The TextRunner system (Banko and Etzioni,
2008) is trained using a CRF classifier on S-V-O
tuples from a parsed corpus as positive examples,
and tuples that violate phrasal structure as negative
ones. The examples are described using features
that do not require parsing or semantic role labe-
ling. Features include part-of-speech tags, regular
expressions (detecting capitalization, punctuation,
etc.), context words belonging to closed classes, and
conjunctions of features occurring in adjacent posi-
tions within six words of the current word.
TextRunner achieves P=0.94, R=0.65, and F-
Score=0.77 on the AVA pattern (Banko and Etzioni,
2008). We note that all relations in the test sen-
tences involve a predicate connecting two named en-
tities, or a named entity and a date.2 The authors
kindly made available to us for research purposes a
database of about 2 bln AVA extractions produced
by TextRunner; this database was used in the expe-
riments reported below.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.998323333333333">
We randomly sampled essays written on 10 diffe-
rent prompts, 200 essays per prompt. Essays are
graded on the scale of 1-6; the distribution of grades
</bodyText>
<table confidence="0.913665666666667">
is shown in table 1.
Grade 1 2 3 4 5 6
% 0.6 4.9 23.5 42.6 23.8 4.7
</table>
<tableCaption confidence="0.999399">
Table 1: The distribution of grades for 2,000 essays.
</tableCaption>
<page confidence="0.685744">
2
</page>
<footnote confidence="0.457079">
http://www.cs.washington.edu/research/knowitall/hlt-
naacl08-data.txt
</footnote>
<page confidence="0.973861">
64
</page>
<subsectionHeader confidence="0.391186">
\x0c5 Building Queries from Essays
</subsectionHeader>
<bodyText confidence="0.998402571428571">
We define a query as a 3-tuple &lt;NE,?,NP&gt;,3 where
NE is a named entity and NP is a noun phrase from
the same or neighboring sentence in a test-taker es-
say (the selection process is described in section
5.2). We use the pattern of predicate matches against
the TextRunner database to assess the degree and the
equivocality of the connection between NE and NP.
</bodyText>
<subsectionHeader confidence="0.971141">
5.1 Named Entities in Test-Taker Essay
</subsectionHeader>
<bodyText confidence="0.997713333333333">
We use the Stanford Named Entity Recognizer
(Finkel et al., 2005) that tags named entities as peo-
ple, locations, organizations, and miscellaneous. We
annotated a sample of 90 essays for named entities;
the sample yielded 442 tokens, which we classified
as shown in Table 2. The Enamex classes (people,
locations, organizations) account for 58% of all the
entities in the sample. The recognizers recall of
people and locations is excellent (though they are
not always classified correctly see caption of Ta-
ble 2), although test-taker essays feature additional
entity types that are not detected as well.
</bodyText>
<table confidence="0.989118444444444">
Category Recall Examples
Location 0.98 Iraq, USA
Person 0.96 George W. Bush, Freud
Org. 0.87 Guggenheim Foundation
Gov. 0.79 No Child Left Behind
Awards 0.79 Nobel Prize
Events 0.68 Civil War, World War I
Sci &amp; Tech 0.59 GPS, Windows 3.11
Art 0.44 Beowulf, Little Women
</table>
<tableCaption confidence="0.996331">
Table 2: Recall of the Stanford NER by category. Note
</tableCaption>
<bodyText confidence="0.997789125">
that an entity is counted as recalled as long as it is iden-
tified as belonging to any NE category, even if it is mis-
classified. For example, Freud is tagged as location, but
we count it towards the recall of people.
In terms of precision, we observed that the tagger
made few clear mistakes, such as tagging sentence-
initial adverbs and their mis-spelled versions as
named entities (Eventhough, Afterall). The bulk of
</bodyText>
<page confidence="0.989715">
3
</page>
<bodyText confidence="0.999724133333333">
We do not attempt matching the predicate, as (1) in many
cases there is no clearly lexicalized predicate (see the discussion
of single step patterns in section 5.2) and (2) adding a predicate
field would make matches against the database sparser (see sec-
tion 6.1).
the 96 items over-generated by the tagger are in the
grey area while we havent marked them, they
are not clearly mistakes. A common case are names
of national and religious groups, such as Muslim
or Turkish, or capitalizations of otherwise common
nouns for emphasis and elevation, such as Arts or
Masters. Given our objective to ground the queries
in items with specific referents, these are less sui-
table. If all such cases are counted as mistakes, the
taggers precision is 82%.
</bodyText>
<subsectionHeader confidence="0.999771">
5.2 Selection of NPs
</subsectionHeader>
<bodyText confidence="0.998782294117647">
We employ a grammar-based approach for selecting
NPs. We use the Stanford dependency parser (de
Marneffe et al., 2006; Klein and Manning, 2003) to
determine dependency relations.
In order to find out which dependency paths con-
nect between named entities and clearly related NPs
in essays, we manually marked concepts related to
95 NEs in 10 randomly sampled essays. We marked
210 query-able concepts in total. The resulting 210
dependency paths were classified according to the
direction of the movement.
Out of the 210 paths, 51 (24%) contain a single
upward or downard step, that is, are cases where
the NE is the head of the constituent in which the
NP is embedded, or the other way around. Some
examples are shown in Figure 1. Note that the pre-
dicate connecting NE and NP is not lexicalized, but
the existence of connection is signaled by the close-
knit grammatical pattern.
The most prolific family of paths starts with an
upward step, followed by a sequences of 1-4 down-
wards steps; 71 (34%) of all paths are of this type.
Most typically, the first upward move connects the
NE to the predicate of which it is an argument, and,
down from there, to either the head of another argu-
ment () or to an arguments heads modifier ().
These are explicit relations, where the relation is
typically lexicalized by the predicate.
We expand the context of extraction beyond a sin-
gle sentence only for NEs classified as PERSON. We
apply a gazetteer of private names by gender from
US Census 2010 to expand a NE of a given gen-
der with the appropriate personal pronouns; a word
that is a part of the original name (only surname, for
</bodyText>
<page confidence="0.968536">
4
</page>
<bodyText confidence="0.331429">
NE=Kroemer; NP=Heterojunction Bipolar Transitor
</bodyText>
<page confidence="0.994852">
65
</page>
<bodyText confidence="0.929081166666667">
\x0c a Nobel Prize in a science field
Chaucer, in the 14 century, ...
the prestige of the Nobel Prize
Kidmans talent
Kroemer received the Nobel Prize
Kroemer received the Nobel Prize for his work
</bodyText>
<figureCaption confidence="0.8176885">
on the Heterojunction Bipolar Transitor4
Figure 1: Examples of dependency paths used for query
construction.
example), is also considered an anaphor and a can-
</figureCaption>
<bodyText confidence="0.9826942">
didate for expansion. We expand the context of the
PERSON entity as long as the subsequent sentence
uses any of the anaphors for the name. This way, we
hope to capture an extended discussion of a named
entity and construct queries around its anaphoric
mentions just as we do around the regular, NE men-
tion. A name that is not predominantly male or fe-
male is not expanded with personal pronouns. Ta-
ble 3 shows the distribution of queries automatically
generated from the sample of 2,000 essays.
</bodyText>
<equation confidence="0.609057888888889">
2,817 15.9%
798 4.5%
813 4.6%
372 2.1%
4,940 27.8%
2,691 15.1%
1,568 8.8%
3,772 21.2%
total 17,771 100%
</equation>
<tableCaption confidence="0.9974">
Table 3: Distribution of queries by path type.
</tableCaption>
<sectionHeader confidence="0.754569" genericHeader="method">
6 Matching and Filtering Queries
</sectionHeader>
<subsectionHeader confidence="0.763555">
6.1 Relaxation for improved matching
</subsectionHeader>
<bodyText confidence="0.998398611111111">
To estimate the coverage of the fact repository with
respect to the queries extracted from essays, we sub-
mit each query to the TextRunner repository in the
&lt;NE,?,NP&gt; format and record the number of times
the repository returned any matches at all. The per-
centage of matched queries is 21%. To increase the
chances of finding a match, we process the NP to re-
move determiners and pre-modifiers of the head that
are very frequent words, such as removing a very
from a very beautiful photograph.
Additionally, we produce three variants of the NP.
The first, NP1, contains only the sequence of nouns
ending with the head noun; in the example, NP1
would be photograph. The second variant, NP2,
contains only the word that is rarest in the whole
of NP. All capitalized words are given the lowest
frequency of 1. Thus, if any of the NP words are
capitalized, the NP2 would either contain an out of
vocabulary word to the left of the first capitalized
word, or the leftmost capitalized word. This means
that names would typically be split such that only the
first name is taken. For example, the NP the author
Orhan Phamuk would generate NP2 Orhan. When
no capitalized words exist, we take the rarest one,
thus a NP category 3 hurricane would yield NP2
hurricane. The third variant only applies to NPs
with capitalized parts, and takes the rightmost capi-
talized word in the query. Thus, the NP the actress
Nicole Kidman would yield NP3 Kidman.
Applying these procedures to every NP inflates
the number of actual queries posed to the TextRun-
ner repository by almost two-fold (31,211 instead of
17,771), while yielding a 50% increase in the num-
ber of cases where at least one variant of the original
query had at least one match against the repository
(from 21% to 35%).
</bodyText>
<subsectionHeader confidence="0.93724">
6.2 Match-specific filters
</subsectionHeader>
<bodyText confidence="0.928571125">
In order to zero in on matches that correpond to fac-
tual statements and indeed pertain to the queried ar-
guments, we implement a number of filters.
Predicate filters
We filter out modal and hedged predicates, using
lists of relevant markers. We remove predicates like
might turn out to be or possibly attended, as well as
future tense predicates (marked with will).
</bodyText>
<subsectionHeader confidence="0.350658">
Argument filters
</subsectionHeader>
<bodyText confidence="0.88284775">
For matches that passed the predicate filters, we
check the arguments. Let mARG be the actual
string that matched ARG (ARG {NE,NP}). Let
EC (Essay Context) refer to source sentence(s) in
</bodyText>
<page confidence="0.88394">
66
</page>
<bodyText confidence="0.967413545454546">
\x0cthe essay.5 We filter out the following matches:
Capitalized words follow ARG in mARG that
are not in EC;
&gt;1 capitalized or rare words precede ARG in
mARG that are not in EC and not honorifics;
mARG is longer than 8 words;
More than 3 words follow ARG in mARG.
The filters target cases where mARG is more spe-
cific than ARG, and so the connection to ARG might
be tenuous, such as ARG=Harriet Beecher Stowe,
mARG = Harriet Beecher Stowe Center.
</bodyText>
<subsectionHeader confidence="0.728411">
6.3 Filters based on overall pattern of matches
</subsectionHeader>
<subsubsectionHeader confidence="0.695773">
6.3.1 Negation filter
</subsubsectionHeader>
<bodyText confidence="0.934999727272727">
For all matches for a given query that passed the
filters in section 6.2, we tally positive vs negative
predicates.6 If the ratio of negative to positive is
above a threshold (we use 0.1), we consider the
query an unsuitable candidate for being potentially
common to all, and therefore do not credit the au-
thor with having mentioned a fact.
This criterion of potential acceptance by a uni-
versal audience fails a query such as &lt;Barack
Obama,?,US citizen&gt;, based on the following pat-
tern of matches:
</bodyText>
<figure confidence="0.995757714285714">
Count Predicate
10 is not
4 is
2 was always
1 is really
1 isnt
1 was not
</figure>
<bodyText confidence="0.95658575">
In a similar fashion, an essay writers statement
that The beating of Rodney King in Los Angeles
... made for tense race relations is not quite in ac-
cord with the 16 hits garnered by the statement The
Los Angeles riots were not caused by the Rodney
King verdict, against other hits with predicates like
erupted after, occurred after, resulted from, were
sparked by, followed.
</bodyText>
<page confidence="0.8745">
5
</page>
<bodyText confidence="0.879031">
A single sentence, unless anaphor-based expansion was
carried out; see section 5.2.
</bodyText>
<page confidence="0.975149">
6
</page>
<bodyText confidence="0.997067107142857">
We use a list of negation markers to detect those.
Somewhat more subtly, the connection between
Albert Einstein and atomic bomb, articulated as For
example, Albert Einsteins accidental development
of the atomic bomb has created a belligerent tech-
nological front by a test-taker, is opposed by 6 hits
with the predicate did not build against matches with
predicates such as paved the way to, led indirectly
to, helped in, created the theory of. The conflicting
accounts seem to reflect a lack of consensus on the
degree of Einsteins responsibility.
The cases above clearly demonstrate the implica-
tions of the argumentative notion of facts used in
our project. Facts are statements that the audience is
prepared to accept without further justification, dif-
ferently from arguments, and even from presump-
tions (statements about what is normal and likely),
for which, as Perelman and Olbrechts-Tyteca (1969)
observe, additional justification is beneficial for
strengthening the audiences adherence. Certainly
in the Obama case and possibly in others, a different
notion of factuality, for example, a notion that em-
phasizes availability of legally acceptable suppor-
ting evidence, would have led to a different result.
Yet, in an ongoing instance of argumentation, the
mere need to resort to such a proof is already a sign
that the audience is not prepared to accept a state-
ment as a fact.
</bodyText>
<subsectionHeader confidence="0.997012">
6.4 Additional filters
</subsectionHeader>
<bodyText confidence="0.9993796">
We also implemented a number of filters aimed at
detecting excessive diversity in the matches, which
could suggest that there is no clear and systema-
tic relation between the NE and the NP. The filters
are conjunctions of thresholds operating over mea-
sures such as purity of matches (percentage of exact
matches in NE or NP), degree of overlap of non-pure
matches with the context of the query in the essay,
clustering of the predicates (recurrence of the same
predicates across matches), general frequencies of
</bodyText>
<sectionHeader confidence="0.7190425" genericHeader="method">
NE and NP.
7 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.992928">
7.1 Manual check of queries
</subsectionHeader>
<bodyText confidence="0.99306425">
A manual check of a small subset of queries was ini-
tially intended as an interim evaluation of the query
construction process, to see how often the produced
queries are deficient candidates for later verification.
</bodyText>
<page confidence="0.996676">
67
</page>
<bodyText confidence="0.9974779">
\x0cHowever, we also decided to include a human fact-
check of the queries that were found to be verifiable,
to see the kinds of factual mistakes made in essays.
A research assistant was asked to classify 500
queries into Wrong (the NE and NP are not
related in the essay), Trivial (almost any NE
could be substituted, as in &lt;WWI,?, Historians&gt;),
Subjective (&lt;T.S.Eliot,?,the most frightening poet
of all time&gt;), VC verifiable and correct, VI veri-
fiable and incorrect. Table 4 shows the distribution.
</bodyText>
<table confidence="0.5464035">
W T S VC VI
18% 13% 13% 54% 2%
</table>
<tableCaption confidence="0.992114">
Table 4: The distribution of query types for 500 queries.
</tableCaption>
<bodyText confidence="0.99575125">
Queries classified as Wrong (18%) mostly cor-
respond to parser mistakes. Trivial and Subjective
queries, while not attributing to the author connec-
tions that she has not made, are of questionable value
as far as fact-checking goes. Perhaps the most sur-
prising figure is the meager amount of verifiable and
incorrect queries. Examples of relevant statements
from essays include (NE and NP are boldfaced):
For example, Paul Gaugin who was a sucess-
ful business man, with a respectable wife and
family, suddenly gave in to the calling of the
arts and left his life. (He was a failing busi-
nessman immediately before leaving family.)
For example, in Jane Austins Little Women,
she portrays the image of a lovely family and
the wonders of womenhood. (The book is by
</bodyText>
<subsectionHeader confidence="0.380398">
Louisa May Alcott.)
</subsectionHeader>
<bodyText confidence="0.99829675">
This occurrence can be seen with the Rod-
ney King problem in California during the late
1980s. (The Rodney King incident occurred
on March 3, 1991).
We see the philosophers Aristotle, Plato,
Socrates and their practical writings of the
political problems and issues of the day.
(Socrates is not known to have left writings.)
First, we observe that factual mistakes are rare.
Furthermore, they seem to pertain to one in a series
of related facts, most of which are correct and testify
to the authors substantial knowledge about the mat-
ter consider Paul Gaugins biography or the con-
tents of Little Women in the examples above. It
is therefore unclear how detrimental the occasional
factual glitches are to the quality of the essay.
</bodyText>
<sectionHeader confidence="0.472179" genericHeader="method">
8 Application to Essay Scoring
</sectionHeader>
<bodyText confidence="0.989673324324324">
We show Pearson correlations between human
scores given to essays and a number of characte-
ristics derived from the work described here, as well
as the partial correlations when the effect of essay
length is factored out. We calculated both the cor-
relations using raw numbers and on a logarithmic
scale, with the latter generally producing higher cor-
realtions. Therefore, we are reporting the correla-
tions between grade and the logarithm of the rele-
vant characteristic. The characteristics are:
#NE The number of NE tokens in an essay.
#Queries The number of queries generated by the
system from the given essay (as described in
section 5.2).
#Matched Queries The number of queries for
which a match was found in the TextRunner
database. If the original query or any of its ex-
pansion variants (see section 6.1) had matches,
the query contributes a count of 1.
#Filtered Matches The number of queries that
passed the filters introduced in section 6. If the
original query or any of its expansion variants
passed the filters, the query contributes a count
of 1.
Table 5 shows the results. First, we find that all
correlations are significant at p=0.05, as well as the
partial correlations exluding the effect of length for 7
out of 10 prompts. All correlations are positive, that
is, the more factual information a writer employs in
an essay, the higher the grade beyond the oft re-
ported correlations between the grade and the length
of an essay (Powers, 2005).
Second, we notice that all characteristics from
the number of named entities to the number of fil-
tered matches produce similar correlation figures.
Third, there are large differences between average
numbers of named entities per essay across prompts.
</bodyText>
<page confidence="0.997839">
68
</page>
<table confidence="0.986568846153846">
\x0cPrompt NE Pearson Corr. with Grade Partial Corr. Removing Length
#NE #Q #Mat. # Filt. #NE #Q #Mat. # Filt.
P1 280 0.144 0.154 0.182 0.185 0.006 0.019 0.058 0.076
P2 406 0.265 0.259 0.274 0.225 0.039 0.053 0.072 0.069
P3 452 0.245 0.225 0.188 0.203 0.049 0.033 0.009 0.051
P4 658 0.327 0.302 0.335 0.327 0.165 0.159 0.177 0.160
P5 704 0.470 0.477 0.473 0.471 0.287 0.294 0.304 0.305
P6 750 0.429 0.415 0.388 0.373 0.271 0.242 0.244 0.257
P7 785 0.470 0.463 0.479 0.469 0.302 0.302 0.341 0.326
P8 838 0.423 0.390 0.406 0.363 0.264 0.228 0.266 0.225
P9 919 0.398 0.445 0.426 0.393 0.158 0.209 0.233 0.219
P10 986 0.455 0.438 0.375 0.336 0.261 0.257 0.170 0.175
AV. 678 0.363 0.357 0.353 0.335 0.180 0.180 0.187 0.186
</table>
<tableCaption confidence="0.999158">
Table 5: Pearson correlation and partial correlation removing the effect of length between a number of characteristics
</tableCaption>
<bodyText confidence="0.99266755">
(all on a log scale) and the grade. The second column shows the total number of identified named entities in the
200-essay sample from the given prompt. The prompts are sorted by the second column.
Generally, the higher the number, the better the num-
ber of named entities in the essay predicts its grade
(the more NEs the higher the grade). This suggests
that the use of named entities might be relatively
irrelevant for some prompts, and much more rele-
vant for others. For example, prompt P10 reads
The arts (painting, music, literature, etc.) reveal
the otherwise hidden ideas and impulses of a soci-
ety, thus practically inviting exemplification using
specific works of art or art movements, while suc-
cess with prompt P1 The human mind will al-
ways be superior to machines because machines are
only tools of human minds is apparently not as
dependent on named entity based exemplification.
Excluding prompts with smaller than average total
number of named entities (&lt;678), the correlations
average 0.40-0.44 across the various characteristics,
with partial correlations averaging 0.25-0.26.
</bodyText>
<sectionHeader confidence="0.498308" genericHeader="discussions">
9 Discussion and Conclusion
</sectionHeader>
<subsectionHeader confidence="0.960729">
9.1 Summary of the main result
</subsectionHeader>
<bodyText confidence="0.999292">
In this article, we proposed a way to measure the
use of factual information in text-taker essays. We
demonstrated that the use of factual information is
indicative of essay quality, observing positive corre-
lations between the count of instances of fact-use in
essays and the grade of the essay, beyond what can
be attributed to a correlation between the total num-
ber of words in an essay and the grade.
</bodyText>
<subsectionHeader confidence="0.749197">
9.2 What is driving the correlations?
</subsectionHeader>
<bodyText confidence="0.99489668">
We also investigated which of the components of
the fact-use measure were responsible for the ob-
served correlations. Specifically, we considered (a)
the number instances of fact-use that were verified
against a database of human-produced assertions,
filtered for controversy and excessive diversity; (b)
the number of instances of fact-use that were verified
against the database, without subsequent filtering;
(c) the number of instances of fact-use identified in
an essay (without checking against the database); (d)
the number of named entities used in an essay (with-
out constructing queries around the entity). These
steps correspond to a gradual relaxation of the full
fact-checking procedure all the way to a proxy mea-
sure that counts the number of named entities.
We observed similar correlations throughout the
relaxation procedure. We therefore conclude that the
number of named entities is the driving force behind
the correlations, with no observed effect of the query
construction and verification procedures.7 This re-
sult could be explained by two factors.
First, a manual check of 500 queries showed that
factual mistakes are rare only 2% of the queries
corresponded to factually incorrect statements. Fur-
thermore, mistakes were often accompanied by the
</bodyText>
<page confidence="0.99391">
7
</page>
<bodyText confidence="0.973973666666667">
While the trend is in the direction of an increase in Pearson
correlations from (a) to (d), the differences are not statistically
significant.
</bodyText>
<page confidence="0.994734">
69
</page>
<bodyText confidence="0.997517272727273">
\x0ctest-takers use of additional facts about the same en-
tity which were correct; this might alleviate the im-
pact of a mistake in the eyes of a grader.
Second, the query verification procedure applied
to only about 35% of the queries those for which
at least one match was found in the database, that
is, 65% of the queries could not be assessed using
the database of 2 bln extractions. The verification
procedure is thus much less robust than the proce-
dure for detecting named entities, which performs at
above &gt;80% recall and precision.
</bodyText>
<subsectionHeader confidence="0.973217">
9.3 Implications for automated scoring
</subsectionHeader>
<bodyText confidence="0.99969109375">
Our results suggest that essays on a general topic
written by adults for a high-stakes exam contain
few incorrect facts, so the potential for a full fact-
checking system to improve correlations with grades
beyond merely detecting the potential for a factual
statement using a named entity recognizer is not
large. While a measure based on the number of
verified facts found in an essay demonstrated a
significant correlation with human scores beyond
the contribution of essay length, a simpler measure
based only on the number of named entities in the
essay demonstrated a similar relationship with hu-
man scores.
Given the similarity in the two features empiri-
cal usefulness, it would seem that the feature that
counts the number of named entities in an essay is a
better candidate, due to its simplicity and robustness.
However, there is another perspective from which a
feature based only on the number of named entities
in an essay may be less suitable for use in scoring:
the perspective of construct validity, the degree to
which a test (or, in this case, a scoring system) ac-
tually measures what it purports to. As mentioned
above, the number of named entities in an essay is,
at best, a proxy measure,8 roughly indicative of the
referencing of factual statements in support of an ar-
gument within an essay. Because the measure itself
is not directly sensitive to how named entities are
used in the essay, though, even entities with no con-
nection to the essay topic would tend to contribute
to the score, and the measure is therefore vulnerable
to manipulation by test-takers.
</bodyText>
<page confidence="0.97175">
8
</page>
<bodyText confidence="0.999499414634146">
For a discussion of proxes vs trins in essay grading, see
(Page and Petersen, 1995).
An obvious strategy to exploit this scoring mecha-
nism would be to simply include more named enti-
ties in an essay, either interspersing them randomly
throughout the text, or including them in long lists of
examples to illustrate a single point. Such a blatant
approach could potentially be detected by the use of
a filter or advisory (Higgins et al., 2006; Landauer
et al., 2003) designed to identify anomalous writing
strategies. However, there could be more subtle ap-
proaches to exploiting such a feature. For example,
it is possible that test-takers might be inclined to in-
crease their use of named entities by adducing more
facts in support of an argument, and would go be-
yond the comfort zone of their actual factual know-
ledge, thus making more factual mistakes. Test gam-
ing strategies have been recognized as a threat to au-
tomated scoring systems for some time (Powers et
al., 2001), and there is evidence based on test tak-
ers own self-reported behavior that this threat is real
(Powers, 2011). This is one major reason why large-
scale operational testing programs (such as GRE or
TOEFL) use automated essay scoring only in com-
bination with human ratings. In sum, the degree to
which a linguistic feature is predictive of human es-
say scores is not the only criterion for evaluation; the
washback effects of using the feature (on writing be-
havior and on instruction) must also be considered.
The second finding of this study is that the ef-
fectiveness of fact-checking for essay assessment is
compromised by the limited coverage of the wealth
of factual statements made by essay writers, with
only 35% of queries garnering any hits at all in a
large general-purpose database of assertions. It is
possible, however, that OpenIE technology can be
used to collect more focused repositories on specific
topics, such as the history of the American Civil
War, which could be used to assess responses to
tasks related to that particular subject matter. This
is one of the directions of our future research.
</bodyText>
<sectionHeader confidence="0.966172" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.7920682">
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the 5th ACM conference on Digital
Libraries, pages 8594. ACM.
Yigal Attali and Jill Burstein. 2006. Automated Es-
</reference>
<page confidence="0.953167">
70
</page>
<reference confidence="0.99923538317757">
\x0csay Scoring With e-rater R
V.2. Journal of Technology,
Learning, and Assessment, 4(3).
Michele Banko and Oren Etzioni. 2008. The Tradeoffs
Between Open and Traditional Relation Extraction. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 2836,
Columbus, OH, June. Association for Computational
Linguistics.
Beata Beigman Klebanov, Jill Burstein, Nitin Madnani,
Adam Faulkner, and Joel Tetreault. 2012. Building
Subjectivity Lexicon(s) From Scratch For Essay Data.
In Proceedings of CICLING, New Delhi, India.
Tao-Hsing Chang, Chia-Hoang Lee, and Yu-Ming
Chang. 2006. Enhancing Automatic Chinese Es-
say Scoring System from Figures-of-Speech. In Pro-
ceedings of the 20th Pacific Asia Conference on Lan-
guage, Information and Computation, pages 2834.
Yen-Yu Chen, Chien-Liang Liu, Chia-Hoang Lee, and
Tao-Hsing Chang. 2010. An Unsupervised Auto-
mated Essay Scoring System. IEEE Transactions on
Intelligent Systems, 25(5):6167.
Nancy Chinchor, Lynette Hirschman, and David Lewis.
1993. Evaluating Message Understanding Systems:
An Analysis of the Third Message Understanding
Conference (MUC-3). Computational Linguistics,
19(3):409449.
Dmitry Davidov and Ari Rappoport. 2009. Geo-mining:
Discovery of Road and Transport Networks Using Di-
rectional Patterns. In Proceedings of EMNLP, pages
267275.
Arijit De and Sunil Kopparapu. 2011. An unsupervised
approach to automated selection of good essays. In
Recent Advances in Intelligent Computational Systems
(RAICS), 2011 IEEE, pages 662 666.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating Typed De-
pendency Parses from Phrase Structure Parses. In Pro-
ceedings of LREC, pages 449454, Genoa, Italy, May.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel Weld. 2008. Open information extraction from
the web. Commun. ACM, 51(12):6874.
Jenny Finkel, Trond Grenager, and Christopher Manning.
2005. Incorporating Non-local Information into In-
formation Extraction Systems by Gibbs Sampling. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 363370,
Ann Arbor, MI, June. Association for Computational
Linguistics.
Ralph Grishman and Beth Sundheim. 1995. Design of
the MUC-6 evaluation. In Proceedings of MUC, pages
111.
Derrick Higgins, Jill Burstein, and Yigal Attali. 2006.
Identifying off-topic student essays without topic-
specific training data. Natural Language Engineering,
12(2):145159.
Tsunenori Ishioka and Masayuki Kameda. 2006. Auto-
mated Japanese Essay Scoring System based on Arti-
cles Written by Experts. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 233240, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Tuomo Kakkonen and Erkki Sutinen. 2004. Automatic
assessment of the content of essays based on course
materials. In Proceedings of the International Confer-
ence on Information Technology: Research and Edu-
cation, pages 126130, London, UK.
Tuomo Kakkonen, Niko Myller, Jari Timonen, and Erkki
Sutinen. 2005. Automatic Essay Grading with Prob-
abilistic Latent Semantic Analysis. In Proceedings of
the Second Workshop on Building Educational Appli-
cations Using NLP, pages 2936, Ann Arbor, Michi-
gan, June. Association for Computational Linguistics.
Dan Klein and Christopher Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 423430, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
Saul Kripke. 1980. Naming and Necessity. Harvard Uni-
versity Press.
Thomas Landauer, Darrell Laham, and Peter Foltz. 2003.
Automated scoring and annotation of essays with the
Intelligent Essay Assessor. In Mark Shermis and Jill
Burstein, editors, Automated essay scoring: A cross-
disciplinary perspective, pages 87112. Lawrence Erl-
baum Associates, Mahwah, New Jersey.
Leah Larkey. 1998. Automatic essay grading using text
categorization techniques. In Proceedings of SIGIR,
pages 9095, Melbourne, AU.
Benot Lemaire and Philippe Dessus. 2001. A System to
Assess the Semantic Content of Student Essays. Jour-
nal of Educational Computing Research, 24:305320.
Annie Louis and Derrick Higgins. 2010. Off-topic essay
detection using short prompt texts. In Proceedings of
the NAACL HLT 2010 Fifth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 9295, Los Angeles, California, June. Associ-
ation for Computational Linguistics.
Boyan Onyshkevych. 1993. Template design for infor-
mation extraction. In Proceedings of MUC, pages 19
23.
Ellis Page and Nancy Petersen. 1995. The computer
moves into essay grading: Updating the ancient test.
Phi Delta Kappan, 76:561565.
Cham Perelman and Lucie Olbrechts-Tyteca. 1969. The
New Rhetoric: A Treatise on Argumentation. Notre
</reference>
<page confidence="0.972296">
71
</page>
<reference confidence="0.99377324137931">
\x0cDame, Indiana: University of Notre Dame Press.
Translated by John Wilkinson and Purcell Weaver
from French original published in 1958.
Donald Powers, Jill Burstein, Martin Chodorow, Mary
Fowles, and Karen Kukich. 2001. Stumping
E-Rater: Challenging the Validity of Automated
Essay Scoring. ETS research report RR-01-03,
http://www.ets.org/research/policy research reports/rr-
01-03.
Donald Powers. 2005. Wordiness: A selective review
of its influence, and suggestions for investigating
its relevance in tests requiring extended written
responses. ETS research memorandum RM-04-08,
http://www.ets.org/research/policy research reports/rm-
04-08.
Donald Powers. 2011. Scoring the TOEFL
Independent Essay Automatically: Re-
actions of Test Takers and Test Score
Users. ETS research manuscript RM-11-34,
http://www.ets.org/research/policy research reports/rm-
11-34.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a Question Answering System.
In Proceedings of ACL, pages 4147.
Carolyn Rose, Antonio Roqueand, Dumisizwe Bhembe,
and Kurt VanLehn. 2003. A hybrid text classifica-
tion approach for analysis of student essays. In Pro-
ceedings of the Second Workshop on Building Educa-
tional Applications Using NLP, pages 2936.
</reference>
<page confidence="0.938109">
72
</page>
<figure confidence="0.279579">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.745525">
<note confidence="0.941577">b&amp;apos;The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 6372, Montreal, Canada, June 3-8, 2012. c 2012 Association for Computational Linguistics</note>
<title confidence="0.991116">Measuring the Use of Factual Information in Test-Taker Essays</title>
<author confidence="0.988491">Beata Beigman Klebanov</author>
<affiliation confidence="0.934874">Educational Testing Service</affiliation>
<address confidence="0.999679">660 Rosedale Road Princeton, NJ 08541, USA</address>
<email confidence="0.995493">bbeigmanklebanov@ets.org</email>
<author confidence="0.998497">Derrick Higgins</author>
<affiliation confidence="0.944469">Educational Testing Service</affiliation>
<address confidence="0.9997365">660 Rosedale Road Princeton, NJ 08541, USA</address>
<email confidence="0.997328">dhiggins@ets.org</email>
<abstract confidence="0.999407">We describe a study aimed at measuring the use of factual information in test-taker essays and assessing its effectiveness for predicting essay scores. We found medium correlations with the proposed measures, that remained significant after the effect of essay length was factored out. The correlations did not differ substantionally between a simple, relatively robust measure vs a more sophisticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the 5th ACM conference on Digital Libraries,</booktitle>
<pages>8594</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5404" citStr="Agichtein and Gravano, 2000" startWordPosition="867" endWordPosition="870">universal audience. Recent developments in Open Information Extraction make it possible to tap into this vast knowledge resource. Indeed, fact-checking is one of the applications the developers of OpenIE have in mind for their emergent technology (Etzioni et al., 2008). 3 Open Information Extraction Traditionally, the goal of an information extraction system is automated population of structured databases of events or concepts of interest and their properties by analyzing large corpora of text (Chinchor et al., 1993; Onyshkevych, 1993; Grishman and Sundheim, 1995; Ravichandran and Hovy, 2002; Agichtein and Gravano, 2000; Davidov and Rappoport, 2009). 1 For example, Barack Obama picks out precisely one person, and the same one in 2010 as it did in 1990. In contrast, the current US president picks out different people every 4-8 years. For indeteminacy of boundaries, consider a statement like US officials are wealthy. To determine its truth, one must first secure agreement on acceptable referents of US officials. In contrast, the recently proposed Open Information Extraction paradigm aims to detect related pairs of entities without knowing in advance what kinds of relations exist between entities in the source </context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of the 5th ACM conference on Digital Libraries, pages 8594. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yigal Attali</author>
<author>Jill Burstein</author>
</authors>
<date>2006</date>
<booktitle>Automated Es\x0csay Scoring With e-rater R V.2. Journal of Technology, Learning, and Assessment,</booktitle>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="1177" citStr="Attali and Burstein, 2006" startWordPosition="165" endWordPosition="168">st-taker essays and assessing its effectiveness for predicting essay scores. We found medium correlations with the proposed measures, that remained significant after the effect of essay length was factored out. The correlations did not differ substantionally between a simple, relatively robust measure vs a more sophisticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Rose et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an e</context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Yigal Attali and Jill Burstein. 2006. Automated Es\x0csay Scoring With e-rater R V.2. Journal of Technology, Learning, and Assessment, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
</authors>
<title>The Tradeoffs Between Open and Traditional Relation Extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>2836</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, OH,</location>
<contexts>
<context position="6058" citStr="Banko and Etzioni, 2008" startWordPosition="975" endWordPosition="978">. 1 For example, Barack Obama picks out precisely one person, and the same one in 2010 as it did in 1990. In contrast, the current US president picks out different people every 4-8 years. For indeteminacy of boundaries, consider a statement like US officials are wealthy. To determine its truth, one must first secure agreement on acceptable referents of US officials. In contrast, the recently proposed Open Information Extraction paradigm aims to detect related pairs of entities without knowing in advance what kinds of relations exist between entities in the source data and without any seeding (Banko and Etzioni, 2008). The possibility of such extraction in English is attributed by the authors to a small number of syntactic patterns that realize binary relations between entities. In particular, they found that almost 40% of such relations are realized by the argument-verb-argument pattern (henceforth, AVA) (see Table 1 in Banko and Etzioni (2008)). The TextRunner system (Banko and Etzioni, 2008) is trained using a CRF classifier on S-V-O tuples from a parsed corpus as positive examples, and tuples that violate phrasal structure as negative ones. The examples are described using features that do not require </context>
</contexts>
<marker>Banko, Etzioni, 2008</marker>
<rawString>Michele Banko and Oren Etzioni. 2008. The Tradeoffs Between Open and Traditional Relation Extraction. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, pages 2836, Columbus, OH, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Jill Burstein</author>
<author>Nitin Madnani</author>
<author>Adam Faulkner</author>
<author>Joel Tetreault</author>
</authors>
<title>Building Subjectivity Lexicon(s) From Scratch For Essay Data.</title>
<date>2012</date>
<booktitle>In Proceedings of CICLING,</booktitle>
<location>New Delhi, India.</location>
<contexts>
<context position="1708" citStr="Klebanov et al., 2012" startWordPosition="248" endWordPosition="251">grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Rose et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the readers knowledge clearly detract from an essays quality. This paper presents a study on assessing the use of factual knowledge in argumentative essays on general topics written for a graduate school entrance exam. We propose a definition of fact, and an operationalization thereof. We find that the proposed measure has positive medium-strength correlation with essay grade, whi</context>
</contexts>
<marker>Klebanov, Burstein, Madnani, Faulkner, Tetreault, 2012</marker>
<rawString>Beata Beigman Klebanov, Jill Burstein, Nitin Madnani, Adam Faulkner, and Joel Tetreault. 2012. Building Subjectivity Lexicon(s) From Scratch For Essay Data. In Proceedings of CICLING, New Delhi, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao-Hsing Chang</author>
<author>Chia-Hoang Lee</author>
<author>Yu-Ming Chang</author>
</authors>
<title>Enhancing Automatic Chinese Essay Scoring System from Figures-of-Speech.</title>
<date>2006</date>
<booktitle>In Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation,</booktitle>
<pages>2834</pages>
<contexts>
<context position="1729" citStr="Chang et al., 2006" startWordPosition="252" endWordPosition="255">cs, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Rose et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the readers knowledge clearly detract from an essays quality. This paper presents a study on assessing the use of factual knowledge in argumentative essays on general topics written for a graduate school entrance exam. We propose a definition of fact, and an operationalization thereof. We find that the proposed measure has positive medium-strength correlation with essay grade, which remains significan</context>
</contexts>
<marker>Chang, Lee, Chang, 2006</marker>
<rawString>Tao-Hsing Chang, Chia-Hoang Lee, and Yu-Ming Chang. 2006. Enhancing Automatic Chinese Essay Scoring System from Figures-of-Speech. In Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation, pages 2834.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yen-Yu Chen</author>
<author>Chien-Liang Liu</author>
<author>Chia-Hoang Lee</author>
<author>Tao-Hsing Chang</author>
</authors>
<title>An Unsupervised Automated Essay Scoring System.</title>
<date>2010</date>
<journal>IEEE Transactions on Intelligent Systems,</journal>
<volume>25</volume>
<issue>5</issue>
<contexts>
<context position="1371" citStr="Chen et al., 2010" startWordPosition="196" endWordPosition="199">ed out. The correlations did not differ substantionally between a simple, relatively robust measure vs a more sophisticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Rose et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the readers knowledge clearly detract from an </context>
</contexts>
<marker>Chen, Liu, Lee, Chang, 2010</marker>
<rawString>Yen-Yu Chen, Chien-Liang Liu, Chia-Hoang Lee, and Tao-Hsing Chang. 2010. An Unsupervised Automated Essay Scoring System. IEEE Transactions on Intelligent Systems, 25(5):6167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
<author>Lynette Hirschman</author>
<author>David Lewis</author>
</authors>
<title>Evaluating Message Understanding Systems:</title>
<date>1993</date>
<booktitle>An Analysis of the Third Message Understanding Conference (MUC-3). Computational Linguistics,</booktitle>
<contexts>
<context position="5298" citStr="Chinchor et al., 1993" startWordPosition="852" endWordPosition="856">t also those made by people on their blogs is perhaps as close as it gets to a working model of the universal audience. Recent developments in Open Information Extraction make it possible to tap into this vast knowledge resource. Indeed, fact-checking is one of the applications the developers of OpenIE have in mind for their emergent technology (Etzioni et al., 2008). 3 Open Information Extraction Traditionally, the goal of an information extraction system is automated population of structured databases of events or concepts of interest and their properties by analyzing large corpora of text (Chinchor et al., 1993; Onyshkevych, 1993; Grishman and Sundheim, 1995; Ravichandran and Hovy, 2002; Agichtein and Gravano, 2000; Davidov and Rappoport, 2009). 1 For example, Barack Obama picks out precisely one person, and the same one in 2010 as it did in 1990. In contrast, the current US president picks out different people every 4-8 years. For indeteminacy of boundaries, consider a statement like US officials are wealthy. To determine its truth, one must first secure agreement on acceptable referents of US officials. In contrast, the recently proposed Open Information Extraction paradigm aims to detect related </context>
</contexts>
<marker>Chinchor, Hirschman, Lewis, 1993</marker>
<rawString>Nancy Chinchor, Lynette Hirschman, and David Lewis. 1993. Evaluating Message Understanding Systems: An Analysis of the Third Message Understanding Conference (MUC-3). Computational Linguistics, 19(3):409449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Geo-mining: Discovery of Road and Transport Networks Using Directional Patterns.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>267275</pages>
<contexts>
<context position="5434" citStr="Davidov and Rappoport, 2009" startWordPosition="871" endWordPosition="875">velopments in Open Information Extraction make it possible to tap into this vast knowledge resource. Indeed, fact-checking is one of the applications the developers of OpenIE have in mind for their emergent technology (Etzioni et al., 2008). 3 Open Information Extraction Traditionally, the goal of an information extraction system is automated population of structured databases of events or concepts of interest and their properties by analyzing large corpora of text (Chinchor et al., 1993; Onyshkevych, 1993; Grishman and Sundheim, 1995; Ravichandran and Hovy, 2002; Agichtein and Gravano, 2000; Davidov and Rappoport, 2009). 1 For example, Barack Obama picks out precisely one person, and the same one in 2010 as it did in 1990. In contrast, the current US president picks out different people every 4-8 years. For indeteminacy of boundaries, consider a statement like US officials are wealthy. To determine its truth, one must first secure agreement on acceptable referents of US officials. In contrast, the recently proposed Open Information Extraction paradigm aims to detect related pairs of entities without knowing in advance what kinds of relations exist between entities in the source data and without any seeding (</context>
</contexts>
<marker>Davidov, Rappoport, 2009</marker>
<rawString>Dmitry Davidov and Ari Rappoport. 2009. Geo-mining: Discovery of Road and Transport Networks Using Directional Patterns. In Proceedings of EMNLP, pages 267275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arijit De and Sunil Kopparapu</author>
</authors>
<title>An unsupervised approach to automated selection of good essays.</title>
<date>2011</date>
<booktitle>In Recent Advances in Intelligent Computational Systems (RAICS), 2011 IEEE,</booktitle>
<pages>662--666</pages>
<contexts>
<context position="1395" citStr="Kopparapu, 2011" startWordPosition="202" endWordPosition="203">id not differ substantionally between a simple, relatively robust measure vs a more sophisticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Rose et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the readers knowledge clearly detract from an essays quality. This pap</context>
</contexts>
<marker>Kopparapu, 2011</marker>
<rawString>Arijit De and Sunil Kopparapu. 2011. An unsupervised approach to automated selection of good essays. In Recent Advances in Intelligent Computational Systems (RAICS), 2011 IEEE, pages 662 666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>449454</pages>
<location>Genoa, Italy,</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proceedings of LREC, pages 449454, Genoa, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Stephen Soderland</author>
<author>Daniel Weld</author>
</authors>
<date>2008</date>
<booktitle>Open information extraction from the web. Commun. ACM,</booktitle>
<pages>51--12</pages>
<contexts>
<context position="5046" citStr="Etzioni et al., 2008" startWordPosition="814" endWordPosition="817">y of the resource. However, many facts known to various groups of people that could be known to all are absent from any encyclopedia. The knowledge contained in the WWW at large, reaching not only statements explicitly contributed to an encyclopedia but also those made by people on their blogs is perhaps as close as it gets to a working model of the universal audience. Recent developments in Open Information Extraction make it possible to tap into this vast knowledge resource. Indeed, fact-checking is one of the applications the developers of OpenIE have in mind for their emergent technology (Etzioni et al., 2008). 3 Open Information Extraction Traditionally, the goal of an information extraction system is automated population of structured databases of events or concepts of interest and their properties by analyzing large corpora of text (Chinchor et al., 1993; Onyshkevych, 1993; Grishman and Sundheim, 1995; Ravichandran and Hovy, 2002; Agichtein and Gravano, 2000; Davidov and Rappoport, 2009). 1 For example, Barack Obama picks out precisely one person, and the same one in 2010 as it did in 1990. In contrast, the current US president picks out different people every 4-8 years. For indeteminacy of boun</context>
</contexts>
<marker>Etzioni, Banko, Soderland, Weld, 2008</marker>
<rawString>Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel Weld. 2008. Open information extraction from the web. Commun. ACM, 51(12):6874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>363370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="8203" citStr="Finkel et al., 2005" startWordPosition="1325" endWordPosition="1328">e 1: The distribution of grades for 2,000 essays. 2 http://www.cs.washington.edu/research/knowitall/hltnaacl08-data.txt 64 \x0c5 Building Queries from Essays We define a query as a 3-tuple &lt;NE,?,NP&gt;,3 where NE is a named entity and NP is a noun phrase from the same or neighboring sentence in a test-taker essay (the selection process is described in section 5.2). We use the pattern of predicate matches against the TextRunner database to assess the degree and the equivocality of the connection between NE and NP. 5.1 Named Entities in Test-Taker Essay We use the Stanford Named Entity Recognizer (Finkel et al., 2005) that tags named entities as people, locations, organizations, and miscellaneous. We annotated a sample of 90 essays for named entities; the sample yielded 442 tokens, which we classified as shown in Table 2. The Enamex classes (people, locations, organizations) account for 58% of all the entities in the sample. The recognizers recall of people and locations is excellent (though they are not always classified correctly see caption of Table 2), although test-taker essays feature additional entity types that are not detected as well. Category Recall Examples Location 0.98 Iraq, USA Person 0.96 G</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 363370, Ann Arbor, MI, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Beth Sundheim</author>
</authors>
<title>Design of the MUC-6 evaluation.</title>
<date>1995</date>
<booktitle>In Proceedings of MUC,</booktitle>
<pages>111</pages>
<contexts>
<context position="5346" citStr="Grishman and Sundheim, 1995" startWordPosition="859" endWordPosition="862"> is perhaps as close as it gets to a working model of the universal audience. Recent developments in Open Information Extraction make it possible to tap into this vast knowledge resource. Indeed, fact-checking is one of the applications the developers of OpenIE have in mind for their emergent technology (Etzioni et al., 2008). 3 Open Information Extraction Traditionally, the goal of an information extraction system is automated population of structured databases of events or concepts of interest and their properties by analyzing large corpora of text (Chinchor et al., 1993; Onyshkevych, 1993; Grishman and Sundheim, 1995; Ravichandran and Hovy, 2002; Agichtein and Gravano, 2000; Davidov and Rappoport, 2009). 1 For example, Barack Obama picks out precisely one person, and the same one in 2010 as it did in 1990. In contrast, the current US president picks out different people every 4-8 years. For indeteminacy of boundaries, consider a statement like US officials are wealthy. To determine its truth, one must first secure agreement on acceptable referents of US officials. In contrast, the recently proposed Open Information Extraction paradigm aims to detect related pairs of entities without knowing in advance wha</context>
</contexts>
<marker>Grishman, Sundheim, 1995</marker>
<rawString>Ralph Grishman and Beth Sundheim. 1995. Design of the MUC-6 evaluation. In Proceedings of MUC, pages 111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derrick Higgins</author>
<author>Jill Burstein</author>
<author>Yigal Attali</author>
</authors>
<title>Identifying off-topic student essays without topicspecific training data.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="1417" citStr="Higgins et al., 2006" startWordPosition="204" endWordPosition="207">stantionally between a simple, relatively robust measure vs a more sophisticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Rose et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the readers knowledge clearly detract from an essays quality. This paper presents a study on</context>
<context position="29373" citStr="Higgins et al., 2006" startWordPosition="4907" endWordPosition="4910"> essay, though, even entities with no connection to the essay topic would tend to contribute to the score, and the measure is therefore vulnerable to manipulation by test-takers. 8 For a discussion of proxes vs trins in essay grading, see (Page and Petersen, 1995). An obvious strategy to exploit this scoring mechanism would be to simply include more named entities in an essay, either interspersing them randomly throughout the text, or including them in long lists of examples to illustrate a single point. Such a blatant approach could potentially be detected by the use of a filter or advisory (Higgins et al., 2006; Landauer et al., 2003) designed to identify anomalous writing strategies. However, there could be more subtle approaches to exploiting such a feature. For example, it is possible that test-takers might be inclined to increase their use of named entities by adducing more facts in support of an argument, and would go beyond the comfort zone of their actual factual knowledge, thus making more factual mistakes. Test gaming strategies have been recognized as a threat to automated scoring systems for some time (Powers et al., 2001), and there is evidence based on test takers own self-reported beha</context>
</contexts>
<marker>Higgins, Burstein, Attali, 2006</marker>
<rawString>Derrick Higgins, Jill Burstein, and Yigal Attali. 2006. Identifying off-topic student essays without topicspecific training data. Natural Language Engineering, 12(2):145159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsunenori Ishioka</author>
<author>Masayuki Kameda</author>
</authors>
<title>Automated Japanese Essay Scoring System based on Articles Written by Experts.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>233240</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1443" citStr="Ishioka and Kameda, 2006" startWordPosition="208" endWordPosition="211"> simple, relatively robust measure vs a more sophisticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Rose et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the readers knowledge clearly detract from an essays quality. This paper presents a study on assessing the use of fact</context>
</contexts>
<marker>Ishioka, Kameda, 2006</marker>
<rawString>Tsunenori Ishioka and Masayuki Kameda. 2006. Automated Japanese Essay Scoring System based on Articles Written by Experts. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 233240, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tuomo Kakkonen</author>
<author>Erkki Sutinen</author>
</authors>
<title>Automatic assessment of the content of essays based on course materials.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Information Technology: Research and Education,</booktitle>
<pages>126130</pages>
<location>London, UK.</location>
<contexts>
<context position="1494" citStr="Kakkonen and Sutinen, 2004" startWordPosition="216" endWordPosition="219">isticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Rose et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the readers knowledge clearly detract from an essays quality. This paper presents a study on assessing the use of factual knowledge in argumentative essays on general to</context>
</contexts>
<marker>Kakkonen, Sutinen, 2004</marker>
<rawString>Tuomo Kakkonen and Erkki Sutinen. 2004. Automatic assessment of the content of essays based on course materials. In Proceedings of the International Conference on Information Technology: Research and Education, pages 126130, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tuomo Kakkonen</author>
<author>Niko Myller</author>
<author>Jari Timonen</author>
<author>Erkki Sutinen</author>
</authors>
<title>Automatic Essay Grading with Probabilistic Latent Semantic Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>2936</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1466" citStr="Kakkonen et al., 2005" startWordPosition="212" endWordPosition="215"> measure vs a more sophisticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Rose et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the readers knowledge clearly detract from an essays quality. This paper presents a study on assessing the use of factual knowledge in argume</context>
</contexts>
<marker>Kakkonen, Myller, Timonen, Sutinen, 2005</marker>
<rawString>Tuomo Kakkonen, Niko Myller, Jari Timonen, and Erkki Sutinen. 2005. Automatic Essay Grading with Probabilistic Latent Semantic Analysis. In Proceedings of the Second Workshop on Building Educational Applications Using NLP, pages 2936, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="10393" citStr="Klein and Manning, 2003" startWordPosition="1694" endWordPosition="1697">nerated by the tagger are in the grey area while we havent marked them, they are not clearly mistakes. A common case are names of national and religious groups, such as Muslim or Turkish, or capitalizations of otherwise common nouns for emphasis and elevation, such as Arts or Masters. Given our objective to ground the queries in items with specific referents, these are less suitable. If all such cases are counted as mistakes, the taggers precision is 82%. 5.2 Selection of NPs We employ a grammar-based approach for selecting NPs. We use the Stanford dependency parser (de Marneffe et al., 2006; Klein and Manning, 2003) to determine dependency relations. In order to find out which dependency paths connect between named entities and clearly related NPs in essays, we manually marked concepts related to 95 NEs in 10 randomly sampled essays. We marked 210 query-able concepts in total. The resulting 210 dependency paths were classified according to the direction of the movement. Out of the 210 paths, 51 (24%) contain a single upward or downard step, that is, are cases where the NE is the head of the constituent in which the NP is embedded, or the other way around. Some examples are shown in Figure 1. Note that th</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423430, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saul Kripke</author>
</authors>
<title>Naming and Necessity.</title>
<date>1980</date>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="3868" citStr="Kripke, 1980" startWordPosition="612" endWordPosition="613">d OlbrechtsTyteca, 1969, 67). Factuality is thus a matter of selecting certain kinds of data and securing a certain type of agreement over those data. Of the different statements that refer to objective reality, the term facts is used to designate ob63 \x0cjects of precise, limited agreement (Perelman and Olbrechts-Tyteca, 1969, 69). These are contrasted with presumptions statements connected to what is normal and likely (ibid.). We suggest that the distinctions in the scope of the required agreement can be related to the referential device used in a statement: If the reference is more rigid (Kripke, 1980), that is, less prone to change in time and to indeterminacy of the boundaries, the scope of the necessary agreement is likely to be more precise and limited. With proper names prototypically being the most rigid designators, we will focus our efforts on statements about named entities.1 Perhaps the simplest model of the universal audience is an encyclopedia a body of knowledge that is verified by experts, and is, therefore, common to several thinking beings, and could be common to all by virtue of the authority of the experts and the wide availability of the resource. However, many facts know</context>
</contexts>
<marker>Kripke, 1980</marker>
<rawString>Saul Kripke. 1980. Naming and Necessity. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Darrell Laham</author>
<author>Peter Foltz</author>
</authors>
<title>Automated scoring and annotation of essays with the Intelligent Essay Assessor.</title>
<date>2003</date>
<booktitle>Automated essay scoring: A crossdisciplinary perspective,</booktitle>
<pages>87112</pages>
<editor>In Mark Shermis and Jill Burstein, editors,</editor>
<location>Mahwah, New Jersey.</location>
<contexts>
<context position="1327" citStr="Landauer et al., 2003" startWordPosition="187" endWordPosition="191">cant after the effect of essay length was factored out. The correlations did not differ substantionally between a simple, relatively robust measure vs a more sophisticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Rose et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of th</context>
<context position="29397" citStr="Landauer et al., 2003" startWordPosition="4911" endWordPosition="4914">ntities with no connection to the essay topic would tend to contribute to the score, and the measure is therefore vulnerable to manipulation by test-takers. 8 For a discussion of proxes vs trins in essay grading, see (Page and Petersen, 1995). An obvious strategy to exploit this scoring mechanism would be to simply include more named entities in an essay, either interspersing them randomly throughout the text, or including them in long lists of examples to illustrate a single point. Such a blatant approach could potentially be detected by the use of a filter or advisory (Higgins et al., 2006; Landauer et al., 2003) designed to identify anomalous writing strategies. However, there could be more subtle approaches to exploiting such a feature. For example, it is possible that test-takers might be inclined to increase their use of named entities by adducing more facts in support of an argument, and would go beyond the comfort zone of their actual factual knowledge, thus making more factual mistakes. Test gaming strategies have been recognized as a threat to automated scoring systems for some time (Powers et al., 2001), and there is evidence based on test takers own self-reported behavior that this threat is</context>
</contexts>
<marker>Landauer, Laham, Foltz, 2003</marker>
<rawString>Thomas Landauer, Darrell Laham, and Peter Foltz. 2003. Automated scoring and annotation of essays with the Intelligent Essay Assessor. In Mark Shermis and Jill Burstein, editors, Automated essay scoring: A crossdisciplinary perspective, pages 87112. Lawrence Erlbaum Associates, Mahwah, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leah Larkey</author>
</authors>
<title>Automatic essay grading using text categorization techniques.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>9095</pages>
<location>Melbourne, AU.</location>
<contexts>
<context position="1554" citStr="Larkey, 1998" startWordPosition="228" endWordPosition="229">pment of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Rose et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the readers knowledge clearly detract from an essays quality. This paper presents a study on assessing the use of factual knowledge in argumentative essays on general topics written for a graduate school entrance exam. We propose</context>
</contexts>
<marker>Larkey, 1998</marker>
<rawString>Leah Larkey. 1998. Automatic essay grading using text categorization techniques. In Proceedings of SIGIR, pages 9095, Melbourne, AU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benot Lemaire</author>
<author>Philippe Dessus</author>
</authors>
<title>A System to Assess the Semantic Content of Student Essays.</title>
<date>2001</date>
<journal>Journal of Educational Computing Research,</journal>
<pages>24--305320</pages>
<contexts>
<context position="1520" citStr="Lemaire and Dessus, 2001" startWordPosition="220" endWordPosition="223">r construct validity. Implications for development of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Rose et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the readers knowledge clearly detract from an essays quality. This paper presents a study on assessing the use of factual knowledge in argumentative essays on general topics written for a graduat</context>
</contexts>
<marker>Lemaire, Dessus, 2001</marker>
<rawString>Benot Lemaire and Philippe Dessus. 2001. A System to Assess the Semantic Content of Student Essays. Journal of Educational Computing Research, 24:305320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Derrick Higgins</author>
</authors>
<title>Off-topic essay detection using short prompt texts.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>9295</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="1352" citStr="Louis and Higgins, 2010" startWordPosition="192" endWordPosition="195">f essay length was factored out. The correlations did not differ substantionally between a simple, relatively robust measure vs a more sophisticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Rose et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the readers knowledge clear</context>
</contexts>
<marker>Louis, Higgins, 2010</marker>
<rawString>Annie Louis and Derrick Higgins. 2010. Off-topic essay detection using short prompt texts. In Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 9295, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boyan Onyshkevych</author>
</authors>
<title>Template design for information extraction.</title>
<date>1993</date>
<booktitle>In Proceedings of MUC,</booktitle>
<pages>19--23</pages>
<contexts>
<context position="5317" citStr="Onyshkevych, 1993" startWordPosition="857" endWordPosition="858">ople on their blogs is perhaps as close as it gets to a working model of the universal audience. Recent developments in Open Information Extraction make it possible to tap into this vast knowledge resource. Indeed, fact-checking is one of the applications the developers of OpenIE have in mind for their emergent technology (Etzioni et al., 2008). 3 Open Information Extraction Traditionally, the goal of an information extraction system is automated population of structured databases of events or concepts of interest and their properties by analyzing large corpora of text (Chinchor et al., 1993; Onyshkevych, 1993; Grishman and Sundheim, 1995; Ravichandran and Hovy, 2002; Agichtein and Gravano, 2000; Davidov and Rappoport, 2009). 1 For example, Barack Obama picks out precisely one person, and the same one in 2010 as it did in 1990. In contrast, the current US president picks out different people every 4-8 years. For indeteminacy of boundaries, consider a statement like US officials are wealthy. To determine its truth, one must first secure agreement on acceptable referents of US officials. In contrast, the recently proposed Open Information Extraction paradigm aims to detect related pairs of entities w</context>
</contexts>
<marker>Onyshkevych, 1993</marker>
<rawString>Boyan Onyshkevych. 1993. Template design for information extraction. In Proceedings of MUC, pages 19 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellis Page</author>
<author>Nancy Petersen</author>
</authors>
<title>The computer moves into essay grading: Updating the ancient test. Phi Delta Kappan,</title>
<date>1995</date>
<pages>76--561565</pages>
<contexts>
<context position="29017" citStr="Page and Petersen, 1995" startWordPosition="4846" endWordPosition="4849">or, in this case, a scoring system) actually measures what it purports to. As mentioned above, the number of named entities in an essay is, at best, a proxy measure,8 roughly indicative of the referencing of factual statements in support of an argument within an essay. Because the measure itself is not directly sensitive to how named entities are used in the essay, though, even entities with no connection to the essay topic would tend to contribute to the score, and the measure is therefore vulnerable to manipulation by test-takers. 8 For a discussion of proxes vs trins in essay grading, see (Page and Petersen, 1995). An obvious strategy to exploit this scoring mechanism would be to simply include more named entities in an essay, either interspersing them randomly throughout the text, or including them in long lists of examples to illustrate a single point. Such a blatant approach could potentially be detected by the use of a filter or advisory (Higgins et al., 2006; Landauer et al., 2003) designed to identify anomalous writing strategies. However, there could be more subtle approaches to exploiting such a feature. For example, it is possible that test-takers might be inclined to increase their use of nam</context>
</contexts>
<marker>Page, Petersen, 1995</marker>
<rawString>Ellis Page and Nancy Petersen. 1995. The computer moves into essay grading: Updating the ancient test. Phi Delta Kappan, 76:561565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cham Perelman</author>
<author>Lucie Olbrechts-Tyteca</author>
</authors>
<title>The New Rhetoric: A Treatise on Argumentation. Notre \x0cDame, Indiana:</title>
<date>1969</date>
<publisher>Dame Press.</publisher>
<institution>University of Notre</institution>
<contexts>
<context position="3584" citStr="Perelman and Olbrechts-Tyteca, 1969" startWordPosition="562" endWordPosition="565">tion, the notion of fact is uniquely characterized by the idea that is held of agreements of a certain type relating to certain data, those which refer to an objective reality, and, in Poincares words, designate essentially what is common to several thinking beings, and could be common to all (Perelman and OlbrechtsTyteca, 1969, 67). Factuality is thus a matter of selecting certain kinds of data and securing a certain type of agreement over those data. Of the different statements that refer to objective reality, the term facts is used to designate ob63 \x0cjects of precise, limited agreement (Perelman and Olbrechts-Tyteca, 1969, 69). These are contrasted with presumptions statements connected to what is normal and likely (ibid.). We suggest that the distinctions in the scope of the required agreement can be related to the referential device used in a statement: If the reference is more rigid (Kripke, 1980), that is, less prone to change in time and to indeterminacy of the boundaries, the scope of the necessary agreement is likely to be more precise and limited. With proper names prototypically being the most rigid designators, we will focus our efforts on statements about named entities.1 Perhaps the simplest model </context>
<context position="17804" citStr="Perelman and Olbrechts-Tyteca (1969)" startWordPosition="2974" endWordPosition="2977"> test-taker, is opposed by 6 hits with the predicate did not build against matches with predicates such as paved the way to, led indirectly to, helped in, created the theory of. The conflicting accounts seem to reflect a lack of consensus on the degree of Einsteins responsibility. The cases above clearly demonstrate the implications of the argumentative notion of facts used in our project. Facts are statements that the audience is prepared to accept without further justification, differently from arguments, and even from presumptions (statements about what is normal and likely), for which, as Perelman and Olbrechts-Tyteca (1969) observe, additional justification is beneficial for strengthening the audiences adherence. Certainly in the Obama case and possibly in others, a different notion of factuality, for example, a notion that emphasizes availability of legally acceptable supporting evidence, would have led to a different result. Yet, in an ongoing instance of argumentation, the mere need to resort to such a proof is already a sign that the audience is not prepared to accept a statement as a fact. 6.4 Additional filters We also implemented a number of filters aimed at detecting excessive diversity in the matches, w</context>
</contexts>
<marker>Perelman, Olbrechts-Tyteca, 1969</marker>
<rawString>Cham Perelman and Lucie Olbrechts-Tyteca. 1969. The New Rhetoric: A Treatise on Argumentation. Notre \x0cDame, Indiana: University of Notre Dame Press.</rawString>
</citation>
<citation valid="true">
<title>Translated by John Wilkinson and Purcell Weaver from French original published in</title>
<date>1958</date>
<marker>1958</marker>
<rawString>Translated by John Wilkinson and Purcell Weaver from French original published in 1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Powers</author>
<author>Jill Burstein</author>
<author>Martin Chodorow</author>
<author>Mary Fowles</author>
<author>Karen Kukich</author>
</authors>
<title>Stumping E-Rater: Challenging the Validity of Automated Essay Scoring. ETS research report RR-01-03, http://www.ets.org/research/policy research reports/rr01-03.</title>
<date>2001</date>
<contexts>
<context position="29906" citStr="Powers et al., 2001" startWordPosition="4998" endWordPosition="5001"> could potentially be detected by the use of a filter or advisory (Higgins et al., 2006; Landauer et al., 2003) designed to identify anomalous writing strategies. However, there could be more subtle approaches to exploiting such a feature. For example, it is possible that test-takers might be inclined to increase their use of named entities by adducing more facts in support of an argument, and would go beyond the comfort zone of their actual factual knowledge, thus making more factual mistakes. Test gaming strategies have been recognized as a threat to automated scoring systems for some time (Powers et al., 2001), and there is evidence based on test takers own self-reported behavior that this threat is real (Powers, 2011). This is one major reason why largescale operational testing programs (such as GRE or TOEFL) use automated essay scoring only in combination with human ratings. In sum, the degree to which a linguistic feature is predictive of human essay scores is not the only criterion for evaluation; the washback effects of using the feature (on writing behavior and on instruction) must also be considered. The second finding of this study is that the effectiveness of fact-checking for essay assess</context>
</contexts>
<marker>Powers, Burstein, Chodorow, Fowles, Kukich, 2001</marker>
<rawString>Donald Powers, Jill Burstein, Martin Chodorow, Mary Fowles, and Karen Kukich. 2001. Stumping E-Rater: Challenging the Validity of Automated Essay Scoring. ETS research report RR-01-03, http://www.ets.org/research/policy research reports/rr01-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Powers</author>
</authors>
<title>Wordiness: A selective review of its influence, and suggestions for investigating its relevance in tests requiring extended written responses. ETS research memorandum RM-04-08, http://www.ets.org/research/policy research reports/rm04-08.</title>
<date>2005</date>
<contexts>
<context position="22671" citStr="Powers, 2005" startWordPosition="3799" endWordPosition="3800"> a count of 1. #Filtered Matches The number of queries that passed the filters introduced in section 6. If the original query or any of its expansion variants passed the filters, the query contributes a count of 1. Table 5 shows the results. First, we find that all correlations are significant at p=0.05, as well as the partial correlations exluding the effect of length for 7 out of 10 prompts. All correlations are positive, that is, the more factual information a writer employs in an essay, the higher the grade beyond the oft reported correlations between the grade and the length of an essay (Powers, 2005). Second, we notice that all characteristics from the number of named entities to the number of filtered matches produce similar correlation figures. Third, there are large differences between average numbers of named entities per essay across prompts. 68 \x0cPrompt NE Pearson Corr. with Grade Partial Corr. Removing Length #NE #Q #Mat. # Filt. #NE #Q #Mat. # Filt. P1 280 0.144 0.154 0.182 0.185 0.006 0.019 0.058 0.076 P2 406 0.265 0.259 0.274 0.225 0.039 0.053 0.072 0.069 P3 452 0.245 0.225 0.188 0.203 0.049 0.033 0.009 0.051 P4 658 0.327 0.302 0.335 0.327 0.165 0.159 0.177 0.160 P5 704 0.470 </context>
</contexts>
<marker>Powers, 2005</marker>
<rawString>Donald Powers. 2005. Wordiness: A selective review of its influence, and suggestions for investigating its relevance in tests requiring extended written responses. ETS research memorandum RM-04-08, http://www.ets.org/research/policy research reports/rm04-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Powers</author>
</authors>
<title>Scoring the TOEFL Independent Essay Automatically: Reactions of Test Takers and Test Score Users.</title>
<date>2011</date>
<pages>11--34</pages>
<note>ETS research manuscript RM-11-34, http://www.ets.org/research/policy research</note>
<contexts>
<context position="30017" citStr="Powers, 2011" startWordPosition="5019" endWordPosition="5020">ed to identify anomalous writing strategies. However, there could be more subtle approaches to exploiting such a feature. For example, it is possible that test-takers might be inclined to increase their use of named entities by adducing more facts in support of an argument, and would go beyond the comfort zone of their actual factual knowledge, thus making more factual mistakes. Test gaming strategies have been recognized as a threat to automated scoring systems for some time (Powers et al., 2001), and there is evidence based on test takers own self-reported behavior that this threat is real (Powers, 2011). This is one major reason why largescale operational testing programs (such as GRE or TOEFL) use automated essay scoring only in combination with human ratings. In sum, the degree to which a linguistic feature is predictive of human essay scores is not the only criterion for evaluation; the washback effects of using the feature (on writing behavior and on instruction) must also be considered. The second finding of this study is that the effectiveness of fact-checking for essay assessment is compromised by the limited coverage of the wealth of factual statements made by essay writers, with onl</context>
</contexts>
<marker>Powers, 2011</marker>
<rawString>Donald Powers. 2011. Scoring the TOEFL Independent Essay Automatically: Reactions of Test Takers and Test Score Users. ETS research manuscript RM-11-34, http://www.ets.org/research/policy research reports/rm11-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a Question Answering System. In</title>
<date>2002</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>4147</pages>
<contexts>
<context position="5375" citStr="Ravichandran and Hovy, 2002" startWordPosition="863" endWordPosition="866">ts to a working model of the universal audience. Recent developments in Open Information Extraction make it possible to tap into this vast knowledge resource. Indeed, fact-checking is one of the applications the developers of OpenIE have in mind for their emergent technology (Etzioni et al., 2008). 3 Open Information Extraction Traditionally, the goal of an information extraction system is automated population of structured databases of events or concepts of interest and their properties by analyzing large corpora of text (Chinchor et al., 1993; Onyshkevych, 1993; Grishman and Sundheim, 1995; Ravichandran and Hovy, 2002; Agichtein and Gravano, 2000; Davidov and Rappoport, 2009). 1 For example, Barack Obama picks out precisely one person, and the same one in 2010 as it did in 1990. In contrast, the current US president picks out different people every 4-8 years. For indeteminacy of boundaries, consider a statement like US officials are wealthy. To determine its truth, one must first secure agreement on acceptable referents of US officials. In contrast, the recently proposed Open Information Extraction paradigm aims to detect related pairs of entities without knowing in advance what kinds of relations exist be</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a Question Answering System. In Proceedings of ACL, pages 4147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn Rose</author>
<author>Antonio Roqueand</author>
<author>Dumisizwe Bhembe</author>
<author>Kurt VanLehn</author>
</authors>
<title>A hybrid text classification approach for analysis of student essays.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>2936</pages>
<contexts>
<context position="1539" citStr="Rose et al., 2003" startWordPosition="224" endWordPosition="227">ications for development of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Rose et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the readers knowledge clearly detract from an essays quality. This paper presents a study on assessing the use of factual knowledge in argumentative essays on general topics written for a graduate school entrance e</context>
</contexts>
<marker>Rose, Roqueand, Bhembe, VanLehn, 2003</marker>
<rawString>Carolyn Rose, Antonio Roqueand, Dumisizwe Bhembe, and Kurt VanLehn. 2003. A hybrid text classification approach for analysis of student essays. In Proceedings of the Second Workshop on Building Educational Applications Using NLP, pages 2936.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>