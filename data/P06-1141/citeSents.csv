A very common case of this in the CoNLL dataset is that of documents containing references to both The China Daily, a newspaper, and China, the country CITATION,,
CITATION and CITATION condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a previous sentence of the same document,,
CITATION augment a sequential CRF with skip-edges i.e,,
Both these approaches use loopy belief propagation (CITATION; CITATION) for approximate inference,,
The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus, which would be relatively much harder to incorporate in approaches like CITATION and CITATION,,
This also enables us to do inference efficiently since our inference time is merely the inference time of two sequential CRFs; in contrast CITATION reported an increase in running time by a factor of 30 over the sequential CRF, with their Gibbs sampling approximate inference,,
Most work has looked to model non-local dependencies only within a document (Finkel 1125 \x0cet al., 2005; CITATION; CITATION; CITATION),,
CITATION propose a solution to this problem: for each token, they define additional features based on known information, taken from other occurrences of the same token in the document,,
The approach of CITATION makes it possible a to model a broader class of longdistance dependencies than CITATION, because they do not need to make any initial assumptions about which nodes should be connected and they too model dependencies between whole token sequences representing entities and between entity token sequences and their token supersequences that are entities,,
We also compare our performance against CITATION and CITATION and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF,,
CITATION hand-set penalties for inconsistency in entity labeling at different occurrences in the text, based on some statistics from training data,,
Hidden Markov Models (HMMs) (CITATION; CITATION), Conditional Markov Models (CMMs) (CITATION; CITATION), and Conditional Random Fields (CRFs) CITATION have been successfully employed in NER and other information extraction tasks,,
ps inference time down to just the inference time of two sequential CRFs, when compared to approaches such as those of CITATION who report that their inference time with Gibbs sampling goes up by a factor of about 30, compared to the Viterbi algorithm for the sequential CRF,,
We use the approximate randomization test CITATION for statistical significance of the difference between the basic sequential CRF and our second round CRF, which has additional features derived from the output of the first CRF,,
Additionally, our approach makes it possible to do inference in just about twice the inference time with a single sequential CRF; in contrast, approaches like Gibbs Sampling that model the dependencies directly can increase inference time by a factor of 30 CITATION,,
They then employ Gibbs sampling CITATION for dealing with their local feature weights and their non-local penalties to do approximate inference,,
CITATION exploit label consistency information within a document using relatively ad hoc multi-stage labeling procedures,,
 randomization test CITATION for statistical significance of the difference between the basic sequential CRF and our second round CRF, which has additional features derived from the output of the first CRF,,
2 Conditional Random Fields We use a Conditional Random Field (CITATION; CITATION) since it represents the state of the art in sequence modeling and has also been very effective at Named Entity Recognition,,
At the same time, the simplicity of our two-stage approach keeps inference time down to just the inference time of two sequential CRFs, when compared to approaches such as those of CITATION who report that their inference time with Gibbs sampling goes up by a factor of about 30, compared to the Viterbi algorithm for the sequential CRF,,
CITATION define a Relational Markov Network (RMN) which explicitly models long-distance dependencies, and use it to represent relations between entities,,
Most existing work to capture labelconsistency, has attempted to create all n 2 \x01 pairwise dependencies between the different occurrences of an entity, (CITATION; CITATION), where n is the number of occurrences of the given entity,,
7 Related Work Recent work looking to directly model non-local dependencies and do approximate inference are that of CITATION, who use a Relational Markov Network (RMN) CITATION to explicitly model long-distance dependencies, CITATION, who introduce skip-chain CRFs, which add additional non-local edges to the underlying CRF sequence model (which CITATION lack) and CITATION who hand-set penalties for inconsistency in labels based on the training data and then use Gibbs Sampling for doing approximate inference where the goal is to obtain the label sequence that maximizes the product of the CRF objective function and their penalty,,
