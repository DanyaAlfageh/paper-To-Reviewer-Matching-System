<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<reference confidence="0.29998775">
b&apos;First Joint Conference on Lexical and Computational Semantics (*SEM), pages 579585,
Montreal, Canada, June 7-8, 2012. c
2012 Association for Computational Linguistics
sranjans : Semantic Textual Similarity using Maximal Weighted
</reference>
<title confidence="0.87403">
Bipartite Graph Matching
</title>
<author confidence="0.922136">
Sumit Bhagwani, Shrutiranjan Satapathy, Harish Karnick
</author>
<affiliation confidence="0.986263">
Computer Science and Engineering
</affiliation>
<address confidence="0.807971">
IIT Kanpur, Kanpur - 208016, India
</address>
<email confidence="0.989793">
{sumitb,sranjans,hk}@cse.iitk.ac.in
</email>
<sectionHeader confidence="0.990379" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9983565">
The paper aims to come up with a sys-
tem that examines the degree of semantic
equivalence between two sentences. At the
core of the paper is the attempt to grade
the similarity of two sentences by find-
ing the maximal weighted bipartite match
between the tokens of the two sentences.
The tokens include single words, or multi-
words in case of Named Entitites, adjec-
tivally and numerically modified words.
Two token similarity measures are used for
the task - WordNet based similarity, and a
statistical word similarity measure which
overcomes the shortcomings of WordNet
based similarity. As part of three systems
created for the task, we explore a simple
bag of words tokenization scheme, a more
careful tokenization scheme which cap-
tures named entities, times, dates, mone-
tary entities etc., and finally try to capture
context around tokens using grammatical
dependencies.
</bodyText>
<sectionHeader confidence="0.996801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997888976744186">
Semantic Textual Similarity (STS) measures
the degree of semantic equivalence between
texts. The goal of this task is to create a unified
framework for the evaluation of semantic textual
similarity modules and to characterize their im-
pact on NLP applications. The task is part of the
Semantic Evaluation 2012 Workshop (Agirre et
al., 2012).
STS is related to both Textual Entailment and
Paraphrase, but differs in a number of ways and
it is more directly applicable to a number of NLP
tasks. Also, STS is a graded similarity notion -
this graded bidirectional nature of STS is useful
for NLP tasks such as MT evaluation, informa-
tion extraction, question answering, and summa-
rization.
We propose a lexical similarity approach to
grade the similarity of two sentences, where a
maximal weighted bipartite match is found be-
tween the tokens of the two sentences. The ap-
proach is robust enough to apply across different
datasets. The results on the STS test datasets are
encouraging to say the least. The tokens are sin-
gle word tokens in case of the first system, while
in the second system, named and monetary en-
tities, percentages, dates and times are handled
too. A token-token similarity measure is integral
to the approach and we use both a statistical sim-
ilarity measure and a WordNet based word sim-
ilarity measure for the same. In the final run
of the task, apart from capturing the aforemen-
tioned entities, we heuristically extract adjecti-
vally and numerically modified words. Also, the
last run naively attempts to capture the context
around the tokens using grammatical dependen-
cies, which in turn is used to measure context
similarity.
Section 2 discusses the previous work done
in this area. Section 3 describes the datasets,
the baseline system and the evaluation measures
used by the task organizers. Section 4, 5 and 6
introduce the systems developed and discuss the
results of each system. Finally, section 7 con-
</bodyText>
<page confidence="0.995873">
579
</page>
<bodyText confidence="0.9892585">
\x0ccludes the work and section 8 offers suggestions
for future work.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999278394736842">
Various systems exist in literature for tex-
tual similarity measurement, be it bag of
words based models or complex semantic sys-
tems. (Achananuparp et al., 2008) enumerates a
few word overlap measures, like Jaccard Similar-
ity Coefficient, IDF Overlap measures, Phrasal
overlap measures etc, that have been used for
sentential similarity.
(Liu et al., 2008) proposed an approach to cal-
culate sentence similarity, which takes into ac-
count both semantic information and word order.
They define semantic similarity of sentence 1 rel-
ative to sentence 2 as the ratio of the sum of the
word similarity weighted by information content
of words in sentence 1 to the overall information
content included in both sentences. The syntactic
similarity is calculated as the correlation coeffi-
cient between word order vectors.
A similar semantic similarity measure, pro-
posed by (Li et al., 2006), uses a semantic-vector
approach to measure sentence similarity. Sen-
tences are transformed into feature vectors hav-
ing individual words from the sentence pair as
a feature set. Term weights are derived from
the maximum semantic similarity score between
words in the feature vector and words in the cor-
responding sentence. To utilize word order in the
similarity calculation, they define a word order
similarity measure as the normalized difference
of word order between the two sentences. They
have empirically proved that a sentence simi-
larity measure performs the best when semantic
measure is weighted more than syntactic measure
(ratio 4:1). This follows the conclusion from
a psychological experiment conducted by them
which emphasizes the role of semantic informa-
tion over syntactic information in passage under-
standing.
</bodyText>
<sectionHeader confidence="0.962911" genericHeader="method">
3 Task Evaluation
</sectionHeader>
<subsectionHeader confidence="0.969272">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.923367625">
The development datasets are drawn from the
following sources :
MSR Paraphrase : This dataset consists
of pairs of sentences which have been ex-
tracted from news sources on the web.
MSR Video : This dataset consists of
pairs of sentences where each sentence of a
pair tries to summarize the action in a short
video snippet.
SMT Europarl : This dataset consists of
pairs sentences drawn from the proceedings
of the European Parliament, where each
sentence of a pair is a translation from a Eu-
ropean language to English.
In addition to the above sources, the test
datasets also contained the following sources :
SMT News : This dataset consists of ma-
chine translated news conversation sentence
pairs.
On WN : This dataset consists of pairs
of sentences where the first comes from
Ontonotes(Hovy et al., 2006) and the sec-
ond from a WordNet definition. Hence, the
sentences are rather phrases.
</bodyText>
<subsectionHeader confidence="0.97213">
3.2 Baseline
</subsectionHeader>
<bodyText confidence="0.9992338">
The task organizers have used the following
baseline scoring scheme. Scores are produced
using a simple word overlap baseline system.
The input sentences are tokenised by splitting
at white spaces, and then each sentence is rep-
resented as a vector in the multidimensional to-
ken space. Each dimension has 1 if the token is
present in the sentence, 0 otherwise. Similarity
of vectors is computed using the cosine similar-
ity.
</bodyText>
<subsectionHeader confidence="0.99909">
3.3 Evaluation Criteria
</subsectionHeader>
<bodyText confidence="0.9906878">
The scores obtained by the participating systems
are evaluated against the gold standard of the
datasets using a pearson correlation measure. In
order to evaluate the overall performance of the
systems on all the five datasets, the organizers
use three evaluation measures :
ALL : This measure takes the union of all
the test datasets, and finds the Pearson cor-
relation of the system scores with the gold
standard of the union.
</bodyText>
<page confidence="0.969494">
580
</page>
<bodyText confidence="0.9926092">
\x0c ALL Normalized : In this measure, a lin-
ear fit is found for the system scores on
each dataset using a least squared error cri-
terion, and then the union of the linearly fit-
ted scores is used to calculate the Pearson
correlation against the gold standard union.
Weighted Mean : The average of the Pear-
son correlation scores of the systems on the
individual datasets is taken, weighted by the
number of test instances in each dataset.
</bodyText>
<sectionHeader confidence="0.991255" genericHeader="method">
4 SYSTEM 1
</sectionHeader>
<subsectionHeader confidence="0.997754">
4.1 Tokenization Scheme
</subsectionHeader>
<bodyText confidence="0.999226142857143">
Each sentence is tokenized into words, filter-
ing out punctuations and stop-words. The stop-
words are taken from the stop-word list provided
by the NLTK Toolkit (Bird et al., 2009). All
the word tokens are reduced to their lemmatized
form using the Stanford CoreNLP Toolkit (Min-
nen et al., 2001). The tokenization is basic in
nature and doesnt handle named entities, times,
dates, monetary entities or multi-word expres-
sions. The challenge with handling multi-word
tokens is in calculating multi-word token simi-
larity, which is not supported in a WordNet word-
similarity scheme or a statistical word similarity
measure.
</bodyText>
<subsectionHeader confidence="0.992">
4.2 Maximal Weighted Bipartite Match
</subsectionHeader>
<bodyText confidence="0.968973555555556">
A weighted bipartite graph is constructed
where the two sets of vertices are the word-
tokens extracted in the earlier subsection. The
bipartite graph is made complete by assigning an
edge weight to every pair of tokens from the two
sentences. The edge weight is based on a suitable
word similarity measure. We had two resources
at hand - WordNet based word similarity and a
statistical word similarity measure.
</bodyText>
<subsubsectionHeader confidence="0.9867">
4.2.1 WordNet Based Word Similarity
</subsubsectionHeader>
<bodyText confidence="0.9871233125">
The is-a hierarchy of WordNet is used in cal-
culating the word similarity of two words. Nouns
and verbs have separate is-a hierarchies. We use
the Lin word-sense similarity measure (Lin ,
1998a). Adjectives and adverbs do not have an
is-a hierarchy and hence do not figure in the Lin
similarity measure. To disambiguate the Word-
Net sense of a word in a sentence, a variant of
the Simplified Lesk Algorithm (Kilgarriff and
J. Rosenzweig , 2000) is used. WordNet based
word similarity has the following drawbacks :
sparse in named entity content : similarity
of named entities with other words becomes
infeasible to calculate.
doesnt support cross-POS similarity.
applicable only to nouns and verbs.
</bodyText>
<subsubsectionHeader confidence="0.993932">
4.2.2 Statistical Word Similarity
</subsubsectionHeader>
<bodyText confidence="0.998252">
We use DISCO (Kolb , 2008) as our statisti-
cal word similarity measure. DISCO is a tool for
retrieving the distributional similarity between
two given words. Pre-computed word spaces are
freely available for a number of languages. We
use the English Wikipedia word space. One pri-
mary reason for using a statistical word similarity
measure is because of the shortcomings of calcu-
lating cross-POS word similarity when using a
knowledge base like WordNet.
DISCO works as follows : a term-(term,
relative position) matrix is constructed with
weights being pointwise mutual information
scores. From this, a surface level word similar-
ity score is obtained by using Lins information
theoretic measure (Lin , 1998b) for word vector
similarity. This score is used as matrix weights
to get second order word vectors, which are used
to compute a second order word similarity mea-
sure . This measure tries to emulate an LSA like
similarity giving better performance, and hence
is used for the task.
A point to note here is that the precomputed
word spaces that DISCO uses are case sensitive,
which we think is a drawback. We preserve the
case of proper nouns, while all other words are
converted to lower case, prior to evaluating word
similarity scores.
</bodyText>
<subsectionHeader confidence="0.997529">
4.3 Edge Weighting Scheme
</subsectionHeader>
<bodyText confidence="0.996487">
Sentences in the MSR video dataset are simpler
and shorter than the remaining datasets, with a
high degree of POS correspondence between the
</bodyText>
<page confidence="0.991679">
581
</page>
<table confidence="0.9764259">
\x0cDataset DISCO WordNet DISCO + WordNet
MSR Video 0.61 0.71 0.73
MSR Paraphrase 0.62 0.43 0.57
SMT Europarl 0.58 0.44 0.54
Figure 1: Edge Weight Scheme Evaluation on Development Datasets
Category NE Normalized NE
DATE 26th November, November 26 XXXX-11-26
PERCENT 13 percent, 13% %13.0
MONEY 56 dollars, $56, 56$ $56.0
TIME 3 pm, 15:00 T15:00
</table>
<figureCaption confidence="0.943847">
Figure 2: Normalization performed by Stanford CoreNLP
</figureCaption>
<bodyText confidence="0.971887035714286">
tokens of two sentences, as can be observed in
the following example :
A man is riding a bicycle. VS A man is rid-
ing a bike.
This allows for the use of a Knowledge-Base
Word Similarity measure like WordNet word
similarity. All the other datasets have length-
ier sentences, resulting in cross-POS correspon-
dence. Additionally, there is an abundance of
named entities in these datasets. The following
examples, which are drawn from the MSR Para-
phrase dataset, highlight these points :
If convicted of the spying charges, he could
face the death penalty. VS The charges of
espionage and aiding the enemy can carry
the death penalty.
Microsoft has identified the freely dis-
tributed Linux software as one of the biggest
threats to its sales. VS The company
has publicly identified Linux as one of its
biggest competitive threats.
Keeping this in mind, we use DISCO for edge-
weighting in all the datasets except MSR Video.
For MSR Video, we use the following edge
weighting scheme : for same-POS words, Word-
Net similarity is used, DISCO otherwise. This
choice is justified by the results obtained in fig-
ure 1 on the development datasets.
</bodyText>
<subsubsectionHeader confidence="0.870022">
4.3.1 Scoring
</subsubsectionHeader>
<bodyText confidence="0.967805909090909">
A maximal weighted bipartite match is found
for the bipartite graph constructed, using the
Hungarian Algorithm (Kuhn , 1955) - the
intuition behind this being that every keyword
in a sentence matches injectively to a unique
keyword in the other sentence. The maximal
bipartite score is normalized by the sentences
length for two reasons - normalization and
punishment for extra detailing in either sentence.
So the final sentence similarity score between
sentences s1 and s2 is:
</bodyText>
<equation confidence="0.9949635">
sim(s1, s2) = MaximalBipartiteMatchSum(s1,s2)
max(tokens(s1),tokens(s2))
</equation>
<subsectionHeader confidence="0.601567">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.975591714285714">
The results are evaluated on the test datasets
provided for the STS task. Figure 3 compares
the performance of our systems with the top 3
systems for the task. The scores in the figure
are Pearson Correlation scores. Figure 4 shows
the performance and ranks of all our systems. A
total of 89 systems were submitted, including
the baseline. The results are taken from the
Semeval12 Task 6 webpage1
As can be seen, System 1 suffers slightly
on the MSR Paraphrase and Video datasets,
while doing comparably well on the other three
datasets when compared with the top 3 submis-
sions. Our ALL score suffers because we use
</bodyText>
<figure confidence="0.625804333333333">
1
http://www.cs.york.ac.uk/semeval-
2012/task6/index.php?id=results-update
</figure>
<page confidence="0.691047">
582
</page>
<table confidence="0.888076153846154">
\x0cSystem ALL MSR Para-
phrase
MSR Video SMT Eu-
roparl
OnWN SMT News
Rank 1 0.8239 0.6830 0.8739 0.5280 0.6641 0.4937
Rank 2 0.8138 0.6985 0.8620 0.3612 0.7049 0.4683
Rank 3 0.8133 0.7343 0.8803 0.4771 0.6797 0.3989
System 1 0.6529 0.6124 0.7240 0.5581 0.6703 0.4533
System 2 0.6651 0.6254 0.7538 0.5328 0.6649 0.5036
System 3 0.5045 0.6167 0.7061 0.5666 0.5664 0.3968
Li et al. 0.4981 0.6141 0.6084 0.5382 0.6055 0.3760
Baseline 0.3110 0.4334 0.2996 0.4542 0.5864 0.3908
</table>
<figureCaption confidence="0.856154">
Figure 3: Results of top 3 Systems and Our Systems
</figureCaption>
<table confidence="0.617588153846154">
System ALL ALL Rank All Nor-
malized
All Nor-
malized
Rank
Weighted
Mean
Weighted
Mean
Rank
System 1 0.6529 30 0.8018 39 0.6249 12
System 2 0.6651 24 0.8128 22 0.6366 8
System 3 0.5045 62 0.7846 52 0.5905 30
</table>
<figureCaption confidence="0.999575">
Figure 4: Evaluation of our Systems on different criteria
</figureCaption>
<bodyText confidence="0.901051125">
a combination of WordNet and statistical word
similarity measure for the MSR Video dataset,
which affects the Pearson Correlation of all the
datasets combined. The correlation values for
the ALL Normalized criterion are high because
of the linear fitting it performs. We get the best
performance on the Weighted Mean evaluation
criterion.
</bodyText>
<sectionHeader confidence="0.998395" genericHeader="method">
5 SYSTEM 2
</sectionHeader>
<bodyText confidence="0.9989560625">
In System 2, in addition to System 1, we cap-
ture named entities, dates and times, percentages
and monetary entities and normalize them. The
tokens resulting from this can be multi-word be-
cause of named entities. This tokenization strat-
egy gives us the best results among all our three
runs. For capturing and normalizing the above
mentioned expressions, we make use of the Stan-
ford NER Toolkit (Finkel et al., 2005). Some
normalized samples are mentioned in figure 2.
When grading the similarity of multi-word
tokens, we use a second level maximal bipartite
match, which is normalized by the smaller of the
two multi-word token lengths. Thus, similarity
between two multi-word tokens t1 and t2 is
defined as:
</bodyText>
<equation confidence="0.916649">
sim(t1, t2) = MaximalBipartiteMatchSum(t1,t2)
min(words(t1),words(t2))
</equation>
<bodyText confidence="0.9981981">
This was done to ensure that a complete
named entity in the first sentence matches ex-
actly with a partial named entity (indicating the
same entity as the first) in the second sentence.
For eg. John Doe vs John will be given a score
of 1. Such occurrences are frequent in the task
datasets. For the sentence similarity, the score
defined in System 1 is used, where the token
length of a sentence is the number of multi-word
tokens in it.
</bodyText>
<sectionHeader confidence="0.688874" genericHeader="method">
5.1 Results
</sectionHeader>
<bodyText confidence="0.999519571428571">
Refer to figures 3 and 4 for results.
This system gives the best results among all
our systems. The credit for this improvement can
be attributed to recognition and normalization of
named entities, dates and times, percentages and
monetary entities, as the datasets provided con-
tain these in fairly large numbers.
</bodyText>
<page confidence="0.997129">
583
</page>
<sectionHeader confidence="0.550217" genericHeader="method">
\x0c6 SYSTEM 3
</sectionHeader>
<bodyText confidence="0.9355068">
In System 3, in addition to System 2, we heuris-
tically capture compound nouns, adjectivally
and numerically modified words like passenger
plane, easy job, 10 years etc. using the POS
based regular expression
</bodyText>
<figure confidence="0.509052">
[JJ|NN|CD]
NN
POS Tagging is done using the Stanford POS
Tagger Toolkit (Toutanova et al., 2003).
</figure>
<bodyText confidence="0.99529">
To make matching more context dependent,
rather than just a bag of words approach, we
naively attempt to capture the similarity of the
contexts of two tokens. We define the context
of a word in a sentence as all the words in the
sentence which are grammatically related to
it. The grammatical relations are all the col-
lapsed dependencies produced by the Stanford
Dependency parser (Marneffe et al., 2006). The
context of a multi-word token is defined as the
union of contexts of all the words in it. We
further filter the context by removing stop-words
and punctuations in it. The contexts of two
tokens are then used to obtain context/syntactic
similarity between tokens, which is defined
using the Jaccard Similarity Measure:
</bodyText>
<equation confidence="0.991694666666667">
Jaccard(C1, C2) =
|C1 C2|
|C1 C2|
</equation>
<bodyText confidence="0.998879214285714">
A linear combination of word similarity and
context similarity is taken as an edge weight in
the token-token similarity bipartite graph. Moti-
vated by (Li et al., 2006), we chose a ratio of 4:1
for lexical similarity to context similarity.
As in System 2, for multi-word token simi-
larity, we use a second level maximal bipartite
match, normalized by smaller of the two token
lengths. This helps in matching multi-word to-
kens expressing the same meaning with score
1, for e.g. passenger plane VS Cuban plane,
divided Supreme Court VS Supreme Court etc.
The sentence similarity score is the same as the
one defined in System 2.
</bodyText>
<sectionHeader confidence="0.562896" genericHeader="evaluation">
6.1 Results
</sectionHeader>
<bodyText confidence="0.99954">
Refer to figures 3 and 4 for results.
This system gives a reduced performance
compared to our other systems. This could be
due to various factors. Capturing adjectivally and
numerically modified words could be done using
grammatical dependencies instead of a heuristic
POS-tag regular expression. Also, token-token
similarity should be handled in a more precise
way than a generic second level maximal bipar-
tite match. A better context capturing method
can further improve the system.
</bodyText>
<sectionHeader confidence="0.99858" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.929132769230769">
Among the three systems proposed for the task,
System 2 performs best on the test datasets, pri-
marily because it identifies named entities as sin-
gle entities, normalizes dates, times, percentages
and monetary figures. The results for System
3 suffer because of naive context capturing. A
better job can be done using syntacto-semantic
structured representations for the sentences. The
performance of our systems are compared with
(Li et al., 2006) on the test datasets in figure
3. This highlights the improvement of maximal
weighted bipartite matching over greedy match-
ing.
</bodyText>
<sectionHeader confidence="0.99566" genericHeader="references">
8 Future Work
</sectionHeader>
<bodyText confidence="0.996113272727273">
Our objective is to group words together which
share a common meaning. This includes group-
ing adjectival, adverbial, numeric modifiers with
the modified word, group the words of a collo-
quial phrase together, capture multi-word expres-
sions, etc. These word-clusters will form the ver-
tices of the bipartite graph. The other challenge
then is to come up with a suitable cluster-cluster
similarity measure. NLP modules such as Lex-
ical Substitution can help when we are using a
word-word similarity measure at the core.
</bodyText>
<sectionHeader confidence="0.966999" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999419666666667">
The authors would like to thank the anonymous
reviewers for their valuable comments and sug-
gestions to improve the quality of the paper.
</bodyText>
<page confidence="0.996996">
584
</page>
<reference confidence="0.9953285">
\x0cReferences
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. Proceedings of the
41st Meeting of the Association for Computational
Linguistics, pp. 423-430.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw and Ralph Weischedel. 2006.
OntoNotes: The 90% Solution. Proceedings of
HLT/NAACL, New York, 2006.
Eneko Agirre, Daniel Cer, Mona Diab and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6:
A Pilot on Semantic Textual Similarity. In Pro-
ceedings of the 6th International Workshop on Se-
mantic Evaluation (SemEval 2012), in conjunc-
tion with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
G. Minnen, J. Carroll and D. Pearce. 2001. Ap-
plied morphological processing of English. Nat-
ural Language Engineering, 7(3). 207-223.
Harold W. Kuhn. 1955. The Hungarian Method for
the assignment problem. Naval Research Logistics
Quarterly, 2:8397, 1955.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local In-
formation into Information Extraction Systems by
Gibbs Sampling. Proceedings of the 43nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2005), pp. 363-370
Kilgarriff and J. Rosenzweig. 2000. English SEN-
SEVAL : Report and Results. In Proceedings of
the 2nd International Conference on Language Re-
sources and Evaluation, LREC, Athens, Greece.
Kristina Toutanova, Dan Klein, Christopher Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of HLT-NAACL 2003, pp.
252-259.
Lin, D. 1998a. An information-theoretic definition
of similarity. In Proceedings of the International
Conference on Machine Learning.
Lin, D. 1998b. Automatic Retrieval and Clustering of
Similar Words.. In Proceedings of COLING-ACL
1998, Montreal.
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses.
In LREC 2006.
Palakorn Achananuparp, Xiaohua Hu and Shen Xi-
ajiong. 2008. The Evaluation of Sentence Simi-
larity Measures. Science And Technology, 5182,
305-316. Springer.
Peter Kolb. 2008. DISCO: A Multilingual Database
of Distributionally Similar Words. In Proceedings
of KONVENS-2008, Berlin.
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python - An-
alyzing Text with the Natural Language Toolkit.
OReilly Media, 2009
Xiao-Ying Liu, Yi-Ming Zhou, Ruo-Shi Zheng.
2008. Measuring Semantic Similarity Within Sen-
tences. Proceedings of the Seventh International
Conference on Machine Learning and Cybernetics,
Kunming.
Yuhua Li, David McLean, Zuhair A. Bandar, James
D. OShea, and Keeley Crockett. 2006. Sentence
Similarity Based on Semantic Nets and Corpus
Statistics. IEEE Transections on Knowledge and
Data Engineering, Vol. 18, No. 8
</reference>
<page confidence="0.973638">
585
</page>
<figure confidence="0.252523">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.411931">
<note confidence="0.902794333333333">b&apos;First Joint Conference on Lexical and Computational Semantics (*SEM), pages 579585, Montreal, Canada, June 7-8, 2012. c 2012 Association for Computational Linguistics</note>
<title confidence="0.796197">sranjans : Semantic Textual Similarity using Maximal Weighted Bipartite Graph Matching</title>
<author confidence="0.982165">Sumit Bhagwani</author>
<author confidence="0.982165">Shrutiranjan Satapathy</author>
<author confidence="0.982165">Harish Karnick</author>
<affiliation confidence="0.98359">Computer Science and Engineering</affiliation>
<address confidence="0.990209">IIT Kanpur, Kanpur - 208016, India</address>
<email confidence="0.983501">sumitb@cse.iitk.ac.in</email>
<email confidence="0.983501">sranjans@cse.iitk.ac.in</email>
<email confidence="0.983501">hk@cse.iitk.ac.in</email>
<abstract confidence="0.996641913043478">The paper aims to come up with a system that examines the degree of semantic equivalence between two sentences. At the core of the paper is the attempt to grade the similarity of two sentences by finding the maximal weighted bipartite match between the tokens of the two sentences. The tokens include single words, or multiwords in case of Named Entitites, adjectivally and numerically modified words. Two token similarity measures are used for the task - WordNet based similarity, and a statistical word similarity measure which overcomes the shortcomings of WordNet based similarity. As part of three systems created for the task, we explore a simple bag of words tokenization scheme, a more careful tokenization scheme which captures named entities, times, dates, monetary entities etc., and finally try to capture context around tokens using grammatical dependencies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2012</date>
<booktitle>b&apos;First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>579585</pages>
<location>Montreal, Canada,</location>
<marker>Manning, 2012</marker>
<rawString>b&apos;First Joint Conference on Lexical and Computational Semantics (*SEM), pages 579585, Montreal, Canada, June 7-8, 2012. c 2012 Association for Computational Linguistics sranjans : Semantic Textual Similarity using Maximal Weighted \x0cReferences Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. Proceedings of the 41st Meeting of the Association for Computational Linguistics, pp. 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<date>2006</date>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw and Ralph Weischedel. 2006.</rawString>
</citation>
<citation valid="true">
<title>OntoNotes: The 90% Solution.</title>
<date>2006</date>
<booktitle>Proceedings of HLT/NAACL,</booktitle>
<location>New York,</location>
<marker>2006</marker>
<rawString>OntoNotes: The 90% Solution. Proceedings of HLT/NAACL, New York, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>J Carroll</author>
<author>D Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<pages>207--223</pages>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>G. Minnen, J. Carroll and D. Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3). 207-223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold W Kuhn</author>
</authors>
<title>The Hungarian Method for the assignment problem.</title>
<date>1955</date>
<journal>Naval Research Logistics Quarterly,</journal>
<volume>2</volume>
<marker>Kuhn, 1955</marker>
<rawString>Harold W. Kuhn. 1955. The Hungarian Method for the assignment problem. Naval Research Logistics Quarterly, 2:8397, 1955.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>363--370</pages>
<location>Athens, Greece.</location>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370 Kilgarriff and J. Rosenzweig. 2000. English SENSEVAL : Report and Results. In Proceedings of the 2nd International Conference on Language Resources and Evaluation, LREC, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Partof-Speech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>252--259</pages>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Partof-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-NAACL 2003, pp. 252-259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998a. An information-theoretic definition of similarity. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words..</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL 1998,</booktitle>
<location>Montreal.</location>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998b. Automatic Retrieval and Clustering of Similar Words.. In Proceedings of COLING-ACL 1998, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses.</rawString>
</citation>
<citation valid="true">
<authors>
<author>In LREC</author>
</authors>
<date>2006</date>
<marker>LREC, 2006</marker>
<rawString>In LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Palakorn Achananuparp</author>
<author>Xiaohua Hu</author>
<author>Shen Xiajiong</author>
</authors>
<title>The Evaluation of Sentence Similarity Measures.</title>
<date>2008</date>
<journal>Science And Technology,</journal>
<volume>5182</volume>
<pages>305--316</pages>
<publisher>Springer.</publisher>
<marker>Achananuparp, Hu, Xiajiong, 2008</marker>
<rawString>Palakorn Achananuparp, Xiaohua Hu and Shen Xiajiong. 2008. The Evaluation of Sentence Similarity Measures. Science And Technology, 5182, 305-316. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Kolb</author>
</authors>
<title>DISCO: A Multilingual Database of Distributionally Similar Words.</title>
<date>2008</date>
<booktitle>In Proceedings of KONVENS-2008,</booktitle>
<location>Berlin.</location>
<marker>Kolb, 2008</marker>
<rawString>Peter Kolb. 2008. DISCO: A Multilingual Database of Distributionally Similar Words. In Proceedings of KONVENS-2008, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<title>Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit.</title>
<date>2009</date>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>OReilly Media</author>
</authors>
<title>Xiao-Ying Liu, Yi-Ming Zhou, Ruo-Shi Zheng.</title>
<date>2009</date>
<booktitle>Proceedings of the Seventh International Conference on Machine Learning and Cybernetics,</booktitle>
<location>Kunming.</location>
<marker>Media, 2009</marker>
<rawString>OReilly Media, 2009 Xiao-Ying Liu, Yi-Ming Zhou, Ruo-Shi Zheng. 2008. Measuring Semantic Similarity Within Sentences. Proceedings of the Seventh International Conference on Machine Learning and Cybernetics, Kunming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David McLean</author>
<author>Zuhair A Bandar</author>
<author>James D OShea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence Similarity Based on Semantic Nets and Corpus Statistics.</title>
<date>2006</date>
<journal>IEEE Transections on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<marker>Li, McLean, Bandar, OShea, Crockett, 2006</marker>
<rawString>Yuhua Li, David McLean, Zuhair A. Bandar, James D. OShea, and Keeley Crockett. 2006. Sentence Similarity Based on Semantic Nets and Corpus Statistics. IEEE Transections on Knowledge and Data Engineering, Vol. 18, No. 8</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>