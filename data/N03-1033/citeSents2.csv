Indeed, as for the voted perceptron of CITATION, we can get performance gains by reducing the support threshold for features to be included in the model,,
Whereas CITATION used feature support cutoffs and early stopping to stop overfitting of the model, and CITATION contends that including low support features harms a maximum entropy model, our results show that low support features are useful in a regularized maximum entropy model,,
Table 6 contrasts our results with those from CITATION,,
Indeed, the result of CITATION that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized,,
We extracted tagged sentences from the parse trees.5 We split the data into training, development, and test sets as in CITATION,,
The \x0cTagger Support cutoff Accuracy CITATION 0 96.60% 5 96.72% Model 3W+TAGS variant 1 96.97% 5 96.93% Table 6: Effect of changing common word feature cutoffs (features with support cutoff are excluded from the model),,
For example, CITATION attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization,,
This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in CITATION, using the same data splits, and a larger error reduction of 12.1% from the more similar ,,
ot new to this work: Gaussian prior smoothing is advocated in CITATION, and used in all the stochastic LFG work CITATION,,
3 Experiments The part of speech tagged data used in our experiments is the Wall Street Journal data from Penn Treebank III CITATION,,
At any rate, regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: CITATION, CITATION, and CITATION all present unregularized models,,
