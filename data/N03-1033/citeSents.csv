An interesting example of a common tagging error of the simpler models which could be corrected by a deterministic fixup rule of the kind used in the IDIOMTAG module of CITATION is the expression as X as (often, as far as),,
8CITATION noted that such a simple model got 90.25%, but this was with no unknown word model beyond a prior distri,,
At any rate, regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: CITATION, CITATION, and CITATION all present unregularized models,,
Indeed, the result of CITATION that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized,,
2 Bidirectional Dependency Networks When building probabilistic models for tag sequences, we often decompose the global probability of sequences using a directed graphical model (e.g., an HMM CITATION or a conditional Markov model (CMM) CITATION),,
Whereas CITATION used feature support cutoffs and early stopping to stop overfitting of the model, and CITATION contends that including low support features harms a maximum entropy model, our results show that low support features are useful in a regularized maximum entropy model,,
he usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in CITATION, and used in all the stochastic LFG work CITATION,,
3 Experiments The part of speech tagged data used in our experiments is the Wall Street Journal data from Penn Treebank III CITATION,,
ot new to this work: Gaussian prior smoothing is advocated in CITATION, and used in all the stochastic LFG work CITATION,,
CITATION illustrates very clearly how tagging performance increases as training set size grows, largely because the percentage of unknown words decreases while system performance on them increases (they become increasingly restricted as to word class),,
Table 6 contrasts our results with those from CITATION,,
While modern taggers may be more principled than the classic CLAWS tagger CITATION, they are in some respects inferior in their use of lexical information: CLAWS, through its IDIOMTAG module, categorically captured many important, correct taggings of frequent idiomatic word sequences,,
\x0cgests that the final accuracy number presented here could be slightly improved upon by classifier combination, it is worth noting that not only is this tagger better than any previous single tagger, but it also appears to outperform CITATION, the best-known combination tagger (they report an accuracy of 97.16% over the same WSJ data, but using a larger training set, which should favor them),,
These issues are further discussed in CITATION,,
The per-state models in this paper are log-linear models, building upon the models in CITATION and CITATION, though some models are in fact strictly simpler,,
Regardless of whether one is using HMMs, maximum entropy conditional sequence models, or other techniques like decision trees, most systems work in one direction through the sequence (normally left to right, but occasionally right to left, e.g., CITATION),,
3Despite use of names like label bias CITATION or observation bias, these effects are really just unwanted explaining-away effects (CITATION, 19), where two nodes which are not actually in causal competition have been modeled as if they were,,
Indeed, the result of CITATION t,,
Words surrounding the current word have been occasionally used in taggers, such as CITATION, Brills transformation based tagger CITATION, and the HMM model of CITATION, but nevertheless, the only lexicalization consistently included in tagging models is the dependence of the part of speech tag of a word on the word itself,,
Indeed, as for the voted perceptron of CITATION, we can get performance gains by reducing the support threshold for features to be included in the model,,
The usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in CITATION, and used in all the stochastic LFG work CITATION,,
3.3 Unknown word features Most of the models presented here use a set of unknown word features basically inherited from CITATION, which include using character n-gram prefixes and suffixes (for n up to 4), and detectors for a few other prominent features of words, such as capitalization, hyphens, and numbers,,
Indeed, the result of CITATION that including low support features helps a voted perceptron mode,,
For example, consider a case of observation bias CITATION for a first-order left-toright CMM,,
There are a few exceptions, such as Brills transformation-based learning CITATION, but most of the best known and most successful approaches of recent years have been unidirectional,,
Rather, it is a more general dependency network CITATION,,
This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in CITATION, using the same data splits, and a larger error reduction of 12.1% from the more similar ,,
Secondly, while all taggers use lexical information, and, indeed, it is well-known that lexical probabilities are much more revealing than tag sequence probabilities CITATION, most taggers make quite limited use of lexical probabilities (compared with, for example, the bilexical probabilities commonly used in current statistical parsers),,
We extracted tagged sentences from the parse trees.5 We split the data into training, development, and test sets as in CITATION,,
For example, CITATION attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization,,
ors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in CITATION, and used in all the stochastic LFG work CITATION,,
It is useful to contrast this framework with the conditional random fields of CITATION,,
Figure 3 gives pseudocode for the concrete case of the network in figure 1(d); the general case is similar, and is in fact just a max-plus version of standard inference algorithms for Bayes nets (CITATION, 97),,
CITATION raise this baseline to 92.34%, and with our sophisticated unknown word model, it gets even higher,,
Doing error analysis on unknown words on a simple tagging model (with ht0, t1i, ht0, t1, t2i, and hw0, t0i features) suggested several additional specialized features that can usefully improve 9CITATION use ht1, t0, w0i templates in their full-second order HMM, achieving an accuracy of 96.86%,,
This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in CITATION, using the same data splits, and a larger error reduction of 12.1% from the more similar best previous loglinear model in CITATION,,
High-performance taggers typically also include joint three-tag counts in some way, either as tag trigrams CITATION or tag-triple features (CITATION, CITATION),,
8CITATION noted that such a simple model got 90.25%, but this was with no unknown word model beyond a prior distribution over tags,,
The \x0cTagger Support cutoff Accuracy CITATION 0 96.60% 5 96.72% Model 3W+TAGS variant 1 96.97% 5 96.93% Table 6: Effect of changing common word feature cutoffs (features with support cutoff are excluded from the model),,
