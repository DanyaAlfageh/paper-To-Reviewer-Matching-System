8CITATION noted that such a simple model got 90.25%, but this was with no unknown word model beyond a prior distribution over tags,,
CITATION raise this baseline to 92.34%, and with our sophisticated unknown word model, it gets even higher,,
This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in CITATION, using the same data splits, and a larger error reduction of 12.1% from the more similar best previous loglinear model in CITATION,,
2 Bidirectional Dependency Networks When building probabilistic models for tag sequences, we often decompose the global probability of sequences using a directed graphical model (e.g., an HMM CITATION or a conditional Markov model (CMM) CITATION),,
CITATION illustrates very clearly how tagging performance increases as training set size grows, largely because the percentage of unknown words decreases while system performance on them increases (they become increasingly restricted as to word class),,
8CITATION noted that such a simple model got 90.25%, but this was with no unknown word model beyond a prior distri,,
High-performance taggers typically also include joint three-tag counts in some way, either as tag trigrams CITATION or tag-triple features (CITATION, CITATION),,
\x0cgests that the final accuracy number presented here could be slightly improved upon by classifier combination, it is worth noting that not only is this tagger better than any previous single tagger, but it also appears to outperform CITATION, the best-known combination tagger (they report an accuracy of 97.16% over the same WSJ data, but using a larger training set, which should favor them),,
Regardless of whether one is using HMMs, maximum entropy conditional sequence models, or other techniques like decision trees, most systems work in one direction through the sequence (normally left to right, but occasionally right to left, e.g., CITATION),,
There are a few exceptions, such as Brills transformation-based learning CITATION, but most of the best known and most successful approaches of recent years have been unidirectional,,
Words surrounding the current word have been occasionally used in taggers, such as CITATION, Brills transformation based tagger CITATION, and the HMM model of CITATION, but nevertheless, the only lexicalization consistently included in tagging models is the dependence of the part of speech tag of a word on the word itself,,
Secondly, while all taggers use lexical information, and, indeed, it is well-known that lexical probabilities are much more revealing than tag sequence probabilities CITATION, most taggers make quite limited use of lexical probabilities (compared with, for example, the bilexical probabilities commonly used in current statistical parsers),,
While modern taggers may be more principled than the classic CLAWS tagger CITATION, they are in some respects inferior in their use of lexical information: CLAWS, through its IDIOMTAG module, categorically captured many important, correct taggings of frequent idiomatic word sequences,,
CITATION illustrates very clearly how tagging performance increases as training set size grows, largely because the percentage of unknown words decreases while system performance on them increases (they become increasingly restricted as to word class),,
8CITATION noted that such a simple model got 90.25%, but this was with no unknown word model beyond a prior distribution over tags,,
CITATION raise this baseline to 92.34%, and with our sophisticated unknown word model, it gets even higher,,
The usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in CITATION, and used in all the stochastic LFG work CITATION,,
For example, CITATION attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization,,
At any rate, regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: CITATION, CITATION, and CITATION all present unregularized models,,
Indeed, the result of CITATION t,,
Regardless of whether one is using HMMs, maximum entropy conditional sequence models, or other techniques like decision trees, most systems work in one direction through the sequence (normally left to right, but occasionally right to left, e.g., CITATION),,
There are a few exceptions, such as Brills transformation-based learning CITATION, but most of the best known and most successful approaches of recent years have been unidirectional,,
Indeed, as for the voted perceptron of CITATION, we can get performance gains by reducing the support threshold for features to be included in the model,,
This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in CITATION, using the same data splits, and a larger error reduction of 12.1% from the more similar ,,
3 Experiments The part of speech tagged data used in our experiments is the Wall Street Journal data from Penn Treebank III CITATION,,
We extracted tagged sentences from the parse trees.5 We split the data into training, development, and test sets as in CITATION,,
The \x0cTagger Support cutoff Accuracy CITATION 0 96.60% 5 96.72% Model 3W+TAGS variant 1 96.97% 5 96.93% Table 6: Effect of changing common word feature cutoffs (features with support cutoff are excluded from the model),,
ot new to this work: Gaussian prior smoothing is advocated in CITATION, and used in all the stochastic LFG work CITATION,,
For example, CITATION attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization,,
At any rate, regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: CITATION, CITATION, and CITATION all present unregularized models,,
Indeed, the result of CITATION that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized,,
Whereas CITATION used feature support cutoffs and early stopping to stop overfitting of the model, and CITATION contends that including low support features harms a maximum entropy model, our results show that low support features are useful in a regularized maximum entropy model,,
Table 6 contrasts our results with those from CITATION,,
3Despite use of names like label bias CITATION or observation bias, these effects are really just unwanted explaining-away effects (CITATION, 19), where two nodes which are not actually in causal competition have been modeled as if they were,,
Figure 3 gives pseudocode for the concrete case of the network in figure 1(d); the general case is similar, and is in fact just a max-plus version of standard inference algorithms for Bayes nets (CITATION, 97),,
Rather, it is a more general dependency network CITATION,,
These issues are further discussed in CITATION,,
The usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in CITATION, and used in all the stochastic LFG work CITATION,,
For example, CITATION attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization,,
At any rate, regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: CITATION, CITATION, and CITATION all present unregularized models,,
Indeed, the result of CITATION that including low support features helps a voted perceptron mode,,
For example, consider a case of observation bias CITATION for a first-order left-toright CMM,,
3Despite use of names like label bias CITATION or observation bias, these effects are really just unwanted explaining-away effects (CITATION, 19), where two nodes which are not actually in causal competition have been modeled as if they were,,
It is useful to contrast this framework with the conditional random fields of CITATION,,
Words surrounding the current word have been occasionally used in taggers, such as CITATION, Brills transformation based tagger CITATION, and the HMM model of CITATION, but nevertheless, the only lexicalization consistently included in tagging models is the dependence of the part of speech tag of a word on the word itself,,
3 Experiments The part of speech tagged data used in our experiments is the Wall Street Journal data from Penn Treebank III CITATION,,
We extracted tagged sentences from the parse trees.5 We split the data into training, development, and test sets as in CITATION,,
Secondly, while all taggers use lexical information, and, indeed, it is well-known that lexical probabilities are much more revealing than tag sequence probabilities CITATION, most taggers make quite limited use of lexical probabilities (compared with, for example, the bilexical probabilities commonly used in current statistical parsers),,
While modern taggers may be more principled than the classic CLAWS tagger CITATION, they are in some respects inferior in their use of lexical information: CLAWS, through its IDIOMTAG module, categorically captured many important, correct taggings of frequent idiomatic word sequences,,
An interesting example of a common tagging error of the simpler models which could be corrected by a deterministic fixup rule of the kind used in the IDIOMTAG module of CITATION is the expression as X as (often, as far as),,
This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in CITATION, using the same data splits, and a larger error reduction of 12.1% from the more similar best previous loglinear model in CITATION,,
2 Bidirectional Dependency Networks When building probabilistic models for tag sequences, we often decompose the global probability of sequences using a directed graphical model (e.g., an HMM CITATION or a conditional Markov model (CMM) CITATION),,
The per-state models in this paper are log-linear models, building upon the models in CITATION and CITATION, though some models are in fact strictly simpler,,
High-performance taggers typically also include joint three-tag counts in some way, either as tag trigrams CITATION or tag-triple features (CITATION, CITATION),,
3.3 Unknown word features Most of the models presented here use a set of unknown word features basically inherited from CITATION, which include using character n-gram prefixes and suffixes (for n up to 4), and detectors for a few other prominent features of words, such as capitalization, hyphens, and numbers,,
Doing error analysis on unknown words on a simple tagging model (with ht0, t1i, ht0, t1, t2i, and hw0, t0i features) suggested several additional specialized features that can usefully improve 9CITATION use ht1, t0, w0i templates in their full-second order HMM, achieving an accuracy of 96.86%,,
he usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in CITATION, and used in all the stochastic LFG work CITATION,,
For example, CITATION attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization,,
At any rate, regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: CITATION, CITATION, and CITATION all present unregularized models,,
Indeed, the result of CITATION that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized,,
Whereas CITATION used feature support cutoffs and early stopping to stop overfitting of the model, and CITATION contends that including low support features harms a maximum entropy model, our results show that low support features are useful in a regularized maximum entropy model,,
Table 6 contrasts our results with those from CITATION,,
3.3 Unknown word features Most of the models presented here use a set of unknown word features basically inherited from CITATION, which include using character n-gram prefixes and suffixes (for n up to 4), and detectors for a few other prominent features of words, such as capitalization, hyphens, and numbers,,
Doing error analysis on unknown words on a simple tagging model (with ht0, t1i, ht0, t1, t2i, and hw0, t0i features) suggested several additional specialized features that can usefully improve 9CITATION use ht1, t0, w0i templates in their full-second order HMM, achieving an accuracy of 96.86%,,
This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in CITATION, using the same data splits, and a larger error reduction of 12.1% from the more similar best previous loglinear model in CITATION,,
2 Bidirectional Dependency Networks When building probabilistic models for tag sequences, we often decompose the global probability of sequences using a directed graphical model (e.g., an HMM CITATION or a conditional Markov model (CMM) CITATION),,
The per-state models in this paper are log-linear models, building upon the models in CITATION and CITATION, though some models are in fact strictly simpler,,
High-performance taggers typically also include joint three-tag counts in some way, either as tag trigrams CITATION or tag-triple features (CITATION, CITATION),,
ors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in CITATION, and used in all the stochastic LFG work CITATION,,
For example, CITATION attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization,,
At any rate, regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: CITATION, CITATION, and CITATION all present unregularized models,,
Indeed, the result of CITATION that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized,,
The usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in CITATION, and used in all the stochastic LFG work CITATION,,
For example, CITATION attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization,,
At any rate, regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: CITATION, CITATION, and CITATION all present unregularized models,,
Indeed, the result of CITATION that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized,,
