b3 CITATION: For each mention, form the intersection between the predicted cluster and the true cluster for that mention.,,
MUC CITATION: For each true cluster, compute the number of predicted clusters which need to be merged to cover the true cluster.,,
2.1 Data Sets In this work we use the following data sets: Development: (see Section 3) ACE2004-ROTH-DEV: Dev set split of the ACE 2004 training set utilized in CITATION.,,
Testing: (see Section 4) ACE2004-CULOTTA-TEST: Test set split of the ACE 2004 training set utilized in CITATION and CITATION.,,
Consists of 107 documents.2 ACE2004-NWIRE: ACE 2004 Newswire set to compare against CITATION.,,
rocedure with predicated and true clusters reversed.4 CEAF CITATION: For a similarity function between predicted and true clusters, CEAF scores the best match between true and predicted clusters using this function.,,
At a high level, our system resembles a pairwise coreference model (CITATION; CITATION; CITATION); for each mention mi, we select either a single-best antecedent amongst the previous mentions m1, .,,
4 The MUC measure is problematic when the system predicts many more clusters than actually exist (CITATION; CITATION); also, singleton clusters do not contribute to evaluation.,,
1153 \x0cWhile much research (CITATION; CITATION; Haghighi a,,
#1 (((((( ) ) ) ) ) ) subject of the [exhibition]2 (a) (b) Figure 3: NP structure annotation: In (a) we have the raw parse from the CITATION parser with the mentions annotated by entity.,,
The one exploited most in coreference work (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION) is the appositive construction.,,
.8 +SYN-CONSTR 78.3 70.5 74.2 84.0 71.0 76.9 71.3 45.4 55.5 70.8 70.8 70.8 +SEM-COMPAT 77.9 74.1 75.9 81.8 74.3 77.9 68.2 51.2 58.5 72.5 72.5 72.5 ACE2004-CULOTTA-TEST BASIC-FLAT 68.6 60.9 64.5 80.3 68.0 73.6 57.1 30.5 39.8 66.5 66.5 66.5 BASIC-TREE 71.2 63.2 67.0 81.6 69.3 75.0 60.1 34.5 43.9 67.9 67.9 67.9 +SYN-COMPAT 74.6 65.2 69.6 84.2 70.3 76.6 66.7 37.2 47.8 69.2 69.2 69.2 +SYN-CONSTR 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3 69.6 69.6 69.6 +SEM-COMPAT 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5 73.3 73.3 73.3 Supervised Results CITATION - - - 86.7 73.2 79.3 - - - - - - CITATION 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2 - - - MUC6-TEST +SEM-COMPAT 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3 72.0 72.0 72.0 Unsupervised Results CITATION 83.0 75.8 79.2 - - - 63.0 57.0 60.0 - - - Supervised Results CITATION 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 - - - ACE2004-NWIRE +SEM-COMPAT 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7 71.5 71.5 71.5 Unsupervised Results CITATION 71.3 70.5 70.9 - - - 62.6 38.9 48.0 - - - Table 2: Experimental Results (See Section 4): When comparisons between systems are presented, the largest r,,
We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from CITATION and CITATION.,,
Both of these systems were supervised systems discriminatively trained to maximize b3 and used features from many different structured resources including WordNet, as well as domain-specific features CITATION.,,
We should note that in our work we use neither the gold mention types (we do not model pre-nominals separately) nor do we use the gold NER tags which CITATION does.,,
forms both CITATION (an unsupervised Markov Logic Network system which uses explicit constraints) and CITATION (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures.11 Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of CITATION.,,
Overall, we conclude that our system outperforms state-of-the-art unsupervised systems12 and is in the range of the state-of-the art systems of CITATION and CITATION.,,
Testing: (see Section 4) ACE2004-CULOTTA-TEST: Test set split of the ACE 2004 training set utilized in CITATION and CITATION.,,
Consists of 107 documents.2 ACE2004-NWIRE: ACE 2004 Newswire set to compare against CITATION.,,
Unlabeled: (see Section 3.2) BLIPP: 1.8 million sentences of newswire parsed with the CITATION parser.,,
WIKI: 25k articles of English Wikipedia abstracts parsed by the CITATION parser.3 No labeled coreference data; used for mining semantic information.,,
Mention heads are determined by parsing the given mention span with the Stanford parser CITATION and using the Collins head rules CITATION; CITATION showed that using syntactic heads strongly outperformed a simple rightmost headword rule.,,
This choice reflects the intuition of CITATION that speakers only use pronominal mentions when there are not interveni,,
As these varied factors have given rise to a multitude of weak features, recent work has focused on how best to learn to combine them using models over reference structures (CITATION; CITATION; CITATION).,,
2.1 Data Sets In this work we use the following data sets: Development: (see Section 3) ACE2004-ROTH-DEV: Dev set split of the ACE 2004 training set utilized in CITATION.,,
Testing: (see Section 4) ACE2004-CULOTTA-TEST: Test set split of the ACE 2004 training set utilized in CITATION and CITATION.,,
Consists of 107 documents.2 ACE2004-NWIRE: ACE 2004 Newswire set to compare against CITATION.,,
Unlabeled: (see Section 3.2) BLIPP: 1.8 million sentences of newswire parsed with the CITATION parser.,,
WIKI: 25k articles of English Wikipedia abstracts parsed by the CITATION parser.3 No labeled,,
, 2002; CITATION); for each mention mi, we select either a single-best antecedent amongst the previous mentions m1, .,,
4 The MUC measure is problematic when the system predicts many more clusters than actually exist (CITATION; CITATION); also, singleton clusters do not contribute to evaluation.,,
1153 \x0cWhile much research (CITATION; CITATION; CITATION; CITATION; CITATION) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision (as in Ng and Cardie (2002) and CITATION) which can and does cause system errors.,,
& & \' \' \' NNP Pablo NNP Picasso , , NP-APPOS#1 (((((( ) ) ) ) ) ) subject of the [exhibition]2 (a) (b) Figure 3: NP structure annotation: In (a) we have the raw parse from the CITATION parser with the mentions annotated by entity.,,
The one exploited most in coreference work (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION) is the appositive construction.,,
.8 68.5 72.9 84.1 69.7 76.2 71.0 43.1 53.4 69.8 69.8 69.8 +SYN-CONSTR 78.3 70.5 74.2 84.0 71.0 76.9 71.3 45.4 55.5 70.8 70.8 70.8 +SEM-COMPAT 77.9 74.1 75.9 81.8 74.3 77.9 68.2 51.2 58.5 72.5 72.5 72.5 ACE2004-CULOTTA-TEST BASIC-FLAT 68.6 60.9 64.5 80.3 68.0 73.6 57.1 30.5 39.8 66.5 66.5 66.5 BASIC-TREE 71.2 63.2 67.0 81.6 69.3 75.0 60.1 34.5 43.9 67.9 67.9 67.9 +SYN-COMPAT 74.6 65.2 69.6 84.2 70.3 76.6 66.7 37.2 47.8 69.2 69.2 69.2 +SYN-CONSTR 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3 69.6 69.6 69.6 +SEM-COMPAT 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5 73.3 73.3 73.3 Supervised Results CITATION - - - 86.7 73.2 79.3 - - - - - - CITATION 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2 - - - MUC6-TEST +SEM-COMPAT 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3 72.0 72.0 72.0 Unsupervised Results CITATION 83.0 75.8 79.2 - - - 63.0 57.0 60.0 - - - Supervised Results CITATION 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 - - - ACE2004-NWIRE +SEM-COMPAT 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7 71.5 71.5 71.5 Unsupervised Results CITATION 71.3 70.5 70.9 - - - 62.6 38.9 48.0 - - - Table 2: Experimental Results (See Section 4): Whe,,
We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from CITATION and CITATION.,,
Both of these systems were supervised systems discriminatively trained to maximize b3 and used features from many different structured resources including WordNet, as well as domain-specific features CITATION.,,
We should note that in our work we use neither the gold mention types (we do not model pre-nominals separately) nor do we use the gold NER tags which CITATION does.,,
forms both CITATION (an unsupervised Markov Logic Network system which uses explicit constraints) and CITATION (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures.11 Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of CITATION.,,
Overall, we conclude that our system outperforms state-of-the-art unsupervised systems12 and is in the range of the state-of-the art systems of CITATION and CITATION.,,
As these varied factors have given rise to a multitude of weak features, recent work has focused on how best to learn to combine them using models over reference structures (CITATION; CITATION; CITATION).,,
At a high level, our system resembles a pairwise coreference model (CITATION; CITATION; CITATION); for each mention mi, we select either a single-best antecedent amongst the previous mentions m1, .,,
4 The MUC measure is problematic when the system predicts many more clusters than actually exist (CITATION; CITATION); also, singleton clusters do not contribute to evaluation.,,
1153 \x0cWhile much research (CITATION; CITATION; CITATION; CITATION; CITATION) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision (as in Ng and Cardie (2002) and CITATION) which can and does cause system errors.,,
81.6 69.3 75.0 60.1 34.5 43.9 67.9 67.9 67.9 +SYN-COMPAT 74.6 65.2 69.6 84.2 70.3 76.6 66.7 37.2 47.8 69.2 69.2 69.2 +SYN-CONSTR 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3 69.6 69.6 69.6 +SEM-COMPAT 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5 73.3 73.3 73.3 Supervised Results CITATION - - - 86.7 73.2 79.3 - - - - - - CITATION 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2 - - - MUC6-TEST +SEM-COMPAT 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3 72.0 72.0 72.0 Unsupervised Results CITATION 83.0 75.8 79.2 - - - 63.0 57.0 60.0 - - - Supervised Results CITATION 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 - - - ACE2004-NWIRE +SEM-COMPAT 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7 71.5 71.5 71.5 Unsupervised Results CITATION 71.3 70.5 70.9 - - - 62.6 38.9 48.0 - - - Table 2: Experimental Results (See Section 4): When comparisons between systems are presented, the largest result is bolded.,,
forms both CITATION (an unsupervised Markov Logic Network system which uses explicit constraints) and CITATION (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures.11 Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of CITATION.,,
Overall, we conclude that our system outperforms state-of-the-art unsupervised systems12 and is in the range of the state-of-the art systems of CITATION and CITATION.,,
Instead, we opt to utilize the Stanford NER tagger CITATION over the sentences in a document and annotate each NP with the NER label assigned to that mention head.,,
 Stanford parser CITATION and using the Collins head rules CITATION; CITATION showed that using syntactic heads strongly outperformed a simple rightmost headword rule.,,
This choice reflects the intuition of CITATION that speakers only use pronominal mentions when there are not intervening compatible S !!!!!!!,,
4 The MUC measure is problematic when the system predicts many more clusters than actually exist (CITATION; CITATION); also, singleton clusters do not contribute to evaluation.,,
1153 \x0cWhile much research (CITATION; CITATION; CITATION; CITATION; CITATION) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision (as in Ng and Cardie (2002) and CITATION) which can and does cause system errors.,,
For pronouns, this is used to mean that 11 CITATION took essentially the same approach but did so on non-comparable data.,,
12 CITATION outperformed CITATION.,,
Unfortunately, we cannot compare against CITATION since we do not have access to the version of the ACE data used in their evaluation.,,
Rather than opt to manually create a set of these coreference patterns as in CITATION, we instead opt to automatically extract these patterns from large corpora as in CITATION and CITATION.,,
For instance consider the example in Figure 1, the mention America is closest to its in flat mention distance, but syntactically Nintendo of America holds a more prominent syntactic position relative to the pronoun which, as CITATION argues, is key to discourse salience.,,
3.2 Semantic Knowledge While appositives and related syntactic constructions can resolve some cases of non-pronominal reference, most cases require semantic knowledge about the various entities as well as the verbs used in conjunction with those entities to disambiguate references CITATION.,,
aining set utilized in CITATION and CITATION.,,
Consists of 107 documents.2 ACE2004-NWIRE: ACE 2004 Newswire set to compare against CITATION.,,
Unlabeled: (see Section 3.2) BLIPP: 1.8 million sentences of newswire parsed with the CITATION parser.,,
WIKI: 25k articles of English Wikipedia abstracts parsed by the CITATION parser.3 No labeled coreference data; used for mining semantic information.,,
Mention heads are determined by parsing the given mention span with the Stanford parser CITATION and using the Collins head rules CITATION; CITATION showed that using syntactic heads strongly outperformed a simple rightmost headword rule.,,
This choice reflects the intuition of CITATION that speakers only use,,
&quot; &quot; # # # # # # # # # NP $$$$ % % % % NP-APPOS#1 NN painter NP-PERS & & & \' \' \' NNP Pablo NNP Picasso , , NP-APPOS#1 (((((( ) ) ) ) ) ) subject of the [exhibition]2 (a) (b) Figure 3: NP structure annotation: In (a) we have the raw parse from the CITATION parser with the mentions annotated by entity.,,
The one exploited most in coreference work (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION) is the appositive construction.,,
As these varied factors have given rise to a multitude of weak features, recent work has focused on how best to learn to combine them using models over reference structures (CITATION; CITATION; CITATION).,,
For pronouns, this is used to mean that 11 CITATION took essentially the same approach but did so on non-comparable data.,,
12 CITATION outperformed CITATION.,,
Unfortunately, we cannot compare against CITATION since we do not have access to the version of the ACE data used in their evaluation.,,
painter NP-PERS & & & \' \' \' NNP Pablo NNP Picasso , , NP-APPOS#1 (((((( ) ) ) ) ) ) subject of the [exhibition]2 (a) (b) Figure 3: NP structure annotation: In (a) we have the raw parse from the CITATION parser with the mentions annotated by entity.,,
The one exploited most in coreference work (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION) is the appositive construction.,,
MUC CITATION: For each true cluster, compute the number of predicted clusters which need to be merged to cover the true cluster.,,
Recall is given by the same procedure with predicated and true clusters reversed.4 CEAF CITATION: For a similarity function between predicted and true clusters, CEAF scores the best match between true and predicted clusters using this function.,,
At a high level, our system resembles a pairwise coreference model (CITATION; CITATION; CITATION); for each mention mi, we select either a sing,,
s given by the same procedure with predicated and true clusters reversed.4 CEAF CITATION: For a similarity function between predicted and true clusters, CEAF scores the best match between true and predicted clusters using this function.,,
At a high level, our system resembles a pairwise coreference model (CITATION; CITATION; CITATION); for each mention mi, we select either a single-best antecedent amongst the previous mentions m1, .,,
4 The MUC measure is problematic when the system predicts many more clusters than actually exist (CITATION; CITATION); also, singleton clusters do not contribute to evaluation.,,
1153 \x0cWhile much research (CITATION; Culott,,
 % % % NP-APPOS#1 NN painter NP-PERS & & & \' \' \' NNP Pablo NNP Picasso , , NP-APPOS#1 (((((( ) ) ) ) ) ) subject of the [exhibition]2 (a) (b) Figure 3: NP structure annotation: In (a) we have the raw parse from the CITATION parser with the mentions annotated by entity.,,
The one exploited most in coreference work (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION) is the appositive construction.,,
For pronouns, this is used to mean that 11 CITATION took essentially the same approach but did so on non-comparable data.,,
12 CITATION outperformed CITATION.,,
Unfortunately, we cannot compare against CITATION since we do not have access to the version of the ACE data used in their evaluation.,,
Rather than opt to manually create a set of these coreference patterns as in CITATION, we instead opt to automatically extract these patterns from large corpora as in CITATION and CITATION.,,
2.1 Data Sets In this work we use the following data sets: Development: (see Section 3) ACE2004-ROTH-DEV: Dev set split of the ACE 2004 training set utilized in CITATION.,,
Testing: (see Section 4) ACE2004-CULOTTA-TEST: Test set split of the ACE 2004 training set utilized in CITATION and CITATION.,,
Consists of 107 documents.2 ACE2004-NWIRE: ACE 2004 Newswire set to compare against CITATION.,,
Unlabeled: (see Section 3.2) BLIPP: 1.8 million sentences of newswire parsed with the CITATION parser.,,
WIKI: 25k articles of English Wikipedia abstracts parsed by the CITATION parser.3 No labeled coreference data; used for mining semantic information.,,
4 The MUC measure is problematic when the system predicts many more clusters than actually exist (CITATION; CITATION); also, singleton clusters do not contribute to evaluation.,,
1153 \x0cWhile much research (CITATION; CITATION; CITATION; CITATION; CITATION) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision (as in Ng and Cardie (2002) and CITATION) which can and does cause system errors.,,
Mention heads are determined by parsing the given mention span with the Stanford parser CITATION and using the Collins head rules CITATION; CITATION showed that using syntactic heads strongly outperformed a simple rightmost headword rule.,,
This choice reflects the intuition of CITATION that speakers only use pronominal mentions when there are not intervening compatible S !!!!!!!,,
 NNP Picasso , , NP-APPOS#1 (((((( ) ) ) ) ) ) subject of the [exhibition]2 (a) (b) Figure 3: NP structure annotation: In (a) we have the raw parse from the CITATION parser with the mentions annotated by entity.,,
The one exploited most in coreference work (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION) is the appositive construction.,,
Predicate Nominatives: Another syntactic constraint exploited in CITATION is the predicate nominative construction, where the object of a copular verb (forms of the verb be) is constrained to corefer with its subject (e.g.,,
68.6 60.9 64.5 80.3 68.0 73.6 57.1 30.5 39.8 66.5 66.5 66.5 BASIC-TREE 71.2 63.2 67.0 81.6 69.3 75.0 60.1 34.5 43.9 67.9 67.9 67.9 +SYN-COMPAT 74.6 65.2 69.6 84.2 70.3 76.6 66.7 37.2 47.8 69.2 69.2 69.2 +SYN-CONSTR 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3 69.6 69.6 69.6 +SEM-COMPAT 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5 73.3 73.3 73.3 Supervised Results CITATION - - - 86.7 73.2 79.3 - - - - - - CITATION 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2 - - - MUC6-TEST +SEM-COMPAT 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3 72.0 72.0 72.0 Unsupervised Results CITATION 83.0 75.8 79.2 - - - 63.0 57.0 60.0 - - - Supervised Results CITATION 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 - - - ACE2004-NWIRE +SEM-COMPAT 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7 71.5 71.5 71.5 Unsupervised Results CITATION 71.3 70.5 70.9 - - - 62.6 38.9 48.0 - - - Table 2: Experimental Results (See Section 4): When comparisons between systems are presented, the largest result is bolded.,,
gold NER tags which CITATION does.,,
forms both CITATION (an unsupervised Markov Logic Network system which uses explicit constraints) and CITATION (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures.11 Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of CITATION.,,
Overall, we conclude that our system outperforms state-of-the-art unsupervised systems12 and is in the range of the state-of-the art systems of CITATION and CITATION.,,
For pronouns, this is used to mean that 11 CITATION took essentially the same approach but did so on non-comparable data.,,
12 CITATION outperformed CITATION.,,
Unfortunately, we cannot compare against CITATION since we do not have access to the version of the ACE data used in their evaluation.,,
Rather than opt to manually create a set of these coreference patterns as in CITATION, we instead opt to automatically extract these patterns from large corpora as in CITATION and CITATION.,,
Recall is given by the same procedure with predicated and true clusters reversed.4 CEAF CITATION: For a similarity function between predicted and true clusters, CEAF scores the best match between true and predicted clusters using this function.,,
At a high level, our system resembles a pairwise coreference model (CITATION; CITATION; CITATION); for each mention mi, we select either a single-best antecedent amongst the previous mentions m1, .,,
4 The MUC measure is problematic when the system predicts many more clusters than actually exist (CITATION; CITATION); also, singleton clusters do not contribute to evaluation.,,
# # # # # NP $$$$ % % % % NP-APPOS#1 NN painter NP-PERS & & & \' \' \' NNP Pablo NNP Picasso , , NP-APPOS#1 (((((( ) ) ) ) ) ) subject of the [exhibition]2 (a) (b) Figure 3: NP structure annotation: In (a) we have the raw parse from the CITATION parser with the mentions annotated by entity.,,
The one exploited most in coreference work (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION) is the appositive construction.,,
b3 CITATION: For each mention, form the intersection between the predicted cluster and the true cluster for that mention.,,
MUC CITATION: For each true cluster, compute the number of predicted clusters which need to be merged to cover the true cluster.,,
Recall is given by the same procedure with predicated and true clusters reversed.4 CEAF CITATION: For a similarity function between predicted and true clusters, CEAF scores the best match between true and predicted clusters using this function.,,
