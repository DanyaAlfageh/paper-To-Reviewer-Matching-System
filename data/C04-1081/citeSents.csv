Many machine learning approaches have been proposed (CITATION; CITATION; CITATION),,
Unknown words cause segmentation errors in that these outof-vocabulary words in input text are often incorrectly segmented into single-character or other overly-short words CITATION,,
Segment confidence is estimated using constrained forward-backward CITATION,,
The standard forward-backward algorithm CITATION calculates Zx, the total likelihood of all label sequences y given a sequence x,,
|x), can be efficiently determined using the Viterbi algorithm CITATION,,
An N-best list of labeling sequences can also be obtained using modified Viterbi algorithm and A* search CITATION,,
Traditional maximum entropy learning algorithms, such as GIS and IIS (della CITATION), can be used to train CRFs,,
However, our implementation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (CITATION; CITATION),,
Machine learning approaches are more desirable and have been successful in both unsupervised learning CITATION and supervised learning CITATION,,
CITATION uses class-based language for word segmentation where some word category information can be incorporated,,
CITATION use a hierarchical hidden Markov Model to incorporate lexical knowledge,,
A recent advance in this area is CITATION, in which the author uses a sliding-window maximum entropy classifier to tag Chinese characters into one of four position tags, and then covert these tags into a segmentation using rules,,
However, a traditional maximum entropy tagger, as used in CITATION, labels charac,,
A recent advance in this area is CITATION, in which the author uses a sliding-window maximum entropy classifier to tag Chinese characters into one of four position tags, and then covert these tags into a segmentation using rules,,
However, a traditional maximum entropy tagger, as used in CITATION, labels characters without considering dependencies among the predicted segmentation labels that is inherent in the state transitions of finitestate sequence models,,
Linear-chain conditional random fields (CRFs) CITATION are models that address both issues above,,
ion CITATION has made comparisons easier,,
2 Conditional Random Fields Conditional random fields (CRFs) are undirected graphical models trained to maximize a conditional probability CITATION,,
h CITATION,,
Traditional maximum entropy learning algorithms, such as GIS and IIS (della CITATION), can be used to train CRFs,,
However, our implementation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (CITATION; CITATION),,
Many machine learning approaches have been proposed (CITATION; CITATION; CITATION),,
See CITATION for more details and further experiments,,
In dictionary-based methods, a predefined dictionary is used along with hand-generated rules for segmenting input sequence CITATION,,
Machine learning approaches are more desirable and have been successful in both unsupervised learning CITATION and supervised learning CITATION,,
CITATION uses class-based language for word segmentation where some word category information can be incorporated,,
CITATION us,,
The most probable label sequence for an input x, y = arg max y P(y|x), can be efficiently determined using the Viterbi algorithm CITATION,,
An N-best list of labeling sequences can also be obtained using modified Viterbi algorithm and A* search CITATION,,
Traditional maximum entropy learning algorithms, such as GIS and IIS (della CITATION), can be used to train CRFs,,
Segment confidence is estimated using constrained forward-backward CITATION,,
The standard forward-backward algorithm CITATION calculates Zx, the total likelihood of all label sequences y given a sequence x,,
The most probable label sequence for an input x, y = arg max y P(y|x), can be efficiently determined using the Viterbi algorithm CITATION,,
An N-best list of labeling sequences can also be obtained using modified Viterbi algorithm and A* search CITATION,,
Traditional maximum entropy learning algorithms, such as GIS and IIS (della CITATION), can be used to train CRFs,,
However, our implementation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (CITATION; Sha and Perei,,
Traditional maximum entropy learning algorithms, such as GIS and IIS (della CITATION), can be used to train CRFs,,
However, our implementation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (CITATION; CITATION),,
A recent Chinese word segmentation competition CITATION has made comparisons easier,,
2 Conditional Random Fields Conditional random fields (CRFs) are undirected graphical models trained to maximize a conditional probability CITATION,,
5 Experiments and Analysis To make a comprehensive evaluation, we use all four of the datasets from a recent Chinese word segmentation bake-off competition CITATION,,
Closed Precision Recall F1 Roov CTB 0.828 0.870 0.849 0.550 PK 0.935 0.947 0.941 0.660 HK 0.917 0.940 0.928 0.531 AS 0.950 0.962 0.956 0.292 Open Precision Recall F1 Roov CTB 0.889 0.898 0.894 0.619 PK 0.941 0.952 0.946 0.676 HK 0.944 0.948 0.946 0.629 AS 0.953 0.961 0.957 0.403 Table 3: Overall results of CRF segmentation on closed and open tests To compare our results against other systems, we summarize the competition results reported in CITATION in Table 4,,
S01 is one of the best segmentation systems in mainland China CITATION,,
This is due to significant inconsistent segmentation in training and testing CITATION,,
Third, consider a comparison of our results with site S12, who use a sliding-window maximum entropy model CITATION,,
For Chinese, there has been significant research on finding word boundaries in unsegmented sequences (see CITATION for a review),,
In dictionary-based methods, a predefined dictionary is used along with hand-generated rules for segmenting input sequence CITATION,,
In dictionary-based methods, a predefined dictionary is used along with hand-generated rules for segmenting input sequence CITATION,,
Machine learning approaches are more desirable and have been successful in both unsupervised learning CITATION and supervised learning CITATION,,
CITATION uses class-based language for word segmentation where some word category information can be incorporated,,
CITATION use a hierarchical hidden Markov Model to incorp,,
One solution to this problem could use a named entity extractor to recognize proper names; this was found to be very helpful in CITATION,,
In addition, compared to simple models like n-gram language models CITATION, another shortcoming of CRF-based segmenters is that it requires significantly longer training time,,
Many machine learning approaches have been proposed (CITATION; CITATION; CITATION),,
For Chinese, there has been significant research on finding word boundaries in unsegmented sequences (see CITATION for a review),,
In dictionary-based methods, a predefined dictionary is used along with hand-generated rules for segmenting input sequence CITATION,,
Machine learning approaches are more desirable and have been successful in both unsupervised learning CITATION and supervised learning CITATION,,
One solution to this problem could use a named entity extractor to recognize proper names; this was found to be very helpful in CITATION,,
In addition, compared to simple models like n-gram language models CITATION, another shortcoming of CRF-based segmenters is that it requires significantly longer training time,,
CITATION uses class-based language for word segmentation where some word category information can be incorporated,,
CITATION use a hierarchical hidden Markov Model to incorporate lexical knowledge,,
A recent advance in this area is CITATION, in which the author uses a sliding-window maximum entropy classifier to tag Chinese characters into one of four position tags, and then covert these tags into a segmentation using rules,,
However, a traditional maximum entropy tagger, as used in CITATION, labels characters without considering dependencies among the predicted segmentation labels that is inherent in the state transitions of finitestate sequence models,,
Linear-chain conditional random fields (CRFs) CITATION are models that addr,,
This is due to significant inconsistent segmentation in training and testing CITATION,,
Third, consider a comparison of our results with site S12, who use a sliding-window maximum entropy model CITATION,,
and Schuurmans, 2001) and supervised learning CITATION,,
CITATION uses class-based language for word segmentation where some word category information can be incorporated,,
CITATION use a hierarchical hidden Markov Model to incorporate lexical knowledge,,
A recent advance in this area is CITATION, in which the author uses a sliding-window maximum entropy classifier to tag Chinese characters into one of four position tags, and then covert these tags into a segmentation using rules,,
However, a traditional maximum entropy tagger, as used in CITATION, labels characters without considering dependencies among the predicted segmentation labels that is inherent in the state transitions of fi,,
S01 is one of the best segmentation systems in mainland China CITATION,,
This is due to significant inconsistent segmentation in training and testing CITATION,,
