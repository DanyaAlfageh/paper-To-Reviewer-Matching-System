<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<figure confidence="0.772749375">
b&apos;Robust German Noun Chunking
With a Probabilistic Context-Free Grammar
Helmut Schmid and Sabine Schulte im Walde\x03
Institut f\x7f
ur Maschinelle Sprachverarbeitung
Universit\x7f
at Stuttgart
Azenbergstra\x19e 12, 70174 Stuttgart, Germany
</figure>
<email confidence="0.629945">
fschmid,schulteg@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.945686" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998876357142857">
We present a noun chunker for German which is
based on a head-lexicalised probabilistic context-
free grammar. A manually developed grammar
was semi-automatically extended with robustness
rules in order to allow parsing of unrestricted text.
The model parameters were learned from unlabelled
training data by a probabilistic context-free parser.
For extracting noun chunks, the parser generates
all possible noun chunk analyses, scores them with
a novel algorithm which maximizes the best chunk
sequence criterion, and chooses the most probable
chunk sequence. An evaluation of the chunker on
2,140 hand-annotated noun chunks yielded 92% re-
call and 93% precision.
</bodyText>
<sectionHeader confidence="0.997718" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993064">
A noun chunker marks the noun chunks in a sen-
tence as in the following example:
</bodyText>
<figure confidence="0.987369071428571">
(Wirtschaftsbosse)
economy chefs
mit
with
(zweifelhaftem
doubtable
Ruf)
reputation
sind
are
an
in
(der
the
in
in
(Engp\x7f
assen)
bottlenecks
angewandten
applied
F\x7f
uhrung)
guidance
(des
of the
Landes)
country
</figure>
<figureCaption confidence="0.499468">
beteiligt.
</figureCaption>
<bodyText confidence="0.987450347826087">
involved.
`Leading economists with doubtable repu-
tations are involved in guiding the country
in times of bottlenecks.\&apos;
A tool which identi\x0ces noun chunks is useful for
term extraction (most technical terms are nouns or
complex noun groups), for lexicographic purposes
(see (Tapanainen and J\x7f
arvinen, 1998) on syntacti-
cally organised concordancing), and as index terms
for information retrieval. Chunkers may also mark
other types of chunks like verb groups, adverbial
phrases or adjectival phrases.
Several methods have been developed for noun
chunking. Church\&apos;s noun phrase tagger (Church,
1988), one of the \x0crst noun chunkers, was based on a
Hidden Markov Model (HMM) similar to those used
\x03 Thanks to Mats Rooth and Uli Heid for many helpful com-
ments.
for part-of-speech tagging. Another HMM-based ap-
proach has been developed by Mats Rooth (Rooth,
1992). It integrates two HMMs; one of them mod-
els noun chunks internally, the other models the
context of noun chunks. Abney\&apos;s cascaded \x0cnite-
state parser (Abney, 1996) also contains a process-
ing step which recognises noun chunks and other
types of chunks. Ramshaw and Marcus (Ramshaw
and Marcus, 1995) successfully applied Eric Brill\&apos;s
transformation-based learning method to the chunk-
ing problem. Voutilainen\&apos;s NPtool (Voutilainen,
1993) is based on his constraint-grammar system.
Finally, Brants (Brants, 1999) described a Ger-
man chunker which was implemented with cascaded
Markov Models.
In this paper, a probabilistic context-free parser
is applied to the noun chunking task. The Ger-
man grammar used in the experiments was semi-
automatically extended with robustness rules in or-
der to be able to process arbitrary input. The gram-
mar parameters were trained on unlabelled data. A
novel algorithm is used for noun chunk extraction.
It maximises the probability of the chunk set.
The following section introduces the grammar
framework, followed by a description of the chunk-
ing algorithm in section 3, and the experiments and
their evaluation in section 4.
</bodyText>
<sectionHeader confidence="0.98166" genericHeader="method">
2 The Grammar
</sectionHeader>
<bodyText confidence="0.999901">
The German grammar is a head-lexicalised proba-
bilistic context-free grammar. Section 2.1 de\x0cnes
probabilistic context-free grammars and their head-
lexicalised re\x0cnement. Section 2.2 introduces our
grammar architecture, focusing on noun chunks.
The robustness rules for the chunker are described
in section 2.3.
</bodyText>
<subsectionHeader confidence="0.846227">
2.1 (Head-Lexicalised) Probabilistic
Context-Free Grammars
</subsectionHeader>
<bodyText confidence="0.951476463414634">
A probabilistic context-free grammar (PCFG) is a
context-free grammar which assigns a probability P
to each context-free grammar rule in the rule set
R. The probability of a parse tree T is de\x0cned
as
Q
r2R P(r)jrj, where jrj is the number of times
rule r was applied to build T. The parameters of
\x0cPCFGs can be learned from unparsed corpora us-
ing the Inside-Outside algorithm (Lari and Young,
1990).
Head-lexicalised probabilistic context-free gram-
mars (H-L PCFG) (Carroll and Rooth, 1998) ex-
tend the PCFG approach by incorporating informa-
tion about the lexical head of constituents into the
probabilistic model.1
Each node in a parse of a H-
L PCFG is labelled with a category and the lexi-
cal head of the category. A H-L PCFG rule looks
like a PCFG rule in which one of the daughters
has been marked as the head. The rule probabili-
ties Prule(C ! \x0bjC) are replaced by lexicalised rule
probabilities Prule(C ! \x0bjC;h) where h is the lex-
ical head of the mother constituent C. The prob-
ability of a rule therefore depends not only on the
category of the mother node, but also on its lexi-
cal head. Assume that the grammar has two rules
VP ! V NP and VP ! V. Then the transitive verb
buy should have a higher probability for the for-
mer rule whereas the latter rule should be more
likely for intransitive verbs like sleep. H-L PCFGs
incorporate another type of parameters called lexi-
cal choice probabilities. The lexical choice probabil-
ity Pchoice(hdjCd;Cm;hm) represents the probability
that a node of category Cd with a mother node of
category Cm and lexical head hm bears the lexical
head hd. The probability of a parse tree is obtained
by multiplying lexicalised rule probabilities and lex-
ical choice probabilities for all nodes. Since it is
possible to transform H-L PCFGs into PCFGs, the
PCFG algorithms are applicable to H-L PCFGs.
</bodyText>
<subsectionHeader confidence="0.960118">
2.2 Noun Chunks in the German Grammar
</subsectionHeader>
<bodyText confidence="0.986560142857143">
Currently, the German grammarcontains 4,619rules
and covers 92% of our 15 million words of verb
\x0cnal and relative clauses2
. The structural noun
chunk concept in the grammar is de\x0cned accord-
ing to Abney\&apos;s chunk style (Abney, 1991) who de-
scribes chunks as syntactic units which correspond
in some way to prosodic patterns, containing a con-
tent word surrounded by some function word(s): all
words from the beginning of the noun phrase to the
head noun are included.3
The di\x0berent kinds of noun
chunks covered by our grammar are listed below and
illustrated with examples:
</bodyText>
<footnote confidence="0.942986333333333">
\x0f a combination of a non-obligatory determiner,
optional adjectives or cardinals and the noun
1Other types of lexicalised PCFGs have been described in
(Charniak, 1997), (Collins, 1997), (Goodman, 1997), (Chelba
and Jelinek, 1998) and (Eisner and Satta, 1999).
2The restricted corpora were extracted automatically from
the Huge German Corpus (HGC), a collection of German
newspapers as well as specialised magazines for industry, law,
computer science.
3As you will see below, there is one exception, noun chunks
re\x0cned by a proper name, which end with the name instead
of the head noun.
</footnote>
<figure confidence="0.972969">
itself:
(1) eine
a
gute
good
Idee
idea
(2) vielen
for many
Menschen
people
(3) deren
whose
k\x7f
unstliche
arti\x0ccial
Stimme
voice
(4) elf
eleven
Ladungen
cargos
(5) Wasser
water
</figure>
<bodyText confidence="0.920794">
and prepositional phrases where the de\x0cnite ar-
ticle of the embedded noun chunk is morpholog-
ically combined with a preposition, so the pure
noun chunk could not be separated:
</bodyText>
<figure confidence="0.971725166666667">
(6) zum
at the
Schluss
end
\x0f personal pronouns: ich (I), mir (me)
\x0f re
exive pronouns: mich (myself), sich (him-
self/herself/itself)
\x0f possessive pronouns:
(7) Meins
Mine
ist
is
sauber.
clean.
\x0f demonstrative pronouns:
(8) Jener
That one
f\x7f
ahrt
goes
viel
much
schneller.
faster.
\x0f inde\x0cnite pronouns:
(9) Einige
Some
sind durchgefallen.
failed.
\x0f relative pronouns:
(10) Ich
I
mag
like
Menschen,
people
die
who
ehrlich sind.
are honest.
\x0f nominalised adjectives: Wichtigem (important
things)
\x0f proper names: Christoph Kolumbus
\x0f a noun chunk re\x0cned by a proper name:
(11) der
the
Eroberer
conquerer
Christoph
Christoph
Kolumbus
Columbus
\x0f cardinals indicating a year:
(12) Ich
I
begann
started
1996.
1996.
</figure>
<bodyText confidence="0.946680666666667">
The chunks may be recursive in case they appear as
complement of an adjectival phrase, as in (der (im
Regen) wartende Sohn) (the son who was waiting in
the rain).
Noun chunks have features for case, without fur-
ther agreement features for nouns and verbs. The
case is constrained by the function of the noun
chunk, as verbal or adjectival complement with nom-
inative, accusative, dative or genitive case, as mod-
i\x0cer with genitive case, or as part of a prepositional
\x0cphrase (also in the special case representing a prepo-
sitional phrase itself) with accusative or dative case.
Both structure and case of noun phrases may be
ambiguous and have to be disambiguated:
\x0f ambiguity concerning structure:
diesen (this) is disregarding the context a
demonstrativepronoun ambiguousbetween rep-
resenting a standalone noun chunk (cf. example
(8)) or a determiner within a noun chunk (cf.
example (2))
\x0f ambiguity concerning case:
die Beitr\x7f
age (the contributions) is disregarding
the context ambiguous between nominative and
accusative case
The disambiguation is learned during grammar
training, since the lexicalised rule probabilities as
well as the lexical choice probabilities tend to enforce
the correct structure and case information. Con-
sidering the above examples, the trained grammar
should be able to parse diesen Krieg (this war) as
one noun chunk instead of two (with diesen repre-
senting a standalone noun chunk) because of (i) the
preferred use of demonstrative pronouns as deter-
miners (! lexicalised rule probabilities), and (ii) the
lexical coherence between the two words (! lexical
choice probabilities); in a sentence like er zahlte die
Beitr\x7f
age (he paid the contributions) the accusative
case of the latter noun chunk should be identi\x0ced
because of the lexical coherence between the verb
zahlen (pay) and the lexical head of the subcate-
gorised noun phrase Beitrag (contribution) as re-
lated direct object head (! lexical choice probabil-
ities).
</bodyText>
<subsectionHeader confidence="0.998215">
2.3 Robustness Rules
</subsectionHeader>
<bodyText confidence="0.990738416666667">
The German grammar coversover 90% of the clauses
of our verb \x0cnal and relative clause corpora. This
is su\x0ecient for the extraction of lexical information,
e.g. the subcategorisation of verbs (see (Beil et al.,
1999)). For chunking, however, it is usually neces-
sary to analyse all sentences. Therefore, the gram-
mar was augmented with a set of robustness rules.
Three types of robustness rules have been consid-
ered, namely unigram rules, bigram rules and tri-
gram rules.
Unigram rules are rules of the form X ! YP X,
where YP is a grammatical category and X is a new
category. If such a rule is added for each grammar
category4
, the coverage is 100% because the gram-
mar is then able to generateany sequence of category
labels. In practice, some of the rules can be omitted
while still retaining full coverage: e.g. the rule X !
4Also needed are two rules which start and terminate the
\\X chain&amp;quot;. We used the rules TOP ! START X and X ! END.
START and END expand to SGML tags which mark the begin-
ning and the end of a sentence, respectively.
ADV X is not necessary if the grammar already con-
tains the rules ADVP ! ADV and X ! ADVP X. Uni-
gram rules are insensitive to their context so that all
permutations of the categories which are generated
by the X chain have the same probability.
The second type of robustnessrules, called trigram
rules (Carroll and Rooth, 1998) is more context sen-
sitive. Trigram rules have the form X:Y ! Y Y:Z
where X, Y, Z are categories and X:Y and Y:Z are
new categories. Trigram rules choose the next cat-
egory on the basis of the two preceding categories.
Therefore the number of rules grows as the number
of categories raises to the third power. For exam-
ple, 125,000 trigram rules are needed to generate 50
di\x0berent categories in arbitrary order.
Since unigram rules are context insensitive and
trigram rules are too numerous, a third type of ro-
bustness rules, called bigram rules, was developed.
A bigram rule actually consists of two rules, a rule
of the form :Y ! Y Y: which generates the con-
stituent Y deterministically, and a rule Y: ! :Z
which selects the next constituent Z based on the
current one. Given n categories, we obtain n rules
of the \x0crst form and n2
rules of the second form.
Even when categories which directly project to some
other category were omitted in the generation of the
bigram rules for our German grammar, the num-
ber of rules was still fairly large. Hence we gener-
alised some of the grammatical categories by adding
additional chain rules. For example, the preposi-
tional phrase categories PP.Akk:an, PP.Akk:auf,
PP.Akk:gegen etc. were generalised to PPX by
adding the rules PPX ! PP.Akk:an etc. Instead of
n + 1 bigram rules for each of the 23 prepositional
categories, we now obtained only n + 2 rules with
the new category PPX. Altogether, 3,332 robustness
rules were added.
</bodyText>
<sectionHeader confidence="0.9963" genericHeader="method">
3 Chunking
</sectionHeader>
<bodyText confidence="0.9980255">
A head-lexicalised probabilistic context-free parser,
called LoPar (Schmid, 1999), was used for pars-
ing. The functionality of LoPar encompasses purely
symbolic parsing as well as Viterbi parsing, inside-
outside computation, POS tagging, chunking and
training with PCFGs as well as H-L PCFGs. Be-
cause of the large number of parameters in partic-
ular of H-L PCFGs, the parser smoothes the prob-
ability distributions in order to avoid zero proba-
bilities. The absolute discounting method (Ney et
al., 1994) was adapted to fractional counts for this
purpose. LoPar also supports lemmatisation of the
lexical heads of a H-L PCFG. The input to the
parser consists of ambiguously tagged words. The
tags are provided by a German morphological anal-
yser (Schiller and St\x7f
ockert, 1995).
The best chunk set of a sentence is de\x0cned as the
set of chunks (with category, start and end position)
\x0cfor which the sum of the probabilities of all parses
which contain exactly that chunk set is maximal.
The chunk set of the most likely parse (i.e. Viterbi
parse) is not necessarily the best chunk set according
to this de\x0cnition, as the following PCFG shows.
</bodyText>
<table confidence="0.51999425">
S ! A 0.6 B ! x 1.0
S ! B 0.4 C ! x 1.0
A ! C 0.5 D ! x 1.0
A ! D 0.5
</table>
<bodyText confidence="0.8629689">
This grammar generates the three parse trees (S
(A (C x))), (S (A (D x))), and (S (B x)). The
parse tree probabilities are 0.3, 0.3 and 0.4, respec-
tively. The last parse is therefore the Viterbi parse of
x. Now assume that fA,Bg is the set of chunk cate-
gories. The most likely chunk set is then f(A,0,1)g
because the sum of the probabilities of all parses
which contain A is 0.6, whereas the sum over the
probabilities of all parses containing B is only 0.4.
computeChunks is a slightly simpli\x0ced pseudo-
code version of the actual chunking algorithm:
computeChunks(G, prule)
Initialize
oat array p[GV ]
Initialize chunk set array chunks[GV ]
for each vertex v in GV in bottom-up order do
if v is an or-node then
Initialize
oat array prob[chunks[d(v)]] to 0
for each daughter u 2 d(v) do
</bodyText>
<figure confidence="0.8695013">
prob[chunks[u]] prob[chunks[u]]+ p[u]
chunks[v] argmaxc prob[c]
p[v] prob[chunks[v]]
else
p[v] prule[rule(v)]
Q
u2d(v) p[u]
chunks[v]
S
u2d(v) chunks[u]
</figure>
<bodyText confidence="0.939571769230769">
if v is labelled with a chunk category C then
chunks[v] chunks[v] [ f(C;start(v);end(v))g
return chunks[root(G)]
computeChunks takes two arguments. The \x0crst ar-
gument is a parse forest G which is represented as an
and-or-graph. GV is the set of vertices. The second
argument is the rule probability vector. d is a func-
tion which returns the daughters of a vertex. The al-
gorithm computes the best chunk set chunks[v] and
the corresponding probability p[v] for all vertices v
in bottom-up order. chunks[d(v)] returns the set of
chunk sets of the daughter nodes of vertex v. rule(v)
returns the rule which created v and is only de\x0cned
for and-nodes. start(v) and end(v) return the start
and end position of the constituent represented by
v.
The chunking algorithm was experimentally com-
pared with chunk extraction from Viterbi parses. In
35 out of 41 evaluation runs with di\x0berent parame-
ter settings5
, the f-score of the chunking algorithm
5The runs di\x0bered wrt. training strategy and number of
iterations. See section 4 for details.
was better than that of the Viterbi algorithm. The
average f-score of the chunking algorithm was 84.7 %
compared to 84.0 % for the Viterbi algorithm.
</bodyText>
<sectionHeader confidence="0.999349" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999157">
We performed two main chunking experiments. Ini-
tially, the parser trained the chunk grammar based
on the restricted grammar described in section 2 ac-
cording to four di\x0berent training strategies. A pre-
ferred training strategy was then applied to inves-
tigate the potential of grammar re\x0cnement and ex-
tended training data.
</bodyText>
<subsectionHeader confidence="0.998033">
4.1 Training
</subsectionHeader>
<bodyText confidence="0.998137983050847">
In the \x0crst experiment, the chunker version of the
grammar was trained on a corpus comprising a 1
million word subcorpus of relative clauses, a 1 mil-
lion word subcorpus of verb \x0cnal clauses and 2 mil-
lion words of consecutive text. All data had been
extracted from the Huge German Corpus. The test
data used for the later evaluation was not included
in the training corpus.
For training strategy 1, the chunker grammar was
\x0crst trained on the whole corpus in unlexicalised
mode, i.e. like a PCFG. The parameters were rees-
timated once in the middle and once at the end of
the corpus. In the next step, the grammar was lexi-
calised, i.e. the parser computed the parse probabil-
ities with the unlexicalised model, but extracted fre-
quencies for the lexicalised model. These frequencies
were summed over the whole corpus. Three more
iterations on the whole corpus followed in which
the parameters of the lexicalised model were rees-
timated.
The parametersof the unlexicalisedchunker gram-
mar were initialised in the following way: a fre-
quency of 7500 was assigned to all original grammar
rules and 0 to the majority of robustness rules. The
parameters were then estimated on the basis of these
frequencies. Because of the smoothing, the proba-
bilities of the robustness rules were small but not
zero.
For training strategy 2, the chunker rules were
initialised with frequencies from a grammar without
robustness rule extensions, which had been trained
unlexicalised on a 4 million subcorpus of verb \x0cnal
clauses and a 4 million word subcorpus of relative
clauses.
Training strategy 3 again set the frequency of the
original rules to 7500 and of the robustness rules to
0. The parser trained with three unlexicalised iter-
ations over the whole training corpus, reestimating
the parameters only at the end of the corpus, in or-
der to \x0cnd out whether the lexicalised probabilistic
parser had been better than the fully trained unlexi-
calised parser on the task of chunk parsing. Training
strategy 4 repeated this procedure, but with initial-
\x0cising the chunker frequencies on basis of a trained
grammar.
For each training strategy, further iterations were
added until the precision and recall values ceased to
improve.
For the second part of the experiments, the base
grammar was extended with a few simple verb-\x0crst
and verb-second clause rules. Strategy 4 was applied
for training the chunker
(A) on the same training corpus as before, i.e. 2
million words of relative and verb \x0cnal clauses,
and 2 million words of unrestricted corpus data
from the HGC,
(B) on a training corpus consisting of 10 million
words of unrestricted corpus data from the
HGC.
</bodyText>
<subsectionHeader confidence="0.988746">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.995021695652174">
The evaluation of the chunker was carried out
on noun chunks from 378 unrestricted sentences
from the German newspaper Frankfurter Allgemeine
Zeitung (FAZ). Two persons independently anno-
tated all noun chunks in the corpus {a total of 2,140
noun chunks{, according to the noun chunk de\x0c-
nition in section 2.2, without considering grammar
coverage, i.e. noun chunks not actually covered by
the grammar (e.g. noun chunk ellipsis such as die
kleinen [ ]N) were annotated as such. As labels, we
used the identi\x0cer NC plus caseinformation: NC.Nom,
NC.Acc, NC.Dat, NC.Gen. In addition, we included
identi\x0cers for prepositional phrases where the prepo-
sition is morphologically merged with the de\x0cnite
article, (cf. example (6)), also including case infor-
mation: PNC.Acc, PNC.Dat.
For each training strategy described in section 4.1
we evaluated the chunker before the training process
and after each training iteration: the model in its
current training state parsed the test sentences and
extracted the most probable chunk sequence as de-
\x0cned in section 3. We then compared the extracted
noun chunks with the hand-annotated data, accord-
</bodyText>
<footnote confidence="0.61157425">
ing to
\x0f the range of the chunks, i.e. did the chunker
\x0cnd a chunk at all?
\x0f the range and the identi\x0cer of the chunks, i.e.
</footnote>
<bodyText confidence="0.9630957">
did the chunker \x0cnd a chunk and identify the
correct syntactic category and case?
Figures 1 and 2 display the results of the eval-
uation in the \x0crst experiment,6
according to noun
chunk range only and according to noun chunk
range, syntactic category and case, respectively.
Bold font highlights the best versions.
Training strategy 2 with two iterations of lexi-
calised training produced the best f-scores for noun
</bodyText>
<footnote confidence="0.903033666666667">
6The lexicalised chunker versions obtained by strategy 2
were also utilised for parsing the test sentences unlexicalised.
chunk boundary recognition if unlexicalised parsing
</footnote>
<bodyText confidence="0.875443555555556">
was done. The respective precision and recall val-
ues were 93.06% and 92.19%. For recognising noun
chunks with range, category and case, the best chun-
ker version was created by training strategy 4, after
\x0cve iterations of unlexicalised training; precision and
recall values were 79.28% and 76.75%, respectively.
From the experimental results, we can conclude
that:
1. initialisation of the chunker grammar frequen-
cies on the basis of a trained grammar improves
the untrained version of the chunker, but the
di\x0berence vanishes in the training process
2. unlexicalised parsingis su\x0ecient for noun chunk
extraction; for extraction of chunks with case
information, unlexicalised training turned out
to be even more successful than a combination
with lexicalised training
Figures 3 and 4 display the results of the evalu-
ation concerning the second experiment, compared
to the initial values from the \x0crst experiment.
Extending the base grammar and the training cor-
pus slightly increased precision and recall values for
recognising noun chunks according to range only.
The main improvement was in noun chunk recogni-
tion according to range, category and case: precision
and recall values increased to 83.88% and 83.21%,
respectively.
</bodyText>
<subsectionHeader confidence="0.997282">
4.3 Failure Analysis
</subsectionHeader>
<bodyText confidence="0.998678838709677">
A comparison of the parsed noun chunks with the
annotated data showed that failure in detecting a
noun chunk was mainly caused by proper names,
for example Netanjahu, abbreviations like OSZE, or
composita like South China Morning Post. The di-
versity of proper names makes it di\x0ecult for the
chunker to learn them properly. On the one hand,
the lexical information for proper names is unreliable
because many proper names were not recognised as
such. On the other hand, most proper names are
too rare to learn reliable statistics for them.
Minor mistakes were caused by (a) articles which
are morphologically identical to noun chunks con-
sisting of a pronoun, for example den Rentnern (the
pensionersdat) was analysed as two noun chunks, den
(demonstrative pronoun) and Rentnern, (b) capital
letter confusion: since German nouns typically start
with capital letters, sentence beginnings are wrongly
interpreted as nouns, for example W\x7f
urden as the
conditional of the auxiliary werden (to become) is
interpreted as the dative case of W\x7f
urde (dignity), (c)
noun chunk internal punctuation as in seine ` Part-
ner \&apos; (his ` partners \&apos;).
Failure in assigning the correct syntactic cate-
gory and case to a noun chunk was mainly caused
by (a) assigning accusative case instead of nomina-
tive case, and (b) assigning dative case or nomina-
\x0cStrategy 1 Strategy 2 {parsed unlex{ Strategy 3 Strategy 4
prec rec prec rec prec rec prec rec prec rec
</bodyText>
<table confidence="0.9932256">
untrained 83.63% 83.63% 90.22% 90.18% 90.22% 90.18% 83.63% 83.63% 90.22% 90.18%
unlex1 91.33% 89.62% 92.84% 91.58% 92.84% 91.58% 91.33% 89.62% 92.84% 91.58%
unlex2 92.55% 90.04% 93.01% 91.49%
unlex3 92.78% 90.22% 92.95% 91.25%
unlex4 93.09% 90.79%
unlex5 93.29% 90.32%
lex0 89.13% 89.71% 90.12% 90.41% 93.01% 91.49%
lex1 88.37% 89.52% 88.97% 89.76% 93.02% 91.67%
lex2 88.25% 89.57% 89.79% 90.46% 93.06% 92.19%
lex3 88.17% 89.62% 89.42% 90.13% 93.05% 92.05%
</table>
<figureCaption confidence="0.933059">
Figure 1: Comparing training strategies: noun chunk evaluation according to range only
</figureCaption>
<bodyText confidence="0.8168185">
Strategy 1 Strategy 2 {parsed unlex{ Strategy 3 Strategy 4
prec rec prec rec prec rec prec rec prec rec
</bodyText>
<table confidence="0.9915048">
untrained 63.52% 63.52% 72.02% 71.98% 72.02% 71.98% 63.52% 63.52% 72.02% 71.98%
unlex1 74.50% 73.11% 75.87% 74.84% 75.87% 74.84% 74.50% 73.11% 75.87% 74.84%
unlex2 76.88% 74.79% 78.27% 76.99%
unlex3 77.97% 75.82% 78.70% 77.27%
unlex4 78.80% 76.85%
unlex5 79.28% 76.75%
lex0 73.68% 73.15% 75.10% 75.35% 78.27% 76.99%
lex1 72.02% 72.97% 74.69% 75.35% 77.27% 76.15%
lex2 72.76% 73.85% 75.26% 75.82% 77.48% 76.75%
lex3 71.97% 73.15% 75.03% 75.63% 77.45% 76.61%
</table>
<figureCaption confidence="0.986301">
Figure 2: Comparing training strategies: noun chunk evaluation according to range and label
</figureCaption>
<bodyText confidence="0.99286775">
tive case instead of accusative case. The confusion
between nominative and accusative case is due to
the fact that both cases are expressed by identical
morphology in the feminine and neutral genders in
German. The morphologic similarity between ac-
cusative and dative is less substantial, but especially
proper names and bare nouns are still subject to con-
fusion. As the evaluation results show, the distinc-
tion between the cases could be learned in general,
but morphological similarity and in addition the rel-
atively free word order in German impose high de-
mands on the necessary probability model.
</bodyText>
<sectionHeader confidence="0.998619" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.999820866666667">
We presented a German noun chunker for unre-
stricted text. The chunker is based on a head-
lexicalised probabilistic context-free grammar and
trained on unlabelled data. The base grammar was
semi-automatically augmented with robustness rules
in order to cover unrestricted input. An algorithm
for chunk extraction was developed which maximises
the probability of the chunk sets rather than the
probability of single parses like the Viterbi algo-
rithm.
German noun chunks were detected with 93% pre-
cision and 92% recall. Asking the chunker to addi-
tionally identify the syntactic category and the case
of the chunks resulted in recall of 83% and precision
of 84%. A comparison of di\x0berent training strate-
gies showed that unlexicalised parsing information
was su\x0ecient for noun chunk extraction with and
without case information. The base grammar played
an important role in the chunker development: (i)
building the chunker on the basis of an already
trained grammar improved the chunker rules, and
(ii) re\x0cning the base grammarwith even simple verb-
\x0crst and verb-second rules improved accuracy, so it
should be worthwhile to further extend the grammar
rules. Increasing the amount of training data also
improved noun chunk recognition, especially case
disambiguation. Better heuristics for guessing the
parts-of-speech of unknown words should further im-
prove the noun chunk recognition, since many errors
were caused by unknown words.
</bodyText>
<sectionHeader confidence="0.986196" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994261333333333">
Steven Abney. 1991. Parsing by Chunks. In Robert
Berwick, Steven Abney, and Carol Tenny, editors,
Principle-Based Parsing. Kluwer Academic Pub-
lishers.
Steven Abney. 1996. Partial Parsing via Finite-
State Cascades. In Proceedings of the ESSLLI \&apos;96
Robust Parsing Workshop.
Franz Beil, Glenn Carroll, Detlef Prescher, Stefan
Riezler, and Mats Rooth. 1999. Inside-Outside
Estimation of a Lexicalized PCFG for German.
In Proceedings of the 37th Annual Meeting of the
ACL, pages 269{276.
</reference>
<table confidence="0.901843260869565">
\x0cStrategy 4
Initial A B
prec rec prec rec prec rec
untrained 90.22% 90.18% 90.43% 90.60% 90.43% 90.60%
unlex1 92.84% 91.58% 91.65% 91.35% 91.52% 91.35%
unlex2 93.01% 91.49% 92.45% 91.58% 91.89% 91.67%
unlex3 92.95% 91.25% 92.64% 91.21% 92.21% 91.86%
unlex4 93.09% 90.79% 93.11% 91.07% 92.73% 91.86%
unlex5 93.29% 90.32% 93.20% 91.02% 92.73% 91.86%
unlex6 92.91% 91.96%
unlex7 92.83% 92.10%
Figure 3: Grammar and training data extensions: noun chunk evaluation according to range only
Strategy 4
Initial A B
prec rec prec rec prec rec
untrained 72.02% 71.98% 74.42% 74.56% 74.42% 74.56%
unlex1 75.87% 74.84% 78.51% 78.25% 77.60% 77.46%
unlex2 78.27% 76.99% 80.74% 79.98% 79.89% 79.70%
unlex3 78.70% 77.27% 81.24% 79.98% 81.17% 80.87%
unlex4 78.80% 76.85% 81.83% 80.03% 82.44% 81.67%
unlex5 79.28% 76.75% 81.85% 79.93% 82.53% 81.76%
unlex6 82.94% 82.09%
unlex7 83.88% 83.21%
</table>
<figureCaption confidence="0.971879">
Figure 4: Grammar and training data extensions: noun chunk evaluation according to range and label
</figureCaption>
<reference confidence="0.987537397058824">
Thorsten Brants. 1999. Cascaded markov models.
In Proceedings of EACL\&apos;99.
Glenn Carroll and Mats Rooth. 1998. Valence In-
duction with a Head-Lexicalized PCFG. In Pro-
ceedings of Third Conference on Empirical Meth-
ods in Natural Language Processing.
Eugene Charniak. 1997. Statistical Parsing with a
Context-Free Grammar and Word Statistics. In
Proceedings of the 14th National Conference on
Arti\x0ccial Intelligence.
Ciprian Chelba and Frederick Jelinek. 1998. Ex-
ploiting Syntactic Structure for Language Mod-
eling. In Proceedings of the 36th Annual Meeting
of the ACL.
Kenneth W. Church. 1988. A Stochastic Parts Pro-
gram and Noun Phrase Parser for unrestricted
Text. In Proceedings of the Second Conference on
Applied Natural Language Processing, pages 136{
143.
Michael Collins. 1997. Three Generative, Lexi-
calised Models for Statistical Parsing. In Proceed-
ings of the 35th Annual Meeting of the ACL.
Jason Eisner and Giorgio Satta. 1999. E\x0ecient
Parsing for Bilexical Context-Free Grammars and
Head Automaton Grammars. In Proceedings of
the 37th Annual Meeting of the ACL, pages 457{
464.
Joshua Goodman. 1996. Parsing Algorithms and
Metrics. In Proceedings of the 34th Annual Meet-
ing of the ACL, pages 177{183.
Joshua Goodman. 1997. Probabilistic Feature
Grammars. In Proceedings of the 5th Interna-
tional Workshop on Parsing Technologies, pages
89{100.
K. Lari and S. Young. 1990. The Estimation
of Stochastic Context-Free Grammars using the
Inside-Outside Algorithm. Computation Speech
and Language Processing, 4:35{56.
Hermann Ney, U. Essen, and R. Kneser. 1994.
On Structuring Probabilistic Dependencies in
Stochastic Language Modelling. Computer Speech
and Language, 8:1{38.
L. Ramshaw and M. Marcus. 1995. Text Chunking
using Transformation-Based Learning. In Pro-
ceedings of the Third Workshop on Very Large
Corpora, pages 82{94.
Mats Rooth. 1992. Statistical NP Tagging. Unpub-
lished manuscript.
Anne Schiller and Chris St\x7f
ockert, 1995. DMOR.
Institut f\x7f
ur Maschinelle Sprachverarbeitung, Uni-
versit\x7f
at Stuttgart.
Helmut Schmid. 1999. Lopar: Design and Im-
plementation. Technical report, Institut f\x7f
ur
Maschinelle Sprachverarbeitung, Universit\x7f
at
Stuttgart.
Pasi Tapanainen and Timo J\x7f
arvinen. 1998. De-
pendency Concordances. International Journal of
Lexicography, 11(3):187{203.
Atro Voutilainen. 1993. NPtool, a Detector of En-
glish Noun Phrases. In Proceedings of the Work-
shop on Very Large Corpora, pages 48{57.
\x0c&apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.132093">
<title confidence="0.9733095">b&apos;Robust German Noun Chunking With a Probabilistic Context-Free Grammar</title>
<author confidence="0.272215">Helmut Schmid</author>
<author confidence="0.272215">Sabine Schulte im Walde\x</author>
<affiliation confidence="0.40879975">Institut f\x7f ur Maschinelle Sprachverarbeitung Universit\x7f at Stuttgart</affiliation>
<address confidence="0.821357">Azenbergstra\x19e 12, 70174 Stuttgart, Germany</address>
<email confidence="0.999421">fschmid,schulteg@ims.uni-stuttgart.de</email>
<abstract confidence="0.994103666666666">We present a noun chunker for German which is based on a head-lexicalised probabilistic contextfree grammar. A manually developed grammar was semi-automatically extended with robustness rules in order to allow parsing of unrestricted text. The model parameters were learned from unlabelled training data by a probabilistic context-free parser. For extracting noun chunks, the parser generates all possible noun chunk analyses, scores them with a novel algorithm which maximizes the best chunk sequence criterion, and chooses the most probable chunk sequence. An evaluation of the chunker on 2,140 hand-annotated noun chunks yielded 92% recall and 93% precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Parsing by Chunks.</title>
<date>1991</date>
<editor>In Robert Berwick, Steven Abney, and Carol Tenny, editors, Principle-Based Parsing.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="5803" citStr="Abney, 1991" startWordPosition="906" endWordPosition="907"> category Cd with a mother node of category Cm and lexical head hm bears the lexical head hd. The probability of a parse tree is obtained by multiplying lexicalised rule probabilities and lexical choice probabilities for all nodes. Since it is possible to transform H-L PCFGs into PCFGs, the PCFG algorithms are applicable to H-L PCFGs. 2.2 Noun Chunks in the German Grammar Currently, the German grammarcontains 4,619rules and covers 92% of our 15 million words of verb \x0cnal and relative clauses2 . The structural noun chunk concept in the grammar is de\x0cned according to Abney\&apos;s chunk style (Abney, 1991) who describes chunks as syntactic units which correspond in some way to prosodic patterns, containing a content word surrounded by some function word(s): all words from the beginning of the noun phrase to the head noun are included.3 The di\x0berent kinds of noun chunks covered by our grammar are listed below and illustrated with examples: \x0f a combination of a non-obligatory determiner, optional adjectives or cardinals and the noun 1Other types of lexicalised PCFGs have been described in (Charniak, 1997), (Collins, 1997), (Goodman, 1997), (Chelba and Jelinek, 1998) and (Eisner and Satta, 1</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Steven Abney. 1991. Parsing by Chunks. In Robert Berwick, Steven Abney, and Carol Tenny, editors, Principle-Based Parsing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Partial Parsing via FiniteState Cascades.</title>
<date>1996</date>
<booktitle>In Proceedings of the ESSLLI \&apos;96 Robust Parsing Workshop.</booktitle>
<contexts>
<context position="2306" citStr="Abney, 1996" startWordPosition="337" endWordPosition="338"> other types of chunks like verb groups, adverbial phrases or adjectival phrases. Several methods have been developed for noun chunking. Church\&apos;s noun phrase tagger (Church, 1988), one of the \x0crst noun chunkers, was based on a Hidden Markov Model (HMM) similar to those used \x03 Thanks to Mats Rooth and Uli Heid for many helpful comments. for part-of-speech tagging. Another HMM-based approach has been developed by Mats Rooth (Rooth, 1992). It integrates two HMMs; one of them models noun chunks internally, the other models the context of noun chunks. Abney\&apos;s cascaded \x0cnitestate parser (Abney, 1996) also contains a processing step which recognises noun chunks and other types of chunks. Ramshaw and Marcus (Ramshaw and Marcus, 1995) successfully applied Eric Brill\&apos;s transformation-based learning method to the chunking problem. Voutilainen\&apos;s NPtool (Voutilainen, 1993) is based on his constraint-grammar system. Finally, Brants (Brants, 1999) described a German chunker which was implemented with cascaded Markov Models. In this paper, a probabilistic context-free parser is applied to the noun chunking task. The German grammar used in the experiments was semiautomatically extended with robust</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Steven Abney. 1996. Partial Parsing via FiniteState Cascades. In Proceedings of the ESSLLI \&apos;96 Robust Parsing Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Beil</author>
<author>Glenn Carroll</author>
<author>Detlef Prescher</author>
<author>Stefan Riezler</author>
<author>Mats Rooth</author>
</authors>
<title>Inside-Outside Estimation of a Lexicalized PCFG for German.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL,</booktitle>
<pages>269--276</pages>
<contexts>
<context position="10055" citStr="Beil et al., 1999" startWordPosition="1571" endWordPosition="1574">ical choice probabilities); in a sentence like er zahlte die Beitr\x7f age (he paid the contributions) the accusative case of the latter noun chunk should be identi\x0ced because of the lexical coherence between the verb zahlen (pay) and the lexical head of the subcategorised noun phrase Beitrag (contribution) as related direct object head (! lexical choice probabilities). 2.3 Robustness Rules The German grammar coversover 90% of the clauses of our verb \x0cnal and relative clause corpora. This is su\x0ecient for the extraction of lexical information, e.g. the subcategorisation of verbs (see (Beil et al., 1999)). For chunking, however, it is usually necessary to analyse all sentences. Therefore, the grammar was augmented with a set of robustness rules. Three types of robustness rules have been considered, namely unigram rules, bigram rules and trigram rules. Unigram rules are rules of the form X ! YP X, where YP is a grammatical category and X is a new category. If such a rule is added for each grammar category4 , the coverage is 100% because the grammar is then able to generateany sequence of category labels. In practice, some of the rules can be omitted while still retaining full coverage: e.g. th</context>
</contexts>
<marker>Beil, Carroll, Prescher, Riezler, Rooth, 1999</marker>
<rawString>Franz Beil, Glenn Carroll, Detlef Prescher, Stefan Riezler, and Mats Rooth. 1999. Inside-Outside Estimation of a Lexicalized PCFG for German. In Proceedings of the 37th Annual Meeting of the ACL, pages 269{276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Cascaded markov models.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL\&apos;99.</booktitle>
<contexts>
<context position="2653" citStr="Brants, 1999" startWordPosition="385" endWordPosition="386">or part-of-speech tagging. Another HMM-based approach has been developed by Mats Rooth (Rooth, 1992). It integrates two HMMs; one of them models noun chunks internally, the other models the context of noun chunks. Abney\&apos;s cascaded \x0cnitestate parser (Abney, 1996) also contains a processing step which recognises noun chunks and other types of chunks. Ramshaw and Marcus (Ramshaw and Marcus, 1995) successfully applied Eric Brill\&apos;s transformation-based learning method to the chunking problem. Voutilainen\&apos;s NPtool (Voutilainen, 1993) is based on his constraint-grammar system. Finally, Brants (Brants, 1999) described a German chunker which was implemented with cascaded Markov Models. In this paper, a probabilistic context-free parser is applied to the noun chunking task. The German grammar used in the experiments was semiautomatically extended with robustness rules in order to be able to process arbitrary input. The grammar parameters were trained on unlabelled data. A novel algorithm is used for noun chunk extraction. It maximises the probability of the chunk set. The following section introduces the grammar framework, followed by a description of the chunking algorithm in section 3, and the ex</context>
</contexts>
<marker>Brants, 1999</marker>
<rawString>Thorsten Brants. 1999. Cascaded markov models. In Proceedings of EACL\&apos;99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Mats Rooth</author>
</authors>
<title>Valence Induction with a Head-Lexicalized PCFG.</title>
<date>1998</date>
<booktitle>In Proceedings of Third Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4182" citStr="Carroll and Rooth, 1998" startWordPosition="619" endWordPosition="622">s. The robustness rules for the chunker are described in section 2.3. 2.1 (Head-Lexicalised) Probabilistic Context-Free Grammars A probabilistic context-free grammar (PCFG) is a context-free grammar which assigns a probability P to each context-free grammar rule in the rule set R. The probability of a parse tree T is de\x0cned as Q r2R P(r)jrj, where jrj is the number of times rule r was applied to build T. The parameters of \x0cPCFGs can be learned from unparsed corpora using the Inside-Outside algorithm (Lari and Young, 1990). Head-lexicalised probabilistic context-free grammars (H-L PCFG) (Carroll and Rooth, 1998) extend the PCFG approach by incorporating information about the lexical head of constituents into the probabilistic model.1 Each node in a parse of a HL PCFG is labelled with a category and the lexical head of the category. A H-L PCFG rule looks like a PCFG rule in which one of the daughters has been marked as the head. The rule probabilities Prule(C ! \x0bjC) are replaced by lexicalised rule probabilities Prule(C ! \x0bjC;h) where h is the lexical head of the mother constituent C. The probability of a rule therefore depends not only on the category of the mother node, but also on its lexical</context>
<context position="11206" citStr="Carroll and Rooth, 1998" startWordPosition="1785" endWordPosition="1788"> of the rules can be omitted while still retaining full coverage: e.g. the rule X ! 4Also needed are two rules which start and terminate the \\X chain&amp;quot;. We used the rules TOP ! START X and X ! END. START and END expand to SGML tags which mark the beginning and the end of a sentence, respectively. ADV X is not necessary if the grammar already contains the rules ADVP ! ADV and X ! ADVP X. Unigram rules are insensitive to their context so that all permutations of the categories which are generated by the X chain have the same probability. The second type of robustnessrules, called trigram rules (Carroll and Rooth, 1998) is more context sensitive. Trigram rules have the form X:Y ! Y Y:Z where X, Y, Z are categories and X:Y and Y:Z are new categories. Trigram rules choose the next category on the basis of the two preceding categories. Therefore the number of rules grows as the number of categories raises to the third power. For example, 125,000 trigram rules are needed to generate 50 di\x0berent categories in arbitrary order. Since unigram rules are context insensitive and trigram rules are too numerous, a third type of robustness rules, called bigram rules, was developed. A bigram rule actually consists of tw</context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>Glenn Carroll and Mats Rooth. 1998. Valence Induction with a Head-Lexicalized PCFG. In Proceedings of Third Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Parsing with a Context-Free Grammar and Word Statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th National Conference on Arti\x0ccial Intelligence.</booktitle>
<contexts>
<context position="6316" citStr="Charniak, 1997" startWordPosition="988" endWordPosition="989">uctural noun chunk concept in the grammar is de\x0cned according to Abney\&apos;s chunk style (Abney, 1991) who describes chunks as syntactic units which correspond in some way to prosodic patterns, containing a content word surrounded by some function word(s): all words from the beginning of the noun phrase to the head noun are included.3 The di\x0berent kinds of noun chunks covered by our grammar are listed below and illustrated with examples: \x0f a combination of a non-obligatory determiner, optional adjectives or cardinals and the noun 1Other types of lexicalised PCFGs have been described in (Charniak, 1997), (Collins, 1997), (Goodman, 1997), (Chelba and Jelinek, 1998) and (Eisner and Satta, 1999). 2The restricted corpora were extracted automatically from the Huge German Corpus (HGC), a collection of German newspapers as well as specialised magazines for industry, law, computer science. 3As you will see below, there is one exception, noun chunks re\x0cned by a proper name, which end with the name instead of the head noun. itself: (1) eine a gute good Idee idea (2) vielen for many Menschen people (3) deren whose k\x7f unstliche arti\x0ccial Stimme voice (4) elf eleven Ladungen cargos (5) Wasser wa</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical Parsing with a Context-Free Grammar and Word Statistics. In Proceedings of the 14th National Conference on Arti\x0ccial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Exploiting Syntactic Structure for Language Modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="6378" citStr="Chelba and Jelinek, 1998" startWordPosition="994" endWordPosition="997">ed according to Abney\&apos;s chunk style (Abney, 1991) who describes chunks as syntactic units which correspond in some way to prosodic patterns, containing a content word surrounded by some function word(s): all words from the beginning of the noun phrase to the head noun are included.3 The di\x0berent kinds of noun chunks covered by our grammar are listed below and illustrated with examples: \x0f a combination of a non-obligatory determiner, optional adjectives or cardinals and the noun 1Other types of lexicalised PCFGs have been described in (Charniak, 1997), (Collins, 1997), (Goodman, 1997), (Chelba and Jelinek, 1998) and (Eisner and Satta, 1999). 2The restricted corpora were extracted automatically from the Huge German Corpus (HGC), a collection of German newspapers as well as specialised magazines for industry, law, computer science. 3As you will see below, there is one exception, noun chunks re\x0cned by a proper name, which end with the name instead of the head noun. itself: (1) eine a gute good Idee idea (2) vielen for many Menschen people (3) deren whose k\x7f unstliche arti\x0ccial Stimme voice (4) elf eleven Ladungen cargos (5) Wasser water and prepositional phrases where the de\x0cnite article of </context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 1998. Exploiting Syntactic Structure for Language Modeling. In Proceedings of the 36th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for unrestricted Text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="1874" citStr="Church, 1988" startWordPosition="264" endWordPosition="265">eiligt. involved. `Leading economists with doubtable reputations are involved in guiding the country in times of bottlenecks.\&apos; A tool which identi\x0ces noun chunks is useful for term extraction (most technical terms are nouns or complex noun groups), for lexicographic purposes (see (Tapanainen and J\x7f arvinen, 1998) on syntactically organised concordancing), and as index terms for information retrieval. Chunkers may also mark other types of chunks like verb groups, adverbial phrases or adjectival phrases. Several methods have been developed for noun chunking. Church\&apos;s noun phrase tagger (Church, 1988), one of the \x0crst noun chunkers, was based on a Hidden Markov Model (HMM) similar to those used \x03 Thanks to Mats Rooth and Uli Heid for many helpful comments. for part-of-speech tagging. Another HMM-based approach has been developed by Mats Rooth (Rooth, 1992). It integrates two HMMs; one of them models noun chunks internally, the other models the context of noun chunks. Abney\&apos;s cascaded \x0cnitestate parser (Abney, 1996) also contains a processing step which recognises noun chunks and other types of chunks. Ramshaw and Marcus (Ramshaw and Marcus, 1995) successfully applied Eric Brill\&apos;</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Kenneth W. Church. 1988. A Stochastic Parts Program and Noun Phrase Parser for unrestricted Text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136{ 143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three Generative, Lexicalised Models for Statistical Parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="6333" citStr="Collins, 1997" startWordPosition="990" endWordPosition="991"> concept in the grammar is de\x0cned according to Abney\&apos;s chunk style (Abney, 1991) who describes chunks as syntactic units which correspond in some way to prosodic patterns, containing a content word surrounded by some function word(s): all words from the beginning of the noun phrase to the head noun are included.3 The di\x0berent kinds of noun chunks covered by our grammar are listed below and illustrated with examples: \x0f a combination of a non-obligatory determiner, optional adjectives or cardinals and the noun 1Other types of lexicalised PCFGs have been described in (Charniak, 1997), (Collins, 1997), (Goodman, 1997), (Chelba and Jelinek, 1998) and (Eisner and Satta, 1999). 2The restricted corpora were extracted automatically from the Huge German Corpus (HGC), a collection of German newspapers as well as specialised magazines for industry, law, computer science. 3As you will see below, there is one exception, noun chunks re\x0cned by a proper name, which end with the name instead of the head noun. itself: (1) eine a gute good Idee idea (2) vielen for many Menschen people (3) deren whose k\x7f unstliche arti\x0ccial Stimme voice (4) elf eleven Ladungen cargos (5) Wasser water and prepositi</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three Generative, Lexicalised Models for Statistical Parsing. In Proceedings of the 35th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>E\x0ecient Parsing for Bilexical Context-Free Grammars and Head Automaton Grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL,</booktitle>
<pages>457--464</pages>
<contexts>
<context position="6407" citStr="Eisner and Satta, 1999" startWordPosition="999" endWordPosition="1002">style (Abney, 1991) who describes chunks as syntactic units which correspond in some way to prosodic patterns, containing a content word surrounded by some function word(s): all words from the beginning of the noun phrase to the head noun are included.3 The di\x0berent kinds of noun chunks covered by our grammar are listed below and illustrated with examples: \x0f a combination of a non-obligatory determiner, optional adjectives or cardinals and the noun 1Other types of lexicalised PCFGs have been described in (Charniak, 1997), (Collins, 1997), (Goodman, 1997), (Chelba and Jelinek, 1998) and (Eisner and Satta, 1999). 2The restricted corpora were extracted automatically from the Huge German Corpus (HGC), a collection of German newspapers as well as specialised magazines for industry, law, computer science. 3As you will see below, there is one exception, noun chunks re\x0cned by a proper name, which end with the name instead of the head noun. itself: (1) eine a gute good Idee idea (2) vielen for many Menschen people (3) deren whose k\x7f unstliche arti\x0ccial Stimme voice (4) elf eleven Ladungen cargos (5) Wasser water and prepositional phrases where the de\x0cnite article of the embedded noun chunk is mo</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Jason Eisner and Giorgio Satta. 1999. E\x0ecient Parsing for Bilexical Context-Free Grammars and Head Automaton Grammars. In Proceedings of the 37th Annual Meeting of the ACL, pages 457{ 464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing Algorithms and Metrics.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL,</booktitle>
<pages>177--183</pages>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Parsing Algorithms and Metrics. In Proceedings of the 34th Annual Meeting of the ACL, pages 177{183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Probabilistic Feature Grammars.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th International Workshop on Parsing Technologies,</booktitle>
<pages>89--100</pages>
<contexts>
<context position="6350" citStr="Goodman, 1997" startWordPosition="992" endWordPosition="993">rammar is de\x0cned according to Abney\&apos;s chunk style (Abney, 1991) who describes chunks as syntactic units which correspond in some way to prosodic patterns, containing a content word surrounded by some function word(s): all words from the beginning of the noun phrase to the head noun are included.3 The di\x0berent kinds of noun chunks covered by our grammar are listed below and illustrated with examples: \x0f a combination of a non-obligatory determiner, optional adjectives or cardinals and the noun 1Other types of lexicalised PCFGs have been described in (Charniak, 1997), (Collins, 1997), (Goodman, 1997), (Chelba and Jelinek, 1998) and (Eisner and Satta, 1999). 2The restricted corpora were extracted automatically from the Huge German Corpus (HGC), a collection of German newspapers as well as specialised magazines for industry, law, computer science. 3As you will see below, there is one exception, noun chunks re\x0cned by a proper name, which end with the name instead of the head noun. itself: (1) eine a gute good Idee idea (2) vielen for many Menschen people (3) deren whose k\x7f unstliche arti\x0ccial Stimme voice (4) elf eleven Ladungen cargos (5) Wasser water and prepositional phrases wher</context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>Joshua Goodman. 1997. Probabilistic Feature Grammars. In Proceedings of the 5th International Workshop on Parsing Technologies, pages 89{100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S Young</author>
</authors>
<title>The Estimation of Stochastic Context-Free Grammars using the Inside-Outside Algorithm. Computation Speech and Language Processing,</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="4091" citStr="Lari and Young, 1990" startWordPosition="608" endWordPosition="611">ed re\x0cnement. Section 2.2 introduces our grammar architecture, focusing on noun chunks. The robustness rules for the chunker are described in section 2.3. 2.1 (Head-Lexicalised) Probabilistic Context-Free Grammars A probabilistic context-free grammar (PCFG) is a context-free grammar which assigns a probability P to each context-free grammar rule in the rule set R. The probability of a parse tree T is de\x0cned as Q r2R P(r)jrj, where jrj is the number of times rule r was applied to build T. The parameters of \x0cPCFGs can be learned from unparsed corpora using the Inside-Outside algorithm (Lari and Young, 1990). Head-lexicalised probabilistic context-free grammars (H-L PCFG) (Carroll and Rooth, 1998) extend the PCFG approach by incorporating information about the lexical head of constituents into the probabilistic model.1 Each node in a parse of a HL PCFG is labelled with a category and the lexical head of the category. A H-L PCFG rule looks like a PCFG rule in which one of the daughters has been marked as the head. The rule probabilities Prule(C ! \x0bjC) are replaced by lexicalised rule probabilities Prule(C ! \x0bjC;h) where h is the lexical head of the mother constituent C. The probability of a </context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S. Young. 1990. The Estimation of Stochastic Context-Free Grammars using the Inside-Outside Algorithm. Computation Speech and Language Processing, 4:35{56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>U Essen</author>
<author>R Kneser</author>
</authors>
<date>1994</date>
<booktitle>On Structuring Probabilistic Dependencies in Stochastic Language Modelling. Computer Speech and Language,</booktitle>
<pages>8--1</pages>
<contexts>
<context position="13193" citStr="Ney et al., 1994" startWordPosition="2124" endWordPosition="2127">we now obtained only n + 2 rules with the new category PPX. Altogether, 3,332 robustness rules were added. 3 Chunking A head-lexicalised probabilistic context-free parser, called LoPar (Schmid, 1999), was used for parsing. The functionality of LoPar encompasses purely symbolic parsing as well as Viterbi parsing, insideoutside computation, POS tagging, chunking and training with PCFGs as well as H-L PCFGs. Because of the large number of parameters in particular of H-L PCFGs, the parser smoothes the probability distributions in order to avoid zero probabilities. The absolute discounting method (Ney et al., 1994) was adapted to fractional counts for this purpose. LoPar also supports lemmatisation of the lexical heads of a H-L PCFG. The input to the parser consists of ambiguously tagged words. The tags are provided by a German morphological analyser (Schiller and St\x7f ockert, 1995). The best chunk set of a sentence is de\x0cned as the set of chunks (with category, start and end position) \x0cfor which the sum of the probabilities of all parses which contain exactly that chunk set is maximal. The chunk set of the most likely parse (i.e. Viterbi parse) is not necessarily the best chunk set according to</context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>Hermann Ney, U. Essen, and R. Kneser. 1994. On Structuring Probabilistic Dependencies in Stochastic Language Modelling. Computer Speech and Language, 8:1{38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Text Chunking using Transformation-Based Learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<contexts>
<context position="2440" citStr="Ramshaw and Marcus, 1995" startWordPosition="357" endWordPosition="360"> noun chunking. Church\&apos;s noun phrase tagger (Church, 1988), one of the \x0crst noun chunkers, was based on a Hidden Markov Model (HMM) similar to those used \x03 Thanks to Mats Rooth and Uli Heid for many helpful comments. for part-of-speech tagging. Another HMM-based approach has been developed by Mats Rooth (Rooth, 1992). It integrates two HMMs; one of them models noun chunks internally, the other models the context of noun chunks. Abney\&apos;s cascaded \x0cnitestate parser (Abney, 1996) also contains a processing step which recognises noun chunks and other types of chunks. Ramshaw and Marcus (Ramshaw and Marcus, 1995) successfully applied Eric Brill\&apos;s transformation-based learning method to the chunking problem. Voutilainen\&apos;s NPtool (Voutilainen, 1993) is based on his constraint-grammar system. Finally, Brants (Brants, 1999) described a German chunker which was implemented with cascaded Markov Models. In this paper, a probabilistic context-free parser is applied to the noun chunking task. The German grammar used in the experiments was semiautomatically extended with robustness rules in order to be able to process arbitrary input. The grammar parameters were trained on unlabelled data. A novel algorithm i</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. Ramshaw and M. Marcus. 1995. Text Chunking using Transformation-Based Learning. In Proceedings of the Third Workshop on Very Large Corpora, pages 82{94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
</authors>
<date>1992</date>
<note>Statistical NP Tagging. Unpublished manuscript.</note>
<contexts>
<context position="2140" citStr="Rooth, 1992" startWordPosition="310" endWordPosition="311">hic purposes (see (Tapanainen and J\x7f arvinen, 1998) on syntactically organised concordancing), and as index terms for information retrieval. Chunkers may also mark other types of chunks like verb groups, adverbial phrases or adjectival phrases. Several methods have been developed for noun chunking. Church\&apos;s noun phrase tagger (Church, 1988), one of the \x0crst noun chunkers, was based on a Hidden Markov Model (HMM) similar to those used \x03 Thanks to Mats Rooth and Uli Heid for many helpful comments. for part-of-speech tagging. Another HMM-based approach has been developed by Mats Rooth (Rooth, 1992). It integrates two HMMs; one of them models noun chunks internally, the other models the context of noun chunks. Abney\&apos;s cascaded \x0cnitestate parser (Abney, 1996) also contains a processing step which recognises noun chunks and other types of chunks. Ramshaw and Marcus (Ramshaw and Marcus, 1995) successfully applied Eric Brill\&apos;s transformation-based learning method to the chunking problem. Voutilainen\&apos;s NPtool (Voutilainen, 1993) is based on his constraint-grammar system. Finally, Brants (Brants, 1999) described a German chunker which was implemented with cascaded Markov Models. In this </context>
</contexts>
<marker>Rooth, 1992</marker>
<rawString>Mats Rooth. 1992. Statistical NP Tagging. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Schiller</author>
<author>Chris St\x7f ockert</author>
</authors>
<date>1995</date>
<booktitle>DMOR. Institut f\x7f ur Maschinelle Sprachverarbeitung, Universit\x7f at Stuttgart.</booktitle>
<marker>Schiller, ockert, 1995</marker>
<rawString>Anne Schiller and Chris St\x7f ockert, 1995. DMOR. Institut f\x7f ur Maschinelle Sprachverarbeitung, Universit\x7f at Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Lopar: Design and Implementation.</title>
<date>1999</date>
<booktitle>Institut f\x7f ur Maschinelle Sprachverarbeitung, Universit\x7f at Stuttgart.</booktitle>
<tech>Technical report,</tech>
<contexts>
<context position="12775" citStr="Schmid, 1999" startWordPosition="2057" endWordPosition="2058">ation of the bigram rules for our German grammar, the number of rules was still fairly large. Hence we generalised some of the grammatical categories by adding additional chain rules. For example, the prepositional phrase categories PP.Akk:an, PP.Akk:auf, PP.Akk:gegen etc. were generalised to PPX by adding the rules PPX ! PP.Akk:an etc. Instead of n + 1 bigram rules for each of the 23 prepositional categories, we now obtained only n + 2 rules with the new category PPX. Altogether, 3,332 robustness rules were added. 3 Chunking A head-lexicalised probabilistic context-free parser, called LoPar (Schmid, 1999), was used for parsing. The functionality of LoPar encompasses purely symbolic parsing as well as Viterbi parsing, insideoutside computation, POS tagging, chunking and training with PCFGs as well as H-L PCFGs. Because of the large number of parameters in particular of H-L PCFGs, the parser smoothes the probability distributions in order to avoid zero probabilities. The absolute discounting method (Ney et al., 1994) was adapted to fractional counts for this purpose. LoPar also supports lemmatisation of the lexical heads of a H-L PCFG. The input to the parser consists of ambiguously tagged words</context>
</contexts>
<marker>Schmid, 1999</marker>
<rawString>Helmut Schmid. 1999. Lopar: Design and Implementation. Technical report, Institut f\x7f ur Maschinelle Sprachverarbeitung, Universit\x7f at Stuttgart.</rawString>
</citation>
<citation valid="true">
<title>Pasi Tapanainen and Timo J\x7f arvinen.</title>
<date>1998</date>
<journal>International Journal of Lexicography,</journal>
<volume>11</volume>
<issue>3</issue>
<marker>1998</marker>
<rawString>Pasi Tapanainen and Timo J\x7f arvinen. 1998. Dependency Concordances. International Journal of Lexicography, 11(3):187{203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atro Voutilainen</author>
</authors>
<title>NPtool, a Detector of English Noun Phrases.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora,</booktitle>
<pages>48--57</pages>
<contexts>
<context position="2579" citStr="Voutilainen, 1993" startWordPosition="375" endWordPosition="376"> those used \x03 Thanks to Mats Rooth and Uli Heid for many helpful comments. for part-of-speech tagging. Another HMM-based approach has been developed by Mats Rooth (Rooth, 1992). It integrates two HMMs; one of them models noun chunks internally, the other models the context of noun chunks. Abney\&apos;s cascaded \x0cnitestate parser (Abney, 1996) also contains a processing step which recognises noun chunks and other types of chunks. Ramshaw and Marcus (Ramshaw and Marcus, 1995) successfully applied Eric Brill\&apos;s transformation-based learning method to the chunking problem. Voutilainen\&apos;s NPtool (Voutilainen, 1993) is based on his constraint-grammar system. Finally, Brants (Brants, 1999) described a German chunker which was implemented with cascaded Markov Models. In this paper, a probabilistic context-free parser is applied to the noun chunking task. The German grammar used in the experiments was semiautomatically extended with robustness rules in order to be able to process arbitrary input. The grammar parameters were trained on unlabelled data. A novel algorithm is used for noun chunk extraction. It maximises the probability of the chunk set. The following section introduces the grammar framework, fo</context>
</contexts>
<marker>Voutilainen, 1993</marker>
<rawString>Atro Voutilainen. 1993. NPtool, a Detector of English Noun Phrases. In Proceedings of the Workshop on Very Large Corpora, pages 48{57. \x0c&apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>