6.1 The k-best+residual Semiring When restricted to local features, cube pruning and cube summing can be seen as proper semirk-best proof CITATION k-best+residual Viterbi proof CITATION all proof CITATION Viterbi CITATION ignore proof inside CITATION ignore residual k = 0 k = k = 1 Figure 2: Semirings generalized by k-best+residual,,
Cube pruning reduces to an implementation of the k-best semiring CITATION, and cube summing reduces to a novel semiring we call the k-best+residual semiring,,
That is associative follows from associativity of max-k, shown by CITATION,,
7 CITATION described a related approach to approximate summing using the chart computed during cube pruning, but did not keep track of the residual terms as we do here,,
hiang, 2007; Huang and CITATION) was proposed as a way to leverage existing dynamic programming algorithms that find optimal-scoring derivations or structures when only local features are involved,,
Meanwhile, some learning algorithms, like maximum likelihood for conditional log-linear models CITATION, unsupervised models CITATION, and models with hidden variables (CITATION; CITATION; CITATION), require summing over the scores of many structures to calculate marginals,,
We first review the semiring-weighted logic programming view of dynamic programming algorithms CITATION and identify an intuitive property of a program called proof locality that follows from feature locality in the underlying probability model (2),,
It is inspired by cube pruning (CITATION; Huang and CITATION) in its computation of non-local features dynamically using scored k-best lists, but also maintains additional residual quantities used in calculating approximate marginals,,
When restricted to local features, cube summing reduces to a novel semiring (k-best+residual) that generalizes many of the semirings of CITATION,,
alues CITATION, and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pruning, which we discuss in 3, integer linear programming CITATION, in which arbitrary features can act as constraints on y, and approximate solutions like CITATION, in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest,,
3 Approximate Decoding Cube pruning (CITATION; Huang and CITATION) is an approximate technique for decoding (Eq,,
Some stem from work on graphical models, including loopy belief propagation (CITATION; CITATION), Gibbs sampling CITATION, sequential Monte Carlo methods such as particle filtering CITATION, and variational inference (CITATION; CITATION; CITATION),,
Also relevant are stacked learning CITATION, interpretable as approximation of non-local feature values CITATION, and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pruning, which we discuss in 3, integer linear programming CITATION, in which arbitrary features can act as constraints on y, and approximate solutions like CITATION, in which an exact solution to a related decoding problem is found and then modified to fit the pro,,
tool for performing probabilistic inference CITATION,,
Arithmetic circuits are amenable to automatic differentiation in the reverse mode CITATION, commonly used in backpropagation algorithms,,
This formal framework was the basis for the Dyna programming language, which permits a declarative specification of the logic program and compiles it into an efficient, agendabased, bottom-up procedure CITATION,,
Solvers have been proposed by CITATION, by CITATION using a hypergraph representation, and by CITATION,,
Because Goodmans and Eisner et al.s algorithms assume semirings, adapting them for cube summing is non-trivial.9 To generalize Goodmans algorithm, we suggest using the directed-graph data structure known variously as an arithmetic circuit or computation graph.10 Arithmetic circuits have recently drawn interest in the graphical model community as a 9 The bottom-up agenda algorithm in CITATION might possibly be generalized so that associativity, distributivity, and binary operators are not required (John Blatz, p.c.),,
Arithmetic circuits are amenable to automatic differentiation in the reverse mode CITATION, commonly used in backpropagation algorithms,,
Importantly, this permits us to calculate the exact gradient of the approximate summation with respect to axiom values, following CITATION,,
Another differentiation technique, implemented within the semiring, is given by CITATION,,
Cube pruning is based on the k-best algorithms of CITATION, which save time over generic semiring implementations through lazy computation in both the aggregation and combination operations,,
Arithmetic circuits are amenable to automatic differentiation in the reverse mode CITATION, commonly used in backpropagation algorithms,,
Importantly, this permits us to calculate the exact gradient of the approximate summation with respect to axiom values, following CITATION,,
Another differentiation technique, implemented within the semiring, is given by CITATION,,
Cube pruning is based on the k-best algorithms of CITATION, which save time over generic semiring implementations through lazy computation in both the aggregation and combination operations,,
Some stem from work on graphical models, including loopy belief propagation (CITATION; CITATION), Gibbs sampling CITATION, sequential Monte Carlo methods such as particle filtering CITATION, and variational inference (CITATION; CITATION; CITATION),,
Also relevant are stacked learning CITATION, interpretable as approximation of non-local feature values CITATION, and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pruning, which we discuss in 3, integer linear programming (Roth a,,
6.1 The k-best+residual Semiring When restricted to local features, cube pruning and cube summing can be seen as proper semirk-best proof CITATION k-best+residual Viterbi proof CITATION all proof CITATION Viterbi CITATION ignore proof inside CITATION ignore residual k = 0 k = k = 1 Figure 2: Semirings generalized by k-best+residual,,
Cube pruning reduces to an implementation of the k-best semiring CITATION, and cube summing reduces to a novel semiring we call the k-best+residual semiring,,
That is associative follows from associativity of max-k, shown by CITATION,,
The computation over proof lists is identical to that performed in the k-best proof semiring, which was shown to be a semiring by CITATION,,
It is inspired by cube pruning (CITATION; Huang and CITATION) in its computation of non-local features dynamically using scored k-best lists, but also maintains additional residual quantities used in calculating approximate marginals,,
When restricted to local features, cube summing reduces to a novel semiring (k-best+residual) that generalizes many of the semirings of CITATION,,
CITATION showed a connection between DP (specifically, as used in parsing) and logic programming, and CITATION augmented such logic programs with semiring weights, giving an algebraic explanation for the intuitive connections among classes of algorithms with the same logical structure,,
This is often done using backpointers, but can also be accomplished by representing the most probable proof for each theorem in its entirety as part of the semiring value CITATION,,
This is similar to the k-best semiring defined by CITATION,,
Solvers have been proposed by CITATION, by CITATION using a hypergraph representation, and by CITATION,,
Because Goodmans and Eisner et al.s algorithms assume semirings, adapting them for cube summing is non-trivial.9 To generalize Goodmans algorithm, we suggest using the directed-graph data structure known variously as an arithmetic circuit or computation graph.10 Arithmetic circuits have recently drawn interest in the graphical model community as a 9 The bottom-up agenda algorithm in CITATION might possibly be generalized so that associativity, distributivity, and binary operators are not r,,
6.1 The k-best+residual Semiring When restricted to local features, cube pruning and cube summing can be seen as proper semirk-best proof CITATION k-best+residual Viterbi proof CITATION all proof CITATION Viterbi CITATION ignore proof inside CITATION ignore residual k = 0 k = k = 1 Figure 2: Semirings generalized by k-best+residual,,
Cube pruning reduces to an implementation of the k-best semiring CITATION, and cube summing reduces to a novel semiring we call the k-best+residual semiring,,
tool for performing probabilistic inference CITATION,,
Arithmetic circuits are amenable to automatic differentiation in the reverse mode CITATION, commonly used in backpropagation algorithms,,
Importantly, this permits us to calculate the exact gradient of the approximate summation with respect to axiom values, following CITATION,,
Another differentiation technique, implemented within the semiring, is given by CITATION,,
Cube pruning is based on the k-best algorithms of CITATION, which save time over generic semiring implementations through lazy computation in both the aggregation and combination operations,,
Arithmetic circuits are amenable to automatic differentiation in the reverse mode CITATION, commonly used in backpropagation algorithms,,
Importantly, this permits us to calculate the exact gradient of the approximate summation with respect to axiom values, following CITATION,,
Another differentiation technique, implemented within the semiring, is given by CITATION,,
Cube pruning is based on the k-best algorithms of CITATION, which save time over generic semiring implementations through lazy computation in both the aggregation and combination operations,,
It is inspired by cube pruning (CITATION; Huang and CITATION) in its computation of non-local features dynamically using scored k-best lists, but also maintains additional residual quantities used in calculating approximate marginals,,
When restricted to local features, cube summing reduces to a novel semiring (k-best+residual) that generalizes many of the semirings of CITATION,,
 et al., 2008), and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pruning, which we discuss in 3, integer linear programming CITATION, in which arbitrary features can act as constraints on y, and approximate solutions like CITATION, in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest,,
3 Approximate Decoding Cube pruning (CITATION; Huang and CITATION) is an approximate technique for decoding (Eq,,
For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as NGramTree features CITATION that score the string of terminals and nonterminals along the path from word j to word j + 1 when two constituents CY,i,j and CZ,j,k are combined,,
Some stem from work on graphical models, including loopy belief propagation (CITATION; CITATION), Gibbs sampling CITATION, sequential Monte Carlo methods such as particle filtering CITATION, and variational inference (CITATION; CITATION; CITATION),,
Also relevant are stacked learning CITATION, interpretable as approximation of non-local feature values CITATION, and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pruning, which we discuss in 3, integer linear programming CITATION, in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (20,,
Solvers have been proposed by CITATION, by CITATION using a hypergraph representation, and by CITATION,,
Because Goodmans and Eisner et al.s algorithms assume semirings, adapting them for cube summing is non-trivial.9 To generalize Goodmans algorithm, we suggest using the directed-graph data structure known variously as an arithmetic circuit or computation graph.10 Arithmetic circuits have recently drawn interest in the graphical model community as a 9 The bottom-up agenda algorithm in CITATION might possibly be generalized so that associativity, distributivity, and binary operators are not required (John Blatz, p.c.),,
Recently cube pruning (CITATION; Huang and CITATION) was proposed as a way to leverage existing dynamic programming algorithms that find optimal-scoring derivations or structures when only local features are involved,,
Meanwhile, some learning algorithms, like maximum likelihood for conditional log-linear models CITATION, unsupervised models CITATION, and models with hidden variables (CITATION; CITATION; CITATION), require summing over the scores of many structures to calculate marginals,,
We first review the semiring-weighted logic programming view of dynamic programming algorithms CITATION and identify an intuitive property of a program called proof locality that follows from feature locality in the underlying probability model (2),,
Some stem from work on graphical models, including loopy belief propagation (CITATION; CITATION), Gibbs sampling CITATION, sequential Monte Carlo methods such as particle filtering CITATION, and variational inference (CITATION; CITATION; CITATION),,
Also relevant are stacked learning CITATION, interpretable as approximation of non-local feature values CITATION, and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pruning, which we discuss in 3, integer linear programming CITATION, in which arbitrary features can act as constraints on y, and approximate solutions like CITATION, in which an exact solution to a rel,,
Recently cube pruning (CITATION; Huang and CITATION) was proposed as a way to leverage existing dynamic programming algorithms that find optimal-scoring derivations or structures when only local features are involved,,
Meanwhile, some learning algorithms, like maximum likelihood for conditional log-linear models CITATION, unsupervised models CITATION, and models with hidden variables (CITATION; CITATION; CITATION), require summing over the scores of many structures to calculate marginals,,
We first review the semiring-weighted logic programming view of dynamic programming algorithms CITATION and identify an intuitive property of a program called proof locality that follows from feature locality in the underlying probability model (2),,
Some stem from work on graphical models, including loopy belief propagation (CITATION; CITATION), Gibbs sampling CITATION, sequential Monte Carlo methods such as particle filtering CITATION, and variational inference (CITATION; CITATION; CITATION),,
Also relevant are stacked learning CITATION, interpretable as approximation of non-local feature values CITATION, and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pruning, which we discuss in 3, integer linear programming CITATION, in which arbitrary features can act as constraints on y, and app,,
utton and McCallum, 2004; CITATION), Gibbs sampling CITATION, sequential Monte Carlo methods such as particle filtering CITATION, and variational inference (CITATION; CITATION; CITATION),,
Also relevant are stacked learning CITATION, interpretable as approximation of non-local feature values CITATION, and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pruning, which we discuss in 3, integer linear programming CITATION, in which arbitrary features can act as constraints on y, and approximate solutions like CITATION, in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest,,
3 Approximate Decoding Cube pruning (CITATION; Huang and CITATION) is an approximate technique for decoding (Eq,,
Some stem from work on graphical models, including loopy belief propagation (CITATION; CITATION), Gibbs sampling CITATION, sequential Monte Carlo methods such as particle filtering CITATION, and variational inference (CITATION; CITATION; CITATION),,
Also relevant are stacked learning CITATION, interpretable as approximation of non-local feature values CITATION, and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pruning, which we discuss in 3, integer linear programming CITATION, in which arbitrary features can act as constraints on y, and approximate solutions like CITATION, in which ,,
Some stem from work on graphical models, including loopy belief propagation (CITATION; CITATION), Gibbs sampling CITATION, sequential Monte Carlo methods such as particle filtering CITATION, and variational inference (CITATION; CITATION; CITATION),,
Also relevant are stacked learning CITATION, interpretable as approximation of non-local feature values CITATION, and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pruning, which we discuss in 3, integer linear programming CITATION, in which arbitrary features can act as constraints on y, and approximate solutions like CITATION, in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest,,
3 Approximate Decoding Cube pruning (CITATION; Huang and Chia,,
nce (CITATION; CITATION; CITATION),,
Also relevant are stacked learning CITATION, interpretable as approximation of non-local feature values CITATION, and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pruning, which we discuss in 3, integer linear programming CITATION, in which arbitrary features can act as constraints on y, and approximate solutions like CITATION, in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest,,
3 Approximate Decoding Cube pruning (CITATION; Huang and CITATION) is an approximate technique for decoding (Eq,,
Recently cube pruning (CITATION; Huang and CITATION) was proposed as a way to leverage existing dynamic programming algorithms that find optimal-scoring derivations or structures when only local features are involved,,
Meanwhile, some learning algorithms, like maximum likelihood for conditional log-linear models CITATION, unsupervised models CITATION, and models with hidden variables (CITATION; CITATION; CITATION), require summing over the scores of many structures to calculate marginals,,
We first review the semiring-weighted logic programming view of dynamic programming algorithms CITATION and identify an intuitive property of a program called proof locality that follows from feature locality in the underlying probability model (2),,
 2005), sequential Monte Carlo methods such as particle filtering CITATION, and variational inference (CITATION; CITATION; CITATION),,
Also relevant are stacked learning CITATION, interpretable as approximation of non-local feature values CITATION, and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pruning, which we discuss in 3, integer linear programming CITATION, in which arbitrary features can act as constraints on y, and approximate solutions like CITATION, in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest,,
3 Approximate Decoding Cube pruning (CITATION; Huang and CITATION) is an approximate technique for decoding (Eq,,
Meanwhile, some learning algorithms, like maximum likelihood for conditional log-linear models CITATION, unsupervised models CITATION, and models with hidden variables (CITATION; CITATION; CITATION), require summing over the scores of many structures to calculate marginals,,
We first review the semiring-weighted logic programming view of dynamic programming algorithms CITATION and identify an intuitive property of a program called proof locality that follows from feature locality in the underlying probability model (2),,
CITATION showed a connection between DP (specifically, as used in parsing) and logic programming, and CITATION augmented such logic programs with semiring weights, giving an algebraic explanation for the intuitive connections among classes of algorithms with the same logical structure,,
Some stem from work on graphical models, including loopy belief propagation (CITATION; CITATION), Gibbs sampling CITATION, sequential Monte Carlo methods such as particle filtering CITATION, and variational inference (CITATION; CITATION; CITATION),,
Also relevant are stacked learning CITATION, interpretable as approximation of non-local feature values CITATION, and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pruning, which we discuss in,,
Some stem from work on graphical models, including loopy belief propagation (CITATION; CITATION), Gibbs sampling CITATION, sequential Monte Carlo methods such as particle filtering CITATION, and variational inference (CITATION; CITATION; CITATION),,
Also relevant are stacked learning CITATION, interpretable as approximation of non-local feature values CITATION, and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pruning, which we discuss in 3, integer linear programming CITATION, in which arbitrary features can act as constraints on y, and approximate solutions like CITATION, in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest,,
3 Approximate Decoding Cube pruning (CITATION; Huang and CITATION) is an approximate technique f,,
Some stem from work on graphical models, including loopy belief propagation (CITATION; CITATION), Gibbs sampling CITATION, sequential Monte Carlo methods such as particle filtering CITATION, and variational inference (CITATION; CITATION; CITATION),,
Also relevant are stacked learning CITATION, interpretable as approximation of non-local feature values CITATION, and M-estimation CITATION, which allows training without inference,,
These include beam search CITATION, cube pru,,
6.1 The k-best+residual Semiring When restricted to local features, cube pruning and cube summing can be seen as proper semirk-best proof CITATION k-best+residual Viterbi proof CITATION all proof CITATION Viterbi CITATION ignore proof inside CITATION ignore residual k = 0 k = k = 1 Figure 2: Semirings generalized by k-best+residual,,
Cube pruning reduces to an implementation of the k-best semiring CITATION, and cube summing reduces to a novel semiring we call the k-best+residual semiring,,
That is associative follows from associativity of max-k, shown by CITATION,,
tly cube pruning (CITATION; Huang and CITATION) was proposed as a way to leverage existing dynamic programming algorithms that find optimal-scoring derivations or structures when only local features are involved,,
Meanwhile, some learning algorithms, like maximum likelihood for conditional log-linear models CITATION, unsupervised models CITATION, and models with hidden variables (CITATION; CITATION; CITATION), require summing over the scores of many structures to calculate marginals,,
We first review the semiring-weighted logic programming view of dynamic programming algorithms CITATION and identify an intuitive property of a program called proof locality that follows from feature locality in the underlying probability model (2),,
