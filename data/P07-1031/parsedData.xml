<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<bodyText confidence="0.3919415">
b&amp;apos;Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 240247,
Prague, Czech Republic, June 2007. c
</bodyText>
<sectionHeader confidence="0.300716" genericHeader="abstract">
2007 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.924353">
Adding Noun Phrase Structure to the Penn Treebank
</title>
<author confidence="0.957298">
David Vadas and James R. Curran
</author>
<affiliation confidence="0.9939195">
School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.893844">
NSW 2006, Australia
</address>
<email confidence="0.940756">
dvadas1, james\x01 @it.usyd.edu.au
</email>
<sectionHeader confidence="0.987934" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.9961960625">
The Penn Treebank does not annotate
within base noun phrases (NPs), commit-
ting only to flat structures that ignore the
complexity of English NPs. This means
that tools trained on Treebank data cannot
learn the correct internal structure of NPs.
This paper details the process of adding
gold-standard bracketing within each
noun phrase in the Penn Treebank. We
then examine the consistency and reliabil-
ity of our annotations. Finally, we use
this resource to determine NP structure
using several statistical approaches, thus
demonstrating the utility of the corpus.
This adds detail to the Penn Treebank that
is necessary for many NLP applications.
</bodyText>
<sectionHeader confidence="0.996943" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.962848">
The Penn Treebank (Marcus et al., 1993) is perhaps
the most influential resource in Natural Language
Processing (NLP). It is used as a standard train-
ing and evaluation corpus in many syntactic analysis
tasks, ranging from part of speech (POS) tagging and
chunking, to full parsing.
Unfortunately, the Penn Treebank does not anno-
tate the internal structure of base noun phrases, in-
stead leaving them flat. This significantly simplified
and sped up the manual annotation process.
Therefore, any system trained on Penn Treebank
data will be unable to model the syntactic and se-
mantic structure inside base-NPs.
The following NP is an example of the flat struc-
ture of base-NPs within the Penn Treebank:
(NP (NNP Air) (NNP Force) (NN contract))
Air Force is a specific entity and should form a sep-
arate constituent underneath the NP, as in our new
annotation scheme:
</bodyText>
<equation confidence="0.499408">
(NP
(NML (NNP Air) (NNP Force))
(NN contract))
</equation>
<bodyText confidence="0.9995465">
We use NML to specify that Air Force together is a
nominal modifier of contract. Adding this annota-
tion better represents the true syntactic and seman-
tic structure, which will improve the performance of
downstream NLP systems.
Our main contribution is a gold-standard labelled
bracketing for every ambiguous noun phrase in the
Penn Treebank. We describe the annotation guide-
lines and process, including the use of named en-
tity data to improve annotation quality. We check
the correctness of the corpus by measuring inter-
annotator agreement, by reannotating the first sec-
tion, and by comparing against the sub-NP structure
in DepBank (King et al., 2003).
We also give an analysis of our extended Tree-
bank, quantifying how much structure we have
added, and how it is distributed across NPs. Fi-
nally, we test the utility of the extended Treebank for
training statistical models on two tasks: NP bracket-
ing (Lauer, 1995; Nakov and Hearst, 2005) and full
parsing (Collins, 1999).
This new resource will allow any system or anno-
tated corpus developed from the Penn Treebank, to
represent noun phrase structure more accurately.
</bodyText>
<page confidence="0.994167">
240
</page>
<sectionHeader confidence="0.783199" genericHeader="method">
\x0c2 Motivation
</sectionHeader>
<bodyText confidence="0.994784621621622">
Many approaches to identifying base noun phrases
have been explored as part of chunking (Ramshaw
and Marcus, 1995), but determining sub-NP struc-
ture is rarely addressed. We could use multi-word
expressions (MWEs) to identify some structure. For
example, knowing stock market is a MWE may help
bracket stock market prices correctly, and Named
Entities (NEs) can be used the same way. However,
this only resolves NPs dominating MWEs or NEs.
Understanding base-NP structure is important,
since otherwise parsers will propose nonsensical
noun phrases like Force contract by default and pass
them onto downstream components. For example,
Question Answering (QA) systems need to supply
an NP as the answer to a factoid question, often us-
ing a parser to identify candidate NPs to return to
the user. If the parser never generates the correct
sub-NP structure, then the system may return a non-
sensical answer even though the correct dominating
noun phrase has been found.
Base-NP structure is also important for anno-
tated data derived from the Penn Treebank. For
instance, CCGbank (Hockenmaier, 2003) was cre-
ated by semi-automatically converting the Treebank
phrase structure to Combinatory Categorial Gram-
mar (CCG) (Steedman, 2000) derivations. Since CCG
derivations are binary branching, they cannot di-
rectly represent the flat structure of the Penn Tree-
bank base-NPs.
Without the correct bracketing in the Treebank,
strictly right-branching trees were created for all
base-NPs. This has an unwelcome effect when con-
junctions occur within an NP (Figure 1). An addi-
tional grammar rule is needed just to get a parse, but
it is still not correct (Hockenmaier, 2003, p. 64). The
awkward conversion results in bracketing (a) which
should be (b):
</bodyText>
<figure confidence="0.58785175">
(a) (consumer ((electronics) and
(appliances (retailing chain))))
(b) ((((consumer electronics) and
appliances) retailing) chain)
</figure>
<bodyText confidence="0.9945595">
We have previously experimented with using NEs to
improve parsing performance on CCGbank. Due to
the mis-alignment of NEs and right-branching NPs,
the increase in performance was negligible.
</bodyText>
<equation confidence="0.717080941176471">
N
N/N
consumer
N
N/N
electronics
N
conj
and
N
N/N
appliances
N
N/N
retailing
N
chain
</equation>
<figureCaption confidence="0.99699">
Figure 1: CCG derivation from Hockenmaier (2003)
</figureCaption>
<sectionHeader confidence="0.98868" genericHeader="method">
3 Background
</sectionHeader>
<bodyText confidence="0.939751666666667">
The NP bracketing task has often been posed in
terms of choosing between the left or right branch-
ing structure of three word noun compounds:
</bodyText>
<listItem confidence="0.8266975">
(a) (world (oil prices)) Right-branching
(b) ((crude oil) prices) Left-branching
</listItem>
<bodyText confidence="0.997666310344827">
Most approaches to the problem use unsupervised
methods, based on competing association strength
between two of the words in the compound (Mar-
cus, 1980, p. 253). There are two possible models
to choose from: dependency or adjacency. The de-
pendency model compares the association between
words 1-2 to words 1-3, while the adjacency model
compares words 1-2 to words 2-3.
Lauer (1995) has demonstrated superior perfor-
mance of the dependency model using a test set
of 244 (216 unique) noun compounds drawn from
Groliers encyclopedia. This data has been used to
evaluate most research since. He uses Rogets the-
saurus to smooth words into semantic classes, and
then calculates association between classes based
on their counts in a training set also drawn from
Groliers. He achieves 80.7% accuracy using POS
tags to indentify bigrams in the training set.
Lapata and Keller (2004) derive estimates from
web counts, and only compare at a lexical level,
achieving 78.7% accuracy. Nakov and Hearst (2005)
also use web counts, but incorporate additional
counts from several variations on simple bigram
queries, including queries for the pairs of words con-
catenated or joined by a hyphen. This results in an
impressive 89.3% accuracy.
There have also been attempts to solve this task
using supervised methods, even though the lack of
gold-standard data makes this difficult. Girju et al.
</bodyText>
<page confidence="0.996092">
241
</page>
<bodyText confidence="0.9963607">
\x0c(2005) draw a training set from raw WSJ text and use
it to train a decision tree classifier achieving 73.1%
accuracy. When they shuffled their data with Lauers
to create a new test and training split, their accu-
racy increased to 83.1% which may be a result of
the \x02 10% duplication in Lauers test set.
We have created a new NP bracketing data set
from our extended Treebank by extracting all right-
most three noun sequences from base-NPs. Our ini-
tial experiments are presented in Section 6.1.
</bodyText>
<sectionHeader confidence="0.993823" genericHeader="method">
4 Corpus Creation
</sectionHeader>
<bodyText confidence="0.9890555">
According to Marcus et al. (1993), asking annota-
tors to markup base-NP structure significantly re-
duced annotation speed, and for this reason base-
NPs were left flat. The bracketing guidelines (Bies
et al., 1995) also mention the considerable difficulty
of identifying the correct scope for nominal modi-
fiers. We found however, that while there are cer-
tainly difficult cases, the vast majority are quite sim-
ple and can be annotated reliably. Our annotation
philosophy can be summarised as:
</bodyText>
<listItem confidence="0.988442666666666">
1. most cases are easy and fit a common pattern;
2. prefer the implicit right-branching structure for
difficult decisions. Finance jargon was a com-
mon source of these;
3. mark very difficult to bracket NPs and discuss
with other annotators later;
</listItem>
<bodyText confidence="0.926668222222222">
During this process we identified numerous cases
that require a more sophisticated annotation scheme.
There are genuine flat cases, primarily names like
John A. Smith, that we would like to distinguish from
implicitly right-branching NPs in the next version of
the corpus. Although our scheme is still developing,
we believe that the current annotation is already use-
ful for statistical modelling, and we demonstrate this
empirically in Section 6.
</bodyText>
<subsectionHeader confidence="0.991118">
4.1 Annotation Process
</subsectionHeader>
<bodyText confidence="0.999091">
Our annotation guidelines1 are based on those de-
veloped for annotating full sub-NP structure in the
biomedical domain (Kulick et al., 2004). The anno-
tation guidelines for this biomedical corpus (an ad-
dendum to the Penn Treebank guidelines) introduce
the use of NML nodes to mark internal NP structure.
</bodyText>
<page confidence="0.892995">
1
</page>
<bodyText confidence="0.999831675675675">
The guidelines and corpus are available on our webpages.
In summary, our guidelines leave right-branching
structures untouched, and insert labelled brackets
around left-branching structures. The label of the
newly created constituent is NML or JJP, depending
on whether its head is a noun or an adjective. We
also chose not to alter the existing Penn Treebank
annotation, even though the annotators found many
errors during the annotation process. We wanted to
keep our extended Treebank as similar to the origi-
nal as possible, so that they remain comparable.
We developed a bracketing tool, which identifies
ambiguous NPs and presents them to the user for
disambiguation. An ambiguous NP is any (possibly
non-base) NP with three or more contiguous chil-
dren that are either single words or another NP. Cer-
tain common patterns, such as three words begin-
ning with a determiner, are unambiguous, and were
filtered out. The annotator is also shown the entire
sentence surrounding the ambiguous NP.
The bracketing tool often suggests a bracket-
ing using rules based mostly on named entity tags,
which are drawn from the BBN corpus (Weischedel
and Brunstein, 2005). For example, since Air Force
is given ORG tags, the tool suggests that they be
bracketed together first. Other suggestions come
from previous bracketings of the same words, which
helps to keep the annotator consistent.
Two post processes were carried out to increase
annotation consistency and correctness. 915 diffi-
cult NPs were marked by the annotator and were then
discussed with two other experts. Secondly, cer-
tain phrases that occurred numerous times and were
non-trivial to bracket, e.g. London Interbank Of-
fered Rate, were identified. An extra pass was made
through the corpus, ensuring that every instance of
these phrases was bracketed consistently.
</bodyText>
<subsectionHeader confidence="0.998065">
4.2 Annotation Time
</subsectionHeader>
<bodyText confidence="0.9996418">
Annotation initially took over 9 hours per section of
the Treebank. However, with practice this was re-
duced to about 3 hours per section. Each section
contains around 2500 ambiguous NPs, i.e. annotat-
ing took approximately 5 seconds per NP. Most NPs
require no bracketing, or fit into a standard pattern
which the annotator soon becomes accustomed to,
hence the task can be performed quite quickly.
For the original bracketing of the Treebank, anno-
tators performed at 375475 words per hour after a
</bodyText>
<page confidence="0.9991">
242
</page>
<table confidence="0.984927">
\x0cPREC. RECALL F-SCORE
Brackets 89.17 87.50 88.33
Dependencies 96.40 96.40 96.40
Brackets, revised 97.56 98.03 97.79
Dependencies, revised 99.27 99.27 99.27
</table>
<tableCaption confidence="0.995959">
Table 1: Agreement between annotators
</tableCaption>
<bodyText confidence="0.990602428571429">
few weeks, and increased to about 1000 words per
hour after gaining more experience (Marcus et al.,
1993). For our annotation process, counting each
word in every NP shown, our speed was around 800
words per hour. This figure is not unexpected, as the
task was not large enough to get more than a months
experience, and there is less structure to annotate.
</bodyText>
<sectionHeader confidence="0.8807765" genericHeader="method">
5 Corpus Analysis
5.1 Inter-annotator Agreement
</sectionHeader>
<bodyText confidence="0.997066555555556">
The annotation was performed by the first author.
A second Computational Linguistics PhD student
also annotated Section 23, allowing inter-annotator
agreement, and the reliability of the annotations, to
be measured. This also maximised the quality of the
section used for parser testing.
We measured the proportion of matching brack-
ets and dependencies between annotators, shown in
Table 1, both before and after they discussed cases
of disagreement and revised their annotations. The
number of dependencies is fixed by the length of the
NP, so the dependency precision and recall are the
same. Counting matched brackets is a harsher eval-
uation, as there are many NPs that both annotators
agree should have no additional bracketing, which
are not taken into account by the metric.
The disagreements occurred for a small number
of repeated instances, such as this case:
</bodyText>
<equation confidence="0.9863562">
(NP (NP (NNP Goldman)
(NML (NNP Goldman) (, ,)
(, ,) (NNP Sachs)
(NNP Sachs) ) (CC &amp;) (NNP Co) )
(CC &amp;) (NNP Co) )
</equation>
<bodyText confidence="0.995718875">
The first annotator felt that Goldman , Sachs
should form their own NML constituent, while the
second annotator did not.
We can also look at exact matching on NPs, where
the annotators originally agreed in 2667 of 2908
cases (91.71%), and after revision, in 2864 of 2907
cases (98.52%). These results demonstrate that high
agreement rates are achievable for these annotations.
</bodyText>
<table confidence="0.983203666666667">
MATCHED TOTAL %
By dependency 1409 (1315) 1479 95.27 (88.91)
By noun phrase 562 (489) 626 89.78 (78.12)
By dependency,
only annotated NPs
578 (543) 627 92.19 (86.60)
By noun phrase,
only annotated NPs
186 (162) 229 81.22 (70.74)
</table>
<tableCaption confidence="0.985278">
Table 2: Agreement with DepBank
</tableCaption>
<subsectionHeader confidence="0.996521">
5.2 DepBank Agreement
</subsectionHeader>
<bodyText confidence="0.999142352941176">
Another approach to measuring annotator reliabil-
ity is to compare with an independently annotated
corpus on the same text. We used the PARC700 De-
pendency Bank (King et al., 2003) which consists of
700 Section 23 sentences annotated with labelled de-
pendencies. We use the Briscoe and Carroll (2006)
version of DepBank, a 560 sentence subset used to
evaluate the RASP parser.
Some translation is required to compare our
brackets to DepBank dependencies. We map the
brackets to dependencies by finding the head of the
NP, using the Collins (1999) head finding rules,
and then creating a dependency between each other
childs head and this head. This does not work per-
fectly, and mismatches occur because of which de-
pendencies DepBank marks explicitly, and how it
chooses heads. The errors are investigated manually
to determine their cause.
The results are shown in Table 2, with the num-
ber of agreements before manual checking shown in
parentheses. Once again the dependency numbers
are higher than those at the NP level. Similarly, when
we only look at cases where we have inserted some
annotations, we are looking at more difficult cases
and the score is not as high.
The results of this analysis are quite positive.
Over half of the disagreements that occur (in ei-
ther measure) are caused by how company names
are bracketed. While we have always separated the
company name from post-modifiers such as Corp
and Inc, DepBank does not in most cases. These
results show that consistently and correctly bracket-
ing noun phrase structure is possible, and that inter-
annotator agreement is at an acceptable level.
</bodyText>
<subsectionHeader confidence="0.999329">
5.3 Corpus Composition and Consistency
</subsectionHeader>
<bodyText confidence="0.995908">
Looking at the entire Penn Treebank corpus, the
annotation tool finds 60959 ambiguous NPs out of
the 432639 NPs in the corpus (14.09%). 22851 of
</bodyText>
<page confidence="0.997253">
243
</page>
<table confidence="0.99443525">
\x0cLEVEL COUNT POS TAGS EXAMPLE
1073 JJ JJ NNS big red cars
1581 DT JJ NN NN a high interest rate
NP
1693 JJ NN NNS high interest rates
3557 NNP NNP NNP John A. Smith
1468 NN NN (interest rate) rises
1538 JJ NN (heavy truck) rentals
NML
1650 NNP NNP NNP (A. B. C.) Corp
8524 NNP NNP (John Smith) Jr.
82 JJ JJ (dark red) car
114 RB JJ (very high) rates
JJP
122 JJ CC JJ (big and red) apples
160 JJ ( smart ) cars
</table>
<tableCaption confidence="0.793477">
Table 3: Common POS tag sequences
these (37.49%) had brackets inserted by the annota-
</tableCaption>
<bodyText confidence="0.997373419354839">
tor. This is as we expect, as the majority of NPs are
right-branching. Of the brackets added, 22368 were
NML nodes, while 863 were JJP.
To compare, we can count the number of existing
NP and ADJP nodes found in the NPs that the brack-
eting tool presents. We find there are 32772 NP chil-
dren, and 579 ADJP, which are quite similar num-
bers to the amount of nodes we have added. From
this, we can say that our annotation process has in-
troduced almost as much structural information into
NPs as there was in the original Penn Treebank.
Table 3 shows the most common POS tag se-
quences for NP, NML and JJP nodes. An example
is given showing typical words that match the POS
tags. For NML and JJP, we also show the words
bracketed, as they would appear under an NP node.
We checked the consistency of the annotations by
identifying NPs with the same word sequence and
checking whether they were always bracketed iden-
tically. After the first pass through, there were 360
word sequences with multiple bracketings, which
occurred in 1923 NP instances. 489 of these in-
stances differed from the majority case for that se-
quence, and were probably errors.
The annotator had marked certain difficult and
commonly repeating NPs. From this we generated a
list of phrases, and then made another pass through
the corpus, synchronising all instances that con-
tained one of these phrases. After this, only 150 in-
stances differed from the majority case. Inspecting
these remaining inconsistencies showed cases like:
</bodyText>
<equation confidence="0.99359">
(NP-TMP (NML (NNP Nov.) (CD 15))
(, ,)
(CD 1999))
</equation>
<bodyText confidence="0.999561695652174">
where we were inconsistent in inserting the NML node
because the Penn Treebank sometimes already has
the structure annotated under an NP node. Since we
do not make changes to existing brackets, we cannot
fix these cases. Other inconsistencies are rare, but
will be examined and corrected in a future release.
The annotator made a second pass over Section
00 to correct changes made after the beginning of
the annotation process. Comparing the two passes
can give us some idea of how the annotator changed
as he grew more practiced at the task.
We find that the old and new versions are identi-
cal in 88.65% of NPs, with labelled precision, recall
and F-score being 97.17%, 76.69% and 85.72% re-
spectively. This tells us that there were many brack-
ets originally missed that were added in the second
pass. This is not surprising since the main problem
with how Section 00 was annotated originally was
that company names were not separated from their
post-modifier (such as Corp). Besides this, it sug-
gests that there is not a great deal of difference be-
tween an annotator just learning the task, and one
who has had a great deal of experience with it.
</bodyText>
<subsectionHeader confidence="0.994325">
5.4 Named Entity Suggestions
</subsectionHeader>
<bodyText confidence="0.998156071428571">
We have also evaluated how well the suggestion fea-
ture of the annotation tool performs. In particular,
we want to determine how useful named entities are
in determining the correct bracketing.
We ran the tool over the original corpus, follow-
ing NE-based suggestions where possible. We find
that when evaluated against our annotations, the F-
score is 50.71%. We need to look closer at the pre-
cision and recall though, as they are quite different.
The precision of 93.84% is quite high. However,
there are many brackets where the entities do not
help at all, and so the recall of this method was only
34.74%. This suggests that a NE feature may help to
identify the correct bracketing in one third of cases.
</bodyText>
<sectionHeader confidence="0.998554" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.998200333333333">
Having bracketed NPs in the Penn Treebank, we now
describe our initial experiments on how this addi-
tional level of annotation can be exploited.
</bodyText>
<subsectionHeader confidence="0.999841">
6.1 NP Bracketing Data
</subsectionHeader>
<bodyText confidence="0.996148">
The obvious first task to consider is noun phrase
bracketing itself. We implement a similar system to
</bodyText>
<page confidence="0.999249">
244
</page>
<table confidence="0.99779">
\x0cCORPUS # ITEMS LEFT RIGHT
Penn Treebank 5582 58.99% 41.01%
Lauers 244 66.80% 33.20%
</table>
<tableCaption confidence="0.996598">
Table 4: Comparison of NP bracketing corpora
</tableCaption>
<table confidence="0.973095">
N-GRAM MATCH
Unigrams 51.20%
Adjacency bigrams 6.35%
Dependency bigrams 3.85%
All bigrams 5.83%
Trigrams 1.40%
</table>
<tableCaption confidence="0.989967">
Table 5: Lexical overlap
</tableCaption>
<bodyText confidence="0.999529863636364">
Lauer (1995), described in Section 3, and report on
results from our own data and Lauers original set.
First, we extracted three word noun sequences
from all the ambiguous NPs. If the last three chil-
dren are nouns, then they became an example in our
data set. If there is a NML node containing the first
two nouns then it is left-branching, otherwise it is
right-branching. Because we are only looking at the
right-most part of the NP, we know that we are not
extracting any nonsensical items. We also remove
all items where the nouns are all part of a named
entity to eliminate flat structure cases.
Statistics about the new data set and Lauers data
set are given in Table 4. As can be seen, the Penn
Treebank based corpus is significantly larger, and
has a more even mix of left and right-branching noun
phrases. We also measured the amount of lexical
overlap between the two corpora, shown in Table 5.
This displays the percentage of n-grams in Lauers
corpus that are also in our corpus. We can clearly
see that the two corpora are quite dissimilar, as even
on unigrams barely half are shared.
</bodyText>
<subsectionHeader confidence="0.999382">
6.2 NP Bracketing Results
</subsectionHeader>
<bodyText confidence="0.9857147">
With our new data set, we began running experi-
ments similar to those carried out in the literature
(Nakov and Hearst, 2005). We implemented both an
adjacency and dependency model, and three differ-
ent association measures: raw counts, bigram proba-
bility, and \x03\x05\x04 . We draw our counts from a corpus of
n-gram counts calculated over 1 trillion words from
the web (Brants and Franz, 2006).
The results from the experiments, on both our and
Lauers data set, are shown in Table 6. Our results
</bodyText>
<table confidence="0.860332857142857">
ASSOC. MEASURE LAUER PTB
Raw counts, adj. 75.41% 77.46%
Raw counts, dep. 77.05% 68.85%
Probability, adj. 71.31% 76.42%
Probability, dep. 80.33% 69.56%
\x03\x06\x04 , adj. 71.31% 77.93%
\x03 \x04 , dep. 74.59% 68.92%
</table>
<tableCaption confidence="0.945094">
Table 6: Bracketing task, unsupervised results
</tableCaption>
<table confidence="0.99916825">
FEATURES LAUER 10-FOLD CROSS
All features 80.74% 89.91% (1.04%)
Lexical 71.31% 84.52% (1.77%)
n-gram counts 75.41% 82.50% (1.49%)
Probability 72.54% 78.34% (2.11%)
\x07\t\x08 75.41% 80.10% (1.71%)
Adjacency model 72.95% 79.52% (1.32%)
Dependency model 78.69% 72.86% (1.48%)
Both models 76.23% 79.67% (1.42%)
-Lexical 79.92% 85.72% (0.77%)
-n-gram counts 80.74% 89.11% (1.39%)
-Probability 79.10% 89.79% (1.22%)
-\x07\t\x08 80.74% 89.79% (0.98%)
-Adjacency model 81.56% 89.63% (0.96%)
-Dependency model 81.15% 89.72% (0.86%)
-Both models 81.97% 89.63% (0.95%)
</table>
<tableCaption confidence="0.998706">
Table 7: Bracketing task, supervised results
</tableCaption>
<bodyText confidence="0.98665376">
on Lauers corpus are similar to those reported pre-
viously, with the dependency model outperforming
the adjacency model on all measures. The bigram
probability scores highest out of all the measures,
while the \x03 \x04 score performed the worst.
The results on the new corpus are even more sur-
prising, with the adjacency model outperforming the
dependency model by a wide margin. The \x03
\x04 mea-
sure gives the highest accuracy, but still only just
outperforms the raw counts. Our analysis shows
that the good performance of the adjacency model
comes from the large number of named entities in
the corpus. When we remove all items that have any
word as an entity, the results change, and the de-
pendency model is superior. We also suspect that
another cause of the unusual results is the different
proportions of left and right-branching NPs.
With a large annotated corpus, we can now run
supervised NP bracketing experiments. We present
two configurations in Table 7: training on our corpus
and testing on Lauers set; and performing 10-fold
cross validation using our corpus alone.
The feature set we explore encodes the informa-
tion we used in the unsupervised experiments. Ta-
</bodyText>
<page confidence="0.998718">
245
</page>
<table confidence="0.9915298">
\x0cOVERALL ONLY NML JJP NOT NML JJP
PREC. RECALL F-SCORE PREC. RECALL F-SCORE PREC. RECALL F-SCORE
Original 88.93 88.90 88.92 88.93 88.90 88.92
NML and JJP bracketed 88.63 88.29 88.46 77.93 62.93 69.63 88.85 88.93 88.89
Relabelled brackets 88.17 87.88 88.02 91.93 51.38 65.91 87.86 88.65 88.25
</table>
<tableCaption confidence="0.99868">
Table 8: Parsing performance
</tableCaption>
<bodyText confidence="0.998580541666667">
ble 7 shows the performance with: all features, fol-
lowed by the individual features, and finally, after
removing individual features.
The feature set includes: lexical features for each
n-gram in the noun compound; n-gram counts for
unigrams, bigrams and trigrams; raw probability and
\x03\x06\x04 association scores for all three bigrams in the
compound; and the adjacency and dependency re-
sults for all three association measures. We dis-
cretised the non-binary features using an implemen-
tation of Fayyad and Iranis (1993) algorithm, and
classify using MegaM2.
The results on Lauers set demonstrate that the
dependency model performs well by itself but not
with the other features. In fact, a better result comes
from using every feature except those from the de-
pendency and adjacency models. It is also impres-
sive how good the performance is, considering the
large differences between our data set and Lauers.
These differences also account for the disparate
cross-validation figures. On this data, the lexical fea-
tures perform the best, which is to be expected given
the nature of the corpus. The best model in this case
comes from using all the features.
</bodyText>
<subsectionHeader confidence="0.999397">
6.3 Collins Parsing
</subsectionHeader>
<bodyText confidence="0.997640071428571">
We can also look at the impact of our new annota-
tions upon full statistical parsing. We use Bikels
implementation (Bikel, 2004) of Collins parser
(Collins, 1999) in order to carry out these experi-
ments, using the non-deficient Collins settings. The
numbers we give are labelled bracket precision, re-
call and F-scores for all sentences. Bikel mentions
that base-NPs are treated very differently in Collins
parser, and so it will be interesting to observe the
results using our new annotations.
Firstly, we compare the parsers performance on
the original Penn Treebank and the new NML and JJP
bracketed version. Table 8 shows that the new brack-
ets make parsing marginally more difficult overall
</bodyText>
<page confidence="0.808018">
2
</page>
<bodyText confidence="0.983531571428571">
Available at http://www.cs.utah.edu/ hal/megam/
(by about 0.5% in F-score).
The performance on only the new NML and JJP
brackets is not very high. This shows the difficulty
of correctly bracketing NPs. Conversely, the figures
for all brackets except NML and JJP are only a tiny
amount less in our extended corpus. This means that
performance for other phrases is hardly changed by
the new NP brackets.
We also ran an experiment where the new NML and
JJP labels were relabelled as NP and ADJP. These
are the labels that would be given if NPs were orig-
inally bracketed with the rest of the Penn Treebank.
This meant the model would not have to discrim-
inate between two different types of noun and ad-
jective structure. The performance, as shown in Ta-
ble 8, was even lower with this approach, suggesting
that the distinction is larger than we anticipated. On
the other hand, the precision on NML and JJP con-
stituents was quite high, so the parser is able to iden-
tify at least some of the structure very well.
</bodyText>
<sectionHeader confidence="0.997121" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999852157894737">
The work presented in this paper is a first step to-
wards accurate representation of noun phrase struc-
ture in NLP corpora. There are several distinctions
that our annotation currently ignores that we would
like to identify correctly in the future. Firstly, NPs
with genuine flat structure are currently treated as
implicitly right branching. Secondly, there is still
ambiguity in determining the head of a noun phrase.
Although Collins head finding rules work in most
NPs, there are cases such as IBM Australia where
the head is not the right-most noun. Similarly, ap-
position is very common in the Penn Treebank, in
NPs such as John Smith , IBM president. We would
like to be able to identify these multi-head constructs
properly, rather than simply treating them as a single
entity (or even worse, as two different entities).
Having the correct NP structure also means that
we can now represent the true structure in CCGbank,
one of the problems we described earlier. Transfer-
</bodyText>
<page confidence="0.98828">
246
</page>
<bodyText confidence="0.999060357142857">
\x0cring our annotations should be fairly simple, requir-
ing just a few changes to how NPs are treated in the
current translation process.
The addition of consistent, gold-standard, noun
phrase structure to a large corpus is a significant
achievement. We have shown that the these anno-
tations can be created in a feasible time frame with
high inter-annotator agreement of 98.52% (measur-
ing exact NP matches). The new brackets cause only
a small drop in parsing performance, and no signifi-
cant decrease on the existing structure. As NEs were
useful for suggesting brackets automatically, we in-
tend to incorporate NE information into statistical
parsing models in the future.
Our annotated corpus can improve the perfor-
mance of any system that relies on NPs from parsers
trained on the Penn Treebank. A Collins parser
trained on our corpus is now able to identify sub-
NP brackets, making it of use in other NLP systems.
QA systems, for example, will be able to exploit in-
ternal NP structure. In the future, we will improve
the parsers performance on NML and JJP brackets.
We have provided a significantly larger corpus
for analysing NP structure than has ever been made
available before. This is integrated within perhaps
the most influential corpus in NLP. The large num-
ber of systems trained on Penn Treebank data can all
benefit from the extended resource we have created.
</bodyText>
<sectionHeader confidence="0.962606" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.92571125">
We would like to thank Matthew Honnibal, our sec-
ond annotator, who also helped design the guide-
lines; Toby Hawker, for implementing the dis-
cretiser; Mark Lauer for releasing his data; and
the anonymous reviewers for their helpful feed-
back. This work has been supported by the Aus-
tralian Research Council under Discovery Projects
DP0453131 and DP0665973.
</bodyText>
<sectionHeader confidence="0.925809" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999418696969697">
Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre.
1995. Bracketing guidelines for Treebank II style Penn Tree-
bank project. Technical report, University of Pennsylvania.
Dan Bikel. 2004. On the Parameter Space of Generative Lexi-
calized Statistical Parsing Models. Ph.D. thesis, University
of Pennsylvania.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram version
1. Linguistic Data Consortium.
Ted Briscoe and John Carroll. 2006. Evaluating the accuracy
of an unlexicalized statistical parser on the PARC DepBank.
In Proceedings of the Poster Session of COLING/ACL-06.
Sydney, Australia.
Michael Collins. 1999. Head-Driven Statistical Models for Nat-
ural Language Parsing. Ph.D. thesis, University of Pennsyl-
vania.
Usama M. Fayyad and Keki B. Irani. 1993. Multi-interval dis-
cretization of continuous-valued attributes for classification
learning. In Proceedings of the 13th International Joint Con-
ference on Artifical Intelligence (IJCAI93), pages 1022
1029. Chambery, France.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe.
2005. On the semantics of noun compounds. Journal of
Computer Speech and Language - Special Issue on Multi-
word Expressions, 19(4):313330.
Julia Hockenmaier. 2003. Data and Models for Statistical Pars-
ing with Combinatory Categorial Grammar. Ph.D. thesis,
University of Edinburgh.
Tracy Holloway King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC700
dependency bank. In Proceedings of the 4th International
Workshop on Linguistically Interpreted Corpora (LINC-03).
Budapest, Hungary.
Seth Kulick, Ann Bies, Mark Libeman, Mark Mandel, Ryan
McDonald, Martha Palmer, Andrew Schein, and Lyle Ungar.
2004. Integrated annotation for biomedical information ex-
traction. In Proceedings of the Human Language Technology
Conference of the North American Chapter of the Associa-
tion for Computational Linguistics. Boston.
Mirella Lapata and Frank Keller. 2004. The web as a base-
line: Evaluating the performance of unsupervised web-based
models for a range of NLP tasks. In Proceedings of the Hu-
man Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Linguis-
tics, pages 121128. Boston.
Mark Lauer. 1995. Corpus statistics meet the compound noun:
Some empirical results. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguistics.
Cambridge, MA.
Mitchell Marcus. 1980. A Theory of Syntactic Recognition for
Natural Language. MIT Press, Cambridge, MA.
Mitchell Marcus, Beatrice Santorini, and Mary Marcinkiewicz.
1993. Building a large annotated corpus of English: The
Penn Treebank. Computational Linguistics, 19(2):313330.
Preslav Nakov and Marti Hearst. 2005. Search engine statistics
beyond the n-gram: Application to noun compound brack-
eting. In Proceedings of CoNLL-2005, Ninth Conference on
Computational Natural Language Learning. Ann Arbor, MI.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In Proceedings of
the Third ACL Workshop on Very Large Corpora. Cambridge
MA, USA.
Mark Steedman. 2000. The Syntactic Process. MIT Press, Cam-
bridge, MA.
Ralph Weischedel and Ada Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report, Lin-
guistic Data Consortium.
</reference>
<page confidence="0.97926">
247
</page>
<figure confidence="0.252228">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.467287">
<note confidence="0.970264666666667">b&amp;apos;Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 240247, Prague, Czech Republic, June 2007. c 2007 Association for Computational Linguistics</note>
<title confidence="0.63989">Adding Noun Phrase Structure to the Penn Treebank</title>
<author confidence="0.999898">David Vadas</author>
<author confidence="0.999898">James R Curran</author>
<affiliation confidence="0.9996265">School of Information Technologies University of Sydney</affiliation>
<address confidence="0.996451">NSW 2006, Australia</address>
<email confidence="0.957378">dvadas1,james\x01@it.usyd.edu.au</email>
<abstract confidence="0.978356">The Penn Treebank does not annotate within base noun phrases (NPs), committing only to flat structures that ignore the complexity of English NPs. This means that tools trained on Treebank data cannot learn the correct internal structure of NPs. This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank. We then examine the consistency and reliability of our annotations. Finally, we use this resource to determine NP structure using several statistical approaches, thus demonstrating the utility of the corpus. This adds detail to the Penn Treebank that is necessary for many NLP applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Robert MacIntyre</author>
</authors>
<title>Bracketing guidelines for Treebank II style Penn Treebank project.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="7637" citStr="Bies et al., 1995" startWordPosition="1215" endWordPosition="1218">1% accuracy. When they shuffled their data with Lauers to create a new test and training split, their accuracy increased to 83.1% which may be a result of the \x02 10% duplication in Lauers test set. We have created a new NP bracketing data set from our extended Treebank by extracting all rightmost three noun sequences from base-NPs. Our initial experiments are presented in Section 6.1. 4 Corpus Creation According to Marcus et al. (1993), asking annotators to markup base-NP structure significantly reduced annotation speed, and for this reason baseNPs were left flat. The bracketing guidelines (Bies et al., 1995) also mention the considerable difficulty of identifying the correct scope for nominal modifiers. We found however, that while there are certainly difficult cases, the vast majority are quite simple and can be annotated reliably. Our annotation philosophy can be summarised as: 1. most cases are easy and fit a common pattern; 2. prefer the implicit right-branching structure for difficult decisions. Finance jargon was a common source of these; 3. mark very difficult to bracket NPs and discuss with other annotators later; During this process we identified numerous cases that require a more sophis</context>
</contexts>
<marker>Bies, Ferguson, Katz, MacIntyre, 1995</marker>
<rawString>Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre. 1995. Bracketing guidelines for Treebank II style Penn Treebank project. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Bikel</author>
</authors>
<title>On the Parameter Space of Generative Lexicalized Statistical Parsing Models.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="25090" citStr="Bikel, 2004" startWordPosition="4119" endWordPosition="4120">better result comes from using every feature except those from the dependency and adjacency models. It is also impressive how good the performance is, considering the large differences between our data set and Lauers. These differences also account for the disparate cross-validation figures. On this data, the lexical features perform the best, which is to be expected given the nature of the corpus. The best model in this case comes from using all the features. 6.3 Collins Parsing We can also look at the impact of our new annotations upon full statistical parsing. We use Bikels implementation (Bikel, 2004) of Collins parser (Collins, 1999) in order to carry out these experiments, using the non-deficient Collins settings. The numbers we give are labelled bracket precision, recall and F-scores for all sentences. Bikel mentions that base-NPs are treated very differently in Collins parser, and so it will be interesting to observe the results using our new annotations. Firstly, we compare the parsers performance on the original Penn Treebank and the new NML and JJP bracketed version. Table 8 shows that the new brackets make parsing marginally more difficult overall 2 Available at http://www.cs.utah.</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Dan Bikel. 2004. On the Parameter Space of Generative Lexicalized Statistical Parsing Models. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram version 1. Linguistic Data Consortium.</title>
<date>2006</date>
<contexts>
<context position="21309" citStr="Brants and Franz, 2006" startWordPosition="3521" endWordPosition="3524">in Table 5. This displays the percentage of n-grams in Lauers corpus that are also in our corpus. We can clearly see that the two corpora are quite dissimilar, as even on unigrams barely half are shared. 6.2 NP Bracketing Results With our new data set, we began running experiments similar to those carried out in the literature (Nakov and Hearst, 2005). We implemented both an adjacency and dependency model, and three different association measures: raw counts, bigram probability, and \x03\x05\x04 . We draw our counts from a corpus of n-gram counts calculated over 1 trillion words from the web (Brants and Franz, 2006). The results from the experiments, on both our and Lauers data set, are shown in Table 6. Our results ASSOC. MEASURE LAUER PTB Raw counts, adj. 75.41% 77.46% Raw counts, dep. 77.05% 68.85% Probability, adj. 71.31% 76.42% Probability, dep. 80.33% 69.56% \x03\x06\x04 , adj. 71.31% 77.93% \x03 \x04 , dep. 74.59% 68.92% Table 6: Bracketing task, unsupervised results FEATURES LAUER 10-FOLD CROSS All features 80.74% 89.91% (1.04%) Lexical 71.31% 84.52% (1.77%) n-gram counts 75.41% 82.50% (1.49%) Probability 72.54% 78.34% (2.11%) \x07\t\x08 75.41% 80.10% (1.71%) Adjacency model 72.95% 79.52% (1.32%)</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram version 1. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank.</title>
<date>2006</date>
<booktitle>In Proceedings of the Poster Session of COLING/ACL-06.</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="13820" citStr="Briscoe and Carroll (2006)" startWordPosition="2213" endWordPosition="2216">t rates are achievable for these annotations. MATCHED TOTAL % By dependency 1409 (1315) 1479 95.27 (88.91) By noun phrase 562 (489) 626 89.78 (78.12) By dependency, only annotated NPs 578 (543) 627 92.19 (86.60) By noun phrase, only annotated NPs 186 (162) 229 81.22 (70.74) Table 2: Agreement with DepBank 5.2 DepBank Agreement Another approach to measuring annotator reliability is to compare with an independently annotated corpus on the same text. We used the PARC700 Dependency Bank (King et al., 2003) which consists of 700 Section 23 sentences annotated with labelled dependencies. We use the Briscoe and Carroll (2006) version of DepBank, a 560 sentence subset used to evaluate the RASP parser. Some translation is required to compare our brackets to DepBank dependencies. We map the brackets to dependencies by finding the head of the NP, using the Collins (1999) head finding rules, and then creating a dependency between each other childs head and this head. This does not work perfectly, and mismatches occur because of which dependencies DepBank marks explicitly, and how it chooses heads. The errors are investigated manually to determine their cause. The results are shown in Table 2, with the number of agreeme</context>
</contexts>
<marker>Briscoe, Carroll, 2006</marker>
<rawString>Ted Briscoe and John Carroll. 2006. Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank. In Proceedings of the Poster Session of COLING/ACL-06. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2935" citStr="Collins, 1999" startWordPosition="466" endWordPosition="467">nnotation guidelines and process, including the use of named entity data to improve annotation quality. We check the correctness of the corpus by measuring interannotator agreement, by reannotating the first section, and by comparing against the sub-NP structure in DepBank (King et al., 2003). We also give an analysis of our extended Treebank, quantifying how much structure we have added, and how it is distributed across NPs. Finally, we test the utility of the extended Treebank for training statistical models on two tasks: NP bracketing (Lauer, 1995; Nakov and Hearst, 2005) and full parsing (Collins, 1999). This new resource will allow any system or annotated corpus developed from the Penn Treebank, to represent noun phrase structure more accurately. 240 \x0c2 Motivation Many approaches to identifying base noun phrases have been explored as part of chunking (Ramshaw and Marcus, 1995), but determining sub-NP structure is rarely addressed. We could use multi-word expressions (MWEs) to identify some structure. For example, knowing stock market is a MWE may help bracket stock market prices correctly, and Named Entities (NEs) can be used the same way. However, this only resolves NPs dominating MWEs </context>
<context position="14066" citStr="Collins (1999)" startWordPosition="2256" endWordPosition="2257">29 81.22 (70.74) Table 2: Agreement with DepBank 5.2 DepBank Agreement Another approach to measuring annotator reliability is to compare with an independently annotated corpus on the same text. We used the PARC700 Dependency Bank (King et al., 2003) which consists of 700 Section 23 sentences annotated with labelled dependencies. We use the Briscoe and Carroll (2006) version of DepBank, a 560 sentence subset used to evaluate the RASP parser. Some translation is required to compare our brackets to DepBank dependencies. We map the brackets to dependencies by finding the head of the NP, using the Collins (1999) head finding rules, and then creating a dependency between each other childs head and this head. This does not work perfectly, and mismatches occur because of which dependencies DepBank marks explicitly, and how it chooses heads. The errors are investigated manually to determine their cause. The results are shown in Table 2, with the number of agreements before manual checking shown in parentheses. Once again the dependency numbers are higher than those at the NP level. Similarly, when we only look at cases where we have inserted some annotations, we are looking at more difficult cases and th</context>
<context position="25124" citStr="Collins, 1999" startWordPosition="4124" endWordPosition="4125">very feature except those from the dependency and adjacency models. It is also impressive how good the performance is, considering the large differences between our data set and Lauers. These differences also account for the disparate cross-validation figures. On this data, the lexical features perform the best, which is to be expected given the nature of the corpus. The best model in this case comes from using all the features. 6.3 Collins Parsing We can also look at the impact of our new annotations upon full statistical parsing. We use Bikels implementation (Bikel, 2004) of Collins parser (Collins, 1999) in order to carry out these experiments, using the non-deficient Collins settings. The numbers we give are labelled bracket precision, recall and F-scores for all sentences. Bikel mentions that base-NPs are treated very differently in Collins parser, and so it will be interesting to observe the results using our new annotations. Firstly, we compare the parsers performance on the original Penn Treebank and the new NML and JJP bracketed version. Table 8 shows that the new brackets make parsing marginally more difficult overall 2 Available at http://www.cs.utah.edu/ hal/megam/ (by about 0.5% in </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Usama M Fayyad</author>
<author>Keki B Irani</author>
</authors>
<title>Multi-interval discretization of continuous-valued attributes for classification learning.</title>
<date>1993</date>
<booktitle>In Proceedings of the 13th International Joint Conference on Artifical Intelligence (IJCAI93),</booktitle>
<pages>1022--1029</pages>
<location>Chambery, France.</location>
<marker>Fayyad, Irani, 1993</marker>
<rawString>Usama M. Fayyad and Keki B. Irani. 1993. Multi-interval discretization of continuous-valued attributes for classification learning. In Proceedings of the 13th International Joint Conference on Artifical Intelligence (IJCAI93), pages 1022 1029. Chambery, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Dan Moldovan</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
</authors>
<title>On the semantics of noun compounds.</title>
<date>2005</date>
<journal>Journal of Computer Speech and Language - Special Issue on Multiword Expressions,</journal>
<volume>19</volume>
<issue>4</issue>
<marker>Girju, Moldovan, Tatu, Antohe, 2005</marker>
<rawString>Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe. 2005. On the semantics of noun compounds. Journal of Computer Speech and Language - Special Issue on Multiword Expressions, 19(4):313330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="4196" citStr="Hockenmaier, 2003" startWordPosition="665" endWordPosition="666">important, since otherwise parsers will propose nonsensical noun phrases like Force contract by default and pass them onto downstream components. For example, Question Answering (QA) systems need to supply an NP as the answer to a factoid question, often using a parser to identify candidate NPs to return to the user. If the parser never generates the correct sub-NP structure, then the system may return a nonsensical answer even though the correct dominating noun phrase has been found. Base-NP structure is also important for annotated data derived from the Penn Treebank. For instance, CCGbank (Hockenmaier, 2003) was created by semi-automatically converting the Treebank phrase structure to Combinatory Categorial Grammar (CCG) (Steedman, 2000) derivations. Since CCG derivations are binary branching, they cannot directly represent the flat structure of the Penn Treebank base-NPs. Without the correct bracketing in the Treebank, strictly right-branching trees were created for all base-NPs. This has an unwelcome effect when conjunctions occur within an NP (Figure 1). An additional grammar rule is needed just to get a parse, but it is still not correct (Hockenmaier, 2003, p. 64). The awkward conversion resu</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and Models for Statistical Parsing with Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tracy Holloway King</author>
<author>Richard Crouch</author>
<author>Stefan Riezler</author>
<author>Mary Dalrymple</author>
<author>Ronald M Kaplan</author>
</authors>
<title>The PARC700 dependency bank.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC-03).</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="2614" citStr="King et al., 2003" startWordPosition="409" endWordPosition="412">ce together is a nominal modifier of contract. Adding this annotation better represents the true syntactic and semantic structure, which will improve the performance of downstream NLP systems. Our main contribution is a gold-standard labelled bracketing for every ambiguous noun phrase in the Penn Treebank. We describe the annotation guidelines and process, including the use of named entity data to improve annotation quality. We check the correctness of the corpus by measuring interannotator agreement, by reannotating the first section, and by comparing against the sub-NP structure in DepBank (King et al., 2003). We also give an analysis of our extended Treebank, quantifying how much structure we have added, and how it is distributed across NPs. Finally, we test the utility of the extended Treebank for training statistical models on two tasks: NP bracketing (Lauer, 1995; Nakov and Hearst, 2005) and full parsing (Collins, 1999). This new resource will allow any system or annotated corpus developed from the Penn Treebank, to represent noun phrase structure more accurately. 240 \x0c2 Motivation Many approaches to identifying base noun phrases have been explored as part of chunking (Ramshaw and Marcus, 1</context>
<context position="13701" citStr="King et al., 2003" startWordPosition="2194" endWordPosition="2197">ases (91.71%), and after revision, in 2864 of 2907 cases (98.52%). These results demonstrate that high agreement rates are achievable for these annotations. MATCHED TOTAL % By dependency 1409 (1315) 1479 95.27 (88.91) By noun phrase 562 (489) 626 89.78 (78.12) By dependency, only annotated NPs 578 (543) 627 92.19 (86.60) By noun phrase, only annotated NPs 186 (162) 229 81.22 (70.74) Table 2: Agreement with DepBank 5.2 DepBank Agreement Another approach to measuring annotator reliability is to compare with an independently annotated corpus on the same text. We used the PARC700 Dependency Bank (King et al., 2003) which consists of 700 Section 23 sentences annotated with labelled dependencies. We use the Briscoe and Carroll (2006) version of DepBank, a 560 sentence subset used to evaluate the RASP parser. Some translation is required to compare our brackets to DepBank dependencies. We map the brackets to dependencies by finding the head of the NP, using the Collins (1999) head finding rules, and then creating a dependency between each other childs head and this head. This does not work perfectly, and mismatches occur because of which dependencies DepBank marks explicitly, and how it chooses heads. The </context>
</contexts>
<marker>King, Crouch, Riezler, Dalrymple, Kaplan, 2003</marker>
<rawString>Tracy Holloway King, Richard Crouch, Stefan Riezler, Mary Dalrymple, and Ronald M. Kaplan. 2003. The PARC700 dependency bank. In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC-03). Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seth Kulick</author>
<author>Ann Bies</author>
<author>Mark Libeman</author>
<author>Mark Mandel</author>
<author>Ryan McDonald</author>
<author>Martha Palmer</author>
<author>Andrew Schein</author>
<author>Lyle Ungar</author>
</authors>
<title>Integrated annotation for biomedical information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<location>Boston.</location>
<contexts>
<context position="8772" citStr="Kulick et al., 2004" startWordPosition="1394" endWordPosition="1397">rs later; During this process we identified numerous cases that require a more sophisticated annotation scheme. There are genuine flat cases, primarily names like John A. Smith, that we would like to distinguish from implicitly right-branching NPs in the next version of the corpus. Although our scheme is still developing, we believe that the current annotation is already useful for statistical modelling, and we demonstrate this empirically in Section 6. 4.1 Annotation Process Our annotation guidelines1 are based on those developed for annotating full sub-NP structure in the biomedical domain (Kulick et al., 2004). The annotation guidelines for this biomedical corpus (an addendum to the Penn Treebank guidelines) introduce the use of NML nodes to mark internal NP structure. 1 The guidelines and corpus are available on our webpages. In summary, our guidelines leave right-branching structures untouched, and insert labelled brackets around left-branching structures. The label of the newly created constituent is NML or JJP, depending on whether its head is a noun or an adjective. We also chose not to alter the existing Penn Treebank annotation, even though the annotators found many errors during the annotat</context>
</contexts>
<marker>Kulick, Bies, Libeman, Mandel, McDonald, Palmer, Schein, Ungar, 2004</marker>
<rawString>Seth Kulick, Ann Bies, Mark Libeman, Mark Mandel, Ryan McDonald, Martha Palmer, Andrew Schein, and Lyle Ungar. 2004. Integrated annotation for biomedical information extraction. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics. Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>The web as a baseline: Evaluating the performance of unsupervised web-based models for a range of NLP tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>121128</pages>
<location>Boston.</location>
<contexts>
<context position="6403" citStr="Lapata and Keller (2004)" startWordPosition="1009" endWordPosition="1012">del compares the association between words 1-2 to words 1-3, while the adjacency model compares words 1-2 to words 2-3. Lauer (1995) has demonstrated superior performance of the dependency model using a test set of 244 (216 unique) noun compounds drawn from Groliers encyclopedia. This data has been used to evaluate most research since. He uses Rogets thesaurus to smooth words into semantic classes, and then calculates association between classes based on their counts in a training set also drawn from Groliers. He achieves 80.7% accuracy using POS tags to indentify bigrams in the training set. Lapata and Keller (2004) derive estimates from web counts, and only compare at a lexical level, achieving 78.7% accuracy. Nakov and Hearst (2005) also use web counts, but incorporate additional counts from several variations on simple bigram queries, including queries for the pairs of words concatenated or joined by a hyphen. This results in an impressive 89.3% accuracy. There have also been attempts to solve this task using supervised methods, even though the lack of gold-standard data makes this difficult. Girju et al. 241 \x0c(2005) draw a training set from raw WSJ text and use it to train a decision tree classifi</context>
</contexts>
<marker>Lapata, Keller, 2004</marker>
<rawString>Mirella Lapata and Frank Keller. 2004. The web as a baseline: Evaluating the performance of unsupervised web-based models for a range of NLP tasks. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 121128. Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Corpus statistics meet the compound noun: Some empirical results.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="2877" citStr="Lauer, 1995" startWordPosition="457" endWordPosition="458">ous noun phrase in the Penn Treebank. We describe the annotation guidelines and process, including the use of named entity data to improve annotation quality. We check the correctness of the corpus by measuring interannotator agreement, by reannotating the first section, and by comparing against the sub-NP structure in DepBank (King et al., 2003). We also give an analysis of our extended Treebank, quantifying how much structure we have added, and how it is distributed across NPs. Finally, we test the utility of the extended Treebank for training statistical models on two tasks: NP bracketing (Lauer, 1995; Nakov and Hearst, 2005) and full parsing (Collins, 1999). This new resource will allow any system or annotated corpus developed from the Penn Treebank, to represent noun phrase structure more accurately. 240 \x0c2 Motivation Many approaches to identifying base noun phrases have been explored as part of chunking (Ramshaw and Marcus, 1995), but determining sub-NP structure is rarely addressed. We could use multi-word expressions (MWEs) to identify some structure. For example, knowing stock market is a MWE may help bracket stock market prices correctly, and Named Entities (NEs) can be used the </context>
<context position="5911" citStr="Lauer (1995)" startWordPosition="931" endWordPosition="932">kground The NP bracketing task has often been posed in terms of choosing between the left or right branching structure of three word noun compounds: (a) (world (oil prices)) Right-branching (b) ((crude oil) prices) Left-branching Most approaches to the problem use unsupervised methods, based on competing association strength between two of the words in the compound (Marcus, 1980, p. 253). There are two possible models to choose from: dependency or adjacency. The dependency model compares the association between words 1-2 to words 1-3, while the adjacency model compares words 1-2 to words 2-3. Lauer (1995) has demonstrated superior performance of the dependency model using a test set of 244 (216 unique) noun compounds drawn from Groliers encyclopedia. This data has been used to evaluate most research since. He uses Rogets thesaurus to smooth words into semantic classes, and then calculates association between classes based on their counts in a training set also drawn from Groliers. He achieves 80.7% accuracy using POS tags to indentify bigrams in the training set. Lapata and Keller (2004) derive estimates from web counts, and only compare at a lexical level, achieving 78.7% accuracy. Nakov and </context>
<context position="19805" citStr="Lauer (1995)" startWordPosition="3258" endWordPosition="3259">e correct bracketing in one third of cases. 6 Experiments Having bracketed NPs in the Penn Treebank, we now describe our initial experiments on how this additional level of annotation can be exploited. 6.1 NP Bracketing Data The obvious first task to consider is noun phrase bracketing itself. We implement a similar system to 244 \x0cCORPUS # ITEMS LEFT RIGHT Penn Treebank 5582 58.99% 41.01% Lauers 244 66.80% 33.20% Table 4: Comparison of NP bracketing corpora N-GRAM MATCH Unigrams 51.20% Adjacency bigrams 6.35% Dependency bigrams 3.85% All bigrams 5.83% Trigrams 1.40% Table 5: Lexical overlap Lauer (1995), described in Section 3, and report on results from our own data and Lauers original set. First, we extracted three word noun sequences from all the ambiguous NPs. If the last three children are nouns, then they became an example in our data set. If there is a NML node containing the first two nouns then it is left-branching, otherwise it is right-branching. Because we are only looking at the right-most part of the NP, we know that we are not extracting any nonsensical items. We also remove all items where the nouns are all part of a named entity to eliminate flat structure cases. Statistics </context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Corpus statistics meet the compound noun: Some empirical results. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5680" citStr="Marcus, 1980" startWordPosition="892" endWordPosition="894">e mis-alignment of NEs and right-branching NPs, the increase in performance was negligible. N N/N consumer N N/N electronics N conj and N N/N appliances N N/N retailing N chain Figure 1: CCG derivation from Hockenmaier (2003) 3 Background The NP bracketing task has often been posed in terms of choosing between the left or right branching structure of three word noun compounds: (a) (world (oil prices)) Right-branching (b) ((crude oil) prices) Left-branching Most approaches to the problem use unsupervised methods, based on competing association strength between two of the words in the compound (Marcus, 1980, p. 253). There are two possible models to choose from: dependency or adjacency. The dependency model compares the association between words 1-2 to words 1-3, while the adjacency model compares words 1-2 to words 2-3. Lauer (1995) has demonstrated superior performance of the dependency model using a test set of 244 (216 unique) noun compounds drawn from Groliers encyclopedia. This data has been used to evaluate most research since. He uses Rogets thesaurus to smooth words into semantic classes, and then calculates association between classes based on their counts in a training set also drawn </context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Mitchell Marcus. 1980. A Theory of Syntactic Recognition for Natural Language. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1092" citStr="Marcus et al., 1993" startWordPosition="160" endWordPosition="163">y to flat structures that ignore the complexity of English NPs. This means that tools trained on Treebank data cannot learn the correct internal structure of NPs. This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank. We then examine the consistency and reliability of our annotations. Finally, we use this resource to determine NP structure using several statistical approaches, thus demonstrating the utility of the corpus. This adds detail to the Penn Treebank that is necessary for many NLP applications. 1 Introduction The Penn Treebank (Marcus et al., 1993) is perhaps the most influential resource in Natural Language Processing (NLP). It is used as a standard training and evaluation corpus in many syntactic analysis tasks, ranging from part of speech (POS) tagging and chunking, to full parsing. Unfortunately, the Penn Treebank does not annotate the internal structure of base noun phrases, instead leaving them flat. This significantly simplified and sped up the manual annotation process. Therefore, any system trained on Penn Treebank data will be unable to model the syntactic and semantic structure inside base-NPs. The following NP is an example </context>
<context position="7460" citStr="Marcus et al. (1993)" startWordPosition="1187" endWordPosition="1190">the lack of gold-standard data makes this difficult. Girju et al. 241 \x0c(2005) draw a training set from raw WSJ text and use it to train a decision tree classifier achieving 73.1% accuracy. When they shuffled their data with Lauers to create a new test and training split, their accuracy increased to 83.1% which may be a result of the \x02 10% duplication in Lauers test set. We have created a new NP bracketing data set from our extended Treebank by extracting all rightmost three noun sequences from base-NPs. Our initial experiments are presented in Section 6.1. 4 Corpus Creation According to Marcus et al. (1993), asking annotators to markup base-NP structure significantly reduced annotation speed, and for this reason baseNPs were left flat. The bracketing guidelines (Bies et al., 1995) also mention the considerable difficulty of identifying the correct scope for nominal modifiers. We found however, that while there are certainly difficult cases, the vast majority are quite simple and can be annotated reliably. Our annotation philosophy can be summarised as: 1. most cases are easy and fit a common pattern; 2. prefer the implicit right-branching structure for difficult decisions. Finance jargon was a c</context>
<context position="11579" citStr="Marcus et al., 1993" startWordPosition="1842" endWordPosition="1845">Ps, i.e. annotating took approximately 5 seconds per NP. Most NPs require no bracketing, or fit into a standard pattern which the annotator soon becomes accustomed to, hence the task can be performed quite quickly. For the original bracketing of the Treebank, annotators performed at 375475 words per hour after a 242 \x0cPREC. RECALL F-SCORE Brackets 89.17 87.50 88.33 Dependencies 96.40 96.40 96.40 Brackets, revised 97.56 98.03 97.79 Dependencies, revised 99.27 99.27 99.27 Table 1: Agreement between annotators few weeks, and increased to about 1000 words per hour after gaining more experience (Marcus et al., 1993). For our annotation process, counting each word in every NP shown, our speed was around 800 words per hour. This figure is not unexpected, as the task was not large enough to get more than a months experience, and there is less structure to annotate. 5 Corpus Analysis 5.1 Inter-annotator Agreement The annotation was performed by the first author. A second Computational Linguistics PhD student also annotated Section 23, allowing inter-annotator agreement, and the reliability of the annotations, to be measured. This also maximised the quality of the section used for parser testing. We measured </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Search engine statistics beyond the n-gram: Application to noun compound bracketing.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005, Ninth Conference on Computational Natural Language Learning.</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2902" citStr="Nakov and Hearst, 2005" startWordPosition="459" endWordPosition="462">se in the Penn Treebank. We describe the annotation guidelines and process, including the use of named entity data to improve annotation quality. We check the correctness of the corpus by measuring interannotator agreement, by reannotating the first section, and by comparing against the sub-NP structure in DepBank (King et al., 2003). We also give an analysis of our extended Treebank, quantifying how much structure we have added, and how it is distributed across NPs. Finally, we test the utility of the extended Treebank for training statistical models on two tasks: NP bracketing (Lauer, 1995; Nakov and Hearst, 2005) and full parsing (Collins, 1999). This new resource will allow any system or annotated corpus developed from the Penn Treebank, to represent noun phrase structure more accurately. 240 \x0c2 Motivation Many approaches to identifying base noun phrases have been explored as part of chunking (Ramshaw and Marcus, 1995), but determining sub-NP structure is rarely addressed. We could use multi-word expressions (MWEs) to identify some structure. For example, knowing stock market is a MWE may help bracket stock market prices correctly, and Named Entities (NEs) can be used the same way. However, this o</context>
<context position="6524" citStr="Nakov and Hearst (2005)" startWordPosition="1028" endWordPosition="1031">uer (1995) has demonstrated superior performance of the dependency model using a test set of 244 (216 unique) noun compounds drawn from Groliers encyclopedia. This data has been used to evaluate most research since. He uses Rogets thesaurus to smooth words into semantic classes, and then calculates association between classes based on their counts in a training set also drawn from Groliers. He achieves 80.7% accuracy using POS tags to indentify bigrams in the training set. Lapata and Keller (2004) derive estimates from web counts, and only compare at a lexical level, achieving 78.7% accuracy. Nakov and Hearst (2005) also use web counts, but incorporate additional counts from several variations on simple bigram queries, including queries for the pairs of words concatenated or joined by a hyphen. This results in an impressive 89.3% accuracy. There have also been attempts to solve this task using supervised methods, even though the lack of gold-standard data makes this difficult. Girju et al. 241 \x0c(2005) draw a training set from raw WSJ text and use it to train a decision tree classifier achieving 73.1% accuracy. When they shuffled their data with Lauers to create a new test and training split, their acc</context>
<context position="21039" citStr="Nakov and Hearst, 2005" startWordPosition="3477" endWordPosition="3480">ew data set and Lauers data set are given in Table 4. As can be seen, the Penn Treebank based corpus is significantly larger, and has a more even mix of left and right-branching noun phrases. We also measured the amount of lexical overlap between the two corpora, shown in Table 5. This displays the percentage of n-grams in Lauers corpus that are also in our corpus. We can clearly see that the two corpora are quite dissimilar, as even on unigrams barely half are shared. 6.2 NP Bracketing Results With our new data set, we began running experiments similar to those carried out in the literature (Nakov and Hearst, 2005). We implemented both an adjacency and dependency model, and three different association measures: raw counts, bigram probability, and \x03\x05\x04 . We draw our counts from a corpus of n-gram counts calculated over 1 trillion words from the web (Brants and Franz, 2006). The results from the experiments, on both our and Lauers data set, are shown in Table 6. Our results ASSOC. MEASURE LAUER PTB Raw counts, adj. 75.41% 77.46% Raw counts, dep. 77.05% 68.85% Probability, adj. 71.31% 76.42% Probability, dep. 80.33% 69.56% \x03\x06\x04 , adj. 71.31% 77.93% \x03 \x04 , dep. 74.59% 68.92% Table 6: Br</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005. Search engine statistics beyond the n-gram: Application to noun compound bracketing. In Proceedings of CoNLL-2005, Ninth Conference on Computational Natural Language Learning. Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third ACL Workshop on Very Large Corpora.</booktitle>
<location>Cambridge MA, USA.</location>
<contexts>
<context position="3218" citStr="Ramshaw and Marcus, 1995" startWordPosition="508" endWordPosition="511">k (King et al., 2003). We also give an analysis of our extended Treebank, quantifying how much structure we have added, and how it is distributed across NPs. Finally, we test the utility of the extended Treebank for training statistical models on two tasks: NP bracketing (Lauer, 1995; Nakov and Hearst, 2005) and full parsing (Collins, 1999). This new resource will allow any system or annotated corpus developed from the Penn Treebank, to represent noun phrase structure more accurately. 240 \x0c2 Motivation Many approaches to identifying base noun phrases have been explored as part of chunking (Ramshaw and Marcus, 1995), but determining sub-NP structure is rarely addressed. We could use multi-word expressions (MWEs) to identify some structure. For example, knowing stock market is a MWE may help bracket stock market prices correctly, and Named Entities (NEs) can be used the same way. However, this only resolves NPs dominating MWEs or NEs. Understanding base-NP structure is important, since otherwise parsers will propose nonsensical noun phrases like Force contract by default and pass them onto downstream components. For example, Question Answering (QA) systems need to supply an NP as the answer to a factoid q</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third ACL Workshop on Very Large Corpora. Cambridge MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4328" citStr="Steedman, 2000" startWordPosition="683" endWordPosition="684">omponents. For example, Question Answering (QA) systems need to supply an NP as the answer to a factoid question, often using a parser to identify candidate NPs to return to the user. If the parser never generates the correct sub-NP structure, then the system may return a nonsensical answer even though the correct dominating noun phrase has been found. Base-NP structure is also important for annotated data derived from the Penn Treebank. For instance, CCGbank (Hockenmaier, 2003) was created by semi-automatically converting the Treebank phrase structure to Combinatory Categorial Grammar (CCG) (Steedman, 2000) derivations. Since CCG derivations are binary branching, they cannot directly represent the flat structure of the Penn Treebank base-NPs. Without the correct bracketing in the Treebank, strictly right-branching trees were created for all base-NPs. This has an unwelcome effect when conjunctions occur within an NP (Figure 1). An additional grammar rule is needed just to get a parse, but it is still not correct (Hockenmaier, 2003, p. 64). The awkward conversion results in bracketing (a) which should be (b): (a) (consumer ((electronics) and (appliances (retailing chain)))) (b) ((((consumer electr</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Ada Brunstein</author>
</authors>
<title>BBN pronoun coreference and entity type corpus.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Linguistic Data Consortium.</institution>
<contexts>
<context position="10094" citStr="Weischedel and Brunstein, 2005" startWordPosition="1608" endWordPosition="1611">, so that they remain comparable. We developed a bracketing tool, which identifies ambiguous NPs and presents them to the user for disambiguation. An ambiguous NP is any (possibly non-base) NP with three or more contiguous children that are either single words or another NP. Certain common patterns, such as three words beginning with a determiner, are unambiguous, and were filtered out. The annotator is also shown the entire sentence surrounding the ambiguous NP. The bracketing tool often suggests a bracketing using rules based mostly on named entity tags, which are drawn from the BBN corpus (Weischedel and Brunstein, 2005). For example, since Air Force is given ORG tags, the tool suggests that they be bracketed together first. Other suggestions come from previous bracketings of the same words, which helps to keep the annotator consistent. Two post processes were carried out to increase annotation consistency and correctness. 915 difficult NPs were marked by the annotator and were then discussed with two other experts. Secondly, certain phrases that occurred numerous times and were non-trivial to bracket, e.g. London Interbank Offered Rate, were identified. An extra pass was made through the corpus, ensuring tha</context>
</contexts>
<marker>Weischedel, Brunstein, 2005</marker>
<rawString>Ralph Weischedel and Ada Brunstein. 2005. BBN pronoun coreference and entity type corpus. Technical report, Linguistic Data Consortium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>