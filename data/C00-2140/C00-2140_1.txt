b'DiaSumm: Flexible Summarization of
Spontaneous Dialogues in Unrestricted Domains
Klaus Zechner and Alex Waibel
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
fzechner,waibelg@cs.cmu.edu
Abstract
In this paper, we present a summarization system
for spontaneous dialogues which consists of a novel
multi-stage architecture. It is speci\x0ccally aimed at
addressing issues related to the nature of the texts
being spoken vs. written and being dialogical vs.
monological. The system is embedded in a graph-
ical user interface and was developed and tested on
transcripts of recorded telephone conversations in
English and Spanish (Callhome).
1 Introduction
Summarization of written documents has recently
been a focus for much research in NLP (e.g., (Mani
and Maybury, 1997; AAAI, 1998; Mani et al., 1998;
ACL, 2000), to name some of the major events in
this \x0celd in the past few years). However, very lit-
tle attention has been given so far to the summa-
rization of spoken language, even less of conversa-
tions vs. monological texts. We believe that sum-
marization of speech will become increasingly more
important,as the amountof online audio data grows
and demand for rapid browsing, skimming, and ac-
cess of speech data increases. Another application
which particularly pertains to our interest in spo-
ken dialogue summarizationwouldbe the generation
of meeting minutes for archival purposes and/or to
update participants joining at later stages on the
progress of the conversation so far.
Summarization of dialogues within limited do-
mains has been attempted within the context of
the Verbmobil project (\\protocol generation",
(Alexandersson and Poller, 1998)) or by SRI\'s MIMI
summarizer (Kameyama et al., 1996). Recent work
on spoken language summarization in unrestricted
domains has focused almost exclusively on Broad-
cast News, mostly due to the spoken language track
of recent TREC evaluations (Garofolo et al., 1997;
Garofolo et al., 1999). (Waibel et al., 1998) describe
a Meeting Browser where summaries can be gener-
ated using technology established for written texts.
(Valenza et al., 1999) go one step further and incor-
porate knowledge from the speech recognizer (con-
\x0cdence scores) into their summarization system, as
well.
We argue that the nature of spoken dialogues, to-
gether with their textual representations as speech
recognizer hypotheses, requires a set of speci\x0cc ap-
proaches to make summarization feasible for this
text genre.
As a demonstrable proof of concept, we present
the multi-stage architecture of the summarization
system DiaSumm which can 
exibly deal with spo-
ken dialogues in English and Spanish, without any
restrictions of domain. Since it cannot rely on any
domain speci\x0cc knowledge base, it uses shallow sta-
tistical approaches and presents (possibly modi\x0ced)
extracts from the original text as summary.
We present results of several evaluations of our
system using humantranscripts of spontaneous tele-
phone conversations inEnglish and Spanishfromthe
Callhome corpus ((LDC), 1996), in particular the
accuracy of the topic segmentation and information
condensing components (sections 6 and 7). Also, for
the purpose of a global evaluation, a user study was
performed which addressed information access time
and accuracy of retained informationcomparing dif-
ferent versions of summaries (section 10).
This paper is organized as follows: In the next sec-
tion, we provide an overview about the main issues
for summarization of spoken dialogues and indicate
the approaches we are taking in our system. We
then present the system architecture (section 3), fol-
lowedby a detailed description ofthe majorbuilding
blocks (sections 4 to 8). After a brief characteriza-
tion of the GUI (section 9) we describe a user study
for global system evaluation in section 10. We con-
clude the paper with a summaryand a brief outlook
in section 11.
2 Issues and Approaches: Overview
In this section, we give an overview about the main
issues that any summarizationsystem for spoken di-
alogues has to address and indicate the approach we
are taking for each of these in DiaSumm.
In a general sense, when dealing with written
texts, usually there is plenty of information avail-
able which can be used for the purpose of summa-
\x0crization, such as capitalization, punctuation marks,
titles, passage headers, paragraph boundaries, or
other mark-ups. Unfortunately, however, none of
this holds for speech data which arrives as a stream
of word tokens from a recognizer, cut into \\utter-
ances" by using a silence heuristic.
2.1 Lack of clause boundaries
One of the most serious issues is the lack of sentence
or clause boundaries in spoken dialogues which is
particularly problematic since sentences, clauses, or
paragraphs are considered the \\minimal units" in
virtually all existing summarization systems. When
humans speak, they sometimes pause during a
clause, and not always at the end of a clause, which
means that the output of a recognizer (which usu-
allyuses somesilence-heuristics to cut the segments)
frequently does not match logical sentence or clause
boundaries. Looking at \x0cve English Callhome di-
alogues with an average number of 320 utterances
each, we \x0cnd on average 30 such \\continuations" of
logical clauses over automaticallydetermined acous-
tic segment boundaries. In a summary, this can
cause a reduction in coherence and readability of
the output.
We address this issue by linking adjacent turns
of the same speaker together if the silence between
them is less than a given constant (section 4).
2.2 Distributed information
Since we have multi-party conversations as opposed
to monological texts, sometimes the crucial infor-
mation is found in a question-answer-pair, i.e., it
involves more than one speaker; extracting only the
question or only the answer would be meaningless
in many cases. We found that on average about
10% of the speaker turns belong to such question-
answer pairs in \x0cve examined English Callhome
dialogues. Often, either the question or the answer
is very short and does not contain any words with
high relevance. In order not to \\lose" these short
turns at a later stage, when only the most relevant
turns are extracted, we link them to the matching
question/answer ahead of time, using two di\x0berent
methods to detect questions and their answers (sec-
tion 4).
2.3 Dis
uent speech
Speech dis
uencies in spontaneous conversations |
such as \x0cllers, repetitions, repairs, or un\x0cnished
clauses | can make transcripts (and summary ex-
tracts) quite hard to read and also introduce an un-
wanted bias to relevance computations (e.g., word
repetitions would cause a higher word count for the
repeated content words; words in un\x0cnished clauses
would be included in the word count.)
To alleviate this problem, we employ a clean-up
\x0clter pipeline, which eliminates \x0cller words and rep-
etitions, and segments the turns into short clauses
(section 5). We also remove incomplete clauses, typ-
ically sentence-initial repairs, at this stage of our
system. This \\cleaning-up" serves two main pur-
poses: (i) it increases the readability (for the \x0cnally
extracted segments); and (ii) it makes the text more
tractable by subsequent modules.
The followingexamplecompares aturn before and
after the clean-up component:
before: I MEAN WE LOSE WE LOSE I CAN\'T I
CAN\'T DO ANYTHING ABOUT IT SO
after: we lose / i can\'t do anything
about it
2.4 Lack of topic boundaries
Callhome speech data is multi-topical but does
not include mark-up for paragraphs, nor any topic-
informative headers. Typically, we \x0cnd about 5{10
di\x0berent topics within a 10-minute segment of a di-
alogue, i.e., the topic changes about every 1{2 min-
utes in these conversations. To facilitate browsing
and summarization, we thus have to discover topi-
cally coherent segments automatically. This is done
using a TextTiling approach, adapted from (Hearst,
1997) (section 6).
2.5 Speech recognizer errors
Last but not least, we face the problem of imper-
fect word accuracy of speech recognizers, particu-
larly when dealing with spontaneous speech over a
large vocabulary and over a low bandwidth channel,
such as the Callhome databases which we mainly
used for development, testing, and evaluation of our
system. Current recognizers typically exhibit word
error rates for these corpora in the order of 50%. In
DiaSumm\'s information condensation component,
the relevance weights of speaker turns can be ad-
justed to take into account their word con\x0cdence
scores from the speech recognizer. That way we can
reduce the likelihood of extracting passages with a
larger amount of word misrecognitions (Zechner and
Waibel, 2000). In this paper, however, the focus will
be exclusively on results of our evaluations on hu-
mangenerated transcripts. No informationfromthe
speech recognizer nor fromthe acoustic signal (other
than inter-utterance pause durations) are used. We
are aware that in particular prosodic information
may be of help for tasks such as the detection of
sentence boundaries, speech acts, or topic bound-
aries (Hirschberg and Nakatani, 1998; Shriberg et
al., 1998; Stolcke et al., 2000), but the investigation
of the integration of this additional source of infor-
mationis beyond the scope of this paper and left for
future work.
3 System Architecture
The global system architecture of DiaSumm is a
pipeline of the following four major components:
\x0cTRANS
Turn Linking
Clean-up Filter
Topic Segmentation
Information Condensation
Telegraphic Reduction
CLEAN
and TELE
input for
TRANS
CLEAN
TELE
input for
Figure 1: System architecture
turn linking; clean-up \x0clter; topic segmentation; and
information condensation. A \x0cfth component is
added at the end for the purpose of telegraphic re-
duction, so that we can maximize the information
content in a given amount of space. The system ar-
chitecture is shown in Figure 1. It also indicates the
three major types of summaries which can be gener-
ated by DiaSumm: trans (\\transcript"): not using
the linking and clean-up components; clean: us-
ing the main four components; tele (\\telegraphic"
summary): additionally,using the telegraphicreduc-
tion component.
The followingsections describe the components of
DiaSumm in more detail.
4 Turn Linking
The two main objectives of this component are: (i)
to form turns which contain a set of full (and not
partial) clauses; and (ii) to form turn-pairs in cases
where we have a question-answer pair in the dia-
logue.
Toachieve the \x0crst objective, we scan the inputfor
adjacent turns ofone speaker and linkthemtogether
if their time-stamp distance is below a pre-speci\x0ced
threshold \x12. If the threshold is too small, we don\'t
get most of the (logical) turn continuations across
utterance boundaries, if it is too large, we run the
risk of \\skipping"over short but potentially relevant
fragments of the speaker on the other channel. We
experimented with thresholds between 0.0 and 2.0
seconds and determined a local performance maxi-
mum around \x12 = 1:0.
For the second objective, to formturn-pairs which
comprise a question-answer information exchange
between twodialogueparticipants, we need to detect
wh- and yes-no-questions in the dialogue. We tested
English Spanish
Annotated Data
turns 1603 1185
Wh-questions 42 78
yes-no-questions 43 98
questions total 85 (5.3%) 176 (14.9%)
Automatic Detection Results (F1)
SA classi\x0cer 0.24 0.22
POS rules 0.22 0.37
random baseline 0.02 0.13
Table 1: Q-A-pair distribution in the data and ex-
perimental results for automatic Q-A-detection
two approaches: (a) a HMM based speech act (SA)
classi\x0cer (Ries, 1999) and (b) a set of part-of-speech
(POS) based rules. The SA classi\x0cer was trained on
dialogues which were manuallyannotated for speech
acts, using parts of the Switchboard corpus (God-
frey et al., 1992) for English and Callhome for
Spanish. The corresponding answers for the de-
tected questions were hypothesized in the \x0crst turn
with a di\x0berent speaker, followingthe question-turn.
Table 1 shows the results of these experiments for 5
English and 5 Spanish Callhome dialogues, com-
pared to a baseline of randomlyassigning n question
speech acts, n being the number of question-turns
marked by human annotators. We report F1-scores,
where F1 = 2PR
P+R with P =precision and R=recall.
We note that while the results for the SA-classi\x0cer
and the rule-based approach are very similar for En-
glish, the rule-based approach yields better results
for Spanish. The much higher random baseline for
Spanish can be explained by the higher incidence of
questions in the Spanish data (14.9% vs. 5.3% for
English).
5 Clean-up Filter
The clean-up component is a sequence of modules
which serve the purposes of (a) rendering the tran-
scripts more readable, (b) simplifying the input for
subsequent components, and (c) avoiding unwanted
bias for relevance computations (see section 2). All
this has to happen without losing essential informa-
tion that could be relevant in a summary. While
other work(Heeman et al.,1996; Stolcke et al.,1998)
was concerned with building classi\x0cers that can de-
tect and possibly correct various speech dis
uencies,
our implementation is of a much simpler design. It
does not require as much manual annotated train-
ing data and uses individual components for every
major category of dis
uency.1
1While we have not yet numerically evaluated the perfor-
manceof this component,its outputis deemedverynaturalto
readby systemusers. Sincethe focusand goalsof thiscompo-
nent are somewhat di\x0berent than previous work in that area,
meaningful comparisons are hard to make.
\x0cSingle or multiple word repetitions, \x0cllers (e.g.,
\\uhm"), and discourse markers without semantic
content (e.g., \\you know") are removed from the in-
put, some short forms are expanded (e.g., \\we\'ll"
! \\we will"), and frequent word sequences are
combined into a single token (e.g., \\a lot of" !
\\a lot of").
Longer turns are segmented into short clauses,
which are de\x0cned as consisting of at least a sub-
ject and an in
ected verbal form. While (Stolcke
and Shriberg, 1996) use n-gram models for this task,
and (Gavald\x12
a et al., 1997) use neural networks, we
decided to use a rule-based approach (using word
and POS information), whose performance proved
to be comparable with the results in the cited pa-
pers (F1 > 0:85, error < 0:05).2
For several of the clean-up \x0clter\'s components, we
make use of Brill\'s POS tagger (Brill, 1994). For
English, we use a modi\x0ced version of Brill\'s original
tagset, and the tagger was adapted andretrained for
spoken language corpora (Callhome and Switch-
board) (Zechner, 1997). For Spanish, we created
our own tag set, derived from the LDC lexicon and
fromthe CRATERproject (Le\x13
on, 1994),andtrained
the tagger on manually annotated Callhome dia-
logues. Furthermore, a POS based shallow chunk
parser (Zechner and Waibel, 1998) is used to \x0clter
out likely candidates for incomplete clauses due to
speech repair or interruption by the other speaker.
6 Topic Segmentation
Since Callhome dialogues are alwaysmulti-topical,
segmenting them into topical units is an important
step in our summarization system. This allows us
to provide \\signature" information (frequent con-
tent words) about every topic to the user as a help
for faster browsing and accessing the data. Fur-
thermore, the subsequent information condensation
component can work on smallerparts of the dialogue
and thus operate more e\x0eciently.
Following (Boguraev and Kennedy, 1997; Barzi-
lay and Elhadad, 1997) who use TextTiling (Hearst,
1997) for their summarization systems of written
text, we adapted this algorithm (its block compar-
ison version) for speech data: we choose turns to
be minimal units and compute block similarity be-
tween blocks of k turns every d turns. We use 9
English and 15 Spanish Callhome dialogues, man-
ually annotated for topic boundaries, to determine
the optimum values for a set of TextTiling param-
eters and at the same time to evaluate the accu-
racy of this algorithm. To do this, we ran an n-fold
cross-validation (\\jack-kni\x0cng") where all dialogues
but one are used to determine the best parameters
(\\train set") and the remaining dialogue is used as
2The comparison was done on the same data set as used
in (Gavald\x12
a et al., 1997).
English Spanish
blocksize k 25 15
sample distance d 2 2
rounds of smoothing r 2 1
smoothing width s 2 1
Table 2: Optimal TextTiling parameters for English
and Spanish Callhome dialogues
English Spanish
number of dialogues 9 15
random baseline 0.34 0.35
test set avg. (\\unseen data") 0.58 0.53
train set avg. (\\seen data") 0.69 0.58
Table 3: Topic segmentation results for English and
Spanish Callhome dialogues (F1-scores)
a held-out data set for evaluation (\\test set"). This
process is repeated n times and average results are
reported. Table 2 shows the set of parameters which
worked best for most dialogues and Table 3 shows
the evaluation results of the cross-validation exper-
iment. F1-scores improve by 18{24% absolute over
the random baseline for unseen and by 23{35% for
seen data, the performance for English being better
than for Spanish. These results, albeit achieved on
a quite di\x0berent text genre, are well in line with the
results in (Hearst, 1997) who reports an absolute im-
provement of about 20% over a random baseline for
seen data.
7 Information Condensation
The informationcondensation component is the core
of our system. Its purpose is to determine weights
for terms and turns (or linked turn-pairs) and then
to rank the turns according to their relevance within
each topical segment of the dialogue.
For term-weighting, tf*idf -inspired formulae
(Salton and Buckley, 1990) are used to emphasize
words which are in the \\middle range" of frequency
in the dialogue and do not appear in a stop list.3
For turn-ranking, we use a version of the \\maximal
marginal relevance" (MMR) algorithm (Carbonell
and Goldstein, 1998), where emphasis is given to
turns which contain many highly weighted terms for
the current segment (\\salience") and are su\x0eciently
dissimilar to previously ranked turns (to minimize
redundancy).
For 9 English and 14 Spanish dialogues, the \\most
relevant" turns were marked by human coders. We
rana series of cross-validationexperiments to(a) op-
timize the parameters of this component related to
tf*idf and MMR computation and to (b) determine
3For English, our stop list comprises 557 words, for Span-
ish, 831 words.
\x0chowwellthis informationcondensing componentcan
match the human relevance annotations.
Summarization results are computed using 11-pt-
avg precision scores for ranked turn lists where the
maximum precision of the list of retrieved turns
is averaged in the 11 evenly spaced intervals be-
tween recall=[0,0.1),[0.1,0.2), ... [1.0,1.1) (Salton
and McGill, 1983).4 Table 4 shows the results from
these experiments. Similar to other experiments in
the summarizationliterature (Mani et al., 1998), we
\x0cnd a wide performance variation across di\x0berent
texts.
8 Telegraphic Reduction
The purpose of this component is to maximizeinfor-
mation in a \x0cxed amount of space. We shorten the
output of the summarizer to a \\telegraphic style";
that way, more information can be included in a
summary of k words (or n bytes). Since we only
use shallow methods for textual analysis that do
not generate a dependency structure, we cannot use
complex methods for text reduction as described,
e.g., in (Jing, 2000). Our method simply excludes
words occurring in the stop list from the summary,
except for some highlyinformativewords such as \\I"
or \
ot".
9 User Interface and System
Performance
Since we want to enable interactive summarization
which allows a user to browse through a dialogue
quickly to search for information he is interested
in, we have integrated our summarization system
into a JAVA-based graphical user interface (\\Meet-
ing Browser") (Bett et al., 2000). This interface also
integrates the output of a speech recognizer (Yu et
al., 1999), and can display a wide variety of infor-
mation about a conversation, including speech acts,
dialogue games, and emotions.
For summarization, the user can determine the
size of the summary and which topical segments
he wants to have displayed. He can also focus
the summary on particular content words (\\query-
based summary") or exclude words from considera-
tion (\\dynamic stop list expansion").
Summarizing a 10 minute segment of a Call-
home dialogue with our system takes on average less
than 30 seconds on a 167 MHz 320 MB Sun Ultra1
workstation.5
4We are aware that this annotationand evaluationscheme
is far from optimal: it does neither re
ect the fact that turns
are not necessarily the best units for extraction nor that the
11-pt-avg precision score is not optimally suited for the sum-
marization task. We thus have recently developed a new
word-based method for annotation and evaluation of spon-
taneous speech (Zechner, 2000).
5The average was computed over \x0cve English dialogues.
10 Human Study
10.1 Experiment Setup
In order to evaluate the system as a whole, we con-
ducted a study with humansin the loop to be able to
compare three types of summaries (trans, clean,
tele, see section 3) with the full original transcript.
We address these two main questions in this study:
(i) how fast can information be identi\x0ced using dif-
ferent types of summaries? (ii) how accurately is the
information preserved, comparing di\x0berent types of
summaries?
We did not only ask the user \
arrow" questions
for a speci\x0cc piece of information | along the lines
of the Q-A-evaluation part of the SUMMAC confer-
ence (Mani et al., 1998) | but also very \\global",
non-speci\x0cc questions, tied to a particular (topical)
segment of the dialogue.
The experiment was conducted as follows: Sub-
jects were given24texts each, accompaniedbyeither
a generic question (\\What is the topic of the discus-
sion in this text segment?") or three speci\x0cc ques-
tions (e.g., \\Which clothes did speaker A buy?").
The texts were drawn from \x0cve topical segments
each from \x0cve English Callhome dialogues.6 They
have four di\x0berent formats: (a) full transcripts (i.e.,
the transcript of the whole segment) (full); (b)
summaryof the raw transcripts (without linkingand
clean-up) (trans); (c) cleaned-up summary (using
all four major components of our system) (clean);
and (d) telegram summary (derived from (c), using
also the telegraphic reduction component) (tele).
The texts of formats (b), (c), and (d) were gener-
ated to have the same length: 40% of (a), i.e., we
use a 60% reduction rate. All these formats can
be accompanied by either a generic or three speci\x0cc
questions, hence there are eight types of tasks for
each of the 24 texts.
We divided the subjects in eight groups such that
no subject had to perform more than one task on
the same text and we distributed the di\x0berent tasks
evenly for each group. Thus we can make unbiased
comparisons across texts and tasks.
The answer accuracy vs. a pre-de\x0cned answer key
was manually assessed on a 6 point discrete scale
between 0.0 and 1.0.
10.2 Results and Discussion
Of the 27 subjects taking part in this experiment,
we included 24 subjects in the evaluation; 3 sub-
jects were excluded who were extreme outliers with
respect to average answer time or score (not within
\x16 +,2stddev).
From the results in Table 5 we observe the fol-
lowing trends with respect to answer accuracy and
response time:
6One of the 25 segments was set aside for demonstration
purposes.
\x0cEnglish Spanish
number of dialogues 9 14
turns per dialogue marked as relevant by human coders 12% 25%
11-pt-avg precision (average over topical segments) 0.45 0.59
score variation between dialogues 0.2{0.49 0.15{0.8
Table 4: Summarization results for English and Spanish Callhome
Format full trans clean tele
Time vs. Acc. Time Acc. Time Acc. Time Acc. Time Acc.
generic (q = 72) 75.2 0.814 53.9 0.739 52.6 0.617 54.4 0.622
speci\x0cc (q = 216) 109.1 0.834 82.2 0.624 88.0 0.593 91.6 0.665
Table 5: Average answer times (in sec) and accuracy scores ([0.0-1.0]) over eight di\x0berent tasks (number of
subjects=24; q=number of questions per task type).
summary type trans clean tele
generic / indicative 90.8 75.8 76.4
speci\x0cc / informative 74.8 71.0 79.7
Table6: Relativeanswer accuracies in%fordi\x0berent
summaries
\x0f generic questions (\\indicative summaries", the
task being to identify the topic of a text): The
two cleaned up summaries took about the same
time to process but had lower accuracy scores
than the version directly using the transcript.
\x0f speci\x0cc questions (\\informative summaries",
the task being to \x0cnd speci\x0cc informationin the
text): (1) The accuracy advantage of the raw
transcript summaries (trans) over the cleaned
up versions (clean) is only small (not statis-
tically signi\x0ccant: t=0.748)7. (2) There is a
superiority of the tele-summary to both other
kinds (tele is signi\x0ccantly more accurate than
clean for p < 0:05).
Fromthis we conjecture that our methods for cus-
tomization of the summaries to spoken dialogues is
mostly relevant for informative, but not so much
for indicative summarization. We think that other
methods, such as lists of signature phrases would be
more e\x0bective to use for the latter purpose.
Table 6 shows the answer accuracy for the three
di\x0berent summary types relative to the accuracy of
the full transcript texts of the same segments (\\rela-
tive answer accuracy"). We observe that the relative
accuracy reduction for all summaries is markedly
lower than the reduction of text size: all summaries
were reduced from the full transcripts by 60%,
whereas the answer accuracy only drops between 9%
(trans) and 24% (clean) for the generic questions,
7In fact, in 2 of 5 dialogues, the clean summary scores
are higher than those of the trans summaries.
and between 20% (tele) and 29% (clean) for the
speci\x0cc questions. This proves that our system is
able to retain most of the relevant information in
the summaries.
As for average answer times, we see a marked re-
duction (30%) of all summaries compared to the full
texts in the generic case; for the speci\x0cc case, the
time reduction is somewhat smaller (15%{25%).
One shortcoming of the current system is that it
operates on turns (or turn-pairs) as minimal units
for extraction. In future work, we will investigate
possibilities to reduce the minimal units of extrac-
tion to the level of clauses or sentences, without giv-
ing up the idea of linking cross-speaker information.
11 Summary and Future Work
We have presented a summarization system for spo-
ken dialogues which is constructed to address key
di\x0berences of spoken vs. written language, dialogues
vs. monologues, and multi-topical vs. mono-topical
texts. The system cleans up the input for speech
dis
uencies, links turns together into coherent in-
formation units, determines topical segments, and
extracts the most relevant pieces of information in
a user-customizable way. Evaluations of major sys-
tem components and of the system as a whole were
performed. The results of a user study show that
with a summary size of 40%, between 71% and 91%
of the information of the full text is retained in the
summary, depending on the type of summary and
the types of questions being asked.
We are currently extending the system to be able
tohandle di\x0berent levels ofgranularityfor extraction
(clauses, sentences, turns). Furthermore, we plan to
investigate the integration of prosodic information
into several components of our system.
12 Acknowledgements
Wewanttothankthe annotators fortheir e\x0borts and
Klaus Ries for providing the automatic speech act
\x0ctagger. We appreciate comments and suggestions
from Alon Lavie, Marsal Gavald\x12
a, Jade Goldstein,
Thomas MacCracken, and the anonymous reviewers
on earlier drafts of this paper.
This work was funded in part by the Verbmobil
project of the Federal Republic of Germany, ATR {
Interpreting Telecommunications Research Labora-
tories of Japan, and the US Department of Defense.
References
AAAI, editor. 1998. Proceedings of the AAAI-98 Spring
Symposium on Intelligent Text Summarization, Stanford,
CA.
ACL. 2000. Proceedings of the ANLP/NAACL-2000 Work-
shop on Automatic Summarization, Seattle, WA, May.
Jan Alexandersson and Peter Poller. 1998. Towards mul-
tilingual protocol generation for spontaneous speech dia-
logues. In Proceedings of the INLG-98, Niagara-on-the-
lake, Canada, August.
Regina Barzilay and Michael Elhadad. 1997. Using lexical
chains for text summarization. In ACL/EACL-97 Work-
shop on Intelligent and Scalable Text Summarization.
Michael Bett, Ralph Gross, Hua Yu, Xiaojin Zhu, Yue Pan,
Jie Yang, and Alex Waibel. 2000. Multimodal meeting
tracker. In Proceedings of the Conference on Content-
Based Multimedia Information Access, RIAO-2000, Paris,
France, April.
Branimir Boguraev and Christopher Kennedy. 1997.
Salience-based characterisation of text documents. In
ACL/EACL-97 Workshop on Intelligent and Scalable Text
Summarization.
Eric Brill. 1994. Some advancesin transformation-basedpart
of speech tagging. In Proceeedings of AAAI-94.
Jaime Carbonelland Jade Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents and
producing summaries. In Proceedings of the 21st ACM-
SIGIR International Conference on Research and Devel-
opment in Information Retrieval, Melbourne, Australia.
John S. Garofolo, Ellen M. Voorhees, Vincent M. Stanford,
and Karen Sparck Jones. 1997. TREC-6 1997 spoken doc-
ument retrieval track overview and results. In Proceed-
ings of the 1997 TREC-6 Conference, Gaithersburg, MD,
November, pages 83{91.
John S. Garofolo, Ellen M. Voorhees, Cedric G. P. Auzanne,
and Vincent M. Stanford. 1999. Spoken document re-
trieval: 1998 evaluation and investigation of new metrics.
In Proceedings of the ESCA workshop: Accessing informa-
tion in spoken audio, pages 1{7. Cambridge, UK, April.
Marsal Gavald\x12
a, Klaus Zechner, and Gregory Aist. 1997.
High performancesegmentation of spontaneous speech us-
ing part of speech and trigger word information. In Pro-
ceedings of the 5th ANLP Conference, Washington DC,
pages 12{15.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
Switchboard: telephone speech corpus for research and
development. In Proceedings of the ICASSP-92, volume 1,
pages 517{520.
Marti A. Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational Lin-
guistics, 23(1):33{64, March.
Peter A. Heeman, Kyung ho Loken-Kim, and James F. Allen.
1996. Combining the detection and correction of speech
repairs. In Proceedings of ICSLP-96.
Julia Hirschberg and Christine Nakatani. 1998. Acoustic
indicators of topic segmentation. In Proceedings of the
ICSLP-98, Sydney, Australia.
Hongyan Jing. 2000. Sentence reduction for automatic text
summarization. In Proceedings of ANLP-NAACL-2000,
Seattle, WA, May, pages 310{315.
Megumi Kameyama, Goh Kawai, and Isao Arima. 1996. A
real-time system for summarizing human-human sponta-
neous spoken dialogues. In Proceedings of the ICSLP-96,
pages 681{684.
Linguistic Data Consortium (LDC). 1996. CallHome and
CallFriend LVCSR databases.
Fernando S\x13
anchez Le\x13
on. 1994. Spanish tagset for the
CRATER project. http://xxx.lanl.gov/cmp-lg/9406023.
Inderjeet Mani and Mark Maybury, editors. 1997. Proceed-
ings of the ACL/EACL\'97 Workshop on Intelligent Scal-
able Text Summarization, Madrid, Spain.
Inderjeet Mani, David House, Gary Klein, Lynette
Hirschman, Leo Obrst, Therese Firmin, Michael
Chrzanowski, and Beth Sundheim. 1998. The TIP-
STER SUMMAC text summarization evaluation. Mitre
Technical Report MTR 98W0000138, October 1998.
Klaus Ries. 1999. HMM and neural network based speech
act detection. In Proceedings of the ICASSP-99, Phoenix,
Arizona, March.
Gerard Salton and Chris Buckley. 1990. Flexible text match-
ing for information retrieval. Technical report, Cornell
University, Department of Computer Science, TR 90-1158,
September.
Gerard Salton and Michael J. McGill. 1983. Introduction to
Modern Information Retrieval. McGraw Hill, Tokyo etc.
Elizabeth Shriberg, Rebecca Bates, Andreas Stolcke, Paul
Taylor, DanielJurafsky, Klaus Ries, Noah Coccaro, Rachel
Martin, Marie Meteer, and Carol Van Ess-Dykema. 1998.
Can prosody aid the automaticclassi\x0ccationof dialog acts
in conversational speech? Language and Speech, 41(3-
4):439{487.
Andreas Stolcke and Elizabeth Shriberg. 1996. Automatic
linguistic segmentation of conversational speech. In Pro-
ceedings of the ICSLP-96, pages 1005{1008.
Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates, Mari
Ostendorf, Dilek Hakkani, Madeleine Plauche, G\x7f
okhan
T\x7f
ur, and Yu Lu. 1998. Automatic detection of sentence
boundaries and dis
uencies based on recognized words. In
Proceedings of the ICSLP-98, Sydney, Australia, Decem-
ber, volume 5, pages 2247{2250.
Andreas Stolcke, Elizabeth Shriberg,Dilek Hakkani-T\x7f
ur, and
G\x7f
okhan T\x7f
ur. 2000. Prosody-based automatic segmenta-
tion of speech into sentences and topics. Speech Commu-
nication, 32(1-2).
Robin Valenza, Tony Robinson, Marianne Hickey, and Roger
Tucker. 1999. Summarisation of spoken audio through in-
formation extraction. In Proceedings of the ESCA work-
shop: Accessing information in spoken audio, pages 111{
116. Cambridge, UK, April.
Alex Waibel, Michael Bett, and Michael Finke. 1998. Meet-
ing browser: Tracking and summarizingmeetings. In Pro-
ceedings of the DARPA Broadcast News Workshop.
Hua Yu, Michael Finke, and Alex Waibel. 1999. Progress
in automatic meeting transcription. In Proceedings of
EUROSPEECH-99, Budapest, Hungary, September.
Klaus Zechner and Alex Waibel. 1998. Using chunk based
partial parsing of spontaneous speech in unrestricted do-
mains for reducing word error rate in speech recognition.
In Proceedings of COLING-ACL 98, Montreal, Canada.
KlausZechnerandAlex Waibel. 2000. Minimizingword error
rate in textual summaries of spoken language. In Proceed-
ings of the First Meeting of the North American Chapter of
the Association for Computational Linguistics, NAACL-
2000, Seattle, WA, April/May, pages 186{193.
Klaus Zechner. 1997. Building chunk level represen-
tations for spontaneous speech in unrestricted do-
mains: The CHUNKY system and its application to
reranking N-best lists of a speech recognizer. Mas-
ter\'s thesis (project report), CMU, available from:
http://www.cs.cmu.edu/~zechner/publications.html.
Klaus Zechner. 2000. A word-based annota-
tion and evaluation scheme for summariza-
tion of spontaneous speech. Available from
http://www.cs.cmu.edu/~zechner/publications.html.
\x0c'