<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.298938">
b&amp;apos;Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 6368,
</note>
<address confidence="0.466804">
Sofia, Bulgaria, August 4-9 2013. c
</address>
<title confidence="0.498407">
2013 Association for Computational Linguistics
PARMA: A Predicate Argument Aligner
</title>
<author confidence="0.940056666666667">
Travis Wolfe, Benjamin Van Durme, Mark Dredze, Nicholas Andrews,
Charley Beller, Chris Callison-Burch, Jay DeYoung, Justin Snyder,
Jonathan Weese, Tan Xu
</author>
<title confidence="0.3742055">
, and Xuchen Yao
Human Language Technology Center of Excellence
</title>
<author confidence="0.415086">
Johns Hopkins University, Baltimore, Maryland USA
</author>
<affiliation confidence="0.996558">
University of Maryland, College Park, Maryland USA
</affiliation>
<sectionHeader confidence="0.97441" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999817">
We introduce PARMA, a system for cross-
document, semantic predicate and argu-
ment alignment. Our system combines a
number of linguistic resources familiar to
researchers in areas such as recognizing
textual entailment and question answering,
integrating them into a simple discrimina-
tive model. PARMA achieves state of the
art results on an existing and a new dataset.
We suggest that previous efforts have fo-
cussed on data that is biased and too easy,
and we provide a more difficult dataset
based on translation data with a low base-
line which we beat by 17% F1.
</bodyText>
<sectionHeader confidence="0.998267" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99938112">
A key step of the information extraction pipeline
is entity disambiguation, in which discovered en-
tities across many sentences and documents must
be organized to represent real world entities. The
NLP community has a long history of entity dis-
ambiguation both within and across documents.
While most information extraction work focuses
on entities and noun phrases, there have been a
few attempts at predicate, or event, disambigua-
tion. Commonly a situational predicate is taken to
correspond to either an event or a state, lexically
realized in verbs such as elect or nominaliza-
tions such as election. Similar to entity coref-
erence resolution, almost all of this work assumes
unanchored mentions: predicate argument tuples
are grouped together based on coreferent events.
The first work on event coreference dates back to
Bagga and Baldwin (1999). More recently, this
task has been considered by Bejan and Harabagiu
(2010) and Lee et al. (2012). As with unanchored
entity disambiguation, these methods rely on clus-
tering methods and evaluation metrics.
Another view of predicate disambiguation seeks
to link or align predicate argument tuples to an ex-
isting anchored resource containing references to
events or actions, similar to anchored entity dis-
ambiguation (entity linking) (Dredze et al., 2010;
Han and Sun, 2011). The most relevant, and per-
haps only, work in this area is that of Roth and
Frank (2012) who linked predicates across docu-
ment pairs, measuring the F1 of aligned pairs.
Here we present PARMA, a new system for pred-
icate argument alignment. As opposed to Roth and
Frank, PARMA is designed as a a trainable plat-
form for the incorporation of the sort of lexical se-
mantic resources used in the related areas of Rec-
ognizing Textual Entailment (RTE) and Question
Answering (QA). We demonstrate the effective-
ness of this approach by achieving state of the art
performance on the data of Roth and Frank despite
having little relevant training data. We then show
that while the lemma match heuristic provides a
strong baseline on this data, this appears to be an
artifact of their data creation process (which was
heavily reliant on word overlap). In response, we
evaluate on a new and more challenging dataset for
predicate argument alignment derived from multi-
ple translation data. We release PARMA as a new
framework for the incorporation and evaluation of
new resources for predicate argument alignment.1
</bodyText>
<sectionHeader confidence="0.995279" genericHeader="introduction">
2 PARMA
</sectionHeader>
<bodyText confidence="0.9407714">
PARMA (Predicate ARguMent Aligner) is a
pipelined system with a wide variety of features
used to align predicates and arguments in two doc-
uments. Predicates are represented as mention
spans and arguments are represented as corefer-
ence chains (sets of mention spans) provided by
in-document coreference resolution systems such
as included in the Stanford NLP toolkit. Results
indicated that the chains are of sufficient quality
so as not to limit performance, though future work
</bodyText>
<footnote confidence="0.369438">
1
https://github.com/hltcoe/parma
</footnote>
<page confidence="0.930617">
63
</page>
<equation confidence="0.405554">
\x0cRF
</equation>
<bodyText confidence="0.71468825">
Australian [police]1 have [arrested]2 a man in the western city of Perth over an alleged [plot]3 to [bomb]4 Israeli diplomatic
[buildings]5 in the country , police and the suspect s [lawyer]6 [said]7
Federal [police]1 have [arrested]2 a man over an [alleged]5 [plan]3 to [bomb]4 Israeli diplomatic [posts]8 in Australia , the
suspect s [attorney]6 [said]7 Tuesday
</bodyText>
<sectionHeader confidence="0.756737" genericHeader="method">
LDC MTC
</sectionHeader>
<bodyText confidence="0.906941333333333">
As I [walked]1 to the [veranda]2 side , I [saw]2 that a [tent]3 is being decorated for [Mahfil-e-Naat]4 -LRB- A [get-together]5
in which the poetic lines in praise of Prophet Mohammad are recited -RRB-
I [came]1 towards the [balcony]2 , and while walking over there I [saw]2 that a [camp]3 was set up outside for the [Naatia]4
</bodyText>
<figure confidence="0.561084">
[meeting]5 .
</figure>
<figureCaption confidence="0.999575">
Figure 1: Example of gold-standard alignment pairs from Roth and Franks data set and our data set
</figureCaption>
<bodyText confidence="0.992117230769231">
created from the LDCs Multiple Translation Corpora. The RF data set exhibits high lexical overlap,
where most of the alignments are between identical words like police-police and said-said. The LDC
MTC was constructed to increase lexical diversity, leading to more challenging alignments like veranda-
balcony and tent-camp
may relax this assumption.
We refer to a predicate or an argument as an
item with type predicate or argument. An align-
ment between two documents is a subset of all
pairs of items in either documents with the same
type.2 We call the two documents being aligned
the source document S and the target document
T. Items are referred to by their index, and ai,j is a
binary variable representing an alignment between
</bodyText>
<equation confidence="0.60992475">
item i in S and item j in T. A full alignment is an
assignment ~
a = {aij : i NS, j NT }, where
NS and NT are the set of item indices for S and T
</equation>
<bodyText confidence="0.997474">
respectively.
We train a logistic regression model on exam-
ple alignmentsand maximize the likelihood of a
document alignment under the assumption that the
item alignments are independent. Our objective
is to maximize the log-likelihood of all p(S, T)
with an L1 regularizer (with parameter ). After
learning model parameters w by regularized max-
imum likelihood on training data, we introducing
a threshold on alignment probabilities to get a
classifier. We perform line search on and choose
the value that maximizes F1 on dev data. Train-
ing was done using the Mallet toolkit (McCallum,
2002).
</bodyText>
<subsectionHeader confidence="0.517987">
2.1 Features
</subsectionHeader>
<bodyText confidence="0.998968714285714">
The focus of PARMA is the integration of a diverse
range of features based on existing lexical seman-
tic resources. We built PARMA on a supervised
framework to take advantage of this wide variety
of features since they can describe many different
correlated aspects of generation. The following
features cover the spectrum from high-precision
</bodyText>
<page confidence="0.972995">
2
</page>
<bodyText confidence="0.993671909090909">
Note that type is not the same thing as part of speech: we
allow nominal predicates like death.
to high-recall. Each feature has access to the pro-
posed argument or predicate spans to be linked and
the containing sentences as context. While we use
supervised learning, some of the existing datasets
for this task are very small. For extra training data,
we pool material from different datasets and use
the multi-domain split feature space approach to
learn dataset specific behaviors (Daume, 2007).
Features in general are defined over mention
spans or head tokens, but we split these features
to create separate feature-spaces for predicates and
arguments.3
For argument coref chains we heuristically
choose a canonical mention to represent each
chain, and some features only look at this canon-
ical mention. The canonical mention is cho-
sen based on length,4 information about the head
word,5 and position in the document.6 In most
cases, coref chains that are longer than one are
proper nouns and the canonical mention is the first
and longest mention (outranking pronominal ref-
erences and other name shortenings).
PPDB We use lexical features from the Para-
phrase Database (PPDB) (Ganitkevitch et al.,
2013). PPDB is a large set of paraphrases ex-
tracted from bilingual corpora using pivoting tech-
niques. We make use of the English lexical portion
which contains over 7 million rules for rewriting
terms like planet and earth. PPDB offers a
variety of conditional probabilities for each (syn-
chronous context free grammar) rule, which we
</bodyText>
<page confidence="0.989633">
3
</page>
<bodyText confidence="0.9980765">
While conceptually cleaner, In practice we found this
splitting to have no impact on performance.
</bodyText>
<page confidence="0.981669">
4
</page>
<bodyText confidence="0.840567">
in tokens, not counting some words like determiners and
auxiliary verbs
</bodyText>
<page confidence="0.957545">
5
</page>
<bodyText confidence="0.916504">
like its part of speech tag and whether the it was tagged
as a named entity
</bodyText>
<page confidence="0.993255">
6
</page>
<bodyText confidence="0.999184">
mentions that appear earlier in the document and earlier
in a given sentence are given preference
</bodyText>
<page confidence="0.996398">
64
</page>
<bodyText confidence="0.993773320512821">
\x0ctreat as independent experts. For each of these rule
probabilities (experts), we find all rules that match
the head tokens of a given alignment and have a
feature for the max and harmonic mean of the log
probabilities of the resulting rule set.
FrameNet FrameNet is a lexical database based
on Charles Fillmores Frame Semantics (Fill-
more, 1976; Baker et al., 1998). The database
(and the theory) is organized around seman-
tic frames that can be thought of as descrip-
tions of events. Frames crucially include spec-
ification of the participants, or Frame Elements,
in the event. The Destroying frame, for in-
stance, includes frame elements Destroyer or
Cause Undergoer. Frames are related to other
frames through inheritance and perspectivization.
For instance the frames Commerce buy and
Commerce sell (with respective lexical real-
izations buy and sell) are both perspectives of
Commerce goods-transfer (no lexical re-
alizations) which inherits from Transfer (with
lexical realization transfer).
We compute a shortest path between headwords
given edges (hypernym, hyponym, perspectivized
parent and child) in FrameNet and bucket by dis-
tance to get features. We also have a binary feature
for whether two tokens evoke the same frame.
TED Alignments Given two predicates or argu-
ments in two sentences, we attempt to align the
two sentences they appear in using a Tree Edit
Distance (TED) model that aligns two dependency
trees, based on the work described by (Yao et al.,
2013). We represent a node in a dependency tree
with three fields: lemma, POS tag and the type
of dependency relation to the nodes parent. The
TED model aligns one tree with the other using
the dynamic programming algorithm of Zhang and
Shasha (1989) with three predefined edits: dele-
tion, insertion and substitution, seeking a solution
yielding the minimum edit cost. Once we have
built a tree alignment, we extract features for 1)
whether the heads of the two phrases are aligned
and 2) the count of how many tokens are aligned
in both trees.
WordNet WordNet (Miller, 1995) is a database
of information (synonyms, hypernyms, etc.) per-
taining to words and short phrases. For each entry,
WordNet provides a set of synonyms, hypernyms,
etc. Given two spans, we use WordNet to deter-
mine semantic similarity by measuring how many
synonym (or other) edges are needed to link two
terms. Similar words will have a short distance.
For features, we find the shortest path linking the
head words of two mentions using synonym, hy-
pernym, hyponym, meronym, and holonym edges
and bucket the length.
String Transducer To represent similarity be-
tween arguments that are names, we use a stochas-
tic edit distance model. This stochastic string-to-
string transducer has latent edit and no edit
regions where the latent regions allow the model
to assign high probability to contiguous regions of
edits (or no edits), which are typical between vari-
ations of person names. In an edit region, param-
eters govern the relative probability of insertion,
deletion, substitution, and copy operations. We
use the transducer model of Andrews et al. (2012).
Since in-domain name pairs were not available, we
picked 10,000 entities at random from Wikipedia
to estimate the transducer parameters. The entity
labels were used as weak supervision during EM,
as in Andrews et al. (2012).
For a pair of mention spans, we compute the
conditional log-likelihood of the two mentions go-
ing both ways, take the max, and then bucket to get
binary features. We duplicate these features with
copies that only fire if both mentions are tagged as
PER, ORG or LOC.
</bodyText>
<sectionHeader confidence="0.997439" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.983131333333333">
We consider three datasets for evaluating PARMA.
For richer annotations that include lemmatiza-
tions, part of speech, NER, and in-doc corefer-
ence, we pre-processed each of the datasets using
tools7 similar to those used to create the Annotated
Gigaword corpus (Napoles et al., 2012).
Extended Event Coreference Bank Based on
the dataset of Bejan and Harabagiu (2010), Lee et
al. (2012) introduced the Extended Event Coref-
erence Bank (EECB) to evaluate cross-document
event coreference. EECB provides document clus-
ters, within which entities and events may corefer.
Our task is different from Lee et al. but we can
modify the corpus setup to support our task. To
produce source and target document pairs, we se-
lect the first document within every cluster as the
source and each of the remaining documents as
target documents (i.e. N 1 pairs for a cluster
of size N). This yielded 437 document pairs.
Roth and Frank The only existing dataset for
our task is from Roth and Frank (2012) (RF), who
</bodyText>
<page confidence="0.968139">
7
</page>
<footnote confidence="0.385726">
https://github.com/cnap/anno-pipeline
</footnote>
<page confidence="0.993371">
65
</page>
<bodyText confidence="0.991057755102041">
\x0cannotated documents from the English Gigaword
Fifth Edition corpus (Parker et al., 2011). The data
was generated by clustering similar news stories
from Gigaword using TF-IDF cosine similarity of
their headlines. This corpus is small, containing
only 10 document pairs in the development set and
60 in the test set. To increase the training size,
we train PARMA with 150 randomly selected doc-
ument pairs from both EECB and MTC, and the
entire dev set from Roth and Frank using multi-
domain feature splitting. We tuned the threshold
on the Roth and Frank dev set, but choose the
regularizer based on a grid search on a 5-fold
version of the EECB dataset.
Multiple Translation Corpora We constructed
a new predicate argument alignment dataset
based on the LDC Multiple Translation Corpora
(MTC),8 which consist of multiple English trans-
lations for foreign news articles. Since these mul-
tiple translations are semantically equivalent, they
provide a good resource for aligned predicate ar-
gument pairs. However, finding good pairs is a
challenge: we want pairs with significant overlap
so that they have predicates and arguments that
align, but not documents that are trivial rewrites
of each other. Roth and Frank selected document
pairs based on clustering, meaning that the pairs
had high lexical overlap, often resulting in mini-
mal rewrites of each other. As a result, despite ig-
noring all context, their baseline method (lemma-
alignment) worked quite well.
To create a more challenging dataset, we se-
lected document pairs from the multiple transla-
tions that minimize the lexical overlap (in En-
glish). Because these are translations, we know
that there are equivalent predicates and arguments
in each pair, and that any lexical variation pre-
serves meaning. Therefore, we can select pairs
with minimal lexical overlap in order to create
a system that truly stresses lexically-based align-
ment systems.
Each document pair has a correspondence be-
tween sentences, and we run GIZA++ on these
sentences to produce token-level alignments. We
take all aligned nouns as arguments and all aligned
verbs (excluding be-verbs, light verbs, and report-
ing verbs) as predicates. We then add negative ex-
amples by randomly substituting half of the sen-
tences in one document with sentences from an-
</bodyText>
<page confidence="0.703665">
8
</page>
<figure confidence="0.922078615384615">
LDC2010T10, LDC2010T11, LDC2010T12,
LDC2010T14, LDC2010T17, LDC2010T23, LDC2002T01,
LDC2003T18, and LDC2005T05
0.3 0.4 0.5 0.6 0.7 0.8
0.0
0.2
0.4
0.6
0.8
1.0
Performance vs Lexical Overlap
Doc-pair Cosine Similarity
F1
</figure>
<figureCaption confidence="0.997487">
Figure 2: We plotted the PARMAs performance on
</figureCaption>
<bodyText confidence="0.976857322580645">
each of the document pairs. Red squares show the
F1 for individual document pairs drawn from Roth
and Franks data set, and black circles show F1 for
our Multiple Translation Corpora test set. The x-
axis represents the cosine similarity between the
document pairs. On the RF data set, performance
is correlated with lexical similarity. On our more
lexically diverse set, this is not the case. This
could be due to the fact that some of the docu-
ments in the RF sets are minor re-writes of the
same newswire story, making them easy to align.
other corpus, guaranteed to be unrelated. The
amount of substitutions we perform can vary the
relatedness of the two documents in terms of
the predicates and arguments that they talk about.
This reflects our expectation of real world data,
where we do not expect perfect overlap in predi-
cates and arguments between a source and target
document, as you would in translation data.
Lastly, we prune any document pairs that have
more than 80 predicates or arguments or have a
Jaccard index on bags of lemmas greater than 0.5,
to give us a dataset of 328 document pairs.
Metric We use precision, recall, and F1. For the
RF dataset, we follow Roth and Frank (2012) and
Cohn et al. (2008) and evaluate on a version of F1
that considers SURE and POSSIBLE links, which
are available in the RF data. Given an alignment
to be scored A and a reference alignment B which
contains SURE and POSSIBLE links, Bs and Bp re-
spectively, precision and recall are:
</bodyText>
<equation confidence="0.955365571428571">
P =
|A Bp|
|A|
R =
|A Bs|
|Bs|
(1)
</equation>
<page confidence="0.952724">
66
</page>
<table confidence="0.9990795">
\x0cF1 P R
EECB lemma 63.5 84.8 50.8
PARMA 74.3 80.5 69.0
RF lemma 48.3 40.3 60.3
Roth and Frank 54.8 59.7 50.7
PARMA 57.6 52.4 64.0
MTC lemma 42.1 51.3 35.7
PARMA 59.2 73.4 49.6
</table>
<tableCaption confidence="0.99743">
Table 1: PARMA outperforms the baseline lemma
</tableCaption>
<bodyText confidence="0.972843">
matching system on the three test sets, drawn from
the Extended Event Coreference Bank, Roth and
Franks data, and our set created from the Multiple
Translation Corpora. PARMA achieves a higher F1
and recall score than Roth and Franks reported
result.
and F1 as the harmonic mean of the two. Results
for EECB and MTC reflect 5-fold cross validation,
and RF uses the given dev/test split.
Lemma baseline Following Roth and Frank we
include a lemma baseline, in which two predicates
or arguments align if they have the same lemma.9
</bodyText>
<sectionHeader confidence="0.999938" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999143444444444">
On every dataset PARMA significantly improves
over the lemma baselines (Table 1). On RF,
compared to Roth and Frank, the best published
method for this task, we also improve, making
PARMA the state of the art system for this task.
Furthermore, we expect that the smallest improve-
ments over Roth and Frank would be on RF, since
there is little training data. We also note that com-
pared to Roth and Frank we obtain much higher
recall but lower precision.
We also observe that MTC was more challeng-
ing than the other datasets, with a lower lemma
baseline. Figure 2 shows the correlation between
document similarity and document F1 score for
RF and MTC. While for RF these two measures
are correlated, they are uncorrelated for MTC. Ad-
ditionally, there is more data in the MTC dataset
which has low cosine similarity than in RF.
</bodyText>
<sectionHeader confidence="0.998299" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9949518">
PARMA achieves state of the art performance on
three datasets for predicate argument alignment.
It builds on the development of lexical semantic
resources and provides a platform for learning to
utilize these resources. Additionally, we show that
</bodyText>
<page confidence="0.939393">
9
</page>
<bodyText confidence="0.99973825">
We could not reproduce lemma from Roth and Frank
(shown in Table 1) due to a difference in lemmatizers. We ob-
tained 55.4; better than their system but worse than PARMA.
task difficulty can be strongly tied to lexical simi-
larity if the evaluation dataset is not chosen care-
fully, and this provides an artificially high baseline
in previous work. PARMA is robust to drops in lex-
ical similarity and shows large improvements in
those cases. PARMA will serve as a useful bench-
mark in determining the value of more sophis-
ticated models of predicate-argument alignment,
which we aim to address in future work.
While our system is fully supervised, and thus
dependent on manually annotated examples, we
observed here that this requirement may be rela-
tively modest, especially for in-domain data.
</bodyText>
<sectionHeader confidence="0.944027" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.992553916666667">
We thank JHU HLTCOE for hosting the winter
MiniSCALE workshop that led to this collabora-
tive work. This material is based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program). The U.S. Government
is authorized to reproduce and distribute reprints
for Governmental purposes. The views and con-
clusions contained in this publication are those of
the authors and should not be interpreted as repre-
senting official policies or endorsements of NSF,
DARPA, or the U.S. Government.
</bodyText>
<sectionHeader confidence="0.986019" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992099708333333">
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In Empirical Methods in Natural
Language Processing (EMNLP).
Amit Bagga and Breck Baldwin. 1999. Cross-
document event coreference: Annotations, exper-
iments, and observations. In Proceedings of the
Workshop on Coreference and its Applications,
pages 18. Association for Computational Linguis-
tics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL 98, pages 8690, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL 10, pages 14121422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.982147">
67
</page>
<reference confidence="0.998732296875">
\x0cTrevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
put. Linguist., 34(4):597614, December.
Hal Daume. 2007. Frustratingly easy domain adap-
tation. In Annual meeting-association for computa-
tional linguistics, volume 45, page 256.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Conference on
Computational Linguistics (Coling).
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language and Speech, 280(1):20
32.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 945
954. Association for Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL).
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):3941.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In AKBC-
WEKEX Workshop at NAACL 2012, June.
Robert Parker, David Graff, Jumbo Kong, Ke Chen,
and Kazuaki Maeda. 2011. English gigaword fifth
edition.
Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable
texts: A new corpus for a new task. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 218
227, Montreal, Canada, 7-8 June. Association for
Computational Linguistics.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013. Answer extraction as
sequence tagging with tree edit distance. In North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):12451262, De-
cember.
</reference>
<page confidence="0.943904">
68
</page>
<figure confidence="0.282505">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.324626">
<note confidence="0.860403666666667">b&amp;apos;Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 6368, Sofia, Bulgaria, August 4-9 2013. c 2013 Association for Computational Linguistics</note>
<title confidence="0.969139">PARMA: A Predicate Argument Aligner</title>
<author confidence="0.998577333333333">Travis Wolfe</author>
<author confidence="0.998577333333333">Benjamin Van_Durme</author>
<author confidence="0.998577333333333">Mark Dredze</author>
<author confidence="0.998577333333333">Nicholas Andrews</author>
<author confidence="0.998577333333333">Charley Beller</author>
<author confidence="0.998577333333333">Chris Callison-Burch</author>
<author confidence="0.998577333333333">Jay DeYoung</author>
<author confidence="0.998577333333333">Justin Snyder</author>
<author confidence="0.998577333333333">Jonathan Weese</author>
<author confidence="0.998577333333333">Tan Xu</author>
<affiliation confidence="0.89344925">and Xuchen Yao Human Language Technology Center of Excellence Johns Hopkins University, Baltimore, Maryland USA University of Maryland, College Park, Maryland USA</affiliation>
<abstract confidence="0.977402266666667">We introduce PARMA, a system for crossdocument, semantic predicate and argument alignment. Our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering, integrating them into a simple discriminative model. PARMA achieves state of the art results on an existing and a new dataset. We suggest that previous efforts have focussed on data that is biased and too easy, and we provide a more difficult dataset based on translation data with a low baseline which we beat by 17% F1.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Andrews</author>
<author>Jason Eisner</author>
<author>Mark Dredze</author>
</authors>
<title>Name phylogeny: A generative model of string variation.</title>
<date>2012</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="11698" citStr="Andrews et al. (2012)" startWordPosition="1903" endWordPosition="1906">using synonym, hypernym, hyponym, meronym, and holonym edges and bucket the length. String Transducer To represent similarity between arguments that are names, we use a stochastic edit distance model. This stochastic string-tostring transducer has latent edit and no edit regions where the latent regions allow the model to assign high probability to contiguous regions of edits (or no edits), which are typical between variations of person names. In an edit region, parameters govern the relative probability of insertion, deletion, substitution, and copy operations. We use the transducer model of Andrews et al. (2012). Since in-domain name pairs were not available, we picked 10,000 entities at random from Wikipedia to estimate the transducer parameters. The entity labels were used as weak supervision during EM, as in Andrews et al. (2012). For a pair of mention spans, we compute the conditional log-likelihood of the two mentions going both ways, take the max, and then bucket to get binary features. We duplicate these features with copies that only fire if both mentions are tagged as PER, ORG or LOC. 3 Evaluation We consider three datasets for evaluating PARMA. For richer annotations that include lemmatizat</context>
</contexts>
<marker>Andrews, Eisner, Dredze, 2012</marker>
<rawString>Nicholas Andrews, Jason Eisner, and Mark Dredze. 2012. Name phylogeny: A generative model of string variation. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Crossdocument event coreference: Annotations, experiments, and observations.</title>
<date>1999</date>
<booktitle>In Proceedings of the Workshop on Coreference and its Applications,</booktitle>
<pages>18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1974" citStr="Bagga and Baldwin (1999)" startWordPosition="301" endWordPosition="304">istory of entity disambiguation both within and across documents. While most information extraction work focuses on entities and noun phrases, there have been a few attempts at predicate, or event, disambiguation. Commonly a situational predicate is taken to correspond to either an event or a state, lexically realized in verbs such as elect or nominalizations such as election. Similar to entity coreference resolution, almost all of this work assumes unanchored mentions: predicate argument tuples are grouped together based on coreferent events. The first work on event coreference dates back to Bagga and Baldwin (1999). More recently, this task has been considered by Bejan and Harabagiu (2010) and Lee et al. (2012). As with unanchored entity disambiguation, these methods rely on clustering methods and evaluation metrics. Another view of predicate disambiguation seeks to link or align predicate argument tuples to an existing anchored resource containing references to events or actions, similar to anchored entity disambiguation (entity linking) (Dredze et al., 2010; Han and Sun, 2011). The most relevant, and perhaps only, work in this area is that of Roth and Frank (2012) who linked predicates across document</context>
</contexts>
<marker>Bagga, Baldwin, 1999</marker>
<rawString>Amit Bagga and Breck Baldwin. 1999. Crossdocument event coreference: Annotations, experiments, and observations. In Proceedings of the Workshop on Coreference and its Applications, pages 18. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics -Volume 1, ACL 98,</booktitle>
<pages>8690</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8973" citStr="Baker et al., 1998" startWordPosition="1459" endWordPosition="1462"> tokens, not counting some words like determiners and auxiliary verbs 5 like its part of speech tag and whether the it was tagged as a named entity 6 mentions that appear earlier in the document and earlier in a given sentence are given preference 64 \x0ctreat as independent experts. For each of these rule probabilities (experts), we find all rules that match the head tokens of a given alignment and have a feature for the max and harmonic mean of the log probabilities of the resulting rule set. FrameNet FrameNet is a lexical database based on Charles Fillmores Frame Semantics (Fillmore, 1976; Baker et al., 1998). The database (and the theory) is organized around semantic frames that can be thought of as descriptions of events. Frames crucially include specification of the participants, or Frame Elements, in the event. The Destroying frame, for instance, includes frame elements Destroyer or Cause Undergoer. Frames are related to other frames through inheritance and perspectivization. For instance the frames Commerce buy and Commerce sell (with respective lexical realizations buy and sell) are both perspectives of Commerce goods-transfer (no lexical realizations) which inherits from Transfer (with lexi</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics -Volume 1, ACL 98, pages 8690, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Unsupervised event coreference resolution with rich linguistic features.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL 10,</booktitle>
<pages>14121422</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2050" citStr="Bejan and Harabagiu (2010)" startWordPosition="313" endWordPosition="316">st information extraction work focuses on entities and noun phrases, there have been a few attempts at predicate, or event, disambiguation. Commonly a situational predicate is taken to correspond to either an event or a state, lexically realized in verbs such as elect or nominalizations such as election. Similar to entity coreference resolution, almost all of this work assumes unanchored mentions: predicate argument tuples are grouped together based on coreferent events. The first work on event coreference dates back to Bagga and Baldwin (1999). More recently, this task has been considered by Bejan and Harabagiu (2010) and Lee et al. (2012). As with unanchored entity disambiguation, these methods rely on clustering methods and evaluation metrics. Another view of predicate disambiguation seeks to link or align predicate argument tuples to an existing anchored resource containing references to events or actions, similar to anchored entity disambiguation (entity linking) (Dredze et al., 2010; Han and Sun, 2011). The most relevant, and perhaps only, work in this area is that of Roth and Frank (2012) who linked predicates across document pairs, measuring the F1 of aligned pairs. Here we present PARMA, a new syst</context>
<context position="12568" citStr="Bejan and Harabagiu (2010)" startWordPosition="2046" endWordPosition="2049">tion spans, we compute the conditional log-likelihood of the two mentions going both ways, take the max, and then bucket to get binary features. We duplicate these features with copies that only fire if both mentions are tagged as PER, ORG or LOC. 3 Evaluation We consider three datasets for evaluating PARMA. For richer annotations that include lemmatizations, part of speech, NER, and in-doc coreference, we pre-processed each of the datasets using tools7 similar to those used to create the Annotated Gigaword corpus (Napoles et al., 2012). Extended Event Coreference Bank Based on the dataset of Bejan and Harabagiu (2010), Lee et al. (2012) introduced the Extended Event Coreference Bank (EECB) to evaluate cross-document event coreference. EECB provides document clusters, within which entities and events may corefer. Our task is different from Lee et al. but we can modify the corpus setup to support our task. To produce source and target document pairs, we select the first document within every cluster as the source and each of the remaining documents as target documents (i.e. N 1 pairs for a cluster of size N). This yielded 437 document pairs. Roth and Frank The only existing dataset for our task is from Roth </context>
</contexts>
<marker>Bejan, Harabagiu, 2010</marker>
<rawString>Cosmin Adrian Bejan and Sanda Harabagiu. 2010. Unsupervised event coreference resolution with rich linguistic features. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL 10, pages 14121422, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cTrevor Cohn</author>
<author>Chris Callison-Burch</author>
<author>Mirella Lapata</author>
</authors>
<title>Constructing corpora for the development and evaluation of paraphrase systems.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="17005" citStr="Cohn et al. (2008)" startWordPosition="2780" endWordPosition="2783">utions we perform can vary the relatedness of the two documents in terms of the predicates and arguments that they talk about. This reflects our expectation of real world data, where we do not expect perfect overlap in predicates and arguments between a source and target document, as you would in translation data. Lastly, we prune any document pairs that have more than 80 predicates or arguments or have a Jaccard index on bags of lemmas greater than 0.5, to give us a dataset of 328 document pairs. Metric We use precision, recall, and F1. For the RF dataset, we follow Roth and Frank (2012) and Cohn et al. (2008) and evaluate on a version of F1 that considers SURE and POSSIBLE links, which are available in the RF data. Given an alignment to be scored A and a reference alignment B which contains SURE and POSSIBLE links, Bs and Bp respectively, precision and recall are: P = |A Bp| |A| R = |A Bs| |Bs| (1) 66 \x0cF1 P R EECB lemma 63.5 84.8 50.8 PARMA 74.3 80.5 69.0 RF lemma 48.3 40.3 60.3 Roth and Frank 54.8 59.7 50.7 PARMA 57.6 52.4 64.0 MTC lemma 42.1 51.3 35.7 PARMA 59.2 73.4 49.6 Table 1: PARMA outperforms the baseline lemma matching system on the three test sets, drawn from the Extended Event Corefe</context>
</contexts>
<marker>Cohn, Callison-Burch, Lapata, 2008</marker>
<rawString>\x0cTrevor Cohn, Chris Callison-Burch, and Mirella Lapata. 2008. Constructing corpora for the development and evaluation of paraphrase systems. Comput. Linguist., 34(4):597614, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Annual meeting-association for computational linguistics,</booktitle>
<volume>45</volume>
<pages>256</pages>
<contexts>
<context position="7210" citStr="Daume, 2007" startWordPosition="1169" endWordPosition="1170">e many different correlated aspects of generation. The following features cover the spectrum from high-precision 2 Note that type is not the same thing as part of speech: we allow nominal predicates like death. to high-recall. Each feature has access to the proposed argument or predicate spans to be linked and the containing sentences as context. While we use supervised learning, some of the existing datasets for this task are very small. For extra training data, we pool material from different datasets and use the multi-domain split feature space approach to learn dataset specific behaviors (Daume, 2007). Features in general are defined over mention spans or head tokens, but we split these features to create separate feature-spaces for predicates and arguments.3 For argument coref chains we heuristically choose a canonical mention to represent each chain, and some features only look at this canonical mention. The canonical mention is chosen based on length,4 information about the head word,5 and position in the document.6 In most cases, coref chains that are longer than one are proper nouns and the canonical mention is the first and longest mention (outranking pronominal references and other </context>
</contexts>
<marker>Daume, 2007</marker>
<rawString>Hal Daume. 2007. Frustratingly easy domain adaptation. In Annual meeting-association for computational linguistics, volume 45, page 256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Paul McNamee</author>
<author>Delip Rao</author>
<author>Adam Gerber</author>
<author>Tim Finin</author>
</authors>
<title>Entity disambiguation for knowledge base population.</title>
<date>2010</date>
<booktitle>In Conference on Computational Linguistics (Coling).</booktitle>
<contexts>
<context position="2427" citStr="Dredze et al., 2010" startWordPosition="370" endWordPosition="373">chored mentions: predicate argument tuples are grouped together based on coreferent events. The first work on event coreference dates back to Bagga and Baldwin (1999). More recently, this task has been considered by Bejan and Harabagiu (2010) and Lee et al. (2012). As with unanchored entity disambiguation, these methods rely on clustering methods and evaluation metrics. Another view of predicate disambiguation seeks to link or align predicate argument tuples to an existing anchored resource containing references to events or actions, similar to anchored entity disambiguation (entity linking) (Dredze et al., 2010; Han and Sun, 2011). The most relevant, and perhaps only, work in this area is that of Roth and Frank (2012) who linked predicates across document pairs, measuring the F1 of aligned pairs. Here we present PARMA, a new system for predicate argument alignment. As opposed to Roth and Frank, PARMA is designed as a a trainable platform for the incorporation of the sort of lexical semantic resources used in the related areas of Recognizing Textual Entailment (RTE) and Question Answering (QA). We demonstrate the effectiveness of this approach by achieving state of the art performance on the data of </context>
</contexts>
<marker>Dredze, McNamee, Rao, Gerber, Finin, 2010</marker>
<rawString>Mark Dredze, Paul McNamee, Delip Rao, Adam Gerber, and Tim Finin. 2010. Entity disambiguation for knowledge base population. In Conference on Computational Linguistics (Coling).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frame semantics and the nature of language. Annals of the New York Academy</title>
<date>1976</date>
<booktitle>of Sciences: Conference on the Origin and Development of Language and Speech,</booktitle>
<volume>280</volume>
<issue>1</issue>
<pages>32</pages>
<contexts>
<context position="8952" citStr="Fillmore, 1976" startWordPosition="1456" endWordPosition="1458">erformance. 4 in tokens, not counting some words like determiners and auxiliary verbs 5 like its part of speech tag and whether the it was tagged as a named entity 6 mentions that appear earlier in the document and earlier in a given sentence are given preference 64 \x0ctreat as independent experts. For each of these rule probabilities (experts), we find all rules that match the head tokens of a given alignment and have a feature for the max and harmonic mean of the log probabilities of the resulting rule set. FrameNet FrameNet is a lexical database based on Charles Fillmores Frame Semantics (Fillmore, 1976; Baker et al., 1998). The database (and the theory) is organized around semantic frames that can be thought of as descriptions of events. Frames crucially include specification of the participants, or Frame Elements, in the event. The Destroying frame, for instance, includes frame elements Destroyer or Cause Undergoer. Frames are related to other frames through inheritance and perspectivization. For instance the frames Commerce buy and Commerce sell (with respective lexical realizations buy and sell) are both perspectives of Commerce goods-transfer (no lexical realizations) which inherits fro</context>
</contexts>
<marker>Fillmore, 1976</marker>
<rawString>Charles J. Fillmore. 1976. Frame semantics and the nature of language. Annals of the New York Academy of Sciences: Conference on the Origin and Development of Language and Speech, 280(1):20 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Ppdb: The paraphrase database.</title>
<date>2013</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. Ppdb: The paraphrase database. In North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Le Sun</author>
</authors>
<title>A generative entitymention model for linking entities with knowledge base.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>945--954</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Han, Le Sun, 2011</marker>
<rawString>Xianpei Han and Le Sun. 2011. A generative entitymention model for linking entities with knowledge base. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 945 954. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Marta Recasens</author>
<author>Angel Chang</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Joint entity and event coreference resolution across documents.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL).</booktitle>
<contexts>
<context position="2072" citStr="Lee et al. (2012)" startWordPosition="318" endWordPosition="321">focuses on entities and noun phrases, there have been a few attempts at predicate, or event, disambiguation. Commonly a situational predicate is taken to correspond to either an event or a state, lexically realized in verbs such as elect or nominalizations such as election. Similar to entity coreference resolution, almost all of this work assumes unanchored mentions: predicate argument tuples are grouped together based on coreferent events. The first work on event coreference dates back to Bagga and Baldwin (1999). More recently, this task has been considered by Bejan and Harabagiu (2010) and Lee et al. (2012). As with unanchored entity disambiguation, these methods rely on clustering methods and evaluation metrics. Another view of predicate disambiguation seeks to link or align predicate argument tuples to an existing anchored resource containing references to events or actions, similar to anchored entity disambiguation (entity linking) (Dredze et al., 2010; Han and Sun, 2011). The most relevant, and perhaps only, work in this area is that of Roth and Frank (2012) who linked predicates across document pairs, measuring the F1 of aligned pairs. Here we present PARMA, a new system for predicate argum</context>
<context position="12587" citStr="Lee et al. (2012)" startWordPosition="2050" endWordPosition="2053">onditional log-likelihood of the two mentions going both ways, take the max, and then bucket to get binary features. We duplicate these features with copies that only fire if both mentions are tagged as PER, ORG or LOC. 3 Evaluation We consider three datasets for evaluating PARMA. For richer annotations that include lemmatizations, part of speech, NER, and in-doc coreference, we pre-processed each of the datasets using tools7 similar to those used to create the Annotated Gigaword corpus (Napoles et al., 2012). Extended Event Coreference Bank Based on the dataset of Bejan and Harabagiu (2010), Lee et al. (2012) introduced the Extended Event Coreference Bank (EECB) to evaluate cross-document event coreference. EECB provides document clusters, within which entities and events may corefer. Our task is different from Lee et al. but we can modify the corpus setup to support our task. To produce source and target document pairs, we select the first document within every cluster as the source and each of the remaining documents as target documents (i.e. N 1 pairs for a cluster of size N). This yielded 437 document pairs. Roth and Frank The only existing dataset for our task is from Roth and Frank (2012) (R</context>
</contexts>
<marker>Lee, Recasens, Chang, Surdeanu, Jurafsky, 2012</marker>
<rawString>Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, and Dan Jurafsky. 2012. Joint entity and event coreference resolution across documents. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://www.cs.umass.edu/ mccallum/mallet.</note>
<contexts>
<context position="6354" citStr="McCallum, 2002" startWordPosition="1030" endWordPosition="1031">item indices for S and T respectively. We train a logistic regression model on example alignmentsand maximize the likelihood of a document alignment under the assumption that the item alignments are independent. Our objective is to maximize the log-likelihood of all p(S, T) with an L1 regularizer (with parameter ). After learning model parameters w by regularized maximum likelihood on training data, we introducing a threshold on alignment probabilities to get a classifier. We perform line search on and choose the value that maximizes F1 on dev data. Training was done using the Mallet toolkit (McCallum, 2002). 2.1 Features The focus of PARMA is the integration of a diverse range of features based on existing lexical semantic resources. We built PARMA on a supervised framework to take advantage of this wide variety of features since they can describe many different correlated aspects of generation. The following features cover the spectrum from high-precision 2 Note that type is not the same thing as part of speech: we allow nominal predicates like death. to high-recall. Each feature has access to the proposed argument or predicate spans to be linked and the containing sentences as context. While w</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://www.cs.umass.edu/ mccallum/mallet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="10649" citStr="Miller, 1995" startWordPosition="1734" endWordPosition="1735">on the work described by (Yao et al., 2013). We represent a node in a dependency tree with three fields: lemma, POS tag and the type of dependency relation to the nodes parent. The TED model aligns one tree with the other using the dynamic programming algorithm of Zhang and Shasha (1989) with three predefined edits: deletion, insertion and substitution, seeking a solution yielding the minimum edit cost. Once we have built a tree alignment, we extract features for 1) whether the heads of the two phrases are aligned and 2) the count of how many tokens are aligned in both trees. WordNet WordNet (Miller, 1995) is a database of information (synonyms, hypernyms, etc.) pertaining to words and short phrases. For each entry, WordNet provides a set of synonyms, hypernyms, etc. Given two spans, we use WordNet to determine semantic similarity by measuring how many synonym (or other) edges are needed to link two terms. Similar words will have a short distance. For features, we find the shortest path linking the head words of two mentions using synonym, hypernym, hyponym, meronym, and holonym edges and bucket the length. String Transducer To represent similarity between arguments that are names, we use a sto</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):3941.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Matthew Gormley</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Annotated gigaword.</title>
<date>2012</date>
<booktitle>In AKBCWEKEX Workshop at NAACL 2012,</booktitle>
<marker>Napoles, Gormley, Van Durme, 2012</marker>
<rawString>Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. 2012. Annotated gigaword. In AKBCWEKEX Workshop at NAACL 2012, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Jumbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2011</date>
<note>English gigaword fifth edition.</note>
<contexts>
<context position="13330" citStr="Parker et al., 2011" startWordPosition="2171" endWordPosition="2174"> clusters, within which entities and events may corefer. Our task is different from Lee et al. but we can modify the corpus setup to support our task. To produce source and target document pairs, we select the first document within every cluster as the source and each of the remaining documents as target documents (i.e. N 1 pairs for a cluster of size N). This yielded 437 document pairs. Roth and Frank The only existing dataset for our task is from Roth and Frank (2012) (RF), who 7 https://github.com/cnap/anno-pipeline 65 \x0cannotated documents from the English Gigaword Fifth Edition corpus (Parker et al., 2011). The data was generated by clustering similar news stories from Gigaword using TF-IDF cosine similarity of their headlines. This corpus is small, containing only 10 document pairs in the development set and 60 in the test set. To increase the training size, we train PARMA with 150 randomly selected document pairs from both EECB and MTC, and the entire dev set from Roth and Frank using multidomain feature splitting. We tuned the threshold on the Roth and Frank dev set, but choose the regularizer based on a grid search on a 5-fold version of the EECB dataset. Multiple Translation Corpora We con</context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2011</marker>
<rawString>Robert Parker, David Graff, Jumbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English gigaword fifth edition.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael Roth</author>
<author>Anette Frank</author>
</authors>
<title>Aligning predicate argument structures in monolingual comparable texts: A new corpus for a new task.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>218--227</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Canada,</location>
<contexts>
<context position="2536" citStr="Roth and Frank (2012)" startWordPosition="392" endWordPosition="395">on event coreference dates back to Bagga and Baldwin (1999). More recently, this task has been considered by Bejan and Harabagiu (2010) and Lee et al. (2012). As with unanchored entity disambiguation, these methods rely on clustering methods and evaluation metrics. Another view of predicate disambiguation seeks to link or align predicate argument tuples to an existing anchored resource containing references to events or actions, similar to anchored entity disambiguation (entity linking) (Dredze et al., 2010; Han and Sun, 2011). The most relevant, and perhaps only, work in this area is that of Roth and Frank (2012) who linked predicates across document pairs, measuring the F1 of aligned pairs. Here we present PARMA, a new system for predicate argument alignment. As opposed to Roth and Frank, PARMA is designed as a a trainable platform for the incorporation of the sort of lexical semantic resources used in the related areas of Recognizing Textual Entailment (RTE) and Question Answering (QA). We demonstrate the effectiveness of this approach by achieving state of the art performance on the data of Roth and Frank despite having little relevant training data. We then show that while the lemma match heuristi</context>
<context position="13184" citStr="Roth and Frank (2012)" startWordPosition="2153" endWordPosition="2156">2010), Lee et al. (2012) introduced the Extended Event Coreference Bank (EECB) to evaluate cross-document event coreference. EECB provides document clusters, within which entities and events may corefer. Our task is different from Lee et al. but we can modify the corpus setup to support our task. To produce source and target document pairs, we select the first document within every cluster as the source and each of the remaining documents as target documents (i.e. N 1 pairs for a cluster of size N). This yielded 437 document pairs. Roth and Frank The only existing dataset for our task is from Roth and Frank (2012) (RF), who 7 https://github.com/cnap/anno-pipeline 65 \x0cannotated documents from the English Gigaword Fifth Edition corpus (Parker et al., 2011). The data was generated by clustering similar news stories from Gigaword using TF-IDF cosine similarity of their headlines. This corpus is small, containing only 10 document pairs in the development set and 60 in the test set. To increase the training size, we train PARMA with 150 randomly selected document pairs from both EECB and MTC, and the entire dev set from Roth and Frank using multidomain feature splitting. We tuned the threshold on the Roth</context>
<context position="16982" citStr="Roth and Frank (2012)" startWordPosition="2775" endWordPosition="2778">ted. The amount of substitutions we perform can vary the relatedness of the two documents in terms of the predicates and arguments that they talk about. This reflects our expectation of real world data, where we do not expect perfect overlap in predicates and arguments between a source and target document, as you would in translation data. Lastly, we prune any document pairs that have more than 80 predicates or arguments or have a Jaccard index on bags of lemmas greater than 0.5, to give us a dataset of 328 document pairs. Metric We use precision, recall, and F1. For the RF dataset, we follow Roth and Frank (2012) and Cohn et al. (2008) and evaluate on a version of F1 that considers SURE and POSSIBLE links, which are available in the RF data. Given an alignment to be scored A and a reference alignment B which contains SURE and POSSIBLE links, Bs and Bp respectively, precision and recall are: P = |A Bp| |A| R = |A Bs| |Bs| (1) 66 \x0cF1 P R EECB lemma 63.5 84.8 50.8 PARMA 74.3 80.5 69.0 RF lemma 48.3 40.3 60.3 Roth and Frank 54.8 59.7 50.7 PARMA 57.6 52.4 64.0 MTC lemma 42.1 51.3 35.7 PARMA 59.2 73.4 49.6 Table 1: PARMA outperforms the baseline lemma matching system on the three test sets, drawn from th</context>
</contexts>
<marker>Roth, Frank, 2012</marker>
<rawString>Michael Roth and Anette Frank. 2012. Aligning predicate argument structures in monolingual comparable texts: A new corpus for a new task. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 218 227, Montreal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
<author>Peter Clark</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Answer extraction as sequence tagging with tree edit distance.</title>
<date>2013</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<marker>Yao, Van Durme, Clark, Callison-Burch, 2013</marker>
<rawString>Xuchen Yao, Benjamin Van Durme, Peter Clark, and Chris Callison-Burch. 2013. Answer extraction as sequence tagging with tree edit distance. In North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zhang</author>
<author>D Shasha</author>
</authors>
<title>Simple fast algorithms for the editing distance between trees and related problems.</title>
<date>1989</date>
<journal>SIAM J. Comput.,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="10324" citStr="Zhang and Shasha (1989)" startWordPosition="1677" endWordPosition="1680">d child) in FrameNet and bucket by distance to get features. We also have a binary feature for whether two tokens evoke the same frame. TED Alignments Given two predicates or arguments in two sentences, we attempt to align the two sentences they appear in using a Tree Edit Distance (TED) model that aligns two dependency trees, based on the work described by (Yao et al., 2013). We represent a node in a dependency tree with three fields: lemma, POS tag and the type of dependency relation to the nodes parent. The TED model aligns one tree with the other using the dynamic programming algorithm of Zhang and Shasha (1989) with three predefined edits: deletion, insertion and substitution, seeking a solution yielding the minimum edit cost. Once we have built a tree alignment, we extract features for 1) whether the heads of the two phrases are aligned and 2) the count of how many tokens are aligned in both trees. WordNet WordNet (Miller, 1995) is a database of information (synonyms, hypernyms, etc.) pertaining to words and short phrases. For each entry, WordNet provides a set of synonyms, hypernyms, etc. Given two spans, we use WordNet to determine semantic similarity by measuring how many synonym (or other) edge</context>
</contexts>
<marker>Zhang, Shasha, 1989</marker>
<rawString>K. Zhang and D. Shasha. 1989. Simple fast algorithms for the editing distance between trees and related problems. SIAM J. Comput., 18(6):12451262, December.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>