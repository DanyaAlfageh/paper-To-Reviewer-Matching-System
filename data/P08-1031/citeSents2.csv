We employ Gibbs sampling, previously used in NLP by CITATION and CITATION, among others,,
This technique repeatedly samples from the conditional distributions of each hidden variable, eventually converging on a Markov chain whose stationary distribution is the posterior distribution of the hidden variables in the model CITATION,,
