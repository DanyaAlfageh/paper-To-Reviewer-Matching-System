The correlated topic model CITATION is one way to account for relationships between hidden topics; more structured representations, such as hierarchies, may also be considered,,
 is inspired by methods in the topic modeling literature, such as Latent Dirichlet Allocation (LDA) CITATION, where topics are treated as hidden variables that govern the distribution of words in a text,,
Recent work has examined coupling topic models with explicit supervision (CITATION; CITATION),,
This approach is inspired by methods in the topic modeling literature, such as Latent Dirichlet Allocation (LDA) CITATION, where topics are treated as hidden variables that govern the distribution of words in a text,,
Recent work has examined coupling topic models with explicit supervision (CITATION; CITATION),,
Our model is inherently capable of using any arbitrary source of similarity information; for a discussion of similarity metrics, see CITATION,,
4.2 Document-level Distributional Analysis Our analysis of the document text is based on probabilistic topic models such as LDA CITATION,,
Cohens kappa, a measure of interrater agreement ranging from zero to one, was 0.78 for this subset, indicating high agreement CITATION,,
Impact of paraphrasing As previously observed in entailment research CITATION, paraphrasing information contributes greatly to improved performance on semantic inference,,
We employ Gibbs sampling, previously used in NLP by CITATION and CITATION, among others,,
This technique repeatedly samples from the conditional distributions of each hidden variable, eventually converging on a Markov chain whose stationary distribution is the posterior distribution of the hidden variables in the model CITATION,,
We employ Gibbs sampling, previously used in NLP by CITATION and CITATION, among others,,
This technique repeatedly samples from the conditional distributions of each hidden variable, eventually converging on a Markov chain whose stationary distribution is the posterior distribution of the hidden variables in the model CITATION,,
We employ Gibbs sampling, previously used in NLP by CITATION and CITATION, among others,,
This technique repeatedly samples from the conditional distributions of each hidden variable, eventually converging on a Markov chain whose stationary distribution is the posterior distribution of the hidden variables in the model CITATION,,
2 Related Work Review Analysis Our approach relates to previous work on property extraction from reviews (CITATION; CITATION; CITATION),,
2 Related Work Review Analysis Our approach relates to previous work on property extraction from reviews (CITATION; CITATION; CITATION),,
 (e.g., CITATION), though a more theoretically sound treatment of the similarity matrix is an area for future research,,
Our model is inherently capable of using any arbitrary source of similarity information; for a discussion of similarity metrics, see CITATION,,
4.2 Document-level Distributional Analysis Our analysis of the document text is based on probabilistic topic models such as LDA CITATION,,
2 Related Work Review Analysis Our approach relates to previous work on property extraction from reviews (CITATION; CITATION; CITATION),,
For this purpose, we use the Rand Index CITATION, a measure of cluster similarity,,
become widely available (Vickery and WunschVincent, 2007; CITATION),,
 the topic modeling literature, such as Latent Dirichlet Allocation (LDA) CITATION, where topics are treated as hidden variables that govern the distribution of words in a text,,
Recent work has examined coupling topic models with explicit supervision (CITATION; CITATION),,
Such simplifying assumptions have been previously used with success in NLP (e.g., CITATION), though a more theoretically sound treatment of the similarity matrix is an area for future research,,
Our model is inherently capable of using any arbitrary source of similarity information; for a discussion of similarity metrics, see CITATION,,
