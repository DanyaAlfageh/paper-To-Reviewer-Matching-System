We repeat Ramshaw and Marcus Transformation Based NP chunking CITATION algorithm by substituting supertags for POS tags in the dataset,,
However, in CITATION, the accuracy of \x02\x04\x03\x06\x05\x11\x03\x04\x12\x10\x0b was achieved by a Viterbi search program that took about 5 days to supertag the test data,,
Since PMM suffers from the label bias problem CITATION, we have used two methods to cope with this problem,,
Since PMM suffers from the label bias problem CITATION, we have use,,
CITATION achieved an accuracy of \x02\x04\x03\x06\x05\x1a\t\x1b\x02\x10\x0b by combination of 5 distinct supertaggers,,
CITATION approached chucking by using Transformation Based Learning(TBL),,
We first use the fast TBL CITATION, a Transformation Based Learning algorithm, to repeat Ramshaw and Marcus experiment, and then apply the same program to our new dataset,,
The number of Srinivas(97) is based on footnote 1 of CITATION,,
Then we combine the results via pairwise voting as in (van Halteren et al., 1998; CITATION) as the final supertag,,
CITATION proposed a two-phase parsing model which includes chunking and attaching,,
mation than POS tags, which makes supertagging a useful pre-parsing tool, so-called, almost parsing CITATION,,
The accuracy of \x02\x04\x03\x06\x05\x11\x02\x04\x12\x10\x0b with our individual TBL chunker is close to results of POS-tag-based systems \x0cusing advanced machine learning algorithms, such as \x02\x04\x0f\x06\x05\x11\x0fA\x07\x17\x0b by voted MBL chunkers CITATION, \x02\x04\x03\x06\x05\x11\x10\x0b by SNoW chunker CITATION,,
CITATION proposed a new algorithm for parameter estimation as an alternate to CRF,,
Firstly supertags encode much more syntactical information than POS tags, which makes supertagging a useful pre-parsing tool, so-called, almost parsing CITATION,,
CITATION re,,
Following the estimation of distribution function in CITATION, we define confidence with a sigmoid r k\x15$ Vh& V D 7 Z 7 \\)(Cj \t \tbKtsuL 9 NOvxwyNz\x1fX{ \x1f HE |}~ |\x7f R:9 F R 7 (3) where 3 is the threshold of pqk , and s is set to 1,,
We test our supertagger on both the hand-coded supertags used in CITATION as well as the supertags extracted from Penn Treebank(PTB) (CITATION; CITATION),,
In CITATION, supertagging was used for NP chunking and it achieved an F-score of \x02\x04\x03\x06\x05\x08\x07\x17\x0b ,,
4.4 Related Work CITATION implemented an MEMM model for supertagging which is analogous to the POS tagging model of CITATION,,
In order to exploit syntactic dependencies in a larger context, we propose a new model of supertagging based on Sparse Network of Winnow (SNoW) CITATION,,
The distribution mass is then defined with normalized confidence #GkA$ V\x1b& V D 7 Z 7 \\&lt;(Cj r k $ V\x1b& V DE7 Z 7 \\&lt;( \x1f r k\x04$ Vh& V D 7 Z 7 \\)( (4) 4.2 Label Bias Problem In CITATION, it is shown that PMM and other non-generative finite-state models based on next-state classifiers share a weakness which they called the label bias problem: the transitions leaving a given state compete only,,
CITATION combined the traditional trigram model and head trigram model in their trigram mixed model,,
CITATION proposed a head trigram model in which the lexical selection of a word depended on the supertags of the previous two head words , instead of the supertags of the two words immediately leading the word of interest,,
CITATION reported a similar result with a trigram supertagger,,
In our first experiment, we use the same dataset as that of CITATION for our experiments,,
Our next experiment is with the set of supertags abstracted from PTB with Fei Xias LexTract CITATION,,
Compared with the supertaggers with the same decoding complexity CITATION, our algorithm achieves an error reduction of \x16\x17\x05\x1a\t\x0c\x0b ,,
By combining the results of these two supertaggers with pairwise voting, we achieve an accuracy of \x02\x04\x03\x06\x05\x08\x07 \t\x0c\x0b , an error reduction of \x03\x06\x05\x1a\t\x0c\x0b compared to \x02\x04\x03\x06\x05\x11\x03\x04\x12\x10\x0b , the best supertagging result to date CITATION,,
3 SNoW Sparse Network of Winnow (SNoW) CITATION is a learning architecture that is specially tailored for learning in the presence of a very large number of features where the decision for a single sample depends on only a small number of features,,
We repeat Ramshaw and Marcus Transformation Based NP chunking CITATION test by substituting supertags for POS tags in the dataset,,
CITATION proposed three methods of using classifiers in sequential inference, which are HMM, PMM and CSCL,,
It is noted in CITATION that one of the important properites of the sparse architecture of SNoW is that the complexity of processing an example depends only on the number of features active in it, \x1c\x1e\x1d , and is independent of the total number of features, \x1c \x1f , observed over the life time of the system and this is important in domains in which the total number of features in very large, but only a small number of them is active in each example,,
The counterpart of our algorithm in CITATION is the beam search on Model 8 with width of 5, which is the same as the beam width in our algorithm,,
The distribution mass is then defined with normalized confidence #GkA$ V\x1b& V D 7 Z 7 \\&lt;(Cj r k $ V\x1b& V DE7 Z 7 \\&lt;( \x1f r k\x04$ Vh& V D 7 Z 7 \\)( (4) 4.2 Label Bias Problem In CITATION, it is shown that PMM and other non-generative finite-state models based on next-state classifiers share a weakness which they called the label bias problem: the transitions leaving a given state compete only against each other, rather than against all other transitions in the model,,
We also propose a novel method of applying SNoW to sequential models in a way analogous to the Projection-base Markov Model (PMM) used in CITATION,,
We have tested our algorithms on both the handcoded tag set used in CITATION and supertags extracted for Penn Treebank(PTB),,
Xia extracted an LTAG-style grammar from PTB, and repeated Srinivas experiment CITATION on her supertag set,,
It is noted in CITATION that SNoWs output provides, in addition to the prediction, a robust confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference,,
On the same dataset as that of CITATION, our new supertagger achieves an accuracy of \x02\x04\x03\x06\x05\x08\x07 \t\x0c\x0b ,,
The previous best result on chunking in literature was achieved by Regularized Winnow CITATION, which took some of the pa,,
2 This number is based on footnote 1 of CITATION,,
Then we use Srinivas supertagger CITATION to supertag both the training and test data,,
The previous best result on chunking in literature was achieved by Regularized Winnow CITATION, which took some of the parsing results given by an English Slot Grammar-based parser as input to the chunker,,
On the dataset used in CITATION, our supertagger achieves an accuracy of \x02\x04\x03\x06\x05\x08\x07 \t\x0c\x0b ,,
Many machine learning techniques have been successfully applied to chunking tasks, such as Regularized Winnow CITATION, SVMs CITATION, CRFs CITATION, Maximum Entropy Model CITATION, Memory Based Learning CITATION and SNoW CITATION,,
As a first attempt, we use fast TBL CITATION, a TBL program, to repeat Ramshaw and Marcus experiment on the standard dataset,,
In CITATION, SNoW was used for text chunking,,
We first run the Brill POS tagger CITATION on both the training and the test data, and use POS tags as part of the input,,
1 Introduction In Lexicalized Tree-Adjoining Grammar (LTAG) (CITATION; CITATION), each word in a sentence is associated with an elementary tree, or a supertag CITATION,,
It is noted in CITATION that one of the important properites of the sparse architecture of SNoW is that the complexity of processing an example depends only on the number of features active in it, \x1c\x1e\x1d , and is independent of the total number of features, \x1c \x1f , observed over the life time of,,
twork of Winnow (SNoW) CITATION,,
We use the same pairwise voting algorithm as in CITATION,,
Both training and test data are first tagged by Brills POS tagger CITATION,,
2 Supertagging and NP Chunking In CITATION trigram models were used for supertagging, in which Good-Turing discounting technique and Katzs back-off model were employed,,
