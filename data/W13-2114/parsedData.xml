<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.4746895">
b&apos;Proceedings of the 14th European Workshop on Natural Language Generation, pages 105114,
Sofia, Bulgaria, August 8-9 2013. c
</bodyText>
<sectionHeader confidence="0.318383" genericHeader="abstract">
2013 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.957193">
Generating Natural Language Questions to Support Learning On-Line
</title>
<author confidence="0.995672">
David Lindberg Fred Popowich
</author>
<affiliation confidence="0.9462715">
School of Computing Science
Simon Fraser University
</affiliation>
<address confidence="0.876759">
Burnaby, BC, CANADA
</address>
<email confidence="0.958383">
dll4,popowich@sfu.ca
</email>
<author confidence="0.992865">
John Nesbit Phil Winne
</author>
<affiliation confidence="0.946256">
Faculty of Education
Simon Fraser University
</affiliation>
<address confidence="0.88254">
Burnaby, BC, CANADA
</address>
<email confidence="0.985724">
nesbit,winne@sfu.ca
</email>
<sectionHeader confidence="0.990381" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999378210526316">
When instructors prepare learning materi-
als for students, they frequently develop
accompanying questions to guide learn-
ing. Natural language processing technol-
ogy can be used to automatically generate
such questions but techniques used have
not fully leveraged semantic information
contained in the learning materials or the
full context in which the question genera-
tion task occurs. We introduce a sophisti-
cated template-based approach that incor-
porates semantic role labels into a system
that automatically generates natural lan-
guage questions to support online learn-
ing. While we have not yet incorporated
the full learning context into our approach,
our preliminary evaluation and evaluation
methodology indicate our approach is a
promising one for supporting learning.
</bodyText>
<sectionHeader confidence="0.996978" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998551470588235">
Ample research (e.g., Callender and McDaniel,
2007) shows that learners learn more, and more
deeply, if they are prompted to examine their
learning materials while and after they study. Of-
ten, these prompts consist of questions related to
the learning materials. After reading a given pas-
sage or section of text, learners are familiar with
learning exercises which consist of questions they
need to answer.
Questioning is one of the most common and in-
tensively studied instructional strategies used by
teachers (Rus and Graesser, 1989). Questions em-
bedded in text, or presented while learners are
studying text, are hypothesized to promote self-
explanation which is known to increase compre-
hension and enhance transfer of learning (e.g.,
Rittle-Johnson, 2006).
Traditionally, these questions have been con-
structed by educators. Recent research, though,
has investigated how natural language processing
techniques can be used to automatically generate
these questions (Kalady et al., 2010; Varga and
Ha, 2010; Ali et al., 2010; Mannem et al., 2010).
While the automated approaches have generally
focussed on syntactic features, we propose an ap-
proach that also takes semantic features into ac-
count, in conjunction with domain dependent and
domain independent templates motivated by ed-
ucational research. After introducing our ques-
tion generation system, we will provide a prelimi-
nary analysis of the performance of the system on
educational material, and then outline our future
plans to tailor the questions to the needs of spe-
cific learners and specific learning outcomes.
</bodyText>
<sectionHeader confidence="0.919854" genericHeader="method">
2 Question Generation from Text
</sectionHeader>
<bodyText confidence="0.885771666666667">
The task of question generation (QG) from text
can be broadly divided into three (not entirely dis-
joint) categories: syntax-based, semantics-based,
and template-based. Systems in the syntactic cat-
egory often use elements of semantics and vice-
versa. A system we would call template-based
must to some extent use syntactic and/or seman-
tic information. Regardless of the approach taken,
systems must perform at least four tasks:
</bodyText>
<listItem confidence="0.9200025">
1. content selection: picking spans of source
text (typically single sentences) from which
questions can be generated
2. target identification: determining which spe-
cific words and/or phrases should be asked
about
3. question formulation: determining the appro-
priate question(s) given the content identified
4. surface form generation: producing the final
surface-form realization
</listItem>
<bodyText confidence="0.385695">
Task 2 need not always precede task 3; target
identification can drive question formulation and
</bodyText>
<page confidence="0.999305">
105
</page>
<bodyText confidence="0.979251941176471">
\x0cvice-versa. A system constrained to generating
specific kinds of questions will select only the tar-
gets appropriate for those kinds of questions. Con-
versely, a system with broader generation capa-
bilities might pick targets more freely and (ide-
ally) generate only the questions that are appro-
priate for those targets. We consider the methods
used in performing tasks 2 and 4 to be the pri-
mary discriminators in determining the category
into which a given method is best placed. This is
not the only way one might classify a QG system.
However, we believe this method allows us to best
compare and contrast our approach with previous
approaches.
Syntax-based methods comprise a large portion
of the existing literature. Kalady et al. (2012),
Varga and Ha (2010), Wolfe (1976), and Ali et
al. (2010) provide a sample of these methods. Al-
though each of these efforts has differed on a few
details, they have followed the same basic strat-
egy: parse sentences using a syntactic parser, sim-
plify complex sentences, identify key phrases, and
apply syntactic transformation rules and question
word replacement.
The methods we have labeled semantics-
based use method(s) of target identification (task
2) that are primarily semantic, using techniques
such as semantic role labeling (SRL). Given a sen-
tence, a semantic role labeler identifies the pred-
icates (relations and actions) along with the se-
mantic entities associated with each predicate. Se-
mantic roles, as defined in PropBank (Palmer et
al., 2005), include Arg0, Arg1, ..., Arg5, and
ArgA. A set of modifiers is also defined and in-
</bodyText>
<construct confidence="0.818183166666667">
cludes ArgM-LOC (location), ArgM-EXT (ex-
tent), ArgM-DIS (discourse), ArgM-ADV (adver-
bial), ArgM-NEG (negation), ArgM-MOD (modal
verb), ArgM-CAU (cause), ArgM-TMP (time),
ArgM-PNC (purpose), ArgM-MNR (manner), and
ArgM-DIR (direction). We adopt the shorter
</construct>
<bodyText confidence="0.98774355">
CoNLL SRL shared task naming conventions
(Carreras and Marquez, 2005) (e.g., A0 and AM-
LOC).
Mannem et al. (2010), for example, introduce a
semantics-based system that combines SRL with
syntactic transformations. In the content selec-
tion stage, a single sentence is first parsed with
a semantic role labeler to identify potential tar-
gets. Targets are selected using simple selec-
tion criteria. Any of the predicate-specific se-
mantic arguments (A0-A5), if present, are consid-
ered valid targets. Mannem et al. further iden-
tify modifiers AM-MNR, AM-PUNC, AM-CAU,
AM-TMP, AM-LOC, and AM-DIS as potential
targets. These roles are used to generate addi-
tional questions that cannot be attained using only
the A0-A5 roles. For example, AM-LOC can be
used to generate a where question, and an AM-
TMP can be used to generate a when question. Af-
ter targets have been identified, these, along with
the complete SRL parse of the sentence are passed
to the question formulation stage. Two heuristics
are used to rank the generated questions. Ques-
tions are ranked first by the depth of their predi-
cate in the dependency parse of the original ques-
tion. This is based on the assumption that ques-
tions arising from main clauses are more desir-
able than those generated from deeper predicates.
In the second stage, questions with the same rank
are re-ranked according to the number of pronouns
they contain, with questions with fewer pronouns
having higher rank.
One limitation of the syntax and semantics-
based methods is that they generate questions by
rearranging the surface form of sentences. Ques-
tion templates offer the ability to ask questions that
are not so tightly-coupled to the exact wording of
the source text. A question template is any pre-
defined text with placeholder variables to be re-
placed with content from the source text. Ques-
tion templates allow question generation systems
to leverage human expertise in language genera-
tion.
The template-based system of Cai et al. (2006)
uses Natural Language Generation Markup Lan-
guage (NLGML), a language that can be used to
generate not only questions but any natural lan-
guage expression. NLGML uses syntactic pattern
matching and semantic features for content selec-
tion and question templates to guide question for-
mulation and surface-form realization. Note that
a pattern need not specify a complete syntax tree.
Additionally, patterns can impose semantic con-
straints. However, simple copy and paste tem-
plates are not a panacea for surface-form real-
ization. Mechanisms for changing capitalization
of words and changing verb conjugation (when
source sentence verbs are to appear in the output
text) need to be provided: NLGML provides some
such functions.
</bodyText>
<page confidence="0.99915">
106
</page>
<subsectionHeader confidence="0.403977">
\x0c3 Our Approach
</subsectionHeader>
<bodyText confidence="0.99053224">
We develop a template-based framework for QG.
The primary motivation for this decision is the
ability of a template-based approach to generate
questions that are not merely declarative to in-
terrogative transformations. We aim to address
some of the limitations of the existing approaches
outlined in the previous section while leveraging
some of their strengths in novel ways. We com-
bine the benefits of a semantics-based approach,
the most important of which is not being tightly-
constrained by syntax, with the surface-form flex-
ibility of a template-based approach.
The data used to develop our approach was ob-
tained from a collection of 25 documents prepared
for educational research purposes within the Fac-
ulty of Education at SFU. All hand-coded rules
we describe below were motivated by patterns ob-
served in this development data. This collection
was modeled after a high-school science curricu-
lum on global warming, with vocabulary and dis-
course appropriate for learners in that age group.
Although the collection included a glossary of key
terms and their definitions, this resource was used
only for evaluation purposes as described in Sec-
tion 4.
</bodyText>
<subsectionHeader confidence="0.990046">
3.1 Semantic-based templates
</subsectionHeader>
<bodyText confidence="0.984721733333333">
Previous template-based methods have used syn-
tactic pattern matching, which does provide a
great deal of flexibility in specifying sentences
appropriate for generating certain types of ques-
tions. However, this flexibility comes at the ex-
pense of generality. As seen in Wyse and Piwek
(2009), who use Stanford Tregex (Levy and An-
drew, 2006) for pattern matching, the specificity of
syntactic patterns can make it difficult to specify
a syntactic pattern of the desired scope. Further-
more, semantically similar entities can span dif-
ferent syntactic structures, and matching these re-
quires either multiple patterns (in the case of Cai
et al., 2006) or a more complicated pattern (in the
case of Wyse and Piwek, 2009).
If we want to develop templates that are se-
mantically motivated, more flexible in terms of
the content they successfully match, and more ap-
proachable for non-technical users, we need to
move away from syntactic pattern matching. In-
stead, we match semantic patterns. We define a
semantic pattern as the SRL parse of a sentence
and the named entities (if any) contained within
the span of each semantic role. We use Stanford
NER (Finkel et al., 2005) for named entity recog-
nition. Figure 1 shows a sentence and its corre-
sponding semantic pattern. Notice this sentence
has two predicates, each with its own semantic ar-
guments. Each of these predicate-argument struc-
tures is a distinct predicate frame.
</bodyText>
<figureCaption confidence="0.98509">
Figure 1: A sentence and its semantic pattern
</figureCaption>
<bodyText confidence="0.999061692307692">
Even the shallow semantics of SRL can identify
the semantically interesting portions of a sentence,
and these semantically-meaningful substrings can
span a range of syntactic patterns. Figure 2
shows a clear example of this phenomenon. In
this example, we see two sentences expressing
the same semantic relationship between two con-
cepts, namely, the fact that trapped heat causes
the Earths temperature to increase. In one case,
this causation is expressed in an adjective phrase,
while the other uses a sentence-initial preposi-
tional phrase. The parse trees are generated using
the Stanford Parser (Klein and Manning, 2003).
The AM-CAU semantic role captures the cause in
both sentences. It is impossible to accomplish the
same feat with a single NLGML pattern. However,
it is possible to capture both with a single Tregex
pattern.
The principle advantage of semantic pattern
matching is that a single semantic pattern casts a
narrow semantic net while casting a large syntactic
net. This means fewer patterns need to be defined
by the template author, and the patterns are more
compact.
Our templates have three components: plain-
text, slots, and slot options. Plaintext forms the
</bodyText>
<page confidence="0.998959">
107
</page>
<bodyText confidence="0.987307090909091">
\x0cFigure 2: Two different syntax subtrees subsumed
by a single semantic role
skeleton into which semantically-meaningful sub-
strings of a source sentence are inserted to create a
question. The only restrictions on the plaintext is
that it cannot contain any text that looks like a slot
but is not intended as one, and it cannot contain
the character sequence used to delineate the plain-
text from the slots appearing outside the plaintext.
Aside from these restrictions, any desired text is
valid.
Slots facilitate sentence and template matching.
They accept specific semantic arguments, and can
appear inside or outside the plaintext. These pro-
vide the semantic pattern against which a source
sentence is matched. A slot inside the plaintext
acts as a variable to be replaced by the correspond-
ing semantic role text from a matching sentence,
while any slots appearing outside the plaintext
serve only to provide additional pattern match-
ing criteria. The template author does not need
to specify the complete semantic pattern in each
template. Instead, only the portions relevant to the
desired question need to be specified. This is an
important point of contrast between our template-
based approach vs. syntax and semantics-based
approaches. We can choose to generate questions
that do not include any predicates from the source
sentence but instead ask more abstract or general
questions about other semantic constituents. We
believe these kinds of questions are better able to
escape the realm of the factoid, because they are
not constrained to the actions and relations de-
scribed by predicates.
Slot options function much like NLGML func-
tions and are of two types: modifiers and filters.
Modifiers apply transformations to the role text in-
serted into a slot, and filters enforce finer-grained
matching criteria. Predicate slots have their own
distinct set of options, while the other semantic
roles share a common set of options. A templates
slots and filters describe the necessary conditions
for the template to be matched with a source sen-
tence semantic pattern.
</bodyText>
<subsectionHeader confidence="0.995445">
3.2 Predicate slot options
</subsectionHeader>
<bodyText confidence="0.999586">
The predicate filter options restrict the predicates
that can match a predicate slot. With no filter
options specified, any predicate is considered a
match. Table 1 shows the complete list of filters.
</bodyText>
<subsectionHeader confidence="0.762568">
Filter Description
</subsectionHeader>
<bodyText confidence="0.939395666666667">
be predicate lemma must not be be
!be predicate lemma must be be
!have predicate lemma must not be have
</bodyText>
<tableCaption confidence="0.889127">
Table 1: Predicate filters
</tableCaption>
<bodyText confidence="0.999308304347826">
The selection of predicate filters might at first
seem oddly limited. Failing to consider the func-
tional differences between various types of verbs
(particularly auxiliary and copula) would indeed
produce low-quality questions and should in fact
be ignored in most cases. For example, consider
the sentence Dinosaurs, along with many other
animals, became extinct approximately 65 mil-
lion years ago. A question such as What did
dinosaurs, along with many other animals, be-
come? is not particularly useful. We can rec-
ognize copula predicates by their surrounding se-
mantic pattern, so in the broad sense, we do not
need to adopt any copula-specific rules.
The one exception to the above rule is any cop-
ula whose lemma is be. The be and !be filters
allow the presence or absence of such a predicate
to be detected. This capability is useful for two
reasons. First, the presence of such a predicate
gives us an inexpensive way to generate defini-
tion questions, even if the source text is not writ-
ten in the form of a definition. Although this will
over-generate definition questions, non-predicate
</bodyText>
<page confidence="0.99621">
108
</page>
<bodyText confidence="0.92807746">
\x0cfilters can be used to add additional mitigating
constraints. Second, requiring the absence of such
a predicate allows us to actively avoid generat-
ing certain kinds of ungrammatical or meaning-
less questions. Whether using one of these predi-
cates results in ungrammatical questions depends
on the wording of the underlying template, so we
provide the !be filter for the template author to
use as needed. Consider the sentence El Nino
is caused when the westerly winds are unusually
weak. Without the !be filter, one of our tem-
plates would generate the question When can El
Nino be? Applying the !be filter prevents this
question from being generated.
Like copula, auxiliary verbs are often not suit-
able for question generation. Fortunately, many
auxiliary verbs are also modal and are assigned
the label AM-MOD and so do not form predi-
cate frames of their own. Instead, they are in-
cluded in the frame of the predicate they modify.
In other cases auxiliary verbs are not modal, such
as in the sentence So far, scientists have not been
able to predict the long term effects of this wob-
ble. In this case, the auxiliary have is treated as
a separate predicate, but importantly, the span of
its A1 includes the predicate been. We provide
a non-predicate filter to prevent generation when
this overlap is present.
The !have filter is motivated by the observa-
tion that the predicate have can appear as a full,
non-copula predicate (with an A0 and A1) but of-
ten does not yield high-quality questions. For ex-
ample, consider the sentence This effect can have
a large impact on the Earths climate. Without the
!have filter, one of our templates would gener-
ate the question What can this effect have? With
the !have filter, that template does not yield any
questions from the given sentence.
Predicate modifiers allow the template author to
explicitly force a change in conjugation. See Ta-
ble 2 for the complete set of predicate modifiers,
where fps is an abbreviation for first person sin-
gular, sps for second person singular, and so on.
The lemma modifier can appear on its own. How-
ever, all other conjugation changes must specify
both a tense and a person. If no modifiers are used,
the predicate is copied as-is from the source sen-
tence. Although perfect is an aspect rather than a
tense, MorphAdorner1, which we use to conjugate
predicates, defines it as a tense, so we have imple-
</bodyText>
<footnote confidence="0.380144">
1
http://morphadorner.northwestern.edu
mented it as a tense filter.
</footnote>
<table confidence="0.883831111111111">
Modifier Tense Modifier
lemma lemma (dictionary form) fps
pres present sps
prespart present participle tps
past past fpp
pastpart past participle spp
perf perfect tpp
pastperf past perfect
pastperfpart past perfect participle
</table>
<tableCaption confidence="0.917058">
Table 2: Predicate modifiers
</tableCaption>
<subsectionHeader confidence="0.86862">
3.3 Non-predicate slot options
</subsectionHeader>
<bodyText confidence="0.9876339">
The filters for non-predicate slots impose addi-
tional syntactic and named entity restrictions on
any matching role text. As with predicates, the
absence of any non-predicate filters results in the
mere presence of the corresponding semantic role
being sufficient for matching. See Table 3 for the
complete list of non-predicate filters describing re-
strictions on the role text (RT), role span (RS), and
predicate frame (PF) in terms of the semantic type
of named entities (and in some cases in terms of
</bodyText>
<listItem confidence="0.841377285714286">
non-semantic features).
Filter Description
null PF must not contain this semantic role.
!nv RS must not contain a predicate
dur RT must contain DURATION
date RT must contain DATE
!date RT must not contain a DATE
loc RT must contain a LOCATION.
ne RT must contain a named entity
misc RT must contain a MISC
comp RT must contain a comparison
!comma RT must not contain a comma
singular RT must be singular
plural RT must be plural
</listItem>
<tableCaption confidence="0.96664">
Table 3: Non-predicate filters
</tableCaption>
<bodyText confidence="0.997756142857143">
The choice of filters again requires some expla-
nation. The null and !nv filters were foreshad-
owed above. For slots appearing outside the tem-
plates plaintext, the null filter explicitly requires
that the corresponding semantic role not be present
in a source sentence semantic pattern. An A0 slot
paired with the null filter is the mechanism al-
luded to earlier that allows for the recognition of
copula predicates without the need to examine the
predicate itself. The !nv filter can be used to pre-
vent ungrammatical questions. We observe that
if a role span does include a predicate, resulting
questions are often ungrammatical due to the con-
jugation of that predicate. Applying this filter to
</bodyText>
<page confidence="0.998063">
109
</page>
<bodyText confidence="0.99728323880597">
\x0cthe A1 of a predicate prevents generation from a
predicate frame whose predicate is a non-modal
auxiliary verb.
The named entity filters (dur, !dur, date,
loc, ne, and misc) are those most relevant to
the corpus we have used to evaluate our approach
and thus the easiest to experiment with effectively.
Because named entities are used only for filtering,
expanding the set of named entity filters is a trivial
task.
The filters comp, !comma, singular, and
plural are syntax-based filters. With the ex-
ception of !comma, these filters force the exam-
ination of the part-of-speech (POS) tags to de-
tect the desired features. The singular and
plural filters let templates be tailored to singu-
lar and plural arguments in any desired way, be-
yond simply selecting appropriate auxiliary verbs.
The type of comparison we search for when the
comp filter is used is quite specific. We search
for phrases that describe conditions that are atypi-
cal. These can be seen in phrases such unusually
weak, unseasonably warm, strangely absent,
and so on. These phrases are present when a word
whose POS tag is RB (adverb) is followed by a
word whose tag is JJ (adjective). Consider a sen-
tence such as El Nino is caused when the westerly
winds are unusually weak. The comp filter allows
us to generate questions such as What data would
indicate El Nino? or How do the conditions that
cause El Nino differ from normal conditions? Al-
though this heuristic does produce both false pos-
itives and false negatives, other syntactic features
such as comparative adverbs and comparative ad-
jectives are less semantically constrained. Further
investigation is needed to determine more flexible
ways to recognize descriptions of atypical condi-
tions.
We see two situations in which a comma ap-
pears within the span of a single semantic role.
The first situation occurs when a list of nouns is
serving the role, such as in Climate change in-
cludes changes in precipitation, temperature, and
pressure. Here, changes in precipitation, temper-
ature, and pressure is the A1 of the predicate in-
cludes. In cases where a question is only appro-
priate for single concept (e.g. temperature) rather
than a set of concepts, the !comma filter pre-
vents such a question from being generated from
the sentence above. This has implications for role
text containing appositives, the second situation in
which a comma appears within a single role span.
Such roles are rejected when !comma is used.
This is not ideal, as removing appositives does not
cause semantic roles to be lost from a semantic
pattern. Future work will address this problem.
The non-predicate modifiers (Table 4) serve two
purposes: to create more fluent questions and to
remove non-essential text. Note that the -tpp,
which forces the removal of trailing prepositional
phrases, can have undesired results when applied
to certain modifier roles, such as AM-LOC, AM-
MNR, and AM-TMP, when they appear in the tem-
plate plaintext. These modifiers often contain only
a prepositional phrase, and in such cases, -tpp
will result in an empty string being placed into the
template.
</bodyText>
<subsectionHeader confidence="0.854854">
Modifier Effect
</subsectionHeader>
<listItem confidence="0.877729666666667">
-lp If initial token is prep, remove it
-tpp If RT ends with PP, remove PP
-ldt If initial token is determiner, remove it
</listItem>
<tableCaption confidence="0.994789">
Table 4: Non-predicate modifiers
</tableCaption>
<subsectionHeader confidence="0.952992">
3.4 Our QG system
</subsectionHeader>
<bodyText confidence="0.998806888888889">
Figure 3 shows the architecture and data flow of
our QG system. One of the most important things
to observe about this architecture is that the tem-
plates are an external input. They are in no way
coupled to the system and can be modified as
needed without any system modifications.
Compared to most other approaches, we per-
form very little pre-processing. Syntax-based
methods in particular have been motivated to per-
form sentence simplification, because their meth-
ods are more likely to generate meaningful ques-
tions from short, succinct sentences. We have cho-
sen not to perform any sentence simplification.
This decision was motivated by the observation
that common methods of sentence simplification
can eliminate useful semantic content. For exam-
ple, Kalady et al. (2010) claim that prepositional
phrases are often not fundamental to the meaning
of a sentence, so they remove them when simpli-
fying a sentence. However, as Figure 4 shows,
a prepositional phrase can contain important se-
mantic information. In that example, removing the
prepositional phrase causes temporal information
to be lost.
One pre-processing step we do perform is
pronominal anaphora resolution (Charniak and El-
sner, 2009). Even though we do not split com-
</bodyText>
<page confidence="0.996922">
110
</page>
<figureCaption confidence="0.9993655">
\x0cFigure 3: System architecture and data flow
Figure 4: Semantic information can be lost dur-
</figureCaption>
<bodyText confidence="0.9555224">
ing sentence simplification. Removing the prepo-
sitional phrase from the first sentence leaves the
simpler second sentence, but the AM-TMP modi-
fier is lost.
plex sentences and therefore do not create new
sentences in which pronouns are separated from
their antecedents, this kind of anaphora resolution
remains an important step in limiting the number
of vague questions.
Each source sentence is tokenized and anno-
tated with POS tags, named entities, lemmata, and
its SRL parse. SRL is the cornerstone of our ap-
proach. We generate the SRL parse (Collobert
et al., 2011) in order to extract a set of predicate
frames. Questions are generated from individ-
ual predicate frames rather than entire sentences
(unless the sentence contains only one predicate
frame). Given a sentence, the semantic pattern of
each of its predicate frames is compared against
that of each template. Algorithm 1 describes the
process of matching a single predicate frame (pf)
to a single template (t). Although it is not stated in
Algorithm 1, the sentence-level tokenization, lem-
mata, named entities and POS tags are checked
as needed according to the templates slot filters.
If a predicate frame and template are matched,
they are passed to Algorithm 2, which fills tem-
plate slots with role text to produce a question.
Even in the absence of modifiers, all role text re-
ceives some additional processing before being in-
serted into its corresponding slot. These additional
steps include the removal of colons and the things
they introduce and the removal of text contained
in parentheses. We observe that these extra steps
lead to questions that are more meaningful.
Algorithm 1 patternsMatch(pf,t)
for all slot t do
if pf does not have slot.role then
if null 6 slot.filters then
return false
end if
else
for all filter slot.filters do
if pf.role does not match filter then
return false
</bodyText>
<equation confidence="0.378837">
end if
end for
end if
end for
</equation>
<bodyText confidence="0.9526275">
return true
Because we generate questions from predicate
frames rather than entire sentences, two sentences
describing the same semantic entities might result
in duplicate questions. To avoid duplicates we
keep only the first occurrence of a question.
Using slots and filters, we can now create some
interesting templates and see the questions they
</bodyText>
<page confidence="0.995117">
111
</page>
<bodyText confidence="0.974765925925926">
\x0cAlgorithm 2 fillTemplate(t,pf)
question text t.plaintext
for all slot t.plaintext slots do
role text pf.role(slot.role).text
for all modifier slot.modifiers do
applyModifier(role text,modifier)
end for
In question text, replace slot with role text
end for
return question text
yield. Table 5 shows some templates (T) that
match the sentence in Figure 1 and the questions
(Q) that result. Although the questions that are
generated are not answerable from the original
sentence, they were judged answerable from the
source document in our evaluation. The full set of
templates is provided in (Lindberg, 2013).
As recently as 12,500 years ago, the Earth was in the
midst of a glacial age referred to as the Last Ice Age.
T: How would you describe [A2 -lp misc]?
Q: How would you describe the Last Ice Age?
T: Summarize the influence of [A1 -lp !comma !nv] on
the environment.
Q: Summarize the influence of a glacial age on the envi-
ronment.
T: What caused [A2 -lp !nv misc]? ## [A0 null]
Q: What caused the Last Ice Age?
</bodyText>
<tableCaption confidence="0.94688">
Table 5: A few sample templates and questions
</tableCaption>
<sectionHeader confidence="0.994691" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999069082191781">
There remains no standard set of evaluation met-
rics for assessing the quality of question gener-
ation output. Some present no evaluation at all
(Wyse and Piwek, 2009; Stanescu et al., 2008).
Among those who do perform an evaluation, there
does appear to be a consensus that some form
of human evaluation is necessary. Despite this
agreement in principle, approaches tend to diverge
thereafter. There are differences in the evaluation
criteria and the evaluation procedure.
Most previous efforts in QG have not gone be-
yond manual evaluation. While some have gone
a step further and built models for ranking based
on the probability of a question being acceptable
(Heilman and Smith, 2010), these models have not
had a strong basis in pedagogy. While a question
that is both syntactically and semantically well-
formed is considered acceptable in some evalua-
tion schemes, such questions can greatly outnum-
ber the questions that we can reasonably expect a
student would want or have time to answer. We
implement a classifier that attempts to identify the
questions that are the most pedagogically useful.
For our initial evaluation of the performance of
our QG system, we selected a subset of 10 doc-
uments from the collection described in the previ-
ous section. On average, each document contained
25 sentences. From the 10 documents, our system
generated 1472 questions in total, an average of
5.9 questions per sentence. Due to the educational
nature of this material, we needed evaluators with
educational training rather than naive ones. Ac-
cordingly, the questions we generated were evalu-
ated by a graduate student from the Faculty of Ed-
ucation. She was asked to give binary judgements
for grammaticality, semantic validity, vagueness,
answerability, and learning value. For each ques-
tion, two aspects of answerability were evaluated.
The first aspect was whether the question was an-
swerable from the source sentence from which
it was generated. The second was whether the
question was answerable given the source docu-
ment as a whole. The evaluator was given no pre-
determined guidelines regarding the relationships
among the evaluation criteria (e.g., the influence
of vagueness and answerability on learning value).
This aspect of the evaluation was left to her dis-
cretion as an educator. She found that 85% of the
questions were grammatical, with 66% of them ac-
tually making sense. It was determined that 14%
of the questions were answerable from the sen-
tence used to generate them, while 20% of them
were answerable from the document. Finally, she
determined that 17% of the questions had learn-
ing value according to the prescribed learning out-
comes for the curriculum being modeled. Aside
from performing this evaluation, the evaluator was
not involved in this research.
Given this evaluation, we then built a classi-
fier which used logistic regression (L2 regular-
ized log-likelihood) to classify on learning value.
We used length, language model, SRL, named en-
tity, glossary, and syntax features. Length and
language model features measure the token count
and grammaticality of a question and the sentence
from which it was generated. SRL features in-
clude the token count of each semantic role in the
generating predicate frame, whether each role is
required by the matching template, and whether
each roles text is used. Named entity features
indicate the presence of each of nine named en-
tity types in both the source sentence and gener-
ated question. Glossary features note the number
</bodyText>
<page confidence="0.990867">
112
</page>
<bodyText confidence="0.998111233333333">
\x0cof glossary terms that appear in a sentence and
question and a measure of the average importance
of each term, which we calculated from a sim-
ple in-terms-of graph (Winne and Hadwin, 2013)
we constructed from the glossary. This graph has
directed edges between each glossary term and
the terms that appear in its gloss. Syntax fea-
tures identify the depth of the generating predi-
cate frame in the source sentence and the POS tag
of its predicate. Without adding noise, the train-
ing set had 217 questions with learning value and
1101 questions without learning value. The clas-
sifier obtained precision and recall scores of 0.47
and 0.22 respectively for questions with learning
value, along with scores of 0.79 and 0.92 for ques-
tions with no learning value. We then added noise
to the training set by relabelling any grammati-
cal question that made sense as having learning
value. This relabelling resulted in a training set
of 778 questions with learning value and only 540
questions without learning value. The classifier
trained on this noisy set showed a precision score
on learning value questions decreased to 0.29 but
a dramatic increase in recall to 0.81. For questions
with no learning value, the precision increased
slightly to 0.86 which was offset by a dramatic de-
crease in recall to 0.38. So when the system gener-
ates a poor quality question, we have a high prob-
ability of knowing that it is a poor question which
allows us to then filter or discard it.
</bodyText>
<sectionHeader confidence="0.999176" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999913171428571">
We have shown how a template-based method,
using predominately semantic information, can
be used to generate natural language questions
for use in an on-line learning system. Our tem-
plates are based on semantic patterns, which cast
a wide syntactic net and a narrow semantic net.
The template mechanism supports rich selectional
and generational capabilities, generating a large
pool from which questions for learners can be
selected. A simple automated technique for se-
lecting questions with learning value was intro-
duced. Although this automated technique shows
promise for some applications, future investiga-
tion into what constitutes a useful question in the
context of a specific task and an individual learner
is needed. Some might argue that it is risky to
generate questions that cannot be answered from
the source sentence from which they were gener-
ated. Although some questions are generated that
are not answered elsewhere in a document, there
is a benefit in learners being able to recognize that
a particular question is not answerable. Our future
work will expand both on the types of potential
questions generated, and on the selection from the
set of potential questions based on the information
an individual learner (a) knows, (b) has available
in a library of saved sources, (c) has operated
on while studying online (e.g., tagged), and (d)
might find in the Internet. To facilitate this further
research, we will be integrating question genera-
tion into the nStudy system (Hadwin et al., 2010;
Winne and Hadwin, 2013). We will also be per-
forming thorough user studies which will evalu-
ate the generated questions from the learners per-
spective in addition to the educators perspective.
</bodyText>
<sectionHeader confidence="0.9667" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9207055">
This research was supported by an Insight De-
velopment Grant (#430-2012-044) from the So-
cial Sciences and Humanities Research Council
of Canada and a Discovery Grant from the Nat-
ural Sciences and Engineering Research Council
of Canada. The authors are extremely grateful to
Kiran Bisra from the Faculty of Education for pro-
viding information for the evaluation. Finally, spe-
cial thanks to the reviewers for their comments and
suggestions.
</bodyText>
<sectionHeader confidence="0.991218" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997080913043478">
Husam Ali, Yllias Chali, and Sadid A Hasan. 2010.
Automation of question generation from sentences.
In Proceedings of QG2010: The Third Workshop on
Question Generation, pages 5867.
Zhiqiang Cai, Vasile Rus, Hyun-Jeong Joyce Kim,
Suresh C. Susarla, Pavan Karnam, and Arthur C.
Graesser. 2006. Nlgml: A markup language
for question generation. In Thomas Reeves and
Shirley Yamashita, editors, Proceedings of World
Conference on E-Learning in Corporate, Govern-
ment, Healthcare, and Higher Education 2006,
pages 27472752, Honolulu, Hawaii, USA, Octo-
ber. AACE.
Aimee A. Callender and Mark A. McDaniel. 2007.
The benefits of embedded question adjuncts for low
and high structure builders. Journal Of Educational
Psychology (2007), pages 339348.
Xavier Carreras and Llus Marquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of the Ninth Conference on
Computational Natural Language Learning, pages
152164. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.990343">
113
</page>
<reference confidence="0.996352313253012">
\x0cEugene Charniak and Micha Elsner. 2009. Em works
for pronoun anaphora resolution. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 148156. Association for Computational Lin-
guistics.
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:24932537.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363370. Association for Computational Lin-
guistics.
A.F. Hadwin, M. Oshige, C.L.Z. Gress, and P.H.
Winne. 2010. Innovative ways for using nstudy
to orchestrate and research social aspects of self-
regulated learning. Computers in Human Behaviour
(2010), pages 794805.
Michael Heilman and Noah A Smith. 2010. Good
question! statistical ranking for question genera-
tion. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 609617. Association for Computational Lin-
guistics.
Saidalavi Kalady, Ajeesh Elikkottil, and Rajarshi Das.
2010. Natural language question generation using
syntax and keywords. In Proceedings of QG2010:
The Third Workshop on Question Generation, pages
110.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423430. Asso-
ciation for Computational Linguistics.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In LREC 2006.
David Lindberg. 2013. Automatic question generation
from text for self-directed learning. Masters thesis,
Simon Fraser University, Canada.
Prashanth Mannem, Rashmi Prasad, and Aravind Joshi.
2010. Question generation from paragraphs at
upenn: Qgstec system description. In Proceedings
of QG2010: The Third Workshop on Question Gen-
eration, pages 8491.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71106.
Bethany Rittle-Johnson. 2006. Promoting transfer:
Effects of self-explanation and direct instruction.
Child Development (2006), pages 115.
Vasile Rus and Arthur C Graesser. 1989. Classroom
questioning. In School improvement research series.
Liana Stanescu, Cosmin Stoica Spahiu, Anca Ion, and
Andrei Spahiu. 2008. Question generation for
learning evaluation. In Computer Science and In-
formation Technology, 2008. IMCSIT 2008. Interna-
tional Multiconference on, pages 509513. IEEE.
Andrea Varga and Le An Ha. 2010. Wlv: A ques-
tion generation system for the qgstec 2010 task b.
In Proceedings of QG2010: The Third Workshop on
Question Generation, pages 8083.
Philip H Winne and Allyson F Hadwin. 2013. nstudy:
Tracing and supporting self-regulated learning in the
internet. In International handbook of metacog-
nition and learning technologies, pages 293308.
Springer.
John H Wolfe. 1976. Automatic question gener-
ation from text-an aid to independent study. In
ACM SIGCUE Outlook, volume 10, pages 104112.
ACM.
Brendan Wyse and Paul Piwek. 2009. Generating
questions from openlearn study units. In AIED
2009 Workshop Proceedings Volume 1: The 2nd
Workshop on Question Generation, 6-9 July 2009,
Brighton, UK.
</reference>
<page confidence="0.997504">
114
</page>
<figure confidence="0.245081">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.753425">
<note confidence="0.894955333333333">b&apos;Proceedings of the 14th European Workshop on Natural Language Generation, pages 105114, Sofia, Bulgaria, August 8-9 2013. c 2013 Association for Computational Linguistics</note>
<title confidence="0.99623">Generating Natural Language Questions to Support Learning On-Line</title>
<author confidence="0.999942">David Lindberg Fred Popowich</author>
<affiliation confidence="0.9979925">School of Computing Science Simon Fraser University</affiliation>
<address confidence="0.999003">Burnaby, BC, CANADA</address>
<email confidence="0.979756">dll4,popowich@sfu.ca</email>
<author confidence="0.995242">John Nesbit Phil Winne</author>
<affiliation confidence="0.995492">Faculty of Education Simon Fraser University</affiliation>
<address confidence="0.999232">Burnaby, BC, CANADA</address>
<email confidence="0.994364">nesbit,winne@sfu.ca</email>
<abstract confidence="0.9997423">When instructors prepare learning materials for students, they frequently develop accompanying questions to guide learning. Natural language processing technology can be used to automatically generate such questions but techniques used have not fully leveraged semantic information contained in the learning materials or the full context in which the question generation task occurs. We introduce a sophisticated template-based approach that incorporates semantic role labels into a system that automatically generates natural language questions to support online learning. While we have not yet incorporated the full learning context into our approach, our preliminary evaluation and evaluation methodology indicate our approach is a promising one for supporting learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Husam Ali</author>
<author>Yllias Chali</author>
<author>Sadid A Hasan</author>
</authors>
<title>Automation of question generation from sentences.</title>
<date>2010</date>
<journal>Zhiqiang Cai, Vasile Rus, Hyun-Jeong</journal>
<booktitle>In Proceedings of QG2010: The Third Workshop on Question Generation,</booktitle>
<pages>5867</pages>
<contexts>
<context position="2291" citStr="Ali et al., 2010" startWordPosition="329" endWordPosition="332">uestioning is one of the most common and intensively studied instructional strategies used by teachers (Rus and Graesser, 1989). Questions embedded in text, or presented while learners are studying text, are hypothesized to promote selfexplanation which is known to increase comprehension and enhance transfer of learning (e.g., Rittle-Johnson, 2006). Traditionally, these questions have been constructed by educators. Recent research, though, has investigated how natural language processing techniques can be used to automatically generate these questions (Kalady et al., 2010; Varga and Ha, 2010; Ali et al., 2010; Mannem et al., 2010). While the automated approaches have generally focussed on syntactic features, we propose an approach that also takes semantic features into account, in conjunction with domain dependent and domain independent templates motivated by educational research. After introducing our question generation system, we will provide a preliminary analysis of the performance of the system on educational material, and then outline our future plans to tailor the questions to the needs of specific learners and specific learning outcomes. 2 Question Generation from Text The task of questio</context>
<context position="4579" citStr="Ali et al. (2010)" startWordPosition="685" endWordPosition="688"> generation capabilities might pick targets more freely and (ideally) generate only the questions that are appropriate for those targets. We consider the methods used in performing tasks 2 and 4 to be the primary discriminators in determining the category into which a given method is best placed. This is not the only way one might classify a QG system. However, we believe this method allows us to best compare and contrast our approach with previous approaches. Syntax-based methods comprise a large portion of the existing literature. Kalady et al. (2012), Varga and Ha (2010), Wolfe (1976), and Ali et al. (2010) provide a sample of these methods. Although each of these efforts has differed on a few details, they have followed the same basic strategy: parse sentences using a syntactic parser, simplify complex sentences, identify key phrases, and apply syntactic transformation rules and question word replacement. The methods we have labeled semanticsbased use method(s) of target identification (task 2) that are primarily semantic, using techniques such as semantic role labeling (SRL). Given a sentence, a semantic role labeler identifies the predicates (relations and actions) along with the semantic ent</context>
</contexts>
<marker>Ali, Chali, Hasan, 2010</marker>
<rawString>Husam Ali, Yllias Chali, and Sadid A Hasan. 2010. Automation of question generation from sentences. In Proceedings of QG2010: The Third Workshop on Question Generation, pages 5867. Zhiqiang Cai, Vasile Rus, Hyun-Jeong Joyce Kim, Suresh C. Susarla, Pavan Karnam, and Arthur C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graesser</author>
</authors>
<title>Nlgml: A markup language for question generation.</title>
<date>2006</date>
<booktitle>Proceedings of World Conference on E-Learning in Corporate, Government, Healthcare, and Higher Education</booktitle>
<pages>27472752</pages>
<editor>In Thomas Reeves and Shirley Yamashita, editors,</editor>
<publisher>AACE.</publisher>
<location>Honolulu, Hawaii, USA,</location>
<marker>Graesser, 2006</marker>
<rawString>Graesser. 2006. Nlgml: A markup language for question generation. In Thomas Reeves and Shirley Yamashita, editors, Proceedings of World Conference on E-Learning in Corporate, Government, Healthcare, and Higher Education 2006, pages 27472752, Honolulu, Hawaii, USA, October. AACE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aimee A Callender</author>
<author>Mark A McDaniel</author>
</authors>
<date>2007</date>
<contexts>
<context position="1318" citStr="Callender and McDaniel, 2007" startWordPosition="179" endWordPosition="182">ions but techniques used have not fully leveraged semantic information contained in the learning materials or the full context in which the question generation task occurs. We introduce a sophisticated template-based approach that incorporates semantic role labels into a system that automatically generates natural language questions to support online learning. While we have not yet incorporated the full learning context into our approach, our preliminary evaluation and evaluation methodology indicate our approach is a promising one for supporting learning. 1 Introduction Ample research (e.g., Callender and McDaniel, 2007) shows that learners learn more, and more deeply, if they are prompted to examine their learning materials while and after they study. Often, these prompts consist of questions related to the learning materials. After reading a given passage or section of text, learners are familiar with learning exercises which consist of questions they need to answer. Questioning is one of the most common and intensively studied instructional strategies used by teachers (Rus and Graesser, 1989). Questions embedded in text, or presented while learners are studying text, are hypothesized to promote selfexplana</context>
</contexts>
<marker>Callender, McDaniel, 2007</marker>
<rawString>Aimee A. Callender and Mark A. McDaniel. 2007.</rawString>
</citation>
<citation valid="true">
<title>The benefits of embedded question adjuncts for low and high structure builders.</title>
<date>2007</date>
<journal>Journal Of Educational Psychology</journal>
<pages>339348</pages>
<marker>2007</marker>
<rawString>The benefits of embedded question adjuncts for low and high structure builders. Journal Of Educational Psychology (2007), pages 339348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llus Marquez</author>
</authors>
<title>Introduction to the conll-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning,</booktitle>
<pages>152164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5686" citStr="Carreras and Marquez, 2005" startWordPosition="852" endWordPosition="855"> Given a sentence, a semantic role labeler identifies the predicates (relations and actions) along with the semantic entities associated with each predicate. Semantic roles, as defined in PropBank (Palmer et al., 2005), include Arg0, Arg1, ..., Arg5, and ArgA. A set of modifiers is also defined and includes ArgM-LOC (location), ArgM-EXT (extent), ArgM-DIS (discourse), ArgM-ADV (adverbial), ArgM-NEG (negation), ArgM-MOD (modal verb), ArgM-CAU (cause), ArgM-TMP (time), ArgM-PNC (purpose), ArgM-MNR (manner), and ArgM-DIR (direction). We adopt the shorter CoNLL SRL shared task naming conventions (Carreras and Marquez, 2005) (e.g., A0 and AMLOC). Mannem et al. (2010), for example, introduce a semantics-based system that combines SRL with syntactic transformations. In the content selection stage, a single sentence is first parsed with a semantic role labeler to identify potential targets. Targets are selected using simple selection criteria. Any of the predicate-specific semantic arguments (A0-A5), if present, are considered valid targets. Mannem et al. further identify modifiers AM-MNR, AM-PUNC, AM-CAU, AM-TMP, AM-LOC, and AM-DIS as potential targets. These roles are used to generate additional questions that can</context>
</contexts>
<marker>Carreras, Marquez, 2005</marker>
<rawString>Xavier Carreras and Llus Marquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In Proceedings of the Ninth Conference on Computational Natural Language Learning, pages 152164. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cEugene Charniak</author>
<author>Micha Elsner</author>
</authors>
<title>Em works for pronoun anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>148156</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24576" citStr="Charniak and Elsner, 2009" startWordPosition="3945" endWordPosition="3949">erform any sentence simplification. This decision was motivated by the observation that common methods of sentence simplification can eliminate useful semantic content. For example, Kalady et al. (2010) claim that prepositional phrases are often not fundamental to the meaning of a sentence, so they remove them when simplifying a sentence. However, as Figure 4 shows, a prepositional phrase can contain important semantic information. In that example, removing the prepositional phrase causes temporal information to be lost. One pre-processing step we do perform is pronominal anaphora resolution (Charniak and Elsner, 2009). Even though we do not split com110 \x0cFigure 3: System architecture and data flow Figure 4: Semantic information can be lost during sentence simplification. Removing the prepositional phrase from the first sentence leaves the simpler second sentence, but the AM-TMP modifier is lost. plex sentences and therefore do not create new sentences in which pronouns are separated from their antecedents, this kind of anaphora resolution remains an important step in limiting the number of vague questions. Each source sentence is tokenized and annotated with POS tags, named entities, lemmata, and its SR</context>
</contexts>
<marker>Charniak, Elsner, 2009</marker>
<rawString>\x0cEugene Charniak and Micha Elsner. 2009. Em works for pronoun anaphora resolution. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 148156. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>Leon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--24932537</pages>
<contexts>
<context position="25275" citStr="Collobert et al., 2011" startWordPosition="4061" endWordPosition="4064">ata flow Figure 4: Semantic information can be lost during sentence simplification. Removing the prepositional phrase from the first sentence leaves the simpler second sentence, but the AM-TMP modifier is lost. plex sentences and therefore do not create new sentences in which pronouns are separated from their antecedents, this kind of anaphora resolution remains an important step in limiting the number of vague questions. Each source sentence is tokenized and annotated with POS tags, named entities, lemmata, and its SRL parse. SRL is the cornerstone of our approach. We generate the SRL parse (Collobert et al., 2011) in order to extract a set of predicate frames. Questions are generated from individual predicate frames rather than entire sentences (unless the sentence contains only one predicate frame). Given a sentence, the semantic pattern of each of its predicate frames is compared against that of each template. Algorithm 1 describes the process of matching a single predicate frame (pf) to a single template (t). Although it is not stated in Algorithm 1, the sentence-level tokenization, lemmata, named entities and POS tags are checked as needed according to the templates slot filters. If a predicate fra</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:24932537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10682" citStr="Finkel et al., 2005" startWordPosition="1660" endWordPosition="1663">tic structures, and matching these requires either multiple patterns (in the case of Cai et al., 2006) or a more complicated pattern (in the case of Wyse and Piwek, 2009). If we want to develop templates that are semantically motivated, more flexible in terms of the content they successfully match, and more approachable for non-technical users, we need to move away from syntactic pattern matching. Instead, we match semantic patterns. We define a semantic pattern as the SRL parse of a sentence and the named entities (if any) contained within the span of each semantic role. We use Stanford NER (Finkel et al., 2005) for named entity recognition. Figure 1 shows a sentence and its corresponding semantic pattern. Notice this sentence has two predicates, each with its own semantic arguments. Each of these predicate-argument structures is a distinct predicate frame. Figure 1: A sentence and its semantic pattern Even the shallow semantics of SRL can identify the semantically interesting portions of a sentence, and these semantically-meaningful substrings can span a range of syntactic patterns. Figure 2 shows a clear example of this phenomenon. In this example, we see two sentences expressing the same semantic </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F Hadwin</author>
<author>M Oshige</author>
<author>C L Z Gress</author>
<author>P H Winne</author>
</authors>
<title>Innovative ways for using nstudy to orchestrate and research social aspects of selfregulated learning. Computers in Human Behaviour</title>
<date>2010</date>
<pages>794805</pages>
<contexts>
<context position="34514" citStr="Hadwin et al., 2010" startWordPosition="5584" endWordPosition="5587">generated that are not answered elsewhere in a document, there is a benefit in learners being able to recognize that a particular question is not answerable. Our future work will expand both on the types of potential questions generated, and on the selection from the set of potential questions based on the information an individual learner (a) knows, (b) has available in a library of saved sources, (c) has operated on while studying online (e.g., tagged), and (d) might find in the Internet. To facilitate this further research, we will be integrating question generation into the nStudy system (Hadwin et al., 2010; Winne and Hadwin, 2013). We will also be performing thorough user studies which will evaluate the generated questions from the learners perspective in addition to the educators perspective. Acknowledgments This research was supported by an Insight Development Grant (#430-2012-044) from the Social Sciences and Humanities Research Council of Canada and a Discovery Grant from the Natural Sciences and Engineering Research Council of Canada. The authors are extremely grateful to Kiran Bisra from the Faculty of Education for providing information for the evaluation. Finally, special thanks to the </context>
</contexts>
<marker>Hadwin, Oshige, Gress, Winne, 2010</marker>
<rawString>A.F. Hadwin, M. Oshige, C.L.Z. Gress, and P.H. Winne. 2010. Innovative ways for using nstudy to orchestrate and research social aspects of selfregulated learning. Computers in Human Behaviour (2010), pages 794805.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Good question! statistical ranking for question generation. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>609617</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28716" citStr="Heilman and Smith, 2010" startWordPosition="4631" endWordPosition="4634">ing the quality of question generation output. Some present no evaluation at all (Wyse and Piwek, 2009; Stanescu et al., 2008). Among those who do perform an evaluation, there does appear to be a consensus that some form of human evaluation is necessary. Despite this agreement in principle, approaches tend to diverge thereafter. There are differences in the evaluation criteria and the evaluation procedure. Most previous efforts in QG have not gone beyond manual evaluation. While some have gone a step further and built models for ranking based on the probability of a question being acceptable (Heilman and Smith, 2010), these models have not had a strong basis in pedagogy. While a question that is both syntactically and semantically wellformed is considered acceptable in some evaluation schemes, such questions can greatly outnumber the questions that we can reasonably expect a student would want or have time to answer. We implement a classifier that attempts to identify the questions that are the most pedagogically useful. For our initial evaluation of the performance of our QG system, we selected a subset of 10 documents from the collection described in the previous section. On average, each document conta</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A Smith. 2010. Good question! statistical ranking for question generation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 609617. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saidalavi Kalady</author>
<author>Ajeesh Elikkottil</author>
<author>Rajarshi Das</author>
</authors>
<title>Natural language question generation using syntax and keywords.</title>
<date>2010</date>
<booktitle>In Proceedings of QG2010: The Third Workshop on Question Generation,</booktitle>
<pages>110</pages>
<contexts>
<context position="2253" citStr="Kalady et al., 2010" startWordPosition="321" endWordPosition="324">nsist of questions they need to answer. Questioning is one of the most common and intensively studied instructional strategies used by teachers (Rus and Graesser, 1989). Questions embedded in text, or presented while learners are studying text, are hypothesized to promote selfexplanation which is known to increase comprehension and enhance transfer of learning (e.g., Rittle-Johnson, 2006). Traditionally, these questions have been constructed by educators. Recent research, though, has investigated how natural language processing techniques can be used to automatically generate these questions (Kalady et al., 2010; Varga and Ha, 2010; Ali et al., 2010; Mannem et al., 2010). While the automated approaches have generally focussed on syntactic features, we propose an approach that also takes semantic features into account, in conjunction with domain dependent and domain independent templates motivated by educational research. After introducing our question generation system, we will provide a preliminary analysis of the performance of the system on educational material, and then outline our future plans to tailor the questions to the needs of specific learners and specific learning outcomes. 2 Question Ge</context>
<context position="24152" citStr="Kalady et al. (2010)" startWordPosition="3881" endWordPosition="3884">re an external input. They are in no way coupled to the system and can be modified as needed without any system modifications. Compared to most other approaches, we perform very little pre-processing. Syntax-based methods in particular have been motivated to perform sentence simplification, because their methods are more likely to generate meaningful questions from short, succinct sentences. We have chosen not to perform any sentence simplification. This decision was motivated by the observation that common methods of sentence simplification can eliminate useful semantic content. For example, Kalady et al. (2010) claim that prepositional phrases are often not fundamental to the meaning of a sentence, so they remove them when simplifying a sentence. However, as Figure 4 shows, a prepositional phrase can contain important semantic information. In that example, removing the prepositional phrase causes temporal information to be lost. One pre-processing step we do perform is pronominal anaphora resolution (Charniak and Elsner, 2009). Even though we do not split com110 \x0cFigure 3: System architecture and data flow Figure 4: Semantic information can be lost during sentence simplification. Removing the pre</context>
</contexts>
<marker>Kalady, Elikkottil, Das, 2010</marker>
<rawString>Saidalavi Kalady, Ajeesh Elikkottil, and Rajarshi Das. 2010. Natural language question generation using syntax and keywords. In Proceedings of QG2010: The Third Workshop on Question Generation, pages 110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11603" citStr="Klein and Manning, 2003" startWordPosition="1804" endWordPosition="1807">Even the shallow semantics of SRL can identify the semantically interesting portions of a sentence, and these semantically-meaningful substrings can span a range of syntactic patterns. Figure 2 shows a clear example of this phenomenon. In this example, we see two sentences expressing the same semantic relationship between two concepts, namely, the fact that trapped heat causes the Earths temperature to increase. In one case, this causation is expressed in an adjective phrase, while the other uses a sentence-initial prepositional phrase. The parse trees are generated using the Stanford Parser (Klein and Manning, 2003). The AM-CAU semantic role captures the cause in both sentences. It is impossible to accomplish the same feat with a single NLGML pattern. However, it is possible to capture both with a single Tregex pattern. The principle advantage of semantic pattern matching is that a single semantic pattern casts a narrow semantic net while casting a large syntactic net. This means fewer patterns need to be defined by the template author, and the patterns are more compact. Our templates have three components: plaintext, slots, and slot options. Plaintext forms the 107 \x0cFigure 2: Two different syntax sub</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Galen Andrew</author>
</authors>
<title>Tregex and tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>In LREC</booktitle>
<contexts>
<context position="9858" citStr="Levy and Andrew, 2006" startWordPosition="1521" endWordPosition="1525">m on global warming, with vocabulary and discourse appropriate for learners in that age group. Although the collection included a glossary of key terms and their definitions, this resource was used only for evaluation purposes as described in Section 4. 3.1 Semantic-based templates Previous template-based methods have used syntactic pattern matching, which does provide a great deal of flexibility in specifying sentences appropriate for generating certain types of questions. However, this flexibility comes at the expense of generality. As seen in Wyse and Piwek (2009), who use Stanford Tregex (Levy and Andrew, 2006) for pattern matching, the specificity of syntactic patterns can make it difficult to specify a syntactic pattern of the desired scope. Furthermore, semantically similar entities can span different syntactic structures, and matching these requires either multiple patterns (in the case of Cai et al., 2006) or a more complicated pattern (in the case of Wyse and Piwek, 2009). If we want to develop templates that are semantically motivated, more flexible in terms of the content they successfully match, and more approachable for non-technical users, we need to move away from syntactic pattern match</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>Roger Levy and Galen Andrew. 2006. Tregex and tsurgeon: tools for querying and manipulating tree data structures. In LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lindberg</author>
</authors>
<title>Automatic question generation from text for self-directed learning. Masters thesis,</title>
<date>2013</date>
<institution>Simon Fraser University, Canada.</institution>
<contexts>
<context position="27560" citStr="Lindberg, 2013" startWordPosition="4430" endWordPosition="4431"> 111 \x0cAlgorithm 2 fillTemplate(t,pf) question text t.plaintext for all slot t.plaintext slots do role text pf.role(slot.role).text for all modifier slot.modifiers do applyModifier(role text,modifier) end for In question text, replace slot with role text end for return question text yield. Table 5 shows some templates (T) that match the sentence in Figure 1 and the questions (Q) that result. Although the questions that are generated are not answerable from the original sentence, they were judged answerable from the source document in our evaluation. The full set of templates is provided in (Lindberg, 2013). As recently as 12,500 years ago, the Earth was in the midst of a glacial age referred to as the Last Ice Age. T: How would you describe [A2 -lp misc]? Q: How would you describe the Last Ice Age? T: Summarize the influence of [A1 -lp !comma !nv] on the environment. Q: Summarize the influence of a glacial age on the environment. T: What caused [A2 -lp !nv misc]? ## [A0 null] Q: What caused the Last Ice Age? Table 5: A few sample templates and questions 4 Evaluation There remains no standard set of evaluation metrics for assessing the quality of question generation output. Some present no evalu</context>
</contexts>
<marker>Lindberg, 2013</marker>
<rawString>David Lindberg. 2013. Automatic question generation from text for self-directed learning. Masters thesis, Simon Fraser University, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prashanth Mannem</author>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
</authors>
<title>Question generation from paragraphs at upenn: Qgstec system description.</title>
<date>2010</date>
<booktitle>In Proceedings of QG2010: The Third Workshop on Question Generation,</booktitle>
<pages>8491</pages>
<contexts>
<context position="2313" citStr="Mannem et al., 2010" startWordPosition="333" endWordPosition="336">of the most common and intensively studied instructional strategies used by teachers (Rus and Graesser, 1989). Questions embedded in text, or presented while learners are studying text, are hypothesized to promote selfexplanation which is known to increase comprehension and enhance transfer of learning (e.g., Rittle-Johnson, 2006). Traditionally, these questions have been constructed by educators. Recent research, though, has investigated how natural language processing techniques can be used to automatically generate these questions (Kalady et al., 2010; Varga and Ha, 2010; Ali et al., 2010; Mannem et al., 2010). While the automated approaches have generally focussed on syntactic features, we propose an approach that also takes semantic features into account, in conjunction with domain dependent and domain independent templates motivated by educational research. After introducing our question generation system, we will provide a preliminary analysis of the performance of the system on educational material, and then outline our future plans to tailor the questions to the needs of specific learners and specific learning outcomes. 2 Question Generation from Text The task of question generation (QG) from</context>
<context position="5729" citStr="Mannem et al. (2010)" startWordPosition="861" endWordPosition="864">ies the predicates (relations and actions) along with the semantic entities associated with each predicate. Semantic roles, as defined in PropBank (Palmer et al., 2005), include Arg0, Arg1, ..., Arg5, and ArgA. A set of modifiers is also defined and includes ArgM-LOC (location), ArgM-EXT (extent), ArgM-DIS (discourse), ArgM-ADV (adverbial), ArgM-NEG (negation), ArgM-MOD (modal verb), ArgM-CAU (cause), ArgM-TMP (time), ArgM-PNC (purpose), ArgM-MNR (manner), and ArgM-DIR (direction). We adopt the shorter CoNLL SRL shared task naming conventions (Carreras and Marquez, 2005) (e.g., A0 and AMLOC). Mannem et al. (2010), for example, introduce a semantics-based system that combines SRL with syntactic transformations. In the content selection stage, a single sentence is first parsed with a semantic role labeler to identify potential targets. Targets are selected using simple selection criteria. Any of the predicate-specific semantic arguments (A0-A5), if present, are considered valid targets. Mannem et al. further identify modifiers AM-MNR, AM-PUNC, AM-CAU, AM-TMP, AM-LOC, and AM-DIS as potential targets. These roles are used to generate additional questions that cannot be attained using only the A0-A5 roles.</context>
</contexts>
<marker>Mannem, Prasad, Joshi, 2010</marker>
<rawString>Prashanth Mannem, Rashmi Prasad, and Aravind Joshi. 2010. Question generation from paragraphs at upenn: Qgstec system description. In Proceedings of QG2010: The Third Workshop on Question Generation, pages 8491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="5277" citStr="Palmer et al., 2005" startWordPosition="795" endWordPosition="798">d on a few details, they have followed the same basic strategy: parse sentences using a syntactic parser, simplify complex sentences, identify key phrases, and apply syntactic transformation rules and question word replacement. The methods we have labeled semanticsbased use method(s) of target identification (task 2) that are primarily semantic, using techniques such as semantic role labeling (SRL). Given a sentence, a semantic role labeler identifies the predicates (relations and actions) along with the semantic entities associated with each predicate. Semantic roles, as defined in PropBank (Palmer et al., 2005), include Arg0, Arg1, ..., Arg5, and ArgA. A set of modifiers is also defined and includes ArgM-LOC (location), ArgM-EXT (extent), ArgM-DIS (discourse), ArgM-ADV (adverbial), ArgM-NEG (negation), ArgM-MOD (modal verb), ArgM-CAU (cause), ArgM-TMP (time), ArgM-PNC (purpose), ArgM-MNR (manner), and ArgM-DIR (direction). We adopt the shorter CoNLL SRL shared task naming conventions (Carreras and Marquez, 2005) (e.g., A0 and AMLOC). Mannem et al. (2010), for example, introduce a semantics-based system that combines SRL with syntactic transformations. In the content selection stage, a single sentenc</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bethany Rittle-Johnson</author>
</authors>
<title>Promoting transfer: Effects of self-explanation and direct instruction.</title>
<date>2006</date>
<contexts>
<context position="2025" citStr="Rittle-Johnson, 2006" startWordPosition="292" endWordPosition="293">r learning materials while and after they study. Often, these prompts consist of questions related to the learning materials. After reading a given passage or section of text, learners are familiar with learning exercises which consist of questions they need to answer. Questioning is one of the most common and intensively studied instructional strategies used by teachers (Rus and Graesser, 1989). Questions embedded in text, or presented while learners are studying text, are hypothesized to promote selfexplanation which is known to increase comprehension and enhance transfer of learning (e.g., Rittle-Johnson, 2006). Traditionally, these questions have been constructed by educators. Recent research, though, has investigated how natural language processing techniques can be used to automatically generate these questions (Kalady et al., 2010; Varga and Ha, 2010; Ali et al., 2010; Mannem et al., 2010). While the automated approaches have generally focussed on syntactic features, we propose an approach that also takes semantic features into account, in conjunction with domain dependent and domain independent templates motivated by educational research. After introducing our question generation system, we wil</context>
</contexts>
<marker>Rittle-Johnson, 2006</marker>
<rawString>Bethany Rittle-Johnson. 2006. Promoting transfer: Effects of self-explanation and direct instruction.</rawString>
</citation>
<citation valid="false">
<date>2006</date>
<pages>115</pages>
<institution>Child Development</institution>
<contexts>
<context position="7589" citStr="(2006)" startWordPosition="1169" endWordPosition="1169">ouns they contain, with questions with fewer pronouns having higher rank. One limitation of the syntax and semanticsbased methods is that they generate questions by rearranging the surface form of sentences. Question templates offer the ability to ask questions that are not so tightly-coupled to the exact wording of the source text. A question template is any predefined text with placeholder variables to be replaced with content from the source text. Question templates allow question generation systems to leverage human expertise in language generation. The template-based system of Cai et al. (2006) uses Natural Language Generation Markup Language (NLGML), a language that can be used to generate not only questions but any natural language expression. NLGML uses syntactic pattern matching and semantic features for content selection and question templates to guide question formulation and surface-form realization. Note that a pattern need not specify a complete syntax tree. Additionally, patterns can impose semantic constraints. However, simple copy and paste templates are not a panacea for surface-form realization. Mechanisms for changing capitalization of words and changing verb conjugat</context>
</contexts>
<marker>2006</marker>
<rawString>Child Development (2006), pages 115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Arthur C Graesser</author>
</authors>
<title>Classroom questioning. In School improvement research series.</title>
<date>1989</date>
<contexts>
<context position="1802" citStr="Rus and Graesser, 1989" startWordPosition="257" endWordPosition="260">dology indicate our approach is a promising one for supporting learning. 1 Introduction Ample research (e.g., Callender and McDaniel, 2007) shows that learners learn more, and more deeply, if they are prompted to examine their learning materials while and after they study. Often, these prompts consist of questions related to the learning materials. After reading a given passage or section of text, learners are familiar with learning exercises which consist of questions they need to answer. Questioning is one of the most common and intensively studied instructional strategies used by teachers (Rus and Graesser, 1989). Questions embedded in text, or presented while learners are studying text, are hypothesized to promote selfexplanation which is known to increase comprehension and enhance transfer of learning (e.g., Rittle-Johnson, 2006). Traditionally, these questions have been constructed by educators. Recent research, though, has investigated how natural language processing techniques can be used to automatically generate these questions (Kalady et al., 2010; Varga and Ha, 2010; Ali et al., 2010; Mannem et al., 2010). While the automated approaches have generally focussed on syntactic features, we propos</context>
</contexts>
<marker>Rus, Graesser, 1989</marker>
<rawString>Vasile Rus and Arthur C Graesser. 1989. Classroom questioning. In School improvement research series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liana Stanescu</author>
<author>Cosmin Stoica Spahiu</author>
<author>Anca Ion</author>
<author>Andrei Spahiu</author>
</authors>
<title>Question generation for learning evaluation.</title>
<date>2008</date>
<booktitle>In Computer Science and Information Technology,</booktitle>
<pages>509513</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="28218" citStr="Stanescu et al., 2008" startWordPosition="4551" endWordPosition="4554">he Earth was in the midst of a glacial age referred to as the Last Ice Age. T: How would you describe [A2 -lp misc]? Q: How would you describe the Last Ice Age? T: Summarize the influence of [A1 -lp !comma !nv] on the environment. Q: Summarize the influence of a glacial age on the environment. T: What caused [A2 -lp !nv misc]? ## [A0 null] Q: What caused the Last Ice Age? Table 5: A few sample templates and questions 4 Evaluation There remains no standard set of evaluation metrics for assessing the quality of question generation output. Some present no evaluation at all (Wyse and Piwek, 2009; Stanescu et al., 2008). Among those who do perform an evaluation, there does appear to be a consensus that some form of human evaluation is necessary. Despite this agreement in principle, approaches tend to diverge thereafter. There are differences in the evaluation criteria and the evaluation procedure. Most previous efforts in QG have not gone beyond manual evaluation. While some have gone a step further and built models for ranking based on the probability of a question being acceptable (Heilman and Smith, 2010), these models have not had a strong basis in pedagogy. While a question that is both syntactically an</context>
</contexts>
<marker>Stanescu, Spahiu, Ion, Spahiu, 2008</marker>
<rawString>Liana Stanescu, Cosmin Stoica Spahiu, Anca Ion, and Andrei Spahiu. 2008. Question generation for learning evaluation. In Computer Science and Information Technology, 2008. IMCSIT 2008. International Multiconference on, pages 509513. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Varga</author>
<author>Le An Ha</author>
</authors>
<title>Wlv: A question generation system for the qgstec 2010 task b.</title>
<date>2010</date>
<booktitle>In Proceedings of QG2010: The Third Workshop on Question Generation,</booktitle>
<pages>8083</pages>
<contexts>
<context position="2273" citStr="Varga and Ha, 2010" startWordPosition="325" endWordPosition="328">ey need to answer. Questioning is one of the most common and intensively studied instructional strategies used by teachers (Rus and Graesser, 1989). Questions embedded in text, or presented while learners are studying text, are hypothesized to promote selfexplanation which is known to increase comprehension and enhance transfer of learning (e.g., Rittle-Johnson, 2006). Traditionally, these questions have been constructed by educators. Recent research, though, has investigated how natural language processing techniques can be used to automatically generate these questions (Kalady et al., 2010; Varga and Ha, 2010; Ali et al., 2010; Mannem et al., 2010). While the automated approaches have generally focussed on syntactic features, we propose an approach that also takes semantic features into account, in conjunction with domain dependent and domain independent templates motivated by educational research. After introducing our question generation system, we will provide a preliminary analysis of the performance of the system on educational material, and then outline our future plans to tailor the questions to the needs of specific learners and specific learning outcomes. 2 Question Generation from Text T</context>
<context position="4542" citStr="Varga and Ha (2010)" startWordPosition="678" endWordPosition="681">ions. Conversely, a system with broader generation capabilities might pick targets more freely and (ideally) generate only the questions that are appropriate for those targets. We consider the methods used in performing tasks 2 and 4 to be the primary discriminators in determining the category into which a given method is best placed. This is not the only way one might classify a QG system. However, we believe this method allows us to best compare and contrast our approach with previous approaches. Syntax-based methods comprise a large portion of the existing literature. Kalady et al. (2012), Varga and Ha (2010), Wolfe (1976), and Ali et al. (2010) provide a sample of these methods. Although each of these efforts has differed on a few details, they have followed the same basic strategy: parse sentences using a syntactic parser, simplify complex sentences, identify key phrases, and apply syntactic transformation rules and question word replacement. The methods we have labeled semanticsbased use method(s) of target identification (task 2) that are primarily semantic, using techniques such as semantic role labeling (SRL). Given a sentence, a semantic role labeler identifies the predicates (relations and</context>
</contexts>
<marker>Varga, Ha, 2010</marker>
<rawString>Andrea Varga and Le An Ha. 2010. Wlv: A question generation system for the qgstec 2010 task b. In Proceedings of QG2010: The Third Workshop on Question Generation, pages 8083.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip H Winne</author>
<author>Allyson F Hadwin</author>
</authors>
<title>nstudy: Tracing and supporting self-regulated learning in the internet.</title>
<date>2013</date>
<booktitle>In International handbook of metacognition and learning technologies,</booktitle>
<pages>293308</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="31713" citStr="Winne and Hadwin, 2013" startWordPosition="5120" endWordPosition="5123"> grammaticality of a question and the sentence from which it was generated. SRL features include the token count of each semantic role in the generating predicate frame, whether each role is required by the matching template, and whether each roles text is used. Named entity features indicate the presence of each of nine named entity types in both the source sentence and generated question. Glossary features note the number 112 \x0cof glossary terms that appear in a sentence and question and a measure of the average importance of each term, which we calculated from a simple in-terms-of graph (Winne and Hadwin, 2013) we constructed from the glossary. This graph has directed edges between each glossary term and the terms that appear in its gloss. Syntax features identify the depth of the generating predicate frame in the source sentence and the POS tag of its predicate. Without adding noise, the training set had 217 questions with learning value and 1101 questions without learning value. The classifier obtained precision and recall scores of 0.47 and 0.22 respectively for questions with learning value, along with scores of 0.79 and 0.92 for questions with no learning value. We then added noise to the train</context>
<context position="34539" citStr="Winne and Hadwin, 2013" startWordPosition="5588" endWordPosition="5591">t answered elsewhere in a document, there is a benefit in learners being able to recognize that a particular question is not answerable. Our future work will expand both on the types of potential questions generated, and on the selection from the set of potential questions based on the information an individual learner (a) knows, (b) has available in a library of saved sources, (c) has operated on while studying online (e.g., tagged), and (d) might find in the Internet. To facilitate this further research, we will be integrating question generation into the nStudy system (Hadwin et al., 2010; Winne and Hadwin, 2013). We will also be performing thorough user studies which will evaluate the generated questions from the learners perspective in addition to the educators perspective. Acknowledgments This research was supported by an Insight Development Grant (#430-2012-044) from the Social Sciences and Humanities Research Council of Canada and a Discovery Grant from the Natural Sciences and Engineering Research Council of Canada. The authors are extremely grateful to Kiran Bisra from the Faculty of Education for providing information for the evaluation. Finally, special thanks to the reviewers for their comme</context>
</contexts>
<marker>Winne, Hadwin, 2013</marker>
<rawString>Philip H Winne and Allyson F Hadwin. 2013. nstudy: Tracing and supporting self-regulated learning in the internet. In International handbook of metacognition and learning technologies, pages 293308. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John H Wolfe</author>
</authors>
<title>Automatic question generation from text-an aid to independent study.</title>
<date>1976</date>
<journal>In ACM SIGCUE Outlook,</journal>
<volume>10</volume>
<pages>104112</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4556" citStr="Wolfe (1976)" startWordPosition="682" endWordPosition="683">ystem with broader generation capabilities might pick targets more freely and (ideally) generate only the questions that are appropriate for those targets. We consider the methods used in performing tasks 2 and 4 to be the primary discriminators in determining the category into which a given method is best placed. This is not the only way one might classify a QG system. However, we believe this method allows us to best compare and contrast our approach with previous approaches. Syntax-based methods comprise a large portion of the existing literature. Kalady et al. (2012), Varga and Ha (2010), Wolfe (1976), and Ali et al. (2010) provide a sample of these methods. Although each of these efforts has differed on a few details, they have followed the same basic strategy: parse sentences using a syntactic parser, simplify complex sentences, identify key phrases, and apply syntactic transformation rules and question word replacement. The methods we have labeled semanticsbased use method(s) of target identification (task 2) that are primarily semantic, using techniques such as semantic role labeling (SRL). Given a sentence, a semantic role labeler identifies the predicates (relations and actions) alon</context>
</contexts>
<marker>Wolfe, 1976</marker>
<rawString>John H Wolfe. 1976. Automatic question generation from text-an aid to independent study. In ACM SIGCUE Outlook, volume 10, pages 104112. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan Wyse</author>
<author>Paul Piwek</author>
</authors>
<title>Generating questions from openlearn study units.</title>
<date>2009</date>
<booktitle>In AIED 2009 Workshop Proceedings Volume 1: The 2nd Workshop on Question Generation,</booktitle>
<pages>6--9</pages>
<location>Brighton, UK.</location>
<contexts>
<context position="9809" citStr="Wyse and Piwek (2009)" startWordPosition="1513" endWordPosition="1516">as modeled after a high-school science curriculum on global warming, with vocabulary and discourse appropriate for learners in that age group. Although the collection included a glossary of key terms and their definitions, this resource was used only for evaluation purposes as described in Section 4. 3.1 Semantic-based templates Previous template-based methods have used syntactic pattern matching, which does provide a great deal of flexibility in specifying sentences appropriate for generating certain types of questions. However, this flexibility comes at the expense of generality. As seen in Wyse and Piwek (2009), who use Stanford Tregex (Levy and Andrew, 2006) for pattern matching, the specificity of syntactic patterns can make it difficult to specify a syntactic pattern of the desired scope. Furthermore, semantically similar entities can span different syntactic structures, and matching these requires either multiple patterns (in the case of Cai et al., 2006) or a more complicated pattern (in the case of Wyse and Piwek, 2009). If we want to develop templates that are semantically motivated, more flexible in terms of the content they successfully match, and more approachable for non-technical users, </context>
<context position="28194" citStr="Wyse and Piwek, 2009" startWordPosition="4547" endWordPosition="4550">as 12,500 years ago, the Earth was in the midst of a glacial age referred to as the Last Ice Age. T: How would you describe [A2 -lp misc]? Q: How would you describe the Last Ice Age? T: Summarize the influence of [A1 -lp !comma !nv] on the environment. Q: Summarize the influence of a glacial age on the environment. T: What caused [A2 -lp !nv misc]? ## [A0 null] Q: What caused the Last Ice Age? Table 5: A few sample templates and questions 4 Evaluation There remains no standard set of evaluation metrics for assessing the quality of question generation output. Some present no evaluation at all (Wyse and Piwek, 2009; Stanescu et al., 2008). Among those who do perform an evaluation, there does appear to be a consensus that some form of human evaluation is necessary. Despite this agreement in principle, approaches tend to diverge thereafter. There are differences in the evaluation criteria and the evaluation procedure. Most previous efforts in QG have not gone beyond manual evaluation. While some have gone a step further and built models for ranking based on the probability of a question being acceptable (Heilman and Smith, 2010), these models have not had a strong basis in pedagogy. While a question that </context>
</contexts>
<marker>Wyse, Piwek, 2009</marker>
<rawString>Brendan Wyse and Paul Piwek. 2009. Generating questions from openlearn study units. In AIED 2009 Workshop Proceedings Volume 1: The 2nd Workshop on Question Generation, 6-9 July 2009, Brighton, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>