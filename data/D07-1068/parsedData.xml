<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.589442">
b&amp;apos;Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 649657, Prague, June 2007. c
</bodyText>
<sectionHeader confidence="0.412501" genericHeader="abstract">
2007 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.9840065">
A Graph-based Approach to Named Entity Categorization in Wikipedia
Using Conditional Random Fields
</title>
<author confidence="0.927405">
Yotaro Watanabe, Masayuki Asahara and Yuji Matsumoto
</author>
<affiliation confidence="0.8522985">
Graduate School of Information Science
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.995985">
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
</address>
<email confidence="0.996635">
{yotaro-w,masayu-a,matsu}@is.naist.jp
</email>
<sectionHeader confidence="0.990812" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.987564347826087">
This paper presents a method for catego-
rizing named entities in Wikipedia. In
Wikipedia, an anchor text is glossed in a
linked HTML text. We formalize named en-
tity categorization as a task of categorizing
anchor texts with linked HTML texts which
glosses a named entity. Using this repre-
sentation, we introduce a graph structure in
which anchor texts are regarded as nodes.
In order to incorporate HTML structure on
the graph, three types of cliques are defined
based on the HTML tree structure. We pro-
pose a method with Conditional Random
Fields (CRFs) to categorize the nodes on
the graph. Since the defined graph may in-
clude cycles, the exact inference of CRFs is
computationally expensive. We introduce an
approximate inference method using Tree-
based Reparameterization (TRP) to reduce
computational cost. In experiments, our pro-
posed model obtained significant improve-
ments compare to baseline models that use
Support Vector Machines.
</bodyText>
<sectionHeader confidence="0.993316" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9912094">
Named and Numeric Entities (NEs) refer to proper
nouns (e.g. PERSON, LOCATION and ORGANI-
ZATION), time expressions, date expressions and so
on. Since a large number of NEs exist in the world,
unknown expressions appear frequently in texts, and
they become hindrance to real-world text analysis.
To cope with the problem, one effective ways to add
a large number of NEs to gazetteers.
In recent years, NE extraction has been performed
with machine learning based methods. However,
such methods cannot cover all of NEs in texts.
Therefore, it is necessary to extract NEs from ex-
isting resources and use them to identify more NEs.
There are many useful resources on the Web. We fo-
cus on Wikipedia1 as the resource for acquiring NEs.
Wikipedia is a free multilingual online encyclope-
dia and a rapidly growing resource. In Wikipedia,
a large number of NEs are described in titles of ar-
ticles with useful information such as HTML tree
structures and categories. Each article links to other
related articles. According to these characteristics,
they could be an appropriate resource for extracting
NEs.
Since a specific entity or concept is glossed in a
Wikipedia article, we can regard the NE extraction
problem as a document classification problem of the
Wikipedia article. In traditional approaches for doc-
ument classification, in many cases, documents are
classified independently. However, the Wikipedia
articles are hypertexts and they have a rich structure
that is useful for categorization. For example, hyper-
linked mentions (we call them anchor texts) which
are enumerated in a list tend to refer to the articles
that describe other NEs belonging to the same class.
It is expected that improved NE categorization is ac-
complished by capturing such dependencies.
We structure anchor texts and dependencies be-
tween them into a graph, and train graph-based
CRFs to obtain probabilistic models to estimate cat-
egories for NEs in Wikipedia.
</bodyText>
<footnote confidence="0.585284333333333">
So far, several statistical models that can cap-
1
http://wikipedia.org/
</footnote>
<page confidence="0.99947">
649
</page>
<bodyText confidence="0.999556466666667">
\x0cture dependencies between examples have been pro-
posed. There are two types of classification meth-
ods that can capture dependencies: iterative classi-
fication methods (Neville and Jensen, 2000; Lu and
Getoor, 2003b) and collective classification methods
(Getoor et al., 2001; Taskar et al., 2002). In this
paper, we use Conditional Random Fields (CRFs)
(Lafferty et al., 2001) for NE categorization in
Wikipedia.
The rest of the paper is structured as follows. Sec-
tion 2 describes the general framework of CRFs.
Section 3 describes a graph-based CRFs for NE cat-
egorization in Wikipedia. In section 4, we show
the experimental results. Section 5 describes related
work. We conclude in section 6.
</bodyText>
<sectionHeader confidence="0.982132" genericHeader="method">
2 Conditional Random Fields
</sectionHeader>
<bodyText confidence="0.985953272727273">
Conditional Random Fields (CRFs) (Lafferty et al.,
2001) are undirected graphical models that give a
conditional probability distribution p(y|x) in a form
of exponential model.
CRFs are formalized as follows. Let G = {V, E}
be an undirected graph over random variables y and
x, where V is a set of vertices, and E is a set
of edges in the graph G. When a set of cliques
C = {{yc, xc}} are given, CRFs define the con-
ditional probability of a state assignment given an
observation set.
</bodyText>
<equation confidence="0.947069333333333">
p(y|x) =
1
Z(x)
\x02
cC
(xc, yc) (1)
where (xc, yc) is a potential function defined
over cliques, and Z(x) =
\x03
y
\x04
cC (xc, yc) is
</equation>
<bodyText confidence="0.998823">
the partition function.
The potentials are factorized according to the set
of features {fk}.
</bodyText>
<equation confidence="0.985301833333333">
(xc, yc) = exp
\x05
\x06
k
kfk(xc, yc)
\x07
</equation>
<bodyText confidence="0.980650789473684">
(2)
where F = {f1, ..., fK} are feature functions on
the cliques, = {1, ..., K R} are the model
parameters. The parameters are estimated itera-
tive scaling or quasi-Newton method from labeled
data.
The original paper (Lafferty et al., 2001) fo-
cused on linear-chain CRFs, and applied them to
part-of-speech tagging problem. McCallum et al.
(2003), Sutton et al (2004) proposed Dynamic Con-
ditional Random Fields (DCRFs), the generaliza-
tion of linear-chain CRFs, that have complex graph
structure (include cycles). Since DCRFs model
structure contains cycles, it is necessary to use ap-
proximate inference methods to calculate marginal
probability. Tree-based Reparameterization (TRP)
(Wainwright et al., 2003), a schedule for loopy be-
lief propagation, is used for approximate inference
in these papers.
</bodyText>
<sectionHeader confidence="0.730798" genericHeader="method">
3 Graph-based CRFs for NE
</sectionHeader>
<bodyText confidence="0.987070411764706">
Categorization in Wikipedia
In this section we describe how to apply CRFs for
NE categorization in Wikipedia.
Each Wikipedia article describes a specific entity
or concept by a heading word, a definition, and one
or more categories. One possible approach is to clas-
sify each NE described in an article into an appropri-
ate category by exploiting the definition of the arti-
cle. This process can be done one by one without
considering the relationship with other articles.
On the other hand, articles in Wikipedia are
semi-structured texts. Especially lists (&amp;lt;UL&gt; or
&amp;lt;OL&gt;) and tables (&amp;lt;TABLE&gt;) have an important
characteristics, that is, occurrence of elements in
them have some sort of dependencies. Structural
characteristics, such as lists (&amp;lt;UL&gt; or &amp;lt;OL&gt;) or
tables (&amp;lt;TABLE&gt;), are useful becase their ele-
ments have some sort of dependencies.
Figure 2 shows an example of an HTML segment
and the corresponding tree structure. The first an-
chor texts in each list tag (&amp;lt;LI&gt;) tend to be in the
same NE category. Such characteristics are useful
feature for the categorization task. In this paper we
focus on lists which appear frequently in Wikipedia.
Furthermore, there are anchor texts in articles.
Anchor texts are glossed entity or concept described
with links to other pages. With this in mind, our NE
categorization problem can be regarded as NE cat-
egory labeling problem for anchor texts in articles.
Exploiting dependencies of anchor texts that are in-
duced by the HTML structure is expected to improve
categorization performance.
We use CRFs for categorization in which anchor
texts correspond to random variables V in G and de-
</bodyText>
<page confidence="0.942647">
650
</page>
<equation confidence="0.998010701492537">
\x0cSibling ES = {(vT
i , vT
j )|vT
i , vT
j V T
, d(vT
i , ca(vT
i , vT
j )) = d(vT
j , ca(vT
i , vT
j )) = 1, vT
j = ch(pa(vT
j , 1), k),
vT
i = ch(pa(vT
i , 1), max{l|l &amp;lt; k})}
Cousin EC = {(vT
i , vT
j )|vT
i , vT
j V T
, d(vT
i , ca(vT
i , vT
j )) = d(vT
j , ca(vT
i , vT
j )) 2, vT
i = ch(pa(vT
i ), k),
vT
j = ch(pa(vT
j ), k), pa(vT
j , d(vT
j , ca(vT
i , vT
j )) 1) = ch(pa(vT
j , d(vT
j , ca(vT
i , vT
j ))), k),
pa(vT
i , d(vT
i , ca(vT
i , vT
j )) 1) = ch(pa(vT
i , ca(vT
i , vT
j )), max{l|l &amp;lt; k})}
Relative ER = {(vT
i , vT
j )|vT
i , vT
j V T
, d(vT
i , ca(vT
i , vT
j )) = 1, d(vT
j , ca(vT
i , vT
j )) = 3,
pa(vT
j , 2) = ch(pa(vT
j , 3), k), vT
i = ch(pa(vT
i , 1), max{l|l &amp;lt; k})}
</equation>
<figureCaption confidence="0.998789">
Figure 1: The definitions of sibling, cousin and relative cliques, where ES, EC, ER correspond to sets which
</figureCaption>
<bodyText confidence="0.6500515">
consist of anchor text pairs that have sibling, cousin and relative relations respectively.
pendencies between anchor texts are treated as edges
E in G. In the next section, we describe the concrete
way to construct graphs.
</bodyText>
<subsectionHeader confidence="0.985062">
3.1 Constructing a graph from an HTML tree
</subsectionHeader>
<bodyText confidence="0.981017566666667">
An HTML document is an ordered tree. We de-
fine a graph G = (V G, EG) on an HTML tree
T HTML = (V T , ET ): the vertices V G are anchor
texts in the HTML text; the edges E are limited to
cliques of Sibling, Cousin, and Relative, which we
will describe later in the section. These cliques are
intended to encode a NE label dependency between
anchor texts where the two NEs tend to be in the
same or related class, or one NE affects the other
NE label.
Let us consider dependent anchor text pairs in
Figure 2. First, Dillard &amp; Clark and country
rock have a sibling relation over the tree structure,
and appearing the same element of the list. The latter
element in this relation tends to be an attribute or a
concept of the other element in the relation. Second,
Dillard &amp; Clark and Carpenters have a cousin
relation over the tree structure, and they tend to have
a common attribute such as Artist. The elements in
this relation tend to belong to the same class. Third,
Carpenters and Karen Carpenter have a relation
in which Karen Carpenter is a siblings grandchild
in relation to Carpenters over the tree structure.
The latter elements in this relation tends to be a con-
stituent part of the other element in the relation. We
can say that the model can capture dependencies by
dealing with anchor texts that depend on each other
as cliques. Based on the observations as above, we
treat a pair of anchor texts as cliques which satisfy
the condtions in Figure 1.
</bodyText>
<figure confidence="0.999423">
&amp;lt;UL&gt;
&amp;lt;LI&gt;
&amp;lt;A&gt;
&amp;lt;LI&gt; &amp;lt;LI&gt;
&amp;lt;A&gt;
&amp;lt;A&gt;
&amp;lt;UL&gt;
&amp;lt;A&gt;
Dillard &amp;
Clark
country
rock
Carpenters
Karen
Carpenter
Sibling Cousin
Relative
Dillard &amp; Clark ......
...country rock...
Carpenters
Karen Carpenter
</figure>
<figureCaption confidence="0.999228">
Figure 2: Correspondence between tree structure
</figureCaption>
<bodyText confidence="0.81726">
and defined cliques.
Now, we define the three sorts of edges given an
HTML tree. Consider an HTML tree T HTML =
(V T , ET ), where V T and ET are nodes and edges
over the tree. Let d(vT
</bodyText>
<equation confidence="0.967304111111111">
i , vT
j ) be the number of edges
between vT
i and vT
j where vT
i , vT
j V T , pa(vT
i , k)
be k-th generation ancestor of vT
i , ch(vT
i , k) be
vT
i s k-th child, ca(vT
i , vT
j ) be a common ances-
tor of vT
i , vT
j V T . Precise definitions of cliques,
</equation>
<figureCaption confidence="0.739201">
namely Sibling, Cousin, and Relative, are given in
Figure 1. A set of cliques used in our graph-based
CRFs are edges defined in Figure 1 and vertices, i.e.
</figureCaption>
<equation confidence="0.65808">
C = ES EC ER V . Note that they are re-
</equation>
<bodyText confidence="0.96502">
stricted to pairs of the nearest vertices to keep the
graph simple.
</bodyText>
<subsectionHeader confidence="0.995775">
3.2 Model
</subsectionHeader>
<bodyText confidence="0.999425">
We introduce potential functions for cliques to de-
fine conditional probability distribution over CRFs.
Conditional distribution over label set y given ob-
</bodyText>
<page confidence="0.912276">
651
</page>
<equation confidence="0.969853">
\x0cservation set x is defined as:
p(y|x) =
1
Z(x)
\x02
(vi,vj)ESECER
SCR(yi, yj)
\x02
viV
V (yi, x)
(3)
</equation>
<bodyText confidence="0.9904712">
where SCR(yi, yj) is the potential over sibling,
cousin and relative edges, V (yi, x) is the potential
over the nodes, and Z(x) is the partition function.
The potentials SCR(yi, yj) and V (yi, x) factor-
ize according to the features fk and weights k as:
</bodyText>
<equation confidence="0.998191857142857">
SCR (yi, yj) = exp
\x05
\x06
k
kfk(yi, yj)
\x07
(4)
V (yi, x) = exp
\x05
\x06
k\x02
k\x02 fk\x02 (yi, x)
\x07
(5)
</equation>
<bodyText confidence="0.9521896">
fk(yi, yj) captures co-occurrences between labels,
where k {(yi, yj)|Y Y} corresponds to the par-
ticular element of the Cartesian product of the label
set Y. fk\x02(yi, x) captures co-occurrences between
label yi Y and observation features, where k\x04 cor-
responds to the particular element of the label set
and observed features.
The weights of a CRF, = {k, . . . , k\x02 , . . .}
are estimated to maximize the conditional log-
likelihood of the graph in a training dataset
</bodyText>
<equation confidence="0.901788">
D = {\x04x(1), y(1)\x05, \x04x(2), y(2)\x05, . . . , \x04x(N), y(N)\x05}
</equation>
<bodyText confidence="0.922708">
The log-likelihood function can be defined as fol-
lows:
</bodyText>
<equation confidence="0.7039068">
L =
N
\x06
d=1
[
</equation>
<figure confidence="0.526881566666667">
\x06
(vi,vj)E
(d)
S E
(d)
C E
(d)
R
\x06
k
kfk(yi, yj)
+
\x06
viV (d)
\x06
k\x02
k\x02 fk\x02 (yi, x(d)
) logZ(x(d)
)]
\x06
k
2
k
22
\x06
k\x02
2
k\x02
22
(6)
</figure>
<bodyText confidence="0.9913326">
where the last two terms are due to the Gaussian
prior (Chen and Rosenfeld, 1999) used to reduce
overfitting. Quasi-Newton methods, such as L-
BFGS (Liu and Nocedal, 1989) can be used for max-
imizing the function.
</bodyText>
<subsectionHeader confidence="0.974527">
3.3 Tree-based Reparameterization
</subsectionHeader>
<bodyText confidence="0.9971833">
Since the proposed model may include loops, it is
necessary to introduce an approximation to calculate
mariginal probabilities. For this, we use Tree-based
Reparameterization (TRP) (Wainwright et al., 2003)
for approximate inference. TRP enumerates a set of
spanning trees from the graph. Then, inference is
performed by applying an exact inference algorithm
such as Belief Propagation to each of the spanning
trees, and updates of marginal probabilities are con-
tinued until they converge.
</bodyText>
<sectionHeader confidence="0.998667" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.901055">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.994430266666667">
Our dataset is a random selection of 2300 articles
from the Japanese version of Wikipedia as of Octo-
ber 2005. All anchor texts appearing under HTML
&amp;lt;LI&gt; tags are hand-annotated with NE class la-
bel. We use the Extended Named Entity Hierar-
chy (Sekine et al., 2002) as the NE class labeling
guideline, but reduce the number of classes to 13
from the original 200+ by ignoring fine-grained cat-
egories and nearby categories in order to avoid data
sparseness. We eliminate examples that consist of
less than two nodes in the SCR model. There are
16136 anchor texts with 14285 NEs. The number
of Sibling, Cousin and Relative edges in the dataset
are |ES |= 4925, |EC |= 13134 and |ER |= 746
respectively.
</bodyText>
<subsectionHeader confidence="0.983473">
4.2 Experimental settings
</subsectionHeader>
<bodyText confidence="0.998385066666666">
The aims of experiments are the two-fold. Firstly,
we investigate the effect of each cliques. The sev-
eral graphs are composed with the three sorts of
edges. We also compare the graph-based models
with a node-wise method just MaxEnt method not
using any edge dependency. Secondly, we com-
pare the proposed method by CRFs with a baseline
method by Support Vector Machines (SVMs) (Vap-
nik, 1998).
The experimental settings of CRFs and SVMs are
as follows.
CRFs In order to investigate which type of clique
boosts classification performance, we perform ex-
periments on several CRFs models that are con-
structed from combinations of defined cliques. Re-
</bodyText>
<page confidence="0.997468">
652
</page>
<table confidence="0.984532583333333">
\x0cSCR SC SR CR
# of loopy examples 318 (36%) 324 (32%) 101 (1%) 42 (2%)
# of linear chain or tree examples 555 (64%) 631 (62%) 2883 (27%) 1464 (54%)
# of one node examples 0 (0%) 60 (6%) 7800 (72%) 1176 (44%)
# of total examples 873 1015 10784 2682
average # of nodes per example 18.5 15.8 1.5 6.0
S C R I
# of loopy examples 0 (0%) 0 (0%) 0 (0%) 0 (0%)
# of linear chain or tree examples 2913 (26%) 1631 (54%) 237 (2%) 0 (0%)
# of one node examples 8298 (74%) 1380 (46%) 15153 (98%) 16136 (100%)
# of total examples 11211 3011 15390 16136
average # of nodes per example 1.4 5.4 1.05 1
</table>
<tableCaption confidence="0.99949">
Table 1: The dataset details constructed from each model.
</tableCaption>
<bodyText confidence="0.969690125">
sulting models of CRFs evaluated on this experi-
ments are SCR, SC, SR, CR, S, C, R and I (indepen-
dent). Figure 3 shows representative graphs of the
eight models. When the graph are disconnected by
reducing the edges, the classification is performed
on each connected subgraph. We call it an example.
We name the examples according the graph struc-
ture: loopy examples are subgraphs including at
least one cycle; linear chain or tree examples are
subgraphs including not a cycle but at least an edge;
one node examples are subgraphs without edges.
Table 1 shows the distribution of the examples of
each model. Since SCR, SC, SR and CR model have
loopy examples, TRP approximate inference is nec-
essary. To perform training and testing with CRFs,
we use GRMM (Sutton, 2006) with TRP. We set the
</bodyText>
<figure confidence="0.988295195652174">
Gaussian Prior variances for weights as 2 = 10 in
all models.
SC model
C
C
C C
S S
S
S
C
SCR model
C
C
C C
S S
S
S
R
R
C
SR model
S S
S
S
R
R
CR model
C
C
C C
R
R
C
S model
S S
S
S
C model
C
C
C C
C
R model
R
R
I model
</figure>
<figureCaption confidence="0.99979">
Figure 3: An example of graphs constructed by
</figureCaption>
<bodyText confidence="0.899217772727273">
combination of defined cliques. S, C, R in the
model names mean that corresponding model has
Sibling, Cousin, Relative cliques respectively. In
each model, classification is performed on each con-
nected subgraph.
SVMs We introduce two models by SVMs (model
I and model P). In model I, each anchor text is clas-
sified independently. In model P, we ordered the
anchor texts in a linear-chain sequence. Then, we
perform a history-based classification along the se-
quence, in which j 1-th classification result is
used in j-th classification. We use TinySVM with
a linear-kernel. One-versus-rest method is used for
multi-class classification. To perform training and
testing with SVMs, we use TinySVM 2 with a linear-
kernel, and one-versus-rest is used for multi-class
classification. We used the cost of constraint vio-
lation C = 1.
Features for CRFs and SVMs The features used
in the classification with CRFs and SVMs are shown
in Table 2. Japanese morphological analyzer MeCab
3 is used to obtain morphemes.
</bodyText>
<subsectionHeader confidence="0.996181">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.998335428571429">
We evaluate the models by 5 fold cross-validation.
Since the number of examples are different in each
model, the datasets are divided taking the examples
namely, connected subgraphs in SCR model.
The size of divided five sub-data are roughly equal.
We evaluate per-class and total extraction perfor-
mance by F1-value.
</bodyText>
<subsectionHeader confidence="0.870699">
4.4 Results and discussion
</subsectionHeader>
<bodyText confidence="0.94682625">
Table 3 shows the classification accuracy of each
model. The second column N stands for the num-
ber of nodes in the gold data. The second last row
ALL stands for the F1-value of all NE classes.
</bodyText>
<figure confidence="0.4489278">
2
http://www.chasen.org/ taku/software/
TinySVM/
3
http://mecab.sourceforge.net/
</figure>
<page confidence="0.978046">
653
</page>
<table confidence="0.923122958333333">
\x0ctypes feature SVMs CRFs
observation definition (bag-of-words)
(V )
features heading of articles
(V )
heading of articles (morphemes)
(V )
categories articles
(V )
categories articles (morphemes)
(V )
anchor texts
(V )
anchor texts (morphemes)
(V )
parent tags of anchor texts
(V )
text included in the last header of anchor texts
(V )
text included in the last header of anchor texts(morphemes)
(V )
label features between-label feature
(S, C, R)
previous label
</table>
<tableCaption confidence="0.960655">
Table 2: Features used in experiments.
</tableCaption>
<bodyText confidence="0.974067">
means that the corresponding features are used in classification.
The V , S, C and R in CRFs column corresponds to the node, sibling edges, cousin edges and relative edges
respectively.
</bodyText>
<table confidence="0.98378825">
CRFs SVMs
NE CLASS N C CR I R S SC SCR SR I P
PERSON 3315 .7419 .7429 .7453 .7458 .7507 .7533 .7981 .7515 .7383 .7386
TIMEX/NUMEX 2749 .9936 .9944 .9940 .9936 .9938 .9931 .9933 .9940 .9933 .9935
FACILITY 2449 .8546 .8541 .8540 .8516 .8500 .8530 .8495 .8495 .8504 .8560
PRODUCT 1664 .7414 .7540 .7164 .7208 .7130 .7371 .7418 .7187 .7154 .7135
LOCATION 1480 .7265 .7239 .6989 .7048 .6974 .7210 .7232 .7033 .7022 .7132
NATURAL OBJECTS 1132 .3333 .3422 .3476 .3513 .3547 .3294 .3304 .3316 .3670 .3326
ORGANIZATION 991 .7122 .7160 .7100 .7073 .7122 .6961 .5580 .7109 .7141 .7180
VOCATION 303 .9088 .9050 .9075 .9059 .9150 .9122 .9100 .9186 .9091 .9069
EVENT 121 .2740 .2345 .2533 .2667 .2800 .2740 .2759 .2667 .3418 .3500
TITLE 42 .1702 .0889 .2800 .2800 .3462 .2083 .1277 .3462 .2593 .2642
NAME OTHER 24 .0000 .0000 .0000 .0000 .0000 .0000 .0000 .0000 .0690 .0000
UNIT 15 .2353 .1250 .2353 .2353 .2353 .1250 .1250 .2353 .3333 .3158
ALL 14285 .7846 .7862 .7806 .7814 .7817 .7856 .7854 .7823 .7790 .7798
ALL (no articles) 3898 .5476 .5495 .5249 .5274 .5272 .5484 .5465 .5224 .5278 .5386
</table>
<tableCaption confidence="0.999922">
Table 3: Comparison of F1-values of CRFs and SVMs.
</tableCaption>
<page confidence="0.99724">
654
</page>
<bodyText confidence="0.965608829787234">
\x0cThe last row ALL (no article) stands for the F1-
value of all NE classes which have no gloss texts in
Wikipedia.
Relational vs. Independent Among the models
constructed by combination of defined cliques, the
best F1-value is achieved by CR model, followed by
SC, SCR, C, SR, S, R and I. We performed McNe-
mar paired test on labeling disagreements between
CR model of CRFs and I model of CRFs. The
difference was significant (p &amp;lt; 0.01). These re-
sults show that considering dependencies work pos-
itively in obtaining better accuracy than classify-
ing independently. The Cousin cliques provide the
highest accuracy improvement among the three de-
fined cliques. The reason may be that the Cousin
cliques appear frequently in comparison with the
other cliques, and also possess strong dependencies
among anchor texts. As for PERSON, better accu-
racy is achieved in SC and SCR models. In fact,
the PERSON-PERSON pairs frequently appear in
Sibling cliques (435 out of 4925) and in Cousin
cliques (2557 out of 13125) in the dataset. Also, as
for PRODUCT and LOCATION, better accuracy is
achieved in the models that contain Cousin cliques
(C, CR, SC and SCR model). 1072 PRODUCT-
PRODUCT pairs and 738 LOCATION-LOCATION
pairs appear in Cousin cliques. All (no article)
row in Table 3 shows the F1-value of nodes which
have no gloss texts. The F1-value difference be-
tween CR and I model of CRF in ALL (no article)
row is larger than the difference in All row. The
fact means that the dependency information helps to
extract NEs without gloss texts in Wikipedia. We
attempted a different parameter tying in which the
SCR potential functions are tied with a particular ob-
servation feature. This parameter tying is introduced
by Ghamrawi and McCallum (2005). However, we
did not get any improved accuracy.
CRFs vs. SVMs The best model of CRFs (CR
model) outperforms the best model of SVMs (P
model). We performed McNemar paired test on la-
beling disagreements between CR model of CRFs
and P model of SVMs. The difference was signifi-
cant (p &amp;lt; 0.01). In the classes having larger num-
ber of examples, models of CRFs achieve better F1-
values than models of SVMs. However, in several
classes having smaller number of examples such as
</bodyText>
<figure confidence="0.98950175">
0.4 0.5 0.6 0.7 0.8
0.80
0.85
0.90
0.95
Recall
Precision
CR model of CRFs
</figure>
<figureCaption confidence="0.999858">
Figure 4: Precision-Recall curve obtained by vary-
</figureCaption>
<bodyText confidence="0.9874016">
ing the threshold of marginal probability from 1.0
to 0.0.
EVENT and UNIT, models of SVMs achieve signif-
icantly better F1-values than models of CRFs.
Filtering NE Candidates using Marginal Prob-
ability The precision-recall curve obtained by
thresholding the marginal probability of the MAP
estimation in the CR models is shown in Figure 4.
The curve reaches a peak at 0.57 in recall, and the
precision value at that point is 0.97. This preci-
sion and recall values mean that 57% of all NEs can
be classified with approximately 97% accuracy on a
particular thresholding of marginal probability. This
results suggest that the extracted NE candidates can
be filtered with fewer cost by exploiting the marginal
probability.
Training Time The total training times of all
CRFs and SVMs models are shown in Table 4. The
training time tends to increase in case models have
complicated graph structure. For instance, model
SCR has complex graph structure compare to model
I, therefore the SCRs training time is three times
longer than model I. Training the models by SVMs
are faster than training the models by CRFs. The dif-
ference comes from the implementation issues: C++
</bodyText>
<page confidence="0.995199">
655
</page>
<table confidence="0.989956333333333">
\x0cCRFs SVMs
C CR I R S SC SCR SR I P
Training Time (minutes) 207 255 97 90 138 305 316 157 28 29
</table>
<tableCaption confidence="0.99299">
Table 4: Training Time (minutes)
</tableCaption>
<bodyText confidence="0.984562">
vs. Java, differences of feature extraction modules,
and so on. So, the comparing these two is not the
important issue in this experiment.
</bodyText>
<sectionHeader confidence="0.999701" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9989496875">
Wikipedia has become a popular resource for NLP.
Bunescu and Pasca used Wikipedia for detecting and
disambiguating NEs in open domain texts (2006).
Strube and Ponzetto explored the use of Wikipedia
for measuring Semantic Relatedness between two
concepts (2006), and for Coreference Resolution
(2006).
Several CRFs have been explored for informa-
tion extraction from the web. Tang et al. pro-
posed Tree-structured Conditional Random Fields
(TCRFs) (2006) that capture hierarchical structure
of web documents. Zhu et al. proposed Hierar-
chical Conditional Random Fields (HCRFs) (2006)
for product information extraction from Web docu-
ments. TCRFs and HCRFs are similar to our ap-
proach described in section 4 in that the model struc-
ture is induced by page structure. However, the
model structures of these models are different from
our model.
There are statistical models that capture depen-
dencies between examples. There are two types of
classification approaches: iterative (Lu and Getoor,
2003b; Lu and Getoor, 2003a) or collective (Getoor
et al., 2001; Taskar et al., 2002). Lu et al. (2003a;
2003b) proposed link-based classification method
based on logistic regression. This model iterates lo-
cal classification until label assignments converge.
The results vary from the ordering strategy of lo-
cal classification. In contrast to iterative classifica-
tion methods, collective classification methods di-
rectly estimate most likely assignments. Getoor
et al. proposed Probabilistic Relational Models
(PRMs) (2001) which are built upon Bayesian Net-
works. Since Bayesian Networks are directed graph-
ical models, PRMs cannot model directly the cases
where instantiated graph contains cycles. Taskar et
al. proposed Relational Markov Networks (RMNs)
(2002). RMNs are the special case of Conditional
Markov Networks (or Conditional Random Fields)
in which graph structure and parameter tying are de-
termined by SQL-like form.
As for the marginal probability to use as a confi-
dence measure shown in Figure 4, Peng et al. (2004)
has applied linear-chain CRFs to Chinese word seg-
mentation. It is calculated by constrained forward-
backward algorithm (Culotta and McCallum, 2004),
and confident segments are added to the dictionary
in order to improve segmentation accuracy.
</bodyText>
<sectionHeader confidence="0.997873" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.996500733333333">
In this paper, we proposed a method for categorizing
NEs in Wikipedia. We defined three types of cliques
that are constitute dependent anchor texts in con-
struct CRFs graph structure, and introduced poten-
tial functions for them to reflect classification. The
experimental results show that the effectiveness of
capturing dependencies, and proposed CRFs model
can achieve significant improvements compare to
baseline methods with SVMs. The results also show
that the dependency information from the HTML
tree helps to categorize entities without gloss texts
in Wikipedia. The marginal probability of MAP as-
signments can be used as confidence measure of the
entity categorization. We can control the precision
by filtering the confidence measure as PR curve in
</bodyText>
<figureCaption confidence="0.756523">
Figure 4. The measure can be also used as a con-
</figureCaption>
<bodyText confidence="0.995249416666667">
fidence estimator in active learning in CRFs (Kim
et al., 2006), where examples with the most uncer-
tainty are selected for presentation to human anno-
tators.
In future research, we plan to explore NE catego-
rization with more fine-grained label set. For NLP
applications such as QA, NE dictionary with fine-
grained label sets will be a useful resource. How-
ever, generally, classification with statistical meth-
ods becomes difficult in case that the label set is
large, because of the insufficient positive examples.
It is an issue to be resolved in the future.
</bodyText>
<page confidence="0.998345">
656
</page>
<reference confidence="0.996633070707071">
\x0cReferences
Razvan Bunescu and Marius Pasca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, Carnegie Mellon University.
Aron Culotta and Andrew McCallum. 2004. Confi-
dence estimation for information extraction. In Pro-
ceedings of Human Language Technology Conference
and North American Chapter of the Association for
Computational Linguistics (HLT-NAACL).
Lise Getoor, Eran Segal, Ben Taskar, and Daphne Koller.
2001. Probabilistic models of text and link structure
for hypertext classification. In IJCAI Workshop on
Text Learning: Beyond Supervision, 2001.
Nadia Ghamrawi and Andrew McCallum. 2005. Col-
lective multi-label classification. In Fourteenth Con-
ference on Information and Knowledge Management
(CIKM).
Juanzi Li Jie Tang, Mingcai Hong and Bangyong Liang.
2006. Tree-structured conditional random fields for
semantic annotation. In Proceedings of 5th Interna-
tional Conference of Semantic Web (ISWC-06).
Seokhwan Kim, Yu Song, Kyungduk Kim, Jeong-Won
Cha, and Gary Geunbae Lee. 2006. MMR-based ac-
tive machine learning for bio named entity recogni-
tion. In Proceedings of the Human Language Technol-
ogy Conference/North American chapter of the Asso-
ciation for Computational Linguistics annual meeting
(HLT-NAACL06).
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning, pages 282289. Morgan Kaufmann,
San Francisco, CA.
Dong C. Liu and Jorge Nocedal. 1989. The limited mem-
ory BFGS methods for large scale optimization. In
Mathematical Programming 45.
Qing Lu and Lise Getoor. 2003a. Link-based classifica-
tion using labeled and unlabeled data. In Proceedings
of the International Conference On Machine Learning,
Washington DC, August.
Qing Lu and Lise Getoor. 2003b. Link-based text clas-
sification. In Proceedings of the International Joint
Conference on Artificial Intelligence.
Andrew McCallum, Khashayar Rohanimanesh, and
Charles Sutton. 2003. Dynamic conditional random
fields for jointly labeling multiple sequences. In NIPS
Workshop on Syntax, Semantics, and Statistics, De-
cember.
J. Neville and D. Jensen. 2000. Iterative classification
in relational data. In Proceedings of AAAI-2000 Work-
shop on Learning Statistical Models from Relational
Data, pages 1320. AAAI Press.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of
The 20th International Conference on Computational
Linguistics (COLING).
Simone Paolo Ponzetto and Michael Strube. 2006. Ex-
ploiting semantic role labeling, wordnet and wikipedia
for coreference resolution. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Proceed-
ings of the LREC-2002.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In Proceedings of the 21st National Con-
ference on Artificial Intelligence (AAAI-06).
Charles Sutton, Khashayar Rohanimanesh, and Andrew
McCallum. 2004. Dynamic conditional random
fields: Factorized probabilistic models for labeling and
segmenting sequence data. In Proceedings of the 21th
International Conference on Machine Learning.
Charles Sutton. 2006. GRMM: A graphical models
toolkit. http://mallet.cs.umass.edu.
Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In Proceedings of the 18th Conference on Uncertainty
in Artificial Intelligence. Morgan Kaufmann.
Vladimir Vapnik. 1998. Statistical Learning Theory.
Wiley Interscience.
Martin Wainwright, Tommi Jaakkola, and Alan Will-
sky. 2003. Tree-based reparameterization frame-
work for analysis of sum-product and related algo-
rithms. IEEE Transactions on Information Theory,
45(9):11201146.
Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, and Wei-
Ying Ma. 2006. Simultaneous record detection and
attribute labeling in web data extraction. In Proceed-
ings of the 12th ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining.
</reference>
<page confidence="0.970905">
657
</page>
<figure confidence="0.257847">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.348018">
<note confidence="0.920966333333333">b&amp;apos;Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 649657, Prague, June 2007. c 2007 Association for Computational Linguistics</note>
<title confidence="0.9176985">A Graph-based Approach to Named Entity Categorization in Wikipedia Using Conditional Random Fields</title>
<author confidence="0.992226">Yotaro Watanabe</author>
<author confidence="0.992226">Masayuki Asahara</author>
<author confidence="0.992226">Yuji Matsumoto</author>
<affiliation confidence="0.997858">Graduate School of Information Science Nara Institute of Science and Technology</affiliation>
<address confidence="0.999806">8916-5 Takayama, Ikoma, Nara, 630-0192, Japan</address>
<email confidence="0.980187">yotaro-w@is.naist.jp</email>
<email confidence="0.980187">masayu-a@is.naist.jp</email>
<email confidence="0.980187">matsu@is.naist.jp</email>
<abstract confidence="0.980807083333333">This paper presents a method for categorizing named entities in Wikipedia. In Wikipedia, an anchor text is glossed in a linked HTML text. We formalize named entity categorization as a task of categorizing anchor texts with linked HTML texts which glosses a named entity. Using this representation, we introduce a graph structure in which anchor texts are regarded as nodes. In order to incorporate HTML structure on the graph, three types of cliques are defined based on the HTML tree structure. We propose a method with Conditional Random Fields (CRFs) to categorize the nodes on the graph. Since the defined graph may include cycles, the exact inference of CRFs is computationally expensive. We introduce an approximate inference method using Treebased Reparameterization (TRP) to reduce computational cost. In experiments, our proposed model obtained significant improvements compare to baseline models that use Support Vector Machines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences Razvan Bunescu</author>
<author>Marius Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>\x0cReferences Razvan Bunescu and Marius Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="12422" citStr="Chen and Rosenfeld, 1999" startWordPosition="2219" endWordPosition="2222">atures, where k\x04 corresponds to the particular element of the label set and observed features. The weights of a CRF, = {k, . . . , k\x02 , . . .} are estimated to maximize the conditional loglikelihood of the graph in a training dataset D = {\x04x(1), y(1)\x05, \x04x(2), y(2)\x05, . . . , \x04x(N), y(N)\x05} The log-likelihood function can be defined as follows: L = N \x06 d=1 [ \x06 (vi,vj)E (d) S E (d) C E (d) R \x06 k kfk(yi, yj) + \x06 viV (d) \x06 k\x02 k\x02 fk\x02 (yi, x(d) ) logZ(x(d) )] \x06 k 2 k 22 \x06 k\x02 2 k\x02 22 (6) where the last two terms are due to the Gaussian prior (Chen and Rosenfeld, 1999) used to reduce overfitting. Quasi-Newton methods, such as LBFGS (Liu and Nocedal, 1989) can be used for maximizing the function. 3.3 Tree-based Reparameterization Since the proposed model may include loops, it is necessary to introduce an approximation to calculate mariginal probabilities. For this, we use Tree-based Reparameterization (TRP) (Wainwright et al., 2003) for approximate inference. TRP enumerates a set of spanning trees from the graph. Then, inference is performed by applying an exact inference algorithm such as Belief Propagation to each of the spanning trees, and updates of marg</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
</authors>
<title>Confidence estimation for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="25589" citStr="Culotta and McCallum, 2004" startWordPosition="4455" endWordPosition="4458">sian Networks. Since Bayesian Networks are directed graphical models, PRMs cannot model directly the cases where instantiated graph contains cycles. Taskar et al. proposed Relational Markov Networks (RMNs) (2002). RMNs are the special case of Conditional Markov Networks (or Conditional Random Fields) in which graph structure and parameter tying are determined by SQL-like form. As for the marginal probability to use as a confidence measure shown in Figure 4, Peng et al. (2004) has applied linear-chain CRFs to Chinese word segmentation. It is calculated by constrained forwardbackward algorithm (Culotta and McCallum, 2004), and confident segments are added to the dictionary in order to improve segmentation accuracy. 6 Conclusion In this paper, we proposed a method for categorizing NEs in Wikipedia. We defined three types of cliques that are constitute dependent anchor texts in construct CRFs graph structure, and introduced potential functions for them to reflect classification. The experimental results show that the effectiveness of capturing dependencies, and proposed CRFs model can achieve significant improvements compare to baseline methods with SVMs. The results also show that the dependency information fro</context>
</contexts>
<marker>Culotta, McCallum, 2004</marker>
<rawString>Aron Culotta and Andrew McCallum. 2004. Confidence estimation for information extraction. In Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lise Getoor</author>
<author>Eran Segal</author>
<author>Ben Taskar</author>
<author>Daphne Koller</author>
</authors>
<title>Probabilistic models of text and link structure for hypertext classification.</title>
<date>2001</date>
<booktitle>In IJCAI Workshop on Text Learning: Beyond Supervision,</booktitle>
<contexts>
<context position="3786" citStr="Getoor et al., 2001" startWordPosition="580" endWordPosition="583">It is expected that improved NE categorization is accomplished by capturing such dependencies. We structure anchor texts and dependencies between them into a graph, and train graph-based CRFs to obtain probabilistic models to estimate categories for NEs in Wikipedia. So far, several statistical models that can cap1 http://wikipedia.org/ 649 \x0cture dependencies between examples have been proposed. There are two types of classification methods that can capture dependencies: iterative classification methods (Neville and Jensen, 2000; Lu and Getoor, 2003b) and collective classification methods (Getoor et al., 2001; Taskar et al., 2002). In this paper, we use Conditional Random Fields (CRFs) (Lafferty et al., 2001) for NE categorization in Wikipedia. The rest of the paper is structured as follows. Section 2 describes the general framework of CRFs. Section 3 describes a graph-based CRFs for NE categorization in Wikipedia. In section 4, we show the experimental results. Section 5 describes related work. We conclude in section 6. 2 Conditional Random Fields Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models that give a conditional probability distribution p(y|x) in a f</context>
<context position="24477" citStr="Getoor et al., 2001" startWordPosition="4290" endWordPosition="4293"> Fields (TCRFs) (2006) that capture hierarchical structure of web documents. Zhu et al. proposed Hierarchical Conditional Random Fields (HCRFs) (2006) for product information extraction from Web documents. TCRFs and HCRFs are similar to our approach described in section 4 in that the model structure is induced by page structure. However, the model structures of these models are different from our model. There are statistical models that capture dependencies between examples. There are two types of classification approaches: iterative (Lu and Getoor, 2003b; Lu and Getoor, 2003a) or collective (Getoor et al., 2001; Taskar et al., 2002). Lu et al. (2003a; 2003b) proposed link-based classification method based on logistic regression. This model iterates local classification until label assignments converge. The results vary from the ordering strategy of local classification. In contrast to iterative classification methods, collective classification methods directly estimate most likely assignments. Getoor et al. proposed Probabilistic Relational Models (PRMs) (2001) which are built upon Bayesian Networks. Since Bayesian Networks are directed graphical models, PRMs cannot model directly the cases where in</context>
</contexts>
<marker>Getoor, Segal, Taskar, Koller, 2001</marker>
<rawString>Lise Getoor, Eran Segal, Ben Taskar, and Daphne Koller. 2001. Probabilistic models of text and link structure for hypertext classification. In IJCAI Workshop on Text Learning: Beyond Supervision, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadia Ghamrawi</author>
<author>Andrew McCallum</author>
</authors>
<title>Collective multi-label classification.</title>
<date>2005</date>
<booktitle>In Fourteenth Conference on Information and Knowledge Management (CIKM).</booktitle>
<contexts>
<context position="21381" citStr="Ghamrawi and McCallum (2005)" startWordPosition="3773" endWordPosition="3776"> Cousin cliques (C, CR, SC and SCR model). 1072 PRODUCTPRODUCT pairs and 738 LOCATION-LOCATION pairs appear in Cousin cliques. All (no article) row in Table 3 shows the F1-value of nodes which have no gloss texts. The F1-value difference between CR and I model of CRF in ALL (no article) row is larger than the difference in All row. The fact means that the dependency information helps to extract NEs without gloss texts in Wikipedia. We attempted a different parameter tying in which the SCR potential functions are tied with a particular observation feature. This parameter tying is introduced by Ghamrawi and McCallum (2005). However, we did not get any improved accuracy. CRFs vs. SVMs The best model of CRFs (CR model) outperforms the best model of SVMs (P model). We performed McNemar paired test on labeling disagreements between CR model of CRFs and P model of SVMs. The difference was significant (p &amp;lt; 0.01). In the classes having larger number of examples, models of CRFs achieve better F1- values than models of SVMs. However, in several classes having smaller number of examples such as 0.4 0.5 0.6 0.7 0.8 0.80 0.85 0.90 0.95 Recall Precision CR model of CRFs Figure 4: Precision-Recall curve obtained by varying t</context>
</contexts>
<marker>Ghamrawi, McCallum, 2005</marker>
<rawString>Nadia Ghamrawi and Andrew McCallum. 2005. Collective multi-label classification. In Fourteenth Conference on Information and Knowledge Management (CIKM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juanzi Li Jie Tang</author>
<author>Mingcai Hong</author>
<author>Bangyong Liang</author>
</authors>
<title>Tree-structured conditional random fields for semantic annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of 5th International Conference of Semantic Web (ISWC-06).</booktitle>
<marker>Tang, Hong, Liang, 2006</marker>
<rawString>Juanzi Li Jie Tang, Mingcai Hong and Bangyong Liang. 2006. Tree-structured conditional random fields for semantic annotation. In Proceedings of 5th International Conference of Semantic Web (ISWC-06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seokhwan Kim</author>
<author>Yu Song</author>
<author>Kyungduk Kim</author>
<author>Jeong-Won Cha</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>MMR-based active machine learning for bio named entity recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference/North American chapter of the Association for Computational Linguistics annual meeting (HLT-NAACL06).</booktitle>
<marker>Kim, Song, Kim, Cha, Lee, 2006</marker>
<rawString>Seokhwan Kim, Yu Song, Kyungduk Kim, Jeong-Won Cha, and Gary Geunbae Lee. 2006. MMR-based active machine learning for bio named entity recognition. In Proceedings of the Human Language Technology Conference/North American chapter of the Association for Computational Linguistics annual meeting (HLT-NAACL06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282289</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="3888" citStr="Lafferty et al., 2001" startWordPosition="597" endWordPosition="600">tructure anchor texts and dependencies between them into a graph, and train graph-based CRFs to obtain probabilistic models to estimate categories for NEs in Wikipedia. So far, several statistical models that can cap1 http://wikipedia.org/ 649 \x0cture dependencies between examples have been proposed. There are two types of classification methods that can capture dependencies: iterative classification methods (Neville and Jensen, 2000; Lu and Getoor, 2003b) and collective classification methods (Getoor et al., 2001; Taskar et al., 2002). In this paper, we use Conditional Random Fields (CRFs) (Lafferty et al., 2001) for NE categorization in Wikipedia. The rest of the paper is structured as follows. Section 2 describes the general framework of CRFs. Section 3 describes a graph-based CRFs for NE categorization in Wikipedia. In section 4, we show the experimental results. Section 5 describes related work. We conclude in section 6. 2 Conditional Random Fields Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models that give a conditional probability distribution p(y|x) in a form of exponential model. CRFs are formalized as follows. Let G = {V, E} be an undirected graph over r</context>
<context position="5231" citStr="Lafferty et al., 2001" startWordPosition="837" endWordPosition="840">{{yc, xc}} are given, CRFs define the conditional probability of a state assignment given an observation set. p(y|x) = 1 Z(x) \x02 cC (xc, yc) (1) where (xc, yc) is a potential function defined over cliques, and Z(x) = \x03 y \x04 cC (xc, yc) is the partition function. The potentials are factorized according to the set of features {fk}. (xc, yc) = exp \x05 \x06 k kfk(xc, yc) \x07 (2) where F = {f1, ..., fK} are feature functions on the cliques, = {1, ..., K R} are the model parameters. The parameters are estimated iterative scaling or quasi-Newton method from labeled data. The original paper (Lafferty et al., 2001) focused on linear-chain CRFs, and applied them to part-of-speech tagging problem. McCallum et al. (2003), Sutton et al (2004) proposed Dynamic Conditional Random Fields (DCRFs), the generalization of linear-chain CRFs, that have complex graph structure (include cycles). Since DCRFs model structure contains cycles, it is necessary to use approximate inference methods to calculate marginal probability. Tree-based Reparameterization (TRP) (Wainwright et al., 2003), a schedule for loopy belief propagation, is used for approximate inference in these papers. 3 Graph-based CRFs for NE Categorization</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning, pages 282289. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>The limited memory BFGS methods for large scale optimization.</title>
<date>1989</date>
<booktitle>In Mathematical Programming 45.</booktitle>
<contexts>
<context position="12510" citStr="Liu and Nocedal, 1989" startWordPosition="2233" endWordPosition="2236">tures. The weights of a CRF, = {k, . . . , k\x02 , . . .} are estimated to maximize the conditional loglikelihood of the graph in a training dataset D = {\x04x(1), y(1)\x05, \x04x(2), y(2)\x05, . . . , \x04x(N), y(N)\x05} The log-likelihood function can be defined as follows: L = N \x06 d=1 [ \x06 (vi,vj)E (d) S E (d) C E (d) R \x06 k kfk(yi, yj) + \x06 viV (d) \x06 k\x02 k\x02 fk\x02 (yi, x(d) ) logZ(x(d) )] \x06 k 2 k 22 \x06 k\x02 2 k\x02 22 (6) where the last two terms are due to the Gaussian prior (Chen and Rosenfeld, 1999) used to reduce overfitting. Quasi-Newton methods, such as LBFGS (Liu and Nocedal, 1989) can be used for maximizing the function. 3.3 Tree-based Reparameterization Since the proposed model may include loops, it is necessary to introduce an approximation to calculate mariginal probabilities. For this, we use Tree-based Reparameterization (TRP) (Wainwright et al., 2003) for approximate inference. TRP enumerates a set of spanning trees from the graph. Then, inference is performed by applying an exact inference algorithm such as Belief Propagation to each of the spanning trees, and updates of marginal probabilities are continued until they converge. 4 Experiments 4.1 Dataset Our data</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. The limited memory BFGS methods for large scale optimization. In Mathematical Programming 45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Lu</author>
<author>Lise Getoor</author>
</authors>
<title>Link-based classification using labeled and unlabeled data.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference On Machine Learning,</booktitle>
<location>Washington DC,</location>
<contexts>
<context position="3725" citStr="Lu and Getoor, 2003" startWordPosition="572" endWordPosition="575">rticles that describe other NEs belonging to the same class. It is expected that improved NE categorization is accomplished by capturing such dependencies. We structure anchor texts and dependencies between them into a graph, and train graph-based CRFs to obtain probabilistic models to estimate categories for NEs in Wikipedia. So far, several statistical models that can cap1 http://wikipedia.org/ 649 \x0cture dependencies between examples have been proposed. There are two types of classification methods that can capture dependencies: iterative classification methods (Neville and Jensen, 2000; Lu and Getoor, 2003b) and collective classification methods (Getoor et al., 2001; Taskar et al., 2002). In this paper, we use Conditional Random Fields (CRFs) (Lafferty et al., 2001) for NE categorization in Wikipedia. The rest of the paper is structured as follows. Section 2 describes the general framework of CRFs. Section 3 describes a graph-based CRFs for NE categorization in Wikipedia. In section 4, we show the experimental results. Section 5 describes related work. We conclude in section 6. 2 Conditional Random Fields Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models t</context>
<context position="24418" citStr="Lu and Getoor, 2003" startWordPosition="4280" endWordPosition="4283">eb. Tang et al. proposed Tree-structured Conditional Random Fields (TCRFs) (2006) that capture hierarchical structure of web documents. Zhu et al. proposed Hierarchical Conditional Random Fields (HCRFs) (2006) for product information extraction from Web documents. TCRFs and HCRFs are similar to our approach described in section 4 in that the model structure is induced by page structure. However, the model structures of these models are different from our model. There are statistical models that capture dependencies between examples. There are two types of classification approaches: iterative (Lu and Getoor, 2003b; Lu and Getoor, 2003a) or collective (Getoor et al., 2001; Taskar et al., 2002). Lu et al. (2003a; 2003b) proposed link-based classification method based on logistic regression. This model iterates local classification until label assignments converge. The results vary from the ordering strategy of local classification. In contrast to iterative classification methods, collective classification methods directly estimate most likely assignments. Getoor et al. proposed Probabilistic Relational Models (PRMs) (2001) which are built upon Bayesian Networks. Since Bayesian Networks are directed grap</context>
</contexts>
<marker>Lu, Getoor, 2003</marker>
<rawString>Qing Lu and Lise Getoor. 2003a. Link-based classification using labeled and unlabeled data. In Proceedings of the International Conference On Machine Learning, Washington DC, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Lu</author>
<author>Lise Getoor</author>
</authors>
<title>Link-based text classification.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="3725" citStr="Lu and Getoor, 2003" startWordPosition="572" endWordPosition="575">rticles that describe other NEs belonging to the same class. It is expected that improved NE categorization is accomplished by capturing such dependencies. We structure anchor texts and dependencies between them into a graph, and train graph-based CRFs to obtain probabilistic models to estimate categories for NEs in Wikipedia. So far, several statistical models that can cap1 http://wikipedia.org/ 649 \x0cture dependencies between examples have been proposed. There are two types of classification methods that can capture dependencies: iterative classification methods (Neville and Jensen, 2000; Lu and Getoor, 2003b) and collective classification methods (Getoor et al., 2001; Taskar et al., 2002). In this paper, we use Conditional Random Fields (CRFs) (Lafferty et al., 2001) for NE categorization in Wikipedia. The rest of the paper is structured as follows. Section 2 describes the general framework of CRFs. Section 3 describes a graph-based CRFs for NE categorization in Wikipedia. In section 4, we show the experimental results. Section 5 describes related work. We conclude in section 6. 2 Conditional Random Fields Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models t</context>
<context position="24418" citStr="Lu and Getoor, 2003" startWordPosition="4280" endWordPosition="4283">eb. Tang et al. proposed Tree-structured Conditional Random Fields (TCRFs) (2006) that capture hierarchical structure of web documents. Zhu et al. proposed Hierarchical Conditional Random Fields (HCRFs) (2006) for product information extraction from Web documents. TCRFs and HCRFs are similar to our approach described in section 4 in that the model structure is induced by page structure. However, the model structures of these models are different from our model. There are statistical models that capture dependencies between examples. There are two types of classification approaches: iterative (Lu and Getoor, 2003b; Lu and Getoor, 2003a) or collective (Getoor et al., 2001; Taskar et al., 2002). Lu et al. (2003a; 2003b) proposed link-based classification method based on logistic regression. This model iterates local classification until label assignments converge. The results vary from the ordering strategy of local classification. In contrast to iterative classification methods, collective classification methods directly estimate most likely assignments. Getoor et al. proposed Probabilistic Relational Models (PRMs) (2001) which are built upon Bayesian Networks. Since Bayesian Networks are directed grap</context>
</contexts>
<marker>Lu, Getoor, 2003</marker>
<rawString>Qing Lu and Lise Getoor. 2003b. Link-based text classification. In Proceedings of the International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Khashayar Rohanimanesh</author>
<author>Charles Sutton</author>
</authors>
<title>Dynamic conditional random fields for jointly labeling multiple sequences.</title>
<date>2003</date>
<booktitle>In NIPS Workshop on Syntax, Semantics, and Statistics,</booktitle>
<contexts>
<context position="5336" citStr="McCallum et al. (2003)" startWordPosition="853" endWordPosition="856">set. p(y|x) = 1 Z(x) \x02 cC (xc, yc) (1) where (xc, yc) is a potential function defined over cliques, and Z(x) = \x03 y \x04 cC (xc, yc) is the partition function. The potentials are factorized according to the set of features {fk}. (xc, yc) = exp \x05 \x06 k kfk(xc, yc) \x07 (2) where F = {f1, ..., fK} are feature functions on the cliques, = {1, ..., K R} are the model parameters. The parameters are estimated iterative scaling or quasi-Newton method from labeled data. The original paper (Lafferty et al., 2001) focused on linear-chain CRFs, and applied them to part-of-speech tagging problem. McCallum et al. (2003), Sutton et al (2004) proposed Dynamic Conditional Random Fields (DCRFs), the generalization of linear-chain CRFs, that have complex graph structure (include cycles). Since DCRFs model structure contains cycles, it is necessary to use approximate inference methods to calculate marginal probability. Tree-based Reparameterization (TRP) (Wainwright et al., 2003), a schedule for loopy belief propagation, is used for approximate inference in these papers. 3 Graph-based CRFs for NE Categorization in Wikipedia In this section we describe how to apply CRFs for NE categorization in Wikipedia. Each Wiki</context>
</contexts>
<marker>McCallum, Rohanimanesh, Sutton, 2003</marker>
<rawString>Andrew McCallum, Khashayar Rohanimanesh, and Charles Sutton. 2003. Dynamic conditional random fields for jointly labeling multiple sequences. In NIPS Workshop on Syntax, Semantics, and Statistics, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Neville</author>
<author>D Jensen</author>
</authors>
<title>Iterative classification in relational data.</title>
<date>2000</date>
<booktitle>In Proceedings of AAAI-2000 Workshop on Learning Statistical Models from Relational Data,</booktitle>
<pages>1320</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="3704" citStr="Neville and Jensen, 2000" startWordPosition="568" endWordPosition="571">ist tend to refer to the articles that describe other NEs belonging to the same class. It is expected that improved NE categorization is accomplished by capturing such dependencies. We structure anchor texts and dependencies between them into a graph, and train graph-based CRFs to obtain probabilistic models to estimate categories for NEs in Wikipedia. So far, several statistical models that can cap1 http://wikipedia.org/ 649 \x0cture dependencies between examples have been proposed. There are two types of classification methods that can capture dependencies: iterative classification methods (Neville and Jensen, 2000; Lu and Getoor, 2003b) and collective classification methods (Getoor et al., 2001; Taskar et al., 2002). In this paper, we use Conditional Random Fields (CRFs) (Lafferty et al., 2001) for NE categorization in Wikipedia. The rest of the paper is structured as follows. Section 2 describes the general framework of CRFs. Section 3 describes a graph-based CRFs for NE categorization in Wikipedia. In section 4, we show the experimental results. Section 5 describes related work. We conclude in section 6. 2 Conditional Random Fields Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirect</context>
</contexts>
<marker>Neville, Jensen, 2000</marker>
<rawString>J. Neville and D. Jensen. 2000. Iterative classification in relational data. In Proceedings of AAAI-2000 Workshop on Learning Statistical Models from Relational Data, pages 1320. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of The 20th International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="25442" citStr="Peng et al. (2004)" startWordPosition="4434" endWordPosition="4437"> directly estimate most likely assignments. Getoor et al. proposed Probabilistic Relational Models (PRMs) (2001) which are built upon Bayesian Networks. Since Bayesian Networks are directed graphical models, PRMs cannot model directly the cases where instantiated graph contains cycles. Taskar et al. proposed Relational Markov Networks (RMNs) (2002). RMNs are the special case of Conditional Markov Networks (or Conditional Random Fields) in which graph structure and parameter tying are determined by SQL-like form. As for the marginal probability to use as a confidence measure shown in Figure 4, Peng et al. (2004) has applied linear-chain CRFs to Chinese word segmentation. It is calculated by constrained forwardbackward algorithm (Culotta and McCallum, 2004), and confident segments are added to the dictionary in order to improve segmentation accuracy. 6 Conclusion In this paper, we proposed a method for categorizing NEs in Wikipedia. We defined three types of cliques that are constitute dependent anchor texts in construct CRFs graph structure, and introduced potential functions for them to reflect classification. The experimental results show that the effectiveness of capturing dependencies, and propos</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proceedings of The 20th International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting semantic role labeling, wordnet and wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, wordnet and wikipedia for coreference resolution. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
<author>Chikashi Nobata</author>
</authors>
<title>Extended named entity hierarchy.</title>
<date>2002</date>
<booktitle>In Proceedings of the LREC-2002.</booktitle>
<contexts>
<context position="13364" citStr="Sekine et al., 2002" startWordPosition="2368" endWordPosition="2371">parameterization (TRP) (Wainwright et al., 2003) for approximate inference. TRP enumerates a set of spanning trees from the graph. Then, inference is performed by applying an exact inference algorithm such as Belief Propagation to each of the spanning trees, and updates of marginal probabilities are continued until they converge. 4 Experiments 4.1 Dataset Our dataset is a random selection of 2300 articles from the Japanese version of Wikipedia as of October 2005. All anchor texts appearing under HTML &amp;lt;LI&gt; tags are hand-annotated with NE class label. We use the Extended Named Entity Hierarchy (Sekine et al., 2002) as the NE class labeling guideline, but reduce the number of classes to 13 from the original 200+ by ignoring fine-grained categories and nearby categories in order to avoid data sparseness. We eliminate examples that consist of less than two nodes in the SCR model. There are 16136 anchor texts with 14285 NEs. The number of Sibling, Cousin and Relative edges in the dataset are |ES |= 4925, |EC |= 13134 and |ER |= 746 respectively. 4.2 Experimental settings The aims of experiments are the two-fold. Firstly, we investigate the effect of each cliques. The several graphs are composed with the thr</context>
</contexts>
<marker>Sekine, Sudo, Nobata, 2002</marker>
<rawString>Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata. 2002. Extended named entity hierarchy. In Proceedings of the LREC-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Wikirelate! computing semantic relatedness using wikipedia.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI-06).</booktitle>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>Michael Strube and Simone Paolo Ponzetto. 2006. Wikirelate! computing semantic relatedness using wikipedia. In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI-06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Khashayar Rohanimanesh</author>
<author>Andrew McCallum</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="5357" citStr="Sutton et al (2004)" startWordPosition="857" endWordPosition="860">2 cC (xc, yc) (1) where (xc, yc) is a potential function defined over cliques, and Z(x) = \x03 y \x04 cC (xc, yc) is the partition function. The potentials are factorized according to the set of features {fk}. (xc, yc) = exp \x05 \x06 k kfk(xc, yc) \x07 (2) where F = {f1, ..., fK} are feature functions on the cliques, = {1, ..., K R} are the model parameters. The parameters are estimated iterative scaling or quasi-Newton method from labeled data. The original paper (Lafferty et al., 2001) focused on linear-chain CRFs, and applied them to part-of-speech tagging problem. McCallum et al. (2003), Sutton et al (2004) proposed Dynamic Conditional Random Fields (DCRFs), the generalization of linear-chain CRFs, that have complex graph structure (include cycles). Since DCRFs model structure contains cycles, it is necessary to use approximate inference methods to calculate marginal probability. Tree-based Reparameterization (TRP) (Wainwright et al., 2003), a schedule for loopy belief propagation, is used for approximate inference in these papers. 3 Graph-based CRFs for NE Categorization in Wikipedia In this section we describe how to apply CRFs for NE categorization in Wikipedia. Each Wikipedia article describ</context>
</contexts>
<marker>Sutton, Rohanimanesh, McCallum, 2004</marker>
<rawString>Charles Sutton, Khashayar Rohanimanesh, and Andrew McCallum. 2004. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. In Proceedings of the 21th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
</authors>
<title>GRMM: A graphical models toolkit.</title>
<date>2006</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="15887" citStr="Sutton, 2006" startWordPosition="2822" endWordPosition="2823">dels. When the graph are disconnected by reducing the edges, the classification is performed on each connected subgraph. We call it an example. We name the examples according the graph structure: loopy examples are subgraphs including at least one cycle; linear chain or tree examples are subgraphs including not a cycle but at least an edge; one node examples are subgraphs without edges. Table 1 shows the distribution of the examples of each model. Since SCR, SC, SR and CR model have loopy examples, TRP approximate inference is necessary. To perform training and testing with CRFs, we use GRMM (Sutton, 2006) with TRP. We set the Gaussian Prior variances for weights as 2 = 10 in all models. SC model C C C C S S S S C SCR model C C C C S S S S R R C SR model S S S S R R CR model C C C C R R C S model S S S S C model C C C C C R model R R I model Figure 3: An example of graphs constructed by combination of defined cliques. S, C, R in the model names mean that corresponding model has Sibling, Cousin, Relative cliques respectively. In each model, classification is performed on each connected subgraph. SVMs We introduce two models by SVMs (model I and model P). In model I, each anchor text is classifie</context>
</contexts>
<marker>Sutton, 2006</marker>
<rawString>Charles Sutton. 2006. GRMM: A graphical models toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Pieter Abbeel</author>
<author>Daphne Koller</author>
</authors>
<title>Discriminative probabilistic models for relational data.</title>
<date>2002</date>
<booktitle>In Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="3808" citStr="Taskar et al., 2002" startWordPosition="584" endWordPosition="587">mproved NE categorization is accomplished by capturing such dependencies. We structure anchor texts and dependencies between them into a graph, and train graph-based CRFs to obtain probabilistic models to estimate categories for NEs in Wikipedia. So far, several statistical models that can cap1 http://wikipedia.org/ 649 \x0cture dependencies between examples have been proposed. There are two types of classification methods that can capture dependencies: iterative classification methods (Neville and Jensen, 2000; Lu and Getoor, 2003b) and collective classification methods (Getoor et al., 2001; Taskar et al., 2002). In this paper, we use Conditional Random Fields (CRFs) (Lafferty et al., 2001) for NE categorization in Wikipedia. The rest of the paper is structured as follows. Section 2 describes the general framework of CRFs. Section 3 describes a graph-based CRFs for NE categorization in Wikipedia. In section 4, we show the experimental results. Section 5 describes related work. We conclude in section 6. 2 Conditional Random Fields Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models that give a conditional probability distribution p(y|x) in a form of exponential mod</context>
<context position="24499" citStr="Taskar et al., 2002" startWordPosition="4294" endWordPosition="4297">) that capture hierarchical structure of web documents. Zhu et al. proposed Hierarchical Conditional Random Fields (HCRFs) (2006) for product information extraction from Web documents. TCRFs and HCRFs are similar to our approach described in section 4 in that the model structure is induced by page structure. However, the model structures of these models are different from our model. There are statistical models that capture dependencies between examples. There are two types of classification approaches: iterative (Lu and Getoor, 2003b; Lu and Getoor, 2003a) or collective (Getoor et al., 2001; Taskar et al., 2002). Lu et al. (2003a; 2003b) proposed link-based classification method based on logistic regression. This model iterates local classification until label assignments converge. The results vary from the ordering strategy of local classification. In contrast to iterative classification methods, collective classification methods directly estimate most likely assignments. Getoor et al. proposed Probabilistic Relational Models (PRMs) (2001) which are built upon Bayesian Networks. Since Bayesian Networks are directed graphical models, PRMs cannot model directly the cases where instantiated graph conta</context>
</contexts>
<marker>Taskar, Abbeel, Koller, 2002</marker>
<rawString>Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002. Discriminative probabilistic models for relational data. In Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Wiley Interscience.</publisher>
<contexts>
<context position="14216" citStr="Vapnik, 1998" startWordPosition="2515" endWordPosition="2517"> two nodes in the SCR model. There are 16136 anchor texts with 14285 NEs. The number of Sibling, Cousin and Relative edges in the dataset are |ES |= 4925, |EC |= 13134 and |ER |= 746 respectively. 4.2 Experimental settings The aims of experiments are the two-fold. Firstly, we investigate the effect of each cliques. The several graphs are composed with the three sorts of edges. We also compare the graph-based models with a node-wise method just MaxEnt method not using any edge dependency. Secondly, we compare the proposed method by CRFs with a baseline method by Support Vector Machines (SVMs) (Vapnik, 1998). The experimental settings of CRFs and SVMs are as follows. CRFs In order to investigate which type of clique boosts classification performance, we perform experiments on several CRFs models that are constructed from combinations of defined cliques. Re652 \x0cSCR SC SR CR # of loopy examples 318 (36%) 324 (32%) 101 (1%) 42 (2%) # of linear chain or tree examples 555 (64%) 631 (62%) 2883 (27%) 1464 (54%) # of one node examples 0 (0%) 60 (6%) 7800 (72%) 1176 (44%) # of total examples 873 1015 10784 2682 average # of nodes per example 18.5 15.8 1.5 6.0 S C R I # of loopy examples 0 (0%) 0 (0%) 0</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir Vapnik. 1998. Statistical Learning Theory. Wiley Interscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Wainwright</author>
<author>Tommi Jaakkola</author>
<author>Alan Willsky</author>
</authors>
<title>Tree-based reparameterization framework for analysis of sum-product and related algorithms.</title>
<date>2003</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>45</volume>
<issue>9</issue>
<contexts>
<context position="5697" citStr="Wainwright et al., 2003" startWordPosition="903" endWordPosition="906"> are the model parameters. The parameters are estimated iterative scaling or quasi-Newton method from labeled data. The original paper (Lafferty et al., 2001) focused on linear-chain CRFs, and applied them to part-of-speech tagging problem. McCallum et al. (2003), Sutton et al (2004) proposed Dynamic Conditional Random Fields (DCRFs), the generalization of linear-chain CRFs, that have complex graph structure (include cycles). Since DCRFs model structure contains cycles, it is necessary to use approximate inference methods to calculate marginal probability. Tree-based Reparameterization (TRP) (Wainwright et al., 2003), a schedule for loopy belief propagation, is used for approximate inference in these papers. 3 Graph-based CRFs for NE Categorization in Wikipedia In this section we describe how to apply CRFs for NE categorization in Wikipedia. Each Wikipedia article describes a specific entity or concept by a heading word, a definition, and one or more categories. One possible approach is to classify each NE described in an article into an appropriate category by exploiting the definition of the article. This process can be done one by one without considering the relationship with other articles. On the oth</context>
<context position="12792" citStr="Wainwright et al., 2003" startWordPosition="2273" endWordPosition="2276"> N \x06 d=1 [ \x06 (vi,vj)E (d) S E (d) C E (d) R \x06 k kfk(yi, yj) + \x06 viV (d) \x06 k\x02 k\x02 fk\x02 (yi, x(d) ) logZ(x(d) )] \x06 k 2 k 22 \x06 k\x02 2 k\x02 22 (6) where the last two terms are due to the Gaussian prior (Chen and Rosenfeld, 1999) used to reduce overfitting. Quasi-Newton methods, such as LBFGS (Liu and Nocedal, 1989) can be used for maximizing the function. 3.3 Tree-based Reparameterization Since the proposed model may include loops, it is necessary to introduce an approximation to calculate mariginal probabilities. For this, we use Tree-based Reparameterization (TRP) (Wainwright et al., 2003) for approximate inference. TRP enumerates a set of spanning trees from the graph. Then, inference is performed by applying an exact inference algorithm such as Belief Propagation to each of the spanning trees, and updates of marginal probabilities are continued until they converge. 4 Experiments 4.1 Dataset Our dataset is a random selection of 2300 articles from the Japanese version of Wikipedia as of October 2005. All anchor texts appearing under HTML &amp;lt;LI&gt; tags are hand-annotated with NE class label. We use the Extended Named Entity Hierarchy (Sekine et al., 2002) as the NE class labeling gu</context>
</contexts>
<marker>Wainwright, Jaakkola, Willsky, 2003</marker>
<rawString>Martin Wainwright, Tommi Jaakkola, and Alan Willsky. 2003. Tree-based reparameterization framework for analysis of sum-product and related algorithms. IEEE Transactions on Information Theory, 45(9):11201146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Zaiqing Nie</author>
<author>Ji-Rong Wen</author>
<author>Bo Zhang</author>
<author>WeiYing Ma</author>
</authors>
<title>Simultaneous record detection and attribute labeling in web data extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</booktitle>
<marker>Zhu, Nie, Wen, Zhang, Ma, 2006</marker>
<rawString>Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, and WeiYing Ma. 2006. Simultaneous record detection and attribute labeling in web data extraction. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>