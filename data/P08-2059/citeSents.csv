etup to CITATION,,
We selected four binary NLP datasets for evaluation: 20 Newsgroups1 and Reuters CITATION (used by Tong and Koller) and sentiment classification CITATION and spam CITATION,,
we used a similar experimental setup to CITATION,,
We selected four binary NLP datasets for evaluation: 20 Newsgroups1 and Reuters CITATION (used by Tong and Koller) and sentiment classification CITATION and spam CITATION,,
5 Related Work Active learning has been widely used for NLP tasks such as part of speech tagging CITATION, parsing CITATION and word sense disambiguation CITATION,,
Many methods rely on entropy-based scores such as uncertainty sampling CITATION,,
Others use margin based methods, such as CITATION, who combined margin scores with corpus diversity, and CITATION, who considered SVM active learning 235 \x0c100 150 200 250 300 350 400 450 500 Labels 0.65 0.70 0.75 0.80 0.85 0.90 0.95 Test Accuracy 20 Newsgroups PA Random (82.53) CW Random (92.92) PA Margin (88.06) CW Margin (95.39) ACL (95.51) 100 150 200 250 300 350 400 450 500 Labels 0.75 0.80 0.85 0.90 Test Accuracy All PA Random (81.30) CW Random (86.67) PA Margin (83.99) CW Ma,,
CW is an online algorithm inspired by the Passive Aggressive (PA) update CITATION which ensures a positive margin while minimizing parameter change,,
Confidence-weighted (CW) linear classification CITATION, a new online algorithm, maintains a probabilistic measure of parameter confidence leading to a measure of prediction confidence, potentially useful for active learning,,
Furthermore, margin methods can outperform probabilistic methods; CW beats maximum entropy on many NLP tasks CITATION,,
A theoretical analysis of margin based methods selected labels that maximize the reduction of the version space, the hypothesis set consistent with the training data CITATION,,
5 Related Work Active learning has been widely used for NLP tasks such as part of speech tagging CITATION, parsing CITATION and word sense disambiguation CITATION,,
Many methods rely on entropy-based scores such as uncertainty sampling CITATION,,
Others use margin based methods, such as CITATION, who combined margin scores with corpus diversity, and CITATION, who considered SVM active learning 235 \x0c100 150 200 250 300 350 400 450 500 Labels 0.65 0.70 0.75 0.80 0.85 0.90 0.95 Test Accuracy 20 Newsgroups PA Random (82.53) CW Random (92.92) PA Margin (88.06) CW Margin (95.39) ACL (95.51) 100 150 200 250 300 350 400 450 500 Labels 0.75 0.80 0.85 0.90 Test Accuracy All PA Random (81.30) CW Random (86.67) PA Margin (83.99) CW Margin (88.61) ACL (88.79) 0.2 0.4 0.6 0.8 1.0 1.2 1.4 ACL Labels 0.2 0.4 0.6 0.8 1.0 1.2 1.4 CW Margin Labels Reuters 20 Newsgroups Sentiment Spam Figure 1:,,
Active learning for probabilistic methods often uses uncertainty sampling: label the example with the lowest probability prediction (the most uncertain) CITATION,,
The equivalent technique for margin learning associates the margin with prediction certainty: label the example with the lowest margin CITATION,,
5 Related Work Active learning has been widely used for NLP tasks such as part of speech tagging CITATION, parsing CITATION and word sense disambiguation CITATION,,
Many methods rely on entropy-based scores such as uncertainty sampling CITATION,,
Others use margin based methods, such as CITATION, who combined margin scores with corpus diversity, and CITATION, who considered SVM active learning 235 \x0c100 150 200 250 300 350 400 450 500 Labels 0.65 0.70 0.75 0.80 0.85 0.90 0.95 Test Accuracy 20 Newsgroups PA Random (82.53) CW Random (92.92) PA Margin (88.06) CW Margin (95.39) ACL (95.51) 100 150 200 250 300 350 400 450 500 Labels 0.75 0.80 0.85 0.90 Test Accuracy All PA Random (81.30) CW Random (86.67) PA Margin (83.99) CW Margin (88.61) ACL (88.79) 0.2 0.4 0.6 0.8 1.0 1.2 1.4 ACL Labels 0.2 0.4 0.6 0.8 1.0 1.2 1.4 CW ,,
4 Evaluation To evaluate our active learning methods we used a similar experimental setup to CITATION,,
We selected four binary NLP datasets for evaluation: 20 Newsgroups1 and Reuters CITATION (used by Tong and Koller) and sentiment classification CITATION and spam CITATION,,
5 Related Work Active learning has been widely used for NLP tasks such as part of speech tagging CITATION, parsing CITATION and word sense disambiguation CITATION,,
Many methods rely on entropy-based scores such as uncertainty sampling CITATION,,
Others use margin based methods, such as CITATION, who combined margin scores with corpus diversity, and CITATION, who considered SVM active learning 235 \x0c100 150 200 250 300 350 400 450 500 Labels 0.65 0.70 0.75 0.80 0.85 0.90 0.95 Test Accuracy 20 Newsgroups PA Random (82.53) CW Random (92.92) PA Margin (88.06) CW Margin (95.39) ACL (95.51) 100 150 200 250 300 350 400 450 500 Labels 0.75 0.80 0.85 0.9,,
5 Related Work Active learning has been widely used for NLP tasks such as part of speech tagging CITATION, parsing CITATION and word sense disambiguation CITATION,,
Many methods rely on entropy-based scores such as uncertainty sampling CITATION,,
Others use margin based methods, such as CITATION, who combined margin scores with corpus diversity, and CITATION, who considered SVM active learning 235 \x0c100 150 200 250 300 350 400 450 500 Labels 0.65 0.70 0.75 0.80 0.85 0.90 0.95 Test Accuracy 20 Newsgroups PA Random (82.53) CW Random (92.92) PA Margin (88.06) CW Margin (95.39) ACL (95.51) 100 150 200 250 300 350 400 450 500 Labels 0.75 0.80 0.85 0.90 Test Accuracy All PA Random (81.30) CW Random (86.67) PA Margin (83.99) CW Margin (88.61) ACL (88.79) 0.2 0.4 0.6 0.8 1.0 1.2 1.4 ACL Labels 0.2 0.4 0.6 0.8 1.0 1.2 1.4 CW Margin Labels Reuters 20 Newsgroups Sentiment Spam Figure 1: Results averaged over 20 Newsgroups (left) and all datasets (center) ,,
5 Related Work Active learning has been widely used for NLP tasks such as part of speech tagging CITATION, parsing CITATION and word sense disambiguation CITATION,,
Many methods rely on entropy-based scores such as uncertainty sampling CITATION,,
Others use margin based methods, such as CITATION, who combined margin scores with corpus diversity, and CITATION, who considered SVM active learning 235 \x0c100 150 200 250 300 350 400 450 500 Labels 0.65 0.70 0.75 0.80 0.85 0.90 0.95 Test Accuracy 20 Newsgroups PA Random (82.53) CW Random (92.92) PA Margin (88.06) CW Margin (95.39) ACL (95.51) 100 150 200 250 300 350 400 450 500 Labels 0.75 0.80 0.85 0.90 Test Accuracy All PA Random,,
Active learning for probabilistic methods often uses uncertainty sampling: label the example with the lowest probability prediction (the most uncertain) CITATION,,
The equivalent technique for margin learning associates the margin with prediction certainty: label the example with the lowest margin CITATION,,
CITATION motivate this approach by considering the half-space representation of the hypothesis space for learning,,
4 Evaluation To evaluate our active learning methods we used a similar experimental setup to CITATION,,
We selected four binary NLP datasets for evaluation: 20 Newsgroups1 and Reuters CITATION (used by Tong and Koller) and sentiment classification CITATION and spam CITATION,,
Furthermore, margin methods can outperform probabilistic methods; CW beats maximum entropy on many NLP tasks CITATION,,
A theoretical analysis of margin based methods selected labels that maximize the reduction of the version space, the hypothesis set consistent with the training data CITATION,,
