<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.3908745">
b&amp;quot;Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 552559,
Prague, Czech Republic, June 2007. c
</bodyText>
<sectionHeader confidence="0.335467" genericHeader="abstract">
2007 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.765594">
Towards an Iterative Reinforcement Approach for Simultaneous
Document Summarization and Keyword Extraction
</title>
<author confidence="0.96691">
Xiaojun Wan Jianwu Yang Jianguo Xiao
</author>
<affiliation confidence="0.835561">
Institute of Computer Science and Technology
Peking University, Beijing 100871, China
</affiliation>
<email confidence="0.996435">
{wanxiaojun,yangjianwu,xiaojianguo}@icst.pku.edu.cn
</email>
<sectionHeader confidence="0.990704" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.99967365">
Though both document summarization and
keyword extraction aim to extract concise
representations from documents, these two
tasks have usually been investigated inde-
pendently. This paper proposes a novel it-
erative reinforcement approach to simulta-
neously extracting summary and keywords
from single document under the assump-
tion that the summary and keywords of a
document can be mutually boosted. The
approach can naturally make full use of the
reinforcement between sentences and key-
words by fusing three kinds of relation-
ships between sentences and words, either
homogeneous or heterogeneous. Experi-
mental results show the effectiveness of the
proposed approach for both tasks. The cor-
pus-based approach is validated to work
almost as well as the knowledge-based ap-
proach for computing word semantics.
</bodyText>
<sectionHeader confidence="0.998257" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999007019230769">
Text summarization is the process of creating a
compressed version of a given document that de-
livers the main topic of the document. Keyword
extraction is the process of extracting a few salient
words (or phrases) from a given text and using the
words to represent the text. The two tasks are simi-
lar in essence because they both aim to extract
concise representations for documents. Automatic
text summarization and keyword extraction have
drawn much attention for a long time because they
both are very important for many text applications,
including document retrieval, document clustering,
etc. For example, keywords of a document can be
used for document indexing and thus benefit to
improve the performance of document retrieval,
and document summary can help to facilitate users
to browse the search results and improve users
search experience.
Text summaries and keywords can be either
query-relevant or generic. Generic summary and
keyword should reflect the main topics of the
document without any additional clues and prior
knowledge. In this paper, we focus on generic
document summarization and keyword extraction
for single documents.
Document summarization and keyword extrac-
tion have been widely explored in the natural lan-
guage processing and information retrieval com-
munities. A series of workshops and conferences
on automatic text summarization (e.g. SUMMAC,
DUC and NTCIR) have advanced the technology
and produced a couple of experimental online sys-
tems. In recent years, graph-based ranking algo-
rithms have been successfully used for document
summarization (Mihalcea and Tarau, 2004, 2005;
ErKan and Radev, 2004) and keyword extraction
(Mihalcea and Tarau, 2004). Such algorithms make
use of voting or recommendations between
sentences (or words) to extract sentences (or key-
words). Though the two tasks essentially share
much in common, most algorithms have been de-
veloped particularly for either document summari-
zation or keyword extraction.
Zha (2002) proposes a method for simultaneous
keyphrase extraction and text summarization by
using only the heterogeneous sentence-to-word
relationships. Inspired by this, we aim to take into
account all the three kinds of relationships among
sentences and words (i.e. the homogeneous rela-
tionships between words, the homogeneous rela-
tionships between sentences, and the heterogene-
ous relationships between words and sentences) in
</bodyText>
<page confidence="0.993382">
552
</page>
<bodyText confidence="0.998056772727273">
\x0ca unified framework for both document summari-
zation and keyword extraction. The importance of
a sentence (word) is determined by both the impor-
tance of related sentences (words) and the impor-
tance of related words (sentences). The proposed
approach can be considered as a generalized form
of previous graph-based ranking algorithms and
Zhas work (Zha, 2002).
In this study, we propose an iterative reinforce-
ment approach to realize the above idea. The pro-
posed approach is evaluated on the DUC2002
dataset and the results demonstrate its effectiveness
for both document summarization and keyword
extraction. Both knowledge-based approach and
corpus-based approach have been investigated to
compute word semantics and they both perform
very well.
The rest of this paper is organized as follows:
Section 2 introduces related works. The details of
the proposed approach are described in Section 3.
Section 4 presents and discusses the evaluation
results. Lastly we conclude our paper in Section 5.
</bodyText>
<sectionHeader confidence="0.998289" genericHeader="method">
2 Related Works
</sectionHeader>
<subsectionHeader confidence="0.866102">
2.1 Document Summarization
</subsectionHeader>
<bodyText confidence="0.998370029411765">
Generally speaking, single document summariza-
tion methods can be either extraction-based or ab-
straction-based and we focus on extraction-based
methods in this study.
Extraction-based methods usually assign a sali-
ency score to each sentence and then rank the sen-
tences in the document. The scores are usually
computed based on a combination of statistical and
linguistic features, including term frequency, sen-
tence position, cue words, stigma words, topic sig-
nature (Hovy and Lin, 1997; Lin and Hovy, 2000),
etc. Machine learning methods have also been em-
ployed to extract sentences, including unsupervised
methods (Nomoto and Matsumoto, 2001) and su-
pervised methods (Kupiec et al., 1995; Conroy and
OLeary, 2001; Amini and Gallinari, 2002; Shen et
al., 2007). Other methods include maximal mar-
ginal relevance (MMR) (Carbonell and Goldstein,
1998), latent semantic analysis (LSA) (Gong and
Liu, 2001). In Zha (2002), the mutual reinforce-
ment principle is employed to iteratively extract
key phrases and sentences from a document.
Most recently, graph-based ranking methods, in-
cluding TextRank ((Mihalcea and Tarau, 2004,
2005) and LexPageRank (ErKan and Radev, 2004)
have been proposed for document summarization.
Similar to Kleinbergs HITS algorithm (Kleinberg,
1999) or Googles PageRank (Brin and Page,
1998), these methods first build a graph based on
the similarity between sentences in a document and
then the importance of a sentence is determined by
taking into account global information on the
graph recursively, rather than relying only on local
sentence-specific information.
</bodyText>
<subsectionHeader confidence="0.998362">
2.2 Keyword Extraction
</subsectionHeader>
<bodyText confidence="0.99932376">
Keyword (or keyphrase) extraction usually in-
volves assigning a saliency score to each candidate
keyword by considering various features. Krulwich
and Burkey (1996) use heuristics to extract key-
phrases from a document. The heuristics are based
on syntactic clues, such as the use of italics, the
presence of phrases in section headers, and the use
of acronyms. Munoz (1996) uses an unsupervised
learning algorithm to discover two-word key-
phrases. The algorithm is based on Adaptive Reso-
nance Theory (ART) neural networks. Steier and
Belew (1993) use the mutual information statistics
to discover two-word keyphrases.
Supervised machine learning algorithms have
been proposed to classify a candidate phrase into
either keyphrase or not. GenEx (Turney, 2000) and
Kea (Frank et al., 1999; Witten et al., 1999) are
two typical systems, and the most important fea-
tures for classifying a candidate phrase are the fre-
quency and location of the phrase in the document.
More linguistic knowledge (such as syntactic fea-
tures) has been explored by Hulth (2003). More
recently, Mihalcea and Tarau (2004) propose the
TextRank model to rank keywords based on the
co-occurrence links between words.
</bodyText>
<sectionHeader confidence="0.990635" genericHeader="method">
3 Iterative Reinforcement Approach
</sectionHeader>
<subsectionHeader confidence="0.960499">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.986425">
The proposed approach is intuitively based on the
following assumptions:
</bodyText>
<listItem confidence="0.8082954">
Assumption 1: A sentence should be salient if it
is heavily linked with other salient sentences, and a
word should be salient if it is heavily linked with
other salient words.
Assumption 2: A sentence should be salient if it
</listItem>
<bodyText confidence="0.93177675">
contains many salient words, and a word should be
salient if it appears in many salient sentences.
The first assumption is similar to PageRank
which makes use of mutual recommendations
</bodyText>
<page confidence="0.996756">
553
</page>
<bodyText confidence="0.9988997">
\x0cbetween homogeneous objects to rank objects. The
second assumption is similar to HITS if words and
sentences are considered as authorities and hubs
respectively. In other words, the proposed ap-
proach aims to fuse the ideas of PageRank and
HITS in a unified framework.
In more detail, given the heterogeneous data
points of sentences and words, the following three
kinds of relationships are fused in the proposed
approach:
SS-Relationship: It reflects the homogeneous
relationships between sentences, usually computed
by their content similarity.
WW-Relationship: It reflects the homogeneous
relationships between words, usually computed by
knowledge-based approach or corpus-based ap-
proach.
SW-Relationship: It reflects the heterogeneous
relationships between sentences and words, usually
computed as the relative importance of a word in a
</bodyText>
<figureCaption confidence="0.843871333333333">
sentence.
Figure 1 gives an illustration of the relationships.
Figure 1. Illustration of the Relationships
</figureCaption>
<bodyText confidence="0.993930444444444">
The proposed approach first builds three graphs
to reflect the above relationships respectively, and
then iteratively computes the saliency scores of the
sentences and words based on the graphs. Finally,
the algorithm converges and each sentence or word
gets its saliency score. The sentences with high
saliency scores are chosen into the summary, and
the words with high saliency scores are combined
to produce the keywords.
</bodyText>
<subsectionHeader confidence="0.999715">
3.2 Graph Building
</subsectionHeader>
<subsubsectionHeader confidence="0.772972">
3.2.1 Sentence-to-Sentence Graph ( SS-Graph)
</subsubsectionHeader>
<bodyText confidence="0.998921785714286">
Given the sentence collection S={si  |1IiIm} of a
document, if each sentence is considered as a node,
the sentence collection can be modeled as an undi-
rected graph by generating an edge between two
sentences if their content similarity exceeds 0, i.e.
an undirected link between si and sj (iKj) is con-
structed and the associated weight is their content
similarity. Thus, we construct an undirected graph
GSS to reflect the homogeneous relationship be-
tween sentences. The content similarity between
two sentences is computed with the cosine measure.
We use an adjacency matrix U to describe GSS with
each entry corresponding to the weight of a link in
the graph. U= [Uij]mm is defined as follows:
</bodyText>
<equation confidence="0.981088652173913">
otherwise
,
j
, if i
s
s
s
s
U j
i
j
i
ij
0
r
r
r
r
(1)
where i
s and j
s
r
</equation>
<bodyText confidence="0.9892575">
are the corresponding term vec-
tors of sentences si and sj respectively. The weight
associated with term t is calculated with tft.isft,
where tft is the frequency of term t in the sentence
and isft is the inverse sentence frequency of term t,
i.e. 1+log(N/nt), where N is the total number of
sentences and nt is the number of sentences con-
taining term t in a background corpus. Note that
other measures (e.g. Jaccard, Dice, Overlap, etc.)
can also be explored to compute the content simi-
larity between sentences, and we simply choose the
cosine measure in this study.
Then U is normalized to U
~
as follows to make
the sum of each row equal to 1:
</bodyText>
<equation confidence="0.97817505">
erwise
, oth
U
, if
U
U
U
m
j
ij
m
j
ij
ij
ij
0
0
~
1
1 (2)
</equation>
<subsubsectionHeader confidence="0.719025">
3.2.2 Word-to-Word Graph ( WW-Graph)
</subsubsectionHeader>
<bodyText confidence="0.971561545454545">
Given the word collection T={tj|1IjIn } of a docu-
ment1
, the semantic similarity between any two
words ti and tj can be computed using approaches
that are either knowledge-based or corpus-based
(Mihalcea et al., 2006).
Knowledge-based measures of word semantic
similarity try to quantify the degree to which two
words are semantically related using information
drawn from semantic networks. WordNet (Fell-
baum, 1998) is a lexical database where each
</bodyText>
<page confidence="0.888975">
1
</page>
<bodyText confidence="0.9728925">
The stopwords defined in the Smart system have been re-
moved from the collection.
</bodyText>
<figure confidence="0.9079008">
sentence
word
SS
WW
SW
</figure>
<page confidence="0.998806">
554
</page>
<bodyText confidence="0.999205459459459">
\x0cunique meaning of a word is represented by a
synonym set or synset. Each synset has a gloss that
defines the concept that it represents. Synsets are
connected to each other through explicit semantic
relations that are defined in WordNet. Many ap-
proaches have been proposed to measure semantic
relatedness based on WordNet. The measures vary
from simple edge-counting to attempt to factor in
peculiarities of the network structure by consider-
ing link direction, relative path, and density, such
as vector, lesk, hso, lch, wup, path, res, lin and jcn
(Pedersen et al., 2004). For example, cat and
dog has higher semantic similarity than cat
and computer. In this study, we implement the
vector measure to efficiently evaluate the similari-
ties of a large number of word pairs. The vector
measure (Patwardhan, 2003) creates a co
occurrence matrix from a corpus made up of the
WordNet glosses. Each content word used in a
WordNet gloss has an associated context vector.
Each gloss is represented by a gloss vector that is
the average of all the context vectors of the words
found in the gloss. Relatedness between concepts
is measured by finding the cosine between a pair of
gloss vectors.
Corpus-based measures of word semantic simi-
larity try to identify the degree of similarity be-
tween words using information exclusively derived
from large corpora. Such measures as mutual in-
formation (Turney 2001), latent semantic analysis
(Landauer et al., 1998), log-likelihood ratio (Dun-
ning, 1993) have been proposed to evaluate word
semantic similarity based on the co-occurrence
information on a large corpus. In this study, we
simply choose the mutual information to compute
the semantic similarity between word ti and tj as
follows:
</bodyText>
<equation confidence="0.996587961538462">
)
(
)
(
)
(
log
)
(
j
i
j
i
j
i
t
p
t
p
,t
t
p
N
,t
t
sim (3)
</equation>
<bodyText confidence="0.98660784">
which indicates the degree of statistical depend-
ence between ti and tj. Here, N is the total number
of words in the corpus and p(ti) and p(tj) are re-
spectively the probabilities of the occurrences of ti
and tj, i.e. count(ti)/N and count(tj)/N, where
count(ti) and count(tj) are the frequencies of ti and tj.
p(ti, tj) is the probability of the co-occurrence of ti
and tj within a window with a predefined size k, i.e.
count(ti, tj)/N, where count(ti, tj) is the number of
the times ti and tj co-occur within the window.
Similar to the SS-Graph, we can build an undi-
rected graph GWW to reflect the homogeneous rela-
tionship between words, in which each node corre-
sponds to a word and the weight associated with
the edge between any different word ti and tj is
computed by either the WordNet-based vector
measure or the corpus-based mutual information
measure. We use an adjacency matrix V to de-
scribe GWW with each entry corresponding to the
weight of a link in the graph. V= [Vij]nn, where Vij
=sim(ti, tj) if iKj and Vij=0 if i=j.
Then V is similarly normalized to V
~
to make
the sum of each row equal to 1.
</bodyText>
<subsubsectionHeader confidence="0.995161">
3.2.3 Sentence-to-Word Graph ( SW-Graph)
</subsubsectionHeader>
<bodyText confidence="0.960917375">
Given the sentence collection S={si  |1IiIm} and
the word collection T={tj|1IjIn } of a document,
we can build a weighted bipartite graph GSW from S
and T in the following way: if word tj appears in
sentence si, we then create an edge between si and
tj. A nonnegative weight aff(si,tj) is specified on the
edge, which is proportional to the importance of
word tj in sentence si, computed as follows:
</bodyText>
<equation confidence="0.999802631578947">
i
j
j
s
t
t
t
t
t
j
i
isf
tf
isf
tf
,t
s
aff )
( (4)
</equation>
<bodyText confidence="0.9244276">
where t represents a unique term in si and tft, isft
are respectively the term frequency in the sentence
and the inverse sentence frequency.
We use an adjacency (affinity) matrix
W=[Wij]mn to describe GSW with each entry Wij
corresponding to aff(si,tj). Similarly, W is normal-
ized to W
~
to make the sum of each row equal to 1.
In addition, we normalize the transpose of W, i.e.
</bodyText>
<sectionHeader confidence="0.520441" genericHeader="method">
WT
</sectionHeader>
<bodyText confidence="0.874859">
, to W to make the sum of each row in WT
equal to 1.
</bodyText>
<subsectionHeader confidence="0.98904">
3.3 Reinforcement Algorithm
</subsectionHeader>
<bodyText confidence="0.9934626">
We use two column vectors u=[u(si)]m1 and v
=[v(tj)]n1 to denote the saliency scores of the sen-
tences and words in the specified document. The
assumptions introduced in Section 3.1 can be ren-
dered as follows:
</bodyText>
<equation confidence="0.992421375">
j j
ji
i s
u
U
s
u )
(
~
)
(
(5)
i i
ij
j t
v
V
t
v )
(
~
)
(
(6)
j j
ji
i t
v
W
s
u )
(
)
(
(7)
555
\x0ci i
ij
j s
u
W
t
v )
(
~
)
(
(8)
</equation>
<bodyText confidence="0.99888">
After fusing the above equations, we can obtain
the following iterative forms:
</bodyText>
<equation confidence="0.9422976">
n
j
j
ji
m
j
j
ji
i t
v
W
)
s
u
U
*
s
u
1
1
)
(
)
(
~
)
( (9)
m
i
i
ij
n
i
i
ij
j s
u
W
)
t
v
V
*
t
v
1
1
)
(
~
)
(
~
)
( (10)
And the matrix form is:
v
W
u
U
u T
T
)
*
~ (11)
u
W
v
V
v T
T
)
*
~
~ (12)
</equation>
<bodyText confidence="0.946822111111111">
where * and ) specify the relative contributions to
the final saliency scores from the homogeneous
nodes and the heterogeneous nodes and we have
*+)=1. In order to guarantee the convergence of
the iterative form, u and v are normalized after
each iteration.
For numerical computation of the saliency
scores, the initial scores of all sentences and words
are set to 1 and the following two steps are alter-
</bodyText>
<figure confidence="0.64669542">
nated until convergence,
1. Compute and normalize the scores of sen-
tences:
)
(n-
T
)
(n-
T
(n)
)
* 1
1
~
v
W
u
U
u ,
1
(n)
(n)
(n)
/ u
u
u
2. Compute and normalize the scores of words:
)
(n-
T
)
(n-
T
(n)
)
* 1
1 ~
~
u
W
v
V
v ,
1
(n)
(n)
(n)
/ v
v
v
</figure>
<bodyText confidence="0.855389555555556">
where u(n)
and v(n)
denote the vectors computed at
the n-th iteration.
Usually the convergence of the iteration algo-
rithm is achieved when the difference between the
scores computed at two successive iterations for
any sentences and words falls below a given
threshold (0.0001 in this study).
</bodyText>
<sectionHeader confidence="0.926978" genericHeader="method">
4 Empirical Evaluation
</sectionHeader>
<subsectionHeader confidence="0.998287">
4.1 Summarization Evaluation
</subsectionHeader>
<subsubsectionHeader confidence="0.789305">
4.1.1 Evaluation Setup
</subsubsectionHeader>
<bodyText confidence="0.998417846153846">
We used task 1 of DUC2002 (DUC, 2002) for
evaluation. The task aimed to evaluate generic
summaries with a length of approximately 100
words or less. DUC2002 provided 567 English
news articles collected from TREC-9 for single-
document summarization task. The sentences in
each article have been separated and the sentence
information was stored into files.
In the experiments, the background corpus for
using the mutual information measure to compute
word semantics simply consisted of all the docu-
ments from DUC2001 to DUC2005, which could
be easily expanded by adding more documents.
The stopwords were removed and the remaining
words were converted to the basic forms based on
WordNet. Then the semantic similarity values be-
tween the words were computed.
We used the ROUGE (Lin and Hovy, 2003)
toolkit (i.e.ROUGEeval-1.4.2 in this study) for
evaluation, which has been widely adopted by
DUC for automatic summarization evaluation. It
measured summary quality by counting overlap-
ping units such as the n-gram, word sequences and
word pairs between the candidate summary and the
reference summary. ROUGE toolkit reported sepa-
rate scores for 1, 2, 3 and 4-gram, and also for
longest common subsequence co-occurrences.
Among these different scores, unigram-based
ROUGE score (ROUGE-1) has been shown to
agree with human judgment most (Lin and Hovy,
2003). We showed three of the ROUGE metrics in
the experimental results: ROUGE-1 (unigram-
based), ROUGE-2 (bigram-based), and ROUGE-
W (based on weighted longest common subse-
quence, weight=1.2).
In order to truncate summaries longer than the
length limit, we used the -l option 2
in the
ROUGE toolkit.
</bodyText>
<subsubsectionHeader confidence="0.980295">
4.1.2 Evaluation Results
</subsubsectionHeader>
<bodyText confidence="0.99889425">
For simplicity, the parameters in the proposed ap-
proach are simply set to *=)=0.5, which means
that the contributions from sentences and words
are equally important. We adopt the WordNet-
based vector measure (WN) and the corpus-based
mutual information measure (MI) for computing
the semantic similarity between words. When us-
ing the mutual information measure, we heuristi-
cally set the window size k to 2, 5 and 10, respec-
tively.
The proposed approaches with different word
similarity measures (WN and MI) are compared
</bodyText>
<page confidence="0.965617">
2
</page>
<bodyText confidence="0.988663333333333">
The -l option is very important for fair comparison. Some
previous works not adopting this option are likely to overes-
timate the ROUGE scores.
</bodyText>
<page confidence="0.992308">
556
</page>
<bodyText confidence="0.9943199375">
\x0cwith two solid baselines: SentenceRank and Mutu-
alRank. SentenceRank is proposed in Mihalcea and
Tarau (2004) to make use of only the sentence-to-
sentence relationships to rank sentences, which
outperforms most popular summarization methods.
MutualRank is proposed in Zha (2002) to make use
of only the sentence-to-word relationships to rank
sentences and words. For all the summarization
methods, after the sentences are ranked by their
saliency scores, we can apply a variant form of the
MMR algorithm to remove redundancy and choose
both the salient and novel sentences to the sum-
mary. Table 1 gives the comparison results of the
methods before removing redundancy and Table 2
gives the comparison results of the methods after
removing redundancy.
</bodyText>
<figure confidence="0.98921419047619">
System ROUGE-1 ROUGE-2 ROUGE-W
Our Approach
(WN)
0.47100*#
0.20424*#
0.16336#
Our Approach
(MI:k=2)
0.46711#
0.20195#
0.16257#
Our Approach
(MI:k=5)
0.46803#
0.20259#
0.16310#
Our Approach
(MI:k=10)
0.46823#
0.20301#
0.16294#
</figure>
<table confidence="0.9938575">
SentenceRank 0.45591 0.19201 0.15789
MutualRank 0.43743 0.17986 0.15333
</table>
<tableCaption confidence="0.993583">
Table 1. Summarization Performance before Re-
</tableCaption>
<table confidence="0.5604665">
moving Redundancy (w/o MMR)
System ROUGE-1 ROUGE-2 ROUGE-W
</table>
<figure confidence="0.98513975">
Our Approach
(WN)
0.47329*#
0.20249#
0.16352#
Our Approach
(MI:k=2)
0.47281#
0.20281#
0.16373#
Our Approach
(MI:k=5)
0.47282#
0.20249#
0.16343#
Our Approach
(MI:k=10)
0.47223#
0.20225#
0.16308#
</figure>
<table confidence="0.9932145">
SentenceRank 0.46261 0.19457 0.16018
MutualRank 0.43805 0.17253 0.15221
</table>
<tableCaption confidence="0.9202575">
Table 2. Summarization Performance after Remov-
ing Redundancy (w/ MMR)
</tableCaption>
<page confidence="0.409306">
(*
</page>
<bodyText confidence="0.987102303030303">
indicates that the improvement over SentenceRank is sig-
nificant and #
indicates that the improvement over Mutual-
Rank is significant, both by comparing the 95% confidence
intervals provided by the ROUGE package.)
Seen from Tables 1 and 2, the proposed ap-
proaches always outperform the two baselines over
all three metrics with different word semantic
measures. Moreover, no matter whether the MMR
algorithm is applied or not, almost all performance
improvements over MutualRank are significant
and the ROUGE-1 performance improvements
over SentenceRank are significant when using
WordNet-based measure (WN). Word semantics
can be naturally incorporated into the computation
process, which addresses the problem that Sen-
tenceRank cannot take into account word seman-
tics, and thus improves the summarization per-
formance. We also observe that the corpus-based
measure (MI) works almost as well as the knowl-
edge-based measure (WN) for computing word
semantic similarity.
In order to better understand the relative contri-
butions from the sentence nodes and the word
nodes, the parameter * is varied from 0 to 1. The
larger * is, the more contribution is given from the
sentences through the SS-Graph, while the less
contribution is given from the words through the
SW-Graph. Figures 2-4 show the curves over three
ROUGE scores with respect to *. Without loss of
generality, we use the case of k=5 for the MI
measure as an illustration. The curves are similar
to Figures 2-4 when k=2 and k=10.
</bodyText>
<figure confidence="0.987741">
0.435
0.44
0.445
0.45
0.455
0.46
0.465
0.47
0.475
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
*
ROUGE-1
MI(w/o MMR) MI(w/ MMR)
WN(w/o MMR) WN(w/ MMR)
</figure>
<figureCaption confidence="0.990792">
Figure 2. ROUGE-1 vs. *
</figureCaption>
<figure confidence="0.986569071428571">
0.17
0.175
0.18
0.185
0.19
0.195
0.2
0.205
0.21
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
*
ROUGE-2
MI(w/o MMR) MI(w/ MMR)
WN(w/o MMR) WN(w/ MMR)
</figure>
<figureCaption confidence="0.997317">
Figure 3. ROUGE-2 vs. *
</figureCaption>
<figure confidence="0.9582315">
557
\x0c0.151
0.153
0.155
0.157
0.159
0.161
0.163
0.165
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
*
ROUGE-W
MI(w/o MMR) MI(w/ MMR)
WN(w/o MMR) WN(w/ MMR)
</figure>
<figureCaption confidence="0.999901">
Figure 4. ROUGE-W vs. *
</figureCaption>
<bodyText confidence="0.998570125">
Seen from Figures 2-4, no matter whether the
MMR algorithm is applied or not (i.e. w/o MMR
or w/ MMR), the ROUGE scores based on either
word semantic measure (MI or WN) achieves the
peak when * is set between 0.4 and 0.6. The per-
formance values decrease sharply when * is very
large (near to 1) or very small (near to 0). The
curves demonstrate that both the contribution from
the sentences and the contribution from the words
are important for ranking sentences; moreover, the
contributions are almost equally important. Loss of
either contribution will much deteriorate the final
performance.
Similar results and observations have been ob-
tained on task 1 of DUC2001 in our study and the
details are omitted due to page limit.
</bodyText>
<subsectionHeader confidence="0.970589">
4.2 Keyword Evaluation
</subsectionHeader>
<subsubsectionHeader confidence="0.830509">
4.1.1 Evaluation Setup
</subsubsectionHeader>
<bodyText confidence="0.999398846153846">
In this study we performed a preliminary evalua-
tion of keyword extraction. The evaluation was
conducted on the single word level instead of the
multi-word phrase (n-gram) level, in other words,
we compared the automatically extracted unigrams
(words) and the manually labeled unigrams
(words). The reasons were that: 1) there existed
partial matching between phrases and it was not
trivial to define an accurate measure to evaluate
phrase quality; 2) each phrase was in fact com-
posed of a few words, so the keyphrases could be
obtained by combining the consecutive keywords.
We used 34 documents in the first five docu-
ment clusters in DUC2002 dataset (i.e. d061-d065).
At most 10 salient words were manually labeled
for each document to represent the document and
the average number of manually assigned key-
words was 6.8. Each approach returned 10 words
with highest saliency scores as the keywords. The
extracted 10 words were compared with the manu-
ally labeled keywords. The words were converted
to their corresponding basic forms based on
WordNet before comparison. The precision p, re-
call r, F-measure (F=2pr/(p+r)) were obtained for
each document and then the values were averaged
over all documents for evaluation purpose.
</bodyText>
<subsubsectionHeader confidence="0.985861">
4.1.2 Evaluation Results
</subsubsectionHeader>
<bodyText confidence="0.996065666666667">
Table 3 gives the comparison results. The proposed
approaches are compared with two baselines:
WordRank and MutualRank. WordRank is pro-
posed in Mihalcea and Tarau (2004) to make use
of only the co-occurrence relationships between
words to rank words, which outperforms tradi-
tional keyword extraction methods. The window
size k for WordRank is also set to 2, 5 and 10, re-
spectively.
</bodyText>
<figure confidence="0.985345190476191">
System Precision Recall F-measure
Our Approach
(WN)
0.413 0.504 0.454
Our Approach
(MI:k=2)
0.428 0.485 0.455
Our Approach
(MI:k=5)
0.425 0.491 0.456
Our Approach
(MI:k=10)
0.393 0.455 0.422
WordRank
(k=2)
0.373 0.412 0.392
WordRank
(k=5)
0.368 0.422 0.393
WordRank
(k=10)
</figure>
<table confidence="0.749447">
0.379 0.407 0.393
MutualRank 0.355 0.397 0.375
</table>
<tableCaption confidence="0.998">
Table 3. The Performance of Keyword Extraction
</tableCaption>
<bodyText confidence="0.9718639375">
Seen from the table, the proposed approaches
significantly outperform the baseline approaches.
Both the corpus-based measure (MI) and the
knowledge-based measure (WN) perform well on
the task of keyword extraction.
A running example is given below to demon-
strate the results:
Document ID: D062/AP891018-0301
Labeled keywords:
insurance earthquake insurer damage california Francisco
pay
Extracted keywords:
WN: insurance earthquake insurer quake california
spokesman cost million wednesday damage
MI(k=5): insurance insurer earthquake percent benefit
california property damage estimate rate
</bodyText>
<page confidence="0.991223">
558
</page>
<sectionHeader confidence="0.668881" genericHeader="conclusions">
\x0c5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99561675">
In this paper we propose a novel approach to si-
multaneously document summarization and key-
word extraction for single documents by fusing the
sentence-to-sentence, word-to-word, sentence-to-
word relationships in a unified framework. The
semantics between words computed by either cor-
pus-based approach or knowledge-based approach
can be incorporated into the framework in a natural
way. Evaluation results demonstrate the perform-
ance improvement of the proposed approach over
the baselines for both tasks.
In this study, only the mutual information meas-
ure and the vector measure are employed to com-
pute word semantics, and in future work many
other measures mentioned earlier will be investi-
gated in the framework in order to show the ro-
bustness of the framework. The evaluation of key-
word extraction is preliminary in this study, and
we will conduct more thorough experiments to
make the results more convincing. Furthermore,
the proposed approach will be applied to multi-
document summarization and keyword extraction,
which are considered more difficult than single
document summarization and keyword extraction.
</bodyText>
<sectionHeader confidence="0.962391" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.968216">
This work was supported by the National Science
Foundation of China (60642001).
</bodyText>
<sectionHeader confidence="0.981482" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998777">
M. R. Amini and P. Gallinari. 2002. The use of unlabeled data to
improve supervised learning for text summarization. In Pro-
ceedings of SIGIR2002, 105-112.
S. Brin and L. Page. 1998. The anatomy of a large-scale hypertex-
tual Web search engine. Computer Networks and ISDN Sys-
tems, 30(17).
J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-
based reranking for reordering documents and producing
summaries. In Proceedings of SIGIR-1998, 335-336.
J. M. Conroy and D. P. OLeary. 2001. Text summarization via
Hidden Markov Models. In Proceedings of SIGIR2001, 406-
407.
DUC. 2002. The Document Understanding Workshop 2002.
http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html
T. Dunning. 1993. Accurate methods for the statistics of surprise
and coincidence. Computational Linguistics 19, 6174.
G. ErKan and D. R. Radev. 2004. LexPageRank: Prestige in
multi-document text summarization. In Proceedings of
EMNLP2004.
C. Fellbaum. 1998. WordNet: An Electronic Lexical Database.
The MIT Press.
E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin, and C. G.
Nevill-Manning. 1999. Domain-specific keyphrase extraction.
Proceedings of IJCAI-99, pp. 668-673.
Y. H. Gong and X. Liu. 2001. Generic text summarization using
Relevance Measure and Latent Semantic Analysis. In Proceed-
ings of SIGIR2001, 19-25.
E. Hovy and C. Y. Lin. 1997. Automated text summarization in
SUMMARIST. In Proceeding of ACL1997/EACL1997 Wor-
shop on Intelligent Scalable Text Summarization.
A. Hulth. 2003. Improved automatic keyword extraction given
more linguistic knowledge. In Proceedings of EMNLP2003,
Japan, August.
J. M. Kleinberg. 1999. Authoritative sources in a hyperlinked
environment. Journal of the ACM, 46(5):604632.
B. Krulwich and C. Burkey. 1996. Learning user information
interests through the extraction of semantically significant
phrases. In AAAI 1996 Spring Symposium on Machine Learn-
ing in Information Access.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A.trainable document
summarizer. In Proceedings of SIGIR1995, 68-73.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduction to
latent semantic analysis. Discourse Processes 25.
C. Y. Lin and E. Hovy. 2000. The automated acquisition of topic
signatures for text Summarization. In Proceedings of ACL-
2000, 495-501.
C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of summa-
ries using n-gram co-occurrence statistics. In Proceedings of
HLT-NAACL2003, Edmonton, Canada, May.
R. Mihalcea, C. Corley, and C. Strapparava. 2006. Corpus-based
and knowledge-based measures of text semantic similarity. In
Proceedings of AAAI-06.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing order into
texts. In Proceedings of EMNLP2004.
R. Mihalcea and P.Tarau. 2005. A language independent algo-
rithm for single and multiple document summarization. In
Proceedings of IJCNLP2005.
A. Munoz. 1996. Compound key word generation from document
databases using a hierarchical clustering ART model. Intelli-
gent Data Analysis, 1(1).
T. Nomoto and Y. Matsumoto. 2001. A new approach to unsuper-
vised text summarization. In Proceedings of SIGIR2001, 26-34.
S. Patwardhan. 2003. Incorporating dictionary and corpus infor-
mation into a context vector measure of semantic relatedness.
Masters thesis, Univ. of Minnesota, Duluth.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. Word-
Net::Similarity Measuring the relatedness of concepts. In
Proceedings of AAAI-04.
D. Shen, J.-T. Sun, H. Li, Q. Yang, and Z. Chen. 2007. Document
Summarization using Conditional Random Fields. In Proceed-
ings of IJCAI 07.
A. M. Steier and R. K. Belew. 1993. Exporting phrases: A statisti-
cal analysis of topical language. In Proceedings of Second
Symposium on Document Analysis and Information Retrieval,
pp. 179-190.
P. D. Turney. 2000. Learning algorithms for keyphrase extraction.
Information Retrieval, 2:303-336.
P. Turney. 2001. Mining the web for synonyms: PMI-IR versus
LSA on TOEFL. In Proceedings of ECML-2001.
I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G.
Nevill-Manning. 1999. KEA: Practical automatic keyphrase
extraction. Proceedings of Digital Libraries 99 (DL&amp;apos;99), pp.
254-256.
H. Y. Zha. 2002. Generic summarization and keyphrase extraction
using mutual reinforcement principle and sentence clustering.
In Proceedings of SIGIR2002, pp. 113-120.
</reference>
<page confidence="0.934659">
559
</page>
<figure confidence="0.283712">
\x0c&amp;quot;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.670590">
<note confidence="0.930172333333333">b&amp;quot;Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 552559, Prague, Czech Republic, June 2007. c 2007 Association for Computational Linguistics</note>
<title confidence="0.985524">Towards an Iterative Reinforcement Approach for Simultaneous Document Summarization and Keyword Extraction</title>
<author confidence="0.997696">Xiaojun Wan Jianwu Yang Jianguo Xiao</author>
<affiliation confidence="0.999925">Institute of Computer Science and Technology</affiliation>
<address confidence="0.911429">Peking University, Beijing 100871, China</address>
<email confidence="0.985849">wanxiaojun@icst.pku.edu.cn</email>
<email confidence="0.985849">yangjianwu@icst.pku.edu.cn</email>
<email confidence="0.985849">xiaojianguo@icst.pku.edu.cn</email>
<abstract confidence="0.996095">Though both document summarization and keyword extraction aim to extract concise representations from documents, these two tasks have usually been investigated independently. This paper proposes a novel iterative reinforcement approach to simultaneously extracting summary and keywords from single document under the assumption that the summary and keywords of a document can be mutually boosted. The approach can naturally make full use of the reinforcement between sentences and keywords by fusing three kinds of relationships between sentences and words, either homogeneous or heterogeneous. Experimental results show the effectiveness of the proposed approach for both tasks. The corpus-based approach is validated to work almost as well as the knowledge-based approach for computing word semantics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M R Amini</author>
<author>P Gallinari</author>
</authors>
<title>The use of unlabeled data to improve supervised learning for text summarization.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGIR2002,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="5481" citStr="Amini and Gallinari, 2002" startWordPosition="811" endWordPosition="814">we focus on extraction-based methods in this study. Extraction-based methods usually assign a saliency score to each sentence and then rank the sentences in the document. The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including TextRank ((Mihalcea and Tarau, 2004, 2005) and LexPageRank (ErKan and Radev, 2004) have been proposed for document summarization. Similar to Kleinbergs HITS algorithm (Kleinberg, 1999) or Googles PageRank (Brin and Page, 1998), these methods first build a</context>
</contexts>
<marker>Amini, Gallinari, 2002</marker>
<rawString>M. R. Amini and P. Gallinari. 2002. The use of unlabeled data to improve supervised learning for text summarization. In Proceedings of SIGIR2002, 105-112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual Web search engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<volume>30</volume>
<issue>17</issue>
<contexts>
<context position="6052" citStr="Brin and Page, 1998" startWordPosition="894" endWordPosition="897">onroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including TextRank ((Mihalcea and Tarau, 2004, 2005) and LexPageRank (ErKan and Radev, 2004) have been proposed for document summarization. Similar to Kleinbergs HITS algorithm (Kleinberg, 1999) or Googles PageRank (Brin and Page, 1998), these methods first build a graph based on the similarity between sentences in a document and then the importance of a sentence is determined by taking into account global information on the graph recursively, rather than relying only on local sentence-specific information. 2.2 Keyword Extraction Keyword (or keyphrase) extraction usually involves assigning a saliency score to each candidate keyword by considering various features. Krulwich and Burkey (1996) use heuristics to extract keyphrases from a document. The heuristics are based on syntactic clues, such as the use of italics, the prese</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>S. Brin and L. Page. 1998. The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems, 30(17).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>J Goldstein</author>
</authors>
<title>The use of MMR, diversitybased reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGIR-1998,</booktitle>
<pages>335--336</pages>
<contexts>
<context position="5589" citStr="Carbonell and Goldstein, 1998" startWordPosition="827" endWordPosition="830">score to each sentence and then rank the sentences in the document. The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including TextRank ((Mihalcea and Tarau, 2004, 2005) and LexPageRank (ErKan and Radev, 2004) have been proposed for document summarization. Similar to Kleinbergs HITS algorithm (Kleinberg, 1999) or Googles PageRank (Brin and Page, 1998), these methods first build a graph based on the similarity between sentences in a document and then the importance of a sentence is dete</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>J. Carbonell and J. Goldstein. 1998. The use of MMR, diversitybased reranking for reordering documents and producing summaries. In Proceedings of SIGIR-1998, 335-336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Conroy</author>
<author>D P OLeary</author>
</authors>
<title>Text summarization via Hidden Markov Models.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR2001,</booktitle>
<pages>406--407</pages>
<contexts>
<context position="5454" citStr="Conroy and OLeary, 2001" startWordPosition="807" endWordPosition="810">or abstraction-based and we focus on extraction-based methods in this study. Extraction-based methods usually assign a saliency score to each sentence and then rank the sentences in the document. The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including TextRank ((Mihalcea and Tarau, 2004, 2005) and LexPageRank (ErKan and Radev, 2004) have been proposed for document summarization. Similar to Kleinbergs HITS algorithm (Kleinberg, 1999) or Googles PageRank (Brin and Page, 1998), </context>
</contexts>
<marker>Conroy, OLeary, 2001</marker>
<rawString>J. M. Conroy and D. P. OLeary. 2001. Text summarization via Hidden Markov Models. In Proceedings of SIGIR2001, 406-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DUC</author>
</authors>
<title>The Document Understanding Workshop</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>6174</pages>
<note>http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html</note>
<contexts>
<context position="17123" citStr="DUC, 2002" startWordPosition="2900" endWordPosition="2901">d normalize the scores of sentences: ) (nT ) (nT (n) ) * 1 1 ~ v W u U u , 1 (n) (n) (n) / u u u 2. Compute and normalize the scores of words: ) (nT ) (nT (n) ) * 1 1 ~ ~ u W v V v , 1 (n) (n) (n) / v v v where u(n) and v(n) denote the vectors computed at the n-th iteration. Usually the convergence of the iteration algorithm is achieved when the difference between the scores computed at two successive iterations for any sentences and words falls below a given threshold (0.0001 in this study). 4 Empirical Evaluation 4.1 Summarization Evaluation 4.1.1 Evaluation Setup We used task 1 of DUC2002 (DUC, 2002) for evaluation. The task aimed to evaluate generic summaries with a length of approximately 100 words or less. DUC2002 provided 567 English news articles collected from TREC-9 for singledocument summarization task. The sentences in each article have been separated and the sentence information was stored into files. In the experiments, the background corpus for using the mutual information measure to compute word semantics simply consisted of all the documents from DUC2001 to DUC2005, which could be easily expanded by adding more documents. The stopwords were removed and the remaining words we</context>
</contexts>
<marker>DUC, 2002</marker>
<rawString>DUC. 2002. The Document Understanding Workshop 2002. http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics 19, 6174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G ErKan</author>
<author>D R Radev</author>
</authors>
<title>LexPageRank: Prestige in multi-document text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP2004.</booktitle>
<contexts>
<context position="2933" citStr="ErKan and Radev, 2004" startWordPosition="427" endWordPosition="430">ional clues and prior knowledge. In this paper, we focus on generic document summarization and keyword extraction for single documents. Document summarization and keyword extraction have been widely explored in the natural language processing and information retrieval communities. A series of workshops and conferences on automatic text summarization (e.g. SUMMAC, DUC and NTCIR) have advanced the technology and produced a couple of experimental online systems. In recent years, graph-based ranking algorithms have been successfully used for document summarization (Mihalcea and Tarau, 2004, 2005; ErKan and Radev, 2004) and keyword extraction (Mihalcea and Tarau, 2004). Such algorithms make use of voting or recommendations between sentences (or words) to extract sentences (or keywords). Though the two tasks essentially share much in common, most algorithms have been developed particularly for either document summarization or keyword extraction. Zha (2002) proposes a method for simultaneous keyphrase extraction and text summarization by using only the heterogeneous sentence-to-word relationships. Inspired by this, we aim to take into account all the three kinds of relationships among sentences and words (i.e.</context>
<context position="5908" citStr="ErKan and Radev, 2004" startWordPosition="874" endWordPosition="877">lso been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including TextRank ((Mihalcea and Tarau, 2004, 2005) and LexPageRank (ErKan and Radev, 2004) have been proposed for document summarization. Similar to Kleinbergs HITS algorithm (Kleinberg, 1999) or Googles PageRank (Brin and Page, 1998), these methods first build a graph based on the similarity between sentences in a document and then the importance of a sentence is determined by taking into account global information on the graph recursively, rather than relying only on local sentence-specific information. 2.2 Keyword Extraction Keyword (or keyphrase) extraction usually involves assigning a saliency score to each candidate keyword by considering various features. Krulwich and Burkey</context>
</contexts>
<marker>ErKan, Radev, 2004</marker>
<rawString>G. ErKan and D. R. Radev. 2004. LexPageRank: Prestige in multi-document text summarization. In Proceedings of EMNLP2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="11441" citStr="Fellbaum, 1998" startWordPosition="1776" endWordPosition="1778"> measure in this study. Then U is normalized to U ~ as follows to make the sum of each row equal to 1: erwise , oth U , if U U U m j ij m j ij ij ij 0 0 ~ 1 1 (2) 3.2.2 Word-to-Word Graph ( WW-Graph) Given the word collection T={tj|1IjIn } of a document1 , the semantic similarity between any two words ti and tj can be computed using approaches that are either knowledge-based or corpus-based (Mihalcea et al., 2006). Knowledge-based measures of word semantic similarity try to quantify the degree to which two words are semantically related using information drawn from semantic networks. WordNet (Fellbaum, 1998) is a lexical database where each 1 The stopwords defined in the Smart system have been removed from the collection. sentence word SS WW SW 554 \x0cunique meaning of a word is represented by a synonym set or synset. Each synset has a gloss that defines the concept that it represents. Synsets are connected to each other through explicit semantic relations that are defined in WordNet. Many approaches have been proposed to measure semantic relatedness based on WordNet. The measures vary from simple edge-counting to attempt to factor in peculiarities of the network structure by considering link di</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Frank</author>
<author>G W Paynter</author>
<author>I H Witten</author>
<author>C Gutwin</author>
<author>C G Nevill-Manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<booktitle>Proceedings of IJCAI-99,</booktitle>
<pages>668--673</pages>
<contexts>
<context position="7135" citStr="Frank et al., 1999" startWordPosition="1059" endWordPosition="1062">se heuristics to extract keyphrases from a document. The heuristics are based on syntactic clues, such as the use of italics, the presence of phrases in section headers, and the use of acronyms. Munoz (1996) uses an unsupervised learning algorithm to discover two-word keyphrases. The algorithm is based on Adaptive Resonance Theory (ART) neural networks. Steier and Belew (1993) use the mutual information statistics to discover two-word keyphrases. Supervised machine learning algorithms have been proposed to classify a candidate phrase into either keyphrase or not. GenEx (Turney, 2000) and Kea (Frank et al., 1999; Witten et al., 1999) are two typical systems, and the most important features for classifying a candidate phrase are the frequency and location of the phrase in the document. More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank keywords based on the co-occurrence links between words. 3 Iterative Reinforcement Approach 3.1 Overview The proposed approach is intuitively based on the following assumptions: Assumption 1: A sentence should be salient if it is heavily linked with other s</context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin, and C. G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. Proceedings of IJCAI-99, pp. 668-673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y H Gong</author>
<author>X Liu</author>
</authors>
<title>Generic text summarization using Relevance Measure and Latent Semantic Analysis.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR2001,</booktitle>
<contexts>
<context position="5642" citStr="Gong and Liu, 2001" startWordPosition="835" endWordPosition="838">nt. The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including TextRank ((Mihalcea and Tarau, 2004, 2005) and LexPageRank (ErKan and Radev, 2004) have been proposed for document summarization. Similar to Kleinbergs HITS algorithm (Kleinberg, 1999) or Googles PageRank (Brin and Page, 1998), these methods first build a graph based on the similarity between sentences in a document and then the importance of a sentence is determined by taking into account global information on t</context>
</contexts>
<marker>Gong, Liu, 2001</marker>
<rawString>Y. H. Gong and X. Liu. 2001. Generic text summarization using Relevance Measure and Latent Semantic Analysis. In Proceedings of SIGIR2001, 19-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>C Y Lin</author>
</authors>
<title>Automated text summarization in SUMMARIST.</title>
<date>1997</date>
<booktitle>In Proceeding of ACL1997/EACL1997 Worshop on Intelligent Scalable Text Summarization.</booktitle>
<contexts>
<context position="5227" citStr="Hovy and Lin, 1997" startWordPosition="772" endWordPosition="775">presents and discusses the evaluation results. Lastly we conclude our paper in Section 5. 2 Related Works 2.1 Document Summarization Generally speaking, single document summarization methods can be either extraction-based or abstraction-based and we focus on extraction-based methods in this study. Extraction-based methods usually assign a saliency score to each sentence and then rank the sentences in the document. The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including T</context>
</contexts>
<marker>Hovy, Lin, 1997</marker>
<rawString>E. Hovy and C. Y. Lin. 1997. Automated text summarization in SUMMARIST. In Proceeding of ACL1997/EACL1997 Worshop on Intelligent Scalable Text Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hulth</author>
</authors>
<title>Improved automatic keyword extraction given more linguistic knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP2003,</booktitle>
<location>Japan,</location>
<contexts>
<context position="7400" citStr="Hulth (2003)" startWordPosition="1106" endWordPosition="1107">yphrases. The algorithm is based on Adaptive Resonance Theory (ART) neural networks. Steier and Belew (1993) use the mutual information statistics to discover two-word keyphrases. Supervised machine learning algorithms have been proposed to classify a candidate phrase into either keyphrase or not. GenEx (Turney, 2000) and Kea (Frank et al., 1999; Witten et al., 1999) are two typical systems, and the most important features for classifying a candidate phrase are the frequency and location of the phrase in the document. More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank keywords based on the co-occurrence links between words. 3 Iterative Reinforcement Approach 3.1 Overview The proposed approach is intuitively based on the following assumptions: Assumption 1: A sentence should be salient if it is heavily linked with other salient sentences, and a word should be salient if it is heavily linked with other salient words. Assumption 2: A sentence should be salient if it contains many salient words, and a word should be salient if it appears in many salient sentences. The first assumption</context>
</contexts>
<marker>Hulth, 2003</marker>
<rawString>A. Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of EMNLP2003, Japan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1999</date>
<journal>Journal of the ACM,</journal>
<volume>46</volume>
<issue>5</issue>
<contexts>
<context position="6010" citStr="Kleinberg, 1999" startWordPosition="889" endWordPosition="890">rvised methods (Kupiec et al., 1995; Conroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including TextRank ((Mihalcea and Tarau, 2004, 2005) and LexPageRank (ErKan and Radev, 2004) have been proposed for document summarization. Similar to Kleinbergs HITS algorithm (Kleinberg, 1999) or Googles PageRank (Brin and Page, 1998), these methods first build a graph based on the similarity between sentences in a document and then the importance of a sentence is determined by taking into account global information on the graph recursively, rather than relying only on local sentence-specific information. 2.2 Keyword Extraction Keyword (or keyphrase) extraction usually involves assigning a saliency score to each candidate keyword by considering various features. Krulwich and Burkey (1996) use heuristics to extract keyphrases from a document. The heuristics are based on syntactic cl</context>
</contexts>
<marker>Kleinberg, 1999</marker>
<rawString>J. M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5):604632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Krulwich</author>
<author>C Burkey</author>
</authors>
<title>Learning user information interests through the extraction of semantically significant phrases.</title>
<date>1996</date>
<booktitle>In AAAI 1996 Spring Symposium on Machine Learning in Information Access.</booktitle>
<contexts>
<context position="6515" citStr="Krulwich and Burkey (1996)" startWordPosition="962" endWordPosition="965">an and Radev, 2004) have been proposed for document summarization. Similar to Kleinbergs HITS algorithm (Kleinberg, 1999) or Googles PageRank (Brin and Page, 1998), these methods first build a graph based on the similarity between sentences in a document and then the importance of a sentence is determined by taking into account global information on the graph recursively, rather than relying only on local sentence-specific information. 2.2 Keyword Extraction Keyword (or keyphrase) extraction usually involves assigning a saliency score to each candidate keyword by considering various features. Krulwich and Burkey (1996) use heuristics to extract keyphrases from a document. The heuristics are based on syntactic clues, such as the use of italics, the presence of phrases in section headers, and the use of acronyms. Munoz (1996) uses an unsupervised learning algorithm to discover two-word keyphrases. The algorithm is based on Adaptive Resonance Theory (ART) neural networks. Steier and Belew (1993) use the mutual information statistics to discover two-word keyphrases. Supervised machine learning algorithms have been proposed to classify a candidate phrase into either keyphrase or not. GenEx (Turney, 2000) and Kea</context>
</contexts>
<marker>Krulwich, Burkey, 1996</marker>
<rawString>B. Krulwich and C. Burkey. 1996. Learning user information interests through the extraction of semantically significant phrases. In AAAI 1996 Spring Symposium on Machine Learning in Information Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>F Chen</author>
</authors>
<title>A.trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of SIGIR1995,</booktitle>
<pages>68--73</pages>
<contexts>
<context position="5429" citStr="Kupiec et al., 1995" startWordPosition="803" endWordPosition="806">her extraction-based or abstraction-based and we focus on extraction-based methods in this study. Extraction-based methods usually assign a saliency score to each sentence and then rank the sentences in the document. The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including TextRank ((Mihalcea and Tarau, 2004, 2005) and LexPageRank (ErKan and Radev, 2004) have been proposed for document summarization. Similar to Kleinbergs HITS algorithm (Kleinberg, 1999) or Googles PageRan</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>J. Kupiec, J. Pedersen, and F. Chen. 1995. A.trainable document summarizer. In Proceedings of SIGIR1995, 68-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P Foltz</author>
<author>D Laham</author>
</authors>
<title>Introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes 25.</booktitle>
<contexts>
<context position="13036" citStr="Landauer et al., 1998" startWordPosition="2040" endWordPosition="2043">occurrence matrix from a corpus made up of the WordNet glosses. Each content word used in a WordNet gloss has an associated context vector. Each gloss is represented by a gloss vector that is the average of all the context vectors of the words found in the gloss. Relatedness between concepts is measured by finding the cosine between a pair of gloss vectors. Corpus-based measures of word semantic similarity try to identify the degree of similarity between words using information exclusively derived from large corpora. Such measures as mutual information (Turney 2001), latent semantic analysis (Landauer et al., 1998), log-likelihood ratio (Dunning, 1993) have been proposed to evaluate word semantic similarity based on the co-occurrence information on a large corpus. In this study, we simply choose the mutual information to compute the semantic similarity between word ti and tj as follows: ) ( ) ( ) ( log ) ( j i j i j i t p t p ,t t p N ,t t sim (3) which indicates the degree of statistical dependence between ti and tj. Here, N is the total number of words in the corpus and p(ti) and p(tj) are respectively the probabilities of the occurrences of ti and tj, i.e. count(ti)/N and count(tj)/N, where count(ti)</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduction to latent semantic analysis. Discourse Processes 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text Summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL2000,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="5248" citStr="Lin and Hovy, 2000" startWordPosition="776" endWordPosition="779">es the evaluation results. Lastly we conclude our paper in Section 5. 2 Related Works 2.1 Document Summarization Generally speaking, single document summarization methods can be either extraction-based or abstraction-based and we focus on extraction-based methods in this study. Extraction-based methods usually assign a saliency score to each sentence and then rank the sentences in the document. The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including TextRank ((Mihalcea an</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>C. Y. Lin and E. Hovy. 2000. The automated acquisition of topic signatures for text Summarization. In Proceedings of ACL2000, 495-501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL2003,</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="17880" citStr="Lin and Hovy, 2003" startWordPosition="3018" endWordPosition="3021">sh news articles collected from TREC-9 for singledocument summarization task. The sentences in each article have been separated and the sentence information was stored into files. In the experiments, the background corpus for using the mutual information measure to compute word semantics simply consisted of all the documents from DUC2001 to DUC2005, which could be easily expanded by adding more documents. The stopwords were removed and the remaining words were converted to the basic forms based on WordNet. Then the semantic similarity values between the words were computed. We used the ROUGE (Lin and Hovy, 2003) toolkit (i.e.ROUGEeval-1.4.2 in this study) for evaluation, which has been widely adopted by DUC for automatic summarization evaluation. It measured summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary. ROUGE toolkit reported separate scores for 1, 2, 3 and 4-gram, and also for longest common subsequence co-occurrences. Among these different scores, unigram-based ROUGE score (ROUGE-1) has been shown to agree with human judgment most (Lin and Hovy, 2003). We showed three of the ROUGE metrics in t</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of HLT-NAACL2003, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Corley</author>
<author>C Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI-06.</booktitle>
<contexts>
<context position="11243" citStr="Mihalcea et al., 2006" startWordPosition="1747" endWordPosition="1750">taining term t in a background corpus. Note that other measures (e.g. Jaccard, Dice, Overlap, etc.) can also be explored to compute the content similarity between sentences, and we simply choose the cosine measure in this study. Then U is normalized to U ~ as follows to make the sum of each row equal to 1: erwise , oth U , if U U U m j ij m j ij ij ij 0 0 ~ 1 1 (2) 3.2.2 Word-to-Word Graph ( WW-Graph) Given the word collection T={tj|1IjIn } of a document1 , the semantic similarity between any two words ti and tj can be computed using approaches that are either knowledge-based or corpus-based (Mihalcea et al., 2006). Knowledge-based measures of word semantic similarity try to quantify the degree to which two words are semantically related using information drawn from semantic networks. WordNet (Fellbaum, 1998) is a lexical database where each 1 The stopwords defined in the Smart system have been removed from the collection. sentence word SS WW SW 554 \x0cunique meaning of a word is represented by a synonym set or synset. Each synset has a gloss that defines the concept that it represents. Synsets are connected to each other through explicit semantic relations that are defined in WordNet. Many approaches </context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>R. Mihalcea, C. Corley, and C. Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of AAAI-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>TextRank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP2004.</booktitle>
<contexts>
<context position="2903" citStr="Mihalcea and Tarau, 2004" startWordPosition="422" endWordPosition="425">f the document without any additional clues and prior knowledge. In this paper, we focus on generic document summarization and keyword extraction for single documents. Document summarization and keyword extraction have been widely explored in the natural language processing and information retrieval communities. A series of workshops and conferences on automatic text summarization (e.g. SUMMAC, DUC and NTCIR) have advanced the technology and produced a couple of experimental online systems. In recent years, graph-based ranking algorithms have been successfully used for document summarization (Mihalcea and Tarau, 2004, 2005; ErKan and Radev, 2004) and keyword extraction (Mihalcea and Tarau, 2004). Such algorithms make use of voting or recommendations between sentences (or words) to extract sentences (or keywords). Though the two tasks essentially share much in common, most algorithms have been developed particularly for either document summarization or keyword extraction. Zha (2002) proposes a method for simultaneous keyphrase extraction and text summarization by using only the heterogeneous sentence-to-word relationships. Inspired by this, we aim to take into account all the three kinds of relationships a</context>
<context position="5861" citStr="Mihalcea and Tarau, 2004" startWordPosition="867" endWordPosition="870">Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including TextRank ((Mihalcea and Tarau, 2004, 2005) and LexPageRank (ErKan and Radev, 2004) have been proposed for document summarization. Similar to Kleinbergs HITS algorithm (Kleinberg, 1999) or Googles PageRank (Brin and Page, 1998), these methods first build a graph based on the similarity between sentences in a document and then the importance of a sentence is determined by taking into account global information on the graph recursively, rather than relying only on local sentence-specific information. 2.2 Keyword Extraction Keyword (or keyphrase) extraction usually involves assigning a saliency score to each candidate keyword by co</context>
<context position="7442" citStr="Mihalcea and Tarau (2004)" startWordPosition="1110" endWordPosition="1113">sed on Adaptive Resonance Theory (ART) neural networks. Steier and Belew (1993) use the mutual information statistics to discover two-word keyphrases. Supervised machine learning algorithms have been proposed to classify a candidate phrase into either keyphrase or not. GenEx (Turney, 2000) and Kea (Frank et al., 1999; Witten et al., 1999) are two typical systems, and the most important features for classifying a candidate phrase are the frequency and location of the phrase in the document. More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank keywords based on the co-occurrence links between words. 3 Iterative Reinforcement Approach 3.1 Overview The proposed approach is intuitively based on the following assumptions: Assumption 1: A sentence should be salient if it is heavily linked with other salient sentences, and a word should be salient if it is heavily linked with other salient words. Assumption 2: A sentence should be salient if it contains many salient words, and a word should be salient if it appears in many salient sentences. The first assumption is similar to PageRank which makes use of</context>
<context position="19536" citStr="Mihalcea and Tarau (2004)" startWordPosition="3277" endWordPosition="3280">important. We adopt the WordNetbased vector measure (WN) and the corpus-based mutual information measure (MI) for computing the semantic similarity between words. When using the mutual information measure, we heuristically set the window size k to 2, 5 and 10, respectively. The proposed approaches with different word similarity measures (WN and MI) are compared 2 The -l option is very important for fair comparison. Some previous works not adopting this option are likely to overestimate the ROUGE scores. 556 \x0cwith two solid baselines: SentenceRank and MutualRank. SentenceRank is proposed in Mihalcea and Tarau (2004) to make use of only the sentence-tosentence relationships to rank sentences, which outperforms most popular summarization methods. MutualRank is proposed in Zha (2002) to make use of only the sentence-to-word relationships to rank sentences and words. For all the summarization methods, after the sentences are ranked by their saliency scores, we can apply a variant form of the MMR algorithm to remove redundancy and choose both the salient and novel sentences to the summary. Table 1 gives the comparison results of the methods before removing redundancy and Table 2 gives the comparison results o</context>
<context position="25115" citStr="Mihalcea and Tarau (2004)" startWordPosition="4171" endWordPosition="4174">igned keywords was 6.8. Each approach returned 10 words with highest saliency scores as the keywords. The extracted 10 words were compared with the manually labeled keywords. The words were converted to their corresponding basic forms based on WordNet before comparison. The precision p, recall r, F-measure (F=2pr/(p+r)) were obtained for each document and then the values were averaged over all documents for evaluation purpose. 4.1.2 Evaluation Results Table 3 gives the comparison results. The proposed approaches are compared with two baselines: WordRank and MutualRank. WordRank is proposed in Mihalcea and Tarau (2004) to make use of only the co-occurrence relationships between words to rank words, which outperforms traditional keyword extraction methods. The window size k for WordRank is also set to 2, 5 and 10, respectively. System Precision Recall F-measure Our Approach (WN) 0.413 0.504 0.454 Our Approach (MI:k=2) 0.428 0.485 0.455 Our Approach (MI:k=5) 0.425 0.491 0.456 Our Approach (MI:k=10) 0.393 0.455 0.422 WordRank (k=2) 0.373 0.412 0.392 WordRank (k=5) 0.368 0.422 0.393 WordRank (k=10) 0.379 0.407 0.393 MutualRank 0.355 0.397 0.375 Table 3. The Performance of Keyword Extraction Seen from the table,</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>R. Mihalcea and P. Tarau. 2004. TextRank: Bringing order into texts. In Proceedings of EMNLP2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>A language independent algorithm for single and multiple document summarization.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP2005.</booktitle>
<marker>Mihalcea, Tarau, 2005</marker>
<rawString>R. Mihalcea and P.Tarau. 2005. A language independent algorithm for single and multiple document summarization. In Proceedings of IJCNLP2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Munoz</author>
</authors>
<title>Compound key word generation from document databases using a hierarchical clustering ART model.</title>
<date>1996</date>
<journal>Intelligent Data Analysis,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="6724" citStr="Munoz (1996)" startWordPosition="1000" endWordPosition="1001">between sentences in a document and then the importance of a sentence is determined by taking into account global information on the graph recursively, rather than relying only on local sentence-specific information. 2.2 Keyword Extraction Keyword (or keyphrase) extraction usually involves assigning a saliency score to each candidate keyword by considering various features. Krulwich and Burkey (1996) use heuristics to extract keyphrases from a document. The heuristics are based on syntactic clues, such as the use of italics, the presence of phrases in section headers, and the use of acronyms. Munoz (1996) uses an unsupervised learning algorithm to discover two-word keyphrases. The algorithm is based on Adaptive Resonance Theory (ART) neural networks. Steier and Belew (1993) use the mutual information statistics to discover two-word keyphrases. Supervised machine learning algorithms have been proposed to classify a candidate phrase into either keyphrase or not. GenEx (Turney, 2000) and Kea (Frank et al., 1999; Witten et al., 1999) are two typical systems, and the most important features for classifying a candidate phrase are the frequency and location of the phrase in the document. More linguis</context>
</contexts>
<marker>Munoz, 1996</marker>
<rawString>A. Munoz. 1996. Compound key word generation from document databases using a hierarchical clustering ART model. Intelligent Data Analysis, 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nomoto</author>
<author>Y Matsumoto</author>
</authors>
<title>A new approach to unsupervised text summarization.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR2001,</booktitle>
<pages>26--34</pages>
<contexts>
<context position="5385" citStr="Nomoto and Matsumoto, 2001" startWordPosition="795" endWordPosition="798">ng, single document summarization methods can be either extraction-based or abstraction-based and we focus on extraction-based methods in this study. Extraction-based methods usually assign a saliency score to each sentence and then rank the sentences in the document. The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including TextRank ((Mihalcea and Tarau, 2004, 2005) and LexPageRank (ErKan and Radev, 2004) have been proposed for document summarization. Similar to Kleinbergs HITS al</context>
</contexts>
<marker>Nomoto, Matsumoto, 2001</marker>
<rawString>T. Nomoto and Y. Matsumoto. 2001. A new approach to unsupervised text summarization. In Proceedings of SIGIR2001, 26-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
</authors>
<title>Incorporating dictionary and corpus information into a context vector measure of semantic relatedness.</title>
<date>2003</date>
<tech>Masters thesis,</tech>
<institution>Univ. of Minnesota,</institution>
<location>Duluth.</location>
<contexts>
<context position="12400" citStr="Patwardhan, 2003" startWordPosition="1937" endWordPosition="1938">ntic relations that are defined in WordNet. Many approaches have been proposed to measure semantic relatedness based on WordNet. The measures vary from simple edge-counting to attempt to factor in peculiarities of the network structure by considering link direction, relative path, and density, such as vector, lesk, hso, lch, wup, path, res, lin and jcn (Pedersen et al., 2004). For example, cat and dog has higher semantic similarity than cat and computer. In this study, we implement the vector measure to efficiently evaluate the similarities of a large number of word pairs. The vector measure (Patwardhan, 2003) creates a co occurrence matrix from a corpus made up of the WordNet glosses. Each content word used in a WordNet gloss has an associated context vector. Each gloss is represented by a gloss vector that is the average of all the context vectors of the words found in the gloss. Relatedness between concepts is measured by finding the cosine between a pair of gloss vectors. Corpus-based measures of word semantic similarity try to identify the degree of similarity between words using information exclusively derived from large corpora. Such measures as mutual information (Turney 2001), latent seman</context>
</contexts>
<marker>Patwardhan, 2003</marker>
<rawString>S. Patwardhan. 2003. Incorporating dictionary and corpus information into a context vector measure of semantic relatedness. Masters thesis, Univ. of Minnesota, Duluth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>WordNet::Similarity Measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI-04.</booktitle>
<contexts>
<context position="12161" citStr="Pedersen et al., 2004" startWordPosition="1896" endWordPosition="1899">om the collection. sentence word SS WW SW 554 \x0cunique meaning of a word is represented by a synonym set or synset. Each synset has a gloss that defines the concept that it represents. Synsets are connected to each other through explicit semantic relations that are defined in WordNet. Many approaches have been proposed to measure semantic relatedness based on WordNet. The measures vary from simple edge-counting to attempt to factor in peculiarities of the network structure by considering link direction, relative path, and density, such as vector, lesk, hso, lch, wup, path, res, lin and jcn (Pedersen et al., 2004). For example, cat and dog has higher semantic similarity than cat and computer. In this study, we implement the vector measure to efficiently evaluate the similarities of a large number of word pairs. The vector measure (Patwardhan, 2003) creates a co occurrence matrix from a corpus made up of the WordNet glosses. Each content word used in a WordNet gloss has an associated context vector. Each gloss is represented by a gloss vector that is the average of all the context vectors of the words found in the gloss. Relatedness between concepts is measured by finding the cosine between a pair of gl</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. WordNet::Similarity Measuring the relatedness of concepts. In Proceedings of AAAI-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>J-T Sun</author>
<author>H Li</author>
<author>Q Yang</author>
<author>Z Chen</author>
</authors>
<title>Document Summarization using Conditional Random Fields.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI 07.</booktitle>
<contexts>
<context position="5501" citStr="Shen et al., 2007" startWordPosition="815" endWordPosition="818">d methods in this study. Extraction-based methods usually assign a saliency score to each sentence and then rank the sentences in the document. The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including TextRank ((Mihalcea and Tarau, 2004, 2005) and LexPageRank (ErKan and Radev, 2004) have been proposed for document summarization. Similar to Kleinbergs HITS algorithm (Kleinberg, 1999) or Googles PageRank (Brin and Page, 1998), these methods first build a graph based on the </context>
</contexts>
<marker>Shen, Sun, Li, Yang, Chen, 2007</marker>
<rawString>D. Shen, J.-T. Sun, H. Li, Q. Yang, and Z. Chen. 2007. Document Summarization using Conditional Random Fields. In Proceedings of IJCAI 07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Steier</author>
<author>R K Belew</author>
</authors>
<title>Exporting phrases: A statistical analysis of topical language.</title>
<date>1993</date>
<booktitle>In Proceedings of Second Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>179--190</pages>
<contexts>
<context position="6896" citStr="Steier and Belew (1993)" startWordPosition="1024" endWordPosition="1027">an relying only on local sentence-specific information. 2.2 Keyword Extraction Keyword (or keyphrase) extraction usually involves assigning a saliency score to each candidate keyword by considering various features. Krulwich and Burkey (1996) use heuristics to extract keyphrases from a document. The heuristics are based on syntactic clues, such as the use of italics, the presence of phrases in section headers, and the use of acronyms. Munoz (1996) uses an unsupervised learning algorithm to discover two-word keyphrases. The algorithm is based on Adaptive Resonance Theory (ART) neural networks. Steier and Belew (1993) use the mutual information statistics to discover two-word keyphrases. Supervised machine learning algorithms have been proposed to classify a candidate phrase into either keyphrase or not. GenEx (Turney, 2000) and Kea (Frank et al., 1999; Witten et al., 1999) are two typical systems, and the most important features for classifying a candidate phrase are the frequency and location of the phrase in the document. More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank keywords based on </context>
</contexts>
<marker>Steier, Belew, 1993</marker>
<rawString>A. M. Steier and R. K. Belew. 1993. Exporting phrases: A statistical analysis of topical language. In Proceedings of Second Symposium on Document Analysis and Information Retrieval, pp. 179-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Learning algorithms for keyphrase extraction. Information Retrieval,</title>
<date>2000</date>
<pages>2--303</pages>
<contexts>
<context position="7107" citStr="Turney, 2000" startWordPosition="1055" endWordPosition="1056">ich and Burkey (1996) use heuristics to extract keyphrases from a document. The heuristics are based on syntactic clues, such as the use of italics, the presence of phrases in section headers, and the use of acronyms. Munoz (1996) uses an unsupervised learning algorithm to discover two-word keyphrases. The algorithm is based on Adaptive Resonance Theory (ART) neural networks. Steier and Belew (1993) use the mutual information statistics to discover two-word keyphrases. Supervised machine learning algorithms have been proposed to classify a candidate phrase into either keyphrase or not. GenEx (Turney, 2000) and Kea (Frank et al., 1999; Witten et al., 1999) are two typical systems, and the most important features for classifying a candidate phrase are the frequency and location of the phrase in the document. More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank keywords based on the co-occurrence links between words. 3 Iterative Reinforcement Approach 3.1 Overview The proposed approach is intuitively based on the following assumptions: Assumption 1: A sentence should be salient if it is</context>
</contexts>
<marker>Turney, 2000</marker>
<rawString>P. D. Turney. 2000. Learning algorithms for keyphrase extraction. Information Retrieval, 2:303-336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Mining the web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of ECML-2001.</booktitle>
<contexts>
<context position="12986" citStr="Turney 2001" startWordPosition="2035" endWordPosition="2036">measure (Patwardhan, 2003) creates a co occurrence matrix from a corpus made up of the WordNet glosses. Each content word used in a WordNet gloss has an associated context vector. Each gloss is represented by a gloss vector that is the average of all the context vectors of the words found in the gloss. Relatedness between concepts is measured by finding the cosine between a pair of gloss vectors. Corpus-based measures of word semantic similarity try to identify the degree of similarity between words using information exclusively derived from large corpora. Such measures as mutual information (Turney 2001), latent semantic analysis (Landauer et al., 1998), log-likelihood ratio (Dunning, 1993) have been proposed to evaluate word semantic similarity based on the co-occurrence information on a large corpus. In this study, we simply choose the mutual information to compute the semantic similarity between word ti and tj as follows: ) ( ) ( ) ( log ) ( j i j i j i t p t p ,t t p N ,t t sim (3) which indicates the degree of statistical dependence between ti and tj. Here, N is the total number of words in the corpus and p(ti) and p(tj) are respectively the probabilities of the occurrences of ti and tj,</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>P. Turney. 2001. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of ECML-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>G W Paynter</author>
<author>E Frank</author>
<author>C Gutwin</author>
<author>C G Nevill-Manning</author>
</authors>
<title>KEA: Practical automatic keyphrase extraction.</title>
<date>1999</date>
<booktitle>Proceedings of Digital Libraries</booktitle>
<volume>99</volume>
<pages>254--256</pages>
<contexts>
<context position="7157" citStr="Witten et al., 1999" startWordPosition="1063" endWordPosition="1066">ract keyphrases from a document. The heuristics are based on syntactic clues, such as the use of italics, the presence of phrases in section headers, and the use of acronyms. Munoz (1996) uses an unsupervised learning algorithm to discover two-word keyphrases. The algorithm is based on Adaptive Resonance Theory (ART) neural networks. Steier and Belew (1993) use the mutual information statistics to discover two-word keyphrases. Supervised machine learning algorithms have been proposed to classify a candidate phrase into either keyphrase or not. GenEx (Turney, 2000) and Kea (Frank et al., 1999; Witten et al., 1999) are two typical systems, and the most important features for classifying a candidate phrase are the frequency and location of the phrase in the document. More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank keywords based on the co-occurrence links between words. 3 Iterative Reinforcement Approach 3.1 Overview The proposed approach is intuitively based on the following assumptions: Assumption 1: A sentence should be salient if it is heavily linked with other salient sentences, and </context>
</contexts>
<marker>Witten, Paynter, Frank, Gutwin, Nevill-Manning, 1999</marker>
<rawString>I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G. Nevill-Manning. 1999. KEA: Practical automatic keyphrase extraction. Proceedings of Digital Libraries 99 (DL&amp;apos;99), pp. 254-256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Y Zha</author>
</authors>
<title>Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGIR2002,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="3275" citStr="Zha (2002)" startWordPosition="480" endWordPosition="481">.g. SUMMAC, DUC and NTCIR) have advanced the technology and produced a couple of experimental online systems. In recent years, graph-based ranking algorithms have been successfully used for document summarization (Mihalcea and Tarau, 2004, 2005; ErKan and Radev, 2004) and keyword extraction (Mihalcea and Tarau, 2004). Such algorithms make use of voting or recommendations between sentences (or words) to extract sentences (or keywords). Though the two tasks essentially share much in common, most algorithms have been developed particularly for either document summarization or keyword extraction. Zha (2002) proposes a method for simultaneous keyphrase extraction and text summarization by using only the heterogeneous sentence-to-word relationships. Inspired by this, we aim to take into account all the three kinds of relationships among sentences and words (i.e. the homogeneous relationships between words, the homogeneous relationships between sentences, and the heterogeneous relationships between words and sentences) in 552 \x0ca unified framework for both document summarization and keyword extraction. The importance of a sentence (word) is determined by both the importance of related sentences (</context>
<context position="5657" citStr="Zha (2002)" startWordPosition="840" endWordPosition="841">ly computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including TextRank ((Mihalcea and Tarau, 2004, 2005) and LexPageRank (ErKan and Radev, 2004) have been proposed for document summarization. Similar to Kleinbergs HITS algorithm (Kleinberg, 1999) or Googles PageRank (Brin and Page, 1998), these methods first build a graph based on the similarity between sentences in a document and then the importance of a sentence is determined by taking into account global information on the graph recurs</context>
<context position="19704" citStr="Zha (2002)" startWordPosition="3303" endWordPosition="3304">l information measure, we heuristically set the window size k to 2, 5 and 10, respectively. The proposed approaches with different word similarity measures (WN and MI) are compared 2 The -l option is very important for fair comparison. Some previous works not adopting this option are likely to overestimate the ROUGE scores. 556 \x0cwith two solid baselines: SentenceRank and MutualRank. SentenceRank is proposed in Mihalcea and Tarau (2004) to make use of only the sentence-tosentence relationships to rank sentences, which outperforms most popular summarization methods. MutualRank is proposed in Zha (2002) to make use of only the sentence-to-word relationships to rank sentences and words. For all the summarization methods, after the sentences are ranked by their saliency scores, we can apply a variant form of the MMR algorithm to remove redundancy and choose both the salient and novel sentences to the summary. Table 1 gives the comparison results of the methods before removing redundancy and Table 2 gives the comparison results of the methods after removing redundancy. System ROUGE-1 ROUGE-2 ROUGE-W Our Approach (WN) 0.47100*# 0.20424*# 0.16336# Our Approach (MI:k=2) 0.46711# 0.20195# 0.16257# </context>
</contexts>
<marker>Zha, 2002</marker>
<rawString>H. Y. Zha. 2002. Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering. In Proceedings of SIGIR2002, pp. 113-120.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>