 Multi-layered neural networks of this form can approximate arbitrary mappings from inputs to outputs CITATION, whereas a loglinear model alone can only estimate probabilities where the category-conditioned probability distributions P(xidi) of the pre-defined inputs x are in a restricted form of the exponential family CITATION,,
 4We use the cross-entropy error function, which ensures that the minimum of the error function converges to the desired probabilities as the amount of training data increases CITATION,,
 We implement this conditioning as a mixture model CITATION, where the tag predictions are the mixing coefficients,,
 1 Introduction Many statistical parsers (CITATION; CITATION; CITATION) are based on a history-based probability model CITATION, where the probability of each decision in a parse is conditioned on the previous decisions in the parse,,
 Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (CITATION; CITATION; CITATION),,
 in history-based models CITATION, the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1,,,
 tical left-corner parser CITATION, and a PCFG CITATION,,
 The Tags model also does much better than the only other broad coverage neural network parser CITATION,,
 The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq&gt;200) and five recent statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION),,
 The best current model CITATION has only 6% less precision error and only 11% less recall error than the lexicalized model,,
 7 A11 our results are computed with the evalb program following the now-standard criteria in CITATION,,
 tical left-corner parser CITATION, and a PCFG CITATION,,
 The Tags model also does much better than the only other broad coverage neural network parser CITATION,,
 The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq&gt;200) and five recent statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION),,
 The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history (CITATION; CITATION; CITATION),,
res CITATION,,
 The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (CITATION; CITATION),,
 The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in CITATION,,
 Log-linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers CITATION,,
 tical left-corner parser CITATION, and a PCFG CITATION,,
 The Tags model also does much better than the only other broad coverage neural network parser CITATION,,
 The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq&gt;200) and five recent statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION),,
 The best current model CITATION has only 6% less precision error and only 11% less recall error than the lexicalized model,,
 1 Introduction Many statistical parsers (CITATION; CITATION; CITATION) are based on a history-based probability model CITATION, where the probability of each decision in a parse is conditioned on the previous decisions in the parse,,
 Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (CITATION; CITATION; CITATION),,
 CITATION define a kernel over parse trees and apply it to re-ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins\&apos; previous work on re-ranking using a finite set of features CITATION,,
 The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (CITATION; CITATION),,
 1 Introduction Many statistical parsers (CITATION; CITATION; CITATION) are based on a history-based probability model CITATION, where the probability of each decision in a parse is conditioned on the previous decisions in the parse,,
 Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (CITATION; CITATION; CITATION),,
 The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history (CITATION; CITATION; CITATION),,
 We used a publicly available tagger CITATION to tag the words and then used these in the input to the system,,
 7 A11 our results are computed with the evalb program following the now-standard criteria in CITATION,,
 tical left-corner parser CITATION, and a PCFG CITATION,,
 CITATION define a kernel over parse trees and apply it to re-ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins\&apos; previous work on re-ranking using a finite set of features CITATION,,
 The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (CITATION; CITATION),,
 The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in CITATION,,
 Log-linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers CITATION,,
 tical left-corner parser CITATION, and a PCFG CITATION,,
 The Tags model also does much better than the only other broad coverage neural network parser CITATION,,
 The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq&gt;200) and five recent statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION),,
 The best current model CITATION has only 6% less precision error and only 11% less recall error than the lexicalized model,,
 We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (CITATION; CITATION),,
 The resulting parser achieves performance far greater than previous approaches to neural network parsing (CITATION; CITATION), and only marginally below the current state-of-the-art for parsing the Penn Treebank,,
 7 The top panel of table 1 lists the results for the non-lexicalized model (SSN-Tags) and the available results for three other models which only use part-of-speech tags as inputs, another neural network parser CITATION, an earlier statis5 In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus,,
 We used a publicly available tagger CITATION to tag the words and then used these in the input to the system,,
 tical left-corner parser CITATION, and a PCFG CITATION,,
 The Tags model also does much better than the only other broad coverage neural network parser CITATION,,
 The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq&gt;200) and five recent statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION),,
 The best current model CITATION has only 6% less precision error and only 11% less recall error than the lexicalized model,,
 Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (CITATION; CITATION; CITATION),,
 We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (CITATION; CITATION),,
 The resulting parser achieves performance far greater than previous approaches to neural network parsing (CITATION; CITATION), and only marginally below the current state-of-the-art for parsing the Penn Treebank,,
 CITATION define a kernel over parse trees and apply it to re-ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins\&apos; previous work on re-ranking using a finite set of features CITATION,,
 The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (CITATION; CITATION),,
 The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in CITATION,,
 Log-linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers CITATION,,
 We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (CITATION; CITATION),,
 The resulting parser achieves performance far greater than previous approaches to neural network parsing (CITATION; CITATION), and only marginally below the current state-of-the-art for parsing the Penn Treebank,,
 Multi-layered neural networks of this form can approximate arbitrary mappings from inputs to outputs CITATION, whereas a loglinear model alone can only estimate probabilities where the category-conditioned probability distributions P(xidi) of the pre-defined inputs x are in a restricted form of the exponential family CITATION,,
 4We use the cross-entropy error function, which ensures that the minimum of the error function converges to the desired probabilities as the amount of training data increases CITATION,,
 For right-branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial CITATION, as has conditioning on the left-corner child CITATION,,
 Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (CITATION; CITATION; CITATION),,
 We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (CITATION; CITATION),,
 The resulting parser achieves performance far greater than previous approaches to neural network parsing (CITATION; CITATION), and only marginally below the current state-of-the-art for parsing the Penn Treebank,,
 CITATION define a kernel over parse trees and apply it to re-ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins\&apos; previous work on re-ranking using a finite set of features CITATION,,
 The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (CITATION; CITATION),,
 The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in CITATION,,
 Log-linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers CITATION,,
 The ordering which we use here is that of a form of left-corner parser CITATION,,
 We use the binarized version of a leftcorner parser, described in CITATION, where the parse of each non-leftmost child begins with the parent node predicting the child\&apos;s leftmost terminal, and ends with the child\&apos;s root nonterminal attaching to the parent,,
 7 A11 our results are computed with the evalb program following the now-standard criteria in CITATION,,
 tical left-corner parser CITATION, and a PCFG CITATION,,
 The Tags model also does much better than the only other broad coverage neural network parser CITATION,,
 The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq&gt;200) and five recent statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION),,
 135 \x0c6 The Experimental Results The generality and efficiency of the above parsing model makes it possible to test a SSN parser on the Penn Treebank CITATION, and thereby compare its performance directly to other statistical parsing models in the literature,,
 7 The top panel of table 1 lists the results for the non-lexicalized model (SSN-Tags) and the available results for three other models which only use part-of-speech tags as inputs, another neural network parser CITATION, an earlier statis5 In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus,,
 We used a publicly available tagger CITATION to tag the words and then used these in the input to the system,,
 7 A11 our results are computed with the evalb program following the now-standard criteria in CITATION,,
 1 Introduction Many statistical parsers (CITATION; CITATION; CITATION) are based on a history-based probability model CITATION, where the probability of each decision in a parse is conditioned on the previous decisions in the parse,,
 Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (CITATION; CITATION; CITATION),,
 The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history (CITATION; CITATION; CITATION),,
ture space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins\&apos; previous work on re-ranking using a finite set of features CITATION,,
 The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (CITATION; CITATION),,
 The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in CITATION,,
 Log-linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers CITATION,,
 tical left-corner parser CITATION, and a PCFG CITATION,,
 The Tags model also does much better than the only other broad coverage neural network parser CITATION,,
 The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq&gt;200) and five recent statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION),,
 The best current model CITATION has only 6% less precision error and only 11% less recall error than the lexicalized model,,
 For right-branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial CITATION, as has conditioning on the left-corner child CITATION,,
 The ordering which we use here is that of a form of left-corner parser CITATION,,
 We use the binarized version of a leftcorner parser, described in CITATION, where the parse of each non-leftmost child begins with the parent node predicting the child\&apos;s leftmost terminal, and ends with the child\&apos;s root nonterminal attaching to the parent,,
