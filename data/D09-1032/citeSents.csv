1 Introduction The most commonly used evaluation method for summarization during system development and for reporting results in publications is the automatic evaluation metric ROUGE (CITATION; CITATION),,
ROUGE evaluation: NIST also evaluated the summaries automatically using ROUGE (CITATION; CITATION),,
This task is reminiscent of the novelty detection task explored at TREC CITATION,,
In a simpler approach, CITATION use higher order ROUGE scores to approximate both content and linguistic quality,,
4.3 Use of topic words in the summary Summarization systems that directly optimize for more topic signatures during content selection have fared very well in evaluations CITATION,,
Pyramid evaluation: The pyramid evaluation method CITATION has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations CITATION,,
ization during system development and for reporting results in publications is the automatic evaluation metric ROUGE (CITATION; CITATION),,
306 \x0cFor example in CITATION, a large scale fully automatic evaluation of eight summarization systems on 18,000 documents was performed without any human effort,,
The motivation for their work was the considerable variation in content selection choices in model summaries CITATION,,
CITATION proposed a method for coherence evaluation which holds promise but has not been validated so far on large datasets such as those used in TAC and DUC,,
In CITATION, KL and JS divergences between human and machine summary distributions were used to evaluate content selection,,
The identity of the model writer significantly affects summary evaluations (also noted by CITATION, CITATION) and evaluations of the same systems can be rather different when different models are used,,
Vectors contain only topic signatures from the input and all words of the summary Topic signatures are words highly descriptive of the input, as determined by the application of loglikelihood test CITATION,,
ROUGE scores also correlate well with manual evaluations of content based on comparison with a single model summary, as used in the early editions of the Document Understanding Conferences CITATION,,
