Examples of such techniques are Markov Random Fields (CITATION; CITATION; Della CITATION; CITATION), and boosting or perceptron approaches to reranking (CITATION; CITATION; CITATION).,,
(1999) and CITATION use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large.,,
Examples of such techniques are Markov Random Fields (CITATION; CITATION; Della CITATION; CITATION), and boosting or perceptron approaches to reranking (CITATION; CITATION; CITATION).,,
(1999) and CITATION use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large.,,
CITATION and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming methods (Geman and Johnso,,
Examples of such techniques are Markov Random Fields (CITATION; CITATION; Della CITATION; CITATION), and boosting or perceptron approaches to reranking (CITATION; CITATION; CITATION).,,
(1999) and CITATION use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large.,,
CITATION and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming,,
One way around this problem is to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of CITATION.,,
In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of CITATIONa) and CITATION.,,
We will briefly review the perceptron algorithm, and its convergence properties see CITATION for a full description.,,
ogramming methods (CITATION; CITATION) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model.,,
This paper explores an alternative approach to parsing, based on the perceptron training algorithm introduced in CITATION.,,
2.1 Linear Models for NLP We follow the framework outlined in CITATION; 2004).,,
to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of CITATION.,,
In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of CITATIONa) and CITATION.,,
We will briefly review the perceptron algorithm, and its convergence properties see CITATION for a full description.,,
The algorithm and theorems are based on the approach to classification problems described in CITATION.,,
All of the convergence and generalization results in CITATION depend on notions of separability rather than the size of GEN. Two questions come to mind.,,
CITATION discuss how the theory for classification problems can be extended to deal with both of these questions; CITATION describes how these results apply to NLP problems.,,
As a final note, following CITATION, we used the averaged pa,,
For this paper, we used POS tags that were provided either by the Treebank itself (gold standard tags) or by the perceptron POS tagger3 presented in CITATION.,,
Note that the allowable chains in our grammar are what CITATION call connection paths from the partial parse to the next word.,,
Examples of such techniques are Markov Random Fields (CITATION; CITATION; Della CITATION; CITATION), and boosting or perceptron approaches to reranking (CITATION; CITATION; CITATION).,,
(1999) and CITATION use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large.,,
CITATION and Collins and Duffy (2,,
In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of CITATIONa) and CITATION.,,
We will briefly review the perceptron algorithm, and its convergence properties see CITATION for a full description.,,
The algorithm and theorems are based on the approach to classification problems described in CITATION.,,
All of the convergence and generalization results in CITATION depend on notions of separability rather than the size of GEN. Two questions come to mind.,,
CITATION discuss how the theory for classification problems can be extended to deal with both of these questions; CITATION describes how these results apply to NLP problems.,,
As a final note, following CITATION, we used the averaged parameters from the training algorithm in decoding test examples in our experiments.,,
CITATION originally proposed the averaged parameter method; i,,
Examples of such techniques are Markov Random Fields (CITATION; CITATION; Della CITATION; CITATION), and boosting or perceptron approaches to reranking (CITATION; CITATION; CITATION).,,
(1999) and CITATION use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large.,,
CITATION and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dyna,,
(1999) and CITATION use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large.,,
CITATION and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming methods (CITATION; CITATION) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model.,,
This paper explores an alternative approach to parsing, based on the perceptron training algorithm introduced in CITATION.,,
Before inducing the left-child chains and allowable triples from the treebank, the trees are transformed with a selective left-corner transformation CITATION that has been flattened as presented in CITATIONb).,,
Examples of such techniques are Markov Random Fields (CITATION; CITATION; Della CITATION; CITATION), and boosting or perceptron approaches to reranking (CITATION; CITATION; CITATION).,,
(1999) and CITATION use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large.,,
CITATION and Collins and Duffy (2002) rerank the top N p,,
When the FSLC has been applied and the set is restricted to those occurring more than once 2See CITATION for a presentation of the transform/detransform paradigm in parsing.,,
(1999) and CITATION use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large.,,
CITATION and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming methods (CITATION; CITATION) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model.,,
This paper explores an alternative approach to parsing, based on the perceptron training algorithm introduced in CITATION.,,
Examples of such techniques are Markov Random Fields (CITATION; CITATION; Della CITATION; CITATION), and boosting or perceptron approaches to reranking (CITATION; CITATION; CITATION).,,
(1999) and CITATION use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large.,,
One way around this problem is to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of CITATION.,,
In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of CITATIONa) and CITATION.,,
We will briefly review the perceptron algorithm, and its convergence properties see CITATION for a full description.,,
The algorithm and theorems are based on the approach to classification problems described in CITATION.,,
Examples of such techniques are Markov Random Fields (CITATION; CITATION; Della CITATION; CITATION), and boosting or perceptron approaches to reranking (CITATION; CITATION; CITATION).,,
(1999) and CITATION use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large.,,
CITATION and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming methods (CITATION; CITATION) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model.,,
This sort of scenario was used in CITATION for training an n-gram language model using the perceptron algorithm.,,
We implemented the perceptron approach with the same feature set as that of an existing generative model (CITATIONa), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search.,,
One way around this problem is to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of CITATION.,,
In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of CITATIONa) and CITATION.,,
We will briefly review the perceptron algorithm, and its convergence properties see CITATION for a full description.,,
The algorithm and theorems are based on the approach to classification problems described in CITATION.,,
3 A full description of the parsing approach The parser is an incremental beam-search parser very similar to the sort described in CITATIONa; 2004), with some changes in the search strategy to accommodate the perceptron feature weights.,,
Before inducing the left-child chains and allowable triples from the treebank, the trees are transformed with a selective left-corner transformation CITATION that has been flattened as presented in CITATIONb).,,
Unlike in CITATIONa; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word.,,
We then built a generative model with this feature set and the same tree transform, for use with the beam-search parser from CITATION to compare against our baseline perceptron model.,,
We implemented the perceptron approach with the same feature set as that of an existing generative model (CITATIONa), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search.,,
One way around this problem is to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of CITATION.,,
In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of CITATIONa) and CITATION.,,
We will briefly review the perceptron algorithm, and its convergence properties see CITATION for a full description.,,
The algorithm and theorems are based on the approach to classification problems described in CITATION.,,
3 A full description of the parsing approach The parser is an incremental beam-search parser very similar to the sort described in CITATIONa; 2004), with some changes in the search strategy to accommodate the perceptron feature weights.,,
Before inducing the left-child chains and allowable triples from the treebank, the trees are transformed with a selective left-corner transformation CITATION that has been flattened as presented in CITATIONb).,,
Unlike in CITATIONa; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word.,,
We then built a generative model with this feature set and the same tree transform, for use with the beam-search parser from CITATION to compare against our baseline perceptron model.,,
Unlike in CITATIONa; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word.,,
We then built a generative model with this feature set and the same tree transform, for use with the beam-search parser from CITATION to compare against our baseline perceptron model.,,
