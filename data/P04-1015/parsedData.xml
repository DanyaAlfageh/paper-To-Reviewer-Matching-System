<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<figure confidence="0.288186333333333">
b&amp;apos;Incremental Parsing with the Perceptron Algorithm
Michael Collins
MIT CSAIL
</figure>
<email confidence="0.840055">
mcollins@csail.mit.edu
</email>
<author confidence="0.799562">
Brian Roark
</author>
<affiliation confidence="0.601668">
AT&amp;T Labs - Research
</affiliation>
<email confidence="0.966352">
roark@research.att.com
</email>
<sectionHeader confidence="0.988353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996343615384615">
This paper describes an incremental parsing approach
where parameters are estimated using a variant of the
perceptron algorithm. A beam-search algorithm is used
during both training and decoding phases of the method.
The perceptron approach was implemented with the
same feature set as that of an existing generative model
(Roark, 2001a), and experimental results show that it
gives competitive performance to the generative model
on parsing the Penn treebank. We demonstrate that train-
ing a perceptron model to combine with the generative
model during search provides a 2.1 percent F-measure
improvement over the generative model alone, to 88.8
percent.
</bodyText>
<sectionHeader confidence="0.997885" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988318602739726">
In statistical approaches to NLP problems such as tag-
ging or parsing, it seems clear that the representation
used as input to a learning algorithm is central to the ac-
curacy of an approach. In an ideal world, the designer
of a parser or tagger would be free to choose any fea-
tures which might be useful in discriminating good from
bad structures, without concerns about how the features
interact with the problems of training (parameter estima-
tion) or decoding (search for the most plausible candidate
under the model). To this end, a number of recently pro-
posed methods allow a model to incorporate arbitrary
global features of candidate analyses or parses. Exam-
ples of such techniques are Markov Random Fields (Rat-
naparkhi et al., 1994; Abney, 1997; Della Pietra et al.,
1997; Johnson et al., 1999), and boosting or perceptron
approaches to reranking (Freund et al., 1998; Collins,
2000; Collins and Duffy, 2002).
A drawback of these approaches is that in the general
case, they can require exhaustive enumeration of the set
of candidates for each input sentence in both the train-
ing and decoding phases1
. For example, Johnson et al.
(1999) and Riezler et al. (2002) use all parses generated
by an LFG parser as input to an MRF approach given
the level of ambiguity in natural language, this set can
presumably become extremely large. Collins (2000) and
Collins and Duffy (2002) rerank the top N parses from
an existing generative parser, but this kind of approach
1Dynamic programming methods (Geman and Johnson, 2002; Laf-
ferty et al., 2001) can sometimes be used for both training and decod-
ing, but this requires fairly strong restrictions on the features in the
model.
presupposes that there is an existing baseline model with
reasonable performance. Many of these baseline models
are themselves used with heuristic search techniques, so
that the potential gain through the use of discriminative
re-ranking techniques is further dependent on effective
search.
This paper explores an alternative approach to pars-
ing, based on the perceptron training algorithm intro-
duced in Collins (2002). In this approach the training
and decoding problems are very closely related the
training method decodes training examples in sequence,
and makes simple corrective updates to the parameters
when errors are made. Thus the main complexity of the
method is isolated to the decoding problem. We describe
an approach that uses an incremental, left-to-right parser,
with beam search, to find the highest scoring analysis un-
der the model. The same search method is used in both
training and decoding. We implemented the perceptron
approach with the same feature set as that of an existing
generative model (Roark, 2001a), and show that the per-
ceptron model gives performance competitive to that of
the generative model on parsing the Penn treebank, thus
demonstrating that an unnormalized discriminative pars-
ing model can be applied with heuristic search. We also
describe several refinements to the training algorithm,
and demonstrate their impact on convergence properties
of the method.
Finally, we describe training the perceptron model
with the negative log probability given by the generative
model as another feature. This provides the perceptron
algorithm with a better starting point, leading to large
improvements over using either the generative model or
the perceptron algorithm in isolation (the hybrid model
achieves 88.8% f-measure on the WSJ treebank, com-
pared to figures of 86.7% and 86.6% for the separate
generative and perceptron models). The approach is an
extremely simple method for integrating new features
into the generative model: essentially all that is needed
is a definition of feature-vector representations of entire
parse trees, and then the existing parsing algorithms can
be used for both training and decoding with the models.
</bodyText>
<sectionHeader confidence="0.985418" genericHeader="method">
2 The General Framework
</sectionHeader>
<bodyText confidence="0.99797375">
In this section we describe a general framework linear
models for NLP that could be applied to a diverse range
of tasks, including parsing and tagging. We then describe
a particular method for parameter estimation, which is a
generalization of the perceptron algorithm. Finally, we
\x0cgive an abstract description of an incremental parser, and
describe how it can be used with the perceptron algo-
rithm.
</bodyText>
<subsectionHeader confidence="0.931763">
2.1 Linear Models for NLP
</subsectionHeader>
<bodyText confidence="0.930758866666667">
We follow the framework outlined in Collins (2002;
2004). The task is to learn a mapping from inputs x X
to outputs y Y. For example, X might be a set of sen-
tences, with Y being a set of possible parse trees. We
assume:
.Training examples (xi, yi) for i = 1 . . . n.
.A function GEN which enumerates a set of candi-
dates GEN(x) for an input x.
.A representation mapping each (x, y) X Y
to a feature vector (x, y) Rd
.
.A parameter vector Rd
.
The components GEN, and define a mapping from
an input x to an output F(x) through
</bodyText>
<equation confidence="0.9983802">
F(x) = arg max
yGEN(x)
(x, y) (1)
where (x, y) is the inner product
P
</equation>
<bodyText confidence="0.989063520833333">
s ss(x, y).
The learning task is to set the parameter values using
the training examples as evidence. The decoding algo-
rithm is a method for searching for the arg max in Eq. 1.
This framework is general enough to encompass sev-
eral tasks in NLP. In this paper we are interested in pars-
ing, where (xi, yi), GEN, and can be defined as fol-
lows:
Each training example (xi, yi) is a pair where xi is
a sentence, and yi is the gold-standard parse for that
sentence.
Given an input sentence x, GEN(x) is a set of
possible parses for that sentence. For example,
GEN(x) could be defined as the set of possible
parses for x under some context-free grammar, per-
haps a context-free grammar induced from the train-
ing examples.
The representation (x, y) could track arbitrary
features of parse trees. As one example, suppose
that there are m rules in a context-free grammar
(CFG) that defines GEN(x). Then we could define
the ith component of the representation, i(x, y),
to be the number of times the ith context-free rule
appears in the parse tree (x, y). This is implicitly
the representation used in probabilistic or weighted
CFGs.
Note that the difficulty of finding the arg max in Eq. 1
is dependent on the interaction of GEN and . In many
cases GEN(x) could grow exponentially with the size
of x, making brute force enumeration of the members
of GEN(x) intractable. For example, a context-free
grammar could easily produce an exponentially growing
number of analyses with sentence length. For some rep-
resentations, such as the rule-based representation de-
scribed above, the arg max in the set enumerated by the
CFG can be found efficiently, using dynamic program-
ming algorithms, without having to explicitly enumer-
ate all members of GEN(x). However in many cases
we may be interested in representations which do not al-
low efficient dynamic programming solutions. One way
around this problem is to adopt a two-pass approach,
where GEN(x) is the top N analyses under some initial
model, as in the reranking approach of Collins (2000).
In the current paper we explore alternatives to rerank-
ing approaches, namely heuristic methods for finding the
arg max, specifically incremental beam-search strategies
related to the parsers of Roark (2001a) and Ratnaparkhi
(1999).
</bodyText>
<subsectionHeader confidence="0.9260175">
2.2 The Perceptron Algorithm for Parameter
Estimation
</subsectionHeader>
<bodyText confidence="0.971002384615385">
We now consider the problem of setting the parameters,
, given training examples (xi, yi). We will briefly re-
view the perceptron algorithm, and its convergence prop-
erties see Collins (2002) for a full description. The
algorithm and theorems are based on the approach to
classification problems described in Freund and Schapire
(1999).
Figure 1 shows the algorithm. Note that the
most complex step of the method is finding zi =
arg maxzGEN(xi) (xi, z) and this is precisely the
decoding problem. Thus the training algorithm is in prin-
ciple a simple part of the parser: any system will need
a decoding method, and once the decoding algorithm is
implemented the training algorithm is relatively straight-
forward.
We will now give a first theorem regarding the con-
vergence of this algorithm. First, we need the following
definition:
Definition 1 Let GEN(xi) = GEN(xi) {yi}. In
other words GEN(xi) is the set of incorrect candidates
for an example xi. We will say that a training sequence
(xi, yi) for i = 1 . . . n is separable with margin &gt; 0
if there exists some vector U with ||U ||= 1 such that
i, z GEN(xi), U (xi, yi) U (xi, z)
(2)
(||U ||is the 2-norm of U, i.e., ||U ||=
</bodyText>
<equation confidence="0.867183">
pP
s U2
s.)
</equation>
<bodyText confidence="0.999156428571429">
Next, define Ne to be the number of times an error is
made by the algorithm in figure 1 that is, the number of
times that zi 6= yi for some (t, i) pair. We can then state
the following theorem (see (Collins, 2002) for a proof):
Theorem 1 For any training sequence (xi, yi) that is
separable with margin , for any value of T, then for
the perceptron algorithm in figure 1
</bodyText>
<equation confidence="0.692393">
Ne
R2
</equation>
<page confidence="0.831636">
2
</page>
<bodyText confidence="0.958387333333333">
where R is a constant such that i, z
GEN(xi) ||(xi, yi) (xi, z) ||R.
This theorem implies that if there is a parameter vec-
tor U which makes zero errors on the training set, then
after a finite number of iterations the training algorithm
will converge to parameter values with zero training er-
ror. A crucial point is that the number of mistakes is in-
dependent of the number of candidates for each example
\x0cInputs: Training examples (xi, yi) Algorithm:
Initialization: Set = 0 For t = 1 . . . T, i = 1 . . . n
Output: Parameters Calculate zi = arg maxzGEN(xi) (xi, z)
If(zi 6= yi) then = + (xi, yi) (xi, zi)
</bodyText>
<figureCaption confidence="0.995989">
Figure 1: A variant of the perceptron algorithm.
</figureCaption>
<bodyText confidence="0.984182086956522">
(i.e. the size of GEN(xi) for each i), depending only
on the separation of the training data, where separation
is defined above. This is important because in many NLP
problems GEN(x) can be exponential in the size of the
inputs. All of the convergence and generalization results
in Collins (2002) depend on notions of separability rather
than the size of GEN.
Two questions come to mind. First, are there guar-
antees for the algorithm if the training data is not sepa-
rable? Second, performance on a training sample is all
very well, but what does this guarantee about how well
the algorithm generalizes to newly drawn test examples?
Freund and Schapire (1999) discuss how the theory for
classification problems can be extended to deal with both
of these questions; Collins (2002) describes how these
results apply to NLP problems.
As a final note, following Collins (2002), we used the
averaged parameters from the training algorithm in de-
coding test examples in our experiments. Say t
i is the
parameter vector after the ith example is processed on
the tth pass through the data in the algorithm in fig-
ure 1. Then the averaged parameters AV G are defined
</bodyText>
<equation confidence="0.936882333333333">
as AV G =
P
i,t t
</equation>
<bodyText confidence="0.993771">
i/NT. Freund and Schapire (1999)
originally proposed the averaged parameter method; it
was shown to give substantial improvements in accuracy
for tagging tasks in Collins (2002).
</bodyText>
<subsectionHeader confidence="0.9604195">
2.3 An Abstract Description of Incremental
Parsing
</subsectionHeader>
<bodyText confidence="0.984657725">
This section gives a description of the basic incremental
parsing approach. The input to the parser is a sentence
x with length n. A hypothesis is a triple hx, t, ii such
that x is the sentence being parsed, t is a partial or full
analysis of that sentence, and i is an integer specifying
the number of words of the sentence which have been
processed. Each full parse for a sentence will have the
form hx, t, ni. The initial state is hx, , 0i where is a
null or empty analysis.
We assume an advance function ADV which takes
a hypothesis triple as input, and returns a set of new hy-
potheses as output. The advance function will absorb
another word in the sentence: this means that if the input
to ADV is hx, t, ii, then each member of ADV(hx, t, ii)
will have the form hx, t0
,i+1i. Each new analysis t0
will
be formed by somehow incorporating the i+1th word
into the previous analysis t.
With these definitions in place, we can iteratively de-
fine the full set of partial analyses Hi for the first i words
of the sentence as H0(x) = {hx, , 0i}, and Hi(x) =
h0Hi1(x)ADV(h0
) for i = 1 . . . n. The full set of
parses for a sentence x is then GEN(x) = Hn(x) where
n is the length of x.
Under this definition GEN(x) can include a huge
number of parses, and searching for the highest scor-
ing parse, arg maxhHn(x) (h) , will be intractable.
For this reason we introduce one additional function,
FILTER(H), which takes a set of hypotheses H, and re-
turns a much smaller set of filtered hypotheses. Typi-
cally, FILTER will calculate the score (h) for each
h H, and then eliminate partial analyses which have
low scores under this criterion. For example, a simple
version of FILTER would take the top N highest scoring
members of H for some constant N. We can then rede-
fine the set of partial analyses as follows (we use Fi(x)
to denote the set of filtered partial analyses for the first i
words of the sentence):
</bodyText>
<equation confidence="0.948625">
F0(x) = {hx, , 0i}
Fi(x) = FILTER h0Fi1(x)ADV(h0
)
\x01
for i=1 . . . n
</equation>
<bodyText confidence="0.979202722222222">
The parsing algorithm returns arg maxhFn
(h) .
Note that this is a heuristic, in that there is no guar-
antee that this procedure will find the highest scoring
parse, arg maxhHn (h) . Search errors, where
arg maxhFn
(h) 6= arg maxhHn
(h) , will
create errors in decoding test sentences, and also errors in
implementing the perceptron training algorithm in Fig-
ure 1. In this paper we give empirical results that suggest
that FILTER can be chosen in such a way as to give ef-
ficient parsing performance together with high parsing
accuracy.
The exact implementation of the parser will depend on
the definition of partial analyses, of ADV and FILTER,
and of the representation . The next section describes
our instantiation of these choices.
</bodyText>
<sectionHeader confidence="0.796675" genericHeader="method">
3 A full description of the parsing
</sectionHeader>
<bodyText confidence="0.976399428571429">
approach
The parser is an incremental beam-search parser very
similar to the sort described in Roark (2001a; 2004), with
some changes in the search strategy to accommodate the
perceptron feature weights. We first describe the parsing
algorithm, and then move on to the baseline feature set
for the perceptron model.
</bodyText>
<subsectionHeader confidence="0.999701">
3.1 Parser control
</subsectionHeader>
<bodyText confidence="0.86058175">
The input to the parser is a string wn
0 , a grammar G, a
mapping from derivations to feature vectors, and a pa-
rameter vector . The grammar G = (V, T, S
</bodyText>
<equation confidence="0.695593">
, S, C, B)
</equation>
<bodyText confidence="0.913586">
consists of a set of non-terminal symbols V , a set of ter-
minal symbols T, a start symbol S
V , an end-of-
constituent symbol S V , a set of allowable chains C,
and a set of allowable triples B. S is a special empty
non-terminal that marks the end of a constituent. Each
chain is a sequence of non-terminals followed by a ter-
minal symbol, for example hS
</bodyText>
<figure confidence="0.582705055555556">
S NP NN
\x0cS
S
!
!
NP
NN
Trash
. . . . . . . . . . . . .
NN
can
. . . . . . . . . . . . . . . VP
MD
can
. . . . . . . . . . . . . . . . . . . VP
VP
MD
can
</figure>
<figureCaption confidence="0.991588">
Figure 2: Left child chains and connection paths. Dotted
</figureCaption>
<bodyText confidence="0.9117742">
lines represent potential attachments
Trashi. Each allowable triple is a tuple hX, Y, Zi
where X, Y, Z V . The triples specify which non-
terminals Z are allowed to follow a non-terminal Y un-
der a parent X. For example, the triple hS,NP,VPi
specifies that a VP can follow an NP under an S. The
triple hNP,NN,Si would specify that the S symbol can
follow an NN under an NP i.e., that the symbol NN is
allowed to be the final child of a rule with parent NP
The initial state of the parser is the input string alone,
</bodyText>
<equation confidence="0.900761833333333">
wn
0 . In absorbing the first word, we add all chains of the
form S
. . . w0. For example, in figure 2 the chain
hS
S NP NN Trashi is used to construct
</equation>
<bodyText confidence="0.992622235294118">
an analysis for the first word alone. Other chains which
start with S
and end with Trash would give competing
analyses for the first word of the string.
Figure 2 shows an example of how the next word in
a sentence can be incorporated into a partial analysis for
the previous words. For any partial analysis there will
be a set of potential attachment sites: in the example, the
attachment sites are under the NP or the S. There will
also be a set of possible chains terminating in the next
word there are three in the example. Each chain could
potentially be attached at each attachment site, giving
6 ways of incorporating the next word in the example.
For illustration, assume that the set B is {hS,NP,VPi,
hNP,NN,NNi, hNP,NN,Si, hS,NP,VPi}. Then some
of the 6 possible attachments may be disallowed because
they create triples that are not in the set B. For example,
in figure 2 attaching either of the VP chains under the
NP is disallowed because the triple hNP,NN,VPi is not
in B. Similarly, attaching the NN chain under the S will
be disallowed if the triple hS,NP,NNi is not in B. In
contrast, adjoining hNN cani under the NP creates a
single triple, hNP,NN,NNi, which is allowed. Adjoining
either of the VP chains under the S creates two triples,
hS,NP,VPi and hNP,NN,Si, which are both in the set
B.
Note that the allowable chains in our grammar are
what Costa et al. (2001) call connection paths from
the partial parse to the next word. It can be shown that
the method is equivalent to parsing with a transformed
context-free grammar (a first-order Markov grammar)
for brevity we omit the details here.
In this way, given a set of candidates Fi(x) for the first
i words of the string, we can generate a set of candidates
</bodyText>
<table confidence="0.999563">
Tree POS f24 f2-21 f2-21, # &gt; 1
transform tags Type Type OOV Type OOV
None Gold 386 1680 0.1% 1013 0.1%
None Tagged 401 1776 0.1% 1043 0.2%
FSLC Gold 289 1214 0.1% 746 0.1%
FSLC Tagged 300 1294 0.1% 781 0.1%
</table>
<tableCaption confidence="0.997765">
Table 1: Left-child chain type counts (of length &gt; 2) for
</tableCaption>
<bodyText confidence="0.993575909090909">
sections of the Wall St. Journal Treebank, and out-of-
vocabulary (OOV) rate on the held-out corpus.
for the first i + 1 words, h0Fi(x)ADV(h0
), where the
ADV function uses the grammar as described above. We
then calculate (h) for all of these partial hypotheses,
and rank the set from best to worst. A FILTER function is
then applied to this ranked set to give Fi+1. Let hk be the
kth ranked hypothesis in Hi+1(x). Then hk Fi+1 if
and only if (hk) k. In our case, we parameterize
the calculation of k with as follows:
</bodyText>
<equation confidence="0.999414666666667">
k = (h0)
k3
. (3)
</equation>
<bodyText confidence="0.992312457142857">
The problem with using left-child chains is limiting
them in number. With a left-recursive grammar, of
course, the set of all possible left-child chains is infinite.
We use two techniques to reduce the number of left-child
chains: first, we remove some (but not all) of the recur-
sion from the grammar through a tree transform; next,
we limit the left-child chains consisting of more than
two non-terminal categories to those actually observed
in the training data more than once. Left-child chains of
length less than or equal to two are all those observed
in training data. As a practical matter, the set of left-
child chains for a terminal x is taken to be the union of
the sets of left-child chains for all pre-terminal part-of-
speech (POS) tags T for x.
Before inducing the left-child chains and allowable
triples from the treebank, the trees are transformed with a
selective left-corner transformation (Johnson and Roark,
2000) that has been flattened as presented in Roark
(2001b). This transform is only applied to left-recursive
productions, i.e. productions of the form A A.
The transformed trees look as in figure 3. The transform
has the benefit of dramatically reducing the number of
left-child chains, without unduly disrupting the immedi-
ate dominance relationships that provide features for the
model. The parse trees that are returned by the parser are
then de-transformed to the original form of the grammar
for evaluation2
.
Table 1 presents the number of left-child chains of
length greater than 2 in sections 2-21 and 24 of the Penn
Wall St. Journal Treebank, both with and without the
flattened selective left-corner transformation (FSLC), for
gold-standard part-of-speech (POS) tags and automati-
cally tagged POS tags. When the FSLC has been applied
and the set is restricted to those occurring more than once
</bodyText>
<figure confidence="0.962031093750001">
2See Johnson (1998) for a presentation of the transform/de-
transform paradigm in parsing.
\x0c(a)
NP
\x10
\x10
\x10
\x10
NP
\x08
\x08
\x08
NP
\x11
\x11
NNP
Jim
b
b
POS
s
H
H
H
NN
dog
P
P
P
P
PP
,
,
IN
with . . .
l
l
NP
(b)
NP
\x18
\x18
\x18
\x18
\x18
NNP
Jim
POS
s
X
X
X
X
X
NP/NP
\x08
\x08
\x08
NN
dog
H
H
H
NP/NP
PP
\x11
\x11
IN
with .. .
l
l
NP
(c)
NP
NNP
Jim
!
!
!
POS
s
l
l
NP/NP
NN
dog
``````
NP/NP
PP
,
,
IN
with . . .
l
l
NP
</figure>
<figureCaption confidence="0.999182">
Figure 3: Three representations of NP modifications: (a) the original treebank representation; (b) Selective left-corner
</figureCaption>
<bodyText confidence="0.541922">
representation; and (c) a flat structure that is unambiguously equivalent to (b)
</bodyText>
<equation confidence="0.999938">
F0 = {L00, L10} F4 = F3 {L03} F8 = F7 {L21} F12 = F11 {L11}
F1 = F0 {LKP} F5 = F4 {L20} F9 = F8 {CL} F13 = F12 {L30}
F2 = F1 {L01} F6 = F5 {L11} F10 = F9 {LK} F14 = F13 {CCP}
F3 = F2 {L02} F7 = F6 {L30} F11 = F0 {L20} F15 = F14 {CC}
</equation>
<tableCaption confidence="0.958272">
Table 2: Baseline feature set. Features F0 F10 fire at non-terminal nodes. Features F0, F11 F15 fire at terminal
</tableCaption>
<bodyText confidence="0.955917666666667">
nodes.
in the training corpus, we can reduce the total number of
left-child chains of length greater than 2 by half, while
leaving the number of words in the held-out corpus with
an unobserved left-child chain (out-of-vocabulary rate
OOV) to just one in every thousand words.
</bodyText>
<subsectionHeader confidence="0.974878">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.965984380952381">
For this paper, we wanted to compare the results of a
perceptron model with a generative model for a compa-
rable feature set. Unlike in Roark (2001a; 2004), there
is no look-ahead statistic, so we modified the feature set
from those papers to explicitly include the lexical item
and POS tag of the next word. Otherwise the features
are basically the same as in those papers. We then built
a generative model with this feature set and the same
tree transform, for use with the beam-search parser from
Roark (2004) to compare against our baseline perceptron
model.
To concisely present the baseline feature set, let us
establish a notation. Features will fire whenever a new
node is built in the tree. The features are labels from the
left-context, i.e. the already built part of the tree. All
of the labels that we will include in our feature sets are
i levels above the current node in the tree, and j nodes
to the left, which we will denote Lij. Hence, L00 is the
node label itself; L10 is the label of parent of the current
node; L01 is the label of the sibling of the node, imme-
diately to its left; L11 is the label of the sibling of the
parent node, etc. We also include: the lexical head of the
current constituent (CL); the c-commanding lexical head
(CC) and its POS (CCP); and the look-ahead word (LK)
and its POS (LKP). All of these features are discussed at
more length in the citations above. Table 2 presents the
baseline feature set.
In addition to the baseline feature set, we will also
present results using features that would be more dif-
ficult to embed in a generative model. We included
some punctuation-oriented features, which included (i)
a Boolean feature indicating whether the final punctua-
tion is a question mark or not; (ii) the POS label of the
word after the current look-ahead, if the current look-
ahead is punctuation or a coordinating conjunction; and
(iii) a Boolean feature indicating whether the look-ahead
is punctuation or not, that fires when the category imme-
diately to the left of the current position is immediately
preceded by punctuation.
4 Refinements to the Training Algorithm
This section describes two modifications to the basic
training algorithm in figure 1.
</bodyText>
<subsectionHeader confidence="0.995124">
4.1 Making Repeated Use of Hypotheses
</subsectionHeader>
<bodyText confidence="0.974984666666666">
Figure 4 shows a modified algorithm for parameter es-
timation. The input to the function is a gold standard
parse, together with a set of candidates F generated
by the incremental parser. There are two steps. First,
the model is updated as usual with the current example,
which is then added to a cache of examples. Second, the
method repeatedly iterates over the cache, updating the
model at each cached example if the gold standard parse
is not the best scoring parse from among the stored can-
didates for that example. In our experiments, the cache
was restricted to contain the parses from up to N pre-
viously processed sentences, where N was set to be the
size of the training set.
The motivation for these changes is primarily effi-
ciency. One way to think about the algorithms in this
paper is as methods for finding parameter values that sat-
isfy a set of linear constraints one constraint for each
incorrect parse in training data. The incremental parser is
\x0cInput: A gold-standard parse = g for sentence k of N. A set of candidate parses F. Current parameters
. A Cache of triples hgj, Fj, cji for j = 1 . . . N where each gj is a previously generated gold standard
parse, Fj is a previously generated set of candidate parses, and cj is a counter of the number of times that
has been updated due to this particular triple. Parameters T1 and T2 controlling the number of iterations be-
low. In our experiments, T1 = 5 and T2 = 50. Initialize the Cache to include, for j = 1 . . . N, hgj, , T2i.
Step 1: Step 2:
Calculate z = arg maxtF (t) For t = 1 . . . T1, j = 1 . . . N
If (z 6= g) then = + (g) (z) If cj &amp;lt; T2 then
Set the kth triple in the Cache to hg, F, 0i Calculate z = arg maxtFj (t)
</bodyText>
<equation confidence="0.825034">
If (z 6= gj) then
= + (gj) (z)
cj = cj + 1
</equation>
<figureCaption confidence="0.997018">
Figure 4: The refined parameter update method makes repeated use of hypotheses
</figureCaption>
<bodyText confidence="0.99460775">
a method for dynamically generating constraints (i.e. in-
correct parses) which are violated, or close to being vio-
lated, under the current parameter settings. The basic al-
gorithm in Figure 1 is extremely wasteful with the gener-
ated constraints, in that it only looks at one constraint on
each sentence (the arg max), and it ignores constraints
implied by previously parsed sentences. This is ineffi-
cient because the generation of constraints (i.e., parsing
an input sentence), is computationally quite demanding.
More formally, it can be shown that the algorithm in
figure 4 also has the upper bound in theorem 1 on the
number of parameter updates performed. If the cost of
steps 1 and 2 of the method are negligible compared to
the cost of parsing a sentence, then the refined algorithm
will certainly converge no more slowly than the basic al-
gorithm, and may well converge more quickly.
As a final note, we used the parameters T1 and T2 to
limit the number of passes over examples, the aim being
to prevent repeated updates based on outlier examples
which are not separable.
</bodyText>
<subsectionHeader confidence="0.997364">
4.2 Early Update During Training
</subsectionHeader>
<bodyText confidence="0.9995716">
As before, define yi to be the gold standard parse for the
ith sentence, and also define yj
i to be the partial analy-
sis under the gold-standard parse for the first j words of
the ith sentence. Then if yj
</bodyText>
<equation confidence="0.745256">
i /
</equation>
<bodyText confidence="0.9731288">
Fj(xi) a search error has
been made, and there is no possibility of the gold stan-
dard parse yi being in the final set of parses, Fn(xi). We
call the following modification to the parsing algorithm
during training early update: if yj
</bodyText>
<equation confidence="0.71482">
i /
</equation>
<bodyText confidence="0.997066470588235">
Fj(xi), exit the
parsing process, pass yj
i , Fj(xi) to the parameter estima-
tion method, and move on to the next string in the train-
ing set. Intuitively, the motivation behind this is clear. It
makes sense to make a correction to the parameter values
at the point that a search error has been made, rather than
allowing the parser to continue to the end of the sentence.
This is likely to lead to less noisy input to the parameter
estimation algorithm; and early update will also improve
efficiency, as at the early stages of training the parser will
frequently give up after a small proportion of each sen-
tence is processed. It is more difficult to justify from a
formal point of view, we leave this to future work.
Figure 5 shows the convergence of the training algo-
rithm with neither of the two refinements presented; with
just early update; and with both. Early update makes
</bodyText>
<figure confidence="0.9360976">
1 2 3 4 5 6
82
83
84
85
86
87
88
Number of passes over training data
Fmeasure
parsing
accuracy
No early update, no repeated use of examples
Early update, no repeated use of examples
Early update, repeated use of examples
</figure>
<figureCaption confidence="0.996138">
Figure 5: Performance on development data (section f24)
</figureCaption>
<bodyText confidence="0.9645882">
after each pass over the training data, with and without
repeated use of examples and early update.
an enormous difference in the quality of the resulting
model; repeated use of examples gives a small improve-
ment, mainly in recall.
</bodyText>
<sectionHeader confidence="0.988636" genericHeader="method">
5 Empirical results
</sectionHeader>
<bodyText confidence="0.968121214285714">
The parsing models were trained and tested on treebanks
from the Penn Wall St. Journal Treebank: sections 2-21
were kept training data; section 24 was held-out devel-
opment data; and section 23 was for evaluation. After
each pass over the training data, the averaged perceptron
model was scored on the development data, and the best
performing model was used for test evaluation. For this
paper, we used POS tags that were provided either by
the Treebank itself (gold standard tags) or by the per-
ceptron POS tagger3
presented in Collins (2002). The
former gives us an upper bound on the improvement that
we might expect if we integrated the POS tagging with
the parsing.
</bodyText>
<footnote confidence="0.7696515">
3For trials when the generative or perceptron parser was given POS
tagger output, the models were trained on POS tagged sections 2-21,
which in both cases helped performance slightly.
\x0cModel Gold-standard tags POS-tagger tags
</footnote>
<table confidence="0.99419125">
LP LR F LP LR F
Generative 88.1 87.6 87.8 86.8 86.5 86.7
Perceptron (baseline) 87.5 86.9 87.2 86.2 85.5 85.8
Perceptron (w/ punctuation features) 88.1 87.6 87.8 87.0 86.3 86.6
</table>
<tableCaption confidence="0.999714">
Table 3: Parsing results, section 23, all sentences, including labeled precision (LP), labeled recall (LR), and F-measure
</tableCaption>
<bodyText confidence="0.98680055">
Table 3 shows results on section 23, when either gold-
standard or POS-tagger tags are provided to the parser4
.
With the base features, the generative model outperforms
the perceptron parser by between a half and one point,
but with the additional punctuation features, the percep-
tron model matches the generative model performance.
Of course, using the generative model and using the
perceptron algorithm are not necessarily mutually ex-
clusive. Another training scenario would be to include
the generative model score as another feature, with some
weight in the linear model learned by the perceptron al-
gorithm. This sort of scenario was used in Roark et al.
(2004) for training an n-gram language model using the
perceptron algorithm. We follow that paper in fixing the
weight of the generative model, rather than learning the
weight along the the weights of the other perceptron fea-
tures. The value of the weight was empirically optimized
on the held-out set by performing trials with several val-
ues. Our optimal value was 10.
In order to train this model, we had to provide gen-
erative model scores for strings in the training set. Of
course, to be similar to the testing conditions, we can-
not use the standard generative model trained on every
sentence, since then the generative score would be from
a model that had already seen that string in the training
data. To control for this, we built ten generative models,
each trained on 90 percent of the training data, and used
each of the ten to score the remaining 10 percent that was
not seen in that training set. For the held-out and testing
conditions, we used the generative model trained on all
of sections 2-21.
In table 4 we present the results of including the gen-
erative model score along with the other perceptron fea-
tures, just for the run with POS-tagger tags. The gen-
erative model score (negative log probability) effectively
provides a much better initial starting point for the per-
ceptron algorithm. The resulting F-measure on section
23 is 2.1 percent higher than either the generative model
or perceptron-trained model used in isolation.
</bodyText>
<sectionHeader confidence="0.998482" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.990850222222222">
In this paper we have presented a discriminative train-
ing approach, based on the perceptron algorithm with
a couple of effective refinements, that provides a model
capable of effective heuristic search over a very difficult
search space. In such an approach, the unnormalized dis-
criminative parsing model can be applied without either
4When POS tagging is integrated directly into the generative pars-
ing process, the baseline performance is 87.0. For comparison with the
perceptron model, results are shown with pre-tagged input.
</bodyText>
<table confidence="0.9718838">
Model POS-tagger tags
LP LR F
Generative baseline 86.8 86.5 86.7
Perceptron (w/ punctuation features) 87.0 86.3 86.6
Generative + Perceptron (w/ punct) 89.1 88.4 88.8
</table>
<tableCaption confidence="0.999006">
Table 4: Parsing results, section 23, all sentences, in-
</tableCaption>
<bodyText confidence="0.991920806451613">
cluding labeled precision (LP), labeled recall (LR), and
F-measure
an external model to present it with candidates, or poten-
tially expensive dynamic programming. When the train-
ing algorithm is provided the generative model scores as
an additional feature, the resulting parser is quite com-
petitive on this task. The improvement that was derived
from the additional punctuation features demonstrates
the flexibility of the approach in incorporating novel fea-
tures in the model.
Future research will look in two directions. First, we
will look to include more useful features that are diffi-
cult for a generative model to include. This paper was
intended to compare search with the generative model
and the perceptron model with roughly similar feature
sets. Much improvement could potentially be had by
looking for other features that could improve the mod-
els. Secondly, combining with the generative model can
be done in several ways. Some of the constraints on the
search technique that were required in the absence of the
generative model can be relaxed if the generative model
score is included as another feature. In the current paper,
the generative score was simply added as another feature.
Another approach might be to use the generative model
to produce candidates at a word, then assign perceptron
features for those candidates. Such variants deserve in-
vestigation.
Overall, these results show much promise in the use of
discriminative learning techniques such as the perceptron
algorithm to help perform heuristic search in difficult do-
mains such as statistical parsing.
</bodyText>
<sectionHeader confidence="0.963923" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.851151">
The work by Michael Collins was supported by the Na-
tional Science Foundation under Grant No. 0347631.
</bodyText>
<sectionHeader confidence="0.932825" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998559298850574">
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597617.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
\x0ccrete structures and the voted perceptron. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 263270.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In The Proceedings of the
17th International Conference on Machine Learning.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 18.
Michael Collins. 2004. Parameter estimation for sta-
tistical parsing models: Theory and practice of
distribution-free methods. In Harry Bunt, John Car-
roll, and Giorgio Satta, editors, New Developments in
Parsing Technology. Kluwer.
Fabrizio Costa, Vincenzo Lombardo, Paolo Frasconi,
and Giovanni Soda. 2001. Wide coverage incremental
parsing by learning attachment preferences. In Con-
ference of the Italian Association for Artificial Intelli-
gence (AIIA), pages 297307.
Stephen Della Pietra, Vincent Della Pietra, and John Laf-
ferty. 1997. Inducing features of random fields. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 19:380393.
Yoav Freund and Robert Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 3(37):277296.
Yoav Freund, Raj Iyer, Robert Schapire, and Yoram
Singer. 1998. An efficient boosting algorithm for
combining preferences. In Proc. of the 15th Intl. Con-
ference on Machine Learning.
Stuart Geman and Mark Johnson. 2002. Dynamic pro-
gramming for parsing and estimation of stochastic
unification-based grammars. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 279286.
Mark Johnson and Brian Roark. 2000. Compact non-
left-recursive grammars using the selective left-corner
transform and factoring. In Proceedings of the 18th
International Conference on Computational Linguis-
tics (COLING), pages 355361.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
unification-based grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 535541.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):617636.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning, pages 282289.
Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward.
1994. A maximum entropy model for parsing. In Pro-
ceedings of the International Conference on Spoken
Language Processing (ICSLP), pages 803806.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34:151175.
Stefan Riezler, Tracy King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell III, and Mark Johnson.
2002. Parsing the wall street journal using a lexical-
functional grammar and discriminative estimation
techniques. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 271278.
Brian Roark, Murat Saraclar, and Michael Collins. 2004.
Corrective language modeling for large vocabulary
ASR with the perceptron algorithm. In Proceedings
of the International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 749752.
Brian Roark. 2001a. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249276.
Brian Roark. 2001b. Robust Probabilistic Predictive
Syntactic Processing. Ph.D. thesis, Brown University.
http://arXiv.org/abs/cs/0105019.
Brian Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):124.
\x0c&amp;apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.616491">
<title confidence="0.997969">b&amp;apos;Incremental Parsing with the Perceptron Algorithm</title>
<author confidence="0.999692">Michael Collins</author>
<affiliation confidence="0.97011">MIT CSAIL</affiliation>
<email confidence="0.996393">mcollins@csail.mit.edu</email>
<author confidence="0.999989">Brian Roark</author>
<affiliation confidence="0.991681">AT&amp;T Labs - Research</affiliation>
<email confidence="0.999085">roark@research.att.com</email>
<abstract confidence="0.961445785714286">This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm. A beam-search algorithm is used during both training and decoding phases of the method. The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank. We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Stochastic attribute-value grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="1586" citStr="Abney, 1997" startWordPosition="246" endWordPosition="247">gorithm is central to the accuracy of an approach. In an ideal world, the designer of a parser or tagger would be free to choose any features which might be useful in discriminating good from bad structures, without concerns about how the features interact with the problems of training (parameter estimation) or decoding (search for the most plausible candidate under the model). To this end, a number of recently proposed methods allow a model to incorporate arbitrary global features of candidate analyses or parses. Examples of such techniques are Markov Random Fields (Ratnaparkhi et al., 1994; Abney, 1997; Della Pietra et al., 1997; Johnson et al., 1999), and boosting or perceptron approaches to reranking (Freund et al., 1998; Collins, 2000; Collins and Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (200</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Steven Abney. 1997. Stochastic attribute-value grammars. Computational Linguistics, 23(4):597617.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over dis\x0ccrete structures and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>263270</pages>
<contexts>
<context position="1750" citStr="Collins and Duffy, 2002" startWordPosition="270" endWordPosition="273">ht be useful in discriminating good from bad structures, without concerns about how the features interact with the problems of training (parameter estimation) or decoding (search for the most plausible candidate under the model). To this end, a number of recently proposed methods allow a model to incorporate arbitrary global features of candidate analyses or parses. Examples of such techniques are Markov Random Fields (Ratnaparkhi et al., 1994; Abney, 1997; Della Pietra et al., 1997; Johnson et al., 1999), and boosting or perceptron approaches to reranking (Freund et al., 1998; Collins, 2000; Collins and Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (2000) and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming methods (Geman and Johnso</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over dis\x0ccrete structures and the voted perceptron. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 263270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In The Proceedings of the 17th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="1724" citStr="Collins, 2000" startWordPosition="268" endWordPosition="269">tures which might be useful in discriminating good from bad structures, without concerns about how the features interact with the problems of training (parameter estimation) or decoding (search for the most plausible candidate under the model). To this end, a number of recently proposed methods allow a model to incorporate arbitrary global features of candidate analyses or parses. Examples of such techniques are Markov Random Fields (Ratnaparkhi et al., 1994; Abney, 1997; Della Pietra et al., 1997; Johnson et al., 1999), and boosting or perceptron approaches to reranking (Freund et al., 1998; Collins, 2000; Collins and Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (2000) and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming</context>
<context position="7759" citStr="Collins (2000)" startWordPosition="1283" endWordPosition="1284">oduce an exponentially growing number of analyses with sentence length. For some representations, such as the rule-based representation described above, the arg max in the set enumerated by the CFG can be found efficiently, using dynamic programming algorithms, without having to explicitly enumerate all members of GEN(x). However in many cases we may be interested in representations which do not allow efficient dynamic programming solutions. One way around this problem is to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of Collins (2000). In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999). 2.2 The Perceptron Algorithm for Parameter Estimation We now consider the problem of setting the parameters, , given training examples (xi, yi). We will briefly review the perceptron algorithm, and its convergence properties see Collins (2002) for a full description. The algorithm and theorems are based on the approach to classification problems described in Freun</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In The Proceedings of the 17th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>18</pages>
<contexts>
<context position="2929" citStr="Collins (2002)" startWordPosition="463" endWordPosition="464">ogramming methods (Geman and Johnson, 2002; Lafferty et al., 2001) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model. presupposes that there is an existing baseline model with reasonable performance. Many of these baseline models are themselves used with heuristic search techniques, so that the potential gain through the use of discriminative re-ranking techniques is further dependent on effective search. This paper explores an alternative approach to parsing, based on the perceptron training algorithm introduced in Collins (2002). In this approach the training and decoding problems are very closely related the training method decodes training examples in sequence, and makes simple corrective updates to the parameters when errors are made. Thus the main complexity of the method is isolated to the decoding problem. We describe an approach that uses an incremental, left-to-right parser, with beam search, to find the highest scoring analysis under the model. The same search method is used in both training and decoding. We implemented the perceptron approach with the same feature set as that of an existing generative model</context>
<context position="5190" citStr="Collins (2002" startWordPosition="816" endWordPosition="817">e trees, and then the existing parsing algorithms can be used for both training and decoding with the models. 2 The General Framework In this section we describe a general framework linear models for NLP that could be applied to a diverse range of tasks, including parsing and tagging. We then describe a particular method for parameter estimation, which is a generalization of the perceptron algorithm. Finally, we \x0cgive an abstract description of an incremental parser, and describe how it can be used with the perceptron algorithm. 2.1 Linear Models for NLP We follow the framework outlined in Collins (2002; 2004). The task is to learn a mapping from inputs x X to outputs y Y. For example, X might be a set of sentences, with Y being a set of possible parse trees. We assume: .Training examples (xi, yi) for i = 1 . . . n. .A function GEN which enumerates a set of candidates GEN(x) for an input x. .A representation mapping each (x, y) X Y to a feature vector (x, y) Rd . .A parameter vector Rd . The components GEN, and define a mapping from an input x to an output F(x) through F(x) = arg max yGEN(x) (x, y) (1) where (x, y) is the inner product P s ss(x, y). The learning task is to set the parameter </context>
<context position="8236" citStr="Collins (2002)" startWordPosition="1354" endWordPosition="1355">to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of Collins (2000). In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999). 2.2 The Perceptron Algorithm for Parameter Estimation We now consider the problem of setting the parameters, , given training examples (xi, yi). We will briefly review the perceptron algorithm, and its convergence properties see Collins (2002) for a full description. The algorithm and theorems are based on the approach to classification problems described in Freund and Schapire (1999). Figure 1 shows the algorithm. Note that the most complex step of the method is finding zi = arg maxzGEN(xi) (xi, z) and this is precisely the decoding problem. Thus the training algorithm is in principle a simple part of the parser: any system will need a decoding method, and once the decoding algorithm is implemented the training algorithm is relatively straightforward. We will now give a first theorem regarding the convergence of this algorithm. Fi</context>
<context position="10566" citStr="Collins (2002)" startWordPosition="1793" endWordPosition="1794">kes is independent of the number of candidates for each example \x0cInputs: Training examples (xi, yi) Algorithm: Initialization: Set = 0 For t = 1 . . . T, i = 1 . . . n Output: Parameters Calculate zi = arg maxzGEN(xi) (xi, z) If(zi 6= yi) then = + (xi, yi) (xi, zi) Figure 1: A variant of the perceptron algorithm. (i.e. the size of GEN(xi) for each i), depending only on the separation of the training data, where separation is defined above. This is important because in many NLP problems GEN(x) can be exponential in the size of the inputs. All of the convergence and generalization results in Collins (2002) depend on notions of separability rather than the size of GEN. Two questions come to mind. First, are there guarantees for the algorithm if the training data is not separable? Second, performance on a training sample is all very well, but what does this guarantee about how well the algorithm generalizes to newly drawn test examples? Freund and Schapire (1999) discuss how the theory for classification problems can be extended to deal with both of these questions; Collins (2002) describes how these results apply to NLP problems. As a final note, following Collins (2002), we used the averaged pa</context>
<context position="29342" citStr="Collins (2002)" startWordPosition="5274" endWordPosition="5275">amples gives a small improvement, mainly in recall. 5 Empirical results The parsing models were trained and tested on treebanks from the Penn Wall St. Journal Treebank: sections 2-21 were kept training data; section 24 was held-out development data; and section 23 was for evaluation. After each pass over the training data, the averaged perceptron model was scored on the development data, and the best performing model was used for test evaluation. For this paper, we used POS tags that were provided either by the Treebank itself (gold standard tags) or by the perceptron POS tagger3 presented in Collins (2002). The former gives us an upper bound on the improvement that we might expect if we integrated the POS tagging with the parsing. 3For trials when the generative or perceptron parser was given POS tagger output, the models were trained on POS tagged sections 2-21, which in both cases helped performance slightly. \x0cModel Gold-standard tags POS-tagger tags LP LR F LP LR F Generative 88.1 87.6 87.8 86.8 86.5 86.7 Perceptron (baseline) 87.5 86.9 87.2 86.2 85.5 85.8 Perceptron (w/ punctuation features) 88.1 87.6 87.8 87.0 86.3 86.6 Table 3: Parsing results, section 23, all sentences, including labe</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods.</title>
<date>2004</date>
<booktitle>New Developments in Parsing Technology.</booktitle>
<editor>In Harry Bunt, John Carroll, and Giorgio Satta, editors,</editor>
<publisher>Kluwer.</publisher>
<marker>Collins, 2004</marker>
<rawString>Michael Collins. 2004. Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods. In Harry Bunt, John Carroll, and Giorgio Satta, editors, New Developments in Parsing Technology. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Costa</author>
<author>Vincenzo Lombardo</author>
<author>Paolo Frasconi</author>
<author>Giovanni Soda</author>
</authors>
<title>Wide coverage incremental parsing by learning attachment preferences.</title>
<date>2001</date>
<booktitle>In Conference of the Italian Association for Artificial Intelligence (AIIA),</booktitle>
<pages>297307</pages>
<contexts>
<context position="17530" citStr="Costa et al. (2001)" startWordPosition="3113" endWordPosition="3116">ossible attachments may be disallowed because they create triples that are not in the set B. For example, in figure 2 attaching either of the VP chains under the NP is disallowed because the triple hNP,NN,VPi is not in B. Similarly, attaching the NN chain under the S will be disallowed if the triple hS,NP,NNi is not in B. In contrast, adjoining hNN cani under the NP creates a single triple, hNP,NN,NNi, which is allowed. Adjoining either of the VP chains under the S creates two triples, hS,NP,VPi and hNP,NN,Si, which are both in the set B. Note that the allowable chains in our grammar are what Costa et al. (2001) call connection paths from the partial parse to the next word. It can be shown that the method is equivalent to parsing with a transformed context-free grammar (a first-order Markov grammar) for brevity we omit the details here. In this way, given a set of candidates Fi(x) for the first i words of the string, we can generate a set of candidates Tree POS f24 f2-21 f2-21, # &gt; 1 transform tags Type Type OOV Type OOV None Gold 386 1680 0.1% 1013 0.1% None Tagged 401 1776 0.1% 1043 0.2% FSLC Gold 289 1214 0.1% 746 0.1% FSLC Tagged 300 1294 0.1% 781 0.1% Table 1: Left-child chain type counts (of le</context>
</contexts>
<marker>Costa, Lombardo, Frasconi, Soda, 2001</marker>
<rawString>Fabrizio Costa, Vincenzo Lombardo, Paolo Frasconi, and Giovanni Soda. 2001. Wide coverage incremental parsing by learning attachment preferences. In Conference of the Italian Association for Artificial Intelligence (AIIA), pages 297307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--380393</pages>
<contexts>
<context position="1613" citStr="Pietra et al., 1997" startWordPosition="249" endWordPosition="252">to the accuracy of an approach. In an ideal world, the designer of a parser or tagger would be free to choose any features which might be useful in discriminating good from bad structures, without concerns about how the features interact with the problems of training (parameter estimation) or decoding (search for the most plausible candidate under the model). To this end, a number of recently proposed methods allow a model to incorporate arbitrary global features of candidate analyses or parses. Examples of such techniques are Markov Random Fields (Ratnaparkhi et al., 1994; Abney, 1997; Della Pietra et al., 1997; Johnson et al., 1999), and boosting or perceptron approaches to reranking (Freund et al., 1998; Collins, 2000; Collins and Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (2000) and Collins and Duffy (2</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19:380393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>3</volume>
<issue>37</issue>
<contexts>
<context position="8380" citStr="Freund and Schapire (1999)" startWordPosition="1374" endWordPosition="1377">2000). In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999). 2.2 The Perceptron Algorithm for Parameter Estimation We now consider the problem of setting the parameters, , given training examples (xi, yi). We will briefly review the perceptron algorithm, and its convergence properties see Collins (2002) for a full description. The algorithm and theorems are based on the approach to classification problems described in Freund and Schapire (1999). Figure 1 shows the algorithm. Note that the most complex step of the method is finding zi = arg maxzGEN(xi) (xi, z) and this is precisely the decoding problem. Thus the training algorithm is in principle a simple part of the parser: any system will need a decoding method, and once the decoding algorithm is implemented the training algorithm is relatively straightforward. We will now give a first theorem regarding the convergence of this algorithm. First, we need the following definition: Definition 1 Let GEN(xi) = GEN(xi) {yi}. In other words GEN(xi) is the set of incorrect candidates for an</context>
<context position="10928" citStr="Freund and Schapire (1999)" startWordPosition="1853" endWordPosition="1856">ch i), depending only on the separation of the training data, where separation is defined above. This is important because in many NLP problems GEN(x) can be exponential in the size of the inputs. All of the convergence and generalization results in Collins (2002) depend on notions of separability rather than the size of GEN. Two questions come to mind. First, are there guarantees for the algorithm if the training data is not separable? Second, performance on a training sample is all very well, but what does this guarantee about how well the algorithm generalizes to newly drawn test examples? Freund and Schapire (1999) discuss how the theory for classification problems can be extended to deal with both of these questions; Collins (2002) describes how these results apply to NLP problems. As a final note, following Collins (2002), we used the averaged parameters from the training algorithm in decoding test examples in our experiments. Say t i is the parameter vector after the ith example is processed on the tth pass through the data in the algorithm in figure 1. Then the averaged parameters AV G are defined as AV G = P i,t t i/NT. Freund and Schapire (1999) originally proposed the averaged parameter method; i</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 3(37):277296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Raj Iyer</author>
<author>Robert Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>1998</date>
<booktitle>In Proc. of the 15th Intl. Conference on Machine Learning.</booktitle>
<contexts>
<context position="1709" citStr="Freund et al., 1998" startWordPosition="264" endWordPosition="267">ree to choose any features which might be useful in discriminating good from bad structures, without concerns about how the features interact with the problems of training (parameter estimation) or decoding (search for the most plausible candidate under the model). To this end, a number of recently proposed methods allow a model to incorporate arbitrary global features of candidate analyses or parses. Examples of such techniques are Markov Random Fields (Ratnaparkhi et al., 1994; Abney, 1997; Della Pietra et al., 1997; Johnson et al., 1999), and boosting or perceptron approaches to reranking (Freund et al., 1998; Collins, 2000; Collins and Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (2000) and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dyna</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 1998</marker>
<rawString>Yoav Freund, Raj Iyer, Robert Schapire, and Yoram Singer. 1998. An efficient boosting algorithm for combining preferences. In Proc. of the 15th Intl. Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Mark Johnson</author>
</authors>
<title>Dynamic programming for parsing and estimation of stochastic unification-based grammars.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>279286</pages>
<contexts>
<context position="2357" citStr="Geman and Johnson, 2002" startWordPosition="373" endWordPosition="376">and Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (2000) and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming methods (Geman and Johnson, 2002; Lafferty et al., 2001) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model. presupposes that there is an existing baseline model with reasonable performance. Many of these baseline models are themselves used with heuristic search techniques, so that the potential gain through the use of discriminative re-ranking techniques is further dependent on effective search. This paper explores an alternative approach to parsing, based on the perceptron training algorithm introduced in Collins (2002). In this approach the train</context>
</contexts>
<marker>Geman, Johnson, 2002</marker>
<rawString>Stuart Geman and Mark Johnson. 2002. Dynamic programming for parsing and estimation of stochastic unification-based grammars. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 279286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Brian Roark</author>
</authors>
<title>Compact nonleft-recursive grammars using the selective left-corner transform and factoring.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>355361</pages>
<contexts>
<context position="19609" citStr="Johnson and Roark, 2000" startWordPosition="3487" endWordPosition="3490">gh a tree transform; next, we limit the left-child chains consisting of more than two non-terminal categories to those actually observed in the training data more than once. Left-child chains of length less than or equal to two are all those observed in training data. As a practical matter, the set of leftchild chains for a terminal x is taken to be the union of the sets of left-child chains for all pre-terminal part-ofspeech (POS) tags T for x. Before inducing the left-child chains and allowable triples from the treebank, the trees are transformed with a selective left-corner transformation (Johnson and Roark, 2000) that has been flattened as presented in Roark (2001b). This transform is only applied to left-recursive productions, i.e. productions of the form A A. The transformed trees look as in figure 3. The transform has the benefit of dramatically reducing the number of left-child chains, without unduly disrupting the immediate dominance relationships that provide features for the model. The parse trees that are returned by the parser are then de-transformed to the original form of the grammar for evaluation2 . Table 1 presents the number of left-child chains of length greater than 2 in sections 2-21</context>
</contexts>
<marker>Johnson, Roark, 2000</marker>
<rawString>Mark Johnson and Brian Roark. 2000. Compact nonleft-recursive grammars using the selective left-corner transform and factoring. In Proceedings of the 18th International Conference on Computational Linguistics (COLING), pages 355361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Steven Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic unification-based grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>535541</pages>
<contexts>
<context position="1636" citStr="Johnson et al., 1999" startWordPosition="253" endWordPosition="256"> approach. In an ideal world, the designer of a parser or tagger would be free to choose any features which might be useful in discriminating good from bad structures, without concerns about how the features interact with the problems of training (parameter estimation) or decoding (search for the most plausible candidate under the model). To this end, a number of recently proposed methods allow a model to incorporate arbitrary global features of candidate analyses or parses. Examples of such techniques are Markov Random Fields (Ratnaparkhi et al., 1994; Abney, 1997; Della Pietra et al., 1997; Johnson et al., 1999), and boosting or perceptron approaches to reranking (Freund et al., 1998; Collins, 2000; Collins and Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (2000) and Collins and Duffy (2002) rerank the top N p</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic unification-based grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 535541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="20526" citStr="Johnson (1998)" startWordPosition="3636" endWordPosition="3637">pting the immediate dominance relationships that provide features for the model. The parse trees that are returned by the parser are then de-transformed to the original form of the grammar for evaluation2 . Table 1 presents the number of left-child chains of length greater than 2 in sections 2-21 and 24 of the Penn Wall St. Journal Treebank, both with and without the flattened selective left-corner transformation (FSLC), for gold-standard part-of-speech (POS) tags and automatically tagged POS tags. When the FSLC has been applied and the set is restricted to those occurring more than once 2See Johnson (1998) for a presentation of the transform/detransform paradigm in parsing. \x0c(a) NP \x10 \x10 \x10 \x10 NP \x08 \x08 \x08 NP \x11 \x11 NNP Jim b b POS s H H H NN dog P P P P PP , , IN with . . . l l NP (b) NP \x18 \x18 \x18 \x18 \x18 NNP Jim POS s X X X X X NP/NP \x08 \x08 \x08 NN dog H H H NP/NP PP \x11 \x11 IN with .. . l l NP (c) NP NNP Jim ! ! ! POS s l l NP/NP NN dog `````` NP/NP PP , , IN with . . . l l NP Figure 3: Three representations of NP modifications: (a) the original treebank representation; (b) Selective left-corner representation; and (c) a flat structure that is unambiguously equ</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):617636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282289</pages>
<contexts>
<context position="2381" citStr="Lafferty et al., 2001" startWordPosition="377" endWordPosition="381">ack of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (2000) and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming methods (Geman and Johnson, 2002; Lafferty et al., 2001) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model. presupposes that there is an existing baseline model with reasonable performance. Many of these baseline models are themselves used with heuristic search techniques, so that the potential gain through the use of discriminative re-ranking techniques is further dependent on effective search. This paper explores an alternative approach to parsing, based on the perceptron training algorithm introduced in Collins (2002). In this approach the training and decoding problem</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning, pages 282289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>Salim Roukos</author>
<author>R Todd Ward</author>
</authors>
<title>A maximum entropy model for parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>803806</pages>
<contexts>
<context position="1573" citStr="Ratnaparkhi et al., 1994" startWordPosition="241" endWordPosition="245"> as input to a learning algorithm is central to the accuracy of an approach. In an ideal world, the designer of a parser or tagger would be free to choose any features which might be useful in discriminating good from bad structures, without concerns about how the features interact with the problems of training (parameter estimation) or decoding (search for the most plausible candidate under the model). To this end, a number of recently proposed methods allow a model to incorporate arbitrary global features of candidate analyses or parses. Examples of such techniques are Markov Random Fields (Ratnaparkhi et al., 1994; Abney, 1997; Della Pietra et al., 1997; Johnson et al., 1999), and boosting or perceptron approaches to reranking (Freund et al., 1998; Collins, 2000; Collins and Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large.</context>
</contexts>
<marker>Ratnaparkhi, Roukos, Ward, 1994</marker>
<rawString>Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward. 1994. A maximum entropy model for parsing. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), pages 803806.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--151175</pages>
<contexts>
<context position="7991" citStr="Ratnaparkhi (1999)" startWordPosition="1316" endWordPosition="1317">g dynamic programming algorithms, without having to explicitly enumerate all members of GEN(x). However in many cases we may be interested in representations which do not allow efficient dynamic programming solutions. One way around this problem is to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of Collins (2000). In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999). 2.2 The Perceptron Algorithm for Parameter Estimation We now consider the problem of setting the parameters, , given training examples (xi, yi). We will briefly review the perceptron algorithm, and its convergence properties see Collins (2002) for a full description. The algorithm and theorems are based on the approach to classification problems described in Freund and Schapire (1999). Figure 1 shows the algorithm. Note that the most complex step of the method is finding zi = arg maxzGEN(xi) (xi, z) and this is precisely the decoding problem. Thus the training algorithm is in principle a sim</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34:151175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the wall street journal using a lexicalfunctional grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>271278</pages>
<contexts>
<context position="2005" citStr="Riezler et al. (2002)" startWordPosition="315" endWordPosition="318">ently proposed methods allow a model to incorporate arbitrary global features of candidate analyses or parses. Examples of such techniques are Markov Random Fields (Ratnaparkhi et al., 1994; Abney, 1997; Della Pietra et al., 1997; Johnson et al., 1999), and boosting or perceptron approaches to reranking (Freund et al., 1998; Collins, 2000; Collins and Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (2000) and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming methods (Geman and Johnson, 2002; Lafferty et al., 2001) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model. presupposes that there is an existing baseline model with reasonable performance. Many of thes</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell III, and Mark Johnson. 2002. Parsing the wall street journal using a lexicalfunctional grammar and discriminative estimation techniques. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 271278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
</authors>
<title>Corrective language modeling for large vocabulary ASR with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<pages>749752</pages>
<contexts>
<context position="30662" citStr="Roark et al. (2004)" startWordPosition="5485" endWordPosition="5488">er goldstandard or POS-tagger tags are provided to the parser4 . With the base features, the generative model outperforms the perceptron parser by between a half and one point, but with the additional punctuation features, the perceptron model matches the generative model performance. Of course, using the generative model and using the perceptron algorithm are not necessarily mutually exclusive. Another training scenario would be to include the generative model score as another feature, with some weight in the linear model learned by the perceptron algorithm. This sort of scenario was used in Roark et al. (2004) for training an n-gram language model using the perceptron algorithm. We follow that paper in fixing the weight of the generative model, rather than learning the weight along the the weights of the other perceptron features. The value of the weight was empirically optimized on the held-out set by performing trials with several values. Our optimal value was 10. In order to train this model, we had to provide generative model scores for strings in the training set. Of course, to be similar to the testing conditions, we cannot use the standard generative model trained on every sentence, since th</context>
</contexts>
<marker>Roark, Saraclar, Collins, 2004</marker>
<rawString>Brian Roark, Murat Saraclar, and Michael Collins. 2004. Corrective language modeling for large vocabulary ASR with the perceptron algorithm. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 749752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="3542" citStr="Roark, 2001" startWordPosition="561" endWordPosition="562">In this approach the training and decoding problems are very closely related the training method decodes training examples in sequence, and makes simple corrective updates to the parameters when errors are made. Thus the main complexity of the method is isolated to the decoding problem. We describe an approach that uses an incremental, left-to-right parser, with beam search, to find the highest scoring analysis under the model. The same search method is used in both training and decoding. We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search. We also describe several refinements to the training algorithm, and demonstrate their impact on convergence properties of the method. Finally, we describe training the perceptron model with the negative log probability given by the generative model as another feature. This provides the perceptron algorithm with a better starting point, leading to large improve</context>
<context position="7966" citStr="Roark (2001" startWordPosition="1313" endWordPosition="1314"> efficiently, using dynamic programming algorithms, without having to explicitly enumerate all members of GEN(x). However in many cases we may be interested in representations which do not allow efficient dynamic programming solutions. One way around this problem is to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of Collins (2000). In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999). 2.2 The Perceptron Algorithm for Parameter Estimation We now consider the problem of setting the parameters, , given training examples (xi, yi). We will briefly review the perceptron algorithm, and its convergence properties see Collins (2002) for a full description. The algorithm and theorems are based on the approach to classification problems described in Freund and Schapire (1999). Figure 1 shows the algorithm. Note that the most complex step of the method is finding zi = arg maxzGEN(xi) (xi, z) and this is precisely the decoding problem. Thus the training algori</context>
<context position="14525" citStr="Roark (2001" startWordPosition="2512" endWordPosition="2513">ng test sentences, and also errors in implementing the perceptron training algorithm in Figure 1. In this paper we give empirical results that suggest that FILTER can be chosen in such a way as to give efficient parsing performance together with high parsing accuracy. The exact implementation of the parser will depend on the definition of partial analyses, of ADV and FILTER, and of the representation . The next section describes our instantiation of these choices. 3 A full description of the parsing approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights. We first describe the parsing algorithm, and then move on to the baseline feature set for the perceptron model. 3.1 Parser control The input to the parser is a string wn 0 , a grammar G, a mapping from derivations to feature vectors, and a parameter vector . The grammar G = (V, T, S , S, C, B) consists of a set of non-terminal symbols V , a set of terminal symbols T, a start symbol S V , an end-ofconstituent symbol S V , a set of allowable chains C, and a set of allowable triples B. S is a special</context>
<context position="19661" citStr="Roark (2001" startWordPosition="3498" endWordPosition="3499">ing of more than two non-terminal categories to those actually observed in the training data more than once. Left-child chains of length less than or equal to two are all those observed in training data. As a practical matter, the set of leftchild chains for a terminal x is taken to be the union of the sets of left-child chains for all pre-terminal part-ofspeech (POS) tags T for x. Before inducing the left-child chains and allowable triples from the treebank, the trees are transformed with a selective left-corner transformation (Johnson and Roark, 2000) that has been flattened as presented in Roark (2001b). This transform is only applied to left-recursive productions, i.e. productions of the form A A. The transformed trees look as in figure 3. The transform has the benefit of dramatically reducing the number of left-child chains, without unduly disrupting the immediate dominance relationships that provide features for the model. The parse trees that are returned by the parser are then de-transformed to the original form of the grammar for evaluation2 . Table 1 presents the number of left-child chains of length greater than 2 in sections 2-21 and 24 of the Penn Wall St. Journal Treebank, both </context>
<context position="21922" citStr="Roark (2001" startWordPosition="3933" endWordPosition="3934">CCP} F3 = F2 {L02} F7 = F6 {L30} F11 = F0 {L20} F15 = F14 {CC} Table 2: Baseline feature set. Features F0 F10 fire at non-terminal nodes. Features F0, F11 F15 fire at terminal nodes. in the training corpus, we can reduce the total number of left-child chains of length greater than 2 by half, while leaving the number of words in the held-out corpus with an unobserved left-child chain (out-of-vocabulary rate OOV) to just one in every thousand words. 3.2 Features For this paper, we wanted to compare the results of a perceptron model with a generative model for a comparable feature set. Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word. Otherwise the features are basically the same as in those papers. We then built a generative model with this feature set and the same tree transform, for use with the beam-search parser from Roark (2004) to compare against our baseline perceptron model. To concisely present the baseline feature set, let us establish a notation. Features will fire whenever a new node is built in the tree. The features are labels from the left-context, </context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001a. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Robust Probabilistic Predictive Syntactic Processing.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<note>http://arXiv.org/abs/cs/0105019.</note>
<contexts>
<context position="3542" citStr="Roark, 2001" startWordPosition="561" endWordPosition="562">In this approach the training and decoding problems are very closely related the training method decodes training examples in sequence, and makes simple corrective updates to the parameters when errors are made. Thus the main complexity of the method is isolated to the decoding problem. We describe an approach that uses an incremental, left-to-right parser, with beam search, to find the highest scoring analysis under the model. The same search method is used in both training and decoding. We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search. We also describe several refinements to the training algorithm, and demonstrate their impact on convergence properties of the method. Finally, we describe training the perceptron model with the negative log probability given by the generative model as another feature. This provides the perceptron algorithm with a better starting point, leading to large improve</context>
<context position="7966" citStr="Roark (2001" startWordPosition="1313" endWordPosition="1314"> efficiently, using dynamic programming algorithms, without having to explicitly enumerate all members of GEN(x). However in many cases we may be interested in representations which do not allow efficient dynamic programming solutions. One way around this problem is to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of Collins (2000). In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999). 2.2 The Perceptron Algorithm for Parameter Estimation We now consider the problem of setting the parameters, , given training examples (xi, yi). We will briefly review the perceptron algorithm, and its convergence properties see Collins (2002) for a full description. The algorithm and theorems are based on the approach to classification problems described in Freund and Schapire (1999). Figure 1 shows the algorithm. Note that the most complex step of the method is finding zi = arg maxzGEN(xi) (xi, z) and this is precisely the decoding problem. Thus the training algori</context>
<context position="14525" citStr="Roark (2001" startWordPosition="2512" endWordPosition="2513">ng test sentences, and also errors in implementing the perceptron training algorithm in Figure 1. In this paper we give empirical results that suggest that FILTER can be chosen in such a way as to give efficient parsing performance together with high parsing accuracy. The exact implementation of the parser will depend on the definition of partial analyses, of ADV and FILTER, and of the representation . The next section describes our instantiation of these choices. 3 A full description of the parsing approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights. We first describe the parsing algorithm, and then move on to the baseline feature set for the perceptron model. 3.1 Parser control The input to the parser is a string wn 0 , a grammar G, a mapping from derivations to feature vectors, and a parameter vector . The grammar G = (V, T, S , S, C, B) consists of a set of non-terminal symbols V , a set of terminal symbols T, a start symbol S V , an end-ofconstituent symbol S V , a set of allowable chains C, and a set of allowable triples B. S is a special</context>
<context position="19661" citStr="Roark (2001" startWordPosition="3498" endWordPosition="3499">ing of more than two non-terminal categories to those actually observed in the training data more than once. Left-child chains of length less than or equal to two are all those observed in training data. As a practical matter, the set of leftchild chains for a terminal x is taken to be the union of the sets of left-child chains for all pre-terminal part-ofspeech (POS) tags T for x. Before inducing the left-child chains and allowable triples from the treebank, the trees are transformed with a selective left-corner transformation (Johnson and Roark, 2000) that has been flattened as presented in Roark (2001b). This transform is only applied to left-recursive productions, i.e. productions of the form A A. The transformed trees look as in figure 3. The transform has the benefit of dramatically reducing the number of left-child chains, without unduly disrupting the immediate dominance relationships that provide features for the model. The parse trees that are returned by the parser are then de-transformed to the original form of the grammar for evaluation2 . Table 1 presents the number of left-child chains of length greater than 2 in sections 2-21 and 24 of the Penn Wall St. Journal Treebank, both </context>
<context position="21922" citStr="Roark (2001" startWordPosition="3933" endWordPosition="3934">CCP} F3 = F2 {L02} F7 = F6 {L30} F11 = F0 {L20} F15 = F14 {CC} Table 2: Baseline feature set. Features F0 F10 fire at non-terminal nodes. Features F0, F11 F15 fire at terminal nodes. in the training corpus, we can reduce the total number of left-child chains of length greater than 2 by half, while leaving the number of words in the held-out corpus with an unobserved left-child chain (out-of-vocabulary rate OOV) to just one in every thousand words. 3.2 Features For this paper, we wanted to compare the results of a perceptron model with a generative model for a comparable feature set. Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word. Otherwise the features are basically the same as in those papers. We then built a generative model with this feature set and the same tree transform, for use with the beam-search parser from Roark (2004) to compare against our baseline perceptron model. To concisely present the baseline feature set, let us establish a notation. Features will fire whenever a new node is built in the tree. The features are labels from the left-context, </context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001b. Robust Probabilistic Predictive Syntactic Processing. Ph.D. thesis, Brown University. http://arXiv.org/abs/cs/0105019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Robust garden path parsing.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<volume>10</volume>
<issue>1</issue>
<pages>0</pages>
<contexts>
<context position="22287" citStr="Roark (2004)" startWordPosition="3996" endWordPosition="3997">rved left-child chain (out-of-vocabulary rate OOV) to just one in every thousand words. 3.2 Features For this paper, we wanted to compare the results of a perceptron model with a generative model for a comparable feature set. Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word. Otherwise the features are basically the same as in those papers. We then built a generative model with this feature set and the same tree transform, for use with the beam-search parser from Roark (2004) to compare against our baseline perceptron model. To concisely present the baseline feature set, let us establish a notation. Features will fire whenever a new node is built in the tree. The features are labels from the left-context, i.e. the already built part of the tree. All of the labels that we will include in our feature sets are i levels above the current node in the tree, and j nodes to the left, which we will denote Lij. Hence, L00 is the node label itself; L10 is the label of parent of the current node; L01 is the label of the sibling of the node, immediately to its left; L11 is the</context>
</contexts>
<marker>Roark, 2004</marker>
<rawString>Brian Roark. 2004. Robust garden path parsing. Natural Language Engineering, 10(1):124. \x0c&amp;apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>