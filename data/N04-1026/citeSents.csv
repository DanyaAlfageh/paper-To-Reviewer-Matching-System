 It has been hypothesized that the success of such computer dialogue tutors could be further increased by modeling and adapting to student emotion; for example CITATION have shown that adding human-provided emotional scaffolding to an automated reading tutor increases student persistence,,
y reflect natural speech CITATION, such as found in tutoring dialogues,,
 As a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech to train emotion predictors (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), but often predicts emotions using only acoustic-prosodic features that would be automatically available to a dialogue system in real-time,,
, negative/non-negative) (CITATION; CITATION; CITATION), our experiments produced the best predictions using our three-way distinction,,
 In contrast to CITATION, our classifications are contextrelative (relative to other turns in the dialogue), and taskrelative (relative to tutoring), because like CITATION, we are interested in detecting emotional changes across our dialogues,,
 Although CITATION also employ a relative classification, they explicitly associate specific features with emotional utterances,,
2 This inter-annotatoragreement exceeds that of prior studies of emotion annotation in naturally occurring speech 2 \x03\x05\x04\x07\x06\x08\x06\t\x04\x0b \x0c\x0f\x0e\x11\x10\x13\x12\x15\x14\x16\x0c\x17\x0e\x19\x18\x1a\x12 \x1b \x14\x16\x0c\x17\x0e\x19\x18\x1a\x12 CITATION,,
47 in CITATION, and Kappa ranging between 0,,
42 in CITATION),,
 As in CITATION, the machine learning experiments described below use only those 385 student turns where the two annotators agreed on an emotion label,,
normalized temporal features: total turn duration, duration of pause prior to turn, speaking rate, amount of silence in turn Non-Acoustic-Prosodic Features \x1c lexical items in turn \x1c 6 automatic features: turn begin time, turn end time, isTemporalBarge-in, isTemporalOverlap, #words in turn, #syllables in turn \x1c 6 manual features: #false starts in turn, isPriorTutorQuestion, isQuestion, isSemanticBarge-in, #canonical expressions in turn, isGrounding Identifier Features: subject, subject gender, problem Figure 2: Features Per Student Turn Followingother studies of spontaneous dialogues (CITATION; CITATION; CITATION; CITATION), our acoustic-prosodic features represent knowledge of pitch, energy, duration, tempo and pausing,,
 We found that features normalized by first turn were the best predictors of emotion CITATION,,
 Lexical information has been shown to improve speech-based emotion prediction in other domains (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), so our first non-acoustic-prosodic feature represents the transcription3 of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn),,
 Following CITATION, feature usage is reported as the percentage of decisions for which the feature type is queried,,
rrently exploring the use of other emotion annotation schemas for emotion prediction, such as those that incorporate categorizations encompassing multiple dimensions (CITATION; CITATION) and those that examine emotions at smaller units of granularity than turns CITATION,,
, language model, speaking style, dialog act, part-ofspeech, repetition, emotionally salient keywords, wordlevel prosody (CITATION; CITATION; CITATION)) and in text-based applications CITATION,,
 We are also exploring methods of combining information other than by feature level combination, such as data fusion across multiple classifiers (CITATION; CITATION),,
 Much of this research has used databases of speech read by actors or native speakers as training data (often with semantically neutral content) (CITATION; CITATION; CITATION),,
 However, such prototypical emotional speech does not necessarily reflect natural speech CITATION, such as found in tutoring dialogues,,
 As a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech to train emotion predictors (CITATION; CITATION; CITATION; CITATION; Batliner et ,,
, negative/non-negative) (CITATION; CITATION; CITATION), our experiments produced the best predictions using our three-way distinction,,
 In contrast to CITATION, our classifications are contextrelative (relative to other turns in the dialogue), and taskrelative (relative to tutoring), because like CITATION, we are interested in detecting emotional changes across our dialogues,,
 Although CITATION also employ a relative classification, they explicitly associate specific features with emotional utterances,,
turn duration, duration of pause prior to turn, speaking rate, amount of silence in turn Non-Acoustic-Prosodic Features \x1c lexical items in turn \x1c 6 automatic features: turn begin time, turn end time, isTemporalBarge-in, isTemporalOverlap, #words in turn, #syllables in turn \x1c 6 manual features: #false starts in turn, isPriorTutorQuestion, isQuestion, isSemanticBarge-in, #canonical expressions in turn, isGrounding Identifier Features: subject, subject gender, problem Figure 2: Features Per Student Turn Followingother studies of spontaneous dialogues (CITATION; CITATION; CITATION; CITATION), our acoustic-prosodic features represent knowledge of pitch, energy, duration, tempo and pausing,,
 We found that features normalized by first turn were the best predictors of emotion CITATION,,
 Lexical information has been shown to improve speech-based emotion prediction in other domains (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), so our first non-acoustic-prosodic feature represents the transcription3 of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn),,
 6 Adding Context-Level Features Research in other domains (CITATION; CITATION) has shown that features representing the di\x0calogue context can sometimes improve the accuracy of predicting negative user states, compared to the use of features computed from only the turn to be predicted,,
 We are currently exploring the use of other emotion annotation schemas for emotion prediction, such as those that incorporate categorizations encompassing multiple dimensions (CITATION; CITATION) and those that examine emotions at smaller units of granularity than turns CITATION,,
, language model, speaking style, dialog act, part-ofspeech, repetition, emotionally salient keywords, wordlevel prosody (CITATION; CITATION; CITATION)) and in text-based applications CITATION,,
 We are also exploring methods of combining information other than by feature level combination, such as data fusion across multiple classifiers (CITATION; CITATION),,
2 This inter-annotatoragreement exceeds that of prior studies of emotion annotation in naturally occurring speech 2 \x03\x05\x04\x07\x06\x08\x06\t\x04\x0b \x0c\x0f\x0e\x11\x10\x13\x12\x15\x14\x16\x0c\x17\x0e\x19\x18\x1a\x12 \x1b \x14\x16\x0c\x17\x0e\x19\x18\x1a\x12 CITATION,,
47 in CITATION, and Kappa ranging between 0,,
42 in CITATION),,
 As in CITATION, the machine learning experiments described below use only those 385 student turns where the two annotators agreed on an emotion label,,
 These features were motivated by the use of turn position as a feature for emotion prediction in CITATION, and the fact that measures of dialogue interactivity have been shown to correlate with learning gains in tutoring CITATION,,
 The number of words and syllables in a turn provide alternative ways to quantify turn duration CITATION,,
 We are currently exploring the use of other emotion annotation schemas for emotion prediction, such as those that incorporate categorizations encompassing multiple dimensions (CITATION; CITATION) and those that examine emotions at smaller units of granularity than turns CITATION,,
, language model, speaking style, dialog act, part-ofspeech, repetition, emotionally salient keywords, wordlevel prosody (CITATION; CITATION; CITATION)) and in text-based applications CITATION,,
 We are currently exploring the use of other emotion annotation schemas for emotion prediction, such as those that incorporate categorizations encompassing multiple dimensions (CITATION; CITATION) and those that examine emotions at smaller units of granularity than turns CITATION,,
, language model, speaking style, dialog act, part-ofspeech, repetition, emotionally salient keywords, wordlevel prosody (CITATION; CITATION; CITATION)) and in text-based applications CITATION,,
 As a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech to train emotion predictors (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), but often predicts emotions using only acoustic-prosodic features that would be automatically available to a dialogue system in real-time,,
 We found that features normalized by first turn were the best predictors of emotion CITATION,,
 Lexical information has been shown to improve speech-based emotion prediction in other domains (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), so our first non-acoustic-prosodic feature represents the transcription3 of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn),,
 CITATION have argued that hedges can indicate emotional speech (e,,
lized acoustic-prosodic features \x1c lexical: lexical items in turn \x1c autotext: lexical + 6 automatic features \x1c alltext: lexical + 6 automatic + 6 manual features \x1c +ident: each of the above sets + 3 identifier features Figure 3: Feature Sets for Machine Learning We use the Weka machine learning software CITATION to automatically learn our emotion prediction models,,
 In earlier work CITATION, we used Weka to compare a nearest-neighbor classifier, a decision tree learner, and a boosting algorithm,,
 We found that the boosting algorithm, called AdaBoost CITATION, consistently yielded the most robust performance across feature sets and evaluation metrics; in this paper we thus focus on AdaBoosts performance,,
 Boosting algorithms generally enable the accuracy of a weak learning algorithm to be improved by repeatedly applying it to different distributions of training examples CITATION,,
 Following CITATION, we select the decision tree learner as AdaBoosts weak learning algorithm,,
oes not necessarily reflect natural speech CITATION, such as found in tutoring dialogues,,
 As a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech to train emotion predictors (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), but often predicts emotions using only acoustic-prosodic features that would be automatically available to a dialogue system in real-time,,
, negative/non-negative) (CITATION; CITATION; CITATION), our experiments produced the best predictions using our three-way distinction,,
 In contrast to CITATION, our classifications are contextrelative (relative to other turns in the dialogue), and taskrelative (relative to tutoring), because like CITATION, we are interested in detecting emotional changes across our dialogues,,
 Although CITATION also employ a relative classification, they explicitly associate specific features with emotional utterances,,
2 This inter-annotatoragreement exceeds that of prior studies of emotion annotation in naturally occurring speech 2 \x03\x05\x04\x07\x06\x08\x06\t\x04\x0b \x0c\x0f\x0e\x11\x10\x13\x12\x15\x14\x16\x0c\x17\x0e\x19\x18\x1a\x12 \x1b \x14\x16\x0c\x17\x0e\x19\x18\x1a\x12 CITATION,,
47 in CITATION, and Kappa ranging between 0,,
42 in CITATION),,
 As in CITATION, the machine learning experiments described below use only those 385 student turns where the two annotators agreed on an emotion label,,
l features: total turn duration, duration of pause prior to turn, speaking rate, amount of silence in turn Non-Acoustic-Prosodic Features \x1c lexical items in turn \x1c 6 automatic features: turn begin time, turn end time, isTemporalBarge-in, isTemporalOverlap, #words in turn, #syllables in turn \x1c 6 manual features: #false starts in turn, isPriorTutorQuestion, isQuestion, isSemanticBarge-in, #canonical expressions in turn, isGrounding Identifier Features: subject, subject gender, problem Figure 2: Features Per Student Turn Followingother studies of spontaneous dialogues (CITATION; CITATION; CITATION; CITATION), our acoustic-prosodic features represent knowledge of pitch, energy, duration, tempo and pausing,,
speech CITATION, such as found in tutoring dialogues,,
 As a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech to train emotion predictors (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), but often predicts emotions using only acoustic-prosodic features that would be automatically available to a dialogue system in real-time,,
 We found that features normalized by first turn were the best predictors of emotion CITATION,,
 Lexical information has been shown to improve speech-based emotion prediction in other domains (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), so our first non-acoustic-prosodic feature represents the transcription3 of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn),,
 Prior studies (CITATION; CITATION) have shown that subject and gender can play an important role in emotion recognition, because different genders and/or speakers can convey emotions differently,,
 We are currently exploring the use of other emotion annotation schemas for emotion prediction, such as those that incorporate categorizations encompassing multiple dimensions (CITATION; CITATION) and those that examine emotions at smaller units of granularity than turns CITATION,,
, language model, speaking style, dialog act, part-ofspeech, repetition, emotionally salient keywords, wordlevel prosody (CITATION; CITATION; CITATION)) and in text-based applications CITATION,,
 We are also exploring methods of combining information other than by feature level combination, such as data fusion across multiple classifiers (CITATION; CITATION),,
 Much of this research has used databases of speech read by actors or native speakers as training data (often with semantically neutral content) (CITATION; CITATION; CITATION),,
 However, such prototypical emotional speech does not necessarily reflect natural speech CITATION, such as found in tutoring dialogues,,
 We have developed an annotation scheme for hand labeling the student turns in our corpus with respect to three types of perceived emotions CITATION: Negative: a strong expression of emotion such as confused, bored, frustrated, uncertain,,
 We found that features normalized by first turn were the best predictors of emotion CITATION,,
 Lexical information has been shown to improve speech-based emotion prediction in other domains (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), so our first non-acoustic-prosodic feature represents the transcription3 of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn),,
 \x1c speech: 12 normalized acoustic-prosodic features \x1c lexical: lexical items in turn \x1c autotext: lexical + 6 automatic features \x1c alltext: lexical + 6 automatic + 6 manual features \x1c +ident: each of the above sets + 3 identifier features Figure 3: Feature Sets for Machine Learning We use the Weka machine learning software CITATION to automatically learn our emotion prediction models,,
 In earlier work CITATION, we used Weka to compare a nearest-neighbor classifier, a decision tree learner, and a boosting algorithm,,
 We found that the boosting algorithm, called AdaBoost CITATION, consistently yielded the most robust performance across feature sets and evaluation metrics; in this paper we thus focus on AdaBoosts performance,,
 Boosting algorithms generally enable the accuracy of a weak learning algorithm to be improved by repeatedly applying it to different distributions of training examples CITATION,,
 Following CITATION, we select the decision tree learner as,,
 2 The Dialogue System and Corpus We are currently building a spoken dialogue tutorial system called ITSPOKE (Intelligent Tutoring SPOKEn dialogue system) CITATION, with the goal of automatically predicting and adapting to student emotions,,
 ITSPOKE uses as its back-end the text-based Why2-Atlas dialogue tutoring system CITATION,,
al emotional speech does not necessarily reflect natural speech CITATION, such as found in tutoring dialogues,,
 As a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech to train emotion predictors (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), but often predicts emotions using only acoustic-prosodic features that would be automatically available to a dialogue system in real-time,,
 We found that features normalized by first turn were the best predictors of emotion CITATION,,
 Lexical information has been shown to improve speech-based emotion prediction in other domains (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), so our first non-acoustic-prosodic feature represents the transcription3 of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn),,
 6 Adding Context-Level Features Research in other domains (CITATION; CITATION) has shown that features representing the di\x0calogue context can sometimes improve the accuracy of predicting negative user states, compared to the use of features computed from only the turn to be predicted,,
 Much of this research has used databases of speech read by actors or native speakers as training data (often with semantically neutral content) (CITATION; CITATION; CITATION),,
 However, such prototypical emotional speech does not necessarily reflect natural speech CITATION, such as found in tutoring dialogues,,
 Prior studies (CITATION; CITATION) have shown that subject and gender can play an important role in emotion recognition, because different genders and/or speakers can convey emotions differently,,
 In earlier work CITATION, we used Weka to compare a nearest-neighbor classifier, a decision tree learner, and a boosting algorithm,,
 We found that the boosting algorithm, called AdaBoost CITATION, consistently yielded the most robust performance across feature sets and evaluation metrics; in this paper we thus focus on AdaBoosts performance,,
 Boosting algorithms generally enable the accuracy of a weak learning algorithm to be improved by repeatedly applying it to different distributions of training examples CITATION,,
 Following CITATION, we select the decision tree learner as AdaBoosts weak learning algorithm,,
 Much of this research has used databases of speech read by actors or native speakers as training data (often with semantically neutral content) (CITATION; CITATION; CITATION),,
 However, such prototypical emotional speech does not necessarily reflect natural speech CITATION, such as found in tutoring dialogues,,
on schemas for emotion prediction, such as those that incorporate categorizations encompassing multiple dimensions (CITATION; CITATION) and those that examine emotions at smaller units of granularity than turns CITATION,,
, language model, speaking style, dialog act, part-ofspeech, repetition, emotionally salient keywords, wordlevel prosody (CITATION; CITATION; CITATION)) and in text-based applications CITATION,,
 We are also exploring methods of combining information other than by feature level combination, such as data fusion across multiple classifiers (CITATION; CITATION),,
 As a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech to train emotion predictors (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), but often predicts emotions using only acoustic-prosodic features that would be automatically available to a dialogue system in real-time,,
2 This inter-annotatoragreement exceeds that of prior studies of emotion annotation in naturally occurring speech 2 \x03\x05\x04\x07\x06\x08\x06\t\x04\x0b \x0c\x0f\x0e\x11\x10\x13\x12\x15\x14\x16\x0c\x17\x0e\x19\x18\x1a\x12 \x1b \x14\x16\x0c\x17\x0e\x19\x18\x1a\x12 CITATION,,
47 in CITATION, and Kappa ranging between 0,,
42 in CITATION),,
 As in CITATION, the machine learning experiments described below use only those 385 student turns where the two annotators agreed on an emotion label,,
 of pause prior to turn, speaking rate, amount of silence in turn Non-Acoustic-Prosodic Features \x1c lexical items in turn \x1c 6 automatic features: turn begin time, turn end time, isTemporalBarge-in, isTemporalOverlap, #words in turn, #syllables in turn \x1c 6 manual features: #false starts in turn, isPriorTutorQuestion, isQuestion, isSemanticBarge-in, #canonical expressions in turn, isGrounding Identifier Features: subject, subject gender, problem Figure 2: Features Per Student Turn Followingother studies of spontaneous dialogues (CITATION; CITATION; CITATION; CITATION), our acoustic-prosodic features represent knowledge of pitch, energy, duration, tempo and pausing,,
 We found that features normalized by first turn were the best predictors of emotion CITATION,,
 Lexical information has been shown to improve speech-based emotion prediction in other domains (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), so our first non-acoustic-prosodic feature represents the transcription3 of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn),,
 2 The Dialogue System and Corpus We are currently building a spoken dialogue tutorial system called ITSPOKE (Intelligent Tutoring SPOKEn dialogue system) CITATION, with the goal of automatically predicting and adapting to student emotions,,
 ITSPOKE uses as its back-end the text-based Why2-Atlas dialogue tutoring system CITATION,,
 \x1c speech: 12 normalized acoustic-prosodic features \x1c lexical: lexical items in turn \x1c autotext: lexical + 6 automatic features \x1c alltext: lexical + 6 automatic + 6 manual features \x1c +ident: each of the above sets + 3 identifier features Figure 3: Feature Sets for Machine Learning We use the Weka machine learning software CITATION to automatically learn our emotion prediction models,,
 In earlier work CITATION, we used Weka to compare a nearest-neighbor classifier, a decision tree learner, and a boosting algorithm,,
 We found that the boosting algorithm, called AdaBoost CITATION, consistently yielded the most robust performance across feature sets and evaluation metrics; in this paper we thus focus on AdaBoosts performance,,
