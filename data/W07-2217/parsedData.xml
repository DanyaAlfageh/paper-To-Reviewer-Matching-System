<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.391602">
b&amp;apos;Proceedings of the 10th Conference on Parsing Technologies, pages 133143,
Prague, Czech Republic, June 2007. c
</bodyText>
<figure confidence="0.922879333333333">
2007 Association for Computational Linguistics
Dependency Parsing with Second-Order Feature Maps and Annotated
Semantic Information
Massimiliano Ciaramita
Yahoo! Research
Ocata 1, S-08003
Barcelona, Spain
massi@yahoo-inc.com
Giuseppe Attardi
Dipartimento di Informatica
Universita di Pisa
L. B. Pontecorvo 3, I-56127
</figure>
<affiliation confidence="0.606456">
Pisa, Italy
</affiliation>
<email confidence="0.900979">
attardi@di.unipi.it
</email>
<sectionHeader confidence="0.979789" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999745071428571">
This paper investigates new design options
for the feature space of a dependency parser.
We focus on one of the simplest and most
efficient architectures, based on a determin-
istic shift-reduce algorithm, trained with the
perceptron. By adopting second-order fea-
ture maps, the primal form of the perceptron
produces models with comparable accuracy
to more complex architectures, with no need
for approximations. Further gains in accu-
racy are obtained by designing features for
parsing extracted from semantic annotations
generated by a tagger. We provide experi-
mental evaluations on the Penn Treebank.
</bodyText>
<sectionHeader confidence="0.998158" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997242019607843">
A dependency tree represents a sentence as a labeled
directed graph encoding syntactic and semantic in-
formation. The labels on the arcs can represent ba-
sic grammatical relations such as subject and ob-
ject. Dependency trees capture grammatical struc-
tures that can be useful in several language process-
ing tasks such as information extraction (Culotta &amp;
Sorensen, 2004) and machine translation (Ding &amp;
Palmer, 2005). Dependency treebanks are becoming
available in many languages, and several approaches
to dependency parsing on multiple languages have
been evaluated in the CoNLL 2006 and 2007 shared
tasks (Buchholz &amp; Marsi, 2006; Nivre et al., 2007).
Dependency parsing is simpler than constituency
parsing, since dependency trees do not have extra
non-terminal nodes and there is no need for a gram-
mar to generate them. Approaches to dependency
parsing either generate such trees by considering all
possible spanning trees (McDonald et al., 2005), or
build a single tree by means of shift-reduce parsing
actions (Yamada &amp; Matsumoto, 2003). Determinis-
tic dependency parsers which run in linear time have
also been developed (Nivre &amp; Scholz, 2004; Attardi,
2006). These parsers process the sentence sequen-
tially, hence their efficiency makes them suitable for
processing large amounts of text, as required, for ex-
ample, in information retrieval applications.
Recent work on dependency parsing has high-
lighted the benefits of using rich feature sets
and high-order modeling. Yamada and Mat-
sumoto (2003) showed that learning an SVM model
in the dual space with higher-degree polynomial ker-
nel functions improves significantly the parsers ac-
curacy. McDonald and Pereira (2006) have shown
that incorporating second order features relating to
adjacent edge pairs improves the accuracy of max-
imum spanning tree parsers (MST). In the SVM-
based approach, if the training data is large, it is not
feasible to train a single model. Rather, Yamada and
Matsumoto (see also (Hall et al., 2006)) partition the
training data in different sets, on the basis of Part-
of-Speech, then train one dual SVM model per set.
While this approach simplifies the learning task it
makes the parser more sensitive to the error rate of
the POS tagger. The second-order MST algorithm
has cubic time complexity. For non-projective lan-
guages the algorithm is NP-hard and McDonald and
Pereira (2006) introduce an approximate algorithm
to handle such cases.
In this paper we extend shift reduce parsing with
second-order feature maps which explicitly repre-
</bodyText>
<page confidence="0.998204">
133
</page>
<bodyText confidence="0.998510857142857">
\x0csent all feature pairs. Also the augmented fea-
ture sets impose additional computational costs.
However, excellent efficiency/accuracy trade-off is
achieved by using the perceptron algorithm, with-
out the need to resort to approximations, producing
high-accuracy classifiers based on a single model.
We also evaluate a novel set of features for pars-
ing. Recently various forms of shallow semantic
processing have been investigated such as named-
entity recognition (NER), semantic role labeling
(SRL) and relation extraction. Syntactic parsing can
provide useful features for these tasks; e.g., Pun-
yakanok et al. (2005) show that full parsing is effec-
tive for semantic role labeling (see also related ap-
proaches evaluated within the CoNNL 2005 shared
task (Carreras et al., 2005)). However, no evidence
has been provided so far that annotated semantic
information can be leveraged for improving parser
performance. We report experiments showing that
adding features extracted by an entity tagger im-
proves the accuracy of a dependency parser.
</bodyText>
<sectionHeader confidence="0.980228" genericHeader="introduction">
2 Dependency parsing
</sectionHeader>
<bodyText confidence="0.998848454545454">
A dependency parser takes as input a sentence s and
returns a dependency graph d. Figure 1 shows a de-
pendency tree for the sentence Last week CBS Inc.
canceled The People Next Door.1. Dependencies
are represented as labeled arrows from the head of
the relation to the modifier word; thus, in the exam-
ple, Inc. is the modifier of a dependency labeled
SUB (subject) to the main verb, the head, can-
celed.
In statistical syntactic parsing a generator (e.g.,
a PCFG) is used to produce a number of candi-
date trees (Collins, 2000) with associated proba-
bility scores. This approach has been used also
for dependency parsing, generating spanning trees
as candidates and computing the maximum span-
ning tree (MST) using discriminative learning algo-
rithms (McDonald et al., 2005). Second-order MST
dependency parsers currently represent the state of
the art in terms of accuracy. Yamada and Mat-
sumoto (2003) proposed a deterministic classifier-
based parser. Instead of learning directly which
tree to assign to a sentence, the parser learns which
</bodyText>
<page confidence="0.937576">
1
</page>
<bodyText confidence="0.998846523809524">
The figure also contains entity annotations which will be
explained below in Section 4.1.
Shift/Reduce actions to use in building the tree. Pars-
ing is cast as a classification problem: at each step
the parser applies a classifier to the features rep-
resenting its current state to predict which action
to perform on the tree. Similar deterministic ap-
proaches to parsing have been investigated also in
the context of constituent parsing (Wong &amp; Wu,
1999; Kalt, 2004).
Nivre and Scholz (2004) proposed a variant of the
model of Yamada and Matsumoto that reduces the
complexity, from the worst case quadratic to linear.
Attardi (2006) proposed a variant of the rules that
handle non-projective relations while parsing deter-
ministically in a single pass. Shift-reduce algorithms
are simple and efficient, yet competitive in terms
of accuracy: in the CoNLL-X shared task, for sev-
eral languages, there was no statistically significant
difference between second-order MST parsers and
shift-reduce parsers.
</bodyText>
<sectionHeader confidence="0.891078" genericHeader="method">
3 A shift-reduce parser
</sectionHeader>
<bodyText confidence="0.999419142857143">
We build upon DeSR, the shift-reduce parser de-
scribed in (Attardi, 2006). This and Nivre and
Scholzs (2004) provide among the simplest and
most efficient methods. This parser constructs de-
pendency trees by scanning input sentences in a
single left-to-right pass and performing shift/reduce
parsing actions. The parsing algorithm is fully de-
terministic and has linear complexity. The parsers
behavior can be described as repeatedly selecting
and applying a parsing rule to transform its state,
while advancing through the sentence. Each to-
ken is analyzed once and a decision is made lo-
cally concerning the action to take, that is, without
considering global properties of the tree being built.
Nivre (2004) investigated the issue of (strict) incre-
mentality for this type of parsers; i.e., if at any point
of the analysis the processed input forms one con-
nected structure. Nivre found that strict incremen-
tality is not guaranteed within this parsing frame-
work, although for correctly parsed trees the prop-
erty holds in almost 90% of the cases.
</bodyText>
<subsectionHeader confidence="0.999844">
3.1 Parsing algorithm
</subsectionHeader>
<bodyText confidence="0.998483333333333">
The state of the parser is represented by a triple
hS, I, Ai, where S is the stack, I is the list of input
tokens that remain to be processed and A is the arc
</bodyText>
<page confidence="0.999649">
134
</page>
<figureCaption confidence="0.500218">
\x0cFigure 1. A dependency tree from the Penn Treebank, with additional entity annotation from the BBN corpus.
</figureCaption>
<bodyText confidence="0.998935">
relation for the dependency graph, which consists of
a set of labeled arcs (wi, r, wj), where wi, wj W
(the set of tokens), d D (the set of dependencies).
Given an input sentence s, the parser is initialized
to h, s, i, and terminates at configuration hs, , Ai.
There are three parsing schemata:
</bodyText>
<figure confidence="0.886279636363636">
Shift hS,n|I,Ai
hn|S,I,Ai
(1)
Rightr
hs|S,n|I,Ai
hS,n|I,A{(s,r,n)}i
(2)
Leftr
hs|S,n|I,Ai
hS,s|I,A{(n,r,s)}i
(3)
</figure>
<bodyText confidence="0.969558636363636">
The Shift rule advances on the input; each Leftr and
Rightr rule creates a link r between the next input
token n and the top token on the stack s. For produc-
ing labeled dependencies the rules Leftr and Rightr
are instantiated several times once for each depen-
dency label.
Additional parsing actions (cf. (Attardi, 2006))
have been introduced for handling non-projective
dependency trees: i.e., trees that cannot be drawn
in the plane without crossing edges. However, they
are not needed in the experiments reported here,
because in the Penn Treebank used in our experi-
ments dependencies are extracted without consider-
ing empty nodes and the resulting trees are all pro-
jective2.
The pseudo code in Algorithm 1 reproduces
schematically the parsing process.
The function getContext() extracts a vector of
features x relative to the structure built up to that
point from the context of the current token, i.e., from
a subset of I, S and A. The step estimateAction()
predicts a parsing action y, given a trained model
</bodyText>
<page confidence="0.980681">
2
</page>
<bodyText confidence="0.6622095">
Instead, the version of the Penn Treebank used for the
CoNLL 2007 shared task includes also non-projective represen-
tations.
Algorithm 1: DeSR: Dependency Shift Reduce
parser.
input: s = w1, w2, ..., wn
</bodyText>
<figure confidence="0.744592">
begin
S hi
I hw1, w2, ..., wni
A hi
</figure>
<equation confidence="0.889392">
while I 6= hi do
x getContext(S, I, A)
y estimateAction(x, )
performAction(y, S, I, A)
end
</equation>
<bodyText confidence="0.9605905">
and x. The final step performAction() updates the
state according to the predicted parsing rule.
</bodyText>
<subsectionHeader confidence="0.933642">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999799294117647">
The set of features used in this paper were chosen
with a few simple experiments on the development
data as a variant of a generic model. The only fea-
tures of the tokens used are Lemma, Pos and
Dep: Lemma refers to the morphologically sim-
plified form of the token, Pos is the Part-of-Speech
and Dep is the label on a dependency. Child
refers to the child of a node (right or left): up to
two furthest children of a node are considered. Ta-
ble 1 lists which feature is extracted for which to-
ken: negative numbers refer to tokens on the stack,
positive numbers refer to input tokens. As an exam-
ple, POS(-1) is the Part-of-Speech of the token on
the top of the stack, while Lemma(0) is the lemma
of the next token in the input, PosLeftChild(-1) ex-
tracts the Part-of-Speech of the leftmost child of the
token on the top of the stack, etc.
</bodyText>
<page confidence="0.99872">
135
</page>
<table confidence="0.98205325">
\x0cTOKEN
FEATURES Stack Input
Lemma -2 -1 0 1 2 3
Pos -2 -1 0 1 2 3
LemmaLeftChild -1 0
PosLeftChild -1 0
DepLeftChild -1 0
LemmaRightChild -1 0
PosRightChild -1 0
DepRightChild -1
LemmaPrev 0
PosSucc -1
</table>
<tableCaption confidence="0.802574">
Table 1. Configuration of the feature parameters used in
the experiments.
</tableCaption>
<subsectionHeader confidence="0.954071">
3.3 Learning a parsing model with the
</subsectionHeader>
<bodyText confidence="0.952497">
perceptron
The problem of learning a parsing model can be
framed as a classification task where each class
</bodyText>
<equation confidence="0.574477">
yi Y represents one of k possible parsing actions.
</equation>
<bodyText confidence="0.917251">
Each of such actions is associated with a weight vec-
tor k IRd
. Given a datapoint x X, a d-
dimensional vector of binary features in the input
space X, a parsing action is chosen with a winner-
take-all discriminant function:
estimateAction(x, ) = arg max
</bodyText>
<equation confidence="0.842855">
k
f(x, k) (4)
</equation>
<bodyText confidence="0.943253709677419">
when using a linear classifier, such as the perceptron
or SVM, f(u, v) = hu, vi is the inner product be-
tween vectors u and v.
We learn the parameters from the training data
with the perceptron (Rosemblatt, 1958), in the on-
line multiclass formulation of the algorithm (Cram-
mer &amp; Singer, 2003) with uniform negative updates.
The perceptron has been used in previous work on
dependency parsing by Carreras et al. (2006), with
a parser based on Eisners algorithm (Eisner, 2000),
and also on incremental constituent parsing (Collins
&amp; Roark, 2006). Also the MST parser of McDonald
uses a variant of the perceptron algorithm (McDon-
ald, 2006). The choice is motivated by the simplicity
and performance of perceptrons, which have proved
competitive on a number of tasks; e.g., in shallow
parsing, where perceptrons performance is com-
parable to that of Conditional Random Field mod-
els (Sha &amp; Pereira, 2003).
The only adjustable parameter of the model is the
number of instances T to use for training. We fixed
T using the development portion of the data. In
our experiments, the best value is between 20 and
30 times the size of the training data. To regularize
the model we take as the final model the average of
all weight vectors posited during training (Collins,
2002). Algorithm 2 illustrates the perceptron learn-
ing procedure. The final average model can be com-
puted efficiently during training without storing the
individual vectors (e.g., see (Ciaramita &amp; Johnson,
2003)).
</bodyText>
<construct confidence="0.529087">
Algorithm 2: Average multiclass perceptron
</construct>
<equation confidence="0.98907328">
input : S = (xi, yi)N ; 0
k = ~
0, k Y
for t = 1 to T do
choose j
Et = {r Y : hxj, t
ri hxj, t
yj
i}
if |Et |&amp;gt; 0 then
t+1
r = t
r
xj
|Et |, r Et
t+1
yj
= t
yj
+ xj
output: k = 1
T
P
t t
k, k Y
</equation>
<subsectionHeader confidence="0.946347">
3.4 Higher-order feature spaces
</subsectionHeader>
<bodyText confidence="0.964313730769231">
Yamada and Matsumoto (2003) and McDonald and
Pereira (2006) have shown that higher-order fea-
ture representations and modeling can improve pars-
ing accuracy, although at significant computational
costs. To make SVM training feasible in the dual
model with polynomial kernels, Yamada and Mat-
sumoto split the training data into several sets, based
on POS tags, and train a parsing model for each
set. McDonald and Pereiras second-order MST
parser has O(n3) complexity, while for handling
non-projective trees, otherwise an NP-hard problem,
the parser resorts to an approximate algorithm. Here
we discuss how the feature representation can be
enriched to improve parsing while maintaining the
simplicity of the shift-reduce architecture, and per-
forming discriminative learning without partitioning
the training data.
The linear classifier (see Equation 4) learned with
the perceptron is inherently limited in the types of
solutions it can learn. As originally pointed out by
Minsky and Papert (1969), there are problems which
require non-linear solutions that cannot be learned
by such models. A simple workaround this limi-
tation relies on feature maps : IRd
IRh
that
</bodyText>
<page confidence="0.997786">
136
</page>
<bodyText confidence="0.981640266666666">
\x0cmap the input vectors x X into some higher h-
dimensional representation (X) IRh
, the fea-
ture space. The feature space can represent, for ex-
ample, all combinations of individual features in the
input space. We define a feature map which ex-
tracts all second order features of the form xixj;
i.e., (x) = (xi, xj|i = 1, ..., d, j = i, ..., d). The
linear perceptron working in (X) effectively im-
plements a non-linear classifier in the original in-
put space X. One shortcoming of this approach is
that it inflates considerably the feature representa-
tion and might not scale. In general, the number of
features of degree g over an input space of dimen-
sion d is d+g1
</bodyText>
<equation confidence="0.7838425">
g
\x01
</equation>
<bodyText confidence="0.971528357142857">
. In practice, a second-order fea-
ture map can be handled with reasonable efficiency
by the perceptron. We call this the 2nd-order model,
which uses a modified scoring function:
g(x, k) = f((x), k) (5)
where also k is h-dimensional. The proposed fea-
ture map is equivalent to a polynomial kernel func-
tion of degree two. Yamada and Matsumoto (2003)
have shown that the degree two polynomial ker-
nel has superior accuracy than the linear model and
polynomial kernels of higher degrees. However, us-
ing the dual model is not always practical for depen-
dency parsing. The discriminant function of the dual
model is defined as:
</bodyText>
<equation confidence="0.99418225">
f0
(x, ) = arg max
k
N
X
i=1
k,ihx, xiig
(6)
</equation>
<bodyText confidence="0.995388857142857">
where the weights are associated with class-
instance pairs rather than class-feature pairs. With
respect to the discriminant function of equation (4)
there is an additional summation. In principle, the
inner products can be cached in a Kernel matrix to
speed up training.
There are two shortcomings to using such a model
in dependency parsing. First, if the amount of train-
ing data is large it might not be feasible to store the
Kernel matrix; which for a dataset of size N requires
O(N3) computations and O(N2) space. As an ex-
ample, the number of training instances N in the
Penn Treebank is over 1.8 million, caching the Ker-
nel matrix would require several Terabytes of space.
The second shortcoming is independent of training.
In predicting a tree for unseen sentences the model
will have to recompute the inner products between
the observation and all the support vectors; i.e., all
class-instance pairs with k,i &amp;gt; 0. The second-order
feature map with the perceptron is more efficient and
allows faster training and prediction. Training a sin-
gle parsing model avoids a potential loss of accuracy
that occurs when using the technique of partitioning
the training data according to the POS. Inaccurate
predictions of the POS can affect significantly the
accuracy of the actions predicted, while the single
model is more robust, since the POS is just one of
the many features used in prediction.
</bodyText>
<sectionHeader confidence="0.962673" genericHeader="method">
4 Semantic features
</sectionHeader>
<bodyText confidence="0.997756212121212">
Semantic information is used implicitly in parsing.
For example, conditioning on lexical heads pro-
vides a source of semantic information. There have
been a few attempts at using semantic information
more explicitly. Charniaks 1997 parser (1997), de-
fined probability estimates backed off to word clus-
ters. Collins and Koo (Collins &amp; Koo, 2005) in-
troduced an improved reranking model for parsing
which includes a hidden layer of semantic features.
Yi and Palmer (2005) retrained a constituent parser
in which phrases were annotated with argument in-
formation to improve SRL, however this didnt im-
prove over the output of the basic parser.
In recent years there has been a significant
amount of work on semantic annotation tasks such
as named-entity recognition, semantic role labeling
and relation extraction. There is evidence that de-
pendency and constituent parsing can be helpful in
these and other tasks; e.g., by means of tree ker-
nels in question classification and semantic role la-
beling (Zhang &amp; Lee, 2003; Moschitti, 2006).
It is natural to ask if also the opposite holds:
whether semantic annotations can be used to im-
prove parsing. In particular, it would be interesting
to know if entity-like tags can be used for this pur-
pose. One reason for this is that entity tagging is ef-
ficient and does not seem to need parsing for achiev-
ing top performance. Beyond improving traditional
parsing, independently learned semantic tags might
be helpful in adapting a parser to a new domain. To
the best of our knowledge, no evidence has been pro-
duced yet that annotated semantic information can
improve parsing. In the following we investigate
</bodyText>
<page confidence="0.990761">
137
</page>
<bodyText confidence="0.770336">
\x0cadding entity tags as features of our parser.
</bodyText>
<subsectionHeader confidence="0.994031">
4.1 BBN Entity corpus
</subsectionHeader>
<bodyText confidence="0.987683829268293">
The BBN corpus (BBN, 2005) supplements the Wall
Street Journal Penn Treebank with annotation of a
large set of entity types. The corpus includes an-
notation of 12 named entity types (Person, Facility,
Organization, GPE, Location, Nationality, Product,
Event, Work of Art, Law, Language, and Contact-
Info), nine nominal entity types (Person, Facility,
Organization, GPE, Product, Plant, Animal, Sub-
stance, Disease and Game), and seven numeric types
(Date, Time, Percent, Money, Quantity, Ordinal and
Cardinal). Several of these types are further divided
into subtypes3. This corpus provides adequate sup-
port for experimenting semantic features for parsing.
Figure 1 illustrates the annotation layer provided
by the BBN corpus4. It is interesting to notice one
apparent property of the combination of semantic
tags and dependencies. When we consider segments
composed of several words there is exactly one de-
pendency connecting a token outside the segment
with a token inside the segment; e.g., CBS Inc. is
connected outside only through the token Inc., the
subject of the main verb. With respect to the rest of
the tree, segments tend to form units, with their own
internal structure. Intuitively, this information seems
relevant for parsing. This locally-structured patterns
could help particularly simple algorithms like ours,
which have limited knowledge of the global struc-
ture being built.
Table 2 lists the 40 most frequent categories in
sections 2 to 21 of the BBN corpus, and the per-
centage of all entities they represent together more
than 97%. Sections 2-21 are comprised of 949,853
tokens, 23.5% of the tokens have a non-null BBN
entity tag, on average there is one tagged token every
four. The total number of entities is 139,029, 70.5%
of which are named entities and nominal concepts,
17% are numerical types and the remaining 12.5%
describe time entities.
We designed three new features which extract
simple properties of entities from the semantic an-
notation information:
</bodyText>
<page confidence="0.834912">
3
</page>
<table confidence="0.414856">
BBN Corpus documentation.
</table>
<page confidence="0.83609">
4
</page>
<bodyText confidence="0.738633">
The full label for ORG is ORG:Corporation, and
WOA stands for WorkOfArt:Other.
</bodyText>
<sectionHeader confidence="0.479475" genericHeader="method">
TOKEN
FEATURES Stack Input
</sectionHeader>
<equation confidence="0.993537333333333">
AS-0 = EOS+BIO+TAG 0
AS-1 = EOS+BIO+TAG -1 0 1
AS-2 = EOS+BIO+TAG -2 -1 0 1 2
EOS -2 -1 0 1 2
BIO -2 -1 0 1 2
TAG -2 -1 0 1 2
</equation>
<tableCaption confidence="0.991233">
Table 3. Additional configurations for the models with
</tableCaption>
<listItem confidence="0.7160995">
BBN entity features.
EOS: Distance to the end of the segment; e.g.,
</listItem>
<equation confidence="0.992738">
EOS(Last) = 1, EOS(canceled) = 0;
</equation>
<bodyText confidence="0.9942365">
BIO: The first character of the BBN label
for a token; e.g., BIO(CBS) = B, and
</bodyText>
<equation confidence="0.8323275">
BIO(canceled) = 0;
TAG: Full BBN tag for the token; e.g.,
TAG(CBS) = B-ORG:Corporation,
TAG(week) = I-DATE.
</equation>
<bodyText confidence="0.9989245625">
The feature EOS provides information about the rel-
ative position of the token within a segment with re-
spect to the end of the segment. The feature BIO dis-
criminates tokens with no semantic annotation as-
sociated, from tokens within a segment and token
which start a segment. Finally the feature TAG iden-
tifies the full semantic tag associated with the token.
With respect to the former two features this bears
the most fine-grained semantics. Table 3 summa-
rizes six additional models we implemented. The
first three use all additional features together, ap-
plied to different sets of tokens, while the last three
apply only one feature, on top of the base model,
relative to the next token in the input, the following
two tokens in the input, and the previous two tokens
on the stack.
</bodyText>
<subsectionHeader confidence="0.995445">
4.2 Corpus pre-processing
</subsectionHeader>
<bodyText confidence="0.9992974">
The original BBN corpus has its own tokeniza-
tion which often does not reflect the Penn Tree-
bank tokenization; e.g., when an entity intersects
an hyphenated compound, thus third-highest be-
comes thirdORDINAL - highest. This is problem-
atic for combining entity annotation and dependency
trees. Since our main focus is parsing we re-aligned
the BBN Corpus with the Treebank tokenization.
Thus, for example, when an entity splits a Tree-
bank token we extend the entity boundary to contain
</bodyText>
<page confidence="0.99852">
138
</page>
<table confidence="0.997324666666667">
\x0cWSJ-BBN Corpus Categories
Tag % Tag % Tag % Tag %
PER DESC 15.5 ORG:CORP 13.7 DATE:DATE 9.2 ORG DESC:CORP 8.9
PERSON 8.13 MONEY 6.5 CARDINAL 6.0 PERCENT 3.5
GPE:CITY 3.12 GPE:COUNTRY 2.9 ORG:GOV 2.6 NORP:NATION-TY 1.9
DATE:DURATION 1.8 GPE:PROVINCE 1.5 ORG DESC:GOV 1.4 FAC DESC:BLDG 1.1
ORG:OTHER 0.7 PROD DESC:VEHICLE 0.7 ORG DESC:OTHER 0.6 ORDINAL 0.6
TIME 0.5 GPE DESC:COUNTRY 0.5 SUBST:OTHER 0.5 SUBST:FOOD 0.5
DATE:OTHER 0.4 NORP:POLITICAL 0.4 DATE:AGE 0.4 LOC:REGION 0.3
SUBST:CHEM 0.3 WOA:OTHER 0.3 FAC DESC:OTHER 0.3 SUBST:DRUG 0.3
ANIMAL 0.3 GPE DESC:PROVINCE 0.2 PROD:VEHICLE 0.2 GPE DESC:CITY 0.2
PRODUCT:OTHER 0.2 LAW 0.2 ORG:POLITICAL 0.2 ORG:EDU 0.2
</table>
<tableCaption confidence="0.7866025">
Table 2. The 40 most frequent labels in sections 2 to 21 of the Wall Street Journal BBN Corpus and the percentage of
tags occurrences.
</tableCaption>
<bodyText confidence="0.9765505">
the whole original Treebank token, thus obtaining
third-highestORDINAL in the example above.
</bodyText>
<subsectionHeader confidence="0.999388">
4.3 Semantic tagger
</subsectionHeader>
<bodyText confidence="0.99718384">
We treated semantic tags as POS tags. A tagger
was trained on the BBN gold standard annotation
and used it to annotate development and evaluation
data. We briefly describe the tagger (see (Ciaramita
&amp; Altun, 2006) for more details), a Hidden Markov
Model trained with the perceptron algorithm intro-
duced in (Collins, 2002). The tagger uses Viterbi
decoding. Label to label dependencies are limited to
the previous tag (first order HMM). A generic fea-
ture set for NER based on words, lemmas, POS tags,
and word shape features was used.
The tagger is trained on sections 2-21 of the BBN
corpus. As before, section 22 of the BBN corpus
is used for choosing the perceptrons parameter T.
The taggers model is regularized as described for
Algorithm 2. The full BBN tagset is comprised
of 105 classes organized hierarchically, we ignored
the hierarchical organization and treated each tag as
an independent class in the standard BIO encoding.
The tagger evaluated on section 23 achieves an F-
score of 86.8%. The part of speech for the evalua-
tion/development sections was produced with Tree-
Tagger. As a final remark we notice that the taggers
complexity, linear in the length of the sentence, pre-
serves the parsers complexity.
</bodyText>
<sectionHeader confidence="0.703478" genericHeader="method">
5 Parsing experiments
</sectionHeader>
<subsectionHeader confidence="0.922018">
5.1 Data and setup
</subsectionHeader>
<bodyText confidence="0.9934745">
We used the standard partitions of the Wall Street
Journal Penn Treebank (Marcus et al., 1993); i.e.,
sections 2-21 for training, section 22 for develop-
ment and section 23 for evaluation. The constituent
trees were transformed into dependency trees by
means of a program created by Joakim Nivre that
implements the rules proposed by Yamada and Mat-
sumoto, which in turn are based on the head rules
of Collins parser (Collins, 1999)5. The lemma for
each token was produced using the morph func-
tion of the WordNet (Fellbaum, 1998) library6. The
data in the WSJ sections 22 and 23, both for the
parser and for the semantic tagger, was POS-tagged
using TreeTagger7, which has an accuracy of 97.0%
on section 23.
Training a parsing model on the Wall Street Jour-
nal requires a set of 22 classes: 10 of the 11 labels
in the dependency corpus generated from the Penn
Treebank (e.g., subj, obj, sbar, vmod, nmod, root,
etc.) are paired with both a Left and Right actions.
In addition, there is in one rule for the root label
and one for the Shift action. The total number of
features found in training ranges from two hundred
thousand for the 1st-order model to approximately
20 million of the 2nd-order models.
We evaluated several models, each trained with
1st-order and 2nd-order features. The base model
(BASE) only uses the traditional set of features (cf.
Table 1). Models EOS, BIO and TAG each use only
one type of semantic feature with the configuration
described in Table 3. Models AS-0, AS-1, and AS-2
use all three semantic features for the token on the
stack in AS-0, plus the previous token on the stack
and the new token in the input in AS-1, plus an addi-
</bodyText>
<page confidence="0.913523">
5
</page>
<bodyText confidence="0.837676">
The script is available from
</bodyText>
<footnote confidence="0.5377425">
http://w3.msi.vxu.se/%7enivre/research/Penn2Malt.html
6
http://wordnet.princeton.edu
7
TreeTagger is available from http://www.ims.uni-
stuttgart.de/projekte/corplex/TreeTagger/
</footnote>
<page confidence="0.959981">
139
</page>
<table confidence="0.998332666666667">
\x0c1st-order scores 2nd-order scores
DeSR MODEL LAS UAS Imp LAC LAS UAS Imp LAC
BASE 84.01 85.56 - 88.24 89.20 90.55 - 92.22
EOS 84.89 86.37 +5.6 88.94 89.36 90.64 +1.0 92.37
BIO 84.95 86.37 +6.6 89.06 89.63 90.89 +3.6 92.55
TAG 84.76 86.26 +4.8 88.80 89.54 90.81 +2.8 92.55
AS-0 84.40 85.95 +2.7 88.38 89.41 90.72 +1.8 92.38
AS-1 85.13 86.52 +6.6 89.11 89.57 90.77 +2.3 92.49
AS-2 85.32 86.71 +8.0 89.25 89.87 91.10 +5.8 92.68
</table>
<tableCaption confidence="0.998645">
Table 4. Results of the different models on WSJ section 23 using the CoNLL scores Labeled attachment score (LAS),
</tableCaption>
<bodyText confidence="0.9872725">
Unlabeled attachment score (UAS), and Label accuracy score (LAC). The column labeled Imp reports the improve-
ment in terms of relative error reduction with respect to the BASE model for the UAS score. In bold the best results.
tional token from the stack and an additional token
from the input for AS-2 (cf. Table 3).
</bodyText>
<subsectionHeader confidence="0.97438">
5.2 Results of 2nd-order models
</subsectionHeader>
<bodyText confidence="0.9985563">
Table 4 summarizes the results of all experiments.
We report the following scores, obtained with the
CoNLL-X scoring script: labeled attachment score
(LAS), unlabeled attachment score (UAS) and label
accuracy score (LAC). For the UAS score, the most
frequently reported, we include the improvement in
relative error reduction.
The 2nd-order base model improves on all mea-
sures over the 1st-order model by approximately
5%. The UAS score is 90.55%, with an improve-
ment of 4.9%. The magnitude of the improve-
ment is remarkable and reflects the 4.6% improve-
ment that Yamada and Matsumoto (Yamada &amp; Mat-
sumoto, 2003) report going from the linear SVM to
the polynomial of degree two. Our base models ac-
curacy (90.55% UAS) compares well with the ac-
curacy of the parsers based on the polynomial ker-
nel trained with SVM of Yamada and Matsumoto
(UAS 90.3%), and Hall et al. (2006) (UAS 89.4%).
We notice in particular that, given the lack of non-
projective cases/rules, the parser of Hall et al. (2006)
is almost identical to our parser, hence the differ-
ence in accuracy (+1.1%) might effectively be due
to a better classifier. Yamada &amp; Matsumotos parser
is slightly more complex than our parser, and has
quadratic worst-case complexity. Overall, the accu-
racy of the 2nd-order parser is comparable to that of
the 1st-order MST parser (90.7%).
There is no direct evidence that our perceptron
produces better classifiers than SVM. Rather, the
pattern of results produced by the perceptron seems
comparable to that of SVM (Yamada &amp; Matsumoto,
2003). This is a useful finding in itself, given that
the former is more efficient: perceptrons update is
linear while SVM solves a quadratic problem at each
update. However, one major difference between the
two approaches lies in the fact that learning with the
primal model does not require splitting the model
by Part-of-Speech, or other means. As a conse-
quence, beyond the greater simplicity, our method
might benefit from not depending so strongly on the
quality of POS tagging. POS information is encoded
as a feature and contributes its weight to the selec-
tion of the parsing action, together with all addi-
tionally available information. In the SVM-trained
methods the model that makes the prediction for the
parsing rule is essentially chosen by an oracle, the
prediction of the POS tagger. Furthermore, it might
be argued that learning a single model makes a bet-
ter use of the training data by exploiting the cor-
relations between all datapoints, while in the dual
split-training case the interaction is limited to dat-
apoints in the same partition. In any case, second-
order feature maps could be used also with SVM or
other classifiers. The advantage of using the per-
ceptron lies in the unchallenged accuracy/efficiency
trade-off. Finally, we recall that training in the pri-
mal model can be performed fully on-line without
affecting the resulting model nor the complexity of
the algorithm.
</bodyText>
<subsectionHeader confidence="0.889497">
5.3 Results of models with semantic features
</subsectionHeader>
<bodyText confidence="0.987624">
All models based on semantic features improve over
the base model on all measures. The best configura-
</bodyText>
<page confidence="0.995274">
140
</page>
<table confidence="0.99567975">
\x0cParser UAS
Hall et al. 06 89.4
Yamada &amp; Matsumoto 03 90.3
DeSR 90.55
McDonald &amp; Pereira 1st-order MST 90.7
DeSR AS-2 91.1
McDonald &amp; Pereira 2nd-order MST 91.5
Sagae &amp; Lavie 06 92.7
</table>
<tableCaption confidence="0.999197">
Table 5. Comparison of main results on the Penn Tree-
</tableCaption>
<bodyText confidence="0.98851075">
bank dataset.
tion is that of model AS-2 which extracts all seman-
tic features from the widest context. In the 1st-order
AS-2 model the improvement, 86.71% UAS (+8%
relative error reduction) is more marked than in the
2nd-order AS-2 model, 91.1% UAS (+5.8% error
reduction). A possible simple exaplanation is that
some information captured by the semantic features
is correlated with other higher-order features which
do not occur in the 1st-order encoding. Overall the
accuracy of the DeSR parser with semantic informa-
tion is slightly inferior to that of the second-order
MST parser (McDonald &amp; Pereira, 2006) (91.5%
UAS). The best result on this dataset to date (92.7%
UAS) is that of Sagae and Lavie (Sagae &amp; Lavie,
2006) who use a parser which combines the predic-
tions of several pre-existing parsers, including Mc-
Donalds and Nivres parsers. Table 5 lists the main
results to date on the version of the Penn Treebank
for dependency parsing task used in this paper.
In Table 4 we also evaluate the gain obtained by
adding one semantic feature type at a time (cf. rows
EOS/BIO/TAG). These results show that all seman-
tic features provide some improvement (with the du-
bious case of EOS in the 2nd-order model). The
BIO encoding seems to produce the most accurate
features. This could be promising because it sug-
gests that the benefit does not depend only on the
specific tags, but that the segmentation in itself is
important. Hence tagging could improve the adapta-
tion of parsers to new domains even if only generic
tagging methods are available.
</bodyText>
<subsectionHeader confidence="0.560906">
5.4 Remarks on efficiency
</subsectionHeader>
<bodyText confidence="0.512405">
All experiments were performed on a 2.4GHz AMD
Opteron CPU machine with 32GB RAM. The 2nd-
order parser uses almost 3GB of memory. While
</bodyText>
<table confidence="0.9956872">
Parsing time/sec
Parser English Chinese
MST 2n-order 97.52 59.05
MST 1st-order 76.62 49.13
DeSR 36.90 21.22
</table>
<tableCaption confidence="0.8844975">
Table 6. Parsing times for the CoNNL 2007 English and
Chinese datasets for MST and DeSR.
</tableCaption>
<bodyText confidence="0.998805575757576">
it is several times slower and larger than the 1st-
order model8 the 2nd-order model performance is
still competitive. It takes 3 minutes (user time) to
parse section 23, POS tagging included. In train-
ing, the model takes about 1 hour to process the full
dataset once. As a comparison, Hall et al. (2006)
reports 1.5 hours for training the partitioned SVM
model and 10 minutes for parsing the evaluation set
on the same Penn Treebank data. We also compared
directly the parsing time of our parser with that of
the MST parser using the version 0.4.3 of MST-
Parser9. For these experiments we used two datasets
from the CoNLL 2007 shared task for English and
Chinese. Table 6 reports the times, in seconds, to
parse the test sets for these languages on a 3.3GHz
Xeon machine with 4 GB Ram, of the MST 1st and
2nd-order parser and DeSR parser (without semantic
features).
The architecture of the model presented here of-
fers several options for optimization. For exam-
ple, implementing the models with full vectors
rather than hash tables speeds up parsing by a factor
of three, at the expense of memory. Alternatively,
memory load in training can be reduced, at the ex-
pense of time, by using on-line training. However,
the most valuable option for space need reduction
might be to filter out low-frequency second-order
features. Since the frequency of such features seems
to follow a power law distribution, this reduces sig-
nificantly the feature space size even for low thresh-
olds at small accuracy expense. In this paper how-
ever we focused on the full model, no approxima-
tions were required to run the experiments.
</bodyText>
<page confidence="0.960495">
8
</page>
<bodyText confidence="0.9200595">
The 1st-order parser takes 7 seconds (user time) to process
Section 23.
</bodyText>
<page confidence="0.81797">
9
</page>
<tableCaption confidence="0.336052">
Available from sourceforge.net.
</tableCaption>
<page confidence="0.974676">
141
</page>
<sectionHeader confidence="0.891362" genericHeader="conclusions">
\x0c6 Conclusion
</sectionHeader>
<bodyText confidence="0.999910608695652">
We explored the design space of a dependency
parser by modeling and extending the feature repre-
sentation, while adopting one of the simplest parsing
architecture: a single-pass deterministic shift-reduce
algorithm trained with a regularized multiclass per-
ceptron. We showed that with the perceptron it is
possible to adopt higher-order feature maps equiva-
lent to polynomial kernels without need of approx-
imating the model (although this remains an option
for optimization). The resulting models achieve ac-
curacies comparable (or better) to more complex ar-
chitectures based on dual SVM training, and faster
parsing on unseen data. With respect to learning, it is
possible that more sophisticated formulations of the
perceptron (e.g. MIRA (Crammer &amp; Singer, 2003))
could provide further gains in accuracy, as shown
with the MST parser (McDonald et al., 2005).
We also experimented with novel types of se-
mantic features, extracted from the annotations pro-
duced by an entity tagger trained on the BBN cor-
pus. This model further improves over the standard
model yielding an additional 5.8% relative error re-
duction. Although the magnitude of the improve-
ment is not striking, to the best of our knowledge
this is the first encouraging evidence that annotated
semantic information can improve parsing and sug-
gests several options for further research. For exam-
ple, this finding might indicate that this type of ap-
proach, which combines semantic tagging and pars-
ing, is viable for the adaptation of parsing to new
domains for which semantic taggers exist. Seman-
tic features could be also easily included in other
types of dependency parsing algorithms, e.g., MST,
and in current methods for constituent parse rerank-
ing (Collins, 2000; Charniak &amp; Johnson, 2005).
For future research several issues concerning the
semantic features could be tackled. We notice that
more complex semantic features can be designed
and evaluated. For example, it might be useful to
guess the head of segments with simple heuris-
tics, i.e., the guess the node which is more likely to
connect the segment with the rest of the tree, which
all internal components of the entity depend upon.
It would be also interesting to extract semantic fea-
tures from taggers trained on different datasets and
based on different tagsets.
</bodyText>
<sectionHeader confidence="0.966906" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998155">
The first author would like to thank Thomas Hof-
mann for useful inputs concerning the presentation
of the issue of higher-order feature representations
of Section 3.4. We would also like to thank Brian
Roark and the anonymous reviewers for useful com-
ments and pointers to related work.
</bodyText>
<sectionHeader confidence="0.992246" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999366947368421">
G. Attardi. 2006. Experiments with a Multilanguage
Non-Projective Dependency Parser. In Proceedings of
CoNNL-X 2006.
BBN. 2005. BBN Pronoun Coreference and Entity Type
Corpus. Linguistic Data Consortium (LDC) catalog
number LDC2005T33.
S. Buchholz and E. Marsi. 2006. Introduction to
CoNNL-X Shared Task on Multilingual Dependency
Parsing. In Proceedings of CoNNL-X 2006.
X. Carreras and L. Marquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proceedings of CoNLL 2005.
X. Carreras, M. Surdeanu, and L. Marquez. 2006 Pro-
jective Dependency Parsing with Perceptron. In Pro-
ceedings of CoNLL-X.
E. Charniak. 1997. Statistical Parsing with a Context-
Free Grammar and Word Statistics. In Proceedings of
the Fourteenth National Conference on Artificial Intel-
ligence AAAI.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
In Proceedings of ACL 2005.
M. Ciaramita and Y. Altun. 2006. Broad-Coverage Sense
Disambiguation and Information Extraction with a Su-
persense Sequence Tagger. In Proceedings of EMNLP
2006.
M. Ciaramita and M. Johnson. 2003. Supersense Tag-
ging of Unknown Nouns in WordNet. In Proceedings
of EMNLP 2003.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. Thesis, University
of Pennsylvania.
M. Collins. 2000. Discriminative Reranking for Natural
Language Parsing. In Proceedings of ICML 2000.
M. Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings of
EMNLP 2002.
</reference>
<page confidence="0.967341">
142
</page>
<reference confidence="0.999545">
\x0cM. Collins and T. Koo. 2005. Hidden-Variable Mod-
els for Discriminative Reranking. In Proceedings of
EMNLP 2005.
M. Collins and B. Roark. 2004. Incremental Parsing
with the Perceptron Algorithm. In Proceedings of ACL
2004.
K. Crammer and Y. Singer. 2003. Ultraconservative On-
line Algorithms for Multiclass Problems. Journal of
Machine Learning Research 3: pp.951-991.
A. Culotta and J. Sorensen. 2004. Dependency Tree Ker-
nels for Relation Extraction. In Proceedings of ACL
2004.
Y. Ding and M. Palmer. 2005. Machine Translation us-
ing Probabilistic Synchronous Dependency Insertion
Grammars. In Proceedings of ACL 2005.
J. Eisner. 2000. Bilexical Grammars and their Cubic-
Time Parsing Algorithms. In H.C. Bunt and A. Ni-
jholt, eds. New Developments in Natural Language
Parsing, pp. 29-62. Kluwer Academic Publishers.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database MIT Press, Cambridge, MA. 1969.
J. Hall, J. Nivre and J. Nilsson. 2006. Discriminative
Classifiers for Deterministic Dependency Parsing. In
Proceedings of the COLING/ACL 2006.
T. Kalt. 2004. Induction of Greedy Controllers for Deter-
ministic Treebank Parsers. In Proceedings of EMNLP
2004.
M. Marcus, B. Santorini and M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The
Penn Treebank. Computational Linguistics, 19(2): pp.
313-330.
R. McDonald. 2006. Discriminative Training and Span-
ning Tree Algorithms for Dependency Parsing. Ph.D.
Thesis, University of Pennsylvania.
R. McDonald, F. Pereira, K. Ribarov and J. Hajic. 2005.
Non-projective Dependency Parsing using Spanning
Tree Algorithms. In Proceedings of HLT-EMNLP
2005.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proceedings of EACL 2006.
M.L. Minsky and S.A. Papert. 1969. Perceptrons: An
Introduction to Computational Geometry. MIT Press,
Cambridge, MA. 1969.
A. Moschitti. 2006. Efficient Convolution Kernels for
Dependency and Constituent Syntactic Trees. In Pro-
ceedings of ECML 2006.
J. Nivre. 2004. Incrementality in Deterministic Depen-
dency Parsing. In Incremental Parsing: Bringing En-
gineering and Cognition Together. Workshop at ACL-
2004.Spain, 50-57.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel and D. Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing, In Proceedings
of EMNLP-CoNLL 2007.
J. Nivre and M. Scholz. 2004. Deterministic Depen-
dency Parsing of English Text. In Proceedings of
COLING 2004.
V. Punyakanok, D. Roth, and W. Yih. 2005. The Neces-
sity of Syntactic Parsing for Semantic Role Labeling.
In Proceedings of IJCAI 2005.
F. Rosemblatt. 1958. The Perceptron: A Probabilistic
Model for Information Storage and Organization in the
Brain. Psych. Rev., 68: pp. 386-407.
K. Sagae and A. Lavie. 2005. Parser Combination by
Reparsing. In Proceedings of HLT-NAACL 2006.
F. Sha and F. Pereira. 2003. Shallow Parsing with Condi-
tional Random Fields. In Proceedings of HLT-NAACL
2003.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines. In
Proceedings of the Eighth International Workshop on
Parsing Technologies. Nancy, France.
S. Yi and M. Palmer. 2005. The Integration of Syntactic
Parsing and Semantic Role Labeling. In Proceedings
of CoNLL 2005.
A. Wong and D. Wu. 1999. Learning a Lightweight De-
terministic Parser. In Proceedings of EUROSPEECH
1999.
D. Zhang and W.S. Less. 2003. Question Classification
using Support Vector Machines. In Proceedings of SI-
GIR 2003.
</reference>
<page confidence="0.990062">
143
</page>
<figure confidence="0.248361">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.220783">
<note confidence="0.980063333333333">b&amp;apos;Proceedings of the 10th Conference on Parsing Technologies, pages 133143, Prague, Czech Republic, June 2007. c 2007 Association for Computational Linguistics</note>
<title confidence="0.762582">Dependency Parsing with Second-Order Feature Maps and Annotated Semantic Information</title>
<author confidence="0.997789">Massimiliano Ciaramita</author>
<affiliation confidence="0.958445">Yahoo! Research</affiliation>
<address confidence="0.76805">Ocata 1, S-08003 Barcelona, Spain</address>
<email confidence="0.999709">massi@yahoo-inc.com</email>
<author confidence="0.995932">Giuseppe Attardi</author>
<affiliation confidence="0.9947085">Dipartimento di Informatica Universita di Pisa</affiliation>
<address confidence="0.9779365">L. B. Pontecorvo 3, I-56127 Pisa, Italy</address>
<email confidence="0.998187">attardi@di.unipi.it</email>
<abstract confidence="0.9839844">This paper investigates new design options for the feature space of a dependency parser. We focus on one of the simplest and most efficient architectures, based on a deterministic shift-reduce algorithm, trained with the perceptron. By adopting second-order feature maps, the primal form of the perceptron produces models with comparable accuracy to more complex architectures, with no need for approximations. Further gains in accuracy are obtained by designing features for parsing extracted from semantic annotations generated by a tagger. We provide experimental evaluations on the Penn Treebank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Attardi</author>
</authors>
<title>Experiments with a Multilanguage Non-Projective Dependency Parser.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNNL-X</booktitle>
<contexts>
<context position="2248" citStr="Attardi, 2006" startWordPosition="330" endWordPosition="331">ve been evaluated in the CoNLL 2006 and 2007 shared tasks (Buchholz &amp; Marsi, 2006; Nivre et al., 2007). Dependency parsing is simpler than constituency parsing, since dependency trees do not have extra non-terminal nodes and there is no need for a grammar to generate them. Approaches to dependency parsing either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). Deterministic dependency parsers which run in linear time have also been developed (Nivre &amp; Scholz, 2004; Attardi, 2006). These parsers process the sentence sequentially, hence their efficiency makes them suitable for processing large amounts of text, as required, for example, in information retrieval applications. Recent work on dependency parsing has highlighted the benefits of using rich feature sets and high-order modeling. Yamada and Matsumoto (2003) showed that learning an SVM model in the dual space with higher-degree polynomial kernel functions improves significantly the parsers accuracy. McDonald and Pereira (2006) have shown that incorporating second order features relating to adjacent edge pairs impr</context>
<context position="6338" citStr="Attardi (2006)" startWordPosition="981" endWordPosition="982">contains entity annotations which will be explained below in Section 4.1. Shift/Reduce actions to use in building the tree. Parsing is cast as a classification problem: at each step the parser applies a classifier to the features representing its current state to predict which action to perform on the tree. Similar deterministic approaches to parsing have been investigated also in the context of constituent parsing (Wong &amp; Wu, 1999; Kalt, 2004). Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. Attardi (2006) proposed a variant of the rules that handle non-projective relations while parsing deterministically in a single pass. Shift-reduce algorithms are simple and efficient, yet competitive in terms of accuracy: in the CoNLL-X shared task, for several languages, there was no statistically significant difference between second-order MST parsers and shift-reduce parsers. 3 A shift-reduce parser We build upon DeSR, the shift-reduce parser described in (Attardi, 2006). This and Nivre and Scholzs (2004) provide among the simplest and most efficient methods. This parser constructs dependency trees by sc</context>
<context position="8796" citStr="Attardi, 2006" startWordPosition="1384" endWordPosition="1385"> tokens), d D (the set of dependencies). Given an input sentence s, the parser is initialized to h, s, i, and terminates at configuration hs, , Ai. There are three parsing schemata: Shift hS,n|I,Ai hn|S,I,Ai (1) Rightr hs|S,n|I,Ai hS,n|I,A{(s,r,n)}i (2) Leftr hs|S,n|I,Ai hS,s|I,A{(n,r,s)}i (3) The Shift rule advances on the input; each Leftr and Rightr rule creates a link r between the next input token n and the top token on the stack s. For producing labeled dependencies the rules Leftr and Rightr are instantiated several times once for each dependency label. Additional parsing actions (cf. (Attardi, 2006)) have been introduced for handling non-projective dependency trees: i.e., trees that cannot be drawn in the plane without crossing edges. However, they are not needed in the experiments reported here, because in the Penn Treebank used in our experiments dependencies are extracted without considering empty nodes and the resulting trees are all projective2. The pseudo code in Algorithm 1 reproduces schematically the parsing process. The function getContext() extracts a vector of features x relative to the structure built up to that point from the context of the current token, i.e., from a subse</context>
</contexts>
<marker>Attardi, 2006</marker>
<rawString>G. Attardi. 2006. Experiments with a Multilanguage Non-Projective Dependency Parser. In Proceedings of CoNNL-X 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BBN</author>
</authors>
<title>BBN Pronoun Coreference and Entity Type Corpus. Linguistic Data Consortium (LDC) catalog number LDC2005T33.</title>
<date>2005</date>
<contexts>
<context position="18907" citStr="BBN, 2005" startWordPosition="3135" endWordPosition="3136">arsing. In particular, it would be interesting to know if entity-like tags can be used for this purpose. One reason for this is that entity tagging is efficient and does not seem to need parsing for achieving top performance. Beyond improving traditional parsing, independently learned semantic tags might be helpful in adapting a parser to a new domain. To the best of our knowledge, no evidence has been produced yet that annotated semantic information can improve parsing. In the following we investigate 137 \x0cadding entity tags as features of our parser. 4.1 BBN Entity corpus The BBN corpus (BBN, 2005) supplements the Wall Street Journal Penn Treebank with annotation of a large set of entity types. The corpus includes annotation of 12 named entity types (Person, Facility, Organization, GPE, Location, Nationality, Product, Event, Work of Art, Law, Language, and ContactInfo), nine nominal entity types (Person, Facility, Organization, GPE, Product, Plant, Animal, Substance, Disease and Game), and seven numeric types (Date, Time, Percent, Money, Quantity, Ordinal and Cardinal). Several of these types are further divided into subtypes3. This corpus provides adequate support for experimenting sem</context>
</contexts>
<marker>BBN, 2005</marker>
<rawString>BBN. 2005. BBN Pronoun Coreference and Entity Type Corpus. Linguistic Data Consortium (LDC) catalog number LDC2005T33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>Introduction to CoNNL-X Shared Task on Multilingual Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNNL-X</booktitle>
<contexts>
<context position="1715" citStr="Buchholz &amp; Marsi, 2006" startWordPosition="244" endWordPosition="247">ndency tree represents a sentence as a labeled directed graph encoding syntactic and semantic information. The labels on the arcs can represent basic grammatical relations such as subject and object. Dependency trees capture grammatical structures that can be useful in several language processing tasks such as information extraction (Culotta &amp; Sorensen, 2004) and machine translation (Ding &amp; Palmer, 2005). Dependency treebanks are becoming available in many languages, and several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks (Buchholz &amp; Marsi, 2006; Nivre et al., 2007). Dependency parsing is simpler than constituency parsing, since dependency trees do not have extra non-terminal nodes and there is no need for a grammar to generate them. Approaches to dependency parsing either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). Deterministic dependency parsers which run in linear time have also been developed (Nivre &amp; Scholz, 2004; Attardi, 2006). These parsers process the sentence sequentially, hence their effi</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. Introduction to CoNNL-X Shared Task on Multilingual Dependency Parsing. In Proceedings of CoNNL-X 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L Marquez</author>
</authors>
<title>Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling.</title>
<date>2005</date>
<marker>Carreras, Marquez, 2005</marker>
<rawString>X. Carreras and L. Marquez. 2005. Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling.</rawString>
</citation>
<citation valid="true">
<date>2005</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<contexts>
<context position="4227" citStr="(2005)" startWordPosition="637" endWordPosition="637">feature pairs. Also the augmented feature sets impose additional computational costs. However, excellent efficiency/accuracy trade-off is achieved by using the perceptron algorithm, without the need to resort to approximations, producing high-accuracy classifiers based on a single model. We also evaluate a novel set of features for parsing. Recently various forms of shallow semantic processing have been investigated such as namedentity recognition (NER), semantic role labeling (SRL) and relation extraction. Syntactic parsing can provide useful features for these tasks; e.g., Punyakanok et al. (2005) show that full parsing is effective for semantic role labeling (see also related approaches evaluated within the CoNNL 2005 shared task (Carreras et al., 2005)). However, no evidence has been provided so far that annotated semantic information can be leveraged for improving parser performance. We report experiments showing that adding features extracted by an entity tagger improves the accuracy of a dependency parser. 2 Dependency parsing A dependency parser takes as input a sentence s and returns a dependency graph d. Figure 1 shows a dependency tree for the sentence Last week CBS Inc. cance</context>
<context position="17632" citStr="(2005)" startWordPosition="2922" endWordPosition="2922">ns predicted, while the single model is more robust, since the POS is just one of the many features used in prediction. 4 Semantic features Semantic information is used implicitly in parsing. For example, conditioning on lexical heads provides a source of semantic information. There have been a few attempts at using semantic information more explicitly. Charniaks 1997 parser (1997), defined probability estimates backed off to word clusters. Collins and Koo (Collins &amp; Koo, 2005) introduced an improved reranking model for parsing which includes a hidden layer of semantic features. Yi and Palmer (2005) retrained a constituent parser in which phrases were annotated with argument information to improve SRL, however this didnt improve over the output of the basic parser. In recent years there has been a significant amount of work on semantic annotation tasks such as named-entity recognition, semantic role labeling and relation extraction. There is evidence that dependency and constituent parsing can be helpful in these and other tasks; e.g., by means of tree kernels in question classification and semantic role labeling (Zhang &amp; Lee, 2003; Moschitti, 2006). It is natural to ask if also the oppo</context>
</contexts>
<marker>2005</marker>
<rawString>In Proceedings of CoNLL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>M Surdeanu</author>
<author>L Marquez</author>
</authors>
<title>Projective Dependency Parsing with Perceptron.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X.</booktitle>
<contexts>
<context position="11926" citStr="Carreras et al. (2006)" startWordPosition="1939" endWordPosition="1942">d . Given a datapoint x X, a ddimensional vector of binary features in the input space X, a parsing action is chosen with a winnertake-all discriminant function: estimateAction(x, ) = arg max k f(x, k) (4) when using a linear classifier, such as the perceptron or SVM, f(u, v) = hu, vi is the inner product between vectors u and v. We learn the parameters from the training data with the perceptron (Rosemblatt, 1958), in the online multiclass formulation of the algorithm (Crammer &amp; Singer, 2003) with uniform negative updates. The perceptron has been used in previous work on dependency parsing by Carreras et al. (2006), with a parser based on Eisners algorithm (Eisner, 2000), and also on incremental constituent parsing (Collins &amp; Roark, 2006). Also the MST parser of McDonald uses a variant of the perceptron algorithm (McDonald, 2006). The choice is motivated by the simplicity and performance of perceptrons, which have proved competitive on a number of tasks; e.g., in shallow parsing, where perceptrons performance is comparable to that of Conditional Random Field models (Sha &amp; Pereira, 2003). The only adjustable parameter of the model is the number of instances T to use for training. We fixed T using the dev</context>
</contexts>
<marker>Carreras, Surdeanu, Marquez, 2006</marker>
<rawString>X. Carreras, M. Surdeanu, and L. Marquez. 2006 Projective Dependency Parsing with Perceptron. In Proceedings of CoNLL-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical Parsing with a ContextFree Grammar and Word Statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence AAAI.</booktitle>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997. Statistical Parsing with a ContextFree Grammar and Word Statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelligence AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-Fine nBest Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<contexts>
<context position="36415" citStr="Charniak &amp; Johnson, 2005" startWordPosition="6040" endWordPosition="6043">nitude of the improvement is not striking, to the best of our knowledge this is the first encouraging evidence that annotated semantic information can improve parsing and suggests several options for further research. For example, this finding might indicate that this type of approach, which combines semantic tagging and parsing, is viable for the adaptation of parsing to new domains for which semantic taggers exist. Semantic features could be also easily included in other types of dependency parsing algorithms, e.g., MST, and in current methods for constituent parse reranking (Collins, 2000; Charniak &amp; Johnson, 2005). For future research several issues concerning the semantic features could be tackled. We notice that more complex semantic features can be designed and evaluated. For example, it might be useful to guess the head of segments with simple heuristics, i.e., the guess the node which is more likely to connect the segment with the rest of the tree, which all internal components of the entity depend upon. It would be also interesting to extract semantic features from taggers trained on different datasets and based on different tagsets. Acknowledgments The first author would like to thank Thomas Hof</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-Fine nBest Parsing and MaxEnt Discriminative Reranking.</rawString>
</citation>
<citation valid="true">
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="4227" citStr="(2005)" startWordPosition="637" endWordPosition="637">feature pairs. Also the augmented feature sets impose additional computational costs. However, excellent efficiency/accuracy trade-off is achieved by using the perceptron algorithm, without the need to resort to approximations, producing high-accuracy classifiers based on a single model. We also evaluate a novel set of features for parsing. Recently various forms of shallow semantic processing have been investigated such as namedentity recognition (NER), semantic role labeling (SRL) and relation extraction. Syntactic parsing can provide useful features for these tasks; e.g., Punyakanok et al. (2005) show that full parsing is effective for semantic role labeling (see also related approaches evaluated within the CoNNL 2005 shared task (Carreras et al., 2005)). However, no evidence has been provided so far that annotated semantic information can be leveraged for improving parser performance. We report experiments showing that adding features extracted by an entity tagger improves the accuracy of a dependency parser. 2 Dependency parsing A dependency parser takes as input a sentence s and returns a dependency graph d. Figure 1 shows a dependency tree for the sentence Last week CBS Inc. cance</context>
<context position="17632" citStr="(2005)" startWordPosition="2922" endWordPosition="2922">ns predicted, while the single model is more robust, since the POS is just one of the many features used in prediction. 4 Semantic features Semantic information is used implicitly in parsing. For example, conditioning on lexical heads provides a source of semantic information. There have been a few attempts at using semantic information more explicitly. Charniaks 1997 parser (1997), defined probability estimates backed off to word clusters. Collins and Koo (Collins &amp; Koo, 2005) introduced an improved reranking model for parsing which includes a hidden layer of semantic features. Yi and Palmer (2005) retrained a constituent parser in which phrases were annotated with argument information to improve SRL, however this didnt improve over the output of the basic parser. In recent years there has been a significant amount of work on semantic annotation tasks such as named-entity recognition, semantic role labeling and relation extraction. There is evidence that dependency and constituent parsing can be helpful in these and other tasks; e.g., by means of tree kernels in question classification and semantic role labeling (Zhang &amp; Lee, 2003; Moschitti, 2006). It is natural to ask if also the oppo</context>
</contexts>
<marker>2005</marker>
<rawString>In Proceedings of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>Y Altun</author>
</authors>
<title>Broad-Coverage Sense Disambiguation and Information Extraction with a Supersense Sequence Tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="23898" citStr="Ciaramita &amp; Altun, 2006" startWordPosition="3951" endWordPosition="3954"> 0.3 FAC DESC:OTHER 0.3 SUBST:DRUG 0.3 ANIMAL 0.3 GPE DESC:PROVINCE 0.2 PROD:VEHICLE 0.2 GPE DESC:CITY 0.2 PRODUCT:OTHER 0.2 LAW 0.2 ORG:POLITICAL 0.2 ORG:EDU 0.2 Table 2. The 40 most frequent labels in sections 2 to 21 of the Wall Street Journal BBN Corpus and the percentage of tags occurrences. the whole original Treebank token, thus obtaining third-highestORDINAL in the example above. 4.3 Semantic tagger We treated semantic tags as POS tags. A tagger was trained on the BBN gold standard annotation and used it to annotate development and evaluation data. We briefly describe the tagger (see (Ciaramita &amp; Altun, 2006) for more details), a Hidden Markov Model trained with the perceptron algorithm introduced in (Collins, 2002). The tagger uses Viterbi decoding. Label to label dependencies are limited to the previous tag (first order HMM). A generic feature set for NER based on words, lemmas, POS tags, and word shape features was used. The tagger is trained on sections 2-21 of the BBN corpus. As before, section 22 of the BBN corpus is used for choosing the perceptrons parameter T. The taggers model is regularized as described for Algorithm 2. The full BBN tagset is comprised of 105 classes organized hierarchi</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>M. Ciaramita and Y. Altun. 2006. Broad-Coverage Sense Disambiguation and Information Extraction with a Supersense Sequence Tagger. In Proceedings of EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>M Johnson</author>
</authors>
<title>Supersense Tagging of Unknown Nouns in WordNet.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="12979" citStr="Ciaramita &amp; Johnson, 2003" startWordPosition="2112" endWordPosition="2115"> Conditional Random Field models (Sha &amp; Pereira, 2003). The only adjustable parameter of the model is the number of instances T to use for training. We fixed T using the development portion of the data. In our experiments, the best value is between 20 and 30 times the size of the training data. To regularize the model we take as the final model the average of all weight vectors posited during training (Collins, 2002). Algorithm 2 illustrates the perceptron learning procedure. The final average model can be computed efficiently during training without storing the individual vectors (e.g., see (Ciaramita &amp; Johnson, 2003)). Algorithm 2: Average multiclass perceptron input : S = (xi, yi)N ; 0 k = ~ 0, k Y for t = 1 to T do choose j Et = {r Y : hxj, t ri hxj, t yj i} if |Et |&amp;gt; 0 then t+1 r = t r xj |Et |, r Et t+1 yj = t yj + xj output: k = 1 T P t t k, k Y 3.4 Higher-order feature spaces Yamada and Matsumoto (2003) and McDonald and Pereira (2006) have shown that higher-order feature representations and modeling can improve parsing accuracy, although at significant computational costs. To make SVM training feasible in the dual model with polynomial kernels, Yamada and Matsumoto split the training data into sever</context>
</contexts>
<marker>Ciaramita, Johnson, 2003</marker>
<rawString>M. Ciaramita and M. Johnson. 2003. Supersense Tagging of Unknown Nouns in WordNet. In Proceedings of EMNLP 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="25374" citStr="Collins, 1999" startWordPosition="4200" endWordPosition="4201">h TreeTagger. As a final remark we notice that the taggers complexity, linear in the length of the sentence, preserves the parsers complexity. 5 Parsing experiments 5.1 Data and setup We used the standard partitions of the Wall Street Journal Penn Treebank (Marcus et al., 1993); i.e., sections 2-21 for training, section 22 for development and section 23 for evaluation. The constituent trees were transformed into dependency trees by means of a program created by Joakim Nivre that implements the rules proposed by Yamada and Matsumoto, which in turn are based on the head rules of Collins parser (Collins, 1999)5. The lemma for each token was produced using the morph function of the WordNet (Fellbaum, 1998) library6. The data in the WSJ sections 22 and 23, both for the parser and for the semantic tagger, was POS-tagged using TreeTagger7, which has an accuracy of 97.0% on section 23. Training a parsing model on the Wall Street Journal requires a set of 22 classes: 10 of the 11 labels in the dependency corpus generated from the Penn Treebank (e.g., subj, obj, sbar, vmod, nmod, root, etc.) are paired with both a Left and Right actions. In addition, there is in one rule for the root label and one for the</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. Thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative Reranking for Natural Language Parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="5195" citStr="Collins, 2000" startWordPosition="799" endWordPosition="800">ed by an entity tagger improves the accuracy of a dependency parser. 2 Dependency parsing A dependency parser takes as input a sentence s and returns a dependency graph d. Figure 1 shows a dependency tree for the sentence Last week CBS Inc. canceled The People Next Door.1. Dependencies are represented as labeled arrows from the head of the relation to the modifier word; thus, in the example, Inc. is the modifier of a dependency labeled SUB (subject) to the main verb, the head, canceled. In statistical syntactic parsing a generator (e.g., a PCFG) is used to produce a number of candidate trees (Collins, 2000) with associated probability scores. This approach has been used also for dependency parsing, generating spanning trees as candidates and computing the maximum spanning tree (MST) using discriminative learning algorithms (McDonald et al., 2005). Second-order MST dependency parsers currently represent the state of the art in terms of accuracy. Yamada and Matsumoto (2003) proposed a deterministic classifierbased parser. Instead of learning directly which tree to assign to a sentence, the parser learns which 1 The figure also contains entity annotations which will be explained below in Section 4.</context>
<context position="36388" citStr="Collins, 2000" startWordPosition="6038" endWordPosition="6039">lthough the magnitude of the improvement is not striking, to the best of our knowledge this is the first encouraging evidence that annotated semantic information can improve parsing and suggests several options for further research. For example, this finding might indicate that this type of approach, which combines semantic tagging and parsing, is viable for the adaptation of parsing to new domains for which semantic taggers exist. Semantic features could be also easily included in other types of dependency parsing algorithms, e.g., MST, and in current methods for constituent parse reranking (Collins, 2000; Charniak &amp; Johnson, 2005). For future research several issues concerning the semantic features could be tackled. We notice that more complex semantic features can be designed and evaluated. For example, it might be useful to guess the head of segments with simple heuristics, i.e., the guess the node which is more likely to connect the segment with the rest of the tree, which all internal components of the entity depend upon. It would be also interesting to extract semantic features from taggers trained on different datasets and based on different tagsets. Acknowledgments The first author wou</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>M. Collins. 2000. Discriminative Reranking for Natural Language Parsing. In Proceedings of ICML 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="12773" citStr="Collins, 2002" startWordPosition="2084" endWordPosition="2085">s motivated by the simplicity and performance of perceptrons, which have proved competitive on a number of tasks; e.g., in shallow parsing, where perceptrons performance is comparable to that of Conditional Random Field models (Sha &amp; Pereira, 2003). The only adjustable parameter of the model is the number of instances T to use for training. We fixed T using the development portion of the data. In our experiments, the best value is between 20 and 30 times the size of the training data. To regularize the model we take as the final model the average of all weight vectors posited during training (Collins, 2002). Algorithm 2 illustrates the perceptron learning procedure. The final average model can be computed efficiently during training without storing the individual vectors (e.g., see (Ciaramita &amp; Johnson, 2003)). Algorithm 2: Average multiclass perceptron input : S = (xi, yi)N ; 0 k = ~ 0, k Y for t = 1 to T do choose j Et = {r Y : hxj, t ri hxj, t yj i} if |Et |&amp;gt; 0 then t+1 r = t r xj |Et |, r Et t+1 yj = t yj + xj output: k = 1 T P t t k, k Y 3.4 Higher-order feature spaces Yamada and Matsumoto (2003) and McDonald and Pereira (2006) have shown that higher-order feature representations and modeli</context>
<context position="24007" citStr="Collins, 2002" startWordPosition="3970" endWordPosition="3971">R 0.2 LAW 0.2 ORG:POLITICAL 0.2 ORG:EDU 0.2 Table 2. The 40 most frequent labels in sections 2 to 21 of the Wall Street Journal BBN Corpus and the percentage of tags occurrences. the whole original Treebank token, thus obtaining third-highestORDINAL in the example above. 4.3 Semantic tagger We treated semantic tags as POS tags. A tagger was trained on the BBN gold standard annotation and used it to annotate development and evaluation data. We briefly describe the tagger (see (Ciaramita &amp; Altun, 2006) for more details), a Hidden Markov Model trained with the perceptron algorithm introduced in (Collins, 2002). The tagger uses Viterbi decoding. Label to label dependencies are limited to the previous tag (first order HMM). A generic feature set for NER based on words, lemmas, POS tags, and word shape features was used. The tagger is trained on sections 2-21 of the BBN corpus. As before, section 22 of the BBN corpus is used for choosing the perceptrons parameter T. The taggers model is regularized as described for Algorithm 2. The full BBN tagset is comprised of 105 classes organized hierarchically, we ignored the hierarchical organization and treated each tag as an independent class in the standard </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collins</author>
<author>T Koo</author>
</authors>
<title>Hidden-Variable Models for Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="17508" citStr="Collins &amp; Koo, 2005" startWordPosition="2899" endWordPosition="2902"> partitioning the training data according to the POS. Inaccurate predictions of the POS can affect significantly the accuracy of the actions predicted, while the single model is more robust, since the POS is just one of the many features used in prediction. 4 Semantic features Semantic information is used implicitly in parsing. For example, conditioning on lexical heads provides a source of semantic information. There have been a few attempts at using semantic information more explicitly. Charniaks 1997 parser (1997), defined probability estimates backed off to word clusters. Collins and Koo (Collins &amp; Koo, 2005) introduced an improved reranking model for parsing which includes a hidden layer of semantic features. Yi and Palmer (2005) retrained a constituent parser in which phrases were annotated with argument information to improve SRL, however this didnt improve over the output of the basic parser. In recent years there has been a significant amount of work on semantic annotation tasks such as named-entity recognition, semantic role labeling and relation extraction. There is evidence that dependency and constituent parsing can be helpful in these and other tasks; e.g., by means of tree kernels in qu</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>\x0cM. Collins and T. Koo. 2005. Hidden-Variable Models for Discriminative Reranking. In Proceedings of EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>B Roark</author>
</authors>
<title>Incremental Parsing with the Perceptron Algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Collins, Roark, 2004</marker>
<rawString>M. Collins and B. Roark. 2004. Incremental Parsing with the Perceptron Algorithm. In Proceedings of ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative Online Algorithms for Multiclass Problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<volume>3</volume>
<pages>951--991</pages>
<contexts>
<context position="11801" citStr="Crammer &amp; Singer, 2003" startWordPosition="1918" endWordPosition="1922">ere each class yi Y represents one of k possible parsing actions. Each of such actions is associated with a weight vector k IRd . Given a datapoint x X, a ddimensional vector of binary features in the input space X, a parsing action is chosen with a winnertake-all discriminant function: estimateAction(x, ) = arg max k f(x, k) (4) when using a linear classifier, such as the perceptron or SVM, f(u, v) = hu, vi is the inner product between vectors u and v. We learn the parameters from the training data with the perceptron (Rosemblatt, 1958), in the online multiclass formulation of the algorithm (Crammer &amp; Singer, 2003) with uniform negative updates. The perceptron has been used in previous work on dependency parsing by Carreras et al. (2006), with a parser based on Eisners algorithm (Eisner, 2000), and also on incremental constituent parsing (Collins &amp; Roark, 2006). Also the MST parser of McDonald uses a variant of the perceptron algorithm (McDonald, 2006). The choice is motivated by the simplicity and performance of perceptrons, which have proved competitive on a number of tasks; e.g., in shallow parsing, where perceptrons performance is comparable to that of Conditional Random Field models (Sha &amp; Pereira,</context>
<context position="35424" citStr="Crammer &amp; Singer, 2003" startWordPosition="5878" endWordPosition="5881">st parsing architecture: a single-pass deterministic shift-reduce algorithm trained with a regularized multiclass perceptron. We showed that with the perceptron it is possible to adopt higher-order feature maps equivalent to polynomial kernels without need of approximating the model (although this remains an option for optimization). The resulting models achieve accuracies comparable (or better) to more complex architectures based on dual SVM training, and faster parsing on unseen data. With respect to learning, it is possible that more sophisticated formulations of the perceptron (e.g. MIRA (Crammer &amp; Singer, 2003)) could provide further gains in accuracy, as shown with the MST parser (McDonald et al., 2005). We also experimented with novel types of semantic features, extracted from the annotations produced by an entity tagger trained on the BBN corpus. This model further improves over the standard model yielding an additional 5.8% relative error reduction. Although the magnitude of the improvement is not striking, to the best of our knowledge this is the first encouraging evidence that annotated semantic information can improve parsing and suggests several options for further research. For example, thi</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative Online Algorithms for Multiclass Problems. Journal of Machine Learning Research 3: pp.951-991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>J Sorensen</author>
</authors>
<title>Dependency Tree Kernels for Relation Extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1454" citStr="Culotta &amp; Sorensen, 2004" startWordPosition="205" endWordPosition="208">plex architectures, with no need for approximations. Further gains in accuracy are obtained by designing features for parsing extracted from semantic annotations generated by a tagger. We provide experimental evaluations on the Penn Treebank. 1 Introduction A dependency tree represents a sentence as a labeled directed graph encoding syntactic and semantic information. The labels on the arcs can represent basic grammatical relations such as subject and object. Dependency trees capture grammatical structures that can be useful in several language processing tasks such as information extraction (Culotta &amp; Sorensen, 2004) and machine translation (Ding &amp; Palmer, 2005). Dependency treebanks are becoming available in many languages, and several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks (Buchholz &amp; Marsi, 2006; Nivre et al., 2007). Dependency parsing is simpler than constituency parsing, since dependency trees do not have extra non-terminal nodes and there is no need for a grammar to generate them. Approaches to dependency parsing either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single </context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>A. Culotta and J. Sorensen. 2004. Dependency Tree Kernels for Relation Extraction. In Proceedings of ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ding</author>
<author>M Palmer</author>
</authors>
<title>Machine Translation using Probabilistic Synchronous Dependency Insertion Grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1500" citStr="Ding &amp; Palmer, 2005" startWordPosition="212" endWordPosition="215">. Further gains in accuracy are obtained by designing features for parsing extracted from semantic annotations generated by a tagger. We provide experimental evaluations on the Penn Treebank. 1 Introduction A dependency tree represents a sentence as a labeled directed graph encoding syntactic and semantic information. The labels on the arcs can represent basic grammatical relations such as subject and object. Dependency trees capture grammatical structures that can be useful in several language processing tasks such as information extraction (Culotta &amp; Sorensen, 2004) and machine translation (Ding &amp; Palmer, 2005). Dependency treebanks are becoming available in many languages, and several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks (Buchholz &amp; Marsi, 2006; Nivre et al., 2007). Dependency parsing is simpler than constituency parsing, since dependency trees do not have extra non-terminal nodes and there is no need for a grammar to generate them. Approaches to dependency parsing either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree by means of shift-reduce parsing actions </context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Y. Ding and M. Palmer. 2005. Machine Translation using Probabilistic Synchronous Dependency Insertion Grammars. In Proceedings of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Bilexical Grammars and their CubicTime Parsing Algorithms.</title>
<date>2000</date>
<booktitle>New Developments in Natural Language Parsing,</booktitle>
<pages>29--62</pages>
<editor>In H.C. Bunt and A. Nijholt, eds.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="11983" citStr="Eisner, 2000" startWordPosition="1950" endWordPosition="1951">s in the input space X, a parsing action is chosen with a winnertake-all discriminant function: estimateAction(x, ) = arg max k f(x, k) (4) when using a linear classifier, such as the perceptron or SVM, f(u, v) = hu, vi is the inner product between vectors u and v. We learn the parameters from the training data with the perceptron (Rosemblatt, 1958), in the online multiclass formulation of the algorithm (Crammer &amp; Singer, 2003) with uniform negative updates. The perceptron has been used in previous work on dependency parsing by Carreras et al. (2006), with a parser based on Eisners algorithm (Eisner, 2000), and also on incremental constituent parsing (Collins &amp; Roark, 2006). Also the MST parser of McDonald uses a variant of the perceptron algorithm (McDonald, 2006). The choice is motivated by the simplicity and performance of perceptrons, which have proved competitive on a number of tasks; e.g., in shallow parsing, where perceptrons performance is comparable to that of Conditional Random Field models (Sha &amp; Pereira, 2003). The only adjustable parameter of the model is the number of instances T to use for training. We fixed T using the development portion of the data. In our experiments, the bes</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>J. Eisner. 2000. Bilexical Grammars and their CubicTime Parsing Algorithms. In H.C. Bunt and A. Nijholt, eds. New Developments in Natural Language Parsing, pp. 29-62. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="25471" citStr="Fellbaum, 1998" startWordPosition="4217" endWordPosition="4218">he sentence, preserves the parsers complexity. 5 Parsing experiments 5.1 Data and setup We used the standard partitions of the Wall Street Journal Penn Treebank (Marcus et al., 1993); i.e., sections 2-21 for training, section 22 for development and section 23 for evaluation. The constituent trees were transformed into dependency trees by means of a program created by Joakim Nivre that implements the rules proposed by Yamada and Matsumoto, which in turn are based on the head rules of Collins parser (Collins, 1999)5. The lemma for each token was produced using the morph function of the WordNet (Fellbaum, 1998) library6. The data in the WSJ sections 22 and 23, both for the parser and for the semantic tagger, was POS-tagged using TreeTagger7, which has an accuracy of 97.0% on section 23. Training a parsing model on the Wall Street Journal requires a set of 22 classes: 10 of the 11 labels in the dependency corpus generated from the Penn Treebank (e.g., subj, obj, sbar, vmod, nmod, root, etc.) are paired with both a Left and Right actions. In addition, there is in one rule for the root label and one for the Shift action. The total number of features found in training ranges from two hundred thousand fo</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database MIT Press, Cambridge, MA. 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hall</author>
<author>J Nivre</author>
<author>J Nilsson</author>
</authors>
<title>Discriminative Classifiers for Deterministic Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL</booktitle>
<contexts>
<context position="3065" citStr="Hall et al., 2006" startWordPosition="457" endWordPosition="460">cent work on dependency parsing has highlighted the benefits of using rich feature sets and high-order modeling. Yamada and Matsumoto (2003) showed that learning an SVM model in the dual space with higher-degree polynomial kernel functions improves significantly the parsers accuracy. McDonald and Pereira (2006) have shown that incorporating second order features relating to adjacent edge pairs improves the accuracy of maximum spanning tree parsers (MST). In the SVMbased approach, if the training data is large, it is not feasible to train a single model. Rather, Yamada and Matsumoto (see also (Hall et al., 2006)) partition the training data in different sets, on the basis of Partof-Speech, then train one dual SVM model per set. While this approach simplifies the learning task it makes the parser more sensitive to the error rate of the POS tagger. The second-order MST algorithm has cubic time complexity. For non-projective languages the algorithm is NP-hard and McDonald and Pereira (2006) introduce an approximate algorithm to handle such cases. In this paper we extend shift reduce parsing with second-order feature maps which explicitly repre133 \x0csent all feature pairs. Also the augmented feature se</context>
<context position="28575" citStr="Hall et al. (2006)" startWordPosition="4737" endWordPosition="4740">requently reported, we include the improvement in relative error reduction. The 2nd-order base model improves on all measures over the 1st-order model by approximately 5%. The UAS score is 90.55%, with an improvement of 4.9%. The magnitude of the improvement is remarkable and reflects the 4.6% improvement that Yamada and Matsumoto (Yamada &amp; Matsumoto, 2003) report going from the linear SVM to the polynomial of degree two. Our base models accuracy (90.55% UAS) compares well with the accuracy of the parsers based on the polynomial kernel trained with SVM of Yamada and Matsumoto (UAS 90.3%), and Hall et al. (2006) (UAS 89.4%). We notice in particular that, given the lack of nonprojective cases/rules, the parser of Hall et al. (2006) is almost identical to our parser, hence the difference in accuracy (+1.1%) might effectively be due to a better classifier. Yamada &amp; Matsumotos parser is slightly more complex than our parser, and has quadratic worst-case complexity. Overall, the accuracy of the 2nd-order parser is comparable to that of the 1st-order MST parser (90.7%). There is no direct evidence that our perceptron produces better classifiers than SVM. Rather, the pattern of results produced by the perce</context>
<context position="33227" citStr="Hall et al. (2006)" startWordPosition="5519" endWordPosition="5522">were performed on a 2.4GHz AMD Opteron CPU machine with 32GB RAM. The 2ndorder parser uses almost 3GB of memory. While Parsing time/sec Parser English Chinese MST 2n-order 97.52 59.05 MST 1st-order 76.62 49.13 DeSR 36.90 21.22 Table 6. Parsing times for the CoNNL 2007 English and Chinese datasets for MST and DeSR. it is several times slower and larger than the 1storder model8 the 2nd-order model performance is still competitive. It takes 3 minutes (user time) to parse section 23, POS tagging included. In training, the model takes about 1 hour to process the full dataset once. As a comparison, Hall et al. (2006) reports 1.5 hours for training the partitioned SVM model and 10 minutes for parsing the evaluation set on the same Penn Treebank data. We also compared directly the parsing time of our parser with that of the MST parser using the version 0.4.3 of MSTParser9. For these experiments we used two datasets from the CoNLL 2007 shared task for English and Chinese. Table 6 reports the times, in seconds, to parse the test sets for these languages on a 3.3GHz Xeon machine with 4 GB Ram, of the MST 1st and 2nd-order parser and DeSR parser (without semantic features). The architecture of the model present</context>
</contexts>
<marker>Hall, Nivre, Nilsson, 2006</marker>
<rawString>J. Hall, J. Nivre and J. Nilsson. 2006. Discriminative Classifiers for Deterministic Dependency Parsing. In Proceedings of the COLING/ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kalt</author>
</authors>
<title>Induction of Greedy Controllers for Deterministic Treebank Parsers.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="6172" citStr="Kalt, 2004" startWordPosition="954" endWordPosition="955">(2003) proposed a deterministic classifierbased parser. Instead of learning directly which tree to assign to a sentence, the parser learns which 1 The figure also contains entity annotations which will be explained below in Section 4.1. Shift/Reduce actions to use in building the tree. Parsing is cast as a classification problem: at each step the parser applies a classifier to the features representing its current state to predict which action to perform on the tree. Similar deterministic approaches to parsing have been investigated also in the context of constituent parsing (Wong &amp; Wu, 1999; Kalt, 2004). Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. Attardi (2006) proposed a variant of the rules that handle non-projective relations while parsing deterministically in a single pass. Shift-reduce algorithms are simple and efficient, yet competitive in terms of accuracy: in the CoNLL-X shared task, for several languages, there was no statistically significant difference between second-order MST parsers and shift-reduce parsers. 3 A shift-reduce parser We build upon DeSR, the shift-reduce parse</context>
</contexts>
<marker>Kalt, 2004</marker>
<rawString>T. Kalt. 2004. Induction of Greedy Controllers for Deterministic Treebank Parsers. In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<contexts>
<context position="25038" citStr="Marcus et al., 1993" startWordPosition="4142" endWordPosition="4145">Algorithm 2. The full BBN tagset is comprised of 105 classes organized hierarchically, we ignored the hierarchical organization and treated each tag as an independent class in the standard BIO encoding. The tagger evaluated on section 23 achieves an Fscore of 86.8%. The part of speech for the evaluation/development sections was produced with TreeTagger. As a final remark we notice that the taggers complexity, linear in the length of the sentence, preserves the parsers complexity. 5 Parsing experiments 5.1 Data and setup We used the standard partitions of the Wall Street Journal Penn Treebank (Marcus et al., 1993); i.e., sections 2-21 for training, section 22 for development and section 23 for evaluation. The constituent trees were transformed into dependency trees by means of a program created by Joakim Nivre that implements the rules proposed by Yamada and Matsumoto, which in turn are based on the head rules of Collins parser (Collins, 1999)5. The lemma for each token was produced using the morph function of the WordNet (Fellbaum, 1998) library6. The data in the WSJ sections 22 and 23, both for the parser and for the semantic tagger, was POS-tagged using TreeTagger7, which has an accuracy of 97.0% on</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini and M. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2): pp. 313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>Discriminative Training and Spanning Tree Algorithms for Dependency Parsing.</title>
<date>2006</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="12145" citStr="McDonald, 2006" startWordPosition="1975" endWordPosition="1977">classifier, such as the perceptron or SVM, f(u, v) = hu, vi is the inner product between vectors u and v. We learn the parameters from the training data with the perceptron (Rosemblatt, 1958), in the online multiclass formulation of the algorithm (Crammer &amp; Singer, 2003) with uniform negative updates. The perceptron has been used in previous work on dependency parsing by Carreras et al. (2006), with a parser based on Eisners algorithm (Eisner, 2000), and also on incremental constituent parsing (Collins &amp; Roark, 2006). Also the MST parser of McDonald uses a variant of the perceptron algorithm (McDonald, 2006). The choice is motivated by the simplicity and performance of perceptrons, which have proved competitive on a number of tasks; e.g., in shallow parsing, where perceptrons performance is comparable to that of Conditional Random Field models (Sha &amp; Pereira, 2003). The only adjustable parameter of the model is the number of instances T to use for training. We fixed T using the development portion of the data. In our experiments, the best value is between 20 and 30 times the size of the training data. To regularize the model we take as the final model the average of all weight vectors posited dur</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>R. McDonald. 2006. Discriminative Training and Spanning Tree Algorithms for Dependency Parsing. Ph.D. Thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<date>2005</date>
<contexts>
<context position="2034" citStr="McDonald et al., 2005" startWordPosition="294" endWordPosition="297">tion extraction (Culotta &amp; Sorensen, 2004) and machine translation (Ding &amp; Palmer, 2005). Dependency treebanks are becoming available in many languages, and several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks (Buchholz &amp; Marsi, 2006; Nivre et al., 2007). Dependency parsing is simpler than constituency parsing, since dependency trees do not have extra non-terminal nodes and there is no need for a grammar to generate them. Approaches to dependency parsing either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). Deterministic dependency parsers which run in linear time have also been developed (Nivre &amp; Scholz, 2004; Attardi, 2006). These parsers process the sentence sequentially, hence their efficiency makes them suitable for processing large amounts of text, as required, for example, in information retrieval applications. Recent work on dependency parsing has highlighted the benefits of using rich feature sets and high-order modeling. Yamada and Matsumoto (2003) showed that learning an SVM model in the dual </context>
<context position="5439" citStr="McDonald et al., 2005" startWordPosition="833" endWordPosition="836">Inc. canceled The People Next Door.1. Dependencies are represented as labeled arrows from the head of the relation to the modifier word; thus, in the example, Inc. is the modifier of a dependency labeled SUB (subject) to the main verb, the head, canceled. In statistical syntactic parsing a generator (e.g., a PCFG) is used to produce a number of candidate trees (Collins, 2000) with associated probability scores. This approach has been used also for dependency parsing, generating spanning trees as candidates and computing the maximum spanning tree (MST) using discriminative learning algorithms (McDonald et al., 2005). Second-order MST dependency parsers currently represent the state of the art in terms of accuracy. Yamada and Matsumoto (2003) proposed a deterministic classifierbased parser. Instead of learning directly which tree to assign to a sentence, the parser learns which 1 The figure also contains entity annotations which will be explained below in Section 4.1. Shift/Reduce actions to use in building the tree. Parsing is cast as a classification problem: at each step the parser applies a classifier to the features representing its current state to predict which action to perform on the tree. Simila</context>
<context position="35519" citStr="McDonald et al., 2005" startWordPosition="5894" endWordPosition="5897">arized multiclass perceptron. We showed that with the perceptron it is possible to adopt higher-order feature maps equivalent to polynomial kernels without need of approximating the model (although this remains an option for optimization). The resulting models achieve accuracies comparable (or better) to more complex architectures based on dual SVM training, and faster parsing on unseen data. With respect to learning, it is possible that more sophisticated formulations of the perceptron (e.g. MIRA (Crammer &amp; Singer, 2003)) could provide further gains in accuracy, as shown with the MST parser (McDonald et al., 2005). We also experimented with novel types of semantic features, extracted from the annotations produced by an entity tagger trained on the BBN corpus. This model further improves over the standard model yielding an additional 5.8% relative error reduction. Although the magnitude of the improvement is not striking, to the best of our knowledge this is the first encouraging evidence that annotated semantic information can improve parsing and suggests several options for further research. For example, this finding might indicate that this type of approach, which combines semantic tagging and parsin</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov and J. Hajic. 2005.</rawString>
</citation>
<citation valid="true">
<title>Non-projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP</booktitle>
<contexts>
<context position="4227" citStr="(2005)" startWordPosition="637" endWordPosition="637">feature pairs. Also the augmented feature sets impose additional computational costs. However, excellent efficiency/accuracy trade-off is achieved by using the perceptron algorithm, without the need to resort to approximations, producing high-accuracy classifiers based on a single model. We also evaluate a novel set of features for parsing. Recently various forms of shallow semantic processing have been investigated such as namedentity recognition (NER), semantic role labeling (SRL) and relation extraction. Syntactic parsing can provide useful features for these tasks; e.g., Punyakanok et al. (2005) show that full parsing is effective for semantic role labeling (see also related approaches evaluated within the CoNNL 2005 shared task (Carreras et al., 2005)). However, no evidence has been provided so far that annotated semantic information can be leveraged for improving parser performance. We report experiments showing that adding features extracted by an entity tagger improves the accuracy of a dependency parser. 2 Dependency parsing A dependency parser takes as input a sentence s and returns a dependency graph d. Figure 1 shows a dependency tree for the sentence Last week CBS Inc. cance</context>
<context position="17632" citStr="(2005)" startWordPosition="2922" endWordPosition="2922">ns predicted, while the single model is more robust, since the POS is just one of the many features used in prediction. 4 Semantic features Semantic information is used implicitly in parsing. For example, conditioning on lexical heads provides a source of semantic information. There have been a few attempts at using semantic information more explicitly. Charniaks 1997 parser (1997), defined probability estimates backed off to word clusters. Collins and Koo (Collins &amp; Koo, 2005) introduced an improved reranking model for parsing which includes a hidden layer of semantic features. Yi and Palmer (2005) retrained a constituent parser in which phrases were annotated with argument information to improve SRL, however this didnt improve over the output of the basic parser. In recent years there has been a significant amount of work on semantic annotation tasks such as named-entity recognition, semantic role labeling and relation extraction. There is evidence that dependency and constituent parsing can be helpful in these and other tasks; e.g., by means of tree kernels in question classification and semantic role labeling (Zhang &amp; Lee, 2003; Moschitti, 2006). It is natural to ask if also the oppo</context>
</contexts>
<marker>2005</marker>
<rawString>Non-projective Dependency Parsing using Spanning Tree Algorithms. In Proceedings of HLT-EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online Learning of Approximate Dependency Parsing Algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context position="2759" citStr="McDonald and Pereira (2006)" startWordPosition="405" endWordPosition="408">rministic dependency parsers which run in linear time have also been developed (Nivre &amp; Scholz, 2004; Attardi, 2006). These parsers process the sentence sequentially, hence their efficiency makes them suitable for processing large amounts of text, as required, for example, in information retrieval applications. Recent work on dependency parsing has highlighted the benefits of using rich feature sets and high-order modeling. Yamada and Matsumoto (2003) showed that learning an SVM model in the dual space with higher-degree polynomial kernel functions improves significantly the parsers accuracy. McDonald and Pereira (2006) have shown that incorporating second order features relating to adjacent edge pairs improves the accuracy of maximum spanning tree parsers (MST). In the SVMbased approach, if the training data is large, it is not feasible to train a single model. Rather, Yamada and Matsumoto (see also (Hall et al., 2006)) partition the training data in different sets, on the basis of Partof-Speech, then train one dual SVM model per set. While this approach simplifies the learning task it makes the parser more sensitive to the error rate of the POS tagger. The second-order MST algorithm has cubic time complexi</context>
<context position="13309" citStr="McDonald and Pereira (2006)" startWordPosition="2198" endWordPosition="2201">s the final model the average of all weight vectors posited during training (Collins, 2002). Algorithm 2 illustrates the perceptron learning procedure. The final average model can be computed efficiently during training without storing the individual vectors (e.g., see (Ciaramita &amp; Johnson, 2003)). Algorithm 2: Average multiclass perceptron input : S = (xi, yi)N ; 0 k = ~ 0, k Y for t = 1 to T do choose j Et = {r Y : hxj, t ri hxj, t yj i} if |Et |&amp;gt; 0 then t+1 r = t r xj |Et |, r Et t+1 yj = t yj + xj output: k = 1 T P t t k, k Y 3.4 Higher-order feature spaces Yamada and Matsumoto (2003) and McDonald and Pereira (2006) have shown that higher-order feature representations and modeling can improve parsing accuracy, although at significant computational costs. To make SVM training feasible in the dual model with polynomial kernels, Yamada and Matsumoto split the training data into several sets, based on POS tags, and train a parsing model for each set. McDonald and Pereiras second-order MST parser has O(n3) complexity, while for handling non-projective trees, otherwise an NP-hard problem, the parser resorts to an approximate algorithm. Here we discuss how the feature representation can be enriched to improve p</context>
<context position="31629" citStr="McDonald &amp; Pereira, 2006" startWordPosition="5241" endWordPosition="5244">on the Penn Treebank dataset. tion is that of model AS-2 which extracts all semantic features from the widest context. In the 1st-order AS-2 model the improvement, 86.71% UAS (+8% relative error reduction) is more marked than in the 2nd-order AS-2 model, 91.1% UAS (+5.8% error reduction). A possible simple exaplanation is that some information captured by the semantic features is correlated with other higher-order features which do not occur in the 1st-order encoding. Overall the accuracy of the DeSR parser with semantic information is slightly inferior to that of the second-order MST parser (McDonald &amp; Pereira, 2006) (91.5% UAS). The best result on this dataset to date (92.7% UAS) is that of Sagae and Lavie (Sagae &amp; Lavie, 2006) who use a parser which combines the predictions of several pre-existing parsers, including McDonalds and Nivres parsers. Table 5 lists the main results to date on the version of the Penn Treebank for dependency parsing task used in this paper. In Table 4 we also evaluate the gain obtained by adding one semantic feature type at a time (cf. rows EOS/BIO/TAG). These results show that all semantic features provide some improvement (with the dubious case of EOS in the 2nd-order model).</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online Learning of Approximate Dependency Parsing Algorithms. In Proceedings of EACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Minsky</author>
<author>S A Papert</author>
</authors>
<title>Perceptrons: An Introduction to Computational Geometry.</title>
<date>1969</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="14244" citStr="Minsky and Papert (1969)" startWordPosition="2338" endWordPosition="2341">arsing model for each set. McDonald and Pereiras second-order MST parser has O(n3) complexity, while for handling non-projective trees, otherwise an NP-hard problem, the parser resorts to an approximate algorithm. Here we discuss how the feature representation can be enriched to improve parsing while maintaining the simplicity of the shift-reduce architecture, and performing discriminative learning without partitioning the training data. The linear classifier (see Equation 4) learned with the perceptron is inherently limited in the types of solutions it can learn. As originally pointed out by Minsky and Papert (1969), there are problems which require non-linear solutions that cannot be learned by such models. A simple workaround this limitation relies on feature maps : IRd IRh that 136 \x0cmap the input vectors x X into some higher hdimensional representation (X) IRh , the feature space. The feature space can represent, for example, all combinations of individual features in the input space. We define a feature map which extracts all second order features of the form xixj; i.e., (x) = (xi, xj|i = 1, ..., d, j = i, ..., d). The linear perceptron working in (X) effectively implements a non-linear classifier</context>
</contexts>
<marker>Minsky, Papert, 1969</marker>
<rawString>M.L. Minsky and S.A. Papert. 1969. Perceptrons: An Introduction to Computational Geometry. MIT Press, Cambridge, MA. 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees.</title>
<date>2006</date>
<booktitle>In Proceedings of ECML</booktitle>
<contexts>
<context position="18193" citStr="Moschitti, 2006" startWordPosition="3012" endWordPosition="3013">hidden layer of semantic features. Yi and Palmer (2005) retrained a constituent parser in which phrases were annotated with argument information to improve SRL, however this didnt improve over the output of the basic parser. In recent years there has been a significant amount of work on semantic annotation tasks such as named-entity recognition, semantic role labeling and relation extraction. There is evidence that dependency and constituent parsing can be helpful in these and other tasks; e.g., by means of tree kernels in question classification and semantic role labeling (Zhang &amp; Lee, 2003; Moschitti, 2006). It is natural to ask if also the opposite holds: whether semantic annotations can be used to improve parsing. In particular, it would be interesting to know if entity-like tags can be used for this purpose. One reason for this is that entity tagging is efficient and does not seem to need parsing for achieving top performance. Beyond improving traditional parsing, independently learned semantic tags might be helpful in adapting a parser to a new domain. To the best of our knowledge, no evidence has been produced yet that annotated semantic information can improve parsing. In the following we </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>A. Moschitti. 2006. Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In Proceedings of ECML 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Incrementality in Deterministic Dependency Parsing.</title>
<date>2004</date>
<booktitle>In Incremental Parsing: Bringing Engineering and Cognition Together. Workshop at ACL2004.Spain,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="7435" citStr="Nivre (2004)" startWordPosition="1148" endWordPosition="1149">Scholzs (2004) provide among the simplest and most efficient methods. This parser constructs dependency trees by scanning input sentences in a single left-to-right pass and performing shift/reduce parsing actions. The parsing algorithm is fully deterministic and has linear complexity. The parsers behavior can be described as repeatedly selecting and applying a parsing rule to transform its state, while advancing through the sentence. Each token is analyzed once and a decision is made locally concerning the action to take, that is, without considering global properties of the tree being built. Nivre (2004) investigated the issue of (strict) incrementality for this type of parsers; i.e., if at any point of the analysis the processed input forms one connected structure. Nivre found that strict incrementality is not guaranteed within this parsing framework, although for correctly parsed trees the property holds in almost 90% of the cases. 3.1 Parsing algorithm The state of the parser is represented by a triple hS, I, Ai, where S is the stack, I is the list of input tokens that remain to be processed and A is the arc 134 \x0cFigure 1. A dependency tree from the Penn Treebank, with additional entity</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>J. Nivre. 2004. Incrementality in Deterministic Dependency Parsing. In Incremental Parsing: Bringing Engineering and Cognition Together. Workshop at ACL2004.Spain, 50-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S Kubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<date>2007</date>
<booktitle>The CoNLL 2007 Shared Task on Dependency Parsing, In Proceedings of EMNLP-CoNLL</booktitle>
<contexts>
<context position="1736" citStr="Nivre et al., 2007" startWordPosition="248" endWordPosition="251"> sentence as a labeled directed graph encoding syntactic and semantic information. The labels on the arcs can represent basic grammatical relations such as subject and object. Dependency trees capture grammatical structures that can be useful in several language processing tasks such as information extraction (Culotta &amp; Sorensen, 2004) and machine translation (Ding &amp; Palmer, 2005). Dependency treebanks are becoming available in many languages, and several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks (Buchholz &amp; Marsi, 2006; Nivre et al., 2007). Dependency parsing is simpler than constituency parsing, since dependency trees do not have extra non-terminal nodes and there is no need for a grammar to generate them. Approaches to dependency parsing either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). Deterministic dependency parsers which run in linear time have also been developed (Nivre &amp; Scholz, 2004; Attardi, 2006). These parsers process the sentence sequentially, hence their efficiency makes them sui</context>
</contexts>
<marker>Nivre, Hall, Kubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson, S. Riedel and D. Yuret. 2007. The CoNLL 2007 Shared Task on Dependency Parsing, In Proceedings of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>M Scholz</author>
</authors>
<title>Deterministic Dependency Parsing of English Text.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="6197" citStr="Nivre and Scholz (2004)" startWordPosition="956" endWordPosition="959">ed a deterministic classifierbased parser. Instead of learning directly which tree to assign to a sentence, the parser learns which 1 The figure also contains entity annotations which will be explained below in Section 4.1. Shift/Reduce actions to use in building the tree. Parsing is cast as a classification problem: at each step the parser applies a classifier to the features representing its current state to predict which action to perform on the tree. Similar deterministic approaches to parsing have been investigated also in the context of constituent parsing (Wong &amp; Wu, 1999; Kalt, 2004). Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. Attardi (2006) proposed a variant of the rules that handle non-projective relations while parsing deterministically in a single pass. Shift-reduce algorithms are simple and efficient, yet competitive in terms of accuracy: in the CoNLL-X shared task, for several languages, there was no statistically significant difference between second-order MST parsers and shift-reduce parsers. 3 A shift-reduce parser We build upon DeSR, the shift-reduce parser described in (Attardi, </context>
<context position="2232" citStr="Nivre &amp; Scholz, 2004" startWordPosition="326" endWordPosition="329"> multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks (Buchholz &amp; Marsi, 2006; Nivre et al., 2007). Dependency parsing is simpler than constituency parsing, since dependency trees do not have extra non-terminal nodes and there is no need for a grammar to generate them. Approaches to dependency parsing either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). Deterministic dependency parsers which run in linear time have also been developed (Nivre &amp; Scholz, 2004; Attardi, 2006). These parsers process the sentence sequentially, hence their efficiency makes them suitable for processing large amounts of text, as required, for example, in information retrieval applications. Recent work on dependency parsing has highlighted the benefits of using rich feature sets and high-order modeling. Yamada and Matsumoto (2003) showed that learning an SVM model in the dual space with higher-degree polynomial kernel functions improves significantly the parsers accuracy. McDonald and Pereira (2006) have shown that incorporating second order features relating to adjacent</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>J. Nivre and M. Scholz. 2004. Deterministic Dependency Parsing of English Text. In Proceedings of COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The Necessity of Syntactic Parsing for Semantic Role Labeling.</title>
<date>2005</date>
<contexts>
<context position="4227" citStr="Punyakanok et al. (2005)" startWordPosition="633" endWordPosition="637">e133 \x0csent all feature pairs. Also the augmented feature sets impose additional computational costs. However, excellent efficiency/accuracy trade-off is achieved by using the perceptron algorithm, without the need to resort to approximations, producing high-accuracy classifiers based on a single model. We also evaluate a novel set of features for parsing. Recently various forms of shallow semantic processing have been investigated such as namedentity recognition (NER), semantic role labeling (SRL) and relation extraction. Syntactic parsing can provide useful features for these tasks; e.g., Punyakanok et al. (2005) show that full parsing is effective for semantic role labeling (see also related approaches evaluated within the CoNNL 2005 shared task (Carreras et al., 2005)). However, no evidence has been provided so far that annotated semantic information can be leveraged for improving parser performance. We report experiments showing that adding features extracted by an entity tagger improves the accuracy of a dependency parser. 2 Dependency parsing A dependency parser takes as input a sentence s and returns a dependency graph d. Figure 1 shows a dependency tree for the sentence Last week CBS Inc. cance</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2005</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2005. The Necessity of Syntactic Parsing for Semantic Role Labeling.</rawString>
</citation>
<citation valid="true">
<date>2005</date>
<booktitle>In Proceedings of IJCAI</booktitle>
<contexts>
<context position="4227" citStr="(2005)" startWordPosition="637" endWordPosition="637">feature pairs. Also the augmented feature sets impose additional computational costs. However, excellent efficiency/accuracy trade-off is achieved by using the perceptron algorithm, without the need to resort to approximations, producing high-accuracy classifiers based on a single model. We also evaluate a novel set of features for parsing. Recently various forms of shallow semantic processing have been investigated such as namedentity recognition (NER), semantic role labeling (SRL) and relation extraction. Syntactic parsing can provide useful features for these tasks; e.g., Punyakanok et al. (2005) show that full parsing is effective for semantic role labeling (see also related approaches evaluated within the CoNNL 2005 shared task (Carreras et al., 2005)). However, no evidence has been provided so far that annotated semantic information can be leveraged for improving parser performance. We report experiments showing that adding features extracted by an entity tagger improves the accuracy of a dependency parser. 2 Dependency parsing A dependency parser takes as input a sentence s and returns a dependency graph d. Figure 1 shows a dependency tree for the sentence Last week CBS Inc. cance</context>
<context position="17632" citStr="(2005)" startWordPosition="2922" endWordPosition="2922">ns predicted, while the single model is more robust, since the POS is just one of the many features used in prediction. 4 Semantic features Semantic information is used implicitly in parsing. For example, conditioning on lexical heads provides a source of semantic information. There have been a few attempts at using semantic information more explicitly. Charniaks 1997 parser (1997), defined probability estimates backed off to word clusters. Collins and Koo (Collins &amp; Koo, 2005) introduced an improved reranking model for parsing which includes a hidden layer of semantic features. Yi and Palmer (2005) retrained a constituent parser in which phrases were annotated with argument information to improve SRL, however this didnt improve over the output of the basic parser. In recent years there has been a significant amount of work on semantic annotation tasks such as named-entity recognition, semantic role labeling and relation extraction. There is evidence that dependency and constituent parsing can be helpful in these and other tasks; e.g., by means of tree kernels in question classification and semantic role labeling (Zhang &amp; Lee, 2003; Moschitti, 2006). It is natural to ask if also the oppo</context>
</contexts>
<marker>2005</marker>
<rawString>In Proceedings of IJCAI 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rosemblatt</author>
</authors>
<title>The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.</title>
<date>1958</date>
<journal>Psych. Rev.,</journal>
<volume>68</volume>
<pages>386--407</pages>
<contexts>
<context position="11721" citStr="Rosemblatt, 1958" startWordPosition="1907" endWordPosition="1908">blem of learning a parsing model can be framed as a classification task where each class yi Y represents one of k possible parsing actions. Each of such actions is associated with a weight vector k IRd . Given a datapoint x X, a ddimensional vector of binary features in the input space X, a parsing action is chosen with a winnertake-all discriminant function: estimateAction(x, ) = arg max k f(x, k) (4) when using a linear classifier, such as the perceptron or SVM, f(u, v) = hu, vi is the inner product between vectors u and v. We learn the parameters from the training data with the perceptron (Rosemblatt, 1958), in the online multiclass formulation of the algorithm (Crammer &amp; Singer, 2003) with uniform negative updates. The perceptron has been used in previous work on dependency parsing by Carreras et al. (2006), with a parser based on Eisners algorithm (Eisner, 2000), and also on incremental constituent parsing (Collins &amp; Roark, 2006). Also the MST parser of McDonald uses a variant of the perceptron algorithm (McDonald, 2006). The choice is motivated by the simplicity and performance of perceptrons, which have proved competitive on a number of tasks; e.g., in shallow parsing, where perceptrons perf</context>
</contexts>
<marker>Rosemblatt, 1958</marker>
<rawString>F. Rosemblatt. 1958. The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. Psych. Rev., 68: pp. 386-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>A Lavie</author>
</authors>
<title>Parser Combination by Reparsing.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<marker>Sagae, Lavie, 2005</marker>
<rawString>K. Sagae and A. Lavie. 2005. Parser Combination by Reparsing. In Proceedings of HLT-NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow Parsing with Conditional Random Fields.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="12407" citStr="Sha &amp; Pereira, 2003" startWordPosition="2016" endWordPosition="2019"> Singer, 2003) with uniform negative updates. The perceptron has been used in previous work on dependency parsing by Carreras et al. (2006), with a parser based on Eisners algorithm (Eisner, 2000), and also on incremental constituent parsing (Collins &amp; Roark, 2006). Also the MST parser of McDonald uses a variant of the perceptron algorithm (McDonald, 2006). The choice is motivated by the simplicity and performance of perceptrons, which have proved competitive on a number of tasks; e.g., in shallow parsing, where perceptrons performance is comparable to that of Conditional Random Field models (Sha &amp; Pereira, 2003). The only adjustable parameter of the model is the number of instances T to use for training. We fixed T using the development portion of the data. In our experiments, the best value is between 20 and 30 times the size of the training data. To regularize the model we take as the final model the average of all weight vectors posited during training (Collins, 2002). Algorithm 2 illustrates the perceptron learning procedure. The final average model can be computed efficiently during training without storing the individual vectors (e.g., see (Ciaramita &amp; Johnson, 2003)). Algorithm 2: Average mult</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow Parsing with Conditional Random Fields. In Proceedings of HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical Dependency Analysis with Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighth International Workshop on Parsing Technologies.</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="2587" citStr="Yamada and Matsumoto (2003)" startWordPosition="378" endWordPosition="382">h trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). Deterministic dependency parsers which run in linear time have also been developed (Nivre &amp; Scholz, 2004; Attardi, 2006). These parsers process the sentence sequentially, hence their efficiency makes them suitable for processing large amounts of text, as required, for example, in information retrieval applications. Recent work on dependency parsing has highlighted the benefits of using rich feature sets and high-order modeling. Yamada and Matsumoto (2003) showed that learning an SVM model in the dual space with higher-degree polynomial kernel functions improves significantly the parsers accuracy. McDonald and Pereira (2006) have shown that incorporating second order features relating to adjacent edge pairs improves the accuracy of maximum spanning tree parsers (MST). In the SVMbased approach, if the training data is large, it is not feasible to train a single model. Rather, Yamada and Matsumoto (see also (Hall et al., 2006)) partition the training data in different sets, on the basis of Partof-Speech, then train one dual SVM model per set. Whi</context>
<context position="5567" citStr="Yamada and Matsumoto (2003)" startWordPosition="852" endWordPosition="856">odifier word; thus, in the example, Inc. is the modifier of a dependency labeled SUB (subject) to the main verb, the head, canceled. In statistical syntactic parsing a generator (e.g., a PCFG) is used to produce a number of candidate trees (Collins, 2000) with associated probability scores. This approach has been used also for dependency parsing, generating spanning trees as candidates and computing the maximum spanning tree (MST) using discriminative learning algorithms (McDonald et al., 2005). Second-order MST dependency parsers currently represent the state of the art in terms of accuracy. Yamada and Matsumoto (2003) proposed a deterministic classifierbased parser. Instead of learning directly which tree to assign to a sentence, the parser learns which 1 The figure also contains entity annotations which will be explained below in Section 4.1. Shift/Reduce actions to use in building the tree. Parsing is cast as a classification problem: at each step the parser applies a classifier to the features representing its current state to predict which action to perform on the tree. Similar deterministic approaches to parsing have been investigated also in the context of constituent parsing (Wong &amp; Wu, 1999; Kalt, </context>
<context position="13277" citStr="Yamada and Matsumoto (2003)" startWordPosition="2193" endWordPosition="2196">o regularize the model we take as the final model the average of all weight vectors posited during training (Collins, 2002). Algorithm 2 illustrates the perceptron learning procedure. The final average model can be computed efficiently during training without storing the individual vectors (e.g., see (Ciaramita &amp; Johnson, 2003)). Algorithm 2: Average multiclass perceptron input : S = (xi, yi)N ; 0 k = ~ 0, k Y for t = 1 to T do choose j Et = {r Y : hxj, t ri hxj, t yj i} if |Et |&amp;gt; 0 then t+1 r = t r xj |Et |, r Et t+1 yj = t yj + xj output: k = 1 T P t t k, k Y 3.4 Higher-order feature spaces Yamada and Matsumoto (2003) and McDonald and Pereira (2006) have shown that higher-order feature representations and modeling can improve parsing accuracy, although at significant computational costs. To make SVM training feasible in the dual model with polynomial kernels, Yamada and Matsumoto split the training data into several sets, based on POS tags, and train a parsing model for each set. McDonald and Pereiras second-order MST parser has O(n3) complexity, while for handling non-projective trees, otherwise an NP-hard problem, the parser resorts to an approximate algorithm. Here we discuss how the feature representat</context>
<context position="15432" citStr="Yamada and Matsumoto (2003)" startWordPosition="2551" endWordPosition="2554">vely implements a non-linear classifier in the original input space X. One shortcoming of this approach is that it inflates considerably the feature representation and might not scale. In general, the number of features of degree g over an input space of dimension d is d+g1 g \x01 . In practice, a second-order feature map can be handled with reasonable efficiency by the perceptron. We call this the 2nd-order model, which uses a modified scoring function: g(x, k) = f((x), k) (5) where also k is h-dimensional. The proposed feature map is equivalent to a polynomial kernel function of degree two. Yamada and Matsumoto (2003) have shown that the degree two polynomial kernel has superior accuracy than the linear model and polynomial kernels of higher degrees. However, using the dual model is not always practical for dependency parsing. The discriminant function of the dual model is defined as: f0 (x, ) = arg max k N X i=1 k,ihx, xiig (6) where the weights are associated with classinstance pairs rather than class-feature pairs. With respect to the discriminant function of equation (4) there is an additional summation. In principle, the inner products can be cached in a Kernel matrix to speed up training. There are t</context>
<context position="2126" citStr="Yamada &amp; Matsumoto, 2003" startWordPosition="309" endWordPosition="312"> Dependency treebanks are becoming available in many languages, and several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks (Buchholz &amp; Marsi, 2006; Nivre et al., 2007). Dependency parsing is simpler than constituency parsing, since dependency trees do not have extra non-terminal nodes and there is no need for a grammar to generate them. Approaches to dependency parsing either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). Deterministic dependency parsers which run in linear time have also been developed (Nivre &amp; Scholz, 2004; Attardi, 2006). These parsers process the sentence sequentially, hence their efficiency makes them suitable for processing large amounts of text, as required, for example, in information retrieval applications. Recent work on dependency parsing has highlighted the benefits of using rich feature sets and high-order modeling. Yamada and Matsumoto (2003) showed that learning an SVM model in the dual space with higher-degree polynomial kernel functions improves significantly the parsers accu</context>
<context position="28316" citStr="Yamada &amp; Matsumoto, 2003" startWordPosition="4688" endWordPosition="4692">d-order models Table 4 summarizes the results of all experiments. We report the following scores, obtained with the CoNLL-X scoring script: labeled attachment score (LAS), unlabeled attachment score (UAS) and label accuracy score (LAC). For the UAS score, the most frequently reported, we include the improvement in relative error reduction. The 2nd-order base model improves on all measures over the 1st-order model by approximately 5%. The UAS score is 90.55%, with an improvement of 4.9%. The magnitude of the improvement is remarkable and reflects the 4.6% improvement that Yamada and Matsumoto (Yamada &amp; Matsumoto, 2003) report going from the linear SVM to the polynomial of degree two. Our base models accuracy (90.55% UAS) compares well with the accuracy of the parsers based on the polynomial kernel trained with SVM of Yamada and Matsumoto (UAS 90.3%), and Hall et al. (2006) (UAS 89.4%). We notice in particular that, given the lack of nonprojective cases/rules, the parser of Hall et al. (2006) is almost identical to our parser, hence the difference in accuracy (+1.1%) might effectively be due to a better classifier. Yamada &amp; Matsumotos parser is slightly more complex than our parser, and has quadratic worst-c</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical Dependency Analysis with Support Vector Machines. In Proceedings of the Eighth International Workshop on Parsing Technologies. Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Yi</author>
<author>M Palmer</author>
</authors>
<title>The Integration of Syntactic Parsing and Semantic Role Labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<contexts>
<context position="17632" citStr="Yi and Palmer (2005)" startWordPosition="2919" endWordPosition="2922">y of the actions predicted, while the single model is more robust, since the POS is just one of the many features used in prediction. 4 Semantic features Semantic information is used implicitly in parsing. For example, conditioning on lexical heads provides a source of semantic information. There have been a few attempts at using semantic information more explicitly. Charniaks 1997 parser (1997), defined probability estimates backed off to word clusters. Collins and Koo (Collins &amp; Koo, 2005) introduced an improved reranking model for parsing which includes a hidden layer of semantic features. Yi and Palmer (2005) retrained a constituent parser in which phrases were annotated with argument information to improve SRL, however this didnt improve over the output of the basic parser. In recent years there has been a significant amount of work on semantic annotation tasks such as named-entity recognition, semantic role labeling and relation extraction. There is evidence that dependency and constituent parsing can be helpful in these and other tasks; e.g., by means of tree kernels in question classification and semantic role labeling (Zhang &amp; Lee, 2003; Moschitti, 2006). It is natural to ask if also the oppo</context>
</contexts>
<marker>Yi, Palmer, 2005</marker>
<rawString>S. Yi and M. Palmer. 2005. The Integration of Syntactic Parsing and Semantic Role Labeling. In Proceedings of CoNLL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Wong</author>
<author>D Wu</author>
</authors>
<title>Learning a Lightweight Deterministic Parser.</title>
<date>1999</date>
<booktitle>In Proceedings of EUROSPEECH</booktitle>
<contexts>
<context position="6159" citStr="Wong &amp; Wu, 1999" startWordPosition="950" endWordPosition="953">da and Matsumoto (2003) proposed a deterministic classifierbased parser. Instead of learning directly which tree to assign to a sentence, the parser learns which 1 The figure also contains entity annotations which will be explained below in Section 4.1. Shift/Reduce actions to use in building the tree. Parsing is cast as a classification problem: at each step the parser applies a classifier to the features representing its current state to predict which action to perform on the tree. Similar deterministic approaches to parsing have been investigated also in the context of constituent parsing (Wong &amp; Wu, 1999; Kalt, 2004). Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. Attardi (2006) proposed a variant of the rules that handle non-projective relations while parsing deterministically in a single pass. Shift-reduce algorithms are simple and efficient, yet competitive in terms of accuracy: in the CoNLL-X shared task, for several languages, there was no statistically significant difference between second-order MST parsers and shift-reduce parsers. 3 A shift-reduce parser We build upon DeSR, the shift</context>
</contexts>
<marker>Wong, Wu, 1999</marker>
<rawString>A. Wong and D. Wu. 1999. Learning a Lightweight Deterministic Parser. In Proceedings of EUROSPEECH 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhang</author>
<author>W S Less</author>
</authors>
<title>Question Classification using Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<marker>Zhang, Less, 2003</marker>
<rawString>D. Zhang and W.S. Less. 2003. Question Classification using Support Vector Machines. In Proceedings of SIGIR 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>