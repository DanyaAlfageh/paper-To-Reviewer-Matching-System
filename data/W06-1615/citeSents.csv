There are many choices for modeling co-occurrence data (CITATION; CITATION; CITATION),,
In this work we choose to use the technique of structural learning (CITATIONa; CITATIONb),,
Pivot features correspond to the auxiliary problems of CITATIONa),,
For both computational and statistical reasons, though, we follow CITATIONa) and compute a low-dimensional linear approximation to the pivot predictor space,,
CITATIONa) describe several free paramters and extensions to ASO, and we briefly address our choices for these here,,
As in CITATIONa), we observed that setting h between 20 and 100 did not change results significantly, and a lower dimensionality translated to faster run-time,,
We also implemented both of the extensions described in CITATIONa),,
The ASO baseline is an implementation of CITATIONb),,
We chose to compare with ASO because it consistently outperforms cotraining CITATION and clustering methods CITATION,,
We did run experiments with the top-k version of ASO (CITATIONa), which is inspired by cotraining but consistently outperforms it,,
Structural correspondence learning is most similar to that of CITATION, who analyzed a situation with no target domain labeled data,,
Finding correspondences involves estimating the correlations between pivot and non-pivot feautres, and we adapt structural learning (ASO) (CITATIONa; CITATIONb) for this task,,
We also showed how to combine an SCL tagger with target domain labeled data using the classifier combination techniques from CITATION,,
There are many choices for modeling co-occurrence data (CITATION; CITATION; CITATION),,
In this work we choose to use the technique of structural learning (CITATIONa; CITATIONb),,
Pivot features correspond to the auxiliary problems of CITATIONa),,
For both computational and statistical reasons, though, we follow CITATIONa) and compute a low-dimensional linear approximation to the pivot predictor space,,
CITATIONa) describe several free paramters and extensions to ASO, and we briefly address our choices for these here,,
As in CITATIONa), we observed that setting h between 20 and 100 did not change results significantly, and a lower dimensionality translated to faster run-time,,
We also implemented both of the extensions described in CITATIONa),,
The ASO baseline is an implementation of CITATIONb),,
We chose to compare with ASO because it consistently outperforms cotraining CITATION and clustering methods CITATION,,
We did run experiments with the top-k version of ASO (CITATIONa), which is inspired by cotraining but consistently outperforms it,,
Structural correspondence learning is most similar to that of CITATION, who analyzed a situation with no target domain labeled data,,
Finding correspondences involves estimating the correlations between pivot and non-pivot feautres, and we adapt structural learning (ASO) (CITATIONa; CITATIONb) for this task,,
We also showed how to combine an SCL tagger with target domain labeled data using the classifier combination techniques from CITATION,,
 2004) and automatic translators CITATION use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data,,
In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain (CITATION; CITATION; CITATION; CITATION; CITATION; Daume III and Marcu, 2006),,
because it consistently outperforms cotraining CITATION and clustering methods CITATION,,
We did run experiments with the top-k version of ASO (CITATIONa), which is inspired by cotraining but consistently outperforms it,,
Structural correspondence learning is most similar to that of CITATION, who analyzed a situation with no target domain labeled data,,
One important advantage that this work shares with CITATION is that an ,,
There are many choices for modeling co-occurrence data (CITATION; CITATION; CITATION),,
In this work we choose to use the technique of structural learning (CITATIONa; CITATIONb),,
We chose to compare with ASO because it consistently outperforms cotraining CITATION and clustering methods CITATION,,
We did run experiments with the top-k version of ASO (CITATIONa), which is inspired by cotraining but consistently outperforms it,,
Structural correspondence learning is most similar to that of CITATION, who analyzed a situation with no target domain labeled data,,
There are many choices for modeling co-occurrence data (CITATION; CITATION; CITATION),,
In this work we choose to use the technique of structural learning (CITATIONa; CITATIONb),,
cognizers CITATION and automatic translators CITATION use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data,,
In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain (CITATION; CITATION; CITATION; CITATION; CITATION; Daume III and Marcu, 2006),,
For unknown words, SCL gives a relative reduction in error of 19.5% over CITATION, even with 40,000 sentences of source domain training data,,
In this case, we make use of the out-of-domain data by using features of the source domain taggers predictions in training and testing the target domain tagger CITATION,,
Though other methods for incorporating small amounts of training data in the target domain were available, such as those proposed by CITATION and by Daume III and Marcu (2006), we chose this method for its simplicity and consistently good performance,,
CITATION use a Dirichlet prior on the multinomial parameters of a generative parsing model to combine a large amount of training data from a source corpus (WSJ), and small amount of training data from a target corpus (Brown),,
Aside from CITATION, several authors have also given techniques for adapting classification to new domains,,
CITATION first train a classifier on the source data,,
For our experiments we use a version of the discriminative online large-margin learning algorithm MIRA CITATION,,
MIRA has been used successfully for both sequence analysis (CITATIONa) and dependency parsing (CITATIONb),,
systems like speech recognizers CITATION and automatic translators CITATION use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data,,
In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain (CITATION; CITATION; CITATION; CITATION; CITATION; Daume III and Marcu, 2006),,
In the case when some in-domain labeled training data is available, we show how to use SCL together with the classifier combination techniques of CITATION to achieve even greater performance,,
In this case, we use classifiers as features as described in CITATION,,
Finally, we show in Section 7.3 that our SCL PoS 124 \x0c(a) 100 500 1k 5k 40k 75 80 85 90 Results for 561 MEDLINE Test Sentences Number of WSJ Training Sentences Accuracy supervised semiASO SCL (b) Accuracy on 561-sentence test set Words Model All Unknown CITATION 87.2 65.2 supervised 87.9 68.4 semi-ASO 88.4 70.9 SCL 88.9 72.0 (c) Statistical Significance (McNemars) for all words Null Hypothesis p-value semi-ASO vs,,
For unknown words, SCL gives a relative reduction in error of 19.5% over CITATION, even with 40,000 sentences of source domain training data,,
In this case, we make use of the out-of-domain data by using features of the source domain taggers predictions in training and testing the target domain tagger CITATION,,
Though other methods for incorporating small amounts of training data in the target domain were available, such as those proposed by CITATION and by Daume III and Marcu (2006), we chose this method for its simplicity and consistently good performance,,
CITATION use a Dirichlet prior on the multinomial parameters of a generative parsing model to combine a large amount of training data from a source corpus (WSJ), and small amount of training data from a target corpus (Brown),,
Aside from CITATION, several authors have also given techniques for adapting classification to new domains,,
CITATION first train a classifier on the source data,,
lations between pivot and non-pivot feautres, and we adapt structural learning (ASO) (CITATIONa; CITATIONb) for this task,,
We also showed how to combine an SCL tagger with target domain labeled data using the classifier combination techniques from CITATION,,
We are also focusing on other potential applications, including chunking CITATION, named entity recognition (CITATION; CITATIONb; Daume III and Marcu, 2006), and speaker adaptation CITATION,,
We use a McNemar paired test for labeling disagreements CITATION,,
For unknown words, SCL gives a relative reduction in error of 19.5% over CITATION, even with 40,000 sentences of source domain training data,,
We also showed how to combine an SCL tagger with target domain labeled data using the classifier combination techniques from CITATION,,
We are also focusing on other potential applications, including chunking CITATION, named entity recognition (CITATION; CITATIONb; Daume III and Marcu, 2006), and speaker adaptation CITATION,,
utomatic translators CITATION use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data,,
In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain (CITATION; CITATION; CITATION; CITATION; CITATION; Daume III and Marcu, 2006),,
CITATION adapt a WSJ parser to biomedical text without any biomedical treebanked data,,
5 Data Sets and Supervised Tagger 5.1 Source Domain: WSJ We used sections 02-21 of the Penn Treebank CITATION for training,,
xperiments we use a version of the discriminative online large-margin learning algorithm MIRA CITATION,,
MIRA has been used successfully for both sequence analysis (CITATIONa) and dependency parsing (CITATIONb),,
We use the parser described by CITATIONb),,
xperiments we use a version of the discriminative online large-margin learning algorithm MIRA CITATION,,
MIRA has been used successfully for both sequence analysis (CITATIONa) and dependency parsing (CITATIONb),,
We use the parser described by CITATIONb),,
We chose to compare with ASO because it consistently outperforms cotraining CITATION and clustering methods CITATION,,
We did run experiments with the top-k version of ASO (CITATIONa), which is inspired by cotraining but consistently outperforms it,,
Structural correspondence learning is most similar to that of CITATION, who analyzed a situation with no target domain labeled data,,
Discriminative taggers and chunkers have been the state-of-the-art for more than a decade (CITATION; CITATION),,
Furthermore, end-to-end systems like speech recognizers CITATION and automatic translators CITATION use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data,,
In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain (CITATION; CITATION; CITATION; CITATION; CITATION; Daume ,,
For labeled training and testing purposes we use 1061 sentences that have been annotated by humans as part of the Penn BioIE project CITATION,,
There are many choices for modeling co-occurrence data (CITATION; CITATION; CITATION),,
In this work we choose to use the technique of structural learning (CITATIONa; CITATIONb),,
Discriminative taggers and chunkers have been the state-of-the-art for more than a decade (CITATION; CITATION),,
Furthermore, end-to-end systems like speech recognizers CITATION and automatic translators CITATION use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data,,
 a specific structural learning method, alternating structural optimization (ASO) (CITATIONa),,
Figure 1: Part of speech-tagged sentences from both corpora we investigate its use in part of speech (PoS) tagging (CITATION; CITATION),,
As in CITATIONa), we observed that setting h between 20 and 100 did not change results significantly, and a lower dimensionality translated to faster run-time,,
We also implemented both of the extensions described in CITATIONa),,
We used the same 58 feature types as CITATION,,
We found it necessary to make a change to the ASO algorithm as described in CITATIONa),,
In this case, we use classifiers as features as described in CITATION,,
Finally, we show in Section 7.3 that our SCL PoS 124 \x0c(a) 100 500 1k 5k 40k 75 80 85 90 Results for 561 MEDLINE Test Sentences Number of WSJ Training Sentences Accuracy supervised semiASO SCL (b) Accuracy on 561-sentence test set Words Model All Unknown CITATION 87.2 65.2 supervised 87.9 68.4 semi-ASO 88.4 70.9 SCL 88.9 72.0 (c) Statistical Significance (McNemars) for all words Null Hypothesis p-value semi-ASO vs,,
We use a McNemar paired test for labeling disagreements CITATION,,
For unknown words, SCL gives a relative reduction in error of 19.5% over CITATION, even with 40,000 sentences of source domain training data,,
In this case, we make use of the out-of-domain data by using features of the source domain taggers predictions in training and testing the target domain tagger CITATION,,
Though other methods for incorporating small amounts of training data in the target domain were available, such as those proposed by CITATION and by Daume III and Marcu (2006), we chose this method for its simplicity and,,
Furthermore, end-to-end systems like speech recognizers CITATION and automatic translators CITATION use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data,,
In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain (CITATION; CITATION; CITATION; CITATION; CITATION; Daume III and Marcu, 2006),,
CITATION use a Dirichlet prior on the multinomial parameters of a generative parsing model to combine a large amount of training data from a source corpus (WSJ), and small amount of training data from a target corpus (Brown),,
Aside from CITATION, several authors have also given techniques for adapting classification to new domains,,
CITATION first train a classifier on the source data,,
Discriminative taggers and chunkers have been the state-of-the-art for more than a decade (CITATION; CITATION),,
Furthermore, end-to-end systems like speech recognizers CITATION and automatic translators CITATION use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data,,
In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain (CITATION; CITATION; CITATION; Ando, ,,
Discriminative taggers and chunkers have been the state-of-the-art for more than a decade (CITATION; CITATION),,
Furthermore, end-to-end systems like speech recognizers CITATION and automatic translators CITATION use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data,,
We also showed how to combine an SCL tagger with target domain labeled data using the classifier combination techniques from CITATION,,
We are also focusing on other potential applications, including chunking CITATION, named entity recognition (CITATION; CITATIONb; Daume III and Marcu, 2006), and speaker adaptation CITATION,,
ral learning method, alternating structural optimization (ASO) (CITATIONa),,
Figure 1: Part of speech-tagged sentences from both corpora we investigate its use in part of speech (PoS) tagging (CITATION; CITATION),,
