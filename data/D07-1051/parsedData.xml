<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<bodyText confidence="0.622835">
b&apos;Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 486495, Prague, June 2007. c
</bodyText>
<sectionHeader confidence="0.473172" genericHeader="abstract">
2007 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.802832">
An Approach to Text Corpus Construction which Cuts Annotation Costs
and Maintains Reusability of Annotated Data
</title>
<author confidence="0.988867">
Katrin Tomanek Joachim Wermter Udo Hahn
</author>
<affiliation confidence="0.997602">
Jena University Language &amp; Information Engineering (JULIE) Lab
</affiliation>
<address confidence="0.838262">
Furstengraben 30
D-07743 Jena, Germany
</address>
<email confidence="0.822758">
{tomanek|wermter|hahn}@coling-uni-jena.de
</email>
<sectionHeader confidence="0.981592" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.997332285714286">
We consider the impact Active Learning
(AL) has on effective and efficient text cor-
pus annotation, and report on reduction rates
for annotation efforts ranging up until 72%.
We also address the issue whether a corpus
annotated by means of AL using a particu-
lar classifier and a particular feature set can
be re-used to train classifiers different from
the ones employed by AL, supplying alter-
native feature sets as well. We, finally, report
on our experience with the AL paradigm un-
der real-world conditions, i.e., the annota-
tion of large-scale document corpora for the
life sciences.
</bodyText>
<sectionHeader confidence="0.997563" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992779722222222">
The annotation of corpora has become a crucial pre-
requisite for NLP utilities which rely on (semi-) su-
pervised machine learning (ML) techniques. While
stability, by and large, has been reached for tagsets
up until the syntax layer, semantic annotations in
terms of (named) entities, semantic roles, proposi-
tions, events, etc. reveal a high degree of variability
due to the inherent domain-dependence of the under-
lying tagsets. This diversity fuels a continuous need
for creating semantic annotation data anew.
Accordingly, annotation activities will persist and
even increase in number as HLT is expanding on
various technical and scientific domains (e.g., the
life sciences) outside the classical general-language
newspaper genre. Since the provision of annota-
tions is a costly, labor-intensive and error-prone pro-
cess the amount of work and time this activity re-
quires should be minimized to the extent that corpus
data could still be used to effectively train ML-based
NLP components on them. The approach we ad-
vocate does exactly this and yields reduction gains
(compared with standard procedures) ranging be-
tween 48% to 72%, without seriously sacrificing an-
notation quality.
Various techniques to minimize the necessary
amount of annotated training material have al-
ready been investigated. In co-training (Blum and
Mitchell, 1998), e.g., from a small initial set of la-
beled data multiple learners mutually provide new
training material for each other by labeling unseen
examples. Pierce and Cardie (2001) have shown,
however, that for tasks which require large numbers
of labeled examples such as most NLP tasks co-
training might be inadequate because it tends to gen-
erate noisy data. Furthermore, a well compiled ini-
tial training set is a crucial prerequisite for success-
ful co-training. As another alternative for minimiz-
ing annotation work, active learning (AL) is based
on the idea to let the learner have control over the ex-
amples to be manually labeled so as to optimize the
prediction accuracy. Accordingly, AL aims at select-
ing those examples with high utility for the model.
AL (as well as semi-supervised methods) is typi-
cally considered as a learning protocol, i.e., to train
a particular classifier. In contrast, we here propose
to employ AL as a corpus annotation method. A
corpus built on these premises must, however, still
be reusable in a flexible way so that, e.g., train-
ing with modified or improved classifiers is feasible
and reasonable on AL-generated corpora. Baldridge
and Osborne (2004) have already argued that this is
a highly critical requirement because the examples
selected by AL are tuned to one particular classi-
fier. The second major contribution of this paper ad-
</bodyText>
<page confidence="0.99846">
486
</page>
<bodyText confidence="0.9921398">
\x0cdresses this issue and provides empirical evidence
that corpora built with one type of classifier (based
on Maximum Entropy) can reasonably be reused by
another, methodologically related type of classifier
(based on Conditional Random Fields) without re-
quiring changes of the corpus data. We also show
that feature sets being used for training classifiers
can be enhanced without invalidating corpus annota-
tions generated on the basis of AL and, hence, with
a poorer feature set.
</bodyText>
<sectionHeader confidence="0.999627" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997892729166666">
There are mainly two methodological strands of
AL research, viz. optimization approaches which
aim at selecting those examples that optimize some
(algorithm-dependent) objective function, such as
prediction variance (Cohn et al., 1996), and heuris-
tic methods with uncertainty sampling (Lewis and
Catlett, 1994) and query-by-committee (QBC) (Se-
ung et al., 1992) just to name the most prominent
ones. AL has already been applied to several NLP
tasks, such as document classification (Schohn and
Cohn, 2000), POS tagging (Engelson and Dagan,
1996), chunking (Ngai and Yarowsky, 2000), statis-
tical parsing (Thompson et al., 1999; Hwa, 2000),
and information extraction (Lewis and Catlett, 1994;
Thompson et al., 1999).
In a more recent study, Shen et al. (2004) consider
AL for entity recognition based on Support Vector
Machines. Here, the informativeness of an exam-
ple is estimated by the distance to the hyperplane of
the currently learned SVM. It is assumed that an ex-
ample which lies close to the hyperplane has high
chances to have an effect on training. This approach
is essentially limited to the SVM learning scheme as
it solely relies on SVM-internal selection criteria.
Hachey et al. (2005) propose a committee-based
AL approach where the committees classifiers con-
stitute multiple views on the data by employing dif-
ferent feature subsets. The authors focus on (pos-
sible) negative side effects of AL on the annota-
tions. They argue that AL annotations are cogni-
tively more difficult to deal with for the annota-
tors (because of the increased complexity of the se-
lected sentences). Hence, lower annotation quality
and higher per-sentence annotation times might be a
concern.
There are controversial findings on the reusabil-
ity of data annotated by means of AL for the prob-
lem of parse tree selection. Whereas Hwa (2001) re-
ports positive results, Baldridge and Osborne (2004)
argue that AL based on uncertainty sampling may
face serious performance degradation when labeled
data is reused for training a classifier different from
the one employed during AL. For committee-based
AL, however, there is a lack of work on reusabil-
ity. Our experiments of committee-based AL for en-
tity recognition, however, reveal that for this task at
least, reusability can be guaranteed to a very large
extent.
</bodyText>
<sectionHeader confidence="0.994772" genericHeader="method">
3 AL for Corpus Annotation -
</sectionHeader>
<subsectionHeader confidence="0.513714">
Requirements for Practical Use
</subsectionHeader>
<bodyText confidence="0.9985387">
AL frameworks for real-world corpus annotation
should meet the following requirements:
fast selection time cycles AL-based corpus an-
notation is an interactive process in which b
sentences are selected by the AL engine for hu-
man annotation. Once the annotated data is
supplied, the AL engine retrains its underly-
ing classifier(s) on all available annotations and
then re-classifies all unseen corpus items. After
that the most informative (i.e., deviant) b sen-
tences from the set of newly classified data are
selected for the next iteration round. In this ap-
proach the time needed to select the next exam-
ples (which is the idle time of the human an-
notators) has to be kept at an acceptable limit
of a few minutes only. There are various AL
strategies which although they yield theoreti-
cally near-optimal sample selection turn out
to be actually impractible for real-world use
because of excessively high computation times
(cf. Cohn et al. (1996)). Thus, AL-based an-
notation should be based on a computationally
tractable and task-wise feasible and acceptable
selection strategy (even if this might imply a
suboptimal reduction of annotation costs).
reusability The examples AL selects for man-
ual annotation are dependent on the model be-
ing used, up to a certain extent (Baldridge and
Osborne, 2004). During annotation time, how-
ever, the best model might not be known and
</bodyText>
<page confidence="0.997596">
487
</page>
<bodyText confidence="0.9992515">
\x0cmodel tuning (especially the choice of features)
is typically performed once a training corpus
is available. Hence, from a practical point of
view, the resulting corpus should be reusable
with modified classifiers as well.
adaptive stopping criterion An explicit and
adaptive stopping criterion which is sensitive
towards the already achieved level of quality of
the annotated corpus is clearly preferred over
stopping after an a priori fixed number of an-
notation iterations.
If these requirements, especially the first and the
second one, cannot be guaranteed for a specific an-
notation task one should refrain from using AL. The
efficiency of AL-driven annotation (in terms of the
time needed to compile high quality training mate-
rial) might be worse compared to the annotation of
randomly (or subjectively) selected examples.
</bodyText>
<sectionHeader confidence="0.826653" genericHeader="method">
4 Framework for AL-based Named Entity
Annotation
</sectionHeader>
<bodyText confidence="0.999486777777778">
For named entity recognition (NER), each change
of the application domain requires a more or less
profound change of the types of semantic categories
(tags) being used for corpus annotation. Hence, one
may encounter a lack of training material for various
relevant (sub)domains. Once this data is available,
however, one might want to modify the features of
the final classifier with respect to the specific entity
types. Thus, a corpus annotated by means of AL has
to provide the flexibility to modify the features of
the final classifier.
To meet the requirements from above under the
constraints of a real-world annotation task, we
decided for QBC-based AL, a heuristic AL ap-
proach, which is computationally less complex and
resource-greedy than objective function AL meth-
ods (the latter explicitly quantify the differences be-
tween the current and an ideal classifier in terms
of some objective function). Accordingly, we ruled
out uncertainty sampling, another heuristic AL ap-
proach, because it was shown before that QBC is
more efficient and robust (Freund et al., 1997).
QBC is based on the idea to select those examples
for manual annotation on which a committee of clas-
sifiers disagree most in their predictions (Engelson
and Dagan, 1996). A committee consists of a num-
ber of k classifiers of the same type (same learning
algorithm, parameters, and features) but trained on
different subsets of the training data. QBC-based
AL is also iterative. In each AL round the com-
mittees k classifiers are trained on the already an-
notated data C, then a pool of unannotated data P
is predicted with each classifier resulting in n au-
tomatically labeled versions of P. These are then
compared according to their labels. Those with the
highest variance are selected for manual annotation.
</bodyText>
<subsectionHeader confidence="0.999443">
4.1 Selection Strategy
</subsectionHeader>
<bodyText confidence="0.998104666666666">
In each iteration, a batch of b examples is selected
for manual annotation. The informativeness of an
example is estimated in terms of the disagreement,
i.e., the uncertainty among the committees classi-
fiers on classifying a particular example. This is
measured by the vote entropy (Engelson and Dagan,
1996), i.e., the entropy of the distribution of classi-
fications assigned to an example by the classifiers.
Vote entropy is defined on the token level t as:
</bodyText>
<equation confidence="0.9971997">
Dtok(t) :=
1
log k
X
li
V (li, t)
k
log
V (li, t)
k
</equation>
<bodyText confidence="0.996009105263158">
where V (li,t)
k is the ratio of k classifiers where the
label li is assigned to a token t. As (named) en-
tities often span more than a single text token we
consider complete sentences as a reasonable exam-
ple size unit1 for AL and calculate the disagreement
of a sentence Dsent as the mean vote entropy of
its single tokens. Since the vote entropy is mini-
mal when all classifiers agree in their vote, sentences
with high disagreement are preferred for manual an-
notation. With informed decisions of human anno-
tators made available, the potential for future dis-
agreement of the classifier committee on conflicting
instances should decrease. Thus, each AL iteration
selects the b sentences with the highest disagreement
to focus on the most controversial decision prob-
lems.
Besides informativeness, additional criteria can
be envisaged for the selection of examples, e.g., di-
</bodyText>
<page confidence="0.892404">
1
</page>
<bodyText confidence="0.9969395">
Sentence-level examples are but one conceivable grain size
lower grains (such as clauses or phrases) as well as higher
grains (e.g., paragraphs or abstracts) are equally possible, with
different implications for the AL process.
</bodyText>
<page confidence="0.995455">
488
</page>
<bodyText confidence="0.978484083333333">
\x0cfeature class description
orthographical based on regular expressions (e.g. Has-
Dash, IsGreek, ...), token transforma-
tion rule: capital letters replaced by A,
lowercase letters by a, digits by 0,
etc. (e.g., IL2 AA0, have aaaa)
lexical and
morphological
prefix and suffix of length 3, stemmed
version of each token
syntactic the tokens part-of-speech tag
contextual features of neighboring tokens
</bodyText>
<tableCaption confidence="0.907392">
Table 1: Features used for AL
</tableCaption>
<bodyText confidence="0.997028428571429">
versity of a batch and representativeness of the re-
spective example (to avoid outliers) (Shen et al.,
2004). We experimented with these more sophis-
ticated selection strategies but preliminary experi-
ments did not reveal any significant improvement of
the AL performance. Engelson and Dagan (1996)
confirm this observation that, in general, different
(and even more refined) selection methods still yield
similar results. Moreover, strategies incorporating
more selection criteria often require more parame-
ters to be set. However, proper parametrization is
hard to achieve in real-world applications. Using
disagreement exclusively for selection requires only
one parameter, viz. the batch size b, to be specified.
</bodyText>
<subsectionHeader confidence="0.978951">
4.2 Classifier and Features
</subsectionHeader>
<bodyText confidence="0.9979553">
For our AL framework we decided to employ a Max-
imum Entropy (ME) classifier (Berger et al., 1996).
We employ a rich set of features (see Table 1) which
are general enough to be used in most (sub)domains
for entity recognition. We intentionally avoided us-
ing features such as semantic triggers or external
dictionary look-ups because they depend a lot on
the specific subdomain and entity types being used.
However, one might add them to fin- tune the final
classifier, if needed. ME classifiers outperform their
generative counterparts (e.g., Nave Bayesian clas-
sifiers) because they can easily handle overlapping,
probably dependent features which might be con-
tained in rich feature sets. We also favored an ME
classifier over an SVM one because the latter is com-
putationally much more complex on rich feature sets
and multiple classes and is thus not so well suited for
an interactive process like AL.
It has been shown that Conditional Random
Fields (CRF) (Lafferty et al., 2001) achieve higher
performance on many NLP tasks, such as NER, but
on the other hand they are computionally more com-
plex than an ME classifier making them also im-
practical for the interactive AL process. Thus, in
our committee we employ ME classifiers to meet re-
quirement 1 (fast selection time cycles). However,
in the end we want to use the annotated corpora to
train a CRF and will thus examine the reusability
of such an ME-annotated AL corpus for CRFs (cf.
Subsection 5.2).
</bodyText>
<subsectionHeader confidence="0.997493">
4.3 Stopping Criterion
</subsectionHeader>
<bodyText confidence="0.999098833333333">
A question hardly addressed up until now is when to
actually terminate the AL process. Usually, it gets
stopped when the supervized learning performance
of the specific classifier is achieved. The problem
with such an approach is, however, that in prac-
tice one does not know the performance level which
could possibly be achieved on an unannotated cor-
pus.
An apparent way to monitor the progress of the
annotation process is to periodically (e.g., after each
AL iteration) train a classifier on the data annotated
so far and evaluate it against some randomly se-
lected gold standard. When the relative performance
growth of each AL iteration falls below a certain
threshold this might be a good reason to stop the an-
notation. Though this is probably the most reliable
way, it is impractical for many scenarios since as-
sembling and manually annotating a representative
gold standard may already be quite a laborious task.
Thus, a measure from which we can predict the de-
velopment of the learning curve would be beneficial.
One way to achieve this goal is to monitor the rate
of disagreement among the different classifiers after
each iteration. This rate will descend as the classi-
fiers get more and more robust in their predictions
on unseen data. Thus, an average disagreement ap-
proaching zero can be interpreted as an indication
that additional annotations will not render any fur-
ther improvement. In our experiments, we will show
that this is a valid stopping criterion, indeed.
</bodyText>
<sectionHeader confidence="0.9931" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.99909875">
For our experiments, we specified the following
three parameters: the batch size b (i.e., the num-
ber of sentences to be selected for each AL itera-
tion), the size and composition of the initial train-
</bodyText>
<page confidence="0.99276">
489
</page>
<bodyText confidence="0.989155113636364">
\x0cing set, and the number of k classifiers in a com-
mittee. The smaller the batch size, the higher the
AL performance turns out to be. In the special case
of batch size of b = 1 only that example with the
highest disagreement is selected. This is certainly
impractical since after each AL iteration a new com-
mittee of classifiers has to be trained causing unwar-
ranted annotation idle time. We found b = 20 to
be a good compromise between the annotators idle
time and AL performance. The initial training set
also contains 20 sentences which are randomly se-
lected though. Our committee consists of k = 3
classifiers, which is a good trade-off between com-
putational complexity and diversity. Although the
AL iterations were performed on the sentence level,
we report on the number of annotated tokens. Since
sentences may considerably vary in their length the
number of tokens constitutes a better measure for an-
notation costs.
We ran our experiments on two common entity-
annotated corpora from two different domains (see
Table 2). From the general-language newspaper do-
main, we used the English data set of the CoNLL-
2003 shared task (Tjong Kim Sang and De Meul-
der, 2003). It consists of a collection of newswire
articles from the Reuters Corpus,2 which comes
annotated with three entity types: persons, loca-
tions, and organizations. From the sublanguage
biology domain we used the oncology part of the
PENNBIOIE corpus which consists of some 1150
PubMed abstracts. Originally, this corpus contains
gene, variation event, and malignancy entity annota-
tions. Manual annotation after each AL round was
simulated by moving the selected sentences from
the pool of unannotated sentences P to the train-
ing corpus T. For our simulations, we built two
subcorpora by filtering out entity annotations: the
PENNBIOIE gene corpus (PBgene), including the
three gene entity subtypes generic, protein, and rna,
and the PENNBIOIE variation events corpus (PB-
var) corpus including the variation entity subtypes
type, event, location, state-altered, state-generic,
and state-original. We split all three corpora into
two subsets, viz. AL simulation data and gold stan-
</bodyText>
<footnote confidence="0.260785333333333">
dard data on which we evaluate3 a classifier in terms
2
http://trec.nist.gov/
</footnote>
<page confidence="0.93673">
3
</page>
<bodyText confidence="0.901945">
We use a strict evaluation criterion which only counts exact
matches as true positives because annotations having incorrect
corpus data set sentences tokens
</bodyText>
<table confidence="0.9957315">
CONLL AL 14,040 203,617
3 entities Gold 3,453 46,435
PBGENE AL 10,050 249,490
3 entities Gold 1,114 27,563
PBVAR AL 10,050 249,490
6 entities Gold 1,114 27,563
</table>
<tableCaption confidence="0.984071">
Table 2: Corpora used in the Experiments
</tableCaption>
<bodyText confidence="0.898675">
of f-score trained on the annotated corpus after each
AL iteration (learning curve). As far as the CoNLL
corpus is concerned, we have used CoNLLs training
set for AL and CoNLLs test set as gold standard. As
for PBgene and PBvar, we randomly split the cor-
pora into 90% for AL and 10% as gold standard.
In the following experiments we will refer to the
classifiers used in the AL committee as selectors,
and the classifier used for evaluation as the tester.
</bodyText>
<subsectionHeader confidence="0.919119">
5.1 Efficiency of AL and the Applicability of
the Stopping Criterion
</subsectionHeader>
<bodyText confidence="0.998049481481481">
In a first series of experiments, we evaluated whether
AL-based annotations can significantly reduce the
human effort compared to the standard annotation
procedure where sentences are selected randomly
(or subjectively). We also show that disagreement
is an accurate stopping criterion. As described in
Section 4.2, we here employed a committee of ME
classifiers for AL; a CRF was used as tester for both
the AL and the random selection. Figures 1, 2, and 3
depict the learning curves for AL selection and ran-
dom selection (upper two curves) and the respective
disagreement curves (lower curve). The random se-
lection curves contained in these plots are averaged
over three random selection runs.
With AL, we get a maximum f-score of 84.5%
on the CoNLL corpus after about 118,000 tokens. At
about the same number of tokens the disagreement
curve drops down to values of around Dsent = 0.
Comparing AL and random selection, an f-score of
84% is reached after 86,000 and 165,000 tokens,
respectively, which means a reduction of annotation
costs of about 48%. On PBgene, the effect of AL is
comparable: a maximum value of 83.5% f-score is
reached first after about 124,000 tokens, a data point
where hardly any disagreement between the com-
mittees classifiers occurs. For, e.g., an f-score of
boundaries are insufficient for manual corpus annotation.
</bodyText>
<page confidence="0.9965">
490
</page>
<figureCaption confidence="0.999257">
\x0cFigure 1: CoNLL Corpus: Learning/Disagreement Curves
Figure 2: PBgene Corpus: Learning/Disagreement Curves
Figure 3: PBvar Corpus: Learning/Disagreement Curves
</figureCaption>
<table confidence="0.982592857142857">
corpus selection F tokens reduction
CONLL random 84.0 165,000
AL 84.0 86,000 48%
PBGENE random 83.0 101,000
AL 83.0 213,000 53%
PBVAR random 80.0 56,000
AL 80.0 200,000 72%
</table>
<tableCaption confidence="0.956295">
Table 3: Reduction of Annotation Costs Achieved
with AL-based Annotation
</tableCaption>
<bodyText confidence="0.990182419354839">
83%, the annotation effort can be reduced by about
53% using AL. On PBvar, an f-score of about 80%
is reached after 56,000 tokens when using AL se-
lection, while 200,000 tokens are needed with ran-
dom selection. For this task, AL reduces the an-
notation effort by of 72%. Here, the disagreement
curve approaches values of zero after approximately
80,000 tokens. At about this point the learning curve
reaches its maximum of about 81% f-score. Ta-
ble 3 summarizes the reduction of annotation costs
achieved on all three corpora.
Comparing both PENNBIOIE simulations, obvi-
ously, the reduction of annotation costs through AL
is much higher for the variation type entities than for
the gene entities. We hypothesize this to be mainly
due to incomparable entity densities. Whereas the
gene entities are quite frequent (about 1.3 per sen-
tence on average), the variation entities are rather
sparse (0.62 per sentence on average) making it an
ideal playground for AL-based annotation. Our ex-
periments also reveal that disagreement approaching
values of zero is a valid stopping criterion. This is,
under all circumstances, definitely the point when
AL-based annotation should stop because then all
classifiers of the committee vote consistently. Any
further selection even though AL selection is used
is then, actually, a random selection. If, due to
reasons whatsoever, further annotations are wanted,
a direct switch to random selection is advisable be-
cause this is computationally less expensive than
AL-based selection.
</bodyText>
<subsectionHeader confidence="0.994656">
5.2 Reusability
</subsectionHeader>
<bodyText confidence="0.9996232">
To evaluate whether the proposed AL framework for
named entity annotation allows for flexible re-use
of the annotated data, we performed experiments
where we varied both the learning algorithms and
the features of the selectors.
</bodyText>
<page confidence="0.983897">
491
</page>
<figure confidence="0.99196745">
\x0c0.6
0.65
0.7
0.75
0.8
0.85
0.9
140
120
100
80
60
40
20
F-score
K tokens
AL (CRF committee)
AL (ME committee)
AL (NB committee)
random selection
</figure>
<figureCaption confidence="0.995984">
Figure 4: Algorithm Flexibility on PBvar
</figureCaption>
<figure confidence="0.98482015">
0.6
0.65
0.7
0.75
0.8
0.85
0.9
140
120
100
80
60
40
20
F-score
K tokens
AL (CRF committee)
AL (ME committee)
AL (NB committee)
random selection
</figure>
<figureCaption confidence="0.99996">
Figure 5: AlgorithmFflexibility on CoNLL
</figureCaption>
<bodyText confidence="0.98895505882353">
First, we analyzed the effect of different proba-
bilistic classifiers as selectors on the resulting learn-
ing curve of the CRF tester. Figures 4 and 5 show
the learning curves on our original ME committee,
a CRF committee, and also a committee of Nave
Bayes (NB) classifiers. It is not surprising that self-
reuse (CRF selectors and CRF tester) yields the best
results. Switching from CRF selectors to ME selec-
tors has almost no negative effect. Even with a com-
mittee of NB selectors (an ML approach which is
essentially less well suited for the NER task), AL-
based selection is still substantially more efficient
than random selection on both corpora. This shows
that our approach to use the less complex ME clas-
sifiers for the AL selection process has the positive
effect of fast selection cycle times at almost no costs.
This is especially interesting as the performance of
</bodyText>
<figure confidence="0.967160904761905">
0.6
0.65
0.7
0.75
0.8
0.85
0.9
140
120
100
80
60
40
20
F-score
K tokens
all features
sub1
sub2
sub3
random selection
</figure>
<figureCaption confidence="0.99809">
Figure 6: Feature Flexibility on PBvar
</figureCaption>
<figure confidence="0.984056285714286">
0.6
0.65
0.7
0.75
0.8
0.85
0.9
140
120
100
80
60
40
20
F-score
K tokens
all features
sub1
sub2
sub3
random selection
</figure>
<figureCaption confidence="0.999975">
Figure 7: Feature Flexibility on ConLL
</figureCaption>
<bodyText confidence="0.989455846153846">
an ME classifier trained in supervized manner on
the complete corpus is significantly worse (several
percentage points of f-measure) than a CRF. That
means, even though an ME classifier is less well
suited as the final classifier, it works well as a se-
lector for CRFs.4
Second, we ran experiments on selectors with
only some features and our CRF tester with all fea-
tures (cf. Table 1). Feature subset 1 (sub1) contains
all but the syntactic features. In the second subset
(sub2), also morphological and lexical features are
missing. The third set (sub3) only contains ortho-
graphical features. We ran an AL simulation for
</bodyText>
<page confidence="0.981056">
4
</page>
<bodyText confidence="0.993624">
We have also conducted experiments where we varied the
learning algorithms of the tester (we experimented with NB,
ME, MEMM, and CRFs) with comparable results. In a real-
istic scenario, however, on would rather choose a CRF as final
tester over, e.g., a NB.
</bodyText>
<page confidence="0.994621">
492
</page>
<bodyText confidence="0.99945935">
\x0ceach feature subset with a committee of CRF se-
lectors.5 Figures 6 and 7 show the various learning
curves. Here we see that a corpus that was produced
with AL on sub1 can easily be re-used by a tester
with little more features. This is probably the most
realistic scenario: the core features are kept and
only a few specific features (e.g., POS, a dictionary
look-up, chunk information, etc.) are added. When
adding substantially more features to the tester than
were available during AL time, the respective learn-
ing curves drop down towards the learning curve for
random selection. But even with a selector which
has only orthographical features and a tester with
many more features which is actually quite an ex-
treme example and a rather unrealistic scenario for
a real-world application AL is more efficient than
random selection. However, the limits of reusability
are taking shape: on PBvar, the AL selection with
sub3 converges with the random selection curve af-
ter about 100,000 tokens.
</bodyText>
<subsectionHeader confidence="0.975324">
5.3 Findings with Real AL Annotation
</subsectionHeader>
<bodyText confidence="0.999721375">
We currently perform AL entity mention annotations
for an information extraction project in the biomedi-
cal subdomain of immunogenetics. For this purpose,
we retrieved about 200,000 abstracts ( 2,000,000
sentences) as our document pool of unlabeled exam-
ples from PUBMED. By means of random subsam-
pling, only about 40,000 sentences are considered in
each round of AL selection. To regularly monitor
classifier performance, we also perform gold stan-
dard (GS) annotations on 250 randomly chosen ab-
stracts ( 2,200 sentences). In all our annotations of
different entity types so far, we found AL learning
curves similar to the ones reported in our simula-
tion experiments, with classifier performance level-
ling off at around 75% - 85% f-score (depending on
the entity type).
Our annotations also reveal that AL is especially
beneficial when entity mentions are very sparse.
Figure 8 shows the cumulated entity density on AL
and gold standard annotations of cytokine receptors
(specialized proteins for which we annotated six dif-
ferent entity subtypes) very sparse entity types
with less than one entity mention per PUBMED ab-
stract on the average. As can be seen, after 2,000
</bodyText>
<page confidence="0.868145">
5
</page>
<bodyText confidence="0.847986">
Here, we employed CRF instead of ME selectors to isolate
the effect of feature re-usability.
</bodyText>
<figure confidence="0.997817">
0
500
1000
1500
2000
2500
3000
200 400 600 800 1000 1200 1400 1600 1800 2000
entity
mentiones
sentences
GS annotation
AL annotation
</figure>
<figureCaption confidence="0.999845">
Figure 8: Cumulated Entity Density on AL and GS
</figureCaption>
<subsubsectionHeader confidence="0.376426">
Annotations of Cytokine Receptors
</subsubsectionHeader>
<bodyText confidence="0.988772181818182">
sentences the entity density in our AL corpus is al-
most 15 times higher than in our GS corpus. Such a
dense corpus may be more appropriate for classifier
training than a sparse one yielded by random or se-
quential annotations, which may just contain lots of
negative training examples. We have observed com-
parable effects with other entity types, too, and thus
conclude that the sparser entity mentions of a spe-
cific type are in texts, the more beneficial AL-based
annotation is. We report on other aspects of AL for
real annotation projects in Tomanek et al. (2007).
</bodyText>
<sectionHeader confidence="0.976959" genericHeader="discussions">
6 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.999702722222222">
We have shown, for the annotation of (named) en-
tities, that AL is well-suited to speed up annotation
work under realistic conditions. In our simulations
we yielded gains (in the number of tokens) up to
72%. We collected evidence that an average dis-
agreement approaching zero may serve as an adap-
tive stopping criterion for AL-driven annotation and
that a corpus compiled by means of QBC-based AL
is to a large extent reusable by modified classifiers.
These findings stand in contrast to those supplied
by Baldridge and Osborne (2004) who focused on
parse selection. Their research indicates that AL on
selectors with different learning algorithms and fea-
ture sets then used by the tester can easily get worse
than random selection. They conclude that it might
not be be advisable to employ AL in environments
where the final classifier is not very stable.
Our evidence leads us to a re-assessment of AL-
</bodyText>
<page confidence="0.991531">
493
</page>
<bodyText confidence="0.999665590909091">
\x0cbased annotations. First, we employed a committee-
based (QBC) while Baldridge and Osborne per-
formed uncertainty sampling AL. Committee-based
approaches calculate the uncertainty on an exam-
ple in a more implicit way, i.e., by the disagree-
ment among the committees classifiers. With uncer-
tainty sampling, however, the labeling uncertainty
of one classifier is considered directly. In future
work we will directly compare QBC and uncertainty
sampling with respect to data reusability. Second,
whereas Baldridge and Osborne employed AL on a
scoring or ranking problem we focused on classifica-
tion problems. Further research is needed to inves-
tigate whether the problem class (classification with
a fixed and moderate number of classes vs. ranking
large numbers of possible candidates) is responsible
for limited data reusability.
On the basis of our experiments we stipulate that
the proposed AL approach might be applicable with
comparable results to a wider range of corpus anno-
tation tasks, which otherwise would require substan-
tially larger amounts of annotation efforts.
</bodyText>
<sectionHeader confidence="0.943101" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.96262825">
This research was funded by the EC within the
BOOTStrep project (FP6-028099), and by the Ger-
man Ministry of Education and Research within the
StemNet project (01DS001A to 1C).
</bodyText>
<sectionHeader confidence="0.927866" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.963229571428571">
Jason Baldridge and Miles Osborne. 2004. Active learn-
ing and the total cost of annotation. In Dekang Lin
and Dekai Wu, editors, EMNLP 2004 Proceedings of
the 2004 Conference on Empirical Methods in Natural
Language Processing, pages 916. Barcelona, Spain,
July 25-26, 2004. Association for Computational Lin-
guistics.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):3971.
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT98 Proceedings of the 11th Annual Confer-
ence on Computational Learning Theory, pages 92
100. Madison, Wisconsin, USA, July 24-26, 1998.
New York, NY: ACM Press.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artifical Intelligence Research, 4:129145.
Sean Engelson and Ido Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In ACL96 Proceedings of the 34th Annual
Meeting of the Association for Computational Linguis-
tics, pages 319326. University of California at Santa
Cruz, California, U.S.A., 24-27 June 1996. San Fran-
cisco, CA: Morgan Kaufmann.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naf-
tali Tishby. 1997. Selective sampling using the query
by committee algorithm. Machine Learning, 28(2-
3):133168.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In CoNLL-2005 Proceedings of the
9th Conference on Computational Natural Language
Learning, pages 144151. Ann Arbor, MI, USA, June
2005. Association for Computational Linguistics.
Rebecca Hwa. 2000. Sample selection for statistical
grammar induction. In EMNLP/VLC-2000 Proceed-
ings of the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and Very
Large Corpora, pages 4552. Hong Kong, China, Oc-
tober 7-8, 2000.. Association for Computational Lin-
guistics.
Rebecca Hwa. 2001. On minimizing training corpus for
parser acquisition. In Walter Daelemans and Remi
Zajac, editors, CoNLL-2001 Proceedings of the
5th Natural Language Learning Workshop. Toulouse,
France, 6-7 July 2001. Association for Computational
Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In ICML-2001 Proceedings of the 18th In-
ternational Conference on Machine Learning, pages
282289. Williams College, MA, USA, June 28 - July
1, 2001. San Francisco, CA: Morgan Kaufmann.
David D. Lewis and Jason Catlett. 1994. Heteroge-
neous uncertainty sampling for supervised learning.
In William W. Cohen and Haym Hirsh, editors, ICML
94: Proceedings of the 11th International Conference
on Machine Learning, pages 148156. San Francisco,
CA: Morgan Kaufmann.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In ACL00 Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics, pages 117125. Hong Kong, China,
1-8 August 2000. San Francisco, CA: Morgan Kauf-
mann.
</reference>
<page confidence="0.986733">
494
</page>
<reference confidence="0.999630021739131">
\x0cDavid Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Lillian Lee and Donna Harman, editors,
EMNLP 2001 Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Process-
ing, pages 19. Pittsburgh, PA, USA, June 3-4, 2001.
Association for Computational Linguistics.
Greg Schohn and David Cohn. 2000. Less is more: Ac-
tive learning with support vector machines. In ICML
00: Proceedings of the 17th International Conference
on Machine Learning, pages 839846. San Francisco,
CA: Morgan Kaufmann.
H. Sebastian Seung, Manfred Opper, and Haim Som-
polinsky. 1992. Query by committee. In COLT92
Proceedings of the 5th Annual Conference on Compu-
tational Learning Theory, pages 287294. Pittsburgh,
PA, USA, July 27-29, 1992. New York, NY: ACM
Press.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and
Chew Lim Tan. 2004. Multi-criteria-based active
learning for named entity recognition. In ACL04
Proceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 589596.
Barcelona, Spain, July 21-26, 2004. San Francisco,
CA: Morgan Kaufmann.
Cynthia A. Thompson, Mary Elaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In ICML
99: Proceedings of the 16th International Conference
on Machine Learning, pages 406414. Bled, Slovenia,
June 1999. San Francisco, CA: Morgan Kaufmann.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CONLL-2003 shared
task: Language-independent named entity recogni-
tion. In Walter Daelemans and Miles Osborne, edi-
tors, CoNLL-2003 Proceedings of the 7th Confer-
ence on Computational Natural Language Learning,
pages 142147. Edmonton, Canada, 2003. Association
for Computational Linguistics.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Efficient annotation with the Jena ANnota-
tion Environment (JANE). In Proceedings of the ACL
2007 Linguistic Annotation Workshop A Merger of
NLPXML 2007 and FLAC 2007. Prague, Czech Re-
public, June 28-29, 2007. Association for Computa-
tional Linguistics (ACL).
</reference>
<page confidence="0.990252">
495
</page>
<figure confidence="0.245396">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.550789">
<note confidence="0.941289">b&apos;Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 486495, Prague, June 2007. c 2007 Association for Computational Linguistics</note>
<title confidence="0.8637235">An Approach to Text Corpus Construction which Cuts Annotation Costs and Maintains Reusability of Annotated Data</title>
<author confidence="0.966511">Katrin Tomanek Joachim Wermter Udo Hahn</author>
<affiliation confidence="0.99823">Jena University Language &amp; Information Engineering (JULIE) Lab</affiliation>
<address confidence="0.9653775">Furstengraben 30 D-07743 Jena, Germany</address>
<email confidence="0.99886">{tomanek|wermter|hahn}@coling-uni-jena.de</email>
<abstract confidence="0.999759866666667">We consider the impact Active Learning (AL) has on effective and efficient text corpus annotation, and report on reduction rates for annotation efforts ranging up until 72%. We also address the issue whether a corpus annotated by means of AL using a particular classifier and a particular feature set can be re-used to train classifiers different from the ones employed by AL, supplying alternative feature sets as well. We, finally, report on our experience with the AL paradigm under real-world conditions, i.e., the annotation of large-scale document corpora for the life sciences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Miles Osborne</author>
</authors>
<title>Active learning and the total cost of annotation.</title>
<date>2004</date>
<booktitle>EMNLP 2004 Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>916</pages>
<editor>In Dekang Lin and Dekai Wu, editors,</editor>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3640" citStr="Baldridge and Osborne (2004)" startWordPosition="560" endWordPosition="563">ea to let the learner have control over the examples to be manually labeled so as to optimize the prediction accuracy. Accordingly, AL aims at selecting those examples with high utility for the model. AL (as well as semi-supervised methods) is typically considered as a learning protocol, i.e., to train a particular classifier. In contrast, we here propose to employ AL as a corpus annotation method. A corpus built on these premises must, however, still be reusable in a flexible way so that, e.g., training with modified or improved classifiers is feasible and reasonable on AL-generated corpora. Baldridge and Osborne (2004) have already argued that this is a highly critical requirement because the examples selected by AL are tuned to one particular classifier. The second major contribution of this paper ad486 \x0cdresses this issue and provides empirical evidence that corpora built with one type of classifier (based on Maximum Entropy) can reasonably be reused by another, methodologically related type of classifier (based on Conditional Random Fields) without requiring changes of the corpus data. We also show that feature sets being used for training classifiers can be enhanced without invalidating corpus annota</context>
<context position="6207" citStr="Baldridge and Osborne (2004)" startWordPosition="970" endWordPosition="973">ittees classifiers constitute multiple views on the data by employing different feature subsets. The authors focus on (possible) negative side effects of AL on the annotations. They argue that AL annotations are cognitively more difficult to deal with for the annotators (because of the increased complexity of the selected sentences). Hence, lower annotation quality and higher per-sentence annotation times might be a concern. There are controversial findings on the reusability of data annotated by means of AL for the problem of parse tree selection. Whereas Hwa (2001) reports positive results, Baldridge and Osborne (2004) argue that AL based on uncertainty sampling may face serious performance degradation when labeled data is reused for training a classifier different from the one employed during AL. For committee-based AL, however, there is a lack of work on reusability. Our experiments of committee-based AL for entity recognition, however, reveal that for this task at least, reusability can be guaranteed to a very large extent. 3 AL for Corpus Annotation - Requirements for Practical Use AL frameworks for real-world corpus annotation should meet the following requirements: fast selection time cycles AL-based </context>
<context position="7981" citStr="Baldridge and Osborne, 2004" startWordPosition="1258" endWordPosition="1261">pt at an acceptable limit of a few minutes only. There are various AL strategies which although they yield theoretically near-optimal sample selection turn out to be actually impractible for real-world use because of excessively high computation times (cf. Cohn et al. (1996)). Thus, AL-based annotation should be based on a computationally tractable and task-wise feasible and acceptable selection strategy (even if this might imply a suboptimal reduction of annotation costs). reusability The examples AL selects for manual annotation are dependent on the model being used, up to a certain extent (Baldridge and Osborne, 2004). During annotation time, however, the best model might not be known and 487 \x0cmodel tuning (especially the choice of features) is typically performed once a training corpus is available. Hence, from a practical point of view, the resulting corpus should be reusable with modified classifiers as well. adaptive stopping criterion An explicit and adaptive stopping criterion which is sensitive towards the already achieved level of quality of the annotated corpus is clearly preferred over stopping after an a priori fixed number of annotation iterations. If these requirements, especially the first</context>
<context position="29434" citStr="Baldridge and Osborne (2004)" startWordPosition="4782" endWordPosition="4785">ects of AL for real annotation projects in Tomanek et al. (2007). 6 Discussion and Conclusions We have shown, for the annotation of (named) entities, that AL is well-suited to speed up annotation work under realistic conditions. In our simulations we yielded gains (in the number of tokens) up to 72%. We collected evidence that an average disagreement approaching zero may serve as an adaptive stopping criterion for AL-driven annotation and that a corpus compiled by means of QBC-based AL is to a large extent reusable by modified classifiers. These findings stand in contrast to those supplied by Baldridge and Osborne (2004) who focused on parse selection. Their research indicates that AL on selectors with different learning algorithms and feature sets then used by the tester can easily get worse than random selection. They conclude that it might not be be advisable to employ AL in environments where the final classifier is not very stable. Our evidence leads us to a re-assessment of AL493 \x0cbased annotations. First, we employed a committeebased (QBC) while Baldridge and Osborne performed uncertainty sampling AL. Committee-based approaches calculate the uncertainty on an example in a more implicit way, i.e., by</context>
</contexts>
<marker>Baldridge, Osborne, 2004</marker>
<rawString>Jason Baldridge and Miles Osborne. 2004. Active learning and the total cost of annotation. In Dekang Lin and Dekai Wu, editors, EMNLP 2004 Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 916. Barcelona, Spain, July 25-26, 2004. Association for Computational Linguistics. Adam L. Berger, Stephen A. Della Pietra, and Vincent J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Pietra, 1996</marker>
<rawString>Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):3971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT98 Proceedings of the 11th Annual Conference on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<publisher>ACM Press.</publisher>
<location>Madison, Wisconsin, USA,</location>
<contexts>
<context position="2460" citStr="Blum and Mitchell, 1998" startWordPosition="365" endWordPosition="368">age newspaper genre. Since the provision of annotations is a costly, labor-intensive and error-prone process the amount of work and time this activity requires should be minimized to the extent that corpus data could still be used to effectively train ML-based NLP components on them. The approach we advocate does exactly this and yields reduction gains (compared with standard procedures) ranging between 48% to 72%, without seriously sacrificing annotation quality. Various techniques to minimize the necessary amount of annotated training material have already been investigated. In co-training (Blum and Mitchell, 1998), e.g., from a small initial set of labeled data multiple learners mutually provide new training material for each other by labeling unseen examples. Pierce and Cardie (2001) have shown, however, that for tasks which require large numbers of labeled examples such as most NLP tasks cotraining might be inadequate because it tends to generate noisy data. Furthermore, a well compiled initial training set is a crucial prerequisite for successful co-training. As another alternative for minimizing annotation work, active learning (AL) is based on the idea to let the learner have control over the exam</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT98 Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 92 100. Madison, Wisconsin, USA, July 24-26, 1998. New York, NY: ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Cohn</author>
<author>Zoubin Ghahramani</author>
<author>Michael I Jordan</author>
</authors>
<title>Active learning with statistical models.</title>
<date>1996</date>
<journal>Journal of Artifical Intelligence Research,</journal>
<pages>4--129145</pages>
<contexts>
<context position="4564" citStr="Cohn et al., 1996" startWordPosition="702" endWordPosition="705">mum Entropy) can reasonably be reused by another, methodologically related type of classifier (based on Conditional Random Fields) without requiring changes of the corpus data. We also show that feature sets being used for training classifiers can be enhanced without invalidating corpus annotations generated on the basis of AL and, hence, with a poorer feature set. 2 Related Work There are mainly two methodological strands of AL research, viz. optimization approaches which aim at selecting those examples that optimize some (algorithm-dependent) objective function, such as prediction variance (Cohn et al., 1996), and heuristic methods with uncertainty sampling (Lewis and Catlett, 1994) and query-by-committee (QBC) (Seung et al., 1992) just to name the most prominent ones. AL has already been applied to several NLP tasks, such as document classification (Schohn and Cohn, 2000), POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, th</context>
<context position="7628" citStr="Cohn et al. (1996)" startWordPosition="1203" endWordPosition="1206">s) on all available annotations and then re-classifies all unseen corpus items. After that the most informative (i.e., deviant) b sentences from the set of newly classified data are selected for the next iteration round. In this approach the time needed to select the next examples (which is the idle time of the human annotators) has to be kept at an acceptable limit of a few minutes only. There are various AL strategies which although they yield theoretically near-optimal sample selection turn out to be actually impractible for real-world use because of excessively high computation times (cf. Cohn et al. (1996)). Thus, AL-based annotation should be based on a computationally tractable and task-wise feasible and acceptable selection strategy (even if this might imply a suboptimal reduction of annotation costs). reusability The examples AL selects for manual annotation are dependent on the model being used, up to a certain extent (Baldridge and Osborne, 2004). During annotation time, however, the best model might not be known and 487 \x0cmodel tuning (especially the choice of features) is typically performed once a training corpus is available. Hence, from a practical point of view, the resulting corp</context>
</contexts>
<marker>Cohn, Ghahramani, Jordan, 1996</marker>
<rawString>David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan. 1996. Active learning with statistical models. Journal of Artifical Intelligence Research, 4:129145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean Engelson</author>
<author>Ido Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from corpora.</title>
<date>1996</date>
<booktitle>In ACL96 Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>319326</pages>
<publisher>Morgan Kaufmann.</publisher>
<institution>University of California at Santa Cruz,</institution>
<location>California, U.S.A.,</location>
<contexts>
<context position="4873" citStr="Engelson and Dagan, 1996" startWordPosition="751" endWordPosition="754">nerated on the basis of AL and, hence, with a poorer feature set. 2 Related Work There are mainly two methodological strands of AL research, viz. optimization approaches which aim at selecting those examples that optimize some (algorithm-dependent) objective function, such as prediction variance (Cohn et al., 1996), and heuristic methods with uncertainty sampling (Lewis and Catlett, 1994) and query-by-committee (QBC) (Seung et al., 1992) just to name the most prominent ones. AL has already been applied to several NLP tasks, such as document classification (Schohn and Cohn, 2000), POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperplane of the currently learned SVM. It is assumed that an example which lies close to the hyperplane has high chances to have an effect on training. This approach is essentially limited to the SVM learning scheme as it solely relies on </context>
<context position="10185" citStr="Engelson and Dagan, 1996" startWordPosition="1608" endWordPosition="1611">tion task, we decided for QBC-based AL, a heuristic AL approach, which is computationally less complex and resource-greedy than objective function AL methods (the latter explicitly quantify the differences between the current and an ideal classifier in terms of some objective function). Accordingly, we ruled out uncertainty sampling, another heuristic AL approach, because it was shown before that QBC is more efficient and robust (Freund et al., 1997). QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996). A committee consists of a number of k classifiers of the same type (same learning algorithm, parameters, and features) but trained on different subsets of the training data. QBC-based AL is also iterative. In each AL round the committees k classifiers are trained on the already annotated data C, then a pool of unannotated data P is predicted with each classifier resulting in n automatically labeled versions of P. These are then compared according to their labels. Those with the highest variance are selected for manual annotation. 4.1 Selection Strategy In each iteration, a batch of b example</context>
<context position="13087" citStr="Engelson and Dagan (1996)" startWordPosition="2084" endWordPosition="2087">.), token transformation rule: capital letters replaced by A, lowercase letters by a, digits by 0, etc. (e.g., IL2 AA0, have aaaa) lexical and morphological prefix and suffix of length 3, stemmed version of each token syntactic the tokens part-of-speech tag contextual features of neighboring tokens Table 1: Features used for AL versity of a batch and representativeness of the respective example (to avoid outliers) (Shen et al., 2004). We experimented with these more sophisticated selection strategies but preliminary experiments did not reveal any significant improvement of the AL performance. Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results. Moreover, strategies incorporating more selection criteria often require more parameters to be set. However, proper parametrization is hard to achieve in real-world applications. Using disagreement exclusively for selection requires only one parameter, viz. the batch size b, to be specified. 4.2 Classifier and Features For our AL framework we decided to employ a Maximum Entropy (ME) classifier (Berger et al., 1996). We employ a rich set of features (see Table 1) which ar</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>Sean Engelson and Ido Dagan. 1996. Minimizing manual annotation cost in supervised training from corpora. In ACL96 Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 319326. University of California at Santa Cruz, California, U.S.A., 24-27 June 1996. San Francisco, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>H Sebastian Seung</author>
<author>Eli Shamir</author>
<author>Naftali Tishby</author>
</authors>
<title>Selective sampling using the query by committee algorithm.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>28--2</pages>
<contexts>
<context position="10014" citStr="Freund et al., 1997" startWordPosition="1579" endWordPosition="1582"> of AL has to provide the flexibility to modify the features of the final classifier. To meet the requirements from above under the constraints of a real-world annotation task, we decided for QBC-based AL, a heuristic AL approach, which is computationally less complex and resource-greedy than objective function AL methods (the latter explicitly quantify the differences between the current and an ideal classifier in terms of some objective function). Accordingly, we ruled out uncertainty sampling, another heuristic AL approach, because it was shown before that QBC is more efficient and robust (Freund et al., 1997). QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996). A committee consists of a number of k classifiers of the same type (same learning algorithm, parameters, and features) but trained on different subsets of the training data. QBC-based AL is also iterative. In each AL round the committees k classifiers are trained on the already annotated data C, then a pool of unannotated data P is predicted with each classifier resulting in n automatically labeled versions of P. These are </context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1997</marker>
<rawString>Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali Tishby. 1997. Selective sampling using the query by committee algorithm. Machine Learning, 28(2-3):133168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Beatrice Alex</author>
<author>Markus Becker</author>
</authors>
<date>2005</date>
<contexts>
<context position="5526" citStr="Hachey et al. (2005)" startWordPosition="859" endWordPosition="862">00), statistical parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperplane of the currently learned SVM. It is assumed that an example which lies close to the hyperplane has high chances to have an effect on training. This approach is essentially limited to the SVM learning scheme as it solely relies on SVM-internal selection criteria. Hachey et al. (2005) propose a committee-based AL approach where the committees classifiers constitute multiple views on the data by employing different feature subsets. The authors focus on (possible) negative side effects of AL on the annotations. They argue that AL annotations are cognitively more difficult to deal with for the annotators (because of the increased complexity of the selected sentences). Hence, lower annotation quality and higher per-sentence annotation times might be a concern. There are controversial findings on the reusability of data annotated by means of AL for the problem of parse tree sel</context>
</contexts>
<marker>Hachey, Alex, Becker, 2005</marker>
<rawString>Ben Hachey, Beatrice Alex, and Markus Becker. 2005.</rawString>
</citation>
<citation valid="true">
<title>Investigating the effects of selective sampling on the annotation task.</title>
<date>2005</date>
<booktitle>In CoNLL-2005 Proceedings of the 9th Conference on Computational Natural Language Learning,</booktitle>
<pages>144151</pages>
<location>Ann Arbor, MI, USA,</location>
<contexts>
<context position="5526" citStr="(2005)" startWordPosition="862" endWordPosition="862">al parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperplane of the currently learned SVM. It is assumed that an example which lies close to the hyperplane has high chances to have an effect on training. This approach is essentially limited to the SVM learning scheme as it solely relies on SVM-internal selection criteria. Hachey et al. (2005) propose a committee-based AL approach where the committees classifiers constitute multiple views on the data by employing different feature subsets. The authors focus on (possible) negative side effects of AL on the annotations. They argue that AL annotations are cognitively more difficult to deal with for the annotators (because of the increased complexity of the selected sentences). Hence, lower annotation quality and higher per-sentence annotation times might be a concern. There are controversial findings on the reusability of data annotated by means of AL for the problem of parse tree sel</context>
</contexts>
<marker>2005</marker>
<rawString>Investigating the effects of selective sampling on the annotation task. In CoNLL-2005 Proceedings of the 9th Conference on Computational Natural Language Learning, pages 144151. Ann Arbor, MI, USA, June 2005. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample selection for statistical grammar induction.</title>
<date>2000</date>
<booktitle>In EMNLP/VLC-2000 Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>4552</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="4965" citStr="Hwa, 2000" startWordPosition="767" endWordPosition="768">dological strands of AL research, viz. optimization approaches which aim at selecting those examples that optimize some (algorithm-dependent) objective function, such as prediction variance (Cohn et al., 1996), and heuristic methods with uncertainty sampling (Lewis and Catlett, 1994) and query-by-committee (QBC) (Seung et al., 1992) just to name the most prominent ones. AL has already been applied to several NLP tasks, such as document classification (Schohn and Cohn, 2000), POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperplane of the currently learned SVM. It is assumed that an example which lies close to the hyperplane has high chances to have an effect on training. This approach is essentially limited to the SVM learning scheme as it solely relies on SVM-internal selection criteria. Hachey et al. (2005) propose a committee-based AL approach </context>
</contexts>
<marker>Hwa, 2000</marker>
<rawString>Rebecca Hwa. 2000. Sample selection for statistical grammar induction. In EMNLP/VLC-2000 Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 4552. Hong Kong, China, October 7-8, 2000.. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>On minimizing training corpus for parser acquisition.</title>
<date>2001</date>
<booktitle>CoNLL-2001 Proceedings of the 5th Natural Language Learning Workshop.</booktitle>
<pages>6--7</pages>
<editor>In Walter Daelemans and Remi Zajac, editors,</editor>
<location>Toulouse, France,</location>
<contexts>
<context position="6152" citStr="Hwa (2001)" startWordPosition="964" endWordPosition="965">ttee-based AL approach where the committees classifiers constitute multiple views on the data by employing different feature subsets. The authors focus on (possible) negative side effects of AL on the annotations. They argue that AL annotations are cognitively more difficult to deal with for the annotators (because of the increased complexity of the selected sentences). Hence, lower annotation quality and higher per-sentence annotation times might be a concern. There are controversial findings on the reusability of data annotated by means of AL for the problem of parse tree selection. Whereas Hwa (2001) reports positive results, Baldridge and Osborne (2004) argue that AL based on uncertainty sampling may face serious performance degradation when labeled data is reused for training a classifier different from the one employed during AL. For committee-based AL, however, there is a lack of work on reusability. Our experiments of committee-based AL for entity recognition, however, reveal that for this task at least, reusability can be guaranteed to a very large extent. 3 AL for Corpus Annotation - Requirements for Practical Use AL frameworks for real-world corpus annotation should meet the follo</context>
</contexts>
<marker>Hwa, 2001</marker>
<rawString>Rebecca Hwa. 2001. On minimizing training corpus for parser acquisition. In Walter Daelemans and Remi Zajac, editors, CoNLL-2001 Proceedings of the 5th Natural Language Learning Workshop. Toulouse, France, 6-7 July 2001. Association for Computational Linguistics. John D. Lafferty, Andrew McCallum, and Fernando C. N.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML-2001 Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282289</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Williams College, MA, USA,</location>
<marker>Pereira, 2001</marker>
<rawString>Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML-2001 Proceedings of the 18th International Conference on Machine Learning, pages 282289. Williams College, MA, USA, June 28 - July 1, 2001. San Francisco, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Jason Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>ICML 94: Proceedings of the 11th International Conference on Machine Learning,</booktitle>
<pages>148156</pages>
<editor>In William W. Cohen and Haym Hirsh, editors,</editor>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, CA:</location>
<contexts>
<context position="4639" citStr="Lewis and Catlett, 1994" startWordPosition="713" endWordPosition="716">lated type of classifier (based on Conditional Random Fields) without requiring changes of the corpus data. We also show that feature sets being used for training classifiers can be enhanced without invalidating corpus annotations generated on the basis of AL and, hence, with a poorer feature set. 2 Related Work There are mainly two methodological strands of AL research, viz. optimization approaches which aim at selecting those examples that optimize some (algorithm-dependent) objective function, such as prediction variance (Cohn et al., 1996), and heuristic methods with uncertainty sampling (Lewis and Catlett, 1994) and query-by-committee (QBC) (Seung et al., 1992) just to name the most prominent ones. AL has already been applied to several NLP tasks, such as document classification (Schohn and Cohn, 2000), POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperpl</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>David D. Lewis and Jason Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In William W. Cohen and Haym Hirsh, editors, ICML 94: Proceedings of the 11th International Conference on Machine Learning, pages 148156. San Francisco, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>David Yarowsky</author>
</authors>
<title>Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In ACL00 Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>117125</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Hong Kong,</location>
<contexts>
<context position="4909" citStr="Ngai and Yarowsky, 2000" startWordPosition="756" endWordPosition="759">, with a poorer feature set. 2 Related Work There are mainly two methodological strands of AL research, viz. optimization approaches which aim at selecting those examples that optimize some (algorithm-dependent) objective function, such as prediction variance (Cohn et al., 1996), and heuristic methods with uncertainty sampling (Lewis and Catlett, 1994) and query-by-committee (QBC) (Seung et al., 1992) just to name the most prominent ones. AL has already been applied to several NLP tasks, such as document classification (Schohn and Cohn, 2000), POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperplane of the currently learned SVM. It is assumed that an example which lies close to the hyperplane has high chances to have an effect on training. This approach is essentially limited to the SVM learning scheme as it solely relies on SVM-internal selection criteria. Hac</context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>Grace Ngai and David Yarowsky. 2000. Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking. In ACL00 Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 117125. Hong Kong, China, 1-8 August 2000. San Francisco, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cDavid Pierce</author>
<author>Claire Cardie</author>
</authors>
<title>Limitations of co-training for natural language learning from large datasets.</title>
<date>2001</date>
<booktitle>EMNLP 2001 Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>19</pages>
<editor>In Lillian Lee and Donna Harman, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Pittsburgh, PA, USA,</location>
<contexts>
<context position="2634" citStr="Pierce and Cardie (2001)" startWordPosition="393" endWordPosition="396">mized to the extent that corpus data could still be used to effectively train ML-based NLP components on them. The approach we advocate does exactly this and yields reduction gains (compared with standard procedures) ranging between 48% to 72%, without seriously sacrificing annotation quality. Various techniques to minimize the necessary amount of annotated training material have already been investigated. In co-training (Blum and Mitchell, 1998), e.g., from a small initial set of labeled data multiple learners mutually provide new training material for each other by labeling unseen examples. Pierce and Cardie (2001) have shown, however, that for tasks which require large numbers of labeled examples such as most NLP tasks cotraining might be inadequate because it tends to generate noisy data. Furthermore, a well compiled initial training set is a crucial prerequisite for successful co-training. As another alternative for minimizing annotation work, active learning (AL) is based on the idea to let the learner have control over the examples to be manually labeled so as to optimize the prediction accuracy. Accordingly, AL aims at selecting those examples with high utility for the model. AL (as well as semi-s</context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>\x0cDavid Pierce and Claire Cardie. 2001. Limitations of co-training for natural language learning from large datasets. In Lillian Lee and Donna Harman, editors, EMNLP 2001 Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 19. Pittsburgh, PA, USA, June 3-4, 2001. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Schohn</author>
<author>David Cohn</author>
</authors>
<title>Less is more: Active learning with support vector machines.</title>
<date>2000</date>
<booktitle>In ICML 00: Proceedings of the 17th International Conference on Machine Learning,</booktitle>
<pages>839846</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, CA:</location>
<contexts>
<context position="4833" citStr="Schohn and Cohn, 2000" startWordPosition="745" endWordPosition="748">ut invalidating corpus annotations generated on the basis of AL and, hence, with a poorer feature set. 2 Related Work There are mainly two methodological strands of AL research, viz. optimization approaches which aim at selecting those examples that optimize some (algorithm-dependent) objective function, such as prediction variance (Cohn et al., 1996), and heuristic methods with uncertainty sampling (Lewis and Catlett, 1994) and query-by-committee (QBC) (Seung et al., 1992) just to name the most prominent ones. AL has already been applied to several NLP tasks, such as document classification (Schohn and Cohn, 2000), POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperplane of the currently learned SVM. It is assumed that an example which lies close to the hyperplane has high chances to have an effect on training. This approach is essentially limited to the SVM</context>
</contexts>
<marker>Schohn, Cohn, 2000</marker>
<rawString>Greg Schohn and David Cohn. 2000. Less is more: Active learning with support vector machines. In ICML 00: Proceedings of the 17th International Conference on Machine Learning, pages 839846. San Francisco, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sebastian Seung</author>
<author>Manfred Opper</author>
<author>Haim Sompolinsky</author>
</authors>
<title>Query by committee.</title>
<date>1992</date>
<booktitle>In COLT92 Proceedings of the 5th Annual Conference on Computational Learning Theory,</booktitle>
<pages>287294</pages>
<publisher>ACM Press.</publisher>
<location>Pittsburgh, PA, USA,</location>
<contexts>
<context position="4689" citStr="Seung et al., 1992" startWordPosition="720" endWordPosition="724">ields) without requiring changes of the corpus data. We also show that feature sets being used for training classifiers can be enhanced without invalidating corpus annotations generated on the basis of AL and, hence, with a poorer feature set. 2 Related Work There are mainly two methodological strands of AL research, viz. optimization approaches which aim at selecting those examples that optimize some (algorithm-dependent) objective function, such as prediction variance (Cohn et al., 1996), and heuristic methods with uncertainty sampling (Lewis and Catlett, 1994) and query-by-committee (QBC) (Seung et al., 1992) just to name the most prominent ones. AL has already been applied to several NLP tasks, such as document classification (Schohn and Cohn, 2000), POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperplane of the currently learned SVM. It is assumed th</context>
</contexts>
<marker>Seung, Opper, Sompolinsky, 1992</marker>
<rawString>H. Sebastian Seung, Manfred Opper, and Haim Sompolinsky. 1992. Query by committee. In COLT92 Proceedings of the 5th Annual Conference on Computational Learning Theory, pages 287294. Pittsburgh, PA, USA, July 27-29, 1992. New York, NY: ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
<author>Chew Lim Tan</author>
</authors>
<title>Multi-criteria-based active learning for named entity recognition.</title>
<date>2004</date>
<booktitle>In ACL04 Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>589596</pages>
<contexts>
<context position="5086" citStr="Shen et al. (2004)" startWordPosition="785" endWordPosition="788">e some (algorithm-dependent) objective function, such as prediction variance (Cohn et al., 1996), and heuristic methods with uncertainty sampling (Lewis and Catlett, 1994) and query-by-committee (QBC) (Seung et al., 1992) just to name the most prominent ones. AL has already been applied to several NLP tasks, such as document classification (Schohn and Cohn, 2000), POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperplane of the currently learned SVM. It is assumed that an example which lies close to the hyperplane has high chances to have an effect on training. This approach is essentially limited to the SVM learning scheme as it solely relies on SVM-internal selection criteria. Hachey et al. (2005) propose a committee-based AL approach where the committees classifiers constitute multiple views on the data by employing different feature subsets. The author</context>
<context position="12899" citStr="Shen et al., 2004" startWordPosition="2057" endWordPosition="2060">racts) are equally possible, with different implications for the AL process. 488 \x0cfeature class description orthographical based on regular expressions (e.g. HasDash, IsGreek, ...), token transformation rule: capital letters replaced by A, lowercase letters by a, digits by 0, etc. (e.g., IL2 AA0, have aaaa) lexical and morphological prefix and suffix of length 3, stemmed version of each token syntactic the tokens part-of-speech tag contextual features of neighboring tokens Table 1: Features used for AL versity of a batch and representativeness of the respective example (to avoid outliers) (Shen et al., 2004). We experimented with these more sophisticated selection strategies but preliminary experiments did not reveal any significant improvement of the AL performance. Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results. Moreover, strategies incorporating more selection criteria often require more parameters to be set. However, proper parametrization is hard to achieve in real-world applications. Using disagreement exclusively for selection requires only one parameter, viz. the batch size b, to be speci</context>
</contexts>
<marker>Shen, Zhang, Su, Zhou, Tan, 2004</marker>
<rawString>Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew Lim Tan. 2004. Multi-criteria-based active learning for named entity recognition. In ACL04 Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 589596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spain Barcelona</author>
</authors>
<date>2004</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, CA:</location>
<marker>Barcelona, 2004</marker>
<rawString>Barcelona, Spain, July 21-26, 2004. San Francisco, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Mary Elaine Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Active learning for natural language parsing and information extraction.</title>
<date>1999</date>
<booktitle>In ICML 99: Proceedings of the 16th International Conference on Machine Learning,</booktitle>
<pages>406414</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Bled, Slovenia,</location>
<contexts>
<context position="4953" citStr="Thompson et al., 1999" startWordPosition="763" endWordPosition="766">re are mainly two methodological strands of AL research, viz. optimization approaches which aim at selecting those examples that optimize some (algorithm-dependent) objective function, such as prediction variance (Cohn et al., 1996), and heuristic methods with uncertainty sampling (Lewis and Catlett, 1994) and query-by-committee (QBC) (Seung et al., 1992) just to name the most prominent ones. AL has already been applied to several NLP tasks, such as document classification (Schohn and Cohn, 2000), POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperplane of the currently learned SVM. It is assumed that an example which lies close to the hyperplane has high chances to have an effect on training. This approach is essentially limited to the SVM learning scheme as it solely relies on SVM-internal selection criteria. Hachey et al. (2005) propose a committee-based </context>
</contexts>
<marker>Thompson, Califf, Mooney, 1999</marker>
<rawString>Cynthia A. Thompson, Mary Elaine Califf, and Raymond J. Mooney. 1999. Active learning for natural language parsing and information extraction. In ICML 99: Proceedings of the 16th International Conference on Machine Learning, pages 406414. Bled, Slovenia, June 1999. San Francisco, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Walter Daelemans and</booktitle>
<pages>142147</pages>
<editor>Miles Osborne, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Edmonton, Canada,</location>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CONLL-2003 shared task: Language-independent named entity recognition. In Walter Daelemans and Miles Osborne, editors, CoNLL-2003 Proceedings of the 7th Conference on Computational Natural Language Learning, pages 142147. Edmonton, Canada, 2003. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Joachim Wermter</author>
<author>Udo Hahn</author>
</authors>
<title>Efficient annotation with the Jena ANnotation Environment (JANE).</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Linguistic Annotation Workshop A Merger of NLPXML 2007 and FLAC 2007.</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="28870" citStr="Tomanek et al. (2007)" startWordPosition="4689" endWordPosition="4692"> Density on AL and GS Annotations of Cytokine Receptors sentences the entity density in our AL corpus is almost 15 times higher than in our GS corpus. Such a dense corpus may be more appropriate for classifier training than a sparse one yielded by random or sequential annotations, which may just contain lots of negative training examples. We have observed comparable effects with other entity types, too, and thus conclude that the sparser entity mentions of a specific type are in texts, the more beneficial AL-based annotation is. We report on other aspects of AL for real annotation projects in Tomanek et al. (2007). 6 Discussion and Conclusions We have shown, for the annotation of (named) entities, that AL is well-suited to speed up annotation work under realistic conditions. In our simulations we yielded gains (in the number of tokens) up to 72%. We collected evidence that an average disagreement approaching zero may serve as an adaptive stopping criterion for AL-driven annotation and that a corpus compiled by means of QBC-based AL is to a large extent reusable by modified classifiers. These findings stand in contrast to those supplied by Baldridge and Osborne (2004) who focused on parse selection. The</context>
</contexts>
<marker>Tomanek, Wermter, Hahn, 2007</marker>
<rawString>Katrin Tomanek, Joachim Wermter, and Udo Hahn. 2007. Efficient annotation with the Jena ANnotation Environment (JANE). In Proceedings of the ACL 2007 Linguistic Annotation Workshop A Merger of NLPXML 2007 and FLAC 2007. Prague, Czech Republic, June 28-29, 2007. Association for Computational Linguistics (ACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>