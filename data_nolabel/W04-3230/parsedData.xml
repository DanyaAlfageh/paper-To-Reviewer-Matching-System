<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.899552">
b&apos;Applying Conditional Random Fields to Japanese Morphological Analysis
</title>
<author confidence="0.735589">
Taku Kudo
</author>
<affiliation confidence="0.7308295">
Kaoru Yamamoto Yuji Matsumoto
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.737309666666667">
8916-5, Takayama-Cho Ikoma, Nara, 630-0192 Japan
CREST JST, Tokyo Institute of Technology
4259, Nagatuta Midori-Ku Yokohama, 226-8503 Japan
</address>
<email confidence="0.895274">
taku-ku@is.naist.jp, kaoru@lr.pi.titech.ac.jp, matsu@is.naist.jp
</email>
<sectionHeader confidence="0.990068" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99977285">
This paper presents Japanese morphological analy-
sis based on conditional random fields (CRFs). Pre-
vious work in CRFs assumed that observation se-
quence (word) boundaries were fixed. However,
word boundaries are not clear in Japanese, and
hence a straightforward application of CRFs is not
possible. We show how CRFs can be applied to
situations where word boundary ambiguity exists.
CRFs offer a solution to the long-standing prob-
lems in corpus-based or statistical Japanese mor-
phological analysis. First, flexible feature designs
for hierarchical tagsets become possible. Second,
influences of label and length bias are minimized.
We experiment CRFs on the standard testbed corpus
used for Japanese morphological analysis, and eval-
uate our results using the same experimental dataset
as the HMMs and MEMMs previously reported in
this task. Our results confirm that CRFs not only
solve the long-standing problems but also improve
the performance over HMMs and MEMMs.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.97272087037037">
Conditional random fields (CRFs) (Lafferty et al.,
2001) applied to sequential labeling problems are
conditional models, trained to discriminate the cor-
rect sequence from all other candidate sequences
without making independence assumption for fea-
tures. They are considered to be the state-of-the-art
framework to date. Empirical successes with CRFs
have been reported recently in part-of-speech tag-
ging (Lafferty et al., 2001), shallow parsing (Sha
and Pereira, 2003), named entity recognition (Mc-
Callum and Li, 2003), Chinese word segmenta-
tion (Peng et al., 2004), and Information Extraction
(Pinto et al., 2003; Peng and McCallum, 2004).
Previous applications with CRFs assumed that
observation sequence (e.g. word) boundaries are
fixed, and the main focus was to predict label
At present, NTT Communication Science Laboratories,
2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan
taku@cslab.kecl.ntt.co.jp
sequence (e.g. part-of-speech). However, word
boundaries are not clear in non-segmented lan-
guages. One has to identify word segmentation as
well as to predict part-of-speech in morphological
analysis of non-segmented languages. In this pa-
per, we show how CRFs can be applied to situations
where word boundary ambiguity exists.
CRFs offer a solution to the problems in Japanese
morphological analysis with hidden Markov models
(HMMs) (e.g., (Asahara and Matsumoto, 2000)) or
with maximum entropy Markov models (MEMMs)
(e.g., (Uchimoto et al., 2001)). First, as HMMs are
generative, it is hard to employ overlapping fea-
tures stemmed from hierarchical tagsets and non-
independent features of the inputs such as surround-
ing words, word suffixes and character types. These
features have usually been ignored in HMMs, de-
spite their effectiveness in unknown word guessing.
Second, as mentioned in the literature, MEMMs
could evade neither from label bias (Lafferty et
al., 2001) nor from length bias (a bias occurring
because of word boundary ambiguity). Easy se-
quences with low entropy are likely to be selected
during decoding in MEMMs. The consequence is
serious especially in Japanese morphological anal-
ysis due to hierarchical tagsets as well as word
boundary ambiguity. The key advantage of CRFs is
their flexibility to include a variety of features while
avoiding these bias.
In what follows, we describe our motivations of
applying CRFs to Japanese morphological analysis
(Section 2). Then, CRFs and their parameter esti-
mation are provided (Section 3). Finally, we dis-
cuss experimental results (Section 4) and give con-
clusions with possible future directions (Section 5).
</bodyText>
<sectionHeader confidence="0.921151" genericHeader="method">
2 Japanese Morphological Analysis
</sectionHeader>
<subsectionHeader confidence="0.847093">
2.1 Word Boundary Ambiguity
</subsectionHeader>
<bodyText confidence="0.9820738">
Word boundary ambiguity cannot be ignored when
dealing with non-segmented languages. A simple
approach would be to let a character be a token
(i.e., character-based Begin/Inside tagging) so that
boundary ambiguity never occur (Peng et al., 2004).
</bodyText>
<figure confidence="0.97095625">
\x0cInput:
\x02\x01\x04\x03\x06\x05\x08\x07
\t (I live in Metropolis of Tokyo .)
BOS
\x0b (east)
[Noun]
\x0b
\x0c (Tokyo)
[Noun]
\x0c\x04\x0e (Kyoto)
[Noun]
\x0e (Metro.)
[Suffix]
\x0f (in)
[Particle]
\x0f (resemble)
[Verb]
\x10
\x11 (live)
[Verb]
EOS
Lattice:
\x0c (capital)
[Noun]
</figure>
<figureCaption confidence="0.999975">
Figure 1: Example of lattice for Japanese morphological analysis
</figureCaption>
<bodyText confidence="0.99539027027027">
However, B/I tagging is not a standard method in
20-year history of corpus-based Japanese morpho-
logical analysis. This is because B/I tagging cannot
directly reflect lexicons which contain prior knowl-
edge about word segmentation. We cannot ignore
a lexicon since over 90% accuracy can be achieved
even using the longest prefix matching with the lex-
icon. Moreover, B/I tagging produces a number
of redundant candidates which makes the decoding
speed slower.
Traditionally in Japanese morphological analysis,
we assume that a lexicon, which lists a pair of a
word and its corresponding part-of-speech, is avail-
able. The lexicon gives a tractable way to build a
lattice from an input sentence. A lattice represents
all candidate paths or all candidate sequences of to-
kens, where each token denotes a word with its part-
of-speech 1.
Figure 1 shows an example where a total of 6
candidate paths are encoded and the optimal path
is marked with bold type. As we see, the set of la-
bels to predict and the set of states in the lattice are
different, unlike English part-of-speech tagging that
word boundary ambiguity does not exist.
Formally, the task of Japanese morphological
analysis can be defined as follows. Let x be an
input, unsegmented sentence. Let y be a path, a
sequence of tokens where each token is a pair of
word wi and its part-of-speech ti. In other words,
y = (hw1, t1i, . . . , hw#y, t#yi) where #y is the
number of tokens in the path y. Let Y(x) be a set of
candidate paths in a lattice built from the input sen-
tence x and a lexicon. The goal is to select a correct
path y from all candidate paths in the Y(x). The
distinct property of Japanese morphological analy-
sis is that the number of tokens y varies, since the
set of labels and the set of states are not the same.
</bodyText>
<page confidence="0.935747">
1
</page>
<bodyText confidence="0.9977234">
If one cannot build a lattice because no matching word can
be found in the lexicon, unknown word processing is invoked.
Here, candidate tokens are built using character types, such as
hiragana, katakana, Chinese characters, alphabets, and num-
bers.
</bodyText>
<subsectionHeader confidence="0.989895">
2.2 Long-Standing Problems
</subsectionHeader>
<subsubsectionHeader confidence="0.598423">
2.2.1 Hierarchical Tagset
</subsubsectionHeader>
<bodyText confidence="0.99361075">
Japanese part-of-speech (POS) tagsets used in
the two major Japanese morphological analyzers
ChaSen2 and JUMAN3 take the form of a hierar-
chical structure. For example, IPA tagset4 used
in ChaSen consists of three categories: part-of-
speech, conjugation form (cform), and conjugate
type (ctype). The cform and ctype are assigned only
to words that conjugate, such as verbs and adjec-
tives. The part-of-speech has at most four levels of
subcategories. The top level has 15 different cate-
gories, such as Noun, Verb, etc. Noun is subdivided
into Common Noun, Proper Noun and so on. Proper
Noun is again subdivided into Person, Organization
or Place, etc. The bottom level can be thought as
the word level (base form) with which we can com-
pletely discriminate all words as different POS. If
we distinguish each branch of the hierarchical tree
as a different label (ignoring the word level), the to-
tal number amounts to about 500, which is much
larger than the typical English POS tagset such as
Penn Treebank.
The major effort has been devoted how to in-
terpolate each level of the hierarchical structure as
well as to exploit atomic features such as word suf-
fixes and character types. If we only use the bot-
tom level, we suffer from the data sparseness prob-
lem. On the other hand, if we use the top level,
we lack in granularity of POS to capture fine dif-
ferences. For instance, some suffixes (e.g., san or
kun) appear after names, and are helpful to detect
words with Name POS. In addition, the conjugation
form (cfrom) must be distinguished appearing only
in the succeeding position in a bi-gram, since it is
dominated by the word appearing in the next.
Asahara et al. extended HMMs so as to incorpo-
rate 1) position-wise grouping, 2) word-level statis-
</bodyText>
<page confidence="0.669601">
2
</page>
<footnote confidence="0.472295333333333">
http://chasen.naist.jp/
3
http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html
</footnote>
<page confidence="0.877308">
4
</page>
<bodyText confidence="0.909860666666667">
http://chasen.naist.jp/stable/ipadic/
\x0ctics, and 3) smoothing of word and POS level statis-
tics (Asahara and Matsumoto, 2000). However, the
proposed method failed to capture non-independent
features such as suffixes and character types and se-
lected smoothing parameters in an ad-hoc way.
</bodyText>
<subsubsectionHeader confidence="0.876765">
2.2.2 Label Bias and Length Bias
</subsubsectionHeader>
<bodyText confidence="0.9746484">
It is known that maximum entropy Markov mod-
els (MEMMs) (McCallum et al., 2000) or other dis-
criminative models with independently trained next-
state classifiers potentially suffer from the label bias
(Lafferty et al., 2001) and length bias. In Japanese
morphological analysis, they are extremely serious
problems. This is because, as shown in Figure 1,
the branching variance is considerably high, and
the number of tokens varies according to the output
path.
</bodyText>
<equation confidence="0.991269">
P(A, D  |x) = 0.6 * 0.6 * 1.0 = 0.36
P(B  |x) = 0.4 * 1.0 = 0.4
</equation>
<figure confidence="0.9671586875">
BOS A
B
D
C
E
0.6
0.4
1.0
1.0
1.0
1.0
0.4
0.6
EOS
P(A, D  |x) = 0.6 * 0.6 * 1.0 = 0.36
P(B, E  |x) = 0.4 * 1.0 * 1.0 = 0.4
(a) Label bias
BOS
B
D
C
0.4 1.0
1.0
1.0
0.4
EOS
(b) Length bias
P(A,D|x) &amp;lt; P(B,E|x)
P(A,D|x) &amp;lt; P(B |x)
A
0.6
0.6
</figure>
<figureCaption confidence="0.996829">
Figure 2: Label and length bias in a lattice
</figureCaption>
<bodyText confidence="0.99204225">
An example of the label bias is illus-
trated in Figure 2:(a) where the path is
searched by sequential combinations of
maximum entropy models (MEMMs), i.e.,
</bodyText>
<equation confidence="0.997247333333333">
P(y|x) =
Q#y
i=1 p(hwi, tii|hwi1, ti1i). Even
</equation>
<bodyText confidence="0.998862516129032">
if MEMMs learn the correct path A-D with in-
dependently trained maximum entropy models,
the path B-E will have a higher probability and
then be selected in decoding. This is because the
token B has only the single outgoing token E, and
the transition probability for B-E is always 1.0.
Generally speaking, the complexities of transitions
vary according to the tokens, and the transition
probabilities with low-entropy will be estimated
high in decoding. This problem occurs because the
training is performed only using the correct path,
ignoring all other transitions.
Moreover, we cannot ignore the influence of the
length bias either. By the length bias, we mean that
short paths, consisting of a small number of tokens,
are preferred to long path. Even if the transition
probability of each token is small, the total proba-
bility of the path will be amplified when the path is
short 2:(b)). Length bias occurs in Japanese mor-
phological analysis because the number of output
tokens y varies by use of prior lexicons.
Uchimoto et al. attempted a variant of MEMMs
for Japanese morphological analysis with a number
of features including suffixes and character types
(Uchimoto et al., 2001; Uchimoto et al., 2002;
Uchimoto et al., 2003). Although the performance
of unknown words were improved, that of known
words degraded due to the label and length bias.
Wrong segmentation had been reported in sentences
which are analyzed correctly by naive rule-based or
HMMs-based analyzers.
</bodyText>
<sectionHeader confidence="0.982102" genericHeader="method">
3 Conditional Random Fields
</sectionHeader>
<bodyText confidence="0.9997603">
Conditional random fields (CRFs) (Lafferty et al.,
2001) overcome the problems described in Sec-
tion 2.2. CRFs are discriminative models and can
thus capture many correlated features of the inputs.
This allows flexible feature designs for hierarchical
tagsets. CRFs have a single exponential model for
the joint probability of the entire paths given the in-
put sentence, while MEMMs consist of a sequential
combination of exponential models, each of which
estimates a conditional probability of next tokens
given the current state. This minimizes the influ-
ences of the label and length bias.
As explained in Section 2.1, there is word bound-
ary ambiguity in Japanese, and we choose to use
a lattice instead of B/I tagging. This implies that
the set of labels and the set of states are differ-
ent, and the number of tokens #y varies accord-
ing to a path. In order to accomodate this, we de-
fine CRFs for Japanese morphological analysis as
the conditional probability of an output path y =
</bodyText>
<equation confidence="0.988942142857143">
(hw1, t1i, . . . , hw#y, t#yi) given an input sequence
x:
P(y|x) =
1
Zx
exp
\x10#y
X
i=1
X
k
kfk(hwi1, ti1i, hwi, tii)
\x11
,
</equation>
<bodyText confidence="0.494664">
where Zx is a normalization factor over all candi-
date paths, i.e.,
</bodyText>
<equation confidence="0.9977833125">
Zx =
X
y0Y(x)
exp
\x10#y0
X
i=1
X
k
kfk(hw0
i1, t0
i1i, hw0
i, t0
ii)
\x11
,
</equation>
<bodyText confidence="0.980414076923077">
\x0cfk(hwi1, ti1i, hwi, tii) is an arbitrary feature func-
tion over i-th token hwi, tii, and its previous token
hwi1, ti1i 5. k( = {1, . . . , K} RK) is a
learned weight or parameter associated with feature
function fk.
Note that our formulation of CRFs is different
from the widely-used formulations (e.g., (Sha and
Pereira, 2003; McCallum and Li, 2003; Peng et
al., 2004; Pinto et al., 2003; Peng and McCallum,
2004)). The previous applications of CRFs assign
a conditional probability for a label sequence y =
y1, . . . , yT given an input sequence x = x1, . . . , xT
as:
</bodyText>
<equation confidence="0.994610181818182">
P(y|x) =
1
Zx
exp
\x10 T
X
i=1
X
k
kfk(yi1, yi, x)
\x11
</equation>
<bodyText confidence="0.995964">
In our formulation, CRFs deal with word boundary
ambiguity. Thus, the the size of output sequence T
is not fixed through all candidates y Y(x). The
index i is not tied with the input x as in the original
CRFs, but unique to the output y Y(x).
Here, we introduce the global feature vec-
</bodyText>
<equation confidence="0.91886025">
tor F(y, x) = {F1(y, x), . . . , FK(y, x)}, where
Fk(y, x) =
P#y
i=1 fk(hwi1, ti1i, hwi, tii). Using
</equation>
<bodyText confidence="0.768235666666667">
the global feature vector, P(y|x) can also be rep-
resented as P(y|x) = 1
Zx
exp( F(y, x)). The
most probable path y for the input sentence x is then
given by
</bodyText>
<equation confidence="0.9836944">
y = argmax
yY(x)
P(y|x) = argmax
yY(x)
F(y, x),
</equation>
<bodyText confidence="0.98378625">
which can be found with the Viterbi algorithm.
An interesting note is that the decoding process of
CRFs can be reduced into a simple linear combina-
tions over all global features.
</bodyText>
<subsectionHeader confidence="0.999328">
3.1 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.951769666666667">
CRFs are trained using the standard maximum
likelihood estimation, i.e., maximizing the log-
likelihood L of a given training set T =
</bodyText>
<equation confidence="0.995769384615385">
{hxj, yji}N
j=1,
= argmax
RK
L, where
L =
X
j
log(P(yj|xj))
=
X
j
h
log
\x10 X
yY(xj )
exp [F(yj, xj) F(y, xj)]
\x01\x11i
=
X
j
h
F(yj, xj) log(Zxj
)
i
.
</equation>
<page confidence="0.638296">
5
</page>
<bodyText confidence="0.963233214285714">
We could use trigram or more general n-gram feature func-
tions (e.g., fk(hwi
n, ti
ni, . . . , hwi, tii)), however we restrict
ourselves to bi-gram features for clarity.
To maximize L, we have to maximize the dif-
ference between the inner product (or score) of the
correct path F(yj, xj) and those of all other
candidates F(y, xj), y Y(xj). CRFs is
thus trained to discriminate the correct path from
all other candidates, which reduces the influences
of the label and length bias in encoding.
At the optimal point, the first-derivative of the
log-likelihood becomes 0, thus,
</bodyText>
<equation confidence="0.9160871875">
L
k
=
X
j
\x10
Fk(yj, xj) EP (y|xj )
\x02
Fk(y, xj)
\x03\x11
= Ok Ek = 0,
where Ok =
P
j Fk(yj, xj) is the count of fea-
ture k observed in the training data T, and Ek =
P
</equation>
<bodyText confidence="0.98859775">
j EP(y|xj)[Fk(y, xj)] is the expectation of fea-
ture k over the model distribution P(y|x) and T.
The expectation can efficiently be calculated using
a variant of the forward-backward algorithm.
</bodyText>
<equation confidence="0.9878666">
EP(y|x)[Fk(y, x)] =
X
{hw0,t0i,hw,ti}B(x)
hw0,t0i f
k exp(
P
k0 k0 f
k0 ) hw,ti
Zx
,
</equation>
<bodyText confidence="0.947586833333333">
where f
k is an abbreviation for fk(hw0, t0i, hw, ti),
B(x) is a set of all bi-gram sequences observed
in the lattice for x, and hw,ti and hw,ti are the
forward-backward costs given by the following re-
cursive definitions:
</bodyText>
<equation confidence="0.998983523809524">
hw,ti =
X
hw0,t0iLT (hw,ti)
hw0,t0i exp
X
k
kfk(hw0
, t0
i, hw, ti)
\x01
hw,ti =
X
hw0,t0iRT (hw,ti)
hw0,t0i exp
X
k
kfk(hw, ti, hw0
, t0
i)
\x01
,
</equation>
<bodyText confidence="0.9485225">
where LT(hw, ti) and RT(hw, ti) denote a set of
tokens each of which connects to the token hw, ti
from the left and the right respectively. Note that
initial costs of two virtual tokens, hwbos,tbosi and
hweos,teosi, are set to be 1. A normalization constant
is then given by Zx = hweos,teosi(= hwbos,tbosi).
We attempt two types of regularizations in order
to avoid overfitting. They are a Gaussian prior (L2-
norm) (Chen and Rosenfeld, 1999) and a Laplacian
prior (L1-norm) (Goodman, 2004; Peng and Mc-
</bodyText>
<equation confidence="0.990953333333333">
Callum, 2004)
L = C
X
j
log(P(yj|xj))
1
2
\x1a P
k |k |(L1-norm)
P
k |k|2
(L2-norm)
</equation>
<bodyText confidence="0.999316333333333">
\x0cBelow, we refer to CRFs with L1-norm and L2-
norm regularization as L1-CRFs and L2-CRFs re-
spectively. The parameter C R+ is a hyperpa-
rameter of CRFs determined by a cross validation.
L1-CRFs can be reformulated into the con-
strained optimization problem below by letting
</bodyText>
<equation confidence="0.965371130434783">
k = +
k
k :
max : C
X
j
log(P(yj|xj))
X
k
(+
k +
k )/2
s.t., +
k 0,
k 0.
At the optimal point, the following Karush-Kuhun-
Tucker conditions satisfy: +
k [C (Ok Ek)
1/2] = 0,
k [C (Ek Ok) 1/2] = 0, and
|C (Ok Ek) |1/2. These conditions mean
that both +
k and
</equation>
<construct confidence="0.36209975">
k are set to be 0 (i.e., k = 0),
when |C (Ok Ek) |&amp;lt; 1/2. A non-zero weight
is assigned to k, only when |C (Ok Ek) |=
1/2. L2-CRFs, in contrast, give the optimal solution
</construct>
<equation confidence="0.883362333333333">
when L
k
= C (Ok Ek)k = 0. Omitting the
</equation>
<bodyText confidence="0.982873625">
proof, (Ok Ek) 6= 0 can be shown and L2-CRFs
thus give a non-sparse solution where all k have
non-zero weights.
The relationship between two reguralizations
have been studied in Machine Learning community.
(Perkins et al., 2003) reported that L1-regularizer
should be chosen for a problem where most of given
features are irrelevant. On the other hand, L2-
regularizer should be chosen when most of given
features are relevant. An advantage of L1-based
regularizer is that it often leads to sparse solutions
where most of k are exactly 0. The features as-
signed zero weight are thought as irrelevant fea-
tures to classifications. The L2-based regularizer,
also seen in SVMs, produces a non-sparse solution
where all of k have non-zero weights. All features
are used with L2-CRFs.
The optimal solutions of L2-CRFs can be ob-
tained by using traditional iterative scaling algo-
rithms (e.g., IIS or GIS (Pietra et al., 1997)) or more
efficient quasi-Newton methods (e.g., L-BFGS (Liu
and Nocedal, 1989)). For L1-CRFs, constrained op-
timizers (e.g., L-BFGS-B (Byrd et al., 1995)) can be
used.
</bodyText>
<sectionHeader confidence="0.987391" genericHeader="method">
4 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.985015">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999463333333333">
We use two widely-used Japanese annotated cor-
pora in the research community, Kyoto Univer-
sity Corpus ver 2.0 (KC) and RWCP Text Corpus
(RWCP), for our experiments on CRFs. Note that
each corpus has a different POS tagset and details
(e.g., size of training and test dataset) are summa-
rized in Table 1.
One of the advantages of CRFs is that they are
flexible enough to capture many correlated fea-
tures, including overlapping and non-independent
features. We thus use as many features as possi-
ble, which could not be used in HMMs. Table 2
summarizes the set of feature templates used in the
KC data. The templates for RWCP are essentially
the same as those of KC except for the maximum
level of POS subcatgeories. Word-level templates
are employed when the words are lexicalized, i.e.,
those that belong to particle, auxiliary verb, or suf-
fix6. For an unknown word, length of the word, up
to 2 suffixes/prefixes and character types are used
as the features. We use all features observed in the
lattice without any cut-off thresholds. Table 1 also
includes the number of features in both data sets.
We evaluate performance with the standard F-
</bodyText>
<equation confidence="0.628414">
score (F=1) defined as follows:
F=1 =
</equation>
<table confidence="0.8533328">
2 Recall Precision
Recall + Precision
,
where Recall =
# of correct tokens
# of tokens in test corpus
Precision =
# of correct tokens
# of tokens in system output
.
</table>
<bodyText confidence="0.9923801">
In the evaluations of F-scores, three criteria of cor-
rectness are used: seg: (only the word segmentation
is evaluated), top: (word segmentation and the top
level of POS are evaluated), and all: (all informa-
tion is used for evaluation).
The hyperparameters C for L1-CRFs and L2-
CRFs are selected by cross-validation. Experiments
are implemented in C++ and executed on Linux
with XEON 2.8 GHz dual processors and 4.0 Gbyte
of main memory.
</bodyText>
<sectionHeader confidence="0.621289" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.992559166666667">
Tables 3 and 4 show experimental results using
KC and RWCP respectively. The three F-scores
(seg/top/all) for our CRFs and a baseline bi-gram
HMMs are listed.
In Table 3 (KC data set), the results of a variant
of maximum entropy Markov models (MEMMs)
(Uchimoto et al., 2001) and a rule-based analyzer
(JUMAN7) are also shown. To make a fare compar-
ison, we use exactly the same data as (Uchimoto et
al., 2001).
In Table 4 (RWCP data set), the result of an ex-
tended Hidden Markov Models (E-HMMs) (Asa-
</bodyText>
<page confidence="0.990691">
6
</page>
<bodyText confidence="0.9916425">
These lexicalizations are usually employed in Japanese
morphological analysis.
</bodyText>
<page confidence="0.992823">
7
</page>
<bodyText confidence="0.618901666666667">
JUMAN assigns unknown POS to the words not seen in
the lexicon. We simply replace the POS of these words with
the default POS, Noun-SAHEN.
</bodyText>
<tableCaption confidence="0.356023">
\x0cTable 1: Details of Data Set
</tableCaption>
<table confidence="0.949064">
KC RWCP
source Mainich News Article (95) Mainich News Article (94)
lexicon (# of words) JUMAN ver. 3.61 (1,983,173) IPADIC ver. 2.7.0 (379,010)
POS structure 2-levels POS, cfrom, ctype, base form 4-levels POS, cfrom, ctype, base form
# of training sentences 7,958 (Articles on Jan. 1st - Jan. 8th) 10,000 (first 10,000 sentences)
# of training tokens 198,514 265,631
# of test sentences 1,246 (Articles on Jan. 9th) 25,743 (all remaining sentences)
# of test tokens 31,302 655,710
# of features 791,798 580,032
</table>
<tableCaption confidence="0.985063">
Table 2: Feature templates: fk(hw0, t0i, hw, ti)
</tableCaption>
<equation confidence="0.712368">
t0 = hp10, p20, cf0, ct, bw0i, t = hp1, p2, cf, ct, bwi, where p10/p1
</equation>
<bodyText confidence="0.971044666666667">
and p20/p2 are the top and sub categories of POS. cf0/cf and ct0/ct
are the cfrom and ctype respectively. bw0/bw are the base form of the
words w0/w.
</bodyText>
<table confidence="0.550595307692308">
type template
Unigram hp1i
basic features hp1, p2i
w is known hbwi
hbw, p1i
hbw, p1, p2i
w is unknown length of the word w
up to 2 suffixes {, hp1i, hp1, p2i}
up to 2 prefixes {, hp1i, hp1, p2i}
character type {, hp1i, hp1, p2i}
Bigram hp10
, p1i
basic features hp10
</table>
<equation confidence="0.966638666666667">
, p1, p2i
hp10
, p20
, p1i
hp10
, p20
, p1, p2i
hp10
, p20
, cf0
, p1, p2i
hp10
, p20
, ct0
, p1, p2i
hp10
, p20
, cf0
, ct0
, p1, p2i
hp10
, p20
, p1, p2, cfi
hp10
, p20
, p1, p2, cti
hp10
, p20
, p1, p2, cf, cti
hp10
, p20
, cf0
, p1, p2, cfi
hp10
, p20
, ct, p1, p2, cti
hp10
, p20
, cf0
, p1, p2, cti
hp10
, p20
, ct0
, p1, p2, cfi
hp10
, p20
, cf0
, ct0
, p1, p2, cf, cti
w0
is lexicalized hp10
, p20
, cf0
, ct0
, bw0
, p1, p2i
hp10
, p20
, cf0
, ct0
, bw0
, p1, p2, cfi
hp10
, p20
, cf0
, ct0
, bw0
, p1, p2, cti
hp10
, p20
, cf0
, ct0
, bw0
, p1, p2, cf, cti
w is lexicalized hp10
, p20
, p1, p2, cf, ct, bwi
hp10
, p20
, cf0
, p1, p2, cf, ct, bwi
hp10
, p20
, ct0
, p1, p2, cf, ct, bwi
hp10
, p20
, cf0
, ct0
, p1, p2, cf, ct, bwi
w0/w are lexicalized hp10
, p20
, cf0
, ct0
, bw0
, p1, p2, cf, ct, bwi
</equation>
<bodyText confidence="0.999612933333333">
hara and Matsumoto, 2000) trained and tested with
the same corpus is also shown. E-HMMs is applied
to the current implementation of ChaSen. Details of
E-HMMs are described in Section 4.3.2.
We directly evaluated the difference of these sys-
tems using McNemars test. Since there are no
standard methods to evaluate the significance of F
scores, we convert the outputs into the character-
based B/I labels and then employ a McNemars
paired test on the labeling disagreements. This eval-
uation was also used in (Sha and Pereira, 2003). The
results of McNemars test suggest that L2-CRFs is
significantly better than other systems including L1-
CRFs8. The overall results support our empirical
success of morphological analysis based on CRFs.
</bodyText>
<subsectionHeader confidence="0.996547">
4.3 Discussion
</subsectionHeader>
<subsubsectionHeader confidence="0.452989">
4.3.1 CRFs and MEMMs
</subsubsectionHeader>
<bodyText confidence="0.999462096774194">
Uchimoto el al. proposed a variant of MEMMs
trained with a number of features (Uchimoto et al.,
2001). Although they improved the accuracy for un-
known words, they fail to segment some sentences
which are correctly segmented with HMMs or rule-
based analyzers.
Figure 3 illustrates the sentences which are incor-
rectly segmented by Uchimotos MEMMs. The cor-
rect paths are indicated by bold boxes. Uchimoto et
al. concluded that these errors were caused by non-
standard entries in the lexicon. In Figure 3,
(romanticist) and (ones heart) are
unusual spellings and they are normally written as
and respectively. However, we
conjecture that these errors are caused by the influ-
ence of the length bias. To support our claim, these
sentences are correctly segmented by CRFs, HMMs
and rule-based analyzers using the same lexicon as
(Uchimoto et al., 2001). By the length bias, short
paths are preferred to long paths. Thus, single to-
ken or is likely to be selected
compared to multiple tokens / or
/ . Moreover, and have
exactly the same POS (Noun), and transition proba-
bilities of these tokens become almost equal. Con-
sequentially, there is no choice but to select a short
path (single token) in order to maximize the whole
sentence probability.
Table 5 summarizes the number of errors in
HMMs, CRFs and MEMMs, using the KC data set.
Two types of errors, l-error and s-error, are given in
</bodyText>
<page confidence="0.968917">
8
</page>
<bodyText confidence="0.7164435">
In all cases, the p-values are less than 1.0 104
.
</bodyText>
<tableCaption confidence="0.647662">
\x0cTable 3: Results of KC, (F=1 (precision/recall))
system seg top all
</tableCaption>
<table confidence="0.9947176">
L2-CRFs (C =1.2) 98.96 (99.04/98.88) 98.31 (98.39/98.22) 96.75 (96.83/96.67)
L1-CRFs (C =3.0) 98.80 (98.84/98.77) 98.14 (98.18/98.11) 96.55 (96.58/96.51)
MEMMs (Uchimoto 01) 96.44 (95.78/97.10) 95.81 (95.15/96.47) 94.27 (93.62/94.92)
JUMAN (rule-based) 98.70 (98.88/98.51) 98.09 (98.27/97.91) 93.73 (93.91/93.56)
HMMs-bigram (baseline) 96.22 (96.16/96.28) 94.96 (94.90/95.02) 91.85 (91.79/91.90)
</table>
<tableCaption confidence="0.826841">
Table 4: Results of RWCP, (F=1 (precision/recall))
system seg top all
</tableCaption>
<table confidence="0.95553225">
L2-CRFs (C =2.4) 99.11 (99.03/99.20) 98.73 (98.65/98.81) 97.66 (97.58/97.75)
L1-CRFs (C =3.0) 99.00 (98.86/99.13) 98.58 (98.44/98.72) 97.30 (97.16/97.43)
E-HMMs (Asahara 00) 98.87 (98.77/98.97) 98.33 (98.23/98.43) 96.95 (96.85/97.04)
HMMs-bigram (baseline) 98.82 (98.69/98.94) 98.10 (97.97/98.22) 95.90 (95.78/96.03)
</table>
<figure confidence="0.98445625925926">
sea
\x01
particle
\x02\x04\x03\x06\x05
bet
\x07\t\x08\x06
\x0c\x0b
romanticist
\x07\t\x08
romance
\x0b
particle
The romance on the sea they bet is ...
\x0e\x10\x0f
rough waves
\x01
particle
\x11 \x03
lose \x12\x14\x13
not
\x15
heart
\x12\x16\x13 \x15
ones heart
A heart which beats rough waves is ...
MEMMs select
MEMMs select
</figure>
<figureCaption confidence="0.6984295">
Figure 3: Errors with MEMMs
(Correct paths are marked with bold boxes.)
</figureCaption>
<tableCaption confidence="0.972888">
Table 5: Number of errors in KC dataset
</tableCaption>
<table confidence="0.9908595">
# of l-errors # of s-errors
CRFs 79 (40%) 120 (60%)
HMMs 306 (44%) 387 (56%)
MEMMs 416 (70%) 183 (30%)
</table>
<bodyText confidence="0.996532375">
l-error: output longer token than correct one
s-error: output shorter token than correct one
this table. l-error (or s-error) means that a system
incorrectly outputs a longer (or shorter) token than
the correct token respectively. By length bias, long
tokens are preferred to short tokens. Thus, larger
number of l-errors implies that the result is highly
influenced by the length bias.
While the relative rates of l-error and s-error are
almost the same in HMMs and CRFs, the number
of l-errors with MEMMs amounts to 416, which
is 70% of total errors, and is even larger than that
of naive HMMs (306). This result supports our
claim that MEMMs is not sufficient to be applied to
Japanese morphological analysis where the length
bias is inevitable.
</bodyText>
<subsubsectionHeader confidence="0.942342">
4.3.2 CRFs and Extended-HMMs
</subsubsectionHeader>
<bodyText confidence="0.99957719047619">
Asahara et al. extended the original HMMs by 1)
position-wise grouping of POS tags, 2) word-level
statistics, and 3) smoothing of word and POS level
statistics (Asahara and Matsumoto, 2000). All of
these techniques are designed to capture hierarchi-
cal structures of POS tagsets. For instance, in the
position-wise grouping, optimal levels of POS hier-
archies are changed according to the contexts. Best
hierarchies for each context are selected by hand-
crafted rules or automatic error-driven procedures.
CRFs can realize such extensions naturally and
straightforwardly. In CRFs, position-wise grouping
and word-POS smoothing are simply integrated into
a design of feature functions. Parameters k for
each feature are automatically configured by gen-
eral maximum likelihood estimation. As shown in
Table 2, we can employ a number of templates to
capture POS hierarchies. Furthermore, some over-
lapping features (e.g., forms and types of conjuga-
tion) can be used, which was not possible in the ex-
tended HMMs.
</bodyText>
<subsubsectionHeader confidence="0.522992">
4.3.3 L1-CRFs and L2-CRFs
</subsubsectionHeader>
<bodyText confidence="0.998088833333333">
L2-CRFs perform slightly better than L1-CRFs,
which indicates that most of given features
(i.e., overlapping features, POS hierarchies, suf-
fixes/prefixes and character types) are relevant to
both of two datasets. The numbers of active (non-
zero) features used in L1-CRFs are much smaller
(about 1/8 - 1/6) than those in L2-CRFs: (L2-
CRFs: 791,798 (KC) / 580,032 (RWCP) v.s., L1-
CRFs: 90,163 (KC) / 101,757 (RWCP)). L1-CRFs
are worth being examined if there are some practi-
cal constraints (e.g., limits of memory, disk or CPU
resources).
</bodyText>
<sectionHeader confidence="0.77579" genericHeader="conclusions">
\x0c5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999845730769231">
In this paper, we present how conditional random
fields can be applied to Japanese morphological
analysis in which word boundary ambiguity exists.
By virtue of CRFs, 1) a number of correlated fea-
tures for hierarchical tagsets can be incorporated
which was not possible in HMMs, and 2) influences
of label and length bias are minimized which caused
errors in MEMMs. We compare results between
CRFs, MEMMs and HMMs in two Japanese anno-
tated corpora, and CRFs outperform the other ap-
proaches. Although we discuss Japanese morpho-
logical analysis, the proposed approach can be ap-
plicable to other non-segmented languages such as
Chinese or Thai.
There exist some phenomena which cannot be an-
alyzed only with bi-gram features in Japanese mor-
phological analysis. To improve accuracy, tri-gram
or more general n-gram features would be useful.
CRFs have capability of handling such features.
However, the numbers of features and nodes in the
lattice increase exponentially as longer contexts are
captured. To deal with longer contexts, we need a
practical feature selection which effectively trades
between accuracy and efficiency. For this challenge,
McCallum proposes an interesting research avenue
to explore (McCallum, 2003).
</bodyText>
<sectionHeader confidence="0.978357" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.986416666666667">
We would like to thank Kiyotaka Uchimoto and
Masayuki Asahara, who explained the details of
their Japanese morphological analyzers.
</bodyText>
<sectionHeader confidence="0.982911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996823287671233">
Masayuki Asahara and Yuji Matsumoto. 2000. Ex-
tended models and tools for high-performance
part-of-speech tagger. In Proc of COLING, pages
2127.
Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and
Ci You Zhu. 1995. A limited memory algorithm
for bound constrained optimization. SIAM Jour-
nal on Scientific Computing, 16(6):11901208.
Stanley F. Chen and Ronald. Rosenfeld. 1999. A
gaussian prior for smoothing maximum entropy
models. Technical report, Carnegie Mellon Uni-
versity.
Joshua Goodman. 2004. Exponential priors
for maximum entropy models. In Proc. of
HLT/NAACL.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. of ICML, pages 282289.
Dong C. Liu and Jorge Nocedal. 1989. On the
limited memory BFGS method for large scale
optimization. Math. Programming, 45(3, (Ser.
B)):503528.
Andrew McCallum and Wei Li. 2003. Early re-
sults for named entity recognition with condi-
tional random fields, feature induction and web-
enhanced lexicons. In In Proc. of CoNLL.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov mod-
els for information and segmentation. In Proc. of
ICML, pages 591598.
Andrew McCallum. 2003. Efficiently inducing fea-
tures of conditional random fields. In Nineteenth
Conference on Uncertainty in Artificial Intelli-
gence (UAI03).
Fuchun Peng and Andrew McCallum. 2004. Accu-
rate information extraction from research papers.
In Proc. of HLT/NAACL.
Fuchun Peng, Fangfang Feng, and Andrew McCal-
lum. 2004. Chinese segmentation and new word
detection using conditional random fields (to ap-
pear). In Proc. of COLING.
Simon Perkins, Kevin Lacker, and James Thiler.
2003. Grafting: Fast, incremental feature selec-
tion by gradient descent in function space. JMLR,
3:13331356.
Della Pietra, Stephen, Vincent J. Della Pietra, and
John D. Lafferty. 1997. Inducing features of ran-
dom fields. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 19(4):380393.
David Pinto, Andrew McCallum, Xing Wei, and
W. Bruce Croft. 2003. Table extraction using
conditional random fields. In In Proc. of SIGIR,
pages 235242.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proc. of
HLT-NAACL, pages 213220.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi
Isahara. 2001. The unknown word problem: a
morphological analysis of Japanese using maxi-
mum entropy aided by a dictionary. In Proc. of
EMNLP, pages 9199.
Kiyotaka Uchimoto, Chikashi Nobata, Atsushi
Yamada, Satoshi Sekine, and Hitoshi Isahara.
2002. Morphological analysis of the spontaneous
speech corpus. In Proc of COLING, pages 1298
1302.
Kiyotaka Uchimoto, Chikashi Nobata, Atsushi Ya-
mada, and Hitoshi Isahara Satoshi Sekine. 2003.
Morphological analysis of a large spontaneous
speech corpus in Japanese. In Proc. of ACL,
pages 479488.
\x0c&apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.525289">
<title confidence="0.999599">b&apos;Applying Conditional Random Fields to Japanese Morphological Analysis</title>
<author confidence="0.990548">Taku Kudo</author>
<affiliation confidence="0.77611">Kaoru Yamamoto Yuji Matsumoto Nara Institute of Science and Technology</affiliation>
<address confidence="0.98871">8916-5, Takayama-Cho Ikoma, Nara, 630-0192 Japan</address>
<affiliation confidence="0.995224">CREST JST, Tokyo Institute of Technology</affiliation>
<address confidence="0.998024">4259, Nagatuta Midori-Ku Yokohama, 226-8503 Japan</address>
<email confidence="0.9935">taku-ku@is.naist.jp,kaoru@lr.pi.titech.ac.jp,matsu@is.naist.jp</email>
<abstract confidence="0.998893095238095">This paper presents Japanese morphological analysis based on conditional random fields (CRFs). Previous work in CRFs assumed that observation sequence (word) boundaries were fixed. However, word boundaries are not clear in Japanese, and hence a straightforward application of CRFs is not possible. We show how CRFs can be applied to situations where word boundary ambiguity exists. CRFs offer a solution to the long-standing problems in corpus-based or statistical Japanese morphological analysis. First, flexible feature designs for hierarchical tagsets become possible. Second, influences of label and length bias are minimized. We experiment CRFs on the standard testbed corpus used for Japanese morphological analysis, and evaluate our results using the same experimental dataset as the HMMs and MEMMs previously reported in this task. Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extended models and tools for high-performance part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proc of COLING,</booktitle>
<pages>2127</pages>
<contexts>
<context position="2733" citStr="Asahara and Matsumoto, 2000" startWordPosition="386" endWordPosition="389">us was to predict label At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages. In this paper, we show how CRFs can be applied to situations where word boundary ambiguity exists. CRFs offer a solution to the problems in Japanese morphological analysis with hidden Markov models (HMMs) (e.g., (Asahara and Matsumoto, 2000)) or with maximum entropy Markov models (MEMMs) (e.g., (Uchimoto et al., 2001)). First, as HMMs are generative, it is hard to employ overlapping features stemmed from hierarchical tagsets and nonindependent features of the inputs such as surrounding words, word suffixes and character types. These features have usually been ignored in HMMs, despite their effectiveness in unknown word guessing. Second, as mentioned in the literature, MEMMs could evade neither from label bias (Lafferty et al., 2001) nor from length bias (a bias occurring because of word boundary ambiguity). Easy sequences with lo</context>
<context position="8633" citStr="Asahara and Matsumoto, 2000" startWordPosition="1354" endWordPosition="1357"> to capture fine differences. For instance, some suffixes (e.g., san or kun) appear after names, and are helpful to detect words with Name POS. In addition, the conjugation form (cfrom) must be distinguished appearing only in the succeeding position in a bi-gram, since it is dominated by the word appearing in the next. Asahara et al. extended HMMs so as to incorporate 1) position-wise grouping, 2) word-level statis2 http://chasen.naist.jp/ 3 http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html 4 http://chasen.naist.jp/stable/ipadic/ \x0ctics, and 3) smoothing of word and POS level statistics (Asahara and Matsumoto, 2000). However, the proposed method failed to capture non-independent features such as suffixes and character types and selected smoothing parameters in an ad-hoc way. 2.2.2 Label Bias and Length Bias It is known that maximum entropy Markov models (MEMMs) (McCallum et al., 2000) or other discriminative models with independently trained nextstate classifiers potentially suffer from the label bias (Lafferty et al., 2001) and length bias. In Japanese morphological analysis, they are extremely serious problems. This is because, as shown in Figure 1, the branching variance is considerably high, and the </context>
<context position="27125" citStr="Asahara and Matsumoto, 2000" startWordPosition="4678" endWordPosition="4681">result is highly influenced by the length bias. While the relative rates of l-error and s-error are almost the same in HMMs and CRFs, the number of l-errors with MEMMs amounts to 416, which is 70% of total errors, and is even larger than that of naive HMMs (306). This result supports our claim that MEMMs is not sufficient to be applied to Japanese morphological analysis where the length bias is inevitable. 4.3.2 CRFs and Extended-HMMs Asahara et al. extended the original HMMs by 1) position-wise grouping of POS tags, 2) word-level statistics, and 3) smoothing of word and POS level statistics (Asahara and Matsumoto, 2000). All of these techniques are designed to capture hierarchical structures of POS tagsets. For instance, in the position-wise grouping, optimal levels of POS hierarchies are changed according to the contexts. Best hierarchies for each context are selected by handcrafted rules or automatic error-driven procedures. CRFs can realize such extensions naturally and straightforwardly. In CRFs, position-wise grouping and word-POS smoothing are simply integrated into a design of feature functions. Parameters k for each feature are automatically configured by general maximum likelihood estimation. As sho</context>
</contexts>
<marker>Asahara, Matsumoto, 2000</marker>
<rawString>Masayuki Asahara and Yuji Matsumoto. 2000. Extended models and tools for high-performance part-of-speech tagger. In Proc of COLING, pages 2127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard H Byrd</author>
<author>Peihuang Lu</author>
<author>Jorge Nocedal</author>
<author>Ci You Zhu</author>
</authors>
<title>A limited memory algorithm for bound constrained optimization.</title>
<date>1995</date>
<journal>SIAM Journal on Scientific Computing,</journal>
<volume>16</volume>
<issue>6</issue>
<contexts>
<context position="18086" citStr="Byrd et al., 1995" startWordPosition="3089" endWordPosition="3092">ed regularizer is that it often leads to sparse solutions where most of k are exactly 0. The features assigned zero weight are thought as irrelevant features to classifications. The L2-based regularizer, also seen in SVMs, produces a non-sparse solution where all of k have non-zero weights. All features are used with L2-CRFs. The optimal solutions of L2-CRFs can be obtained by using traditional iterative scaling algorithms (e.g., IIS or GIS (Pietra et al., 1997)) or more efficient quasi-Newton methods (e.g., L-BFGS (Liu and Nocedal, 1989)). For L1-CRFs, constrained optimizers (e.g., L-BFGS-B (Byrd et al., 1995)) can be used. 4 Experiments and Discussion 4.1 Experimental Settings We use two widely-used Japanese annotated corpora in the research community, Kyoto University Corpus ver 2.0 (KC) and RWCP Text Corpus (RWCP), for our experiments on CRFs. Note that each corpus has a different POS tagset and details (e.g., size of training and test dataset) are summarized in Table 1. One of the advantages of CRFs is that they are flexible enough to capture many correlated features, including overlapping and non-independent features. We thus use as many features as possible, which could not be used in HMMs. T</context>
</contexts>
<marker>Byrd, Lu, Nocedal, Zhu, 1995</marker>
<rawString>Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and Ci You Zhu. 1995. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16(6):11901208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="16133" citStr="Rosenfeld, 1999" startWordPosition="2733" endWordPosition="2734">ng recursive definitions: hw,ti = X hw0,t0iLT (hw,ti) hw0,t0i exp X k kfk(hw0 , t0 i, hw, ti) \x01 hw,ti = X hw0,t0iRT (hw,ti) hw0,t0i exp X k kfk(hw, ti, hw0 , t0 i) \x01 , where LT(hw, ti) and RT(hw, ti) denote a set of tokens each of which connects to the token hw, ti from the left and the right respectively. Note that initial costs of two virtual tokens, hwbos,tbosi and hweos,teosi, are set to be 1. A normalization constant is then given by Zx = hweos,teosi(= hwbos,tbosi). We attempt two types of regularizations in order to avoid overfitting. They are a Gaussian prior (L2- norm) (Chen and Rosenfeld, 1999) and a Laplacian prior (L1-norm) (Goodman, 2004; Peng and McCallum, 2004) L = C X j log(P(yj|xj)) 1 2 \x1a P k |k |(L1-norm) P k |k|2 (L2-norm) \x0cBelow, we refer to CRFs with L1-norm and L2- norm regularization as L1-CRFs and L2-CRFs respectively. The parameter C R+ is a hyperparameter of CRFs determined by a cross validation. L1-CRFs can be reformulated into the constrained optimization problem below by letting k = + k k : max : C X j log(P(yj|xj)) X k (+ k + k )/2 s.t., + k 0, k 0. At the optimal point, the following Karush-KuhunTucker conditions satisfy: + k [C (Ok Ek) 1/2] = 0, k [C (Ek </context>
</contexts>
<marker>Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald. Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Exponential priors for maximum entropy models.</title>
<date>2004</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<contexts>
<context position="16180" citStr="Goodman, 2004" startWordPosition="2740" endWordPosition="2741">ti) hw0,t0i exp X k kfk(hw0 , t0 i, hw, ti) \x01 hw,ti = X hw0,t0iRT (hw,ti) hw0,t0i exp X k kfk(hw, ti, hw0 , t0 i) \x01 , where LT(hw, ti) and RT(hw, ti) denote a set of tokens each of which connects to the token hw, ti from the left and the right respectively. Note that initial costs of two virtual tokens, hwbos,tbosi and hweos,teosi, are set to be 1. A normalization constant is then given by Zx = hweos,teosi(= hwbos,tbosi). We attempt two types of regularizations in order to avoid overfitting. They are a Gaussian prior (L2- norm) (Chen and Rosenfeld, 1999) and a Laplacian prior (L1-norm) (Goodman, 2004; Peng and McCallum, 2004) L = C X j log(P(yj|xj)) 1 2 \x1a P k |k |(L1-norm) P k |k|2 (L2-norm) \x0cBelow, we refer to CRFs with L1-norm and L2- norm regularization as L1-CRFs and L2-CRFs respectively. The parameter C R+ is a hyperparameter of CRFs determined by a cross validation. L1-CRFs can be reformulated into the constrained optimization problem below by letting k = + k k : max : C X j log(P(yj|xj)) X k (+ k + k )/2 s.t., + k 0, k 0. At the optimal point, the following Karush-KuhunTucker conditions satisfy: + k [C (Ok Ek) 1/2] = 0, k [C (Ek Ok) 1/2] = 0, and |C (Ok Ek) |1/2. These condit</context>
</contexts>
<marker>Goodman, 2004</marker>
<rawString>Joshua Goodman. 2004. Exponential priors for maximum entropy models. In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>282289</pages>
<contexts>
<context position="1404" citStr="Lafferty et al., 2001" startWordPosition="195" endWordPosition="198">-standing problems in corpus-based or statistical Japanese morphological analysis. First, flexible feature designs for hierarchical tagsets become possible. Second, influences of label and length bias are minimized. We experiment CRFs on the standard testbed corpus used for Japanese morphological analysis, and evaluate our results using the same experimental dataset as the HMMs and MEMMs previously reported in this task. Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs. 1 Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applic</context>
<context position="3234" citStr="Lafferty et al., 2001" startWordPosition="465" endWordPosition="468"> to the problems in Japanese morphological analysis with hidden Markov models (HMMs) (e.g., (Asahara and Matsumoto, 2000)) or with maximum entropy Markov models (MEMMs) (e.g., (Uchimoto et al., 2001)). First, as HMMs are generative, it is hard to employ overlapping features stemmed from hierarchical tagsets and nonindependent features of the inputs such as surrounding words, word suffixes and character types. These features have usually been ignored in HMMs, despite their effectiveness in unknown word guessing. Second, as mentioned in the literature, MEMMs could evade neither from label bias (Lafferty et al., 2001) nor from length bias (a bias occurring because of word boundary ambiguity). Easy sequences with low entropy are likely to be selected during decoding in MEMMs. The consequence is serious especially in Japanese morphological analysis due to hierarchical tagsets as well as word boundary ambiguity. The key advantage of CRFs is their flexibility to include a variety of features while avoiding these bias. In what follows, we describe our motivations of applying CRFs to Japanese morphological analysis (Section 2). Then, CRFs and their parameter estimation are provided (Section 3). Finally, we discu</context>
<context position="9050" citStr="Lafferty et al., 2001" startWordPosition="1419" endWordPosition="1422">p://chasen.naist.jp/ 3 http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html 4 http://chasen.naist.jp/stable/ipadic/ \x0ctics, and 3) smoothing of word and POS level statistics (Asahara and Matsumoto, 2000). However, the proposed method failed to capture non-independent features such as suffixes and character types and selected smoothing parameters in an ad-hoc way. 2.2.2 Label Bias and Length Bias It is known that maximum entropy Markov models (MEMMs) (McCallum et al., 2000) or other discriminative models with independently trained nextstate classifiers potentially suffer from the label bias (Lafferty et al., 2001) and length bias. In Japanese morphological analysis, they are extremely serious problems. This is because, as shown in Figure 1, the branching variance is considerably high, and the number of tokens varies according to the output path. P(A, D |x) = 0.6 * 0.6 * 1.0 = 0.36 P(B |x) = 0.4 * 1.0 = 0.4 BOS A B D C E 0.6 0.4 1.0 1.0 1.0 1.0 0.4 0.6 EOS P(A, D |x) = 0.6 * 0.6 * 1.0 = 0.36 P(B, E |x) = 0.4 * 1.0 * 1.0 = 0.4 (a) Label bias BOS B D C 0.4 1.0 1.0 1.0 0.4 EOS (b) Length bias P(A,D|x) &amp;lt; P(B,E|x) P(A,D|x) &amp;lt; P(B |x) A 0.6 0.6 Figure 2: Label and length bias in a lattice An example of the lab</context>
<context position="11392" citStr="Lafferty et al., 2001" startWordPosition="1833" endWordPosition="1836">lysis because the number of output tokens y varies by use of prior lexicons. Uchimoto et al. attempted a variant of MEMMs for Japanese morphological analysis with a number of features including suffixes and character types (Uchimoto et al., 2001; Uchimoto et al., 2002; Uchimoto et al., 2003). Although the performance of unknown words were improved, that of known words degraded due to the label and length bias. Wrong segmentation had been reported in sentences which are analyzed correctly by naive rule-based or HMMs-based analyzers. 3 Conditional Random Fields Conditional random fields (CRFs) (Lafferty et al., 2001) overcome the problems described in Section 2.2. CRFs are discriminative models and can thus capture many correlated features of the inputs. This allows flexible feature designs for hierarchical tagsets. CRFs have a single exponential model for the joint probability of the entire paths given the input sentence, while MEMMs consist of a sequential combination of exponential models, each of which estimates a conditional probability of next tokens given the current state. This minimizes the influences of the label and length bias. As explained in Section 2.1, there is word boundary ambiguity in J</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML, pages 282289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Math. Programming,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="18012" citStr="Liu and Nocedal, 1989" startWordPosition="3078" endWordPosition="3081">uld be chosen when most of given features are relevant. An advantage of L1-based regularizer is that it often leads to sparse solutions where most of k are exactly 0. The features assigned zero weight are thought as irrelevant features to classifications. The L2-based regularizer, also seen in SVMs, produces a non-sparse solution where all of k have non-zero weights. All features are used with L2-CRFs. The optimal solutions of L2-CRFs can be obtained by using traditional iterative scaling algorithms (e.g., IIS or GIS (Pietra et al., 1997)) or more efficient quasi-Newton methods (e.g., L-BFGS (Liu and Nocedal, 1989)). For L1-CRFs, constrained optimizers (e.g., L-BFGS-B (Byrd et al., 1995)) can be used. 4 Experiments and Discussion 4.1 Experimental Settings We use two widely-used Japanese annotated corpora in the research community, Kyoto University Corpus ver 2.0 (KC) and RWCP Text Corpus (RWCP), for our experiments on CRFs. Note that each corpus has a different POS tagset and details (e.g., size of training and test dataset) are summarized in Table 1. One of the advantages of CRFs is that they are flexible enough to capture many correlated features, including overlapping and non-independent features. We</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Math. Programming, 45(3, (Ser. B)):503528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and webenhanced lexicons. In</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="1866" citStr="McCallum and Li, 2003" startWordPosition="261" endWordPosition="265">y solve the long-standing problems but also improve the performance over HMMs and MEMMs. 1 Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of</context>
<context position="12940" citStr="McCallum and Li, 2003" startWordPosition="2115" endWordPosition="2118">hw1, t1i, . . . , hw#y, t#yi) given an input sequence x: P(y|x) = 1 Zx exp \x10#y X i=1 X k kfk(hwi1, ti1i, hwi, tii) \x11 , where Zx is a normalization factor over all candidate paths, i.e., Zx = X y0Y(x) exp \x10#y0 X i=1 X k kfk(hw0 i1, t0 i1i, hw0 i, t0 ii) \x11 , \x0cfk(hwi1, ti1i, hwi, tii) is an arbitrary feature function over i-th token hwi, tii, and its previous token hwi1, ti1i 5. k( = {1, . . . , K} RK) is a learned weight or parameter associated with feature function fk. Note that our formulation of CRFs is different from the widely-used formulations (e.g., (Sha and Pereira, 2003; McCallum and Li, 2003; Peng et al., 2004; Pinto et al., 2003; Peng and McCallum, 2004)). The previous applications of CRFs assign a conditional probability for a label sequence y = y1, . . . , yT given an input sequence x = x1, . . . , xT as: P(y|x) = 1 Zx exp \x10 T X i=1 X k kfk(yi1, yi, x) \x11 In our formulation, CRFs deal with word boundary ambiguity. Thus, the the size of output sequence T is not fixed through all candidates y Y(x). The index i is not tied with the input x as in the original CRFs, but unique to the output y Y(x). Here, we introduce the global feature vector F(y, x) = {F1(y, x), . . . , FK(y,</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and webenhanced lexicons. In In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Maximum entropy markov models for information and segmentation.</title>
<date>2000</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>591598</pages>
<contexts>
<context position="8907" citStr="McCallum et al., 2000" startWordPosition="1398" endWordPosition="1401">nated by the word appearing in the next. Asahara et al. extended HMMs so as to incorporate 1) position-wise grouping, 2) word-level statis2 http://chasen.naist.jp/ 3 http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html 4 http://chasen.naist.jp/stable/ipadic/ \x0ctics, and 3) smoothing of word and POS level statistics (Asahara and Matsumoto, 2000). However, the proposed method failed to capture non-independent features such as suffixes and character types and selected smoothing parameters in an ad-hoc way. 2.2.2 Label Bias and Length Bias It is known that maximum entropy Markov models (MEMMs) (McCallum et al., 2000) or other discriminative models with independently trained nextstate classifiers potentially suffer from the label bias (Lafferty et al., 2001) and length bias. In Japanese morphological analysis, they are extremely serious problems. This is because, as shown in Figure 1, the branching variance is considerably high, and the number of tokens varies according to the output path. P(A, D |x) = 0.6 * 0.6 * 1.0 = 0.36 P(B |x) = 0.4 * 1.0 = 0.4 BOS A B D C E 0.6 0.4 1.0 1.0 1.0 1.0 0.4 0.6 EOS P(A, D |x) = 0.6 * 0.6 * 1.0 = 0.36 P(B, E |x) = 0.4 * 1.0 * 1.0 = 0.4 (a) Label bias BOS B D C 0.4 1.0 1.0 </context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew McCallum, Dayne Freitag, and Fernando Pereira. 2000. Maximum entropy markov models for information and segmentation. In Proc. of ICML, pages 591598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Efficiently inducing features of conditional random fields.</title>
<date>2003</date>
<booktitle>In Nineteenth Conference on Uncertainty in Artificial Intelligence (UAI03).</booktitle>
<marker>McCallum, 2003</marker>
<rawString>Andrew McCallum. 2003. Efficiently inducing features of conditional random fields. In Nineteenth Conference on Uncertainty in Artificial Intelligence (UAI03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Andrew McCallum</author>
</authors>
<title>Accurate information extraction from research papers.</title>
<date>2004</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<contexts>
<context position="1987" citStr="Peng and McCallum, 2004" startWordPosition="281" endWordPosition="284">dom fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages. In this paper, we show how CRFs can be applied to situations where word boundary ambiguity exis</context>
<context position="13005" citStr="Peng and McCallum, 2004" startWordPosition="2127" endWordPosition="2130">) = 1 Zx exp \x10#y X i=1 X k kfk(hwi1, ti1i, hwi, tii) \x11 , where Zx is a normalization factor over all candidate paths, i.e., Zx = X y0Y(x) exp \x10#y0 X i=1 X k kfk(hw0 i1, t0 i1i, hw0 i, t0 ii) \x11 , \x0cfk(hwi1, ti1i, hwi, tii) is an arbitrary feature function over i-th token hwi, tii, and its previous token hwi1, ti1i 5. k( = {1, . . . , K} RK) is a learned weight or parameter associated with feature function fk. Note that our formulation of CRFs is different from the widely-used formulations (e.g., (Sha and Pereira, 2003; McCallum and Li, 2003; Peng et al., 2004; Pinto et al., 2003; Peng and McCallum, 2004)). The previous applications of CRFs assign a conditional probability for a label sequence y = y1, . . . , yT given an input sequence x = x1, . . . , xT as: P(y|x) = 1 Zx exp \x10 T X i=1 X k kfk(yi1, yi, x) \x11 In our formulation, CRFs deal with word boundary ambiguity. Thus, the the size of output sequence T is not fixed through all candidates y Y(x). The index i is not tied with the input x as in the original CRFs, but unique to the output y Y(x). Here, we introduce the global feature vector F(y, x) = {F1(y, x), . . . , FK(y, x)}, where Fk(y, x) = P#y i=1 fk(hwi1, ti1i, hwi, tii). Using th</context>
<context position="16206" citStr="Peng and McCallum, 2004" startWordPosition="2742" endWordPosition="2746"> X k kfk(hw0 , t0 i, hw, ti) \x01 hw,ti = X hw0,t0iRT (hw,ti) hw0,t0i exp X k kfk(hw, ti, hw0 , t0 i) \x01 , where LT(hw, ti) and RT(hw, ti) denote a set of tokens each of which connects to the token hw, ti from the left and the right respectively. Note that initial costs of two virtual tokens, hwbos,tbosi and hweos,teosi, are set to be 1. A normalization constant is then given by Zx = hweos,teosi(= hwbos,tbosi). We attempt two types of regularizations in order to avoid overfitting. They are a Gaussian prior (L2- norm) (Chen and Rosenfeld, 1999) and a Laplacian prior (L1-norm) (Goodman, 2004; Peng and McCallum, 2004) L = C X j log(P(yj|xj)) 1 2 \x1a P k |k |(L1-norm) P k |k|2 (L2-norm) \x0cBelow, we refer to CRFs with L1-norm and L2- norm regularization as L1-CRFs and L2-CRFs respectively. The parameter C R+ is a hyperparameter of CRFs determined by a cross validation. L1-CRFs can be reformulated into the constrained optimization problem below by letting k = + k k : max : C X j log(P(yj|xj)) X k (+ k + k )/2 s.t., + k 0, k 0. At the optimal point, the following Karush-KuhunTucker conditions satisfy: + k [C (Ok Ek) 1/2] = 0, k [C (Ek Ok) 1/2] = 0, and |C (Ok Ek) |1/2. These conditions mean that both + k an</context>
</contexts>
<marker>Peng, McCallum, 2004</marker>
<rawString>Fuchun Peng and Andrew McCallum. 2004. Accurate information extraction from research papers. In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields (to appear).</title>
<date>2004</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="1913" citStr="Peng et al., 2004" startWordPosition="270" endWordPosition="273"> the performance over HMMs and MEMMs. 1 Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages. In this paper, we sho</context>
<context position="4243" citStr="Peng et al., 2004" startWordPosition="620" endWordPosition="623">g these bias. In what follows, we describe our motivations of applying CRFs to Japanese morphological analysis (Section 2). Then, CRFs and their parameter estimation are provided (Section 3). Finally, we discuss experimental results (Section 4) and give conclusions with possible future directions (Section 5). 2 Japanese Morphological Analysis 2.1 Word Boundary Ambiguity Word boundary ambiguity cannot be ignored when dealing with non-segmented languages. A simple approach would be to let a character be a token (i.e., character-based Begin/Inside tagging) so that boundary ambiguity never occur (Peng et al., 2004). \x0cInput: \x02\x01\x04\x03\x06\x05\x08\x07 \t (I live in Metropolis of Tokyo .) BOS \x0b (east) [Noun] \x0b \x0c (Tokyo) [Noun] \x0c\x04\x0e (Kyoto) [Noun] \x0e (Metro.) [Suffix] \x0f (in) [Particle] \x0f (resemble) [Verb] \x10 \x11 (live) [Verb] EOS Lattice: \x0c (capital) [Noun] Figure 1: Example of lattice for Japanese morphological analysis However, B/I tagging is not a standard method in 20-year history of corpus-based Japanese morphological analysis. This is because B/I tagging cannot directly reflect lexicons which contain prior knowledge about word segmentation. We cannot ignore a l</context>
<context position="12959" citStr="Peng et al., 2004" startWordPosition="2119" endWordPosition="2122"> t#yi) given an input sequence x: P(y|x) = 1 Zx exp \x10#y X i=1 X k kfk(hwi1, ti1i, hwi, tii) \x11 , where Zx is a normalization factor over all candidate paths, i.e., Zx = X y0Y(x) exp \x10#y0 X i=1 X k kfk(hw0 i1, t0 i1i, hw0 i, t0 ii) \x11 , \x0cfk(hwi1, ti1i, hwi, tii) is an arbitrary feature function over i-th token hwi, tii, and its previous token hwi1, ti1i 5. k( = {1, . . . , K} RK) is a learned weight or parameter associated with feature function fk. Note that our formulation of CRFs is different from the widely-used formulations (e.g., (Sha and Pereira, 2003; McCallum and Li, 2003; Peng et al., 2004; Pinto et al., 2003; Peng and McCallum, 2004)). The previous applications of CRFs assign a conditional probability for a label sequence y = y1, . . . , yT given an input sequence x = x1, . . . , xT as: P(y|x) = 1 Zx exp \x10 T X i=1 X k kfk(yi1, yi, x) \x11 In our formulation, CRFs deal with word boundary ambiguity. Thus, the the size of output sequence T is not fixed through all candidates y Y(x). The index i is not tied with the input x as in the original CRFs, but unique to the output y Y(x). Here, we introduce the global feature vector F(y, x) = {F1(y, x), . . . , FK(y, x)}, where Fk(y, x</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields (to appear). In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Perkins</author>
<author>Kevin Lacker</author>
<author>James Thiler</author>
</authors>
<title>Grafting: Fast, incremental feature selection by gradient descent in function space.</title>
<date>2003</date>
<journal>JMLR,</journal>
<pages>3--13331356</pages>
<contexts>
<context position="17246" citStr="Perkins et al., 2003" startWordPosition="2953" endWordPosition="2956">t the optimal point, the following Karush-KuhunTucker conditions satisfy: + k [C (Ok Ek) 1/2] = 0, k [C (Ek Ok) 1/2] = 0, and |C (Ok Ek) |1/2. These conditions mean that both + k and k are set to be 0 (i.e., k = 0), when |C (Ok Ek) |&amp;lt; 1/2. A non-zero weight is assigned to k, only when |C (Ok Ek) |= 1/2. L2-CRFs, in contrast, give the optimal solution when L k = C (Ok Ek)k = 0. Omitting the proof, (Ok Ek) 6= 0 can be shown and L2-CRFs thus give a non-sparse solution where all k have non-zero weights. The relationship between two reguralizations have been studied in Machine Learning community. (Perkins et al., 2003) reported that L1-regularizer should be chosen for a problem where most of given features are irrelevant. On the other hand, L2- regularizer should be chosen when most of given features are relevant. An advantage of L1-based regularizer is that it often leads to sparse solutions where most of k are exactly 0. The features assigned zero weight are thought as irrelevant features to classifications. The L2-based regularizer, also seen in SVMs, produces a non-sparse solution where all of k have non-zero weights. All features are used with L2-CRFs. The optimal solutions of L2-CRFs can be obtained b</context>
</contexts>
<marker>Perkins, Lacker, Thiler, 2003</marker>
<rawString>Simon Perkins, Kevin Lacker, and James Thiler. 2003. Grafting: Fast, incremental feature selection by gradient descent in function space. JMLR, 3:13331356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>Vincent J Della Pietra Stephen</author>
<author>John D Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="17934" citStr="Pietra et al., 1997" startWordPosition="3067" endWordPosition="3070">ost of given features are irrelevant. On the other hand, L2- regularizer should be chosen when most of given features are relevant. An advantage of L1-based regularizer is that it often leads to sparse solutions where most of k are exactly 0. The features assigned zero weight are thought as irrelevant features to classifications. The L2-based regularizer, also seen in SVMs, produces a non-sparse solution where all of k have non-zero weights. All features are used with L2-CRFs. The optimal solutions of L2-CRFs can be obtained by using traditional iterative scaling algorithms (e.g., IIS or GIS (Pietra et al., 1997)) or more efficient quasi-Newton methods (e.g., L-BFGS (Liu and Nocedal, 1989)). For L1-CRFs, constrained optimizers (e.g., L-BFGS-B (Byrd et al., 1995)) can be used. 4 Experiments and Discussion 4.1 Experimental Settings We use two widely-used Japanese annotated corpora in the research community, Kyoto University Corpus ver 2.0 (KC) and RWCP Text Corpus (RWCP), for our experiments on CRFs. Note that each corpus has a different POS tagset and details (e.g., size of training and test dataset) are summarized in Table 1. One of the advantages of CRFs is that they are flexible enough to capture ma</context>
</contexts>
<marker>Pietra, Stephen, Lafferty, 1997</marker>
<rawString>Della Pietra, Stephen, Vincent J. Della Pietra, and John D. Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pinto</author>
<author>Andrew McCallum</author>
<author>Xing Wei</author>
<author>W Bruce Croft</author>
</authors>
<title>Table extraction using conditional random fields. In</title>
<date>2003</date>
<booktitle>In Proc. of SIGIR,</booktitle>
<pages>235242</pages>
<contexts>
<context position="1961" citStr="Pinto et al., 2003" startWordPosition="277" endWordPosition="280">tion Conditional random fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages. In this paper, we show how CRFs can be applied to situations where wo</context>
<context position="12979" citStr="Pinto et al., 2003" startWordPosition="2123" endWordPosition="2126">ut sequence x: P(y|x) = 1 Zx exp \x10#y X i=1 X k kfk(hwi1, ti1i, hwi, tii) \x11 , where Zx is a normalization factor over all candidate paths, i.e., Zx = X y0Y(x) exp \x10#y0 X i=1 X k kfk(hw0 i1, t0 i1i, hw0 i, t0 ii) \x11 , \x0cfk(hwi1, ti1i, hwi, tii) is an arbitrary feature function over i-th token hwi, tii, and its previous token hwi1, ti1i 5. k( = {1, . . . , K} RK) is a learned weight or parameter associated with feature function fk. Note that our formulation of CRFs is different from the widely-used formulations (e.g., (Sha and Pereira, 2003; McCallum and Li, 2003; Peng et al., 2004; Pinto et al., 2003; Peng and McCallum, 2004)). The previous applications of CRFs assign a conditional probability for a label sequence y = y1, . . . , yT given an input sequence x = x1, . . . , xT as: P(y|x) = 1 Zx exp \x10 T X i=1 X k kfk(yi1, yi, x) \x11 In our formulation, CRFs deal with word boundary ambiguity. Thus, the the size of output sequence T is not fixed through all candidates y Y(x). The index i is not tied with the input x as in the original CRFs, but unique to the output y Y(x). Here, we introduce the global feature vector F(y, x) = {F1(y, x), . . . , FK(y, x)}, where Fk(y, x) = P#y i=1 fk(hwi1,</context>
</contexts>
<marker>Pinto, McCallum, Wei, Croft, 2003</marker>
<rawString>David Pinto, Andrew McCallum, Xing Wei, and W. Bruce Croft. 2003. Table extraction using conditional random fields. In In Proc. of SIGIR, pages 235242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>213220</pages>
<contexts>
<context position="1816" citStr="Sha and Pereira, 2003" startWordPosition="254" endWordPosition="257">n this task. Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs. 1 Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to p</context>
<context position="12917" citStr="Sha and Pereira, 2003" startWordPosition="2111" endWordPosition="2114">of an output path y = (hw1, t1i, . . . , hw#y, t#yi) given an input sequence x: P(y|x) = 1 Zx exp \x10#y X i=1 X k kfk(hwi1, ti1i, hwi, tii) \x11 , where Zx is a normalization factor over all candidate paths, i.e., Zx = X y0Y(x) exp \x10#y0 X i=1 X k kfk(hw0 i1, t0 i1i, hw0 i, t0 ii) \x11 , \x0cfk(hwi1, ti1i, hwi, tii) is an arbitrary feature function over i-th token hwi, tii, and its previous token hwi1, ti1i 5. k( = {1, . . . , K} RK) is a learned weight or parameter associated with feature function fk. Note that our formulation of CRFs is different from the widely-used formulations (e.g., (Sha and Pereira, 2003; McCallum and Li, 2003; Peng et al., 2004; Pinto et al., 2003; Peng and McCallum, 2004)). The previous applications of CRFs assign a conditional probability for a label sequence y = y1, . . . , yT given an input sequence x = x1, . . . , xT as: P(y|x) = 1 Zx exp \x10 T X i=1 X k kfk(yi1, yi, x) \x11 In our formulation, CRFs deal with word boundary ambiguity. Thus, the the size of output sequence T is not fixed through all candidates y Y(x). The index i is not tied with the input x as in the original CRFs, but unique to the output y Y(x). Here, we introduce the global feature vector F(y, x) = {</context>
<context position="23087" citStr="Sha and Pereira, 2003" startWordPosition="4046" endWordPosition="4049"> , p1, p2, cf, ct, bwi w0/w are lexicalized hp10 , p20 , cf0 , ct0 , bw0 , p1, p2, cf, ct, bwi hara and Matsumoto, 2000) trained and tested with the same corpus is also shown. E-HMMs is applied to the current implementation of ChaSen. Details of E-HMMs are described in Section 4.3.2. We directly evaluated the difference of these systems using McNemars test. Since there are no standard methods to evaluate the significance of F scores, we convert the outputs into the characterbased B/I labels and then employ a McNemars paired test on the labeling disagreements. This evaluation was also used in (Sha and Pereira, 2003). The results of McNemars test suggest that L2-CRFs is significantly better than other systems including L1- CRFs8. The overall results support our empirical success of morphological analysis based on CRFs. 4.3 Discussion 4.3.1 CRFs and MEMMs Uchimoto el al. proposed a variant of MEMMs trained with a number of features (Uchimoto et al., 2001). Although they improved the accuracy for unknown words, they fail to segment some sentences which are correctly segmented with HMMs or rulebased analyzers. Figure 3 illustrates the sentences which are incorrectly segmented by Uchimotos MEMMs. The correct </context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proc. of HLT-NAACL, pages 213220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>The unknown word problem: a morphological analysis of Japanese using maximum entropy aided by a dictionary.</title>
<date>2001</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>9199</pages>
<contexts>
<context position="2811" citStr="Uchimoto et al., 2001" startWordPosition="398" endWordPosition="401">aridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages. In this paper, we show how CRFs can be applied to situations where word boundary ambiguity exists. CRFs offer a solution to the problems in Japanese morphological analysis with hidden Markov models (HMMs) (e.g., (Asahara and Matsumoto, 2000)) or with maximum entropy Markov models (MEMMs) (e.g., (Uchimoto et al., 2001)). First, as HMMs are generative, it is hard to employ overlapping features stemmed from hierarchical tagsets and nonindependent features of the inputs such as surrounding words, word suffixes and character types. These features have usually been ignored in HMMs, despite their effectiveness in unknown word guessing. Second, as mentioned in the literature, MEMMs could evade neither from label bias (Lafferty et al., 2001) nor from length bias (a bias occurring because of word boundary ambiguity). Easy sequences with low entropy are likely to be selected during decoding in MEMMs. The consequence </context>
<context position="11015" citStr="Uchimoto et al., 2001" startWordPosition="1776" endWordPosition="1779">ns. Moreover, we cannot ignore the influence of the length bias either. By the length bias, we mean that short paths, consisting of a small number of tokens, are preferred to long path. Even if the transition probability of each token is small, the total probability of the path will be amplified when the path is short 2:(b)). Length bias occurs in Japanese morphological analysis because the number of output tokens y varies by use of prior lexicons. Uchimoto et al. attempted a variant of MEMMs for Japanese morphological analysis with a number of features including suffixes and character types (Uchimoto et al., 2001; Uchimoto et al., 2002; Uchimoto et al., 2003). Although the performance of unknown words were improved, that of known words degraded due to the label and length bias. Wrong segmentation had been reported in sentences which are analyzed correctly by naive rule-based or HMMs-based analyzers. 3 Conditional Random Fields Conditional random fields (CRFs) (Lafferty et al., 2001) overcome the problems described in Section 2.2. CRFs are discriminative models and can thus capture many correlated features of the inputs. This allows flexible feature designs for hierarchical tagsets. CRFs have a single </context>
<context position="20222" citStr="Uchimoto et al., 2001" startWordPosition="3456" endWordPosition="3459">on is evaluated), top: (word segmentation and the top level of POS are evaluated), and all: (all information is used for evaluation). The hyperparameters C for L1-CRFs and L2- CRFs are selected by cross-validation. Experiments are implemented in C++ and executed on Linux with XEON 2.8 GHz dual processors and 4.0 Gbyte of main memory. 4.2 Results Tables 3 and 4 show experimental results using KC and RWCP respectively. The three F-scores (seg/top/all) for our CRFs and a baseline bi-gram HMMs are listed. In Table 3 (KC data set), the results of a variant of maximum entropy Markov models (MEMMs) (Uchimoto et al., 2001) and a rule-based analyzer (JUMAN7) are also shown. To make a fare comparison, we use exactly the same data as (Uchimoto et al., 2001). In Table 4 (RWCP data set), the result of an extended Hidden Markov Models (E-HMMs) (Asa6 These lexicalizations are usually employed in Japanese morphological analysis. 7 JUMAN assigns unknown POS to the words not seen in the lexicon. We simply replace the POS of these words with the default POS, Noun-SAHEN. \x0cTable 1: Details of Data Set KC RWCP source Mainich News Article (95) Mainich News Article (94) lexicon (# of words) JUMAN ver. 3.61 (1,983,173) IPADI</context>
<context position="23431" citStr="Uchimoto et al., 2001" startWordPosition="4100" endWordPosition="4103"> McNemars test. Since there are no standard methods to evaluate the significance of F scores, we convert the outputs into the characterbased B/I labels and then employ a McNemars paired test on the labeling disagreements. This evaluation was also used in (Sha and Pereira, 2003). The results of McNemars test suggest that L2-CRFs is significantly better than other systems including L1- CRFs8. The overall results support our empirical success of morphological analysis based on CRFs. 4.3 Discussion 4.3.1 CRFs and MEMMs Uchimoto el al. proposed a variant of MEMMs trained with a number of features (Uchimoto et al., 2001). Although they improved the accuracy for unknown words, they fail to segment some sentences which are correctly segmented with HMMs or rulebased analyzers. Figure 3 illustrates the sentences which are incorrectly segmented by Uchimotos MEMMs. The correct paths are indicated by bold boxes. Uchimoto et al. concluded that these errors were caused by nonstandard entries in the lexicon. In Figure 3, (romanticist) and (ones heart) are unusual spellings and they are normally written as and respectively. However, we conjecture that these errors are caused by the influence of the length bias. To suppo</context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 2001</marker>
<rawString>Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 2001. The unknown word problem: a morphological analysis of Japanese using maximum entropy aided by a dictionary. In Proc. of EMNLP, pages 9199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Chikashi Nobata</author>
<author>Atsushi Yamada</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Morphological analysis of the spontaneous speech corpus.</title>
<date>2002</date>
<booktitle>In Proc of COLING,</booktitle>
<pages>1298--1302</pages>
<contexts>
<context position="11038" citStr="Uchimoto et al., 2002" startWordPosition="1780" endWordPosition="1783"> ignore the influence of the length bias either. By the length bias, we mean that short paths, consisting of a small number of tokens, are preferred to long path. Even if the transition probability of each token is small, the total probability of the path will be amplified when the path is short 2:(b)). Length bias occurs in Japanese morphological analysis because the number of output tokens y varies by use of prior lexicons. Uchimoto et al. attempted a variant of MEMMs for Japanese morphological analysis with a number of features including suffixes and character types (Uchimoto et al., 2001; Uchimoto et al., 2002; Uchimoto et al., 2003). Although the performance of unknown words were improved, that of known words degraded due to the label and length bias. Wrong segmentation had been reported in sentences which are analyzed correctly by naive rule-based or HMMs-based analyzers. 3 Conditional Random Fields Conditional random fields (CRFs) (Lafferty et al., 2001) overcome the problems described in Section 2.2. CRFs are discriminative models and can thus capture many correlated features of the inputs. This allows flexible feature designs for hierarchical tagsets. CRFs have a single exponential model for t</context>
</contexts>
<marker>Uchimoto, Nobata, Yamada, Sekine, Isahara, 2002</marker>
<rawString>Kiyotaka Uchimoto, Chikashi Nobata, Atsushi Yamada, Satoshi Sekine, and Hitoshi Isahara. 2002. Morphological analysis of the spontaneous speech corpus. In Proc of COLING, pages 1298 1302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
</authors>
<title>Chikashi Nobata, Atsushi Yamada, and Hitoshi Isahara Satoshi Sekine.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>479488--0</pages>
<marker>Uchimoto, 2003</marker>
<rawString>Kiyotaka Uchimoto, Chikashi Nobata, Atsushi Yamada, and Hitoshi Isahara Satoshi Sekine. 2003. Morphological analysis of a large spontaneous speech corpus in Japanese. In Proc. of ACL, pages 479488. \x0c&apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>