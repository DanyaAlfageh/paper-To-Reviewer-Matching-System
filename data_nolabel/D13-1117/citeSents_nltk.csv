CITATION showed that semi-supervised dropout training for logistic regression captures a similar intuition to techniques such as entropy regularization CITATION and transductive SVMs CITATION, which encourage confident predictions on the unlabeled data,,
5.2 CRF Experiments We evaluate the quadratic dropout regularizer in linear-chain CRFs on two sequence tagging tasks: the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and the SANCL 2012 POS tagging task CITATION ,,
To test the validity of this approximation we compare it to the Gaussian method developed by CITATION on a two-class classification task,,
For the SANCL CITATION POS tagging task, we used the same CRF framework with a much simpler set of features word unigrams: w1, w0, w1 word bigram: (w1, w0) and (w0, w1) F=1 None L2 Drop Dev 89.40 90.73 91.86 Test 84.67 85.82 87.42 Table 4: CoNLL summary of results,,
These improvements are significant at the 0.1% level according to the paired bootstrap resampling method of 2000 iterations CITATION,,
Dropout is is similar in spirit to feature bagging in the deliberate removal of features, but performs the removal in a preset way rather than randomly (CITATION; CITATION; CITATION),,
The effectiveness of this technique is well-known in machine learning (AbuMostafa, 1990; CITATION; Simard et al., 2000; CITATIONa; van der Maaten et al., 2013), but working directly with many corrupted copies of a dataset can be computationally prohibitive,,
2.1 Semi-supervised learning A key observation CITATION is that the noising regularizer R (8), while involving a sum over examples, is independent of the output y,,
In other cases it is possible to develop a new objective function by marginalizing over the artificial noise (CITATION; van der Maaten et al., 2013),,
We focus on dropout noise CITATION, a recently popularized form of artificial feature noise where a random subset of features is omitted independently for each training example,,
Dropout and its variants have been shown to outperform L2 regularization on various tasks (CITATION; CITATION; CITATION),,
Semisupervised dropout has the advantage of only using the predicted label probabilities on the unlabeled data to modulate an L2 regularizer, rather than requiring more heavy-handed modeling of the unlabeled data as in entropy regularization or expectation regularization CITATION,,
Following CITATION and CITATION, we approximate R(, x) using a second-order Taylor expansion, which will allow us to work with only means and covariances of the noised features,,
For example, CITATION showed that training with features that have been corrupted with additive Gaussian noise is equivalent to a form of L2 regula,,
 linear-chain CRFs on two sequence tagging tasks: the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and the SANCL 2012 POS tagging task CITATION ,,
allum, 2005; CITATION),,
Plotted is the test set accuracy with logistic regression as a function of for the L2 regularizer, Gaussian dropout CITATION + additional L2, and quadratic dropout (8) + L2 described in this paper,,
NLP sequence labeling tasks are especially well suited to a semi-supervised approach, as input features are numerous but sparse, and labeled data is expensive to obtain but unlabeled data is abundant (CITATION; CITATION),,
For example, CITATION showed that training with features that have been corrupted with additive Gaussian noise is equivalent to a form of L2 regularization in the low noise limit,,
Sometimes, training with corrupted features reduces to a special form of regularization (CITATION; CITATION; CITATIONb; CITATION),,
1170 \x0cOur approach is based on a second-order approximation to feature noising developed among others by CITATION and CITATION, which allows us to convert dropout noise into a form of adaptive regularization,,
For training the CRF model, we used a comprehensive set of features from CITATION that gives state-of-the-art results on this task,,
