<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<reference confidence="0.310376">
b&apos;Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 103108, Atlanta, Georgia, June 13-14, 2013. c
2013 Association for Computational Linguistics
DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level
</reference>
<figure confidence="0.927830555555556">
Semantic Similarity Estimation
Nikolaos Malandrakis1
, Elias Iosif2
, Vassiliki Prokopi2
, Alexandros Potamianos2
,
Shrikanth Narayanan1
1
Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
</figure>
<page confidence="0.67462">
2
</page>
<affiliation confidence="0.899044">
Department of ECE, Technical University of Crete, 73100 Chania, Greece
</affiliation>
<email confidence="0.9615625">
malandra@usc.edu, iosife@telecom.tuc.gr, vprokopi@isc.tuc.gr, potam@telecom.tuc.gr,
shri@sipi.usc.edu
</email>
<sectionHeader confidence="0.990507" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995227166666667">
This paper describes our submission for the
*SEM shared task of Semantic Textual Sim-
ilarity. We estimate the semantic similarity
between two sentences using regression mod-
els with features: 1) n-gram hit rates (lexical
matches) between sentences, 2) lexical seman-
tic similarity between non-matching words, 3)
string similarity metrics, 4) affective content
similarity and 5) sentence length. Domain
adaptation is applied in the form of indepen-
dent models and a model selection strategy
achieving a mean correlation of 0.47.
</bodyText>
<sectionHeader confidence="0.997503" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99920114893617">
Text semantic similarity estimation has been an ac-
tive research area, thanks to a variety of potential ap-
plications and the wide availability of data afforded
by the world wide web. Semantic textual similar-
ity (STS) estimates can be used for information ex-
traction (Szpektor and Dagan, 2008), question an-
swering (Harabagiu and Hickl, 2006) and machine
translation (Mirkin et al., 2009). Term-level simi-
larity has been successfully applied to problems like
grammar induction (Meng and Siu, 2002) and affec-
tive text categorization (Malandrakis et al., 2011). In
this work, we built on previous research and our sub-
mission to SemEval2012 (Malandrakis et al., 2012)
to create a sentence-level STS model for the shared
task of *SEM 2013 (Agirre et al., 2013).
Semantic similarity between words has been
well researched, with a variety of knowledge-based
(Miller, 1990; Budanitsky and Hirst, 2006) and
corpus-based (Baroni and Lenci, 2010; Iosif and
Potamianos, 2010) metrics proposed. Moving to
sentences increases the complexity exponentially
and as a result has led to measurements of simi-
larity at various levels: lexical (Malakasiotis and
Androutsopoulos, 2007), syntactic (Malakasiotis,
2009; Zanzotto et al., 2009), and semantic (Rinaldi
et al., 2003; Bos and Markert, 2005). Machine trans-
lation evaluation metrics can be used to estimate lex-
ical level similarity (Finch et al., 2005; Perez and
Alfonseca, 2005), including BLEU (Papineni et al.,
2002), a metric using word n-gram hit rates. The pi-
lot task of sentence STS in SemEval 2012 (Agirre et
al., 2012) showed a similar trend towards multi-level
similarity, with the top performing systems utilizing
large amounts of partial similarity metrics and do-
main adaptation (the use of separate models for each
input domain) (Bar et al., 2012; Saric et al., 2012).
Our approach is originally motivated by BLEU
and primarily utilizes hard and soft n-gram hit
rates to estimate similarity. Compared to last year,
we utilize different alignment strategies (to decide
which n-grams should be compared with which).
We also include string similarities (at the token and
character level) and similarity of affective content,
expressed through the difference in sentence arousal
and valence ratings. Finally we added domain adap-
tation: the creation of separate models per domain
and a strategy to select the most appropriate model.
</bodyText>
<sectionHeader confidence="0.99354" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.999757">
Our model is based upon that submitted for the same
task in 2012 (Malandrakis et al., 2012). To esti-
mate semantic similarity metrics we use a super-
vised model with features extracted using corpus-
</bodyText>
<page confidence="0.999464">
103
</page>
<bodyText confidence="0.996575166666667">
\x0cbased word-level similarity metrics. To combine
these metrics into a sentence-level similarity score
we use a modification of BLEU (Papineni et al.,
2002) that utilizes word-level semantic similarities,
string level comparisons and comparisons of affec-
tive content, detailed below.
</bodyText>
<subsectionHeader confidence="0.978069">
2.1 Word level semantic similarity
</subsectionHeader>
<bodyText confidence="0.991850411764706">
Co-occurrence-based. The semantic similarity be-
tween two words, wi and wj, is estimated as their
pointwise mutual information (Church and Hanks,
1990): I(i, j) = log p(i,j)
p(i)p(j), where p(i) and p(j) are
the occurrence probabilities of wi and wj, respec-
tively, while the probability of their co-occurrence
is denoted by p(i, j). In our previous participation
in SemEval12-STS task (Malandrakis et al., 2012)
we employed a modification of the pointwise mutual
information based on the maximum sense similar-
ity assumption (Resnik, 1995) and the minimization
of the respective error in similarity estimation. In
particular, exponential weights were introduced in
order to reduce the overestimation of denominator
probabilities. The modified metric Ia(i, j), is de-
fined as:
</bodyText>
<equation confidence="0.93634925">
Ia(i, j)=
1
2
\x14
log
p(i, j)
p(i)p(j)
+ log
p(i, j)
p(i)p(j)
\x15
. (1)
</equation>
<bodyText confidence="0.987602375">
The weight was estimated on the corpus of (Iosif
and Potamianos, 2012) in order to maximize word
sense coverage in the semantic neighborhood of
each word. The Ia(i, j) metric using the estimated
value of = 0.8 was shown to significantly
outperform I(i, j) and to achieve state-of-the-art
results on standard semantic similarity datasets
(Rubenstein and Goodenough, 1965; Miller and
Charles, 1998; Finkelstein et al., 2002).
Context-based: The fundamental assumption
behind context-based metrics is that similarity
of context implies similarity of meaning (Harris,
1954). A contextual window of size 2H + 1 words
is centered on the word of interest wi and lexical
features are extracted. For every instance of wi
in the corpus the H words left and right of wi
formulate a feature vector vi. For a given value of
H the context-based semantic similarity between
two words, wi and wj, is computed as the cosine
of their feature vectors: QH(i, j) =
vi.vj
||vi  |vj||.
The elements of feature vectors can be weighted
according various schemes [(Iosif and Potamianos,
2010)], while, here we use a binary scheme.
Network-based: The aforementioned similarity
metrics were used for the definition of a semantic
network (Iosif and Potamianos, 2013; Iosif et al.,
2013). A number of similarity metrics were pro-
posed under either the attributional similarity (Tur-
ney, 2006) or the maximum sense similarity (Resnik,
1995) assumptions of lexical semantics1.
</bodyText>
<subsectionHeader confidence="0.997967">
2.2 Sentence level similarities
</subsectionHeader>
<bodyText confidence="0.99967636">
To utilize word-level semantic similarities in the
sentence-level task we use a modified version of
BLEU (Papineni et al., 2002). The model works in
two passes: the first pass identifies exact matches
(similar to baseline BLEU), the second pass com-
pares non-matched terms using semantic similarity.
Non-matched terms from the hypothesis sentence
are compared with all terms of the reference sen-
tence (regardless of whether they were matched dur-
ing the first pass). In the case of bigram and higher
order terms, the process is applied recursively: the
bigrams are decomposed into two words and the
similarity between them is estimated by applying the
same method to the words. All word similarity met-
rics used are peak-to-peak normalized in the [0,1]
range, so they serve as a degree-of-match. The se-
mantic similarity scores from term pairs are summed
(just like n-gram hits) to obtain a BLEU-like hit-rate.
Alignment is performed via maximum similarity:
we iterate on the hypothesis n-grams, left-to-right,
and compare each with the most similar n-gram in
the reference. The features produced by this process
are soft hit-rates (for 1-, 2-, 3-, 4-grams)2. We also
use the hard hit rates produced by baseline BLEU
as features of the final model.
</bodyText>
<subsectionHeader confidence="0.996424">
2.3 String similarities
</subsectionHeader>
<bodyText confidence="0.94902">
We use the following string-based similarity fea-
tures: 1) Longest Common Subsequence Similarity
</bodyText>
<figure confidence="0.910501">
(LCSS) (Lin and Och, 2004) based on the Longest
Common Subsequence (LCS) character-based dy-
</figure>
<page confidence="0.583568">
1
</page>
<bodyText confidence="0.9599575">
The network-based metrics were applied only during the
training phase of the shared task, due to time limitations. They
exhibited almost identical performance as the metric defined by
(1), which was used in the test runs.
</bodyText>
<page confidence="0.973968">
2
</page>
<bodyText confidence="0.9994835">
Note that the features are computed twice on each sentence
pair and then averaged.
</bodyText>
<page confidence="0.998896">
104
</page>
<bodyText confidence="0.986421333333333">
\x0cnamic programming algorithm. LCSS represents the
length of the longest string (or strings) that is a sub-
string (or are substrings) of two or more strings. 2)
Skip bigram co-occurrence measures the overlap of
skip-bigrams between two sentences or phrases. A
skip-bigram is defined as any pair of words in the
sentence order, allowing for arbitrary gaps between
words (Lin and Och, 2004). 3) Containment is de-
fined as the percentage of a sentence that is con-
tained in another sentence. It is a number between
0 and 1, where 1 means the hypothesis sentence is
fully contained in the reference sentence (Broder,
1997). We express containment as the amount of n-
grams of a sentence contained in another. The con-
tainment metric is not symmetric and is calculated
as: c(X, Y ) = |S(X) S(Y )|/S(X), where S(X)
and S(Y ) are all the n-grams of sentences X and Y
respectively.
</bodyText>
<subsectionHeader confidence="0.980536">
2.4 Affective similarity
</subsectionHeader>
<bodyText confidence="0.999066333333333">
We used the method proposed in (Malandrakis et al.,
2011) to estimate affective features. Continuous (va-
lence and arousal) ratings in [1, 1] of any term are
represented as a linear combination of a function of
its semantic similarities to a set of seed words and
the affective ratings of these words, as follows:
</bodyText>
<equation confidence="0.9989702">
v(wj) = a0 +
N
X
i=1
ai v(wi) dij, (2)
</equation>
<bodyText confidence="0.997393125">
where wj is the term we mean to characterize,
w1...wN are the seed words, v(wi) is the valence rat-
ing for seed word wi, ai is the weight corresponding
to seed word wi (that is estimated as described next),
dij is a measure of semantic similarity between wi
and wj (for the purposes of this work, cosine similar-
ity between context vectors is used). The weights ai
are estimated over the Affective norms for English
Words (ANEW) (Bradley and Lang, 1999) corpus.
Using this model we generate affective ratings for
every content word (noun, verb, adjective or adverb)
of every sentence. We assume that these can ad-
equately describe the affective content of the sen-
tences. To create an affective similarity metric we
use the difference of means of the word affective rat-
ings between two sentences.
</bodyText>
<equation confidence="0.888617">
daffect = 2 |(v(s1)) (v(s2)) |(3)
</equation>
<bodyText confidence="0.9980035">
where (v(si)) the mean of content word ratings in-
cluded in sentence i.
</bodyText>
<subsectionHeader confidence="0.921998">
2.5 Fusion
</subsectionHeader>
<bodyText confidence="0.9957935">
The aforementioned features are combined using
one of two possible models. The first model is a
</bodyText>
<equation confidence="0.8772855">
Multiple Linear Regression (MLR) model
DL = a0 +
k
X
n=1
an fk, (4)
</equation>
<bodyText confidence="0.997071176470588">
where DL is the estimated similarity, fk are the un-
supervised semantic similarity metrics and an are
the trainable parameters of the model.
The second model is motivated by an assumption
of cognitive scaling of similarity scores: we expect
that the perception of hit rates is non-linearly af-
fected by the length of the sentences. We call this the
hierarchical fusion scheme. It is a combination of
(overlapping) MLR models, each matching a range
of sentence lengths. The first model DL1 is trained
with sentences with length up to l1, i.e., l l1, the
second model DL2 up to length l2 etc. During test-
ing, sentences with length l [1, l1] are decoded
with DL1, sentences with length l (l1, l2] with
model DL2 etc. Each of these partial models is a
linear fusion model as shown in (4). In this work,
we use four models with l1 = 10, l2 = 20, l3 = 30,
</bodyText>
<equation confidence="0.97315">
l4 = .
</equation>
<bodyText confidence="0.9712096">
Domain adaptation is employed, by creating sep-
arate models per domain (training data source). Be-
yond that, we also create a unified model, trained
on all data to be used as a fallback if an appropriate
model can not be decided upon during evaluation.
</bodyText>
<sectionHeader confidence="0.989235" genericHeader="method">
3 Experimental Procedure and Results
</sectionHeader>
<bodyText confidence="0.999352333333334">
Initially all sentences are pre-processed by the
CoreNLP (Finkel et al., 2005; Toutanova et al.,
2003) suite of tools, a process that includes named
entity recognition, normalization, part of speech tag-
ging, lemmatization and stemming. We evaluated
multiple types of preprocessing per unsupervised
metric and chose different ones depending on the
metric. Word-level semantic similarities, used for
soft comparisons and affective feature extraction,
were computed over a corpus of 116 million web
snippets collected by posing one query for every
word in the Aspell spellchecker (asp, ) vocabulary to
the Yahoo! search engine. Word-level emotional rat-
ings in continuous valence and arousal scales were
produced by a model trained on the ANEW dataset
</bodyText>
<page confidence="0.99835">
105
</page>
<bodyText confidence="0.990767027777778">
\x0cand using contextual similarities. Finally, string sim-
ilarities were calculated over the original unmodified
sentences.
Next, results are reported in terms of correla-
tion between the generated scores and the ground
truth, for each corpus in the shared task, as well as
their weighted mean. Feature selection is applied
to the large candidate feature set using a wrapper-
based backward selection approach on the train-
ing data.The final feature set contains 15 features:
soft hit rates calculated over content word 1- to 4-
grams (4 features), soft hit rates calculated over un-
igrams per part-of-speech, for adjectives, nouns, ad-
verbs, verbs (4 features), BLEU unigram hit rates
for all words and content words (2 features), skip
and containment similarities, containment normal-
ized by sum of sentence lengths or product of sen-
tence lengths (3 features) and affective similarities
for arousal and valence (2 features).
Domain adaptation methods are the only dif-
ference between the three submitted runs. For all
three runs we train one linear model per training set
and a fallback model. For the first run, dubbed lin-
ear, the fallback model is linear and model selection
during evaluation is performed by file name, there-
fore results for the OnWN set are produced by a
model trained with OnWN data, while the rest are
produced by the fallback model. The second run,
dubbed length, uses a hierarchical fallback model
and model selection is performed by file name. The
third run, dubbed adapt, uses the same models as
the first run and each test set is assigned to a model
(i.e., the fallback model is never used). The test set -
model (training) mapping for this run is: OnWN
OnWN, headlines SMTnews, SMT Europarl
and FNWN OnWN.
</bodyText>
<tableCaption confidence="0.883569">
Table 1: Correlation performance for the linear model us-
ing lexical (L), string (S) and affect (A) features
</tableCaption>
<table confidence="0.99128325">
Feature headl. OnWN FNWN SMT mean
L 0.68 0.51 0.23 0.25 0.46
L+S 0.69 0.49 0.23 0.26 0.46
L+S+A 0.69 0.51 0.27 0.28 0.47
</table>
<bodyText confidence="0.8432624">
Results are shown in Tables 1 and 2. Results for
the linear run using subsets of the final feature set
are shown in Table 1. Lexical features (hit rates) are
obviously the most valuable features. String similar-
ities provided us with an improvement in the train-
</bodyText>
<tableCaption confidence="0.998346">
Table 2: Correlation performance on the evaluation set.
</tableCaption>
<table confidence="0.96852525">
Run headl. OnWN FNWN SMT mean
linear 0.69 0.51 0.27 0.28 0.47
length 0.65 0.51 0.25 0.28 0.46
adapt 0.62 0.51 0.33 0.30 0.46
</table>
<bodyText confidence="0.998242761904762">
ing set which is not reflected in the test set. Af-
fect proved valuable, particularly in the most diffi-
cult sets of FNWN and SMT.
Results for the three submission runs are shown
in Table 2. Our best run was the simplest one, using
a purely linear model and effectively no adaptation.
Adding a more aggressive adaptation strategy im-
proved results in the FNWN and SMT sets, so there
is definitely some potential, however the improve-
ment observed is nowhere near that observed in the
training data or the same task of SemEval 2012. We
have to question whether this improvement is an ar-
tifact of the rating distributions of these two sets
(SMT contains virtually only high ratings, FNWN
contains virtually only low ratings): such wild mis-
matches in priors among training and test sets can
be mitigated using more elaborate machine learning
algorithms (rather than employing better semantic
similarity features or algorithms). Overall the sys-
tem performs well in the two sets containing large
similarity rating ranges.
</bodyText>
<sectionHeader confidence="0.997553" genericHeader="method">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9994852">
We have improved over our previous model of sen-
tence semantic similarity. The inclusion of string-
based similarities and more so of affective content
measures proved significant, but domain adaptation
provided mixed results. While expanding the model
to include more layers of similarity estimates is
clearly a step in the right direction, further work is
required to include even more layers. Using syntac-
tic information and more levels of abstraction (e.g.
concepts) are obvious next steps.
</bodyText>
<sectionHeader confidence="0.996659" genericHeader="method">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9460756">
The first four authors have been partially funded
by the PortDial project (Language Resources for
Portable Multilingual Spoken Dialog Systems) sup-
ported by the EU Seventh Framework Programme
(FP7), grant number 296170.
</bodyText>
<page confidence="0.996668">
106
</page>
<bodyText confidence="0.89069225">
\x0cReferences
E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre.
2012. Semeval-2012 task 6: A pilot on semantic tex-
tual similarity. In Proc. SemEval, pages 385393.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In Proc. *SEM.
Gnu aspell. http://www.aspell.net.
D. Bar, C. Biemann, I. Gurevych, and T. Zesch. 2012.
Ukp: Computing semantic textual similarity by com-
bining multiple content similarity measures. In Proc.
SemEval, pages 435440.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673721.
J. Bos and K. Markert. 2005. Recognising textual en-
tailment with logical inference. In Proceedings of the
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, page 628635.
M. Bradley and P. Lang. 1999. Affective norms for En-
glish words (ANEW): Stimuli, instruction manual and
affective ratings. Technical report C-1. The Center for
Research in Psychophysiology, University of Florida.
Andrei Z. Broder. 1997. On the resemblance and con-
tainment of documents. In In Compression and Com-
plexity of Sequences (SEQUENCES97, pages 2129.
</bodyText>
<sectionHeader confidence="0.35626" genericHeader="method">
IEEE Computer Society.
</sectionHeader>
<bodyText confidence="0.593266933333333">
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32:1347.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):2229.
A. Finch, S. Y. Hwang, and E. Sumita. 2005. Using ma-
chine translation evaluation techniques to determine
sentence-level semantic equivalence. In Proceedings
of the 3rd International Workshop on Paraphrasing,
page 1724.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
</bodyText>
<reference confidence="0.886537919354839">
tional Linguistics, pages 363370.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions on Information Systems, 20(1):116131.
S. Harabagiu and A. Hickl. 2006. Methods for Us-
ing Textual Entailment in Open-Domain Question An-
swering. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 905912.
Z. Harris. 1954. Distributional structure. Word,
10(23):146162.
E. Iosif and A. Potamianos. 2010. Unsupervised seman-
tic similarity computation between terms using web
documents. IEEE Transactions on Knowledge and
Data Engineering, 22(11):16371647.
E. Iosif and A. Potamianos. 2012. Semsim: Resources
for normalized semantic similarity computation using
lexical networks. In Proc. Eighth International Con-
ference on Language Resources and Evaluation, pages
34993504.
Elias Iosif and Alexandros Potamianos. 2013. Similarity
Computation Using Semantic Networks Created From
Web-Harvested Data. Natural Language Engineering,
(submitted).
E. Iosif, A. Potamianos, M. Giannoudaki, and K. Zer-
vanou. 2013. Semantic similarity computation for ab-
stract and concrete nouns using network-based distri-
butional semantic models. In 10th International Con-
ference on Computational Semantics (IWCS), pages
328334.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of machine translation quality using longest
common subsequence and skip-bigram statistics. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, ACL 04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
P. Malakasiotis and I. Androutsopoulos. 2007. Learn-
ing textual entailment using svms and string similar-
ity measures. In Proceedings of of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 4247.
P. Malakasiotis. 2009. Paraphrase recognition using ma-
chine learning to combine similarity measures. In Pro-
ceedings of the 47th Annual Meeting of ACL and the
4th Int. Joint Conference on Natural Language Pro-
cessing of AFNLP, pages 4247.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
29772980.
N. Malandrakis, E. Iosif, and A. Potamianos. 2012.
DeepPurple: Estimating sentence semantic similarity
using n-gram regression models and web snippets. In
Proc. Sixth International Workshop on Semantic Eval-
uation (SemEval) The First Joint Conference on
Lexical and Computational Semantics (*SEM), pages
565570.
H. Meng and K.-C. Siu. 2002. Semi-automatic acquisi-
tion of semantic structures for understanding domain-
</reference>
<page confidence="0.988863">
107
</page>
<reference confidence="0.999081905660377">
\x0cspecific natural language queries. IEEE Transactions
on Knowledge and Data Engineering, 14(1):172181.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):128.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235312.
S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-
man, and S. Idan. 2009. Source-language entailment
modeling for translating unknown terms. In Proceed-
ings of the 47th Annual Meeting of ACL and the 4th Int.
Joint Conference on Natural Language Processing of
AFNLP, pages 791799.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311318.
D. Perez and E. Alfonseca. 2005. Application of the
bleu algorithm for recognizing textual entailments. In
Proceedings of the PASCAL Challenges Worshop on
Recognising Textual Entailment.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxanomy. In Proc. of In-
ternational Joint Conference for Artificial Intelligence,
pages 448453.
F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and
D. Molla. 2003. Exploiting paraphrases in a question
answering system. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing, pages 2532.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627633.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 849856.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology, pages 173180.
P. Turney. 2006. Similarity of semantic relations. Com-
putational Linguistics, 32(3):379416.
F. Saric, G. Glavas, M. Karan, J. Snajder, and B. Dal-
belo Basic. 2012. Takelab: Systems for measuring
semantic text similarity. In Proc. SemEval, pages 441
448.
F. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine-learning approach to textual en-
tailment recognition. Natural Language Engineering,
15(4):551582.
</reference>
<page confidence="0.992803">
108
</page>
<figure confidence="0.247564">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.062714">
<note confidence="0.977134333333333">b&apos;Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 103108, Atlanta, Georgia, June 13-14, 2013. c 2013 Association for Computational Linguistics</note>
<title confidence="0.873267">DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level Semantic Similarity Estimation</title>
<note confidence="0.729091444444445">Nikolaos Malandrakis1 , Elias Iosif2 , Vassiliki Prokopi2 , Alexandros Potamianos2 , Shrikanth Narayanan1 1 Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA 2</note>
<affiliation confidence="0.783129">Department of ECE, Technical University of Crete, 73100 Chania, Greece</affiliation>
<email confidence="0.934467">malandra@usc.edu,iosife@telecom.tuc.gr,vprokopi@isc.tuc.gr,potam@telecom.tuc.gr,shri@sipi.usc.edu</email>
<abstract confidence="0.997179384615385">This paper describes our submission for the *SEM shared task of Semantic Textual Similarity. We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, 3) string similarity metrics, 4) affective content similarity and 5) sentence length. Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>c 2013 Association for Computational Linguistics DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level tional Linguistics,</title>
<date>2013</date>
<booktitle>b&apos;Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task,</booktitle>
<pages>103108</pages>
<location>Atlanta, Georgia,</location>
<marker>2013</marker>
<rawString>b&apos;Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 103108, Atlanta, Georgia, June 13-14, 2013. c 2013 Association for Computational Linguistics DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level tional Linguistics, pages 363370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>A Hickl</author>
</authors>
<title>Methods for Using Textual Entailment in Open-Domain Question Answering.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>905912</pages>
<marker>Harabagiu, Hickl, 2006</marker>
<rawString>S. Harabagiu and A. Hickl. 2006. Methods for Using Textual Entailment in Open-Domain Question Answering. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 905912.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<marker>Harris, 1954</marker>
<rawString>Z. Harris. 1954. Distributional structure. Word, 10(23):146162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Iosif</author>
<author>A Potamianos</author>
</authors>
<title>Unsupervised semantic similarity computation between terms using web documents.</title>
<date>2010</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>22</volume>
<issue>11</issue>
<marker>Iosif, Potamianos, 2010</marker>
<rawString>E. Iosif and A. Potamianos. 2010. Unsupervised semantic similarity computation between terms using web documents. IEEE Transactions on Knowledge and Data Engineering, 22(11):16371647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Iosif</author>
<author>A Potamianos</author>
</authors>
<title>Semsim: Resources for normalized semantic similarity computation using lexical networks.</title>
<date>2012</date>
<booktitle>In Proc. Eighth International Conference on Language Resources and Evaluation,</booktitle>
<pages>34993504</pages>
<marker>Iosif, Potamianos, 2012</marker>
<rawString>E. Iosif and A. Potamianos. 2012. Semsim: Resources for normalized semantic similarity computation using lexical networks. In Proc. Eighth International Conference on Language Resources and Evaluation, pages 34993504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elias Iosif</author>
<author>Alexandros Potamianos</author>
</authors>
<title>Similarity Computation Using Semantic Networks Created From Web-Harvested Data. Natural Language Engineering,</title>
<date>2013</date>
<location>(submitted).</location>
<marker>Iosif, Potamianos, 2013</marker>
<rawString>Elias Iosif and Alexandros Potamianos. 2013. Similarity Computation Using Semantic Networks Created From Web-Harvested Data. Natural Language Engineering, (submitted).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Iosif</author>
<author>A Potamianos</author>
<author>M Giannoudaki</author>
<author>K Zervanou</author>
</authors>
<title>Semantic similarity computation for abstract and concrete nouns using network-based distributional semantic models.</title>
<date>2013</date>
<booktitle>In 10th International Conference on Computational Semantics (IWCS),</booktitle>
<pages>328334</pages>
<marker>Iosif, Potamianos, Giannoudaki, Zervanou, 2013</marker>
<rawString>E. Iosif, A. Potamianos, M. Giannoudaki, and K. Zervanou. 2013. Semantic similarity computation for abstract and concrete nouns using network-based distributional semantic models. In 10th International Conference on Computational Semantics (IWCS), pages 328334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL 04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL 04, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Malakasiotis</author>
<author>I Androutsopoulos</author>
</authors>
<title>Learning textual entailment using svms and string similarity measures.</title>
<date>2007</date>
<booktitle>In Proceedings of of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>4247</pages>
<marker>Malakasiotis, Androutsopoulos, 2007</marker>
<rawString>P. Malakasiotis and I. Androutsopoulos. 2007. Learning textual entailment using svms and string similarity measures. In Proceedings of of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 4247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Malakasiotis</author>
</authors>
<title>Paraphrase recognition using machine learning to combine similarity measures.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of ACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP,</booktitle>
<pages>4247</pages>
<marker>Malakasiotis, 2009</marker>
<rawString>P. Malakasiotis. 2009. Paraphrase recognition using machine learning to combine similarity measures. In Proceedings of the 47th Annual Meeting of ACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP, pages 4247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Malandrakis</author>
<author>A Potamianos</author>
<author>E Iosif</author>
<author>S Narayanan</author>
</authors>
<title>Kernel models for affective lexicon creation.</title>
<date>2011</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>29772980</pages>
<marker>Malandrakis, Potamianos, Iosif, Narayanan, 2011</marker>
<rawString>N. Malandrakis, A. Potamianos, E. Iosif, and S. Narayanan. 2011. Kernel models for affective lexicon creation. In Proc. Interspeech, pages 29772980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Malandrakis</author>
<author>E Iosif</author>
<author>A Potamianos</author>
</authors>
<title>DeepPurple: Estimating sentence semantic similarity using n-gram regression models and web snippets.</title>
<date>2012</date>
<booktitle>In Proc. Sixth International Workshop on Semantic Evaluation (SemEval) The First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>565570</pages>
<marker>Malandrakis, Iosif, Potamianos, 2012</marker>
<rawString>N. Malandrakis, E. Iosif, and A. Potamianos. 2012. DeepPurple: Estimating sentence semantic similarity using n-gram regression models and web snippets. In Proc. Sixth International Workshop on Semantic Evaluation (SemEval) The First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Meng</author>
<author>K-C Siu</author>
</authors>
<title>Semi-automatic acquisition of semantic structures for understanding domain\x0cspecific natural language queries.</title>
<date>2002</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>14</volume>
<issue>1</issue>
<marker>Meng, Siu, 2002</marker>
<rawString>H. Meng and K.-C. Siu. 2002. Semi-automatic acquisition of semantic structures for understanding domain\x0cspecific natural language queries. IEEE Transactions on Knowledge and Data Engineering, 14(1):172181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>W Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1998</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<marker>Miller, Charles, 1998</marker>
<rawString>G. Miller and W. Charles. 1998. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<marker>Miller, 1990</marker>
<rawString>G. Miller. 1990. Wordnet: An on-line lexical database. International Journal of Lexicography, 3(4):235312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mirkin</author>
<author>L Specia</author>
<author>N Cancedda</author>
<author>I Dagan</author>
<author>M Dymetman</author>
<author>S Idan</author>
</authors>
<title>Source-language entailment modeling for translating unknown terms.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of ACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP,</booktitle>
<pages>791799</pages>
<marker>Mirkin, Specia, Cancedda, Dagan, Dymetman, Idan, 2009</marker>
<rawString>S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymetman, and S. Idan. 2009. Source-language entailment modeling for translating unknown terms. In Proceedings of the 47th Annual Meeting of ACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP, pages 791799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311318</pages>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Perez</author>
<author>E Alfonseca</author>
</authors>
<title>Application of the bleu algorithm for recognizing textual entailments.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Worshop on Recognising Textual Entailment.</booktitle>
<marker>Perez, Alfonseca, 2005</marker>
<rawString>D. Perez and E. Alfonseca. 2005. Application of the bleu algorithm for recognizing textual entailments. In Proceedings of the PASCAL Challenges Worshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxanomy.</title>
<date>1995</date>
<booktitle>In Proc. of International Joint Conference for Artificial Intelligence,</booktitle>
<pages>448453</pages>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxanomy. In Proc. of International Joint Conference for Artificial Intelligence, pages 448453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rinaldi</author>
<author>J Dowdall</author>
<author>K Kaljurand</author>
<author>M Hess</author>
<author>D Molla</author>
</authors>
<title>Exploiting paraphrases in a question answering system.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2nd International Workshop on Paraphrasing,</booktitle>
<pages>2532</pages>
<marker>Rinaldi, Dowdall, Kaljurand, Hess, Molla, 2003</marker>
<rawString>F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and D. Molla. 2003. Exploiting paraphrases in a question answering system. In Proceedings of the 2nd International Workshop on Paraphrasing, pages 2532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H. Rubenstein and J. B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>I Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>849856</pages>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>I. Szpektor and I. Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 849856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>173180</pages>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 173180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<marker>Turney, 2006</marker>
<rawString>P. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Saric</author>
<author>G Glavas</author>
<author>M Karan</author>
<author>J Snajder</author>
<author>B Dalbelo Basic</author>
</authors>
<title>Takelab: Systems for measuring semantic text similarity.</title>
<date>2012</date>
<booktitle>In Proc. SemEval,</booktitle>
<pages>441--448</pages>
<marker>Saric, Glavas, Karan, Snajder, Basic, 2012</marker>
<rawString>F. Saric, G. Glavas, M. Karan, J. Snajder, and B. Dalbelo Basic. 2012. Takelab: Systems for measuring semantic text similarity. In Proc. SemEval, pages 441 448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Zanzotto</author>
<author>M Pennacchiotti</author>
<author>A Moschitti</author>
</authors>
<title>A machine-learning approach to textual entailment recognition.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>4</issue>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2009</marker>
<rawString>F. Zanzotto, M. Pennacchiotti, and A. Moschitti. 2009. A machine-learning approach to textual entailment recognition. Natural Language Engineering, 15(4):551582.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>