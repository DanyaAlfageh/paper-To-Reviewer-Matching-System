<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.64767">
b&apos;Proceedings of the 43rd Annual Meeting of the ACL, pages 205214,
Ann Arbor, June 2005. c
</bodyText>
<sectionHeader confidence="0.421701" genericHeader="abstract">
2005 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.910597">
Experiments with Interactive Question-Answering
</title>
<author confidence="0.987865">
Sanda Harabagiu, Andrew Hickl, John Lehmann, and Dan Moldovan
</author>
<affiliation confidence="0.947778">
Language Computer Corporation
</affiliation>
<address confidence="0.59139">
Richardson, Texas USA
</address>
<email confidence="0.993764">
sanda@languagecomputer.com
</email>
<sectionHeader confidence="0.990729" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999195428571429">
This paper describes a novel framework
for interactive question-answering (Q/A)
based on predictive questioning. Gen-
erated off-line from topic representations
of complex scenarios, predictive ques-
tions represent requests for information
that capture the most salient (and diverse)
aspects of a topic. We present experimen-
tal results from large user studies (featur-
ing a fully-implemented interactive Q/A
system named FERRET) that demonstrates
that surprising performance is achieved by
integrating predictive questions into the
context of a Q/A dialogue.
</bodyText>
<sectionHeader confidence="0.998281" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999708958333333">
In this paper, we propose a new architecture for
interactive question-answering based on predictive
questioning. We present experimental results from
a currently-implemented interactive Q/A system,
named FERRET, that demonstrates that surprising
performance is achieved by integrating sources of
topic information into the context of a Q/A dialogue.
In interactive Q/A, professional users engage in
extended dialogues with automatic Q/A systems in
order to obtain information relevant to a complex
scenario. Unlike Q/A in isolation, where the per-
formance of a system is evaluated in terms of how
well answers returned by a system meet the specific
information requirements of a single question, the
performance of interactive Q/A systems have tradi-
tionally been evaluated by analyzing aspects of the
dialogue as a whole. Q/A dialogues have been evalu-
ated in terms of (1) efficiency, defined as the number
of questions that the user must pose to find particu-
lar information, (2) effectiveness, defined by the rel-
evance of the answers returned, (3) user satisfaction.
In order to maximize performance in these three
areas, interactive Q/A systems need a predictive di-
alogue architecture that enables them to propose re-
lated questions about the relevant information that
could be returned to a user, given a domain of inter-
est. We argue that interactive Q/A systems depend
on three factors: (1) the effective representation of
the topic of a dialogue, (2) the dynamic recognition
of the structure of the dialogue, and (3) the ability to
return relevant answers to a particular question.
In this paper, we describe results from experi-
ments we conducted with our own interactive Q/A
system, FERRET, under the auspices of the ARDA
AQUAINT1 program, involving 8 different dialogue
scenarios and more than 30 users. The results pre-
sented here illustrate the role of predictive question-
ing in enhancing the performance of Q/A interac-
tions.
In the remainder of this paper, we describe a new
architecture for interactive Q/A. Section 2 presents
the functionality of several of FERRETs modules
and describes the NLP techniques it relies upon. In
Section 3, we present one of the dialogue scenar-
ios and the topic representations we have employed.
Section 4 highlights the management of the inter-
action between the user and FERRET, while Sec-
tion 5 presents the results of evaluating our proposed
</bodyText>
<page confidence="0.788345">
1
</page>
<bodyText confidence="0.7087725">
AQUAINT is an acronym for Advanced QUestion Answer-
ing for INTelligence.
</bodyText>
<page confidence="0.888008">
205
</page>
<figure confidence="0.998255633333333">
\x0cDialogue
Management
Collection
Document
Question
Similarity
Answer
Fusion
(PDN)
Network
Dialogue
Predictive
Answer
Fusion
Context
Management
Dialogue Shell
Online Question Answering
Topic
Predictive Dialogue
Question
Answer
Decomposition
Question
Information
Extraction
Representation
Offline Question Answering
Database (QUAB)
QuestionAnswer
</figure>
<figureCaption confidence="0.999161">
Figure 1: FERRET - A Predictive Interactive Question-Answering Architecture.
</figureCaption>
<bodyText confidence="0.894073">
model, and Section 6 summarizes the conclusions.
</bodyText>
<sectionHeader confidence="0.999283" genericHeader="method">
2 Interactive Question-Answering
</sectionHeader>
<bodyText confidence="0.999633655172414">
We have found that the quality of interactions pro-
duced by an interactive Q/A system can be greatly
enhanced by predicting the range of questions that
a user might ask in the context of a given topic.
If a large database of topic-relevant questions were
available for a wide variety of topics, the accuracy
of a state-of-the-art Q/A system such as (Harabagiu
et al., 2003) could be enhanced.
In FERRET, our interactive Q/A system, we store
such predicted pairs of questions and answers in a
database known as the Question Answer Database
(or QUAB). FERRET uses this large set of topic-
relevant question-and-answer pairs to improve the
interaction with the user by suggesting new ques-
tions. For example, when a user asks a question
like (Q1) (as illustrated in Table 1), FERRET returns
an answer to the question (A1) and proposes (Q2),
(Q3), and (Q4) as suggestions of possible continua-
tions of the dialogue. Users then choose how to con-
tinue the interaction by either (1) ignoring the sug-
gestions made by the system and proposing a differ-
ent question, or by (2) selecting one of the proposed
questions and examining its answer.
Figure 1 illustrates the architecture of FERRET.
The interactions are managed by a dialogue shell,
which processes questions by transforming them
into their corresponding predicate-argument struc-
tures2.
The data collection used in our experiments was
</bodyText>
<page confidence="0.96637">
2
</page>
<bodyText confidence="0.976779">
We have employed the same representation of predicate-
argument structures as those encoded in PropBank. We use a
semantic parser (described in (Surdeanu et al., 2003)) that rec-
</bodyText>
<listItem confidence="0.920988545454545">
ognizes predicate-argument structures.
(Q1) What weapons are included in Egypts stockpiles?
(A1) The Israelis point to comments made by former President Anwar Sadat,
who in 1970 stated that Egypt has biological weapons stored in
refrigerators ready to use against Israel if need be. The program might
include plague, botulism toxin, encephalitis virus, anthrax,
Rift Valley fever and mycotoxicosis.
(Q2) Where did Egypt inherit its first stockpiles of chemical weapons?
(Q3) Is there evidence that Egypt has dismantled its stockpiles of weapons?
(Q4) Where are Egypts weapons stockpiles located?
(Q5) Who oversees Egypts weapons stockpiles?
</listItem>
<tableCaption confidence="0.983317">
Table 1: User question and proposed questions from QUABs
</tableCaption>
<bodyText confidence="0.993415">
made available by the Center for Non-Proliferation
Studies (CNS)3.
Modules from the FERRETs dialogue shell inter-
act with modules from the predictive dialogue block.
Central to the predictive dialogue is the topic repre-
sentation for each scenario, which enables the pop-
ulation of a Predictive Dialogue Network (PDN).
The PDN consists of a large set of questions that
were asked or predicted for each topic. It is a net-
work because questions are related by similarity
links, which are computed by the Question Simi-
larity module. The topic representation enables an
Information Extraction module based on (Surdeanu
and Harabagiu, 2002) to find topic-relevant infor-
mation in the document collection and to use it as
answers for the QUABs. The questions associated
with each predicted answer are generated from pat-
terns that are related to the extraction patterns used
for identifying topic relevant information. The qual-
ity of the dialog between the user and FERRET de-
pends on the quality of the topic representations and
the coverage of the QUABs.
</bodyText>
<page confidence="0.982375">
3
</page>
<bodyText confidence="0.802159">
The Center for Non-Proliferation Studies at the Monterrey
Institute of International Studies distributes collections of print
and online documents on weapons of mass destruction. More
information at: http://cns.miis.edu.
</bodyText>
<page confidence="0.999014">
206
</page>
<sectionHeader confidence="0.80616" genericHeader="method">
\x0cGENERAL BACKGROUND
</sectionHeader>
<listItem confidence="0.947218333333333">
1) Country Profile
3) Military Operations: Army, Navy, Air Force, Leaders, Capabilities, Intentions
4) Allies/Partners: Coalition Forces
5) Weapons: Chemical, Biological, Materials, Stockpiles, Facilities, Access, Research Efforts, Scientists
6) Citizens: Population, Growth Rate, Education
8) Economics: Growth Domestic Product, Growth Rate, Imports
9) Threat Perception: Border and Surrounding States, International, Terrorist Groups
10) Behaviour: Threats, Invasions, Sponsorship and Harboring of Bad Actors
13) Leadership:
7) Industrial: Major Industrires, Exports, Power Sources
14) Behaviour: Threats to use WMDs, Actual Usage, Sophistication of Attack, Anectodal or Simultaneous
Serving as a background to the scenarios, the following list contains subject areas that may be relevant
to the scenarios under examination, and it is provided to assist the analyst in generating questions.
2) Government: Type of, Leadership, Relations
SCENARIO: Assessment of Egypts Biological Weapons
</listItem>
<bodyText confidence="0.933190833333333">
As terrorist Activity in Egypt increases, the Commander
of the United States Army believes a better understanding
of Egypts Military capabilities is needed. Egypts
biological weapons database needs to be updated to
correspond with the Commanders request. Focus your
investigation on Egypts access to old technology,
assistance received from the Soviet Union for development
of their pharmaceutical infrastructure, production of
toxins and BW agents, stockpiles, exportation of these
materials and development technology to Middle Eastern
countries, and the effect that this information will have on
the United States and Coalition Forces in the Middle East.
</bodyText>
<listItem confidence="0.8979028">
Please incorporate any other related information to
your report.
11) Transportation Infrastructure: Kilometers of Road, Rail, Air Runways, Harbors and Ports, Rivers
12) Beliefs: Ideology, Goals, Intentions
15) Weapons: Chemical, Bilogical, Materials, Stockpiles, Facilities, Access
</listItem>
<figureCaption confidence="0.985265">
Figure 2: Example of a Dialogue Scenario.
</figureCaption>
<sectionHeader confidence="0.984625" genericHeader="method">
3 Modeling the Dialogue Topic
</sectionHeader>
<bodyText confidence="0.989628045454545">
Our experiments in interactive Q/A were based on
several scenarios that were presented to us as part
of the ARDA Metrics Challenge Dialogue Work-
shop. Figure 2 illustrates one of these scenarios. It
is to be noted that the general background consists
of a list of subject areas, whereas the scenario is a
narration in which several sub-topics are identified
(e.g. production of toxins or exportation of materi-
als). The creation of scenarios for interactive Q/A
requires several different types of domain-specific
knowledge and a level of operational expertise not
available to most system developers. In addition to
identifying a particular domain of interest, scenar-
ios must specify the set of relevant actors, outcomes,
and related topics that are expected to operate within
the domain of interest, the salient associations that
may exist between entities and events in the sce-
nario, and the specific timeframe and location that
bound the scenario in space and time. In addition,
real-world scenarios also need to identify certain op-
erational parameters as well, such as the identity of
the scenarios sponsor (i.e. the organization spon-
soring the research) and audience (i.e. the organiza-
tion receiving the information), as well as a series of
evidence conditions which specify how much verifi-
cation information must be subject to before it can
be accepted as fact. We assume the set of sub-topics
mentioned in the general background and the sce-
nario can be used together to define a topic structure
that will govern future interactions with the Q/A sys-
tem. In order to model this structure, the topic rep-
resentation that we create considers separate topic
signatures for each sub-topic.
The notion of topic signatures was first introduced
in (Lin and Hovy, 2000). For each subtopic in a sce-
nario, given (a) documents relevant to the sub-topic
and (b) documents not relevant to the subtopic, a sta-
tistical method based on the likelihood ratio is used
to discover a weighted list of the most topic-specific
concepts, known as the topic signature. Later work
by (Harabagiu, 2004) demonstrated that topic sig-
natures can be further enhanced by discovering the
most relevant relations that exist between pairs of
concepts. However, both of these types of topic rep-
resentations are limited by the fact that they require
the identification of topic-relevant documents prior
to the discovery of the topic signatures. In our ex-
periments, we were only presented with a set of doc-
uments relevant to a particular scenario; no further
relevance information was provided for individual
subject areas or sub-topics.
In order to solve the problem of finding relevant
documents for each subtopic, we considered four
different approaches:
Approach 1: All documents in the CNS col-
lection were initially clustered using K-Nearest
Neighbor (KNN) clustering (Dudani, 1976).
Each cluster that contained at least one key-
word that described the sub-topic was deemed
relevant to the topic.
Approach 2: Since individual documents may
contain discourse segments pertaining to differ-
ent sub-topics, we first used TextTiling (Hearst,
1994) to automatically segment all of the doc-
uments in the CNS collection into individual
text tiles. These individual discourse segments
</bodyText>
<page confidence="0.991789">
207
</page>
<bodyText confidence="0.923193833333334">
\x0cthen served as input to the KNN clustering al-
gorithm described in Approach 1.
Approach 3: In this approach, relevant docu-
ments were discovered simultaneously with the
discovery of topic signatures. First, we asso-
ciated a binary seed relation \x02\x01 for each each
sub-topic
\x03
\x01 . (Seed relations were created both
by hand and using the method presented in
(Harabagiu, 2004).) Since seed relations are by
definition relevant to a particular subtopic, they
can be used to determine a binary partition of
the document collection \x04 into (1) a relevant
set of documents \x05\x06\x01 (that is, the documents rel-
evant to relation \x01 ) and (2) a set of non-relevant
documents \x04 -\x05\x06\x01 . Inspired by the method pre-
sented in (Yangarber et al., 2000), a topic sig-
nature (as calculated by (Harabagiu, 2004)) is
then produced for the set of documents in \x05\x07\x01 .
For each subtopic
\x03
\x01 defined as part of the di-
alogue scenario, documents relevant to a cor-
responding seed relation \x01 are added to \x05 iff
the relation \x08\x01 meets the density criterion (as
defined in (Yangarber et al., 2000)). If \t rep-
resents the set of documents where \x02\x01 is recog-
nized, then the density criterion can be defined
as:
</bodyText>
<equation confidence="0.929374833333333">
\x0b
\x0c\x0f\x0e\x10
\x0b
\x0c\x0f\x11\x13\x12
\x0e\x10
\x11\x14
</equation>
<bodyText confidence="0.975691418604651">
. Once \t is added to \x05\x15\x01 , then
a new topic signature is calculated for \x05 . Rela-
tions extracted from the new topic signature can
then be used to determine a new document par-
tition by re-iterating the discovery of the topic
signature and of the documents relevant to each
subtopic.
Approach 4: Approach 4 implements the tech-
nique described in Approach 3, but operates
at the level of discourse segments (or texttiles)
rather than at the level of full documents. As
with Approach 2, segments were produced us-
ing the TextTiling algorithm.
In modeling the dialogue scenarios, we consid-
ered three types of topic-relevant relations: (1)
structural relations, which represent hypernymy
or meronymy relations between topic-relevant con-
cepts, (2) definition relations, which uncover the
characteristic properties of a concept, and (3) ex-
traction relations, which model the most relevant
events or states associated with a sub-topic. Al-
though structural relations and definition relations
are discovered reliably using patterns available from
our Q/A system (Harabagiu et al., 2003), we found
only extraction relations to be useful in determining
the set of documents relevant to a subtopic. Struc-
tural relations were available from concept ontolo-
gies implemented in the Q/A system. The definition
relations were identified by patterns used for pro-
cessing definition questions.
Extraction relations are discovered by processing
documents in order to identify three types of rela-
tions, including: (1) syntactic attachment relations
(including subject-verb, object-verb, and verb-PP
relations), (2) predicate-argument relations, and (3)
salience-based relations that can be used to encode
long-distance dependencies between topic-relevant
concepts. (Salience-based relations are discovered
using a technique first reported in (Harabagiu, 2004)
which approximates a Centering Theory-style ap-
proach (Kameyama, 1997) to the resolution of
coreference.)
Subtopic: Egypts production of toxins and BW agents
</bodyText>
<figure confidence="0.978925">
Topic Signature:
produce phosphorous trichloride (TOXIN)
house ORGANIZATION
cultivate nonpathogenic Bacilus Subtilis (TOXIN)
produce mycotoxins (TOXIN)
acquire FACILITY
Subtopic: Egypts allies and partners
Topic Signature:
provide COUNTRY
cultivate COUNTRY
supply precursors
cooperate COUNTRY
train PERSON
supply knowhow
</figure>
<figureCaption confidence="0.999966">
Figure 3: Example of two topic signatures acquired
</figureCaption>
<bodyText confidence="0.99276">
for the scenario illustrated in Figure 2.
We made the extraction relations associated with
each topic signature more general (a) by replacing
words with their (morphological) root form (e.g.
wounded with wound, weapons with weapon), (b)
by replacing lexemes with their subsuming category
from an ontology of 100,000 words (e.g. truck is re-
placed by VEHICLE, ARTIFACT, or OBJECT), and (c)
by replacing each name with its name class (Egypt
with COUNTRY). Figure 3 illustrates the topic sig-
natures resulting for the scenario illustrated in Fig-
ure 2.
Once extraction relations were obtained for a par-
ticular set of documents, the resulting set of re-
lations were ranked according to a method pro-
posed in (Yangarber, 2003). Under this approach,
</bodyText>
<page confidence="0.981663">
208
</page>
<bodyText confidence="0.817726">
\x0cthe score associated with each relation is given by:
</bodyText>
<figure confidence="0.846632298507463">
\x02\x01\x04\x03 \x06\x05
\x07
\t\x08\x0b
\x0c\x0e
\x10\x0f\x12\x11\x14\x13\x16\x15
\x0b
\x18\x17\x1a\x19\x1c\x1b\x0e\x1d\t\x1e
\x03 \x1f\x06! \x07
\x06\x08 , where &quot; \t#&quot; rep-
resents the cardinality of the documents where the
relation is identified, and
\x03 \x1f\t! \x07
\t\x08 represents sup-
port associated with the relation .
\x03 \x1f\t! \x07
\t\x08 is de-
fined as the sum of the relevance of each document
in \t :
\x03 \x1f\t! \x07
\t\x08$
%\&apos;&amp;)( \x0b \x05*\x05,+
\x07.-
\x08 . The relevance
of a document that contains a topic-significant re-
lation can be defined as: \x05*\x05/+
\x07.-
\x080
214365
\x13
(,7
\x0c
\x07
143
8
\x06\x05 \x01 \x07
\t\x089\x08 , where :
\x03
represents the topic signature
of the subtopic4. The accuracy of the relation, then,
is given by:
8
\x06\x05 \x01 \x07
\t\x08;
&lt;
\x0b
\x07
%\&apos;&amp;)( \x0b \x05*\x05/+&gt;=@?
\x07.-
\x08A3
%CBEDF \x01 \x05*\x05/+&gt;=HG
\x07.-
\x089\x08 . Here, \x05*\x05,+ \x0c ?
\x07.-
\x08 measures the rel-
evance of a subtopic
\x03
\x01 to a particular document
-
,
while \x05*\x05/+ \x0c G
\x07.-
\x08 measures the relevance of
-
to an-
other subtopic,
\x03
B .
</figure>
<bodyText confidence="0.9240219">
We use a different learner for each subtopic in or-
der to train simultaneously on each iteration. (The
calculation of topic signatures continues to iterate
until there are no more relations that can be added
to the overall topic signature.) When the precision
of a relation to a subtopic
\x03
\x01 is computed, it takes
into account the negative evidence of its relevance
to any other subtopic
</bodyText>
<equation confidence="0.579424">
\x03
\x01JI
\x03
B . If
8
\x0e\x05 \x01 \x07
\x06\x08JKML ,
</equation>
<bodyText confidence="0.9675725">
the relation is not included in the topic signature,
where relations are ranked by the score
</bodyText>
<equation confidence="0.958774">
\x03 \x01)\x03 \x06\x05
\x07
\t\x08N
8
\x06\x05 \x01 \x07
\t\x08 \x17 + \x03\x02O \x07 \x03 \x1f\t! \x07
\t\x089\x08 .
</equation>
<bodyText confidence="0.9962785">
Representing topics in terms of relevant concepts
and relations is important for the processing of ques-
tions asked within the context of a given topic. For
interactive Q/A, however, the ideal topic-structured
representation would be in the form of question-
answer pairs (QUABs) that model the individual
segments of the scenario. We have currently cre-
ated two sets of QUABs: a handcrafted set and
an automatically-generated set. For the manually-
created set of QUABs, 4 linguists manually gener-
ated 3210 question-answer pairs for each of the 8
dialogue scenarios considered in our experiments.
In a separate effort, we devised a process for au-
tomatically populating the QUAB for each scenario.
In order to generate question-answer pairs for each
subtopic, we first identified relevant text passages in
the document collection to serve as answers and
then generated individual questions that could be an-
</bodyText>
<page confidence="0.961935">
4
</page>
<bodyText confidence="0.88958125">
Initially, P Q contains only the seed relation. Additional
relations can be added with each iteration.
swered by each answer passage.
R
Answer Identification: We defined an an-
swer passage as a contiguous sequence of sentences
with a positive answer rank and a passage price
of K 4. To select answer passages for each sub-
</bodyText>
<equation confidence="0.895453777777778">
topic
\x03
\x01 , we calculate an answer rank, \x0eSUTWV
\x07
SX\x08Y
%
\x13 ?
\x02\x01\x04\x03 \x06\x05
\x07
</equation>
<bodyText confidence="0.983207571428571">
\x08\x01Z\x08 , that sums across the scores of each
relation from the topic signature that is identified in
the same text window. Initially, the text window
is set to one sentence. (If the sentence is part of a
quote, however, the text window is immediately ex-
panded to encompass the entire sentence that con-
tains the quote.) Each passage with \x0eSUTWV
</bodyText>
<equation confidence="0.9035705">
\x07
SX\x08\\[]L is
</equation>
<bodyText confidence="0.899175789473684">
then considered to be a candidate answer passage.
The text window of each candidate answer passage
is then expanded to include the following sentence.
If the answer rank does not increase with the addi-
tion of the succeeding sentence, then the price (
!
) of
the candidate answer passage is incremented by 1,
otherwise it is decremented by 1. The text window
of each candidate answer passage continues to ex-
pand until
!
_^ . Before the ranked list of candidate
answers can be considered by the Question Genera-
tion module, answer passages with a positive price
!
are stripped of the last
!
sentences.
</bodyText>
<sectionHeader confidence="0.823321" genericHeader="method">
ANSWER
</sectionHeader>
<bodyText confidence="0.9805865">
In the early 1970s, Egyptian President Anwar Sadat
validates that Egypt has a BW stockpile.
</bodyText>
<table confidence="0.921618212121212">
PredicateArgument Structures
P1: validate
arguments: A0 = E2: Answer Type: Definition
A1 = P2: have
arguments: A0 = E3
A1 = E4
ArgMTMP: E1: Answer Type: Time
P3: admit
Reference 4 (relational)
Egyptian President X
E5: BW program
Reference 2 (metonymic)
Reference 3 (partwhole)
QUESTIONS
Definition Pattern: Who is X?
Q1: Who is Anwar Sadat?
Pattern: When did E3 P1 to P2 E4?
Q2: When did Egypt validate to having BW stockpiles?
Pattern: When did E3 P3 to P2 E4?
Q3: When did Egypt admit to having BW stockpiles?
Pattern: When did E3 P3 to P2 E5?
Q4: When did Egypt admint to having a BW program?
E1: &quot;in the early 1970s&quot;; Category: TIME
E2: &quot;Egyptian President Anwar Sadat&quot;; Category: PERSON
E3: &quot;Egypt&quot;; Category: COUNTRY
E4: &quot;BW stockpile&quot;; Category: UNKNOWN
4 entities
2 predicates: P1=&quot;validate&quot;; P2=&quot;has&quot;
PROCESSING
Reference 1 (definitional)
Figure 4: Associating Questions with Answers.
R
Question Generation: In order to automati-
</table>
<listItem confidence="0.820607666666667">
cally generate questions from answer passages, we
considered the following two problems:
Problem 1: Every word in an answer passage
can refer to an entity, a relation, or an event. In
order for question generation be successful, we
must determine whether a particular reference
</listItem>
<page confidence="0.988394">
209
</page>
<bodyText confidence="0.991041444444444">
\x0cis interesting enough to the scenario such that
it deserves to be mentioned in a topic-relevant
question. For example, Figure 4 illustrates an
answer that includes two predicates and four
entities. In this case, four types of reference are
used to associate these linguistic objects with
other related objects: (a) definitional reference,
used to link entity (E1) Anwar Sadat to a cor-
responding attribute Egyptian President, (b)
metonymic reference, since (E1) can be coerced
into (E2), (c) part-whole reference, since BW
stockpiles(E4) necessarily imply the existence
of a BW program(E5), and (d) relational ref-
erence, since validating is subsumed as part
of the meaning of declaring (as determined by
WordNet glosses), while admitting can be de-
fined in terms of declaring, as in declaring [to
be true].
</bodyText>
<sectionHeader confidence="0.631903" genericHeader="method">
ANSWER
</sectionHeader>
<table confidence="0.6844746">
Egyptian Deputy Minister Mahmud Salim states that Egypts
Egyptians have &quot;adequate means of retaliating without delay&quot;.
enemies would never use BW because they are aware that the
Predicates: P1=state; P2 = never use; P3 = be aware;
Causality:
P2(BW) = NONNEGATIVE RESULT(P5); P5 = &quot;obstacle&quot;
Reference: P1 P6 = view
QUESTIONS
Does Egypt view the possesion of BW as an obstacle?
Does Egypt view the possesion of BW as a deterrent?
P4 = have P&quot;4 = &quot;the possesion&quot;
P&quot;4 = &quot;the possesion&quot; = nominalization(P4) = EFFECT(P2(BW))
PROCESSING
specialization
Pattern: Does Egypt P6 P&quot;4(BW) as a P5?
</table>
<figureCaption confidence="0.6398265">
Figure 5: Questions for Implied Causal Relations.
Problem 2: We have found that the identifica-
</figureCaption>
<bodyText confidence="0.995760909090909">
tion of the association between a candidate an-
swer and a question depends on (a) the recogni-
tion of predicates and entities based on both the
output of a named entity recognizer and a se-
mantic parser (Surdeanu et al., 2003) and their
structuring into predicate-argument frames, (b)
the resolution of reference (addressed in Prob-
lem 1), (c) the recognition of implicit rela-
tions between predications stated in the answer.
Some of these implicit relations are referential,
as is the relation between predicates
</bodyText>
<page confidence="0.786941">
8
</page>
<equation confidence="0.908506666666667">
&lt;
and
8\x01
</equation>
<bodyText confidence="0.986029166666667">
illustrated in Figure 4. A special case of im-
plicit relations are the causal relations. Fig-
ure 5 illustrates an answer where a causal re-
lation exists and is marked by the cue phrase
because. Predicates like those in Figure 5
can be phrasal (like
</bodyText>
<figure confidence="0.629053736842105">
8\x03\x02
) or negative (like
8\x03\x02\x1e ).
Causality is established between predicates
8 \x02\x1e
and
8\x05\x04
as they are the ones that ultimately de-
termine the selection of the answer. The predi-
cate
! \x02\x04 can be substituted by its nominalization
since
\x06 O
&lt;
of
8 \x1e is BW, the same argument is
transferred to
8\x07\x02 \x02
\x04 . The causality implied by the
</figure>
<bodyText confidence="0.8544035">
answer from Figure 5 has two components: (1)
the effect (i.e. the predicate
</bodyText>
<equation confidence="0.601259">
8 \x02 \x02
\x04 ) and (2) the re-
</equation>
<bodyText confidence="0.937073291666667">
sult, which eliminates the semantic effect of the
negative polarity item never by implying the
predicate
!\t\x08
, obstacle. The questions that are
generated are based on question patterns asso-
ciated with causal relations and therefore allow
different degrees for the specificity of the resul-
tative, i.e obstacle or deterrent.
We generated several questions for each answer
passage. Questions were generated based on pat-
terns that were acquired to model interrogations
using relations between predicates and their argu-
ments. Such interrogations are based on (1) as-
sociations between the answer type (e.g. DATE)
and the question stem (e.g. when and (2) the
relation between predicates, question stem and the
words that determine the answer type (Narayanan
and Harabagiu, 2004). In order to obtain these
predicate-argument patterns, we used 30% (approxi-
mately 1500 questions) of the handcrafted question-
answer pairs, selected at random from each of the 8
dialogue scenarios. As Figures 4 and 5 illustrate, we
used patterns based on (a) embedded predicates and
</bodyText>
<listItem confidence="0.512565">
(b) causal or counterfactual predicates.
4 Managing Interactive Q/A Dialogues
</listItem>
<bodyText confidence="0.9976516875">
As illustrated in Figure 1, the main idea of man-
aging dialogues in which interactions with the Q/A
system occur is based on the notion of predictions,
i.e. by proposing to the user a small set of questions
that tackle the same subject as her question (as illus-
trated in Table 1). The advantage is that the user can
follow-up with one of the pre-processed questions,
that has a correct answer and resides in one of the
QUABs. This enhances the effectiveness of the dia-
logue. It also may impact on the efficiency, i.e. the
number of questions being asked if the QUABs have
good coverage of the subject areas of the scenario.
Moreover, complex questions, that generally are not
processed with high accuracy by current state-of-
the-art Q/A systems, are associated with predictive
questions that represent decompositions based on
</bodyText>
<page confidence="0.965565">
210
</page>
<bodyText confidence="0.973566777777778">
\x0csimilarities between predicates and arguments of the
original question and the predicted questions.
The selection of the questions from the QUABs
that are proposed for each user question is based on
a similarity-metric that ranks the QUAB questions.
To compute the similarity metric, we have experi-
mented with seven different metrics. The first four
metrics were introduced in (Lytinen and Tomuro,
2002).
</bodyText>
<figure confidence="0.946193086956522">
Similarity Metric 1 is based on two process-
ing steps:
(a) the content words of the questions are
weighted using the \x02\x01\x04\x03
-
\x01 measure used in In-
formation Retrieval \x05 \x01
\x05
\x07
\x01 \x08
\x07
1\x07\x06
\x19\x1c\x1b\x0e\x1d
\x07
\x08\x01\x02\x01Z\x089\x08
\t\x0c\x0b\x02
\x0f\x0e
&amp;\x11\x10
?
, where \x12 is the number of
questions in the QUAB,
-
\x01 \x01 is the num-
</figure>
<figureCaption confidence="0.71173475">
ber of questions containing \x01 and \x02\x01\x02\x01 is
the number of times \x01 appears in the ques-
tion. This allows the user question and any
QUAB question to be transformed into two
</figureCaption>
<figure confidence="0.913402333333333">
vectors, \x13
\x14\x15\x05
\x17\x16\x19\x18
\x05
\x1a
\x18\x19\x1b\x11\x1b\x11\x1b\x11\x18
\x05
\x1d\x1c
\x1e
and \x13\x17\x1f
\x14\x15\x05 \x1f
\x16\x19\x18
\x05 \x1f
\x1a\x1d\x18\x19\x1b\x11\x1b\x11\x1b\x11\x18
\x05 \x1f&quot;!
\x1e
;
(b) the term vector similarity is used to compute
the similarity between the user question and
any question from the QUAB: # \x1b%$
\x07
\x13
\x0f\x18
\x13\x17\x1f \x08J
\x07
% \x01 \x05
?
\x05 \x1f
?
\x08\x02&amp;
\x079\x07
% \x01 \x05
\x1e
?
\x08 \x16
\x1a(\&apos;
\x07
% \x01 \x05
\x1e
\x1f
?
\x08 \x16
\x1a \x08
Similarity Metric 2 is based on the percent of
user question terms that appear in the QUAB
</figure>
<bodyText confidence="0.993248714285714">
question. It is obtained by finding the intersec-
tion of the terms in the term vectors of the two
questions.
Similarity Metric 3 is based on semantic in-
formation available from WordNet. It involves:
(a) finding the minimum path between Word-
Net concepts. Given two terms
</bodyText>
<figure confidence="0.98022953125">
&lt;
and \x1e ,
each with T and ) WordNet senses
\x03
&lt;
*
&lt; \x18\x19\x1b\x11\x1b\x11\x1b\x11\x18
,+.-
and
\x03 \x1e
*
&lt; \x18\x19\x1b\x11\x1b\x11\x1b\x11\x18
\x1d/
-
. The se-
mantic distance between the terms 0
\x07
&lt; \x18
\x1e \x08 is
defined by the minimum of all the possible pair-
wise semantic distances between
\x03
&lt;
and
\x03 \x1e :
0
\x07
&lt; \x18
\x1e \x08
13254
= ?
(
\x0c%\x1676 \x13 G
(
\x0c
\x1a
\t
\x07
\x01
\x18
B \x08 , where
\t
\x07
\x01
\x18
B \x08 is the path length between \x01 and B .
(b) the semantic similarity between the user
question :
8\x14
\x1f
&lt; \x18
\x1f \x1e
\x18\x19\x1b\x11\x1b\x11\x1b\x11\x18
\x1f + \x1e
and the QUAB
question :9\x1f
\x14;:
&lt; \x18
: \x1e
\x18\x19\x1b\x11\x1b\x11\x1b\x11\x18
:&lt;/
\x1e
to be defined
as \x05,)
\x07
:
=\x18
:&gt;\x1f/\x08
? \x11
7
@
6
7
A
\x15CB ? \x11
7
A
6
7
@
\x15
7 @
B
7 A
, where
D \x07
:\x04E
\x18
:&gt;F\x12\x08N
% E (,7HG &lt;
&lt; B\x04IKJ\x0cLNM&lt;OQP M\x04R \x11
E
6
F
\x15
Similarity Metric 4 is based on the question
</figure>
<bodyText confidence="0.951226633333334">
type similarity. Instead of using the question
class, determined by its stem, whenever we
could recognize the answer type expected by
the question, we used it for matching. As back-
off only, we used a question type similarity
based on a matrix akin to the one reported in
(Lytinen and Tomuro, 2002)
Similarity Metric 5 is based on question con-
cepts rather than question terms. In order to
translate question terms into concepts, we re-
placed (a) question stems (i.e. a WH-word +
NP construction) with expected answer types
(taken from the answer type hierarchy em-
ployed by FERRETs Q/A system) and (b)
named entities with corresponding their corre-
sponding classes. Remaining nouns and verbs
were also replaced with their WordNet seman-
tic classes, as well. Each concept was then as-
sociated with a weight: concepts derived from
named entities classes were weighted heavier
than concepts from answer types, which were
in turn weighted heavier than concepts taken
from WordNet clases. Similarity was then com-
puted across matching concepts. 5 The resul-
tant similarity score was based on three vari-
ables:
S
= sum of the weights of all concepts matched
between a user query (T
) and a QUAB query
</bodyText>
<figure confidence="0.836134791666667">
(TVU );
W
= sum of the weights of all unmatched con-
cepts in T
;
X
= sum of the weights of all unmatched con-
cepts in TVU ;
The similarity between T
and TYU was calcu-
lated as
S
3
\x07 !
\&apos;
W
\x08 3
\x07 !
U \&apos;
X
\x08 , where
!
and
!
</figure>
<bodyText confidence="0.8899744">
U were used as coefficients to penalize the con-
tribution of unmatched concepts in T
and TVU
respectively. 6
Similarity Metric 6 is based on the fact that the
</bodyText>
<page confidence="0.963239">
5
</page>
<bodyText confidence="0.975984">
In the case of ambiguous nouns and verbs associated with
multiple WordNet classes, all possible classes for a term were
considered in matching.
</bodyText>
<page confidence="0.964748">
6
</page>
<bodyText confidence="0.969194">
We set Z @ = 0.4 and Z\x17[ = 0.1 in our experiments.
</bodyText>
<page confidence="0.992567">
211
</page>
<listItem confidence="0.9318679">
\x0cQ1: Does Iran have an indigenous CW program?
(1b) Has the plant at Qazvin been linked to CW production?
(1c) What CW does Iran produce?
(1a) How did Iran start its CW program?
Q2: Where are Irans CW facilities located? (2a) What factories in Iran could produce CW?
(2b) Where are Irans stockpiles of CW?
(2c) Where has Iran bought equipment to produce CW?
Q3: What is Irans goal for its CW program? (3a) What motivated Iran to expand its chemical weapons program?
(3b) How do CW figure into Irans longterm strategic plan?
(3c) What are Irans future CW plans?
</listItem>
<figure confidence="0.950353333333333">
QUABs:
QUABs:
QUABs:
Answer(A3):
Answer(A2):
Answer (A1):
</figure>
<bodyText confidence="0.979732833333333">
Although Iran is making a concerted effort to attain an independent production capability for all aspects of chemical
weapons program, it remains dependent on foreign sources for chemical warfarerelated technologies.
According to several sources, Irans primary suspected chemical weapons production facility is located in the city of Damghan.
In their pursuit of regional hegemony, Iran and Iraq probably regard CW weapons and missiles as necessary to support their
political and military objectives. Possession of chemical weapons would likely lead to increased intimidation of their Gulf,
neighbors, as well as increased willingness to confront the United States.
</bodyText>
<figureCaption confidence="0.89854">
Figure 6: A sample interactive Q/A dialogue.
</figureCaption>
<bodyText confidence="0.943836636363637">
QUAB questions are clustered based on their
mapping to a vector of important concepts in
the QUAB.The clustering was done using the
K-Nearest Neighbor (KNN) method (Dudani,
1976). Instead of measuring the similarity be-
tween the user question and each question in
the QUAB, similarities are computed only be-
tween the user question and the centroid of
each cluster.
Similarity Metric 7 was derived from the re-
sults of Similarity Metrics 5 and 6 above. In
this case, if the QUAB question (T U ) that was
deemed to be most similar to a user question
(T
) under Similarity Metric 5 is contained
in the cluster of QUAB questions deemed to
be most similar to T
under Similarity Metric
6, then TVU receives a cluster adjustment score
in order to boost its ranking within its QUAB
cluster. We calculate the cluster adjustment
score as \x02\x01)\x03 \x0e\x05 \x01 &amp; B
</bodyText>
<figure confidence="0.968476">
\x07
TYU\x10\x08
\x07
\x03 )
\x08 \x17
\x07
1 3 \x04 \x10 \x089\x089\x06
\x07
\x03 )\x03\x02 \x17 \x04 \x10 \x08 , where \x04 \x10 represents the difference
</figure>
<bodyText confidence="0.99808425">
in rank between the centroid of the cluster and
the previous rank of the QUAB question T U .
In the currently-implemented version of FERRET,
we used Similarity Metric 5 to automatically iden-
tify the set of 10 QUAB questions that were most
similar to a users question. These question-and-
answer pairs were then returned to the user along
with answers from FERRETs automatic Q/A system
as potential continuations of the Q/A dialogue. We
used the remaining 6 similarity metrics described in
this section to manually assess the impact of simi-
larity on a Q/A dialogue.
</bodyText>
<sectionHeader confidence="0.681292" genericHeader="method">
5 Experiments with Interactive Q/A
Dialogues
</sectionHeader>
<bodyText confidence="0.981050074074074">
To date, we have used FERRET to produce over 90
Q/A dialogues with human users. Figure 6 illustrates
three turns from a real dialogue from a human user
investigating Irans chemical weapons prorgram. As
it can be seen coherence can be established between
the users questions and the systems answers (e.g.
Q3 is related to both A1 and A3) as well as between
the QUABs and the users follow-up questions (e.g.
QUAB (1b) is more related to Q2 than either Q1 or
A1). Coherence alone is not sufficient to analyze the
quality of interactions, however.
In order to better understand interactive Q/A dia-
logues, we have conducted three sets of experiments
with human users of FERRET. In these experiments,
users were allotted two hours to interact with Ferret
to gather information requested by a dialogue sce-
nario similar to the one presented in Figure 2. In
Experiment 1 (E1), 8 U.S. Navy Reserve (USNR)
intelligence analysts used FERRET to research 8 dif-
ferent scenarios related to chemical and biological
weapons. Experiment 2 and Experiment 3 consid-
ered several of the same scenarios addressed in E1:
E2 included 24 mixed teams of analysts and novice
users working with 2 scenarios, while E3 featured 4
USNR analysts working with 6 of the original 8 sce-
narios. (Details for each experiment are provided in
Table 2.) Users were also given a task to focus their
</bodyText>
<page confidence="0.98217">
212
</page>
<bodyText confidence="0.970815333333333">
\x0cresearch; in E1 and E3, users prepared a short report
detailing their findings; in E2, users were given a list
of challenge questions to answer.
</bodyText>
<table confidence="0.960124">
Exp Users QUABs? Scenarios Topics
E1 8 Yes 8 Egypt BW, Russia CW, South
Africa CW, India CW, North
Korea CBW, Pakistan CW,
Libya CW, Iran CW
E2 24 Yes 2 Egypt BW, Russia CW
E3 4 No 6 Egypt BW, Russia CW, North
Korea CBW, Pakistan CW
India CW, Libya CW, Iran CW
</table>
<tableCaption confidence="0.98489">
Table 2: Experiment details
</tableCaption>
<bodyText confidence="0.988538166666667">
In E1 and E2, users had access to a total of 3210
QUAB questions that had been hand-created by de-
velopers for each the 8 dialogue scenarios. (Table 3
provides totals for each scenario.) In E3, users per-
formed research with a version of FERRET that in-
cluded no QUABs at all.
</bodyText>
<table confidence="0.9485523">
Scenario Handcrafted QUABs
INDIA 460
LIBYA 414
IRAN 522
NORTH KOREA 316
PAKISTAN 322
SOUTH AFRICA 454
RUSSIA 366
EGYPT 356
Testing Total 3210
</table>
<tableCaption confidence="0.998672">
Table 3: QUAB distribution over scenarios
</tableCaption>
<bodyText confidence="0.998529">
We have evaluated FERRET by measuring effi-
ciency, effectiveness, and user satisfaction:
Efficiency FERRETs QUAB collection enabled
users in our experiments to find more relevant infor-
mation by asking fewer questions. When manually-
created QUABs were available (E1 and E2), users
submitted an average of 12.25 questions each ses-
sion. When no QUABs were available (E3), users
entered a total of 44.5 questions per session. Table 4
lists the number of QUAB question-answer pairs se-
lected by users and the number of user questions en-
tered by users during the 8 scenarios considered in
E1. In E2, freed from the task of writing a research
report, users asked significantly (p 0.05) fewer
questions and selected fewer QUABs than they did
in E1. (See Table 5).
Effectiveness QUAB question-answer pairs also
improved the overall accuracy of the answers re-
turned by FERRET. To measure the effectiveness of
a Q/A dialogue, human annotators were used to per-
form a post-hoc analysis of how relevant the QUAB
pairs returned by FERRET were to each question
</bodyText>
<table confidence="0.973534">
Country n QUAB User Q Total
(avg.) (avg.) (avg.)
India 2 21.5 13.0 34.5
Libya 2 12.0 9.0 21.0
Iran 2 18.5 11.0 29.5
N.Korea 2 16.5 7.5 34.0
Pakistan 2 29.5 15.5 45.0
S.Africa 2 14.5 6.0 20.5
Russia 2 13.5 15.5 29.0
Egypt 2 15.0 20.5 35.5
TOTAL(E1) 16 17.63 12.25 29.88
</table>
<tableCaption confidence="0.964498">
Table 4: Efficiency of Dialogues in Experiment 1
</tableCaption>
<table confidence="0.9525844">
Country n QUAB User Q Total
(avg.) (avg.) (avg.)
Russia 24 8.2 5.5 13.7
Egypt 24 10.8 7.6 18.4
TOTAL(E2) 48 9.50 6.55 16.05
</table>
<tableCaption confidence="0.994688">
Table 5: Efficiency of Dialogues in Experiment 2
</tableCaption>
<bodyText confidence="0.942382428571429">
entered by a user: each QUAB pair returned was
graded as relevant or irrelevant to a user ques-
tion in a forced-choice task. Aggregate relevance
scores were used to calculate (1) the percentage of
relevant QUAB pairs returned and (2) the mean re-
ciprocal rank (MRR) for each user question. MRR is
defined as &lt;+ % \x01 F
</bodyText>
<equation confidence="0.184089333333333">
&lt;
&lt;
\x13 ?
</equation>
<bodyText confidence="0.9072024">
, whree \x08\x01 is the lowest rank of
any relevant answer for the \x03
\x01\x03\x02
user query7. Table 6
describes the performance of FERRET when each of
the 7 similarity measures presented in Section 4 are
used to return QUAB pairs in response to a query.
When only answers from FERRETs automatic Q/A
system were available to users, only 15.7% of sys-
tem responses were deemed to be relevant to a users
query. In contrast, when manually-generated QUAB
pairs were introduced, as high as 84% of the sys-
tems responses were deemed to be relevant. The
results listed in Table 6 show that the best metric is
Similarity Metric 5. Thse results suggest that the
selection of relevant questions depends on sophis-
ticated similarity measures that rely on conceptual
hierarchies and semantic recognizers.
We evaluated the quality of each of the four
sets of automatically-generated QUABs in a sim-
ilar fashion. For each question submitted by a
user in E1, E2, and E3, we collected the top 5
QUAB question-answer pairs (as determined by
Similarity Metric 5) that FERRET returned. As with
the manually-generated QUABs, the automatically-
</bodyText>
<page confidence="0.985713">
7
</page>
<bodyText confidence="0.994391">
We chose MRR as our scoring metric because it reflects the
fact that a user is most likely to examine the first few answers
from any system, but that all correct answers returned by the
system have some value because users will sometimes examine
a very large list of query results.
</bodyText>
<page confidence="0.995676">
213
</page>
<table confidence="0.9537857">
\x0c% of Top 5 Responses % of Top 1 Responses MRR
Relevant to User Q Relevant to User Q
Without QUAB 15.73% 26.85% 0.325
Similarity 1 82.61% 60.63% 0.703
Similarity 2 79.95% 58.45% 0.681
Similarity 3 79.47% 56.04% 0.664
Similarity 4 78.26% 46.14% 0.592
Similarity 5 84.06% 68.36% 0.753
Similarity 6 81.64% 56.04% 0.671
Similarity 7 84.54% 64.01% 0.730
</table>
<tableCaption confidence="0.991657">
Table 6: Effectiveness of dialogs
</tableCaption>
<bodyText confidence="0.9894">
generated pairs were submitted to human assessors
who annotated each as relevant or irrelevant to the
users query. Aggregate scores are presented in Ta-
ble 7.
</bodyText>
<table confidence="0.8860195">
Egypt Russia
Approach % of Top 5 % of Top 5
Responses Rel. MRR Responses Rel. MRR
to User Q to User Q
Approach 1 40.01% 0.295 60.25% 0.310
Approach 2 36.00% 0.243 72.00% 0.475
Approach 3 44.62% 0.271 60.00% 0.297
Approach 4 68.05% 0.510 68.00% 0.406
</table>
<tableCaption confidence="0.990551">
Table 7: Quality of QUABs acquired automatically
</tableCaption>
<subsectionHeader confidence="0.913535">
User Satisfaction Users were consistently satis-
</subsectionHeader>
<bodyText confidence="0.99598525">
fied with their interactions with FERRET. In all three
experiments, respondents claimed that they found
that FERRET (1) gave meaningful answers, (2) pro-
vided useful suggestions, (3) helped answer spe-
cific questions, and (4) promoted their general un-
derstanding of the issues considered in the scenario.
Complete results of this study are presented in Ta-
ble 88.
</bodyText>
<table confidence="0.8063108125">
Factor E1 E2 E3
Promoted understanding 3.40 3.20 3.75
Helped with specific questions 3.70 3.60 3.25
Make good use of questions 3.40 3.55 3.0
Gave new scenario insights 3.00 3.10 2.2
Gave good collection coverage 3.75 3.70 3.75
Stimulated user thinking 3.50 3.20 2.75
Easy to use 3.50 3.55 4.10
Expanded understanding 3.40 3.20 3.00
Gave meaningful answers 4.10 3.60 2.75
Was helpful 4.00 3.75 3.25
Helped with new search methods 2.75 3.05 2.25
Provided novel suggestions 3.25 3.40 2.65
Is ready for work environment 2.85 2.80 3.25
Would speed up work 3.25 3.25 3.00
Overall like of system 3.75 3.60 3.75
</table>
<tableCaption confidence="0.946553">
Table 8: User Satisfaction Survey Results
</tableCaption>
<sectionHeader confidence="0.998417" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99954625">
We believe that the quality of Q/A interactions de-
pends on the modeling of scenario topics. An ideal
model is provided by question-answer databases
(QUABs) that are created off-line and then used to
</bodyText>
<page confidence="0.97935">
8
</page>
<bodyText confidence="0.993981166666667">
Evaluation scale: 1-does not describe the system, 5-
completely describes the system
make suggestions to a user of potential relevant con-
tinuations of a discourse. In this paper, we have
presented FERRET, an interactive Q/A system which
makes use of a novel Q/A architecture that integrates
QUAB question-answer pairs into the processing of
questions. Experiments with FERRET have shown
that, in addition to being rapidly adopted by users as
valid suggestions, the incorporation of QUABs into
Q/A can greatly improve the overall accuracy of an
interactive Q/A dialogue.
</bodyText>
<sectionHeader confidence="0.991161" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99983835">
S. Dudani. 1976. The distance-weighted k-nearest-neighbour
rule. IEEE Transactions on Systems, Man, and Cybernetics,
SMC-6(4):325327.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, J. Williams,
and J. Bensley. 2003. Answer Mining by Combining Ex-
traction Techniques with Abductive Reasoning. In Proceed-
ings of the Twelfth Text Retrieval Conference (TREC 2003).
Sanda Harabagiu. 2004. Incremental Topic Representations.
In Proceedings of the 20th COLING Conference, Geneva,
Switzerland.
Marti Hearst. 1994. Multi-Paragraph Segmentation of Exposi-
tory Text. In Proceedings of the 32nd Meeting of the Associ-
ation for Computational Linguistics, pages 916.
Megumi Kameyama. 1997. Recognizing Referential Links: An
Information Extraction Perspective. In Workshop of Opera-
tional Factors in Practical, Robust Anaphora Resolution for
Unrestricted Texts, (ACL-97/EACL-97), pages 4653.
Chin-Yew Lin and Eduard Hovy. 2000. The Automated Acqui-
sition of Topic Signatures for Text Summarization. In Pro-
ceedings of the 18th COLING Conference, pages 495501.
S. Lytinen and N. Tomuro. 2002. The Use of Question Types
to Match Questions in FAQFinder. In Papers from the 2002
AAAI Spring Symposium on Mining Answers from Texts and
Knowledge Bases, pages 4653.
Srini Narayanan and Sanda Harabagiu. 2004. Question An-
swering Based on Semantic Structures. In Proceedings of
the 20th COLING Conference, Geneva, Switzerland.
Mihai Surdeanu and Sanda M. Harabagiu. 2002. Infratructure
for open-domanin information extraction. In Conference for
Human Language Technology (HLT-2002).
Mihai Surdeanu, Sanda M. Harabagiu, John Williams, and Paul
Aarseth. 2003. Using predicate-argument structures for in-
formation extraction. In ACL, pages 815.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen, and Silja
Huttunen. 2000. Automatic Acquisition of Domain Knowl-
edge for Information Extraction. In Proceedings of the 18th
COLING Conference, pages 940946.
Roman Yangarber. 2003. Counter-Training in Discovery of
Semantic Patterns. In Proceedings of the 41th Meeting of the
Association for Computational Linguistics, pages 343350.
</reference>
<page confidence="0.992455">
214
</page>
<figure confidence="0.244621">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.742839">
<note confidence="0.944334666666667">b&apos;Proceedings of the 43rd Annual Meeting of the ACL, pages 205214, Ann Arbor, June 2005. c 2005 Association for Computational Linguistics</note>
<title confidence="0.912775">Experiments with Interactive Question-Answering</title>
<author confidence="0.998803">Sanda Harabagiu</author>
<author confidence="0.998803">Andrew Hickl</author>
<author confidence="0.998803">John Lehmann</author>
<author confidence="0.998803">Dan Moldovan</author>
<affiliation confidence="0.999892">Language Computer Corporation</affiliation>
<address confidence="0.989552">Richardson, Texas USA</address>
<email confidence="0.999904">sanda@languagecomputer.com</email>
<abstract confidence="0.998773933333333">This paper describes a novel framework for interactive question-answering (Q/A) based on predictive questioning. Generated off-line from topic representations of complex scenarios, predictive questions represent requests for information that capture the most salient (and diverse) aspects of a topic. We present experimental results from large user studies (featuring a fully-implemented interactive Q/A system named FERRET) that demonstrates that surprising performance is achieved by integrating predictive questions into the context of a Q/A dialogue.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Dudani</author>
</authors>
<title>The distance-weighted k-nearest-neighbour rule.</title>
<date>1976</date>
<journal>IEEE Transactions on Systems, Man, and Cybernetics,</journal>
<volume>6</volume>
<issue>4</issue>
<contexts>
<context position="12272" citStr="Dudani, 1976" startWordPosition="1858" endWordPosition="1859">ese types of topic representations are limited by the fact that they require the identification of topic-relevant documents prior to the discovery of the topic signatures. In our experiments, we were only presented with a set of documents relevant to a particular scenario; no further relevance information was provided for individual subject areas or sub-topics. In order to solve the problem of finding relevant documents for each subtopic, we considered four different approaches: Approach 1: All documents in the CNS collection were initially clustered using K-Nearest Neighbor (KNN) clustering (Dudani, 1976). Each cluster that contained at least one keyword that described the sub-topic was deemed relevant to the topic. Approach 2: Since individual documents may contain discourse segments pertaining to different sub-topics, we first used TextTiling (Hearst, 1994) to automatically segment all of the documents in the CNS collection into individual text tiles. These individual discourse segments 207 \x0cthen served as input to the KNN clustering algorithm described in Approach 1. Approach 3: In this approach, relevant documents were discovered simultaneously with the discovery of topic signatures. Fi</context>
<context position="33406" citStr="Dudani, 1976" startWordPosition="5316" endWordPosition="5317">cal weapons production facility is located in the city of Damghan. In their pursuit of regional hegemony, Iran and Iraq probably regard CW weapons and missiles as necessary to support their political and military objectives. Possession of chemical weapons would likely lead to increased intimidation of their Gulf, neighbors, as well as increased willingness to confront the United States. Figure 6: A sample interactive Q/A dialogue. QUAB questions are clustered based on their mapping to a vector of important concepts in the QUAB.The clustering was done using the K-Nearest Neighbor (KNN) method (Dudani, 1976). Instead of measuring the similarity between the user question and each question in the QUAB, similarities are computed only between the user question and the centroid of each cluster. Similarity Metric 7 was derived from the results of Similarity Metrics 5 and 6 above. In this case, if the QUAB question (T U ) that was deemed to be most similar to a user question (T ) under Similarity Metric 5 is contained in the cluster of QUAB questions deemed to be most similar to T under Similarity Metric 6, then TVU receives a cluster adjustment score in order to boost its ranking within its QUAB cluste</context>
</contexts>
<marker>Dudani, 1976</marker>
<rawString>S. Dudani. 1976. The distance-weighted k-nearest-neighbour rule. IEEE Transactions on Systems, Man, and Cybernetics, SMC-6(4):325327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>C Clark</author>
<author>M Bowden</author>
<author>J Williams</author>
<author>J Bensley</author>
</authors>
<title>Answer Mining by Combining Extraction Techniques with Abductive Reasoning.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twelfth Text Retrieval Conference (TREC</booktitle>
<contexts>
<context position="4244" citStr="Harabagiu et al., 2003" startWordPosition="630" endWordPosition="633">rmation Extraction Representation Offline Question Answering Database (QUAB) QuestionAnswer Figure 1: FERRET - A Predictive Interactive Question-Answering Architecture. model, and Section 6 summarizes the conclusions. 2 Interactive Question-Answering We have found that the quality of interactions produced by an interactive Q/A system can be greatly enhanced by predicting the range of questions that a user might ask in the context of a given topic. If a large database of topic-relevant questions were available for a wide variety of topics, the accuracy of a state-of-the-art Q/A system such as (Harabagiu et al., 2003) could be enhanced. In FERRET, our interactive Q/A system, we store such predicted pairs of questions and answers in a database known as the Question Answer Database (or QUAB). FERRET uses this large set of topicrelevant question-and-answer pairs to improve the interaction with the user by suggesting new questions. For example, when a user asks a question like (Q1) (as illustrated in Table 1), FERRET returns an answer to the question (A1) and proposes (Q2), (Q3), and (Q4) as suggestions of possible continuations of the dialogue. Users then choose how to continue the interaction by either (1) i</context>
<context position="15056" citStr="Harabagiu et al., 2003" startWordPosition="2300" endWordPosition="2303"> documents. As with Approach 2, segments were produced using the TextTiling algorithm. In modeling the dialogue scenarios, we considered three types of topic-relevant relations: (1) structural relations, which represent hypernymy or meronymy relations between topic-relevant concepts, (2) definition relations, which uncover the characteristic properties of a concept, and (3) extraction relations, which model the most relevant events or states associated with a sub-topic. Although structural relations and definition relations are discovered reliably using patterns available from our Q/A system (Harabagiu et al., 2003), we found only extraction relations to be useful in determining the set of documents relevant to a subtopic. Structural relations were available from concept ontologies implemented in the Q/A system. The definition relations were identified by patterns used for processing definition questions. Extraction relations are discovered by processing documents in order to identify three types of relations, including: (1) syntactic attachment relations (including subject-verb, object-verb, and verb-PP relations), (2) predicate-argument relations, and (3) salience-based relations that can be used to en</context>
</contexts>
<marker>Harabagiu, Moldovan, Clark, Bowden, Williams, Bensley, 2003</marker>
<rawString>S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, J. Williams, and J. Bensley. 2003. Answer Mining by Combining Extraction Techniques with Abductive Reasoning. In Proceedings of the Twelfth Text Retrieval Conference (TREC 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
</authors>
<title>Incremental Topic Representations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th COLING Conference,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="11499" citStr="Harabagiu, 2004" startWordPosition="1739" endWordPosition="1740">d together to define a topic structure that will govern future interactions with the Q/A system. In order to model this structure, the topic representation that we create considers separate topic signatures for each sub-topic. The notion of topic signatures was first introduced in (Lin and Hovy, 2000). For each subtopic in a scenario, given (a) documents relevant to the sub-topic and (b) documents not relevant to the subtopic, a statistical method based on the likelihood ratio is used to discover a weighted list of the most topic-specific concepts, known as the topic signature. Later work by (Harabagiu, 2004) demonstrated that topic signatures can be further enhanced by discovering the most relevant relations that exist between pairs of concepts. However, both of these types of topic representations are limited by the fact that they require the identification of topic-relevant documents prior to the discovery of the topic signatures. In our experiments, we were only presented with a set of documents relevant to a particular scenario; no further relevance information was provided for individual subject areas or sub-topics. In order to solve the problem of finding relevant documents for each subtopi</context>
<context position="13052" citStr="Harabagiu, 2004" startWordPosition="1980" endWordPosition="1981"> discourse segments pertaining to different sub-topics, we first used TextTiling (Hearst, 1994) to automatically segment all of the documents in the CNS collection into individual text tiles. These individual discourse segments 207 \x0cthen served as input to the KNN clustering algorithm described in Approach 1. Approach 3: In this approach, relevant documents were discovered simultaneously with the discovery of topic signatures. First, we associated a binary seed relation \x02\x01 for each each sub-topic \x03 \x01 . (Seed relations were created both by hand and using the method presented in (Harabagiu, 2004).) Since seed relations are by definition relevant to a particular subtopic, they can be used to determine a binary partition of the document collection \x04 into (1) a relevant set of documents \x05\x06\x01 (that is, the documents relevant to relation \x01 ) and (2) a set of non-relevant documents \x04 -\x05\x06\x01 . Inspired by the method presented in (Yangarber et al., 2000), a topic signature (as calculated by (Harabagiu, 2004)) is then produced for the set of documents in \x05\x07\x01 . For each subtopic \x03 \x01 defined as part of the dialogue scenario, documents relevant to a correspo</context>
<context position="15815" citStr="Harabagiu, 2004" startWordPosition="2404" endWordPosition="2405">le from concept ontologies implemented in the Q/A system. The definition relations were identified by patterns used for processing definition questions. Extraction relations are discovered by processing documents in order to identify three types of relations, including: (1) syntactic attachment relations (including subject-verb, object-verb, and verb-PP relations), (2) predicate-argument relations, and (3) salience-based relations that can be used to encode long-distance dependencies between topic-relevant concepts. (Salience-based relations are discovered using a technique first reported in (Harabagiu, 2004) which approximates a Centering Theory-style approach (Kameyama, 1997) to the resolution of coreference.) Subtopic: Egypts production of toxins and BW agents Topic Signature: produce phosphorous trichloride (TOXIN) house ORGANIZATION cultivate nonpathogenic Bacilus Subtilis (TOXIN) produce mycotoxins (TOXIN) acquire FACILITY Subtopic: Egypts allies and partners Topic Signature: provide COUNTRY cultivate COUNTRY supply precursors cooperate COUNTRY train PERSON supply knowhow Figure 3: Example of two topic signatures acquired for the scenario illustrated in Figure 2. We made the extraction relat</context>
<context position="26071" citStr="Harabagiu, 2004" startWordPosition="4069" endWordPosition="4070">re based on question patterns associated with causal relations and therefore allow different degrees for the specificity of the resultative, i.e obstacle or deterrent. We generated several questions for each answer passage. Questions were generated based on patterns that were acquired to model interrogations using relations between predicates and their arguments. Such interrogations are based on (1) associations between the answer type (e.g. DATE) and the question stem (e.g. when and (2) the relation between predicates, question stem and the words that determine the answer type (Narayanan and Harabagiu, 2004). In order to obtain these predicate-argument patterns, we used 30% (approximately 1500 questions) of the handcrafted questionanswer pairs, selected at random from each of the 8 dialogue scenarios. As Figures 4 and 5 illustrate, we used patterns based on (a) embedded predicates and (b) causal or counterfactual predicates. 4 Managing Interactive Q/A Dialogues As illustrated in Figure 1, the main idea of managing dialogues in which interactions with the Q/A system occur is based on the notion of predictions, i.e. by proposing to the user a small set of questions that tackle the same subject as h</context>
</contexts>
<marker>Harabagiu, 2004</marker>
<rawString>Sanda Harabagiu. 2004. Incremental Topic Representations. In Proceedings of the 20th COLING Conference, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Multi-Paragraph Segmentation of Expository Text.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>916</pages>
<contexts>
<context position="12531" citStr="Hearst, 1994" startWordPosition="1897" endWordPosition="1898">ular scenario; no further relevance information was provided for individual subject areas or sub-topics. In order to solve the problem of finding relevant documents for each subtopic, we considered four different approaches: Approach 1: All documents in the CNS collection were initially clustered using K-Nearest Neighbor (KNN) clustering (Dudani, 1976). Each cluster that contained at least one keyword that described the sub-topic was deemed relevant to the topic. Approach 2: Since individual documents may contain discourse segments pertaining to different sub-topics, we first used TextTiling (Hearst, 1994) to automatically segment all of the documents in the CNS collection into individual text tiles. These individual discourse segments 207 \x0cthen served as input to the KNN clustering algorithm described in Approach 1. Approach 3: In this approach, relevant documents were discovered simultaneously with the discovery of topic signatures. First, we associated a binary seed relation \x02\x01 for each each sub-topic \x03 \x01 . (Seed relations were created both by hand and using the method presented in (Harabagiu, 2004).) Since seed relations are by definition relevant to a particular subtopic, th</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Marti Hearst. 1994. Multi-Paragraph Segmentation of Expository Text. In Proceedings of the 32nd Meeting of the Association for Computational Linguistics, pages 916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Megumi Kameyama</author>
</authors>
<title>Recognizing Referential Links: An Information Extraction Perspective.</title>
<date>1997</date>
<booktitle>In Workshop of Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts, (ACL-97/EACL-97),</booktitle>
<pages>4653</pages>
<contexts>
<context position="15885" citStr="Kameyama, 1997" startWordPosition="2413" endWordPosition="2414">n relations were identified by patterns used for processing definition questions. Extraction relations are discovered by processing documents in order to identify three types of relations, including: (1) syntactic attachment relations (including subject-verb, object-verb, and verb-PP relations), (2) predicate-argument relations, and (3) salience-based relations that can be used to encode long-distance dependencies between topic-relevant concepts. (Salience-based relations are discovered using a technique first reported in (Harabagiu, 2004) which approximates a Centering Theory-style approach (Kameyama, 1997) to the resolution of coreference.) Subtopic: Egypts production of toxins and BW agents Topic Signature: produce phosphorous trichloride (TOXIN) house ORGANIZATION cultivate nonpathogenic Bacilus Subtilis (TOXIN) produce mycotoxins (TOXIN) acquire FACILITY Subtopic: Egypts allies and partners Topic Signature: provide COUNTRY cultivate COUNTRY supply precursors cooperate COUNTRY train PERSON supply knowhow Figure 3: Example of two topic signatures acquired for the scenario illustrated in Figure 2. We made the extraction relations associated with each topic signature more general (a) by replacin</context>
</contexts>
<marker>Kameyama, 1997</marker>
<rawString>Megumi Kameyama. 1997. Recognizing Referential Links: An Information Extraction Perspective. In Workshop of Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts, (ACL-97/EACL-97), pages 4653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The Automated Acquisition of Topic Signatures for Text Summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th COLING Conference,</booktitle>
<pages>495501</pages>
<contexts>
<context position="11185" citStr="Lin and Hovy, 2000" startWordPosition="1684" endWordPosition="1687">he research) and audience (i.e. the organization receiving the information), as well as a series of evidence conditions which specify how much verification information must be subject to before it can be accepted as fact. We assume the set of sub-topics mentioned in the general background and the scenario can be used together to define a topic structure that will govern future interactions with the Q/A system. In order to model this structure, the topic representation that we create considers separate topic signatures for each sub-topic. The notion of topic signatures was first introduced in (Lin and Hovy, 2000). For each subtopic in a scenario, given (a) documents relevant to the sub-topic and (b) documents not relevant to the subtopic, a statistical method based on the likelihood ratio is used to discover a weighted list of the most topic-specific concepts, known as the topic signature. Later work by (Harabagiu, 2004) demonstrated that topic signatures can be further enhanced by discovering the most relevant relations that exist between pairs of concepts. However, both of these types of topic representations are limited by the fact that they require the identification of topic-relevant documents pr</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The Automated Acquisition of Topic Signatures for Text Summarization. In Proceedings of the 18th COLING Conference, pages 495501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lytinen</author>
<author>N Tomuro</author>
</authors>
<title>The Use of Question Types to Match Questions in FAQFinder.</title>
<date>2002</date>
<booktitle>In Papers from the 2002 AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases,</booktitle>
<pages>4653</pages>
<contexts>
<context position="27667" citStr="Lytinen and Tomuro, 2002" startWordPosition="4325" endWordPosition="4328">he scenario. Moreover, complex questions, that generally are not processed with high accuracy by current state-ofthe-art Q/A systems, are associated with predictive questions that represent decompositions based on 210 \x0csimilarities between predicates and arguments of the original question and the predicted questions. The selection of the questions from the QUABs that are proposed for each user question is based on a similarity-metric that ranks the QUAB questions. To compute the similarity metric, we have experimented with seven different metrics. The first four metrics were introduced in (Lytinen and Tomuro, 2002). Similarity Metric 1 is based on two processing steps: (a) the content words of the questions are weighted using the \x02\x01\x04\x03 - \x01 measure used in Information Retrieval \x05 \x01 \x05 \x07 \x01 \x08 \x07 1\x07\x06 \x19\x1c\x1b\x0e\x1d \x07 \x08\x01\x02\x01Z\x089\x08 \t\x0c\x0b\x02 \x0f\x0e &amp;\x11\x10 ? , where \x12 is the number of questions in the QUAB, - \x01 \x01 is the number of questions containing \x01 and \x02\x01\x02\x01 is the number of times \x01 appears in the question. This allows the user question and any QUAB question to be transformed into two vectors, \x13 \x14\x15\x0</context>
<context position="30412" citStr="Lytinen and Tomuro, 2002" startWordPosition="4800" endWordPosition="4803">estion :9\x1f \x14;: &lt; \x18 : \x1e \x18\x19\x1b\x11\x1b\x11\x1b\x11\x18 :&lt;/ \x1e to be defined as \x05,) \x07 : =\x18 :&gt;\x1f/\x08 ? \x11 7 @ 6 7 A \x15CB ? \x11 7 A 6 7 @ \x15 7 @ B 7 A , where D \x07 :\x04E \x18 :&gt;F\x12\x08N % E (,7HG &lt; &lt; B\x04IKJ\x0cLNM&lt;OQP M\x04R \x11 E 6 F \x15 Similarity Metric 4 is based on the question type similarity. Instead of using the question class, determined by its stem, whenever we could recognize the answer type expected by the question, we used it for matching. As backoff only, we used a question type similarity based on a matrix akin to the one reported in (Lytinen and Tomuro, 2002) Similarity Metric 5 is based on question concepts rather than question terms. In order to translate question terms into concepts, we replaced (a) question stems (i.e. a WH-word + NP construction) with expected answer types (taken from the answer type hierarchy employed by FERRETs Q/A system) and (b) named entities with corresponding their corresponding classes. Remaining nouns and verbs were also replaced with their WordNet semantic classes, as well. Each concept was then associated with a weight: concepts derived from named entities classes were weighted heavier than concepts from answer typ</context>
</contexts>
<marker>Lytinen, Tomuro, 2002</marker>
<rawString>S. Lytinen and N. Tomuro. 2002. The Use of Question Types to Match Questions in FAQFinder. In Papers from the 2002 AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases, pages 4653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Question Answering Based on Semantic Structures.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th COLING Conference,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="26071" citStr="Narayanan and Harabagiu, 2004" startWordPosition="4067" endWordPosition="4070">re generated are based on question patterns associated with causal relations and therefore allow different degrees for the specificity of the resultative, i.e obstacle or deterrent. We generated several questions for each answer passage. Questions were generated based on patterns that were acquired to model interrogations using relations between predicates and their arguments. Such interrogations are based on (1) associations between the answer type (e.g. DATE) and the question stem (e.g. when and (2) the relation between predicates, question stem and the words that determine the answer type (Narayanan and Harabagiu, 2004). In order to obtain these predicate-argument patterns, we used 30% (approximately 1500 questions) of the handcrafted questionanswer pairs, selected at random from each of the 8 dialogue scenarios. As Figures 4 and 5 illustrate, we used patterns based on (a) embedded predicates and (b) causal or counterfactual predicates. 4 Managing Interactive Q/A Dialogues As illustrated in Figure 1, the main idea of managing dialogues in which interactions with the Q/A system occur is based on the notion of predictions, i.e. by proposing to the user a small set of questions that tackle the same subject as h</context>
</contexts>
<marker>Narayanan, Harabagiu, 2004</marker>
<rawString>Srini Narayanan and Sanda Harabagiu. 2004. Question Answering Based on Semantic Structures. In Proceedings of the 20th COLING Conference, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda M Harabagiu</author>
</authors>
<title>Infratructure for open-domanin information extraction.</title>
<date>2002</date>
<booktitle>In Conference for Human Language Technology (HLT-2002).</booktitle>
<contexts>
<context position="6757" citStr="Surdeanu and Harabagiu, 2002" startWordPosition="1025" endWordPosition="1028">e available by the Center for Non-Proliferation Studies (CNS)3. Modules from the FERRETs dialogue shell interact with modules from the predictive dialogue block. Central to the predictive dialogue is the topic representation for each scenario, which enables the population of a Predictive Dialogue Network (PDN). The PDN consists of a large set of questions that were asked or predicted for each topic. It is a network because questions are related by similarity links, which are computed by the Question Similarity module. The topic representation enables an Information Extraction module based on (Surdeanu and Harabagiu, 2002) to find topic-relevant information in the document collection and to use it as answers for the QUABs. The questions associated with each predicted answer are generated from patterns that are related to the extraction patterns used for identifying topic relevant information. The quality of the dialog between the user and FERRET depends on the quality of the topic representations and the coverage of the QUABs. 3 The Center for Non-Proliferation Studies at the Monterrey Institute of International Studies distributes collections of print and online documents on weapons of mass destruction. More i</context>
</contexts>
<marker>Surdeanu, Harabagiu, 2002</marker>
<rawString>Mihai Surdeanu and Sanda M. Harabagiu. 2002. Infratructure for open-domanin information extraction. In Conference for Human Language Technology (HLT-2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda M Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>815</pages>
<contexts>
<context position="5417" citStr="Surdeanu et al., 2003" startWordPosition="820" endWordPosition="823">e how to continue the interaction by either (1) ignoring the suggestions made by the system and proposing a different question, or by (2) selecting one of the proposed questions and examining its answer. Figure 1 illustrates the architecture of FERRET. The interactions are managed by a dialogue shell, which processes questions by transforming them into their corresponding predicate-argument structures2. The data collection used in our experiments was 2 We have employed the same representation of predicateargument structures as those encoded in PropBank. We use a semantic parser (described in (Surdeanu et al., 2003)) that recognizes predicate-argument structures. (Q1) What weapons are included in Egypts stockpiles? (A1) The Israelis point to comments made by former President Anwar Sadat, who in 1970 stated that Egypt has biological weapons stored in refrigerators ready to use against Israel if need be. The program might include plague, botulism toxin, encephalitis virus, anthrax, Rift Valley fever and mycotoxicosis. (Q2) Where did Egypt inherit its first stockpiles of chemical weapons? (Q3) Is there evidence that Egypt has dismantled its stockpiles of weapons? (Q4) Where are Egypts weapons stockpiles loc</context>
<context position="24264" citStr="Surdeanu et al., 2003" startWordPosition="3770" endWordPosition="3773">bstacle&quot; Reference: P1 P6 = view QUESTIONS Does Egypt view the possesion of BW as an obstacle? Does Egypt view the possesion of BW as a deterrent? P4 = have P&quot;4 = &quot;the possesion&quot; P&quot;4 = &quot;the possesion&quot; = nominalization(P4) = EFFECT(P2(BW)) PROCESSING specialization Pattern: Does Egypt P6 P&quot;4(BW) as a P5? Figure 5: Questions for Implied Causal Relations. Problem 2: We have found that the identification of the association between a candidate answer and a question depends on (a) the recognition of predicates and entities based on both the output of a named entity recognizer and a semantic parser (Surdeanu et al., 2003) and their structuring into predicate-argument frames, (b) the resolution of reference (addressed in Problem 1), (c) the recognition of implicit relations between predications stated in the answer. Some of these implicit relations are referential, as is the relation between predicates 8 &lt; and 8\x01 illustrated in Figure 4. A special case of implicit relations are the causal relations. Figure 5 illustrates an answer where a causal relation exists and is marked by the cue phrase because. Predicates like those in Figure 5 can be phrasal (like 8\x03\x02 ) or negative (like 8\x03\x02\x1e ). Causali</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, Sanda M. Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate-argument structures for information extraction. In ACL, pages 815.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
<author>Pasi Tapanainen</author>
<author>Silja Huttunen</author>
</authors>
<title>Automatic Acquisition of Domain Knowledge for Information Extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th COLING Conference,</booktitle>
<pages>940946</pages>
<contexts>
<context position="13433" citStr="Yangarber et al., 2000" startWordPosition="2042" endWordPosition="2045">ered simultaneously with the discovery of topic signatures. First, we associated a binary seed relation \x02\x01 for each each sub-topic \x03 \x01 . (Seed relations were created both by hand and using the method presented in (Harabagiu, 2004).) Since seed relations are by definition relevant to a particular subtopic, they can be used to determine a binary partition of the document collection \x04 into (1) a relevant set of documents \x05\x06\x01 (that is, the documents relevant to relation \x01 ) and (2) a set of non-relevant documents \x04 -\x05\x06\x01 . Inspired by the method presented in (Yangarber et al., 2000), a topic signature (as calculated by (Harabagiu, 2004)) is then produced for the set of documents in \x05\x07\x01 . For each subtopic \x03 \x01 defined as part of the dialogue scenario, documents relevant to a corresponding seed relation \x01 are added to \x05 iff the relation \x08\x01 meets the density criterion (as defined in (Yangarber et al., 2000)). If \t represents the set of documents where \x02\x01 is recognized, then the density criterion can be defined as: \x0b \x0c\x0f\x0e\x10 \x0b \x0c\x0f\x11\x13\x12 \x0e\x10 \x11\x14 . Once \t is added to \x05\x15\x01 , then a new topic signatur</context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>Roman Yangarber, Ralph Grishman, Pasi Tapanainen, and Silja Huttunen. 2000. Automatic Acquisition of Domain Knowledge for Information Extraction. In Proceedings of the 18th COLING Conference, pages 940946.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
</authors>
<title>Counter-Training in Discovery of Semantic Patterns.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>343350</pages>
<contexts>
<context position="17061" citStr="Yangarber, 2003" startWordPosition="2588" endWordPosition="2589">c signature more general (a) by replacing words with their (morphological) root form (e.g. wounded with wound, weapons with weapon), (b) by replacing lexemes with their subsuming category from an ontology of 100,000 words (e.g. truck is replaced by VEHICLE, ARTIFACT, or OBJECT), and (c) by replacing each name with its name class (Egypt with COUNTRY). Figure 3 illustrates the topic signatures resulting for the scenario illustrated in Figure 2. Once extraction relations were obtained for a particular set of documents, the resulting set of relations were ranked according to a method proposed in (Yangarber, 2003). Under this approach, 208 \x0cthe score associated with each relation is given by: \x02\x01\x04\x03 \x06\x05 \x07 \t\x08\x0b \x0c\x0e \x10\x0f\x12\x11\x14\x13\x16\x15 \x0b \x18\x17\x1a\x19\x1c\x1b\x0e\x1d\t\x1e \x03 \x1f\x06! \x07 \x06\x08 , where &quot; \t#&quot; represents the cardinality of the documents where the relation is identified, and \x03 \x1f\t! \x07 \t\x08 represents support associated with the relation . \x03 \x1f\t! \x07 \t\x08 is defined as the sum of the relevance of each document in \t : \x03 \x1f\t! \x07 \t\x08$ %\&apos;&amp;)( \x0b \x05*\x05,+ \x07.- \x08 . The relevance of a document that c</context>
</contexts>
<marker>Yangarber, 2003</marker>
<rawString>Roman Yangarber. 2003. Counter-Training in Discovery of Semantic Patterns. In Proceedings of the 41th Meeting of the Association for Computational Linguistics, pages 343350.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>