Finally, many recent text simplification systems have utilized language models trained only on simplified data (CITATION; CITATION; CITATIONa; Wubben et al., 2012); improvements in simple language modeling could translate into improvements for these systems,,
 simplification sub-problems such as lexical simplification CITATION and predicting text simplicity (Eickhoff et al., 2010),,
ployed language models trained only on unsummarized text (CITATION; CITATION),,
010; CITATIONb),,
; CITATIONa; Wubben et al., 2012) and in other languages CITATION,,
nd Lapata, 2011; CITATIONa; Wubben et al., 2012); improvements in simple language modeling could translate into improvements for these systems,,
oodsend and Lapata, 2011; CITATIONa; Wubben et al., 2012) and in other languages CITATION,,
To make this discrepancy more explicit, we created a sentence aligned data set by aligning the simple and normal articles using the approach from CITATIONb),,
for example the English Gigaword corpus (David CITATION)),,
After preprocessing, the 60K articles represents less than half a million sentences which is orders of magnitude smaller than the amount of normal English data available (for example the English Gigaword corpus (David CITATION)),,
Because frequencies tend to follow a Zipfian distribution, these rare n-grams make up a large portion of n-grams in real data CITATION,,
 and in other languages CITATION,,
Simple English Wikipedia has been previously used for many text simplification approaches (CITATION; CITATION; CITATION; CITATIONa; CITATION; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedia by both automatic measures and human perception (CITATIONb; 1538 \x0csimple normal sentences 385K 2540K words 7.15M 64.7M vocab size 78K 307K Table 2: Summary counts for the simple-normal article aligned data set consisting of 60K article pairs,,
Simple English Wikipedia has been previously used for many text simplification approaches (CITATION; CITATION; CITATION; CITATIONa; CITATION; Wubben et al., 201,,
ed data since the largest text compression data sets contain only a few thousand sentences (CITATION; CITATION; CITATION; CITATION),,
data: - simple-only: simple sentences only - normal-only: normal sentences only - simple-X+normal: X simple sentences combined with a varying number of normal sentences To evaluate the language models we calculated the model perplexity CITATION on the simple side of the held-out data,,
complexity, and grammatical complexity (CITATION; CITATION; CITATIONb),,
2 Related Work If we view the normal data as out-of-domain data, then the problem of combining simple and normal data is similar to the language model domain adaption problem CITATION, in particular cross-domain adaptation CITATION where a domain-specific model is improved by incorporating additional general data,,
Table 1 shows the n-gram overlap proportions in a sentence aligned data set of 137K sentence pairs from aligning Simple English Wikipedia and English Wikipedia articles (CITATIONa).1 The data highlights two conflicting views: does the benefit of additional data outweigh the problem of the source of the data? Throughout the rest of this paper we refer to sentences/articles/text from English Wikipedia as normal and sentences/articles/text from Simple English Wikipedia as simple,,
dze, 2010; CITATION; CITATIONb),,
4.1 Experimental Setup We used trigram language models with interpolated Kneser-Kney discounting trained using the SRI language modeling toolkit CITATION,,
Adaptation techniques have been shown to improve language modeling performance based on perplexity (CITATION,,
Simple English Wikipedia has been previously used for many text simplification approaches (CITATION; CITATION; CITATION; CITATIONa; CITATION; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedia by both automatic measures and human perception (CITATIONb; 1538 \x0csimple normal sentences 385K 2540K words 7.15M 64.7M vocab size ,,
ude smaller than the amount of normal English data available (for example the English Gigaword corpus (David CITATION)),,
Simple English Wikipedia has been previously used for many text simplification approaches (CITATION; CITATION; CITATION; CITATIONa; CITATION; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedia by both automatic measures and human perception (CITATIONb; 1538 \x0csimple normal sentences 385K 2540K words,,
Many recent statistical simplification techniques build upon models from machine translation and utilize a simple language model during simplification/decoding both in English (CITATION; CITATION; CITATIONa; Wubben et al., 2012) and in other languages CITATION,,
cally based on the lexical simplification task from SemEval 2012 CITATION,,
For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (CITATION; CITATION; ,,
This is not the case for all monolingual translation tasks (CITATION; Coh,,
2 Related Work If we view the normal data as out-of-domain data, then the problem of combining simple and normal data is similar to the language model domain adaption problem CITATION, in particular cross-domain adaptation CITATION where a domain-specific model is improved,,
est text compression data sets contain only a few thousand sentences (CITATION; CITATION; CITATION; CITATION),,
See CITATION for the full details of how the evaluation metric is calculated,,
In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain CITATION,,
CITATION),,
Simple English Wikipedia has been previously used for many text simplification approaches (CITATION; CITATION; CITATION; CITATIONa; CITATION; Wubben et al., 2012) and has been shown to be simpler than normal E,,
Finally, many recent text simplification systems have utilized language models trained only on simplified data (CITATION; CITATION; CITATIONa; Wubben et al., 2012); improvements in simple language modeling could translate into impr,,
Simple English Wikipedia has been previously used for many text simplification approaches (CITATION; CITATION; CITATION; CITATIONa; CITATION; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedi,,
problems such as lexical simplification CITATION and predicting text simplicity (Eickhoff et al., 2010),,
Simple English language models have also been used as predictive features in other simplification sub-problems such as lexical simplification CITATION and predicting text simplicity (Eickhoff et al., 2010),,
CITATION provide a survey on the related problem of domain adaptation for machine learning ,,
a; Wubben et al., 2012) and in other languages CITATION,,
Similarly for summarization, systems that have employed language models trained only on unsummarized text (CITATION; CITATION),,
This is not the case for all monolingual translation tasks (CITATION; CITATION),,
Although our goal was not to create the best lexical simplification system, this approach would have ranked 6th out of 11 submitted systems in the SemEval 2012 competition CITATION,,
This approach has the benefit of simplicity, however, better performance for combining related corpora has been seen by domain adaptation techniques which combine the data in more structured ways CITATION,,
In our case, the interpolated model combines the simple model estimate, ps(wi|wi2, wi1), and the normal model estimate, pn(wi|wi2, wi1), linearly (CITATION; CITATION): pinterpolated(wi|wi2, wi1) = pn(wi|wi2, wi1) + (1 ) ps(wi|wi2, wi1) where 0 1,,
Adaptation techniques have been shown to improve language modeling performance based on perplexity CITATION and in application areas such as speech transcription CITATION and machine translation CITATION, though no previous research has examined the language model domain adaptation problem for text simplification,,
CITATION provide a survey on the related problem of domain adaptation for machine learning (also referred to as transfer learning), which utilizes si,,
CITATION provide a survey on the related problem of domain adaptation for machine learning (also referred to as transfer learning), which utilizes similar techniques,,
simple and normal data is similar to the language model domain adaption problem CITATION, in particular cross-domain adaptation CITATION where a domain-specific model is improved by incorporating additional general data,,
Lexical simplification is a sub-problem of the general text simplification problem CITATION; a sentence is simplified by substituting words or phrases in the sentence with simpler variations,,
For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (CITATION; ,,
Simple English Wikipedia has been previously used for many text simplification approaches (CITATION; CITATION; CITATION; CITATIONa; CITATION; Wubben et al., 2012) and has been shown to b,,
To evaluate the rankings, we use the metric from the SemEval 2012 task, the Cohens kappa coefficient CITATION between the system ranking and the human ranking, which we denote the kappa rank score,,
Finally, many recent text simplification systems have utilized language models trained only on simplified data (CITATION; CITATION; CITATIONa; Wubben et al., 2012); improvements in simple language modeling could translate into improvements for these syste,,
For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (CITATION; CITATION; CITATION; CITATION),,
Lexical simplification approaches have been shown to improve the readability of texts (CITATION; CITATION), are useful in domains such as medical texts where major content changes are restricted, and they may be useful as a pre- or post-processing step for general simplification systems,,
