To evaluate the rankings, we use the metric from the SemEval 2012 task, the Cohens kappa coefficient CITATION between the system ranking and the human ranking, which we denote the kappa rank score,,
problems such as lexical simplification CITATION and predicting text simplicity (Eickhoff et al., 2010),,
cally based on the lexical simplification task from SemEval 2012 CITATION,,
For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (CITATION; CITATION; ,,
Finally, many recent text simplification systems have utilized language models trained only on simplified data (CITATION; CITATION; CITATIONa; Wubben et al., 2012); improvements in simple language modeling could translate into impr,,
2 Related Work If we view the normal data as out-of-domain data, then the problem of combining simple and normal data is similar to the language model domain adaption problem CITATION, in particular cross-domain adaptation CITATION where a domain-specific model is improved,,
; CITATIONa; Wubben et al., 2012) and in other languages CITATION,,
2 Related Work If we view the normal data as out-of-domain data, then the problem of combining simple and normal data is similar to the language model domain adaption problem CITATION, in particular cross-domain adaptation CITATION where a domain-specific model is improved by incorporating additional general data,,
for example the English Gigaword corpus (David CITATION)),,
data: - simple-only: simple sentences only - normal-only: normal sentences only - simple-X+normal: X simple sentences combined with a varying number of normal sentences To evaluate the language models we calculated the model perplexity CITATION on the simple side of the held-out data,,
Finally, many recent text simplification systems have utilized language models trained only on simplified data (CITATION; CITATION; CITATIONa; Wubben et al., 2012); improvements in simple language modeling could translate into improvements for these syste,,
In our case, the interpolated model combines the simple model estimate, ps(wi|wi2, wi1), and the normal model estimate, pn(wi|wi2, wi1), linearly (CITATION; CITATION): pinterpolated(wi|wi2, wi1) = pn(wi|wi2, wi1) + (1 ) ps(wi|wi2, wi1) where 0 1,,
CITATION provide a survey on the related problem of domain adaptation for machine learning ,,
complexity, and grammatical complexity (CITATION; CITATION; CITATIONb),,
Although our goal was not to create the best lexical simplification system, this approach would have ranked 6th out of 11 submitted systems in the SemEval 2012 competition CITATION,,
CITATION),,
Simple English Wikipedia has been previously used for many text simplification approaches (CITATION; CITATION; CITATION; CITATIONa; CITATION; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedia by both automatic measures and human perception (CITATIONb; 1538 \x0csimple normal sentences 385K 2540K words 7.15M 64.7M vocab size ,,
This is not the case for all monolingual translation tasks (CITATION; Coh,,
Adaptation techniques have been shown to improve language modeling performance based on perplexity (CITATION,,
Simple English language models have also been used as predictive features in other simplification sub-problems such as lexical simplification CITATION and predicting text simplicity (Eickhoff et al., 2010),,
Simple English Wikipedia has been previously used for many text simplification approaches (CITATION; CITATION; CITATION; CITATIONa; CITATION; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedia by both automatic measures and human perception (CITATIONb; 1538 \x0csimple normal sentences 385K 2540K words,,
Simple English Wikipedia has been previously used for many text simplification approaches (CITATION; CITATION; CITATION; CITATIONa; CITATION; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedi,,
Lexical simplification approaches have been shown to improve the readability of texts (CITATION; CITATION), are useful in domains such as medical texts where major content changes are restricted, and they may be useful as a pre- or post-processing step for general simplification systems,,
CITATION provide a survey on the related problem of domain adaptation for machine learning (also referred to as transfer learning), which utilizes similar techniques,,
Simple English Wikipedia has been previously used for many text simplification approaches (CITATION; CITATION; CITATION; CITATIONa; CITATION; Wubben et al., 2012) and has been shown to be simpler than normal E,,
To make this discrepancy more explicit, we created a sentence aligned data set by aligning the simple and normal articles using the approach from CITATIONb),,
Many recent statistical simplification techniques build upon models from machine translation and utilize a simple language model during simplification/decoding both in English (CITATION; CITATION; CITATIONa; Wubben et al., 2012) and in other languages CITATION,,
For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (CITATION; CITATION; CITATION; CITATION),,
ed data since the largest text compression data sets contain only a few thousand sentences (CITATION; CITATION; CITATION; CITATION),,
Lexical simplification is a sub-problem of the general text simplification problem CITATION; a sentence is simplified by substituting words or phrases in the sentence with simpler variations,,
nd Lapata, 2011; CITATIONa; Wubben et al., 2012); improvements in simple language modeling could translate into improvements for these systems,,
oodsend and Lapata, 2011; CITATIONa; Wubben et al., 2012) and in other languages CITATION,,
Finally, many recent text simplification systems have utilized language models trained only on simplified data (CITATION; CITATION; CITATIONa; Wubben et al., 2012); improvements in simple language modeling could translate into improvements for these systems,,
See CITATION for the full details of how the evaluation metric is calculated,,
 simplification sub-problems such as lexical simplification CITATION and predicting text simplicity (Eickhoff et al., 2010),,
 and in other languages CITATION,,
dze, 2010; CITATION; CITATIONb),,
ployed language models trained only on unsummarized text (CITATION; CITATION),,
010; CITATIONb),,
est text compression data sets contain only a few thousand sentences (CITATION; CITATION; CITATION; CITATION),,
This approach has the benefit of simplicity, however, better performance for combining related corpora has been seen by domain adaptation techniques which combine the data in more structured ways CITATION,,
Similarly for summarization, systems that have employed language models trained only on unsummarized text (CITATION; CITATION),,
Simple English Wikipedia has been previously used for many text simplification approaches (CITATION; CITATION; CITATION; CITATIONa; CITATION; Wubben et al., 2012) and has been shown to b,,
ude smaller than the amount of normal English data available (for example the English Gigaword corpus (David CITATION)),,
For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (CITATION; ,,
After preprocessing, the 60K articles represents less than half a million sentences which is orders of magnitude smaller than the amount of normal English data available (for example the English Gigaword corpus (David CITATION)),,
CITATION provide a survey on the related problem of domain adaptation for machine learning (also referred to as transfer learning), which utilizes si,,
In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain CITATION,,
Simple English Wikipedia has been previously used for many text simplification approaches (CITATION; CITATION; CITATION; CITATIONa; CITATION; Wubben et al., 201,,
This is not the case for all monolingual translation tasks (CITATION; CITATION),,
4.1 Experimental Setup We used trigram language models with interpolated Kneser-Kney discounting trained using the SRI language modeling toolkit CITATION,,
Adaptation techniques have been shown to improve language modeling performance based on perplexity CITATION and in application areas such as speech transcription CITATION and machine translation CITATION, though no previous research has examined the language model domain adaptation problem for text simplification,,
simple and normal data is similar to the language model domain adaption problem CITATION, in particular cross-domain adaptation CITATION where a domain-specific model is improved by incorporating additional general data,,
Table 1 shows the n-gram overlap proportions in a sentence aligned data set of 137K sentence pairs from aligning Simple English Wikipedia and English Wikipedia articles (CITATIONa).1 The data highlights two conflicting views: does the benefit of additional data outweigh the problem of the source of the data? Throughout the rest of this paper we refer to sentences/articles/text from English Wikipedia as normal and sentences/articles/text from Simple English Wikipedia as simple,,
Because frequencies tend to follow a Zipfian distribution, these rare n-grams make up a large portion of n-grams in real data CITATION,,
Simple English Wikipedia has been previously used for many text simplification approaches (CITATION; CITATION; CITATION; CITATIONa; CITATION; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedia by both automatic measures and human perception (CITATIONb; 1538 \x0csimple normal sentences 385K 2540K words 7.15M 64.7M vocab size 78K 307K Table 2: Summary counts for the simple-normal article aligned data set consisting of 60K article pairs,,
a; Wubben et al., 2012) and in other languages CITATION,,
