<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.555758">
b&quot;Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 697704
</bodyText>
<address confidence="0.654504">
Manchester, August 2008
</address>
<title confidence="0.627637">
Exploiting Constituent Dependencies for Tree Kernel-based Semantic
Relation Extraction
Longhua Qian Guodong Zhou Fang Kong Qiaoming Zhu Peide Qian
</title>
<author confidence="0.691365">
Jiangsu Provincial Key Lab for Computer Information Processing Technology
</author>
<affiliation confidence="0.991664">
School of Computer Science and Technology, Soochow University
</affiliation>
<address confidence="0.955835">
1 Shizi Street, Suzhou, China 215006
</address>
<email confidence="0.995948">
{qianlonghua,gdzhou,kongfang,qmzhu,pdqian}@suda.edu.cn
</email>
<sectionHeader confidence="0.990702" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998028428571429">
This paper proposes a new approach to
dynamically determine the tree span for
tree kernel-based semantic relation ex-
traction. It exploits constituent dependen-
cies to keep the nodes and their head
children along the path connecting the
two entities, while removing the noisy in-
formation from the syntactic parse tree,
eventually leading to a dynamic syntactic
parse tree. This paper also explores entity
features and their combined features in a
unified parse and semantic tree, which in-
tegrates both structured syntactic parse
information and entity-related semantic
information. Evaluation on the ACE
RDC 2004 corpus shows that our dy-
namic syntactic parse tree outperforms all
previous tree spans, and the composite
kernel combining this tree kernel with a
linear state-of-the-art feature-based ker-
nel, achieves the so far best performance.
</bodyText>
<sectionHeader confidence="0.998203" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.952264818181818">
Information extraction is one of the key tasks in
natural language processing. It attempts to iden-
tify relevant information from a large amount of
natural language text documents. Of three sub-
tasks defined by the ACE program1
, this paper
focuses exclusively on Relation Detection and
Characterization (RDC) task, which detects and
classifies semantic relationships between prede-
fined types of entities in the ACE corpus. For
2008. Licensed under the Creative Commons Attribution-
</bodyText>
<footnote confidence="0.839641">
Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
1
http://www.ldc.upenn.edu/Projects/ACE/
</footnote>
<bodyText confidence="0.987031023809524">
example, the sentence Microsoft Corp. is based
in Redmond, WA conveys the relation GPE-
AFF.Based between Microsoft Corp. [ORG]
and Redmond [GPE]. Due to limited accuracy
in state-of-the-art syntactic and semantic parsing,
reliably extracting semantic relationships be-
tween named entities in natural language docu-
ments is still a difficult, unresolved problem.
In the literature, feature-based methods have
dominated the research in semantic relation ex-
traction. Featured-based methods achieve prom-
ising performance and competitive efficiency by
transforming a relation example into a set of syn-
tactic and semantic features, such as lexical
knowledge, entity-related information, syntactic
parse trees and deep semantic information. How-
ever, detailed research (Zhou et al., 2005) shows
that its difficult to extract new effective features
to further improve the extraction accuracy.
Therefore, researchers turn to kernel-based
methods, which avoids the burden of feature en-
gineering through computing the similarity of
two discrete objects (e.g. parse trees) directly.
From prior work (Zelenko et al., 2003; Culotta
and Sorensen, 2004; Bunescu and Mooney, 2005)
to current research (Zhang et al., 2006; Zhou et
al., 2007), kernel methods have been showing
more and more potential in relation extraction.
The key problem for kernel methods on rela-
tion extraction is how to represent and capture
the structured syntactic information inherent in
relation instances. While kernel methods using
the dependency tree (Culotta and Sorensen, 2004)
and the shortest dependency path (Bunescu and
Mooney, 2005) suffer from low recall perform-
ance, convolution tree kernels (Zhang et al., 2006;
Zhou et al., 2007) over syntactic parse trees
achieve comparable or even better performance
than feature-based methods.
However, there still exist two problems re-
garding currently widely used tree spans. Zhang
et al. (2006) discover that the Shortest Path-
</bodyText>
<page confidence="0.997049">
697
</page>
<listItem confidence="0.529794333333333">
\x0cenclosed Tree (SPT) achieves the best perform-
ance. Zhou et al. (2007) further extend it to Con-
text-Sensitive Shortest Path-enclosed Tree (CS-
</listItem>
<bodyText confidence="0.998914724137931">
SPT), which dynamically includes necessary
predicate-linked path information. One problem
with both SPT and CS-SPT is that they may still
contain unnecessary information. The other prob-
lem is that a considerable number of useful con-
text-sensitive information is also missing from
SPT/CS-SPT, although CS-SPT includes some
contextual information relating to predicate-
linked path.
This paper proposes a new approach to dy-
namically determine the tree span for relation
extraction by exploiting constituent dependencies
to remove the noisy information, as well as keep
the necessary information in the parse tree. Our
motivation is to integrate dependency informa-
tion, which has been proven very useful to rela-
tion extraction, with the structured syntactic in-
formation to construct a concise and effective
tree span specifically targeted for relation extrac-
tion. Moreover, we also explore interesting com-
bined entity features for relation extraction via a
unified parse and semantic tree.
The other sections in this paper are organized
as follows. Previous work is first reviewed in
Section 2. Then, Section 3 proposes a dynamic
syntactic parse tree while the entity-related se-
mantic tree is described in Section 4. Evaluation
on the ACE RDC corpus is given in Section 5.
Finally, we conclude our work in Section 6.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999611472972973">
Due to space limitation, here we only review
kernel-based methods used in relation extraction.
For those interested in feature-based methods,
please refer to Zhou et al. (2005) for more details.
Zelenko et al. (2003) described a kernel be-
tween shallow parse trees to extract semantic
relations, where a relation instance is trans-
formed into the least common sub-tree connect-
ing the two entity nodes. The kernel matches the
nodes of two corresponding sub-trees from roots
to leaf nodes recursively layer by layer in a top-
down manner. Their method shows successful
results on two simple extraction tasks. Culotta
and Sorensen (2004) proposed a slightly general-
ized version of this kernel between dependency
trees, in which a successful match of two relation
instances requires the nodes to be at the same
layer and in the identical path starting from the
roots to the current nodes. These strong con-
straints make their kernel yield high precision but
very low recall on the ACE RDC 2003 corpus.
Bunescu and Mooney (2005) develop a shortest
path dependency tree kernel, which simply
counts the number of common word classes at
each node in the shortest paths between two enti-
ties in dependency trees. Similar to Culotta and
Sorensen (2004), this method also suffers from
high precision but low recall.
Zhang et al. (2006) describe a convolution tree
kernel (CTK, Collins and Duffy, 2001) to inves-
tigate various structured information for relation
extraction and find that the Shortest Path-
enclosed Tree (SPT) achieves the F-measure of
67.7 on the 7 relation types of the ACE RDC
2004 corpus. One problem with SPT is that it
loses the contextual information outside SPT,
which is usually critical for relation extraction.
Zhou et al. (2007) point out that both SPT and
the convolution tree kernel are context-free. They
expand SPT to CS-SPT by dynamically includ-
ing necessary predicate-linked path information
and extending the standard CTK to context-
sensitive CTK, obtaining the F-measure of 73.2
on the 7 relation types of the ACE RDC 2004
corpus. However, the CS-SPT only recovers part
of contextual information and may contain noisy
information as much as SPT.
In order to fully utilize the advantages of fea-
ture-based methods and kernel-based methods,
researchers turn to composite kernel methods.
Zhao and Grishman (2005) define several fea-
ture-based composite kernels to capture diverse
linguistic knowledge and achieve the F-measure
of 70.4 on the 7 relation types in the ACE RDC
2004 corpus. Zhang et al. (2006) design a com-
posite kernel consisting of an entity linear kernel
and a standard CTK, obtaining the F-measure of
72.1 on the 7 relation types in the ACE RDC
2004 corpus. Zhou et al. (2007) describe a com-
posite kernel to integrate a context-sensitive
CTK and a state-of-the-art linear kernel. It
achieves the so far best F-measure of 75.8 on the
7 relation types in the ACE RDC 2004 corpus.
In this paper, we will further study how to dy-
namically determine a concise and effective tree
span for a relation instance by exploiting con-
stituent dependencies inherent in the parse tree
derivation. We also attempt to fully capture both
the structured syntactic parse information and
entity-related semantic information, especially
combined entity features, via a unified parse and
semantic tree. Finally, we validate the effective-
ness of a composite kernel for relation extraction,
which combines a tree kernel and a linear kernel.
</bodyText>
<page confidence="0.997294">
698
</page>
<bodyText confidence="0.8659852">
\x0c3 Dynamic Syntactic Parse Tree
This section discusses how to generate dynamic
syntactic parse tree by employing constituent
dependencies to overcome the problems existing
in currently used tree spans.
</bodyText>
<subsectionHeader confidence="0.998655">
3.1 Constituent Dependencies in Parse Tree
</subsectionHeader>
<bodyText confidence="0.984103967741935">
Zhang et al. (2006) explore five kinds of tree
spans and find that the Shortest Path-enclosed
Tree (SPT) achieves the best performance. Zhou
et al. (2007) further propose Context-Sensitive
SPT (CS-SPT), which can dynamically deter-
mine the tree span by extending the necessary
predicate-linked path information outside SPT.
However, the key problem of how to represent
the structured syntactic parse tree is still partially
resolved. As we indicate as follows, current tree
spans suffer from two problems:
(1) Both SPT and CS-SPT still contain unnec-
essary information. For example, in the sentence
...bought one of towns two meat-packing
plants, the condensed information one of
plants is sufficient to determine DISC rela-
tionship between the entities one [FAC] and
plants [FAC], while SPT/CS-SPT include the
redundant underlined part. Therefore more un-
necessary information can be safely removed
from SPT/CS-SPT.
(2) CS-SPT only captures part of context-
sensitive information relating to predicate-linked
structure (Zhou et al., 2007) and still loses much
context-sensitive information. Lets take the
same example sentence ...bought one of towns
two meat-packing plants, where indeed there is
no relationship between the entities one [FAC]
and town [GPE]. Nevertheless, the information
contained in SPT/CS-SPT (one of town) may
easily lead to their relationship being misclassi-
fied as DISC, which is beyond our expectation.
Therefore the underlined part outside SPT/CS-
SPT should be recovered so as to differentiate it
from positive instances.
Since dependency plays a key role in many
NLP problems such as syntactic parsing, seman-
tic role labeling as well as semantic relation ex-
traction, our motivation is to exploit dependency
knowledge to distinguish the necessary evidence
from the unnecessary information in the struc-
tured syntactic parse tree.
On one hand, lexical or word-word depend-
ency indicates the relationship among words
occurring in the same sentence, e.g. predicate-
argument dependency means that arguments are
dependent on their target predicates, modifier-
head dependency means that modifiers are de-
pendent on their head words. This dependency
relationship offers a very condensed representa-
tion of the information needed to assess the rela-
tionship in the forms of the dependency tree (Cu-
lotta and Sorensen, 2004) or the shortest depend-
ency path (Bunescu and Mooney, 2005) that in-
cludes both entities.
On the other hand, when the parse tree corre-
sponding to the sentence is derived using deriva-
tion rules from the bottom to the top, the word-
word dependencies extend upward, making a
unique head child containing the head word for
every non-terminal constituent. As indicated as
follows, each CFG rule has the form:
</bodyText>
<equation confidence="0.669135">
P Ln...L1 H R1...Rm
</equation>
<bodyText confidence="0.999051384615385">
Here, P is the parent node, H is the head child of
the rule, Ln...L1 and R1...Rm are left and right
modifiers of H respectively, and both n and m
may be zero. In other words, the parent node P
depends on the head child H, this is what we call
constituent dependency. Vice versa, we can also
determine the head child of a constituent in terms
of constituent dependency. Our hypothesis stipu-
lates that the contribution of the parse tree to es-
tablishing a relationship is almost exclusively
concentrated in the path connecting the two enti-
ties, as well as the head children of constituent
nodes along this path.
</bodyText>
<subsectionHeader confidence="0.8790665">
3.2 Generation of Dynamic Syntactic Parse
Tree
</subsectionHeader>
<bodyText confidence="0.988125043478261">
Starting from the Minimum Complete Tree
(MCT, the complete sub-tree rooted by the near-
est common ancestor of the two entities under
consideration) as the representation of each rela-
tion instance, along the path connecting two enti-
ties, the head child of every node is found ac-
cording to various constituent dependencies.
Then the path nodes and their head children are
kept while any other nodes are removed from the
tree. Eventually we arrive at a tree called Dy-
namic Syntactic Parse Tree (DSPT), which is
dynamically determined by constituent depend-
encies and only contains necessary information
as expected.
There exist a considerable number of constitu-
ent dependencies in CFG as described by Collins
(2003). However, since our task is to extract the
relationship between two named entities, our fo-
cus is on how to condense Noun-Phrases (NPs)
and other useful constituents for relation extrac-
tion. Therefore constituent dependencies can be
classified according to constituent types of the
CFG rules:
</bodyText>
<page confidence="0.988176">
699
</page>
<bodyText confidence="0.971790142857143">
\x0c(1) Modification within base-NPs: base-NPs
mean that they do not directly dominate an NP
themselves, unless the dominated NP is a posses-
sive NP. The noun phrase right above the entity
headword, whose mention type is nominal or
name, can be categorized into this type. In this
case, the entity headword is also the headword of
the noun phrase, thus all the constituents before
the headword are dependent on the headword,
and may be removed from the parse tree, while
the headword and the constituents right after the
headword remain unchanged. For example, in the
sentence ...bought one of towns two meat-
packing plants as illustrated in Figure 1(a), the
constituents before the headword plants can
be removed from the parse tree. In this way the
parse tree one of plants could capture the
DISC relationship more concisely and pre-
cisely. Another interesting example is shown in
Figure 1(b), where the base-NP of the second
entity town is a possessive NP and there is no
relationship between the entities one and
town defined in the ACE corpus. For both SPT
and CS-SPT, this example would be condensed
to one of town and therefore easily misclassi-
fied as the DISC relationship between the two
entities. In the contrast, our DSPT can avoid this
problem by keeping the constituent s and the
headword plants.
(2) Modification to NPs: except base-NPs,
other modification to NPs can be classified into
this type. Usually these NPs are recursive, mean-
ing that they contain another NP as their child.
The CFG rules corresponding to these modifica-
tions may have the following forms:
</bodyText>
<table confidence="0.402638">
NP NP SBAR [relative clause]
NP NP VP [reduced relative]
NP NP PP [PP attachment]
</table>
<bodyText confidence="0.974862714285714">
Here, the NPs in bold mean that the path con-
necting the two entities passes through them. For
every right hand side, the NP in bold is modified
by the constituent following them. That is, the
latter is dependent on the former, and may be
reduced to a single NP. In Figure 1(c) we show a
sentence one of about 500 people nominated
for ..., where there exists a DISC relationship
between the entities one and people. Since
the reduced relative nominated for ... modifies
and is therefore dependent on the people, they
can be removed from the parse tree, that is, the
right side (NP VP) can be reduced to the left
hand side, which is exactly a single NP.
</bodyText>
<figure confidence="0.996890994708995">
(a) Removal of constituents before the headword in base-NP
(b) Keeping of constituents after the headword in base-NP
NN
one
IN
of
DT
the
NN
town
POS
&apos;s
E-FAC
NN
plants
two
CD NN
one
IN
of
NN
town
POS
&apos;s
E-FAC
NN
plants
meat-packing
JJ
NN
one
PP
IN
of
NP
DT
the
NN
town
POS
&apos;s
NN
plants
two
CD NN
one
IN
of
NN
plants
meat-packing
JJ NN
one
IN
of
RB
about
QP
CD
500
NNS
people
...
nominated
VBN
for
IN
VP
PP
...
E2-PER
NN
one
IN
of
NNS
people
NN
property
PRP
he
VP
VBZ IN
in
NP
PP
state
NNS
the
NP
JJ
rental
S
owns
DT NN
property
PRP
he
VP
VBZ
owns
governors from connecticut
NNS IN
NP
E-GPE
NNP
,
,
south
NP
E-GPE
NNP
dakota
NNP
,
,
and
CC
montana
NNP
governors from
NNS IN
montana
NNP
(c) Reduction of modification to NP
(d) Removal of arguments to verb
(e) Reduction of conjuncts for NP coordination
E-GPE
NP
PP
E1-FAC
NP
E2-FAC
NP
E1-FAC
NP
NP
NP
NP
E2-FAC E1-PER
NP
NP
PP
NP
NP
NP
E1-PER
PP
NP
E2-PER
NP
SBAR
E2-PER
S
NP
NP
E1-FAC
PP
NP
NP
E1-PER
NP
E2-GPE
NP
E1-PER
PP
NP
NP
NP
E2-GPE
NP
E1-FAC E2-PER
NP
NP
SBAR
NP
NP
E1-FAC
PP
NP
NP
E2-GPE
NP
NP
E1-PER
PP
NP
NP
E2-GPE
</figure>
<figureCaption confidence="0.999469">
Figure 1. Removal and reduction of constituents using dependencies
</figureCaption>
<page confidence="0.580918">
700
</page>
<bodyText confidence="0.920422377777778">
\x0c(3) Arguments/adjuncts to verbs: this type
includes the CFG rules in which the left side in-
cludes S, SBAR or VP. An argument represents
the subject or object of a verb, while an adjunct
indicates the location, date/time or way of the
action corresponding to the verb. They depend
on the verb and can be removed if they are not
included in the path connecting the two entities.
However, when the parent tag is S or SBAR, and
its child VP is not included in the path, this VP
should be recovered to indicate the predicate
verb. Figure 1(d) shows a sentence ... maintain
rental property he owns in the state, where the
ART.User-or-Owner relation holds between
the entities property and he. While PP can be
removed from the rule (VP VBZ PP), the
VP should be kept in the rule (S NP VP).
Consequently, the tree span looks more concise
and precise for relation extraction.
(4) Coordination conjunctions: In coordina-
tion constructions, several peer conjuncts may be
reduced into a single constituent. Although the
first conjunct is always considered as the head-
word (Collins, 2003), actually all the conjuncts
play an equal role in relation extraction. As illus-
trated in Figure 1(e), the NP coordination in the
sentence (governors from connecticut, south
dakota, and montana) can be reduced to a single
NP (governors from montana) by keeping the
conjunct in the path while removing the other
conjuncts.
(5) Modification to other constituents: ex-
cept for the above four types, other CFG rules
fall into this type, such as modification to PP,
ADVP and PRN etc. These cases are similar to
arguments/adjuncts to verbs, but less frequent
than them, so we will not detail this scenario.
In fact, SPT (Zhang et al., 2006) can be ar-
rived at by carrying out part of the above re-
moval operations using a single rule (i.e. all the
constituents outside the linking path should be
removed) and CS-CSPT (Zhou et al., 2007) fur-
ther recovers part of necessary context-sensitive
information outside SPT, this justifies that SPT
performs well, while CS-SPT outperforms SPT.
</bodyText>
<sectionHeader confidence="0.963617" genericHeader="method">
4 Entity-related Semantic Tree
</sectionHeader>
<bodyText confidence="0.9982736">
Entity semantic features, such as entity headword,
entity type and subtype etc., impose a strong
constraint on relation types in terms of relation
definition by the ACE RDC task. Experiments by
Zhang et al. (2006) show that linear kernel using
only entity features contributes much when com-
bined with the convolution parse tree kernel.
Qian et al. (2007) further indicates that among
these entity features, entity type, subtype, and
mention type, as well as the base form of predi-
cate verb, contribute most while the contribution
of other features, such as entity class, headword
and GPE role, can be ignored.
In order to effectively capture entity-related
semantic features, and their combined features as
well, especially bi-gram or tri-gram features, we
build an Entity-related Semantic Tree (EST) in
three ways as illustrated in Figure 2. In the ex-
ample sentence they re here, which is ex-
cerpted from the ACE RDC 2004 corpus, there
exists a relationship Physical.Located between
the entities they [PER] and here
[GPE.Population-Center]. The features are en-
coded as TP, ST, MT and PVB, which
denote type, subtype, mention-type of the two
entities, and the base form of predicate verb if
existing (nearest to the 2nd
entity along the path
connecting the two entities) respectively. For
example, the tag TP1 represents the type of the
</bodyText>
<page confidence="0.545988">
1st
</page>
<bodyText confidence="0.941022">
entity, and the tag ST2 represents the sub-
type of the 2nd
entity. The three entity-related
semantic tree setups are depicted as follows:
</bodyText>
<figure confidence="0.954514695652174">
TP2
TP1
(a) Bag Of Features(BOF)
ENT
ST2
ST1 MT2
MT1 PVB
(c) Entity-Paired Tree(EPT)
ENT
E1 E2
(b) Feature Paired Tree(FPT)
ENT
TP ST MT
ST1
TP1 MT1 TP2 ST2 MT2
PVB
TP1 TP2 ST1 ST2 MT1 MT2
PVB
PER null PRO GPE Pop. PRO be
PER null PRO GPE Pop. PRO
be
PER GPE null Pop. PRO PRO
be
</figure>
<figureCaption confidence="0.9245785">
Figure 2. Different setups for entity-related se-
mantic tree (EST)
</figureCaption>
<bodyText confidence="0.934611">
(a) Bag of Features (BOF, e.g. Fig. 2(a)): all
feature nodes uniformly hang under the root node,
so the tree kernel simply counts the number of
common features between two relation instances.
This tree setup is similar to linear entity kernel
explored by Zhang et al. (2006).
(b) Feature-Paired Tree (FPT, e.g. Fig. 2(b)):
the features of two entities are grouped into dif-
ferent types according to their feature names, e.g.
TP1 and TP2 are grouped to TP. This tree
setup is aimed to capture the additional similarity
</bodyText>
<page confidence="0.983236">
701
</page>
<bodyText confidence="0.971629272727273">
\x0cof the single feature combined from different
entities, i.e., the first and the second entities.
(c) Entity-Paired Tree (EPT, e.g. Fig. 2(c)): all
the features relating to an entity are grouped to
nodes E1 or E2, thus this tree kernel can fur-
ther explore the equivalence of combined entity
features only relating to one of the entities be-
tween two relation instances.
In fact, the BOF only captures the individual
entity features, while the FPT/EPT can addition-
ally capture the bi-gram/tri-gram features respec-
tively.
Rather than constructing a composite kernel,
we incorporate the EST into the DSPT to pro-
duce a Unified Parse and Semantic Tree (UPST)
to investigate the contribution of the EST to rela-
tion extraction. The entity features can be at-
tached under the top node, the entity nodes, or
directly combined with the entity nodes as in
Figure 1. However, detailed evaluation (Qian et
al., 2007) indicates that the UPST achieves the
best performance when the feature nodes are at-
tached under the top node. Hence, we also attach
three kinds of entity-related semantic trees (i.e.
BOF, FPT and EPT) under the top node of the
DSPT right after its original children. Thereafter,
we employ the standard CTK (Collins and Duffy,
2001) to compute the similarity between two
UPSTs, since this CTK and its variations are
successfully applied in syntactic parsing, seman-
tic role labeling (Moschitti, 2004) and relation
extraction (Zhang et al., 2006; Zhou et al., 2007)
as well.
</bodyText>
<sectionHeader confidence="0.996272" genericHeader="method">
5 Experimentation
</sectionHeader>
<bodyText confidence="0.984426">
This section will evaluate the effectiveness of the
DSPT and the contribution of entity-related se-
mantic information through experiments.
</bodyText>
<subsectionHeader confidence="0.995465">
5.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.940259136363636">
For evaluation, we use the ACE RDC 2004 cor-
pus as the benchmark data. This data set contains
451 documents and 5702 relation instances. It
defines 7 entity types, 7 major relation types and
23 subtypes. For comparison with previous work,
evaluation is done on 347 (nwire/bnews) docu-
ments and 4307 relation instances using 5-fold
cross-validation. Here, the corpus is parsed using
Charniaks parser (Charniak, 2001) and relation
instances are generated by iterating over all pairs
of entity mentions occurring in the same sentence
with given true mentions and coreferential in-
formation. In our experimentations, SVMlight
(Joachims, 1998) with the tree kernel function
(Moschitti, 2004) 2
is selected as our classifier.
For efficiency, we apply the one vs. others
strategy, which builds K classifiers so as to
separate one class from all others. For
comparison purposes, the training parameters C
(SVM) and (tree kernel) are also set to 2.4 and
0.4 respectively.
</bodyText>
<subsectionHeader confidence="0.995724">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.996716666666667">
Table 1 evaluates the contributions of different
kinds of constituent dependencies to extraction
performance on the 7 relation types of the ACE
RDC 2004 corpus using the convolution parse
tree kernel as depicted in Figure 1. The MCT
with only entity-type information is first used as
the baseline, and various constituent dependen-
cies are then applied sequentially to dynamically
reshaping the tree in two different modes:
</bodyText>
<listItem confidence="0.626442333333333">
--[M1] Respective: every constituent depend-
ency is individually applied on MCT.
--[M2] Accumulative: every constituent de-
</listItem>
<bodyText confidence="0.608654">
pendency is incrementally applied on the previ-
ously derived tree span, which begins with the
</bodyText>
<figure confidence="0.907068325">
MCT and eventually gives rise to a Dynamic
Syntactic Parse Tree (DSPT).
Dependency types P(%) R(%) F
MCT (baseline) 75.1 53.8 62.7
Modification within
base-NPs
76.5
(59.8)
59.8
(59.8)
67.1
(67.1)
Modification to NPs
77.0
(76.2)
63.2
(56.9)
69.4
(65.1)
Arguments/adjuncts to verb
77.1
(76.1)
63.9
(57.5)
69.9
(65.5)
Coordination conjunctions
77.3
(77.3)
65.2
(55.1)
70.8
(63.8)
Other modifications
77.4
(75.0)
65.4
(53.7)
70.9
(62.6)
</figure>
<tableCaption confidence="0.99822">
Table 1. Contribution of constituent dependen-
</tableCaption>
<bodyText confidence="0.997298666666667">
cies in respective mode (inside parentheses) and
accumulative mode (outside parentheses)
The table shows that the final DSPT achieves
the best performance of 77.4%/65.4%/70.9 in
precision/recall/F-measure respectively after ap-
plying all the dependencies, with the increase of
F-measure by 8.2 units compared to the baseline
MCT. This indicates that reshaping the tree by
exploiting constituent dependencies may signifi-
cantly improve extraction accuracy largely due to
the increase in recall. It further suggests that con-
stituent dependencies knowledge is very effec-
</bodyText>
<footnote confidence="0.4895825">
2
http://ai-nlp.info.uniroma2.it/moschitti/
</footnote>
<page confidence="0.959161">
702
</page>
<bodyText confidence="0.922214763157894">
\x0ctive and can be fully utilized in tree kernel-based
relation extraction. This table also shows that:
(1) Both modification within base-NPs and
modification to NPs contribute much to perform-
ance improvement, acquiring the increase of F-
measure by 4.4/2.4 units in mode M1 and 4.4/2.3
units in mode M2 respectively. This indicates the
local characteristic of semantic relations, which
can be effectively captured by NPs near the two
involved entities in the DSPT.
(2) All the other three dependencies show mi-
nor contribution to performance enhancement,
they improve the F-measure only by 2.8/0.9/-0.1
units in mode M1 and 0.5/0.9/0.1 units in mode
M2. This may be due to the reason that these de-
pendencies only remove the nodes far from the
two entities.
We compare in Table 2 the performance of
Unified Parse and Semantic Trees with different
kinds of Entity Semantic Tree setups using stan-
dard convolution tree kernel, while the SPT and
DSPT with only entity-type information are
listed for reference. It shows that:
(1) All the three unified parse and semantic
tree kernels significantly outperform the DSPT
kernel, obtaining an average increase of ~4 units
in F-measure. This means that they can effec-
tively capture both the structured syntactic in-
formation and the entity-related semantic fea-
tures.
(2) The Unified Parse and Semantic Tree with
Feature-Paired Tree achieves the best perform-
ance of 80.1/70.7/75.1 in P/R/F respectively,
with an increase of F-measure by 0.4/0.3 units
over BOF and EPT respectively. This suggests
that additional bi-gram entity features captured
by FPT are more useful than tri-gram entity fea-
tures captured by EPT.
</bodyText>
<table confidence="0.977148833333333">
Tree setups P(%) R(%) F
SPT 76.3 59.8 67.1
DSPT 77.4 65.4 70.9
UPST (BOF) 80.4 69.7 74.7
UPST (FPT) 80.1 70.7 75.1
UPST (EPT) 79.9 70.2 74.8
</table>
<tableCaption confidence="0.998989">
Table 2. Performance of Unified Parse and
</tableCaption>
<bodyText confidence="0.9924066">
Semantic Trees (UPSTs) on the 7 relation types
of the ACE RDC 2004 corpus
In Table 3 we summarize the improvements of
different tree setups over SPT. It shows that in a
similar setting, our DSPT outperforms SPT by
</bodyText>
<subsectionHeader confidence="0.819935">
3.8 units in F-measure, while CS-SPT outper-
</subsectionHeader>
<bodyText confidence="0.9991039">
forms SPT by 1.3 units in F-measure. This sug-
gests that the DSPT performs best among these
tree spans. It also shows that the Unified Parse
and Semantic Tree with Feature-Paired Tree per-
form significantly better than the other two tree
setups (i.e., CS-SPT and DSPT) by 6.7/4.2 units
in F-measure respectively. This implies that the
entity-related semantic information is very useful
and contributes much when they are incorporated
into the parse tree for relation extraction.
</bodyText>
<table confidence="0.9331356">
Tree setups P(%) R(%) F
CS-SPT over SPT3
1.5 1.1 1.3
DSPT over SPT 1.1 5.6 3.8
UPST (FPT) over SPT 3.8 10.9 8.0
</table>
<tableCaption confidence="0.997526">
Table 3. Improvements of different tree setups
</tableCaption>
<bodyText confidence="0.9784056875">
over SPT on the ACE RDC 2004 corpus
Finally, Table 4 compares our system with
other state-of-the-art kernel-based systems on the
7 relation types of the ACE RDC 2004 corpus. It
shows that our UPST outperforms all previous
tree setups using one single kernel, and even bet-
ter than two previous composite kernels (Zhang
et al., 2006; Zhao and Grishman, 2005). Fur-
thermore, when the UPST (FPT) kernel is com-
bined with a linear state-of-the-state feature-
based kernel (Zhou et al., 2005) into a composite
one via polynomial interpolation in a setting
similar to Zhou et al. (2007) (i.e. polynomial de-
gree d=2 and coefficient I=0.3), we get the so far
best performance of 77.1 in F-measure for 7 rela-
tion types on the ACE RDC 2004 data set.
</bodyText>
<table confidence="0.923581181818182">
Systems P(%) R(%) F
Ours:
composite kernel
83.0 72.0 77.1
Zhou et al., (2007):
composite kernel
82.2 70.2 75.8
Zhang et al., (2006):
composite kernel
76.1 68.4 72.1
Zhao and Grishman, (2005):4
composite kernel
69.2 70.5 70.4
Ours:
CTK with UPST
80.1 70.7 75.1
Zhou et al., (2007): context-
sensitive CTK with CS-SPT
81.1 66.7 73.2
Zhang et al., (2006):
CTK with SPT
74.1 62.4 67.7
</table>
<tableCaption confidence="0.9889">
Table 4. Comparison of different systems on
</tableCaption>
<reference confidence="0.265845">
the ACE RDC 2004 corpus
</reference>
<page confidence="0.962864">
3
</page>
<bodyText confidence="0.919284">
We arrive at these values by subtracting P/R/F
</bodyText>
<reference confidence="0.975486">
(79.6/5.6/71.9) of Shortest-enclosed Path Tree from P/R/F
(81.1/6.7/73.2) of Dynamic Context-Sensitive Shortest-
enclosed Path Tree according to Table 2 (Zhou et al., 2007)
</reference>
<page confidence="0.983134">
4
</page>
<bodyText confidence="0.973375666666667">
There might be some typing errors for the performance
reported in Zhao and Grishman (2005) since P, R and F do
not match.
</bodyText>
<page confidence="0.9962">
703
</page>
<sectionHeader confidence="0.945721" genericHeader="conclusions">
\x0c6 Conclusion
</sectionHeader>
<bodyText confidence="0.99969732">
This paper further explores the potential of struc-
tured syntactic information for tree kernel-based
relation extraction, and proposes a new approach
to dynamically determine the tree span (DSPT)
for relation instances by exploiting constituent
dependencies. We also investigate different ways
of how entity-related semantic features and their
combined features can be effectively captured in
a Unified Parse and Semantic Tree (UPST).
Evaluation on the ACE RDC 2004 corpus shows
that our DSPT is appropriate for structured repre-
sentation of relation instances. We also find that,
in addition to individual entity features, com-
bined entity features (especially bi-gram) con-
tribute much when they are combined with a
DPST into a UPST. And the composite kernel,
combining the UPST kernel and a linear state-of-
the-art kernel, yields the so far best performance.
For the future work, we will focus on improv-
ing performance of complex structured parse
trees, where the path connecting the two entities
involved in a relationship is too long for current
kernel methods to take effect. Our preliminary
experiment of applying certain discourse theory
exhibits certain positive results.
</bodyText>
<sectionHeader confidence="0.974828" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.8849234">
This research is supported by Project 60673041
under the National Natural Science Foundation
of China, Project 2006AA01Z147 under the
863 National High-Tech Research and Devel-
opment of China, and the National Research
Foundation for the Doctoral Program of Higher
Education of China under Grant No.
20060285008. We would also like to thank the
excellent and insightful comments from the three
anonymous reviewers.
</reference>
<sectionHeader confidence="0.584365" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999736514705882">
Bunescu, Razvan C. and Raymond J. Mooney. 2005.
A Shortest Path Dependency Kernel for Relation
Extraction. In Proceedings of the Human Language
Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP-2005), pages 724-731. Vancover, B.C.
Charniak, Eugene. 2001. Intermediate-head Parsing
for Language Models. In Proceedings of the 39th
Annual Meeting of the Association of Computa-
tional Linguistics (ACL-2001), pages 116-123.
Collins, Michael. 2003. Head-Driven Statistics Mod-
els for Natural Language Parsing. Computational
linguistics, 29(4): 589-617.
Collins, Michael and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Proceedings of
Neural Information Processing Systems (NIPS-
2001), pages 625-632. Cambridge, MA.
Culotta, Aron and Jeffrey Sorensen. 2004. Depend-
ency tree kernels for relation extraction. In Pro-
ceedings of the 42nd Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL-2004),
pages 423-439. Barcelona, Spain.
Joachims, Thorsten. 1998. Text Categorization with
Support Vector Machine: learning with many rele-
vant features. In Proceedings of the 10th European
Conference on Machine Learning (ECML-1998),
pages 137-142. Chemnitz, Germany.
Moschitti, Alessandro. 2004. A Study on Convolution
Kernels for Shallow Semantic Parsing. In Proceed-
ings of the 42nd Annual Meeting of the Association
of Computational Linguistics (ACL-2004). Barce-
lona, Spain.
Qian, Longhua, Guodong Zhou, Qiaoming Zhu and
Peide Qian. 2007. Relation Extraction using Con-
volution Tree Kernel Expanded with Entity Fea-
tures. In Proceedings of the 21st Pacific Asian
Conference on Language, Information and Compu-
tation (PACLIC-21), pages 415-421. Seoul, Korea.
Zelenko, Dmitry, Chinatsu Aone and Anthony Rich-
ardella. 2003. Kernel Methods for Relation Extrac-
tion. Journal of Machine Learning Research,
3(2003): 1083-1106.
Zhang, Min, Jie Zhang, Jian Su and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association of Compu-
tational Linguistics (COLING/ACL-2006), pages
825-832. Sydney, Australia.
Zhao, Shubin and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing of the Association of Computational Linguistics
(ACL-2005), pages 419-426. Ann Arbor, USA.
Zhou, Guodong, Jian Su, Jie Zhang and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meet-
ing of the Association of Computational Linguistics
(ACL-2005), pages 427-434. Ann Arbor, USA.
Zhou, Guodong, Min Zhang, Donghong Ji and
Qiaoming Zhu. 2007. Tree Kernel-based Relation
Extraction with Context-Sensitive Structured Parse
Tree Information. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning (EMNLP/CoNLL-2007), pages
728-736. Prague, Czech.
</reference>
<page confidence="0.993232">
704
</page>
<figure confidence="0.243334">
\x0c&quot;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.446177">
<note confidence="0.755865">b&quot;Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 697704 Manchester, August 2008</note>
<title confidence="0.9950255">Exploiting Constituent Dependencies for Tree Kernel-based Semantic Relation Extraction</title>
<author confidence="0.997872">Longhua Qian Guodong Zhou Fang Kong Qiaoming Zhu Peide Qian</author>
<affiliation confidence="0.9759865">Jiangsu Provincial Key Lab for Computer Information Processing Technology School of Computer Science and Technology, Soochow University</affiliation>
<address confidence="0.946118">1 Shizi Street, Suzhou, China 215006</address>
<email confidence="0.91958">qianlonghua@suda.edu.cn</email>
<email confidence="0.91958">gdzhou@suda.edu.cn</email>
<email confidence="0.91958">kongfang@suda.edu.cn</email>
<email confidence="0.91958">qmzhu@suda.edu.cn</email>
<email confidence="0.91958">pdqian@suda.edu.cn</email>
<abstract confidence="0.995093909090909">This paper proposes a new approach to dynamically determine the tree span for tree kernel-based semantic relation extraction. It exploits constituent dependencies to keep the nodes and their head children along the path connecting the two entities, while removing the noisy information from the syntactic parse tree, eventually leading to a dynamic syntactic parse tree. This paper also explores entity features and their combined features in a unified parse and semantic tree, which integrates both structured syntactic parse information and entity-related semantic information. Evaluation on the ACE RDC 2004 corpus shows that our dynamic syntactic parse tree outperforms all previous tree spans, and the composite kernel combining this tree kernel with a linear state-of-the-art feature-based kernel, achieves the so far best performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>the ACE RDC</author>
</authors>
<title>corpus (79.6/5.6/71.9) of Shortest-enclosed Path Tree from P/R/F (81.1/6.7/73.2) of Dynamic Context-Sensitive Shortestenclosed Path Tree according to Table 2 (Zhou et al.,</title>
<date>2004</date>
<booktitle>Project 60673041 under the National Natural Science Foundation of China, Project 2006AA01Z147 under the 863 National High-Tech Research and Development of China, and the National Research Foundation for the Doctoral Program of Higher Education of China under Grant No.</booktitle>
<contexts>
<context position="1126" citStr="RDC 2004" startWordPosition="151" endWordPosition="152">is paper proposes a new approach to dynamically determine the tree span for tree kernel-based semantic relation extraction. It exploits constituent dependencies to keep the nodes and their head children along the path connecting the two entities, while removing the noisy information from the syntactic parse tree, eventually leading to a dynamic syntactic parse tree. This paper also explores entity features and their combined features in a unified parse and semantic tree, which integrates both structured syntactic parse information and entity-related semantic information. Evaluation on the ACE RDC 2004 corpus shows that our dynamic syntactic parse tree outperforms all previous tree spans, and the composite kernel combining this tree kernel with a linear state-of-the-art feature-based kernel, achieves the so far best performance. 1 Introduction Information extraction is one of the key tasks in natural language processing. It attempts to identify relevant information from a large amount of natural language text documents. Of three subtasks defined by the ACE program1 , this paper focuses exclusively on Relation Detection and Characterization (RDC) task, which detects and classifies semantic r</context>
<context position="7004" citStr="RDC 2004" startWordPosition="1046" endWordPosition="1047">ACE RDC 2003 corpus. Bunescu and Mooney (2005) develop a shortest path dependency tree kernel, which simply counts the number of common word classes at each node in the shortest paths between two entities in dependency trees. Similar to Culotta and Sorensen (2004), this method also suffers from high precision but low recall. Zhang et al. (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Pathenclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus. One problem with SPT is that it loses the contextual information outside SPT, which is usually critical for relation extraction. Zhou et al. (2007) point out that both SPT and the convolution tree kernel are context-free. They expand SPT to CS-SPT by dynamically including necessary predicate-linked path information and extending the standard CTK to contextsensitive CTK, obtaining the F-measure of 73.2 on the 7 relation types of the ACE RDC 2004 corpus. However, the CS-SPT only recovers part of contextual information and may contain noisy information as much as SPT. In order to fully u</context>
<context position="8320" citStr="RDC 2004" startWordPosition="1264" endWordPosition="1265">l methods. Zhao and Grishman (2005) define several feature-based composite kernels to capture diverse linguistic knowledge and achieve the F-measure of 70.4 on the 7 relation types in the ACE RDC 2004 corpus. Zhang et al. (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. Zhou et al. (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. It achieves the so far best F-measure of 75.8 on the 7 relation types in the ACE RDC 2004 corpus. In this paper, we will further study how to dynamically determine a concise and effective tree span for a relation instance by exploiting constituent dependencies inherent in the parse tree derivation. We also attempt to fully capture both the structured syntactic parse information and entity-related semantic information, especially combined entity features, via a unified parse and semantic tree. Finally, we validate the effectiveness of a composite kernel for relation extraction, which combines a tree kernel and a linear kernel. 698 \x0c3 Dynamic Syntactic Parse Tree This section dis</context>
<context position="19990" citStr="RDC 2004" startWordPosition="3236" endWordPosition="3237">ion parse tree kernel. Qian et al. (2007) further indicates that among these entity features, entity type, subtype, and mention type, as well as the base form of predicate verb, contribute most while the contribution of other features, such as entity class, headword and GPE role, can be ignored. In order to effectively capture entity-related semantic features, and their combined features as well, especially bi-gram or tri-gram features, we build an Entity-related Semantic Tree (EST) in three ways as illustrated in Figure 2. In the example sentence they re here, which is excerpted from the ACE RDC 2004 corpus, there exists a relationship Physical.Located between the entities they [PER] and here [GPE.Population-Center]. The features are encoded as TP, ST, MT and PVB, which denote type, subtype, mention-type of the two entities, and the base form of predicate verb if existing (nearest to the 2nd entity along the path connecting the two entities) respectively. For example, the tag TP1 represents the type of the 1st entity, and the tag ST2 represents the subtype of the 2nd entity. The three entity-related semantic tree setups are depicted as follows: TP2 TP1 (a) Bag Of Features(BOF) ENT ST2 ST1</context>
<context position="23110" citStr="RDC 2004" startWordPosition="3762" endWordPosition="3763">OF, FPT and EPT) under the top node of the DSPT right after its original children. Thereafter, we employ the standard CTK (Collins and Duffy, 2001) to compute the similarity between two UPSTs, since this CTK and its variations are successfully applied in syntactic parsing, semantic role labeling (Moschitti, 2004) and relation extraction (Zhang et al., 2006; Zhou et al., 2007) as well. 5 Experimentation This section will evaluate the effectiveness of the DSPT and the contribution of entity-related semantic information through experiments. 5.1 Experimental Setting For evaluation, we use the ACE RDC 2004 corpus as the benchmark data. This data set contains 451 documents and 5702 relation instances. It defines 7 entity types, 7 major relation types and 23 subtypes. For comparison with previous work, evaluation is done on 347 (nwire/bnews) documents and 4307 relation instances using 5-fold cross-validation. Here, the corpus is parsed using Charniaks parser (Charniak, 2001) and relation instances are generated by iterating over all pairs of entity mentions occurring in the same sentence with given true mentions and coreferential information. In our experimentations, SVMlight (Joachims, 1998) wit</context>
<context position="27685" citStr="RDC 2004" startWordPosition="4472" endWordPosition="4473">c features. (2) The Unified Parse and Semantic Tree with Feature-Paired Tree achieves the best performance of 80.1/70.7/75.1 in P/R/F respectively, with an increase of F-measure by 0.4/0.3 units over BOF and EPT respectively. This suggests that additional bi-gram entity features captured by FPT are more useful than tri-gram entity features captured by EPT. Tree setups P(%) R(%) F SPT 76.3 59.8 67.1 DSPT 77.4 65.4 70.9 UPST (BOF) 80.4 69.7 74.7 UPST (FPT) 80.1 70.7 75.1 UPST (EPT) 79.9 70.2 74.8 Table 2. Performance of Unified Parse and Semantic Trees (UPSTs) on the 7 relation types of the ACE RDC 2004 corpus In Table 3 we summarize the improvements of different tree setups over SPT. It shows that in a similar setting, our DSPT outperforms SPT by 3.8 units in F-measure, while CS-SPT outperforms SPT by 1.3 units in F-measure. This suggests that the DSPT performs best among these tree spans. It also shows that the Unified Parse and Semantic Tree with Feature-Paired Tree perform significantly better than the other two tree setups (i.e., CS-SPT and DSPT) by 6.7/4.2 units in F-measure respectively. This implies that the entity-related semantic information is very useful and contributes much when</context>
</contexts>
<marker>RDC, 2004</marker>
<rawString>the ACE RDC 2004 corpus (79.6/5.6/71.9) of Shortest-enclosed Path Tree from P/R/F (81.1/6.7/73.2) of Dynamic Context-Sensitive Shortestenclosed Path Tree according to Table 2 (Zhou et al., 2007) This research is supported by Project 60673041 under the National Natural Science Foundation of China, Project 2006AA01Z147 under the 863 National High-Tech Research and Development of China, and the National Research Foundation for the Doctoral Program of Higher Education of China under Grant No. 20060285008. We would also like to thank the excellent and insightful comments from the three anonymous reviewers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<date>2005</date>
<contexts>
<context position="3174" citStr="Bunescu and Mooney, 2005" startWordPosition="435" endWordPosition="438">ciency by transforming a relation example into a set of syntactic and semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information. However, detailed research (Zhou et al., 2005) shows that its difficult to extract new effective features to further improve the extraction accuracy. Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances. While kernel methods using the dependency tree (Culotta and Sorensen, 2004) and the shortest dependency path (Bunescu and Mooney, 2005) suffer from low recall performance, convolution tree kernels (Zhang et al., 2006; Zhou et al., 2007) over syntactic parse trees achieve comparable or even better </context>
<context position="6442" citStr="Bunescu and Mooney (2005)" startWordPosition="951" endWordPosition="954">nodes. The kernel matches the nodes of two corresponding sub-trees from roots to leaf nodes recursively layer by layer in a topdown manner. Their method shows successful results on two simple extraction tasks. Culotta and Sorensen (2004) proposed a slightly generalized version of this kernel between dependency trees, in which a successful match of two relation instances requires the nodes to be at the same layer and in the identical path starting from the roots to the current nodes. These strong constraints make their kernel yield high precision but very low recall on the ACE RDC 2003 corpus. Bunescu and Mooney (2005) develop a shortest path dependency tree kernel, which simply counts the number of common word classes at each node in the shortest paths between two entities in dependency trees. Similar to Culotta and Sorensen (2004), this method also suffers from high precision but low recall. Zhang et al. (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Pathenclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus. One problem with SPT is that </context>
<context position="11503" citStr="Bunescu and Mooney, 2005" startWordPosition="1744" endWordPosition="1747">ssary evidence from the unnecessary information in the structured syntactic parse tree. On one hand, lexical or word-word dependency indicates the relationship among words occurring in the same sentence, e.g. predicateargument dependency means that arguments are dependent on their target predicates, modifierhead dependency means that modifiers are dependent on their head words. This dependency relationship offers a very condensed representation of the information needed to assess the relationship in the forms of the dependency tree (Culotta and Sorensen, 2004) or the shortest dependency path (Bunescu and Mooney, 2005) that includes both entities. On the other hand, when the parse tree corresponding to the sentence is derived using derivation rules from the bottom to the top, the wordword dependencies extend upward, making a unique head child containing the head word for every non-terminal constituent. As indicated as follows, each CFG rule has the form: P Ln...L1 H R1...Rm Here, P is the parent node, H is the head child of the rule, Ln...L1 and R1...Rm are left and right modifiers of H respectively, and both n and m may be zero. In other words, the parent node P depends on the head child H, this is what we</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Bunescu, Razvan C. and Raymond J. Mooney. 2005.</rawString>
</citation>
<citation valid="false">
<title>A Shortest Path Dependency Kernel for Relation Extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (EMNLP-2005),</booktitle>
<pages>724--731</pages>
<location>Vancover, B.C. Charniak, Eugene.</location>
<marker>2001</marker>
<rawString>A Shortest Path Dependency Kernel for Relation Extraction. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (EMNLP-2005), pages 724-731. Vancover, B.C. Charniak, Eugene. 2001. Intermediate-head Parsing for Language Models. In Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics (ACL-2001), pages 116-123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistics Models for Natural Language Parsing.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<pages>589--617</pages>
<contexts>
<context position="13232" citStr="Collins (2003)" startWordPosition="2041" endWordPosition="2042"> ancestor of the two entities under consideration) as the representation of each relation instance, along the path connecting two entities, the head child of every node is found according to various constituent dependencies. Then the path nodes and their head children are kept while any other nodes are removed from the tree. Eventually we arrive at a tree called Dynamic Syntactic Parse Tree (DSPT), which is dynamically determined by constituent dependencies and only contains necessary information as expected. There exist a considerable number of constituent dependencies in CFG as described by Collins (2003). However, since our task is to extract the relationship between two named entities, our focus is on how to condense Noun-Phrases (NPs) and other useful constituents for relation extraction. Therefore constituent dependencies can be classified according to constituent types of the CFG rules: 699 \x0c(1) Modification within base-NPs: base-NPs mean that they do not directly dominate an NP themselves, unless the dominated NP is a possessive NP. The noun phrase right above the entity headword, whose mention type is nominal or name, can be categorized into this type. In this case, the entity headwo</context>
<context position="18063" citStr="Collins, 2003" startWordPosition="2918" endWordPosition="2919">path, this VP should be recovered to indicate the predicate verb. Figure 1(d) shows a sentence ... maintain rental property he owns in the state, where the ART.User-or-Owner relation holds between the entities property and he. While PP can be removed from the rule (VP VBZ PP), the VP should be kept in the rule (S NP VP). Consequently, the tree span looks more concise and precise for relation extraction. (4) Coordination conjunctions: In coordination constructions, several peer conjuncts may be reduced into a single constituent. Although the first conjunct is always considered as the headword (Collins, 2003), actually all the conjuncts play an equal role in relation extraction. As illustrated in Figure 1(e), the NP coordination in the sentence (governors from connecticut, south dakota, and montana) can be reduced to a single NP (governors from montana) by keeping the conjunct in the path while removing the other conjuncts. (5) Modification to other constituents: except for the above four types, other CFG rules fall into this type, such as modification to PP, ADVP and PRN etc. These cases are similar to arguments/adjuncts to verbs, but less frequent than them, so we will not detail this scenario. </context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Collins, Michael. 2003. Head-Driven Statistics Models for Natural Language Parsing. Computational linguistics, 29(4): 589-617.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>In Proceedings of Neural Information Processing Systems (NIPS2001),</booktitle>
<pages>625--632</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="6808" citStr="Collins and Duffy, 2001" startWordPosition="1011" endWordPosition="1014">ances requires the nodes to be at the same layer and in the identical path starting from the roots to the current nodes. These strong constraints make their kernel yield high precision but very low recall on the ACE RDC 2003 corpus. Bunescu and Mooney (2005) develop a shortest path dependency tree kernel, which simply counts the number of common word classes at each node in the shortest paths between two entities in dependency trees. Similar to Culotta and Sorensen (2004), this method also suffers from high precision but low recall. Zhang et al. (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Pathenclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus. One problem with SPT is that it loses the contextual information outside SPT, which is usually critical for relation extraction. Zhou et al. (2007) point out that both SPT and the convolution tree kernel are context-free. They expand SPT to CS-SPT by dynamically including necessary predicate-linked path information and extending the standard CTK to contextsensitive CTK, obtaining the F-measur</context>
<context position="22649" citStr="Collins and Duffy, 2001" startWordPosition="3690" endWordPosition="3693">ce a Unified Parse and Semantic Tree (UPST) to investigate the contribution of the EST to relation extraction. The entity features can be attached under the top node, the entity nodes, or directly combined with the entity nodes as in Figure 1. However, detailed evaluation (Qian et al., 2007) indicates that the UPST achieves the best performance when the feature nodes are attached under the top node. Hence, we also attach three kinds of entity-related semantic trees (i.e. BOF, FPT and EPT) under the top node of the DSPT right after its original children. Thereafter, we employ the standard CTK (Collins and Duffy, 2001) to compute the similarity between two UPSTs, since this CTK and its variations are successfully applied in syntactic parsing, semantic role labeling (Moschitti, 2004) and relation extraction (Zhang et al., 2006; Zhou et al., 2007) as well. 5 Experimentation This section will evaluate the effectiveness of the DSPT and the contribution of entity-related semantic information through experiments. 5.1 Experimental Setting For evaluation, we use the ACE RDC 2004 corpus as the benchmark data. This data set contains 451 documents and 5702 relation instances. It defines 7 entity types, 7 major relatio</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Collins, Michael and Nigel Duffy. 2001. Convolution Kernels for Natural Language. In Proceedings of Neural Information Processing Systems (NIPS2001), pages 625-632. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association of Computational Linguistics (ACL-2004),</booktitle>
<pages>423--439</pages>
<location>Barcelona,</location>
<contexts>
<context position="3147" citStr="Culotta and Sorensen, 2004" startWordPosition="431" endWordPosition="434">ormance and competitive efficiency by transforming a relation example into a set of syntactic and semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information. However, detailed research (Zhou et al., 2005) shows that its difficult to extract new effective features to further improve the extraction accuracy. Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances. While kernel methods using the dependency tree (Culotta and Sorensen, 2004) and the shortest dependency path (Bunescu and Mooney, 2005) suffer from low recall performance, convolution tree kernels (Zhang et al., 2006; Zhou et al., 2007) over syntactic parse trees achieve</context>
<context position="6054" citStr="Culotta and Sorensen (2004)" startWordPosition="884" endWordPosition="887"> to space limitation, here we only review kernel-based methods used in relation extraction. For those interested in feature-based methods, please refer to Zhou et al. (2005) for more details. Zelenko et al. (2003) described a kernel between shallow parse trees to extract semantic relations, where a relation instance is transformed into the least common sub-tree connecting the two entity nodes. The kernel matches the nodes of two corresponding sub-trees from roots to leaf nodes recursively layer by layer in a topdown manner. Their method shows successful results on two simple extraction tasks. Culotta and Sorensen (2004) proposed a slightly generalized version of this kernel between dependency trees, in which a successful match of two relation instances requires the nodes to be at the same layer and in the identical path starting from the roots to the current nodes. These strong constraints make their kernel yield high precision but very low recall on the ACE RDC 2003 corpus. Bunescu and Mooney (2005) develop a shortest path dependency tree kernel, which simply counts the number of common word classes at each node in the shortest paths between two entities in dependency trees. Similar to Culotta and Sorensen </context>
<context position="11444" citStr="Culotta and Sorensen, 2004" startWordPosition="1733" endWordPosition="1737">on is to exploit dependency knowledge to distinguish the necessary evidence from the unnecessary information in the structured syntactic parse tree. On one hand, lexical or word-word dependency indicates the relationship among words occurring in the same sentence, e.g. predicateargument dependency means that arguments are dependent on their target predicates, modifierhead dependency means that modifiers are dependent on their head words. This dependency relationship offers a very condensed representation of the information needed to assess the relationship in the forms of the dependency tree (Culotta and Sorensen, 2004) or the shortest dependency path (Bunescu and Mooney, 2005) that includes both entities. On the other hand, when the parse tree corresponding to the sentence is derived using derivation rules from the bottom to the top, the wordword dependencies extend upward, making a unique head child containing the head word for every non-terminal constituent. As indicated as follows, each CFG rule has the form: P Ln...L1 H R1...Rm Here, P is the parent node, H is the head child of the rule, Ln...L1 and R1...Rm are left and right modifiers of H respectively, and both n and m may be zero. In other words, the</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Culotta, Aron and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of the 42nd Annual Meeting of the Association of Computational Linguistics (ACL-2004), pages 423-439. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machine: learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the 10th European Conference on Machine Learning (ECML-1998),</booktitle>
<pages>137--142</pages>
<location>Chemnitz, Germany.</location>
<contexts>
<context position="23706" citStr="Joachims, 1998" startWordPosition="3852" endWordPosition="3853">se the ACE RDC 2004 corpus as the benchmark data. This data set contains 451 documents and 5702 relation instances. It defines 7 entity types, 7 major relation types and 23 subtypes. For comparison with previous work, evaluation is done on 347 (nwire/bnews) documents and 4307 relation instances using 5-fold cross-validation. Here, the corpus is parsed using Charniaks parser (Charniak, 2001) and relation instances are generated by iterating over all pairs of entity mentions occurring in the same sentence with given true mentions and coreferential information. In our experimentations, SVMlight (Joachims, 1998) with the tree kernel function (Moschitti, 2004) 2 is selected as our classifier. For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others. For comparison purposes, the training parameters C (SVM) and (tree kernel) are also set to 2.4 and 0.4 respectively. 5.2 Experimental Results Table 1 evaluates the contributions of different kinds of constituent dependencies to extraction performance on the 7 relation types of the ACE RDC 2004 corpus using the convolution parse tree kernel as depicted in Figure 1. The MCT with only entity-</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Joachims, Thorsten. 1998. Text Categorization with Support Vector Machine: learning with many relevant features. In Proceedings of the 10th European Conference on Machine Learning (ECML-1998), pages 137-142. Chemnitz, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A Study on Convolution Kernels for Shallow Semantic Parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association of Computational Linguistics (ACL-2004).</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="22816" citStr="Moschitti, 2004" startWordPosition="3717" endWordPosition="3718">ty nodes, or directly combined with the entity nodes as in Figure 1. However, detailed evaluation (Qian et al., 2007) indicates that the UPST achieves the best performance when the feature nodes are attached under the top node. Hence, we also attach three kinds of entity-related semantic trees (i.e. BOF, FPT and EPT) under the top node of the DSPT right after its original children. Thereafter, we employ the standard CTK (Collins and Duffy, 2001) to compute the similarity between two UPSTs, since this CTK and its variations are successfully applied in syntactic parsing, semantic role labeling (Moschitti, 2004) and relation extraction (Zhang et al., 2006; Zhou et al., 2007) as well. 5 Experimentation This section will evaluate the effectiveness of the DSPT and the contribution of entity-related semantic information through experiments. 5.1 Experimental Setting For evaluation, we use the ACE RDC 2004 corpus as the benchmark data. This data set contains 451 documents and 5702 relation instances. It defines 7 entity types, 7 major relation types and 23 subtypes. For comparison with previous work, evaluation is done on 347 (nwire/bnews) documents and 4307 relation instances using 5-fold cross-validation</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Moschitti, Alessandro. 2004. A Study on Convolution Kernels for Shallow Semantic Parsing. In Proceedings of the 42nd Annual Meeting of the Association of Computational Linguistics (ACL-2004). Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longhua Qian</author>
<author>Guodong Zhou</author>
<author>Qiaoming Zhu</author>
<author>Peide Qian</author>
</authors>
<title>Relation Extraction using Convolution Tree Kernel Expanded with Entity Features.</title>
<date>2007</date>
<booktitle>In Proceedings of the 21st Pacific Asian Conference on Language, Information and Computation (PACLIC-21),</booktitle>
<pages>415--421</pages>
<publisher>Seoul,</publisher>
<contexts>
<context position="19423" citStr="Qian et al. (2007)" startWordPosition="3141" endWordPosition="3144">stituents outside the linking path should be removed) and CS-CSPT (Zhou et al., 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. 4 Entity-related Semantic Tree Entity semantic features, such as entity headword, entity type and subtype etc., impose a strong constraint on relation types in terms of relation definition by the ACE RDC task. Experiments by Zhang et al. (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. Qian et al. (2007) further indicates that among these entity features, entity type, subtype, and mention type, as well as the base form of predicate verb, contribute most while the contribution of other features, such as entity class, headword and GPE role, can be ignored. In order to effectively capture entity-related semantic features, and their combined features as well, especially bi-gram or tri-gram features, we build an Entity-related Semantic Tree (EST) in three ways as illustrated in Figure 2. In the example sentence they re here, which is excerpted from the ACE RDC 2004 corpus, there exists a relations</context>
<context position="22317" citStr="Qian et al., 2007" startWordPosition="3634" endWordPosition="3637"> entity features only relating to one of the entities between two relation instances. In fact, the BOF only captures the individual entity features, while the FPT/EPT can additionally capture the bi-gram/tri-gram features respectively. Rather than constructing a composite kernel, we incorporate the EST into the DSPT to produce a Unified Parse and Semantic Tree (UPST) to investigate the contribution of the EST to relation extraction. The entity features can be attached under the top node, the entity nodes, or directly combined with the entity nodes as in Figure 1. However, detailed evaluation (Qian et al., 2007) indicates that the UPST achieves the best performance when the feature nodes are attached under the top node. Hence, we also attach three kinds of entity-related semantic trees (i.e. BOF, FPT and EPT) under the top node of the DSPT right after its original children. Thereafter, we employ the standard CTK (Collins and Duffy, 2001) to compute the similarity between two UPSTs, since this CTK and its variations are successfully applied in syntactic parsing, semantic role labeling (Moschitti, 2004) and relation extraction (Zhang et al., 2006; Zhou et al., 2007) as well. 5 Experimentation This sect</context>
</contexts>
<marker>Qian, Zhou, Zhu, Qian, 2007</marker>
<rawString>Qian, Longhua, Guodong Zhou, Qiaoming Zhu and Peide Qian. 2007. Relation Extraction using Convolution Tree Kernel Expanded with Entity Features. In Proceedings of the 21st Pacific Asian Conference on Language, Information and Computation (PACLIC-21), pages 415-421. Seoul, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
</authors>
<title>Chinatsu Aone and Anthony Richardella.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<issue>2003</issue>
<pages>1083--1106</pages>
<marker>Zelenko, 2003</marker>
<rawString>Zelenko, Dmitry, Chinatsu Aone and Anthony Richardella. 2003. Kernel Methods for Relation Extraction. Journal of Machine Learning Research, 3(2003): 1083-1106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
</authors>
<title>A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association of Computational Linguistics (COLING/ACL-2006),</booktitle>
<pages>825--832</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3214" citStr="Zhang et al., 2006" startWordPosition="442" endWordPosition="445">a set of syntactic and semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information. However, detailed research (Zhou et al., 2005) shows that its difficult to extract new effective features to further improve the extraction accuracy. Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances. While kernel methods using the dependency tree (Culotta and Sorensen, 2004) and the shortest dependency path (Bunescu and Mooney, 2005) suffer from low recall performance, convolution tree kernels (Zhang et al., 2006; Zhou et al., 2007) over syntactic parse trees achieve comparable or even better performance than feature-based methods. </context>
<context position="6742" citStr="Zhang et al. (2006)" startWordPosition="1001" endWordPosition="1004">dency trees, in which a successful match of two relation instances requires the nodes to be at the same layer and in the identical path starting from the roots to the current nodes. These strong constraints make their kernel yield high precision but very low recall on the ACE RDC 2003 corpus. Bunescu and Mooney (2005) develop a shortest path dependency tree kernel, which simply counts the number of common word classes at each node in the shortest paths between two entities in dependency trees. Similar to Culotta and Sorensen (2004), this method also suffers from high precision but low recall. Zhang et al. (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Pathenclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus. One problem with SPT is that it loses the contextual information outside SPT, which is usually critical for relation extraction. Zhou et al. (2007) point out that both SPT and the convolution tree kernel are context-free. They expand SPT to CS-SPT by dynamically including necessary predicate-linked path information and extendin</context>
<context position="9136" citStr="Zhang et al. (2006)" startWordPosition="1385" endWordPosition="1388">e derivation. We also attempt to fully capture both the structured syntactic parse information and entity-related semantic information, especially combined entity features, via a unified parse and semantic tree. Finally, we validate the effectiveness of a composite kernel for relation extraction, which combines a tree kernel and a linear kernel. 698 \x0c3 Dynamic Syntactic Parse Tree This section discusses how to generate dynamic syntactic parse tree by employing constituent dependencies to overcome the problems existing in currently used tree spans. 3.1 Constituent Dependencies in Parse Tree Zhang et al. (2006) explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT) achieves the best performance. Zhou et al. (2007) further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT. However, the key problem of how to represent the structured syntactic parse tree is still partially resolved. As we indicate as follows, current tree spans suffer from two problems: (1) Both SPT and CS-SPT still contain unnecessary information. For example, in the sentence ...bought one of towns</context>
<context position="18696" citStr="Zhang et al., 2006" startWordPosition="3023" endWordPosition="3026"> the conjuncts play an equal role in relation extraction. As illustrated in Figure 1(e), the NP coordination in the sentence (governors from connecticut, south dakota, and montana) can be reduced to a single NP (governors from montana) by keeping the conjunct in the path while removing the other conjuncts. (5) Modification to other constituents: except for the above four types, other CFG rules fall into this type, such as modification to PP, ADVP and PRN etc. These cases are similar to arguments/adjuncts to verbs, but less frequent than them, so we will not detail this scenario. In fact, SPT (Zhang et al., 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al., 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. 4 Entity-related Semantic Tree Entity semantic features, such as entity headword, entity type and subtype etc., impose a strong constraint on relation types in terms of relation definition by the ACE RDC task. Experiments by Zhang et al. (2006) show that lin</context>
<context position="21166" citStr="Zhang et al. (2006)" startWordPosition="3442" endWordPosition="3445">P2 TP1 (a) Bag Of Features(BOF) ENT ST2 ST1 MT2 MT1 PVB (c) Entity-Paired Tree(EPT) ENT E1 E2 (b) Feature Paired Tree(FPT) ENT TP ST MT ST1 TP1 MT1 TP2 ST2 MT2 PVB TP1 TP2 ST1 ST2 MT1 MT2 PVB PER null PRO GPE Pop. PRO be PER null PRO GPE Pop. PRO be PER GPE null Pop. PRO PRO be Figure 2. Different setups for entity-related semantic tree (EST) (a) Bag of Features (BOF, e.g. Fig. 2(a)): all feature nodes uniformly hang under the root node, so the tree kernel simply counts the number of common features between two relation instances. This tree setup is similar to linear entity kernel explored by Zhang et al. (2006). (b) Feature-Paired Tree (FPT, e.g. Fig. 2(b)): the features of two entities are grouped into different types according to their feature names, e.g. TP1 and TP2 are grouped to TP. This tree setup is aimed to capture the additional similarity 701 \x0cof the single feature combined from different entities, i.e., the first and the second entities. (c) Entity-Paired Tree (EPT, e.g. Fig. 2(c)): all the features relating to an entity are grouped to nodes E1 or E2, thus this tree kernel can further explore the equivalence of combined entity features only relating to one of the entities between two r</context>
<context position="22860" citStr="Zhang et al., 2006" startWordPosition="3722" endWordPosition="3725">tity nodes as in Figure 1. However, detailed evaluation (Qian et al., 2007) indicates that the UPST achieves the best performance when the feature nodes are attached under the top node. Hence, we also attach three kinds of entity-related semantic trees (i.e. BOF, FPT and EPT) under the top node of the DSPT right after its original children. Thereafter, we employ the standard CTK (Collins and Duffy, 2001) to compute the similarity between two UPSTs, since this CTK and its variations are successfully applied in syntactic parsing, semantic role labeling (Moschitti, 2004) and relation extraction (Zhang et al., 2006; Zhou et al., 2007) as well. 5 Experimentation This section will evaluate the effectiveness of the DSPT and the contribution of entity-related semantic information through experiments. 5.1 Experimental Setting For evaluation, we use the ACE RDC 2004 corpus as the benchmark data. This data set contains 451 documents and 5702 relation instances. It defines 7 entity types, 7 major relation types and 23 subtypes. For comparison with previous work, evaluation is done on 347 (nwire/bnews) documents and 4307 relation instances using 5-fold cross-validation. Here, the corpus is parsed using Charniaks</context>
<context position="28842" citStr="Zhang et al., 2006" startWordPosition="4667" endWordPosition="4670">semantic information is very useful and contributes much when they are incorporated into the parse tree for relation extraction. Tree setups P(%) R(%) F CS-SPT over SPT3 1.5 1.1 1.3 DSPT over SPT 1.1 5.6 3.8 UPST (FPT) over SPT 3.8 10.9 8.0 Table 3. Improvements of different tree setups over SPT on the ACE RDC 2004 corpus Finally, Table 4 compares our system with other state-of-the-art kernel-based systems on the 7 relation types of the ACE RDC 2004 corpus. It shows that our UPST outperforms all previous tree setups using one single kernel, and even better than two previous composite kernels (Zhang et al., 2006; Zhao and Grishman, 2005). Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state featurebased kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al. (2007) (i.e. polynomial degree d=2 and coefficient I=0.3), we get the so far best performance of 77.1 in F-measure for 7 relation types on the ACE RDC 2004 data set. Systems P(%) R(%) F Ours: composite kernel 83.0 72.0 77.1 Zhou et al., (2007): composite kernel 82.2 70.2 75.8 Zhang et al., (2006): composite kernel 76.1 68.4 72.1 Zhao and Grishman, (2005):4 co</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Zhang, Min, Jie Zhang, Jian Su and Guodong Zhou. 2006. A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association of Computational Linguistics (COLING/ACL-2006), pages 825-832. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubin Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association of Computational Linguistics (ACL-2005),</booktitle>
<pages>419--426</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="7747" citStr="Zhao and Grishman (2005)" startWordPosition="1161" endWordPosition="1164">lation extraction. Zhou et al. (2007) point out that both SPT and the convolution tree kernel are context-free. They expand SPT to CS-SPT by dynamically including necessary predicate-linked path information and extending the standard CTK to contextsensitive CTK, obtaining the F-measure of 73.2 on the 7 relation types of the ACE RDC 2004 corpus. However, the CS-SPT only recovers part of contextual information and may contain noisy information as much as SPT. In order to fully utilize the advantages of feature-based methods and kernel-based methods, researchers turn to composite kernel methods. Zhao and Grishman (2005) define several feature-based composite kernels to capture diverse linguistic knowledge and achieve the F-measure of 70.4 on the 7 relation types in the ACE RDC 2004 corpus. Zhang et al. (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. Zhou et al. (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. It achieves the so far best F-measure of 75.8 on the 7 relation types in the ACE RDC 2004 corpus. In this paper, we </context>
<context position="28868" citStr="Zhao and Grishman, 2005" startWordPosition="4671" endWordPosition="4674"> is very useful and contributes much when they are incorporated into the parse tree for relation extraction. Tree setups P(%) R(%) F CS-SPT over SPT3 1.5 1.1 1.3 DSPT over SPT 1.1 5.6 3.8 UPST (FPT) over SPT 3.8 10.9 8.0 Table 3. Improvements of different tree setups over SPT on the ACE RDC 2004 corpus Finally, Table 4 compares our system with other state-of-the-art kernel-based systems on the 7 relation types of the ACE RDC 2004 corpus. It shows that our UPST outperforms all previous tree setups using one single kernel, and even better than two previous composite kernels (Zhang et al., 2006; Zhao and Grishman, 2005). Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state featurebased kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al. (2007) (i.e. polynomial degree d=2 and coefficient I=0.3), we get the so far best performance of 77.1 in F-measure for 7 relation types on the ACE RDC 2004 data set. Systems P(%) R(%) F Ours: composite kernel 83.0 72.0 77.1 Zhou et al., (2007): composite kernel 82.2 70.2 75.8 Zhang et al., (2006): composite kernel 76.1 68.4 72.1 Zhao and Grishman, (2005):4 composite kernel 69.2 70.5 7</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Zhao, Shubin and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In Proceedings of the 43rd Annual Meeting of the Association of Computational Linguistics (ACL-2005), pages 419-426. Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Jie Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association of Computational Linguistics (ACL-2005),</booktitle>
<pages>427--434</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="2792" citStr="Zhou et al., 2005" startWordPosition="379" endWordPosition="382"> accuracy in state-of-the-art syntactic and semantic parsing, reliably extracting semantic relationships between named entities in natural language documents is still a difficult, unresolved problem. In the literature, feature-based methods have dominated the research in semantic relation extraction. Featured-based methods achieve promising performance and competitive efficiency by transforming a relation example into a set of syntactic and semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information. However, detailed research (Zhou et al., 2005) shows that its difficult to extract new effective features to further improve the extraction accuracy. Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. The key problem for kernel methods on relation extraction is how to represe</context>
<context position="5600" citStr="Zhou et al. (2005)" startWordPosition="810" endWordPosition="813">interesting combined entity features for relation extraction via a unified parse and semantic tree. The other sections in this paper are organized as follows. Previous work is first reviewed in Section 2. Then, Section 3 proposes a dynamic syntactic parse tree while the entity-related semantic tree is described in Section 4. Evaluation on the ACE RDC corpus is given in Section 5. Finally, we conclude our work in Section 6. 2 Related Work Due to space limitation, here we only review kernel-based methods used in relation extraction. For those interested in feature-based methods, please refer to Zhou et al. (2005) for more details. Zelenko et al. (2003) described a kernel between shallow parse trees to extract semantic relations, where a relation instance is transformed into the least common sub-tree connecting the two entity nodes. The kernel matches the nodes of two corresponding sub-trees from roots to leaf nodes recursively layer by layer in a topdown manner. Their method shows successful results on two simple extraction tasks. Culotta and Sorensen (2004) proposed a slightly generalized version of this kernel between dependency trees, in which a successful match of two relation instances requires t</context>
<context position="28994" citStr="Zhou et al., 2005" startWordPosition="4692" endWordPosition="4695"> CS-SPT over SPT3 1.5 1.1 1.3 DSPT over SPT 1.1 5.6 3.8 UPST (FPT) over SPT 3.8 10.9 8.0 Table 3. Improvements of different tree setups over SPT on the ACE RDC 2004 corpus Finally, Table 4 compares our system with other state-of-the-art kernel-based systems on the 7 relation types of the ACE RDC 2004 corpus. It shows that our UPST outperforms all previous tree setups using one single kernel, and even better than two previous composite kernels (Zhang et al., 2006; Zhao and Grishman, 2005). Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state featurebased kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al. (2007) (i.e. polynomial degree d=2 and coefficient I=0.3), we get the so far best performance of 77.1 in F-measure for 7 relation types on the ACE RDC 2004 data set. Systems P(%) R(%) F Ours: composite kernel 83.0 72.0 77.1 Zhou et al., (2007): composite kernel 82.2 70.2 75.8 Zhang et al., (2006): composite kernel 76.1 68.4 72.1 Zhao and Grishman, (2005):4 composite kernel 69.2 70.5 70.4 Ours: CTK with UPST 80.1 70.7 75.1 Zhou et al., (2007): contextsensitive CTK with CS-SPT 81.1 66.7 73.2 Zhang et al., (200</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>Zhou, Guodong, Jian Su, Jie Zhang and Min Zhang. 2005. Exploring various knowledge in relation extraction. In Proceedings of the 43rd Annual Meeting of the Association of Computational Linguistics (ACL-2005), pages 427-434. Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Min Zhang</author>
<author>Donghong Ji</author>
<author>Qiaoming Zhu</author>
</authors>
<title>Tree Kernel-based Relation Extraction with Context-Sensitive Structured Parse Tree Information.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL-2007),</booktitle>
<pages>728--736</pages>
<location>Prague, Czech.</location>
<contexts>
<context position="3234" citStr="Zhou et al., 2007" startWordPosition="446" endWordPosition="449">nd semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information. However, detailed research (Zhou et al., 2005) shows that its difficult to extract new effective features to further improve the extraction accuracy. Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances. While kernel methods using the dependency tree (Culotta and Sorensen, 2004) and the shortest dependency path (Bunescu and Mooney, 2005) suffer from low recall performance, convolution tree kernels (Zhang et al., 2006; Zhou et al., 2007) over syntactic parse trees achieve comparable or even better performance than feature-based methods. However, there still</context>
<context position="7160" citStr="Zhou et al. (2007)" startWordPosition="1069" endWordPosition="1072">es at each node in the shortest paths between two entities in dependency trees. Similar to Culotta and Sorensen (2004), this method also suffers from high precision but low recall. Zhang et al. (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Pathenclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus. One problem with SPT is that it loses the contextual information outside SPT, which is usually critical for relation extraction. Zhou et al. (2007) point out that both SPT and the convolution tree kernel are context-free. They expand SPT to CS-SPT by dynamically including necessary predicate-linked path information and extending the standard CTK to contextsensitive CTK, obtaining the F-measure of 73.2 on the 7 relation types of the ACE RDC 2004 corpus. However, the CS-SPT only recovers part of contextual information and may contain noisy information as much as SPT. In order to fully utilize the advantages of feature-based methods and kernel-based methods, researchers turn to composite kernel methods. Zhao and Grishman (2005) define sever</context>
<context position="9271" citStr="Zhou et al. (2007)" startWordPosition="1407" endWordPosition="1410">especially combined entity features, via a unified parse and semantic tree. Finally, we validate the effectiveness of a composite kernel for relation extraction, which combines a tree kernel and a linear kernel. 698 \x0c3 Dynamic Syntactic Parse Tree This section discusses how to generate dynamic syntactic parse tree by employing constituent dependencies to overcome the problems existing in currently used tree spans. 3.1 Constituent Dependencies in Parse Tree Zhang et al. (2006) explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT) achieves the best performance. Zhou et al. (2007) further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT. However, the key problem of how to represent the structured syntactic parse tree is still partially resolved. As we indicate as follows, current tree spans suffer from two problems: (1) Both SPT and CS-SPT still contain unnecessary information. For example, in the sentence ...bought one of towns two meat-packing plants, the condensed information one of plants is sufficient to determine DISC relationship between the entities one</context>
<context position="18890" citStr="Zhou et al., 2007" startWordPosition="3059" endWordPosition="3062"> to a single NP (governors from montana) by keeping the conjunct in the path while removing the other conjuncts. (5) Modification to other constituents: except for the above four types, other CFG rules fall into this type, such as modification to PP, ADVP and PRN etc. These cases are similar to arguments/adjuncts to verbs, but less frequent than them, so we will not detail this scenario. In fact, SPT (Zhang et al., 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al., 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. 4 Entity-related Semantic Tree Entity semantic features, such as entity headword, entity type and subtype etc., impose a strong constraint on relation types in terms of relation definition by the ACE RDC task. Experiments by Zhang et al. (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. Qian et al. (2007) further indicates that among these entity features, entity type, s</context>
<context position="22880" citStr="Zhou et al., 2007" startWordPosition="3726" endWordPosition="3729">ure 1. However, detailed evaluation (Qian et al., 2007) indicates that the UPST achieves the best performance when the feature nodes are attached under the top node. Hence, we also attach three kinds of entity-related semantic trees (i.e. BOF, FPT and EPT) under the top node of the DSPT right after its original children. Thereafter, we employ the standard CTK (Collins and Duffy, 2001) to compute the similarity between two UPSTs, since this CTK and its variations are successfully applied in syntactic parsing, semantic role labeling (Moschitti, 2004) and relation extraction (Zhang et al., 2006; Zhou et al., 2007) as well. 5 Experimentation This section will evaluate the effectiveness of the DSPT and the contribution of entity-related semantic information through experiments. 5.1 Experimental Setting For evaluation, we use the ACE RDC 2004 corpus as the benchmark data. This data set contains 451 documents and 5702 relation instances. It defines 7 entity types, 7 major relation types and 23 subtypes. For comparison with previous work, evaluation is done on 347 (nwire/bnews) documents and 4307 relation instances using 5-fold cross-validation. Here, the corpus is parsed using Charniaks parser (Charniak, 2</context>
</contexts>
<marker>Zhou, Zhang, Ji, Zhu, 2007</marker>
<rawString>Zhou, Guodong, Min Zhang, Donghong Ji and Qiaoming Zhu. 2007. Tree Kernel-based Relation Extraction with Context-Sensitive Structured Parse Tree Information. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL-2007), pages 728-736. Prague, Czech.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>