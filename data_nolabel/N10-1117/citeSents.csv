Our approach is also related to edge deletion in Bayesian networks CITATION,,
Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better nonprojective solution CITATION,,
CITATION compute marginal probabilities by using a cutting plane approach that starts with the local polytope and then optimizes some approximation of the log partition function,,
First, how fast is our relaxation approach compared to full marginal inference at comparable dependency accuracy? This requires us to find the best tree in terms of marginal probabilities on the link variables CITATION,,
When looking at the related task of finding the most likely assignment in large graphical models (i.e., MAP inference), we notice that several recent approaches have significantly sped up computation through relaxation methods (CITATION; CITATION),,
4.4 Related Work Our approach is inspired by earlier work on relaxation algorithms for performing MAP inference by incrementally tightening relaxations of a graphical model (Anguelov et al., 2004; CITATION), weighted Finite State Machine CITATION, Integer Linear Program CITATION or Marginal Polytope CITATION,,
We chose the grandparents and siblings model, together with language specific multiroot and projectivity options as taken from CITATION,,
One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling CITATION, another is (loopy) sum-product belief propagation CITATION,,
As in CITATION, we used algorithms from edge-factored parsing to compute BP messages for these factors,,
That is, for each factor i F i def = X i(y)=1 pF (y) = EF [i] (2) For compactness, we follow the convention of CITATION and represent the belief for a variable using the marginal probability of its corresponding unary factor,,
Second, how good is the final relaxed graph as an approximation of the full graph? Finally, how does incremental relaxation scale with sentence length? 5.1 Data and Models We trained and tested on a subset of languages from the CoNLL Dependency Parsing Shared Tasks CITATION: Dutch, Danish, Italian, and English,,
Appendix: Proof Sketches For Proposition 1 we use the primal form of the KL divergence CITATION D ` p0 F ||pF = log ` ZF Z1 F0 hF 0 , F F0 i and represent the ratio ZF Z1 F0 of partition functions as ZF ZF0 = X y ehF0 ,F0 (y)i ZF0 ehG,G(y)i = EF0 h ehG,Gi i where G def = F \\ F0 ,,
For example, since we cannot derive a dynamic program for marginal inference in second order non-projective dependency parsing CITATION, we have non-projective languages such as Dutch using second order projective models if we want to apply DP,,
2.1 Markov Random Fields Following CITATION, we define a probability distribution over all dependency trees as a collection of edges y for a fixed input sentence W,,
Together with a threshold \x0f on this gain we can now adapt the relaxation approach to marginal inference by simply replacing the question, Is i violated? with the question, Is gF0 (i) &gt; \x0f? We can see the latter question as a generalization of the former if we interpret MAP inference as the zerotemperature limit of marginal inference CITATION,,
Moreover, we assume all weights i are positivewithout loss of generality since we can always replace i with its negation 1 i and then change the sign of i CITATION,,
CITATION tackled the MAP problem for dependency parsing by an incremental approach that starts with a relaxation of the problem, solves it, and adds additional constraints only if they are violated,,
