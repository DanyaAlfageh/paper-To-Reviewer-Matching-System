Our approach is also related to edge deletion in Bayesian networks CITATION.,,
Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better nonprojective solution CITATION.,,
One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling CITATION, another is (loopy) sum-product belief propagation CITATION.,,
For example, since we cannot derive a dynamic program for marginal inference in second order non-projective dependency parsing CITATION, we have non-projective languages such as Dutch using second order projective models if we want to apply DP.,,
Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better nonprojective solution CITATION.,,
One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling CITATION, another is (loopy) sum-product belief propagation CITATION.,,
For example, since we cannot derive a dynamic program for marginal inference in second order non-projective dependency parsing CITATION, we have non-projective languages such as Dutch using second order projective models if we want to apply DP.,,
Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better nonprojective solution CITATION.,,
This requires us to find the best tree in terms of marginal probabilities on the link variables CITATION.,,
5.1 Data and Models We trained and tested on a subset of languages from the CoNLL Dependency Parsing Shared Tasks CITATION: Dutch, Danish, Italian, and English.,,
Moreover, we assume all weights i are positivewithout loss of generality since we can always replace i with its negation 1 i and then change the sign of i CITATION.,,
When looking at the related task of finding the most likely assignment in large graphical models (i.e., MAP inference), we notice that several recent approaches have significantly sped up computation through relaxation methods (CITATION; CITATION).,,
CITATION tackled the MAP problem for dependency parsing by an incremental approach that starts with a relaxation of the problem, solves it, and adds additional constraints only if they are violated.,,
4.4 Related Work Our approach is inspired by earlier work on relaxation algorithms for performing MAP inference by incrementally tightening relaxations of a graphical model (Anguelov et al., 2004; CITATION), weighted Finite State Machine CITATION, Integer Linear Program CITATION or Marginal Polytope CITATION.,,
CITATION compute marginal probabilities by using a cutting plane approach that starts with the local polytope and then optimizes some approximation of the log partition function.,,
4.4 Related Work Our approach is inspired by earlier work on relaxation algorithms for performing MAP inference by incrementally tightening relaxations of a graphical model (Anguelov et al., 2004; CITATION), weighted Finite State Machine CITATION, Integer Linear Program CITATION or Marginal Polytope CITATION.,,
CITATION compute marginal probabilities by using a cutting plane approach that starts with the local polytope and then optimizes some approximation of the log partition function.,,
Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better nonprojective solution CITATION.,,
One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling CITATION, another is (loopy) sum-product belief propagation CITATION.,,
We chose the grandparents and siblings model, together with language specific multiroot and projectivity options as taken from CITATION.,,
All our models are defined over a set of binary variables Lij that indicate a dependency between token i and j of the input sentence W. 2.1 Markov Random Fields Following CITATION, we define a probability distribution over all dependency trees as a collection of edges y for a fixed input sentence W. This distribution is represented by an undirected graphical model, or Markov random field (MRF): pF (y) def = 1 Z Y iF i (y) (1) specified by an index set F and a corresponding family (i)F of factors i : Y 7 &lt;+.,,
As in CITATION, we used algorithms from edge-factored parsing to compute BP messages for these factors.,,
This requires us to find the best tree in terms of marginal probabilities on the link variables CITATION.,,
5.1 Data and Models We trained and tested on a subset of languages from the CoNLL Dependency Parsing Shared Tasks CITATION: Dutch, Danish, Italian, and English.,,
4.4 Related Work Our approach is inspired by earlier work on relaxation algorithms for performing MAP inference by incrementally tightening relaxations of a graphical model (Anguelov et al., 2004; CITATION), weighted Finite State Machine CITATION, Integer Linear Program CITATION or Marginal Polytope CITATION.,,
CITATION compute marginal probabilities by using a cutting plane approach that starts with the local polytope and then optimizes some approximation of the log partition function.,,
4.4 Related Work Our approach is inspired by earlier work on relaxation algorithms for performing MAP inference by incrementally tightening relaxations of a graphical model (Anguelov et al., 2004; CITATION), weighted Finite State Machine CITATION, Integer Linear Program CITATION or Marginal Polytope CITATION.,,
CITATION compute marginal probabilities by using a cutting plane approach that starts with the local polytope and then optimizes some approximation of the log partition function.,,
When looking at the related task of finding the most likely assignment in large graphical models (i.e., MAP inference), we notice that several recent approaches have significantly sped up computation through relaxation methods (CITATION; CITATION).,,
4.4 Related Work Our approach is inspired by earlier work on relaxation algorithms for performing MAP inference by incrementally tightening relaxations of a graphical model (Anguelov et al., 2004; CITATION), weighted Finite State Machine CITATION, Integer Linear Program CITATION or Marginal Polytope CITATION.,,
CITATION compute marginal probabilities by using a cutting plane approach that starts with the local polytope and then optimizes some approximation of the log partition function.,,
That is, for each factor i F i def = X i(y)=1 pF (y) = EF [i] (2) For compactness, we follow the convention of CITATION and represent the belief for a variable using the marginal probability of its corresponding unary factor.,,
We can see the latter question as a generalization of the former if we interpret MAP inference as the zerotemperature limit of marginal inference CITATION.,,
Appendix: Proof Sketches For Proposition 1 we use the primal form of the KL divergence CITATION D ` p0 F ||pF = log ` ZF Z1 F0 hF 0 , F F0 i and represent the ratio ZF Z1 F0 of partition functions as ZF ZF0 = X y ehF0 ,F0 (y)i ZF0 ehG,G(y)i = EF0 h ehG,Gi i where G def = F \\ F0 .,,
