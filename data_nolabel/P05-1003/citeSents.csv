CITATION and CITATION show that gradient-based algorithms, particularly limited memory variable metric (LMVM), require much less time to reach convergence, for some NLP tasks, than the iterative scaling methods (Della CITATION) previously used for log-linear optimisation problems,,
In order to make training time manageable4, we collapse the number of POS tags from 48 to 5 following the procedure used in CITATION,,
1 Introduction In recent years, conditional random fields (CRFs) CITATION have shown success on a number of natural language processing (NLP) tasks, including shallow parsing CITATION, named entity recognition CITATION and information extraction from research papers CITATION,,
Usually these experts are designed to 4See CITATION for a scaling method allowing the full POS tagging task with CRFs,,
The concept of combining the distributions of a set of expert models via a weighted product has previously been used in a range of different application areas, including economics and management science CITATION, and NLP CITATION,,
a set of expert models via a weighted product has previously been used in a range of different application areas, including economics and management science CITATION, and NLP CITATION,,
For CRFs with general graphical structure, calculation of Ep(s|o)[ fk] is intractable, but for the linear chain case CITATION describe an efficient dynamic programming procedure for inference, similar in nature to the forward-backward algorithm in h,,
For CRFs with general graphical structure, calculation of Ep(s|o)[ fk] is intractable, but for the linear chain case CITATION describe an efficient dynamic programming procedure for inference, similar in nature to the forw,,
CITATION shows that the KL divergence between q(s |o) and the LOP, can be decomposed into two terms: K(q, pLOP) = E A (4) = wK (q, p) wK (pLOP, p) This tells us that the closeness of the LOP model to q(s |o) is governed by a trade-off between two terms: an E term, which re,,
For our experiments we use the CoNLL-2000 shared task dataset (Tjong Kim CITATION),,
al distributions p(s|o) and a set of non-negative normalised weights w, a logarithmic opinion pool 2 is defined as the distribution: pLOP(s|o) = 1 ZLOP(o) [p(s|o)]w (2) with w 0 and w = 1, and where ZLOP(o) is the normalisation constant: ZLOP(o) = s [p(s|o)]w (3) 2CITATION introduced a variant of the LOP idea called Product of Experts, in which expert distributions are multiplied under a uniform weight distribution,,
actable, but for the linear chain case CITATION describe an efficient dynamic programming procedure for inference, similar in nature to the forward-backward algorithm in hidden Markov models,,
n of Ep(s|o)[ fk] is intractable, but for the linear chain case CITATION describe an efficient dynamic programming procedure for inference, similar in nature to the forward-backward algorithm in hidden Markov models,,
Recently, there have been a number of sophisticated approaches to reducing overfitting in CRFs, including automatic feature induction CITATION and a full Bayesian approach to training and inference CITATION,,
CITATION shows that the KL divergence between q(s |o) and the LOP, can be decomposed into two terms: K(q, pLOP) = E A (4) = wK (q, p) wK (pLOP, p) This tells us that the closeness of the LOP model to q(s |o) is governed by a trade-off between two terms: an E term, which represents the closeness of the individual experts to q(s |o), and an A term, which represents the closeness of the individual experts to the LOP, and therefore indirectly to each other,,
shown success on a number of natural language processing (NLP) tasks, including shallow parsing CITATION, named entity recognition CITATION and information extraction from research papers CITATION,,
cluding shallow parsing CITATION, named entity recognition CITATION and information extraction from research papers CITATION,,
These are based on those found in CITATION,,
We use McNemars matched-pairs test CITATION on point-wise labelling errors to examine the statistical significance of these results,,
Given a set of sequence model experts, indexed by , with conditional distributions p(s|o) and a set of non-negative normalised weights w, a logarithmic opinion pool 2 is defined as the distribution: pLOP(s|o) = 1 ZLOP(o) [p(s|o)]w (2) with w 0 and w = 1, and where ZLOP(o) is the normalisation constant: ZLOP(o) = s [p(s|o)]w (3) 2CITATION introduced a variant of the LOP idea called Product of Experts, in which expert distributions are multiplied under a uniform weight distribution,,
For CRFs with general graphical structure, calculation of Ep(s|o)[ fk] is intractable, but for the linear chain case CITATION describe an efficient dynamic programming procedure for inference, similar in nature to the forward-backward algorithm in hidden Markov models,,
