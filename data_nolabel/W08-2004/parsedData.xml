<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.633346">
b&apos;Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 2532
</note>
<address confidence="0.729657">
Manchester, August 2008
</address>
<title confidence="0.9207135">
Encoding Tree Pair-based Graphs in Learning Algorithms:
the Textual Entailment Recognition Case
</title>
<author confidence="0.970983">
Alessandro Moschitti
</author>
<affiliation confidence="0.979983">
DISI, University of Trento
</affiliation>
<address confidence="0.50592">
Via Sommarive 14
38100 POVO (TN) - Italy
</address>
<email confidence="0.894759">
moschitti@dit.unitn.it
</email>
<author confidence="0.974734">
Fabio Massimo Zanzotto
</author>
<affiliation confidence="0.99222">
DISP, University of Rome Tor Vergata
</affiliation>
<address confidence="0.933509">
Via del Politecnico 1
00133 Roma, Italy
</address>
<email confidence="0.975037">
zanzotto@info.uniroma2.it
</email>
<sectionHeader confidence="0.98983" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998457916666667">
In this paper, we provide a statistical ma-
chine learning representation of textual en-
tailment via syntactic graphs constituted
by tree pairs. We show that the natural way
of representing the syntactic relations be-
tween text and hypothesis consists in the
huge feature space of all possible syntac-
tic tree fragment pairs, which can only be
managed using kernel methods. Experi-
ments with Support Vector Machines and
our new kernels for paired trees show the
validity of our interpretation.
</bodyText>
<sectionHeader confidence="0.997939" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98047725">
Recently, a lot of valuable work on the recogni-
tion of textual entailment (RTE) has been carried
out (Bar Haim et al., 2006). The aim is to detect
implications between sentences like:
</bodyText>
<equation confidence="0.529274666666667">
T1 H1
T1 Wanadoo bought KStones
H1 Wanadoo owns KStones
</equation>
<bodyText confidence="0.995297215686274">
where T1 and H1 stand for text and hypothesis, re-
spectively.
Several models, ranging from the simple lexi-
cal similarity between T and H to advanced Logic
Form Representations, have been proposed (Cor-
ley and Mihalcea, 2005; Glickman and Dagan,
2004; de Salvo Braz et al., 2005; Bos and Mark-
ert, 2005). However, since a linguistic theory able
to analytically show how to computationally solve
the RTE problem has not been developed yet, to
c
2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
design accurate systems, we should rely upon the
application of machine learning. In this perspec-
tive, TE training examples have to be represented
in terms of statistical feature distributions. These
typically consist in word sequences (along with
their lexical similarity) and the syntactic structures
of both text and hypothesis (e.g. their parse trees).
The interesting aspect with respect to other natural
language problems is that, in TE, features useful
at describing an example are composed by pairs of
features from Text and Hypothesis.
For example, using a word representation, a text
and hypothesis pair, hT, Hi, can be represented
by the sequences of words of the two sentences,
i.e. ht1, .., tni and hh1, .., hmi, respectively. If we
carry out a blind and complete statistical correla-
tion analysis of the two sequences, the entailment
property would be described by the set of subse-
quence pairs from T and H, i.e. the set R =
{hst, shi : st = hti1 , .., til
i, sh = hhj1 , .., hjr i, l
n, r m}. The relation set R constitutes a
naive and complete representation of the example
hT, Hi in the feature space {hv, wi : v, w V },
where V is the corpus vocabulary1.
Although the above representation is correct and
complete from a statistically point of view, it suf-
fers from two practical drawbacks: (a) it is expo-
nential in V and (b) it is subject to high degree of
data sparseness which may prevent to carry out ef-
fective learning. The traditional solution for this
problem relates to consider the syntactic structure
of word sequences which provides their general-
ization.
The use of syntactic trees poses the problem
of representing structures in learning algorithms.
</bodyText>
<equation confidence="0.3659765">
1
V
</equation>
<bodyText confidence="0.946605333333333">
is larger than the actual space, which is the one of
all possible subsequences with gaps, i.e. it only contains all
possible concatenations of words respecting their order.
</bodyText>
<page confidence="0.992567">
25
</page>
<bodyText confidence="0.99856348">
\x0cFor this purpose, kernel methods, and in partic-
ular tree kernels allow for representing trees in
terms of all possible subtrees (Collins and Duffy,
2002). Unfortunately, the representation in entail-
ment recognition problems requires the definition
of kernels over graphs constituted by tree pairs,
which are in general different from kernels applied
to single trees. In (Zanzotto and Moschitti, 2006),
this has been addressed by introducing semantic
links (placeholders) between text and hypothesis
parse trees and evaluating two distinct tree ker-
nels for the trees of texts and for those of hypothe-
ses. In order to make such disjoint kernel combi-
nation effective, all possible assignments between
the placeholders of the first and the second en-
tailment pair were generated causing a remarkable
slowdown.
In this paper, we describe the feature space of
all possible tree fragment pairs and we show that it
can be evaluated with a much simpler kernel than
the one used in previous work, both in terms of
design and computational complexity. Moreover,
the experiments on the RTE datasets show that our
proposed kernel provides higher accuracy than the
simple union of tree kernel spaces.
</bodyText>
<sectionHeader confidence="0.973772" genericHeader="method">
2 Fragments of Tree Pair-based Graphs
</sectionHeader>
<bodyText confidence="0.998287307692308">
The previous section has pointed out that RTE can
be seen as a relational problem between word se-
quences of Text and Hypothesis. The syntactic
structures embedded in such sequences can be gen-
eralized by natural language grammars. Such gen-
eralization is very important since it is evident that
entailment cases depend on the syntactic structures
of Text and Hypothesis. More specifically, the set
R described in the previous section can be ex-
tended and generalized by considering syntactic
derivations2 that generate word sequences in the
training examples. This corresponds to the follow-
ing set of tree fragment pairs:
</bodyText>
<equation confidence="0.817576">
R
= {ht, hi : t F(T), h F(H)}, (1)
</equation>
<bodyText confidence="0.993582">
where F() indicates the set of tree fragments of a
parse tree (i.e. the one of the text T or of the hy-
pothesis H). R contains less sparse relations than
R. For instance, given T1 and H1 of the previous
section, we would have the following relational de-
scription:
</bodyText>
<page confidence="0.97971">
2
</page>
<bodyText confidence="0.988191">
By cutting derivation at different depth, different degrees
of generalization can be obtained.
</bodyText>
<equation confidence="0.4348446875">
R =
n
h
NP
NNP
,
NP
NNP
i , h
S
NP VP
,
S
NP VP
i ,
h
S
NP
NNP
VP
VBP
bought
NP
NNP
,
S
NP
NNP
VP
VBP
owns
NP
NNP
i ,
h
VP
VBP
bought
NP
NNP
,
VP
VBP
owns
NP
NNP
i , ..
o
</equation>
<bodyText confidence="0.9993518125">
These features (relational pairs) generalize the
entailment property, e.g. the pair h[VP [VBP bought] [NP]],
[VP [VBP own] [NP]]i generalizes many word sequences,
i.e. those external to the verbal phrases and inter-
nal to the NPs.
We can improve this space by adding semantic
links between the tree fragments. Such links
or placeholders have been firstly proposed in
(Zanzotto and Moschitti, 2006). A placeholder
assigned to a node of t and a node of h states
that such nodes dominate the same (or similar) in-
formation. In particular, placeholders are assigned
to nodes whose words ti in T are equal, similar, or
semantically dependent on words hj in H. Using
placeholders, we obtain a richer fragment pair
based representation that we call Rp, exemplified
</bodyText>
<figure confidence="0.950716880952381">
hereafter:
n
h
S
NP
NNP X
VP
VBP
bought
NP
NNP Y
,
S
NP
NNP X
VP
VBP
owns
NP
NNP Y
i
, h
S
NP VP
VBP
bought
NP
NNP Y
,
S
NP VP
VBP
owns
NP
NNP Y
i
, h
S
NP VP
,
S
NP VP
</figure>
<equation confidence="0.800387">
i , ...
o
</equation>
<bodyText confidence="0.959847428571429">
The placeholders (or variables) indicated with
X and Y specify that the NNPs labeled by
the same variables dominate similar or identical
words. Therefore, an automatic algorithm that
assigns placeholders to semantically similar con-
stituents is needed. Moreover, although Rp con-
tains more semantic and less sparse features than
</bodyText>
<page confidence="0.987014">
26
</page>
<bodyText confidence="0.979789769230769">
\x0cboth R and R, its cardinality is still exponential in
the number of the words of T and H. This means
that standard machine learning algorithms cannot
be applied. In contrast, tree kernels (Collins and
Duffy, 2002) can be used to efficiently generate
the huge space of tree fragments but, to generate
the space of pairs of tree fragments, a new kernel
function has to be defined.
The next section provides a solution to both
problems. i.e. an algorithm for placeholders as-
signments and for the computation of paired tree
kernels which generates R and Rp representa-
tions.
</bodyText>
<figure confidence="0.992408733333334">
F
\x10
VP
V
book
NP
D
a
N
flight
\x11
=
n
VP
V NP
D
a
N
flight
,
VP
V NP
D N
,
NP
D
a
N
flight
,
NP
D
a
N ,
NP
D N
flight
,
NP
D N
,
N
flight
, . . .
o
</figure>
<figureCaption confidence="0.993322">
Figure 1: A syntactic parse tree.
</figureCaption>
<sectionHeader confidence="0.461403" genericHeader="method">
3 Kernels over Semantic Tree Pair-based
Graphs
</sectionHeader>
<bodyText confidence="0.9841621875">
The previous section has shown that placeholders
enrich a tree-based graph with relational informa-
tion, which, in turn, can be captured by means
of word semantic similarities simw(wt, wh), e.g.
(Corley and Mihalcea, 2005; Glickman et al.,
2005). More specifically, we use a two-step greedy
algorithm to anchor the content words (verbs,
nouns, adjectives, and adverbs) in the hypothesis
WH to words in the text WT .
In the first step, each word wh in WH is con-
nected to all words wt in WT that have the max-
imum similarity simw(wt, wh) with it (more than
one wt can have the maximum similarity with wh).
As result, we have a set of anchors A WT WH.
simw(wt, wh) is computed by means of three tech-
niques:
</bodyText>
<listItem confidence="0.961329">
1. Two words are maximally similar if they have
the same surface form wt = wh.
2. Otherwise, WordNet (Miller, 1995) similari-
ties (as in (Corley and Mihalcea, 2005)) and
different relation between words such as verb
entailment and derivational morphology are
applied.
3. The edit distance measure is finally used to
</listItem>
<bodyText confidence="0.98452519047619">
capture the similarity between words that are
missed by the previous analysis (for mis-
spelling errors or for the lack of derivational
forms in WordNet).
In the second step, we select the final anchor set
A A, such that wt (or wh) !hwt, whi A.
The selection is based on a simple greedy algo-
rithm that given two pairs hwt, whi and hw
t, whi
to be selected and a pair hst, shi already selected,
considers word proximity (in terms of number of
words) between wt and st and between w
t and st;
the nearest word will be chosen.
Once the graph has been enriched with seman-
tic information we need to represent it in the learn-
ing algorithm; for this purpose, an interesting ap-
proach is based on kernel methods. Since the con-
sidered graphs are composed by only two trees, we
can carried out a simplified computation of a graph
kernel based on tree kernel pairs.
</bodyText>
<subsectionHeader confidence="0.996117">
3.1 Tree Kernels
</subsectionHeader>
<bodyText confidence="0.9992924">
Tree Kernels (e.g. see NLP applications in (Giu-
glea and Moschitti, 2006; Zanzotto and Moschitti,
2006; Moschitti et al., 2007; Moschitti et al.,
2006; Moschitti and Bejan, 2004)) represent trees
in terms of their substructures (fragments) which
are mapped into feature vector spaces, e.g. Rn.
The kernel function measures the similarity be-
tween two trees by counting the number of their
common fragments. For example, Figure 1 shows
some substructures for the parse tree of the sen-
tence &quot;book a flight&quot;. The main advantage of
tree kernels is that, to compute the substructures
shared by two trees 1 and 2, the whole fragment
space is not used. In the following, we report the
formal definition presented in (Collins and Duffy,
2002).
Given the set of fragments {f1, f2, ..} = F, the
indicator function Ii(n) is equal 1 if the target fi is
rooted at node n and 0 otherwise. A tree kernel is
then defined as:
</bodyText>
<equation confidence="0.9982265">
TK(1, 2) =
X
n1N1
X
n2N2
(n1, n2) (2)
</equation>
<bodyText confidence="0.940777">
where N1 and N2 are the sets of the 1s and 2s
</bodyText>
<page confidence="0.959681">
27
</page>
<bodyText confidence="0.29731">
\x0cnodes, respectively and
</bodyText>
<equation confidence="0.9943902">
(n1, n2) =
|F|
X
i=1
Ii(n1)Ii(n2)
</equation>
<bodyText confidence="0.983342333333333">
The latter is equal to the number of common frag-
ments rooted in the n1 and n2 nodes and can be
evaluated with the following algorithm:
</bodyText>
<listItem confidence="0.913781125">
1. if the productions at n1 and n2 are different
then (n1, n2) = 0;
2. if the productions at n1 and n2 are the
same, and n1 and n2 have only leaf children
(i.e. they are pre-terminals symbols) then
(n1, n2) = 1;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
</listItem>
<equation confidence="0.918359272727273">
(n1, n2) =
nc(n1)
Y
j=1
(1 + (cj
n1
, cj
n2
)) (3)
where nc(n1) is the number of the children of
n1 and cj
</equation>
<bodyText confidence="0.9966924">
n is the j-th child of the node n. Note
that since the productions are the same, nc(n1) =
nc(n2).
Additionally, we add the decay factor by mod-
ifying steps (2) and (3) as follows3:
</bodyText>
<equation confidence="0.997783444444444">
2. (n1, n2) = ,
3. (n1, n2) =
nc(n1)
Y
j=1
(1 + (cj
n1
, cj
n2
</equation>
<bodyText confidence="0.5178955">
)).
The computational complexity of Eq. 2 is
</bodyText>
<equation confidence="0.969021">
O(|N1  ||N2 |) although the average running
</equation>
<bodyText confidence="0.992908">
time tends to be linear (Moschitti, 2006).
</bodyText>
<subsectionHeader confidence="0.999128">
3.2 Tree-based Graph Kernels
</subsectionHeader>
<bodyText confidence="0.9975449">
The above tree kernel function can be applied to
the parse trees of two texts or those of the two hy-
potheses to measure their similarity in terms of the
shared fragments. If we sum the contributions of
the two kernels (for texts and for hypotheses) as
proposed in (Zanzotto and Moschitti, 2006), we
just obtain the feature space of the union of the
fragments which is completely different from the
space of the tree fragments pairs, i.e. R . Note
that the union space is not useful to describe which
</bodyText>
<page confidence="0.988003">
3
</page>
<bodyText confidence="0.9992215">
To have a similarity score between 0 and 1, we also ap-
ply the normalization in the kernel space, i.e. K
</bodyText>
<equation confidence="0.99067975">
(1, 2) =
T K(1,2)
T K(1,1)T K(2,2)
.
</equation>
<bodyText confidence="0.9304965">
grammatical and lexical property is at the same
time held by T and H to trig the implication.
Therefore to generate the space of the frag-
ment pairs we need to define the kernel between
two pairs of entailment examples hT1, H1i and
hT2, H2i as
</bodyText>
<equation confidence="0.996394909090909">
Kp(hT1, H1i, hT2, H2i) =
=
X
n1T1
X
n2T2
X
n3H1
X
n4H2
(n1, n2, n3, n4),
</equation>
<bodyText confidence="0.98828">
where evaluates the number of subtrees rooted
in n1 and n2 combined with those rooted in n3 and
n4. More specifically, each fragment rooted into
the nodes of the two texts trees is combined with
each fragment rooted in the two hypotheses trees.
Now, since the number of subtrees rooted in the
texts is independent of the number of trees rooted
in the hypotheses,
</bodyText>
<equation confidence="0.663319">
(n1, n2, n3, n4) = (n1, n2)(n3, n4).
</equation>
<bodyText confidence="0.846618">
Therefore, we can rewrite Kp as:
</bodyText>
<equation confidence="0.998411625">
Kp(hT1, H1i, hT2, H2i) =
=
X
n1T1
X
n2T2
X
n3H1
X
n4H2
(n1, n2)(n3, n4) =
=
X
n1T1
X
n2T2
(n1, n2)
X
n3H1
X
n4H2
(n3, n4) =
= Kt(T1, T2) Kt(H1, H2).
(4)
</equation>
<bodyText confidence="0.999622714285714">
This result shows that the natural kernel to rep-
resent textual entailment sentences is the kernel
product, which corresponds to the set of all possi-
ble syntactic fragment pairs. Note that, such kernel
can be also used to evaluate the space of fragment
pairs for trees enriched with relational information,
i.e. by placeholders.
</bodyText>
<sectionHeader confidence="0.96947" genericHeader="method">
4 Approximated Graph Kernel
</sectionHeader>
<bodyText confidence="0.999520181818182">
The feature space described in the previous sec-
tion correctly encodes the fragment pairs. How-
ever, such huge space may result inadequate also
for algorithms such as SVMs, which are in general
robust to many irrelevant features. An approxima-
tion of the fragment pair space is given by the ker-
nel described in (Zanzotto and Moschitti, 2006).
Hereafter we illustrate its main points.
First, tree kernels applied to two texts or two hy-
potheses match identical fragments. When place-
holders are added to trees, the labeled fragments
</bodyText>
<page confidence="0.99025">
28
</page>
<bodyText confidence="0.993291733333333">
\x0care matched only if the basic fragments and the
assigned placeholders match. This means that
we should use the same placeholders for all texts
and all hypotheses of the corpus. Moreover, they
should be assigned in a way that similar syntac-
tic structures and similar relational information be-
tween two entailment pairs can be matched, i.e.
same placeholders should be assigned to the po-
tentially similar fragments.
Second, the above task cannot be carried out at
pre-processing time, i.e. when placeholders are
assigned to trees. At the running time, instead,
we can look at the comparing trees and make a
more consistent decision on the type and order of
placeholders. Although, there may be several ap-
proaches to accomplish this task, we apply a basic
heuristic which is very intuitive:
Choose the placeholder assignment that maxi-
mizes the tree kernel function over all possible cor-
respondences
More formally, let A and A be the placeholder sets
of hT, Hi and hT, Hi, respectively, without loss
of generality, we consider |A ||A |and we align
a subset of A to A. The best alignment is the one
that maximizes the syntactic and lexical overlap-
ping of the two subtrees induced by the aligned set
of anchors. By calling C the set of all bijective
mappings from S A, with |S |= |A|, to A,
an element c C is a substitution function. We
define the best alignment cmax the one determined
</bodyText>
<equation confidence="0.980098">
by
cmax = argmaxcC(TK(t(T, c), t(T
, i))+
TK(t(H, c), t(H
, i)),
</equation>
<bodyText confidence="0.942399272727273">
where (1) t(, c) returns the syntactic tree enriched
with placeholders replaced by means of the sub-
stitution c, (2) i is the identity substitution and (3)
TK(1, 2) is a tree kernel function (e.g. the one
specified by Eq. 2) applied to the two trees 1 and
2.
At the same time, the desired similarity value
to be used in the learning algorithm is given
by the kernel sum: TK(t(T, cmax), t(T, i)) +
TK(t(H, cmax), t(H, i)), i.e. by solving the fol-
lowing optimization problem:
</bodyText>
<equation confidence="0.9724755">
Ks(hT, Hi, hT
, H
i) =
maxcC(TK(t(T, c), t(T
, i))+
TK(t(H, c), t(H
, i)),
(5)
</equation>
<bodyText confidence="0.999552606060606">
For example, let us compare the following two
pairs (T1, H1) and (T2, H2) in Fig. 2.
To assign the placeholders 1, 2 and 3 of
(T2, H2) to those of (T1, H1), i.e. X and Y, we
need to maximize the similarity between the two
texts trees and between the two hypotheses trees.
It is straightforward to derive that X=1 and Y=3 al-
low more substructures (i.e. large part of the trees)
to be identical, e.g. [S [NP 1 X VP]] , [VP [VBP
NP 3 Y]], [S [NP 1 X VP [VBP NP 3 Y]]].
Finally, it should be noted that, (a)
Ks(hT, Hi, hT, Hi) is a symmetric function
since the set of derivation C are always computed
with respect to the pair that has the largest anchor
set and (b) it is not a valid kernel as the max
function does not in general produce valid kernels.
However, in (Haasdonk, 2005), it is shown that
when kernel functions are not positive semidef-
inite like in this case, SVMs still solve a data
separation problem in pseudo Euclidean spaces.
The drawback is that the solution may be only a
local optimum. Nevertheless, such solution can
still be valuable as the problem is modeled with a
very rich feature space.
Regarding the computational complexity, run-
ning the above kernel on a large training set may
result very expensive. To overcome this drawback,
in (Moschitti and Zanzotto, 2007), it has been de-
signed an algorithm to factorize the evaluation of
tree subparts with respect to the different substitu-
tion. The resulting speed-up makes the application
of such kernel feasible for datasets of ten of thou-
sands of instances.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.89542625">
The aim of the experiments is to show that the
space of tree fragment pairs is the most effective
to represent Tree Pair-based Graphs for the design
of Textual Entailment classifiers.
</bodyText>
<subsectionHeader confidence="0.990813">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.907160666666667">
To compare our model with previous work we
implemented the following kernels in SVM-light
(Joachims, 1999):
</bodyText>
<equation confidence="0.6791025">
Ks(e1, e2) = Kt(T1, T2) + Kt(H1, H2),
where e1 = hT1, H1i and e2 = hT2, H2i
</equation>
<bodyText confidence="0.993876333333333">
are two text and hypothesis pairs and Kt is
the syntactic tree kernel (Collins and Duffy,
2002) presented in the previous section.
</bodyText>
<equation confidence="0.649646">
Kp(e1, e2) = Kt(T1, T2) Kt(H1, H2),
</equation>
<bodyText confidence="0.982088">
which (as shown in the previous sections) en-
</bodyText>
<page confidence="0.94356">
29
</page>
<figure confidence="0.997680953125">
\x0cT1 H1
S
NP X
NNP X
Wanadoo
VP
VBP
bought
NP Y
NNP Y
KStones
S
NP X
NNP X
Wanadoo
VP
VBP
owns
NP Y
NNP Y
KStones
T2 H2
S
NP 1
NP 1
DT
the
NN 1
president
PP 2
IN
of
NP 2
NNP 2
Miramax
VP
VBP
bought
NP 3
DT
a
NN 3
castle
S
NP 1
NP 1
DT
the
NN 1
president
PP 2
IN
of
NP 2
NNP 2
Miramax
VP
VBP
own
NP 3
DT
a
NN 3
castle
</figure>
<figureCaption confidence="0.904504666666667">
Figure 2: The problem of finding the correct mapping between placeholders
codes the tree fragment pairs with and with-
out placeholders.
</figureCaption>
<equation confidence="0.8597442">
Kmax(e1, e2) = max
cC
Kt(c(T1), c(T2))+
Kt(c(H1), c(H2))
\x01
</equation>
<bodyText confidence="0.940905">
, where c is a possi-
ble placeholder assignment which connects
nodes from the first pair with those of the sec-
ond pair and c() transforms trees according
to c.
</bodyText>
<equation confidence="0.958238">
Kpmx(e1, e2) = max
cC
Kt(c(T1), c(T2))
Kt(c(H1), c(H2))
\x01
.
</equation>
<bodyText confidence="0.9963305">
Note that Kmax is the kernel proposed in (Zanzotto
and Moschitti, 2006) and Kpmx is a hybrid kernel
based on the maximum Kp, which uses the space
of tree fragment pairs. For all the above kernels,
we set the default cost factor and trade-off param-
eters and we set to 0.4.
To experiment with entailment relations, we
used the data sets made available by the first (Da-
gan et al., 2005) and second (Bar Haim et al., 2006)
Recognizing Textual Entailment Challenge. These
corpora are divided in the development sets D1
and D2 and the test sets T1 and T2. D1 contains
567 examples whereas T1, D2 and T2 all have the
same size, i.e. 800 instances. Each example is an
ordered pair of texts for which the entailment rela-
tion has to be decided.
</bodyText>
<subsectionHeader confidence="0.997766">
5.2 Evaluation and Discussion
</subsectionHeader>
<bodyText confidence="0.999849166666667">
Table 1 shows the results of the above kernels
on the split used for the RTE competitions. The
first column reports the kernel model. The second
and third columns illustrate the model accuracy for
RTE1 whereas column 4 and 5 show the accuracy
for RTE2. Moreover, P indicates the use of stan-
dard syntactic trees and P the use of trees enriched
with placeholders. We note that:
First, the space of tree fragment pairs, gener-
ated by Kp improves the one generated by Ks (i.e.
the simple union of the fragments of texts and hy-
potheses) of 4 (58.9% vs 54.9%) and 0.9 (53.5%
vs 52.6%) points on RTE1 and RTE2, respectively.
This suggests that the fragment pairs are more ef-
fective for encoding the syntactic rules describing
the entailment concept.
Second, on RTE1, the introduction of placehold-
ers does not improve Kp or Ks suggesting that for
their correct exploitation an extension of the space
of tree fragment pairs should be modeled.
Third, on RTE2, the impact of placeholders
seems more important but only Kmax and Ks
are able to fully exploit their semantic contribu-
tion. A possible explanation is that in order to
use the set of all possible assignments (required by
Kmax), we needed to prune the too large syntac-
tic trees as also suggested in (Zanzotto and Mos-
chitti, 2006). This may have negatively biased the
statistical distribution of tree fragment pairs.
Finally, although we show that Kp is better
</bodyText>
<page confidence="0.987669">
30
</page>
<table confidence="0.995871166666667">
\x0cKernels RTE1 RTE2
P P P P
Ks 54.9 50.0 52.6 59.5
Kp 58.9 55.5 53.5 56.0
Kmax - 58.25 - 61.0
Kpmx - 50.0 - 56.8
</table>
<tableCaption confidence="0.866427333333333">
Table 1: Accuracy of different kernel models using
(P) and not using ( P) placeholder information on
RTE1 and RTE2.
</tableCaption>
<bodyText confidence="0.9529606">
suited for RTE than the other kernels, its accuracy
is lower than the state-of-the-art in RTE. This is be-
cause the latter uses additional models like the lex-
ical similarity between text and hypothesis, which
greatly improve accuracy.
</bodyText>
<sectionHeader confidence="0.997658" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999797272727273">
In this paper, we have provided a statistical ma-
chine learning representation of textual entailment
via syntactic graphs constituted by tree pairs. We
have analytically shown that the natural way of
representing the syntactic relations between text
and hypothesis in learning algorithms consists in
the huge feature space of all possible syntactic tree
fragment pairs, which can only be managed using
kernel methods.
Therefore, we used tree kernels, which allow for
representing trees in terms of all possible subtrees.
More specifically, we defined a new model for the
entailment recognition problems, which requires
the definition of kernels over graphs constituted by
tree pairs. These are in general different from ker-
nels applied to single trees. We also studied an-
other alternative solution which concerns the use
of semantic links (placeholders) between text and
hypothesis parse trees (to form relevant semantic
fragment pairs) and the evaluation of two distinct
tree kernels for the trees of texts and for those of
hypotheses. In order to make such disjoint kernel
combination effective, all possible assignments be-
tween the placeholders of the first and the second
entailment pair have to be generated (causing a re-
markable slowdown).
Our experiments on the RTE datasets show that
our proposed kernel may provide higher accuracy
than the simple union of tree kernel spaces with a
much simpler and faster algorithm. Future work
will be devoted to make the tree fragment pair
space more effective, e.g. by using smaller and ac-
curate tree representation for text and hypothesis.
</bodyText>
<sectionHeader confidence="0.978367" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998799">
We would like to thank the anonymous reviewers
for their professional and competent reviews and
for their invaluable suggestions.
Alessandro Moschitti would like to thank the Eu-
ropean Union project, LUNA (spoken Language
UNderstanding in multilinguAl communication
systems) contract n 33549 for supporting part of
his research.
</bodyText>
<sectionHeader confidence="0.99115" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995479682926829">
Bar Haim, Roy, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The II PASCAL RTE challenge.
In PASCAL Challenges Workshop, Venice, Italy.
Bos, Johan and Katja Markert. 2005. Recognising
textual entailment with logical inference. In Pro-
ceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 628635, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Collins, Michael and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of ACL02.
Corley, Courtney and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proc. of the
ACL Workshop on Empirical Modeling of Semantic
Equivalence and Entailment, pages 1318, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL RTE challenge. In PASCAL
Challenges Workshop, Southampton, U.K.
de Salvo Braz, R., R. Girju, V. Punyakanok, D. Roth,
and M. Sammons. 2005. An inference model for se-
mantic entailment in natural language. In Proceed-
ings of AAAI, pages 16781679.
Giuglea, Ana-Maria and Alessandro Moschitti. 2006.
Semantic role labeling via framenet, verbnet and
propbank. In Proceedings of Coling-ACL, Sydney,
Australia.
Glickman, Oren and Ido Dagan. 2004. Probabilistic
textual entailment: Generic applied modeling of lan-
guage variability. In Proceedings of the Workshop on
Learning Methods for Text Understanding and Min-
ing, Grenoble, France.
Glickman, Oren, Ido Dagan, and Moshe Koppel. 2005.
Web based probabilistic textual entailment. In Pro-
ceedings of the 1st Pascal Challenge Workshop,
Southampton, UK.
</reference>
<page confidence="0.996473">
31
</page>
<reference confidence="0.999755647058824">
\x0cHaasdonk, Bernard. 2005. Feature space interpretation
of SVMs with indefinite kernels. IEEE Trans Pat-
tern Anal Mach Intell, 27(4):48292, Apr.
Joachims, Thorsten. 1999. Making large-scale svm
learning practical. In Schlkopf, B., C. Burges, and
A. Smola, editors, Advances in Kernel Methods-
Support Vector Learning. MIT Press.
Miller, George A. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):3941, November.
Moschitti, Alessandro and Cosmin Adrian Bejan.
2004. A semantic kernel for predicate argument
classification. In CoNLL-2004, USA.
Moschitti, A. and F. Zanzotto. 2007. Fast and effective
kernels for relational learning from texts. In Ghahra-
mani, Zoubin, editor, Proceedings of the 24th An-
nual International Conference on Machine Learning
(ICML 2007).
Moschitti, Alessandro, Daniele Pighin, and Roberto
Basili. 2006. Semantic Role Labeling via Tree Ker-
nel Joint Inference. In Proceedings of CoNLL-X.
Moschitti, Alessandro, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for question
answer classification. In Proceedings ACL, Prague,
Czech Republic.
Moschitti, Alessandro. 2006. Efficient convolution
kernels for dependency and constituent syntactic
trees. In ECML06.
Zanzotto, Fabio Massimo and Alessandro Moschitti.
2006. Automatic learning of textual entailments
with cross-pair similarities. In Proceedings of the
21st Coling and 44th ACL, pages 401408, Sydney,
Australia, July.
</reference>
<page confidence="0.983518">
32
</page>
<figure confidence="0.251985">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.481633">
<note confidence="0.751348">b&apos;Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 2532 Manchester, August 2008</note>
<title confidence="0.9920915">Encoding Tree Pair-based Graphs in Learning Algorithms: the Textual Entailment Recognition Case</title>
<author confidence="0.999915">Alessandro Moschitti</author>
<affiliation confidence="0.999963">DISI, University of Trento</affiliation>
<address confidence="0.983132">Via Sommarive 14 38100 POVO (TN) - Italy</address>
<email confidence="0.998359">moschitti@dit.unitn.it</email>
<author confidence="0.999524">Fabio Massimo Zanzotto</author>
<affiliation confidence="0.997722">DISP, University of Rome Tor Vergata</affiliation>
<address confidence="0.9635515">Via del Politecnico 1 00133 Roma, Italy</address>
<email confidence="0.993131">zanzotto@info.uniroma2.it</email>
<abstract confidence="0.999700846153846">In this paper, we provide a statistical machine learning representation of textual entailment via syntactic graphs constituted by tree pairs. We show that the natural way of representing the syntactic relations between text and hypothesis consists in the huge feature space of all possible syntactic tree fragment pairs, which can only be managed using kernel methods. Experiments with Support Vector Machines and our new kernels for paired trees show the validity of our interpretation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bar Haim</author>
<author>Ido Dagan Roy</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<date>2006</date>
<booktitle>The II PASCAL RTE challenge. In PASCAL Challenges Workshop,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="1117" citStr="Haim et al., 2006" startWordPosition="164" endWordPosition="167">aper, we provide a statistical machine learning representation of textual entailment via syntactic graphs constituted by tree pairs. We show that the natural way of representing the syntactic relations between text and hypothesis consists in the huge feature space of all possible syntactic tree fragment pairs, which can only be managed using kernel methods. Experiments with Support Vector Machines and our new kernels for paired trees show the validity of our interpretation. 1 Introduction Recently, a lot of valuable work on the recognition of textual entailment (RTE) has been carried out (Bar Haim et al., 2006). The aim is to detect implications between sentences like: T1 H1 T1 Wanadoo bought KStones H1 Wanadoo owns KStones where T1 and H1 stand for text and hypothesis, respectively. Several models, ranging from the simple lexical similarity between T and H to advanced Logic Form Representations, have been proposed (Corley and Mihalcea, 2005; Glickman and Dagan, 2004; de Salvo Braz et al., 2005; Bos and Markert, 2005). However, since a linguistic theory able to analytically show how to computationally solve the RTE problem has not been developed yet, to c 2008. Licensed under the Creative Commons At</context>
<context position="19816" citStr="Haim et al., 2006" startWordPosition="3576" endWordPosition="3579">e placeholder assignment which connects nodes from the first pair with those of the second pair and c() transforms trees according to c. Kpmx(e1, e2) = max cC Kt(c(T1), c(T2)) Kt(c(H1), c(H2)) \x01 . Note that Kmax is the kernel proposed in (Zanzotto and Moschitti, 2006) and Kpmx is a hybrid kernel based on the maximum Kp, which uses the space of tree fragment pairs. For all the above kernels, we set the default cost factor and trade-off parameters and we set to 0.4. To experiment with entailment relations, we used the data sets made available by the first (Dagan et al., 2005) and second (Bar Haim et al., 2006) Recognizing Textual Entailment Challenge. These corpora are divided in the development sets D1 and D2 and the test sets T1 and T2. D1 contains 567 examples whereas T1, D2 and T2 all have the same size, i.e. 800 instances. Each example is an ordered pair of texts for which the entailment relation has to be decided. 5.2 Evaluation and Discussion Table 1 shows the results of the above kernels on the split used for the RTE competitions. The first column reports the kernel model. The second and third columns illustrate the model accuracy for RTE1 whereas column 4 and 5 show the accuracy for RTE2. </context>
</contexts>
<marker>Haim, Roy, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Bar Haim, Roy, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The II PASCAL RTE challenge. In PASCAL Challenges Workshop, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising textual entailment with logical inference.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>628635</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="1532" citStr="Bos and Markert, 2005" startWordPosition="234" endWordPosition="238">w kernels for paired trees show the validity of our interpretation. 1 Introduction Recently, a lot of valuable work on the recognition of textual entailment (RTE) has been carried out (Bar Haim et al., 2006). The aim is to detect implications between sentences like: T1 H1 T1 Wanadoo bought KStones H1 Wanadoo owns KStones where T1 and H1 stand for text and hypothesis, respectively. Several models, ranging from the simple lexical similarity between T and H to advanced Logic Form Representations, have been proposed (Corley and Mihalcea, 2005; Glickman and Dagan, 2004; de Salvo Braz et al., 2005; Bos and Markert, 2005). However, since a linguistic theory able to analytically show how to computationally solve the RTE problem has not been developed yet, to c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. design accurate systems, we should rely upon the application of machine learning. In this perspective, TE training examples have to be represented in terms of statistical feature distributions. These typically consist in word sequences (along with their lexical similarity) and the s</context>
</contexts>
<marker>Bos, Markert, 2005</marker>
<rawString>Bos, Johan and Katja Markert. 2005. Recognising textual entailment with logical inference. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 628635, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL02.</booktitle>
<contexts>
<context position="3864" citStr="Collins and Duffy, 2002" startWordPosition="620" endWordPosition="623">hich may prevent to carry out effective learning. The traditional solution for this problem relates to consider the syntactic structure of word sequences which provides their generalization. The use of syntactic trees poses the problem of representing structures in learning algorithms. 1 V is larger than the actual space, which is the one of all possible subsequences with gaps, i.e. it only contains all possible concatenations of words respecting their order. 25 \x0cFor this purpose, kernel methods, and in particular tree kernels allow for representing trees in terms of all possible subtrees (Collins and Duffy, 2002). Unfortunately, the representation in entailment recognition problems requires the definition of kernels over graphs constituted by tree pairs, which are in general different from kernels applied to single trees. In (Zanzotto and Moschitti, 2006), this has been addressed by introducing semantic links (placeholders) between text and hypothesis parse trees and evaluating two distinct tree kernels for the trees of texts and for those of hypotheses. In order to make such disjoint kernel combination effective, all possible assignments between the placeholders of the first and the second entailment</context>
<context position="7602" citStr="Collins and Duffy, 2002" startWordPosition="1289" endWordPosition="1292">ought NP NNP Y , S NP VP VBP owns NP NNP Y i , h S NP VP , S NP VP i , ... o The placeholders (or variables) indicated with X and Y specify that the NNPs labeled by the same variables dominate similar or identical words. Therefore, an automatic algorithm that assigns placeholders to semantically similar constituents is needed. Moreover, although Rp contains more semantic and less sparse features than 26 \x0cboth R and R, its cardinality is still exponential in the number of the words of T and H. This means that standard machine learning algorithms cannot be applied. In contrast, tree kernels (Collins and Duffy, 2002) can be used to efficiently generate the huge space of tree fragments but, to generate the space of pairs of tree fragments, a new kernel function has to be defined. The next section provides a solution to both problems. i.e. an algorithm for placeholders assignments and for the computation of paired tree kernels which generates R and Rp representations. F \x10 VP V book NP D a N flight \x11 = n VP V NP D a N flight , VP V NP D N , NP D a N flight , NP D a N , NP D N flight , NP D N , N flight , . . . o Figure 1: A syntactic parse tree. 3 Kernels over Semantic Tree Pair-based Graphs The previo</context>
<context position="10810" citStr="Collins and Duffy, 2002" startWordPosition="1879" endWordPosition="1882">itti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. Rn. The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence &quot;book a flight&quot;. The main advantage of tree kernels is that, to compute the substructures shared by two trees 1 and 2, the whole fragment space is not used. In the following, we report the formal definition presented in (Collins and Duffy, 2002). Given the set of fragments {f1, f2, ..} = F, the indicator function Ii(n) is equal 1 if the target fi is rooted at node n and 0 otherwise. A tree kernel is then defined as: TK(1, 2) = X n1N1 X n2N2 (n1, n2) (2) where N1 and N2 are the sets of the 1s and 2s 27 \x0cnodes, respectively and (n1, n2) = |F| X i=1 Ii(n1)Ii(n2) The latter is equal to the number of common fragments rooted in the n1 and n2 nodes and can be evaluated with the following algorithm: 1. if the productions at n1 and n2 are different then (n1, n2) = 0; 2. if the productions at n1 and n2 are the same, and n1 and n2 have only </context>
<context position="18541" citStr="Collins and Duffy, 2002" startWordPosition="3315" endWordPosition="3318">tution. The resulting speed-up makes the application of such kernel feasible for datasets of ten of thousands of instances. 5 Experiments The aim of the experiments is to show that the space of tree fragment pairs is the most effective to represent Tree Pair-based Graphs for the design of Textual Entailment classifiers. 5.1 Experimental Setup To compare our model with previous work we implemented the following kernels in SVM-light (Joachims, 1999): Ks(e1, e2) = Kt(T1, T2) + Kt(H1, H2), where e1 = hT1, H1i and e2 = hT2, H2i are two text and hypothesis pairs and Kt is the syntactic tree kernel (Collins and Duffy, 2002) presented in the previous section. Kp(e1, e2) = Kt(T1, T2) Kt(H1, H2), which (as shown in the previous sections) en29 \x0cT1 H1 S NP X NNP X Wanadoo VP VBP bought NP Y NNP Y KStones S NP X NNP X Wanadoo VP VBP owns NP Y NNP Y KStones T2 H2 S NP 1 NP 1 DT the NN 1 president PP 2 IN of NP 2 NNP 2 Miramax VP VBP bought NP 3 DT a NN 3 castle S NP 1 NP 1 DT the NN 1 president PP 2 IN of NP 2 NNP 2 Miramax VP VBP own NP 3 DT a NN 3 castle Figure 2: The problem of finding the correct mapping between placeholders codes the tree fragment pairs with and without placeholders. Kmax(e1, e2) = max cC Kt(c(</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Collins, Michael and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of ACL02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Corley</author>
<author>Rada Mihalcea</author>
</authors>
<title>Measuring the semantic similarity of texts.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>1318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1454" citStr="Corley and Mihalcea, 2005" startWordPosition="219" endWordPosition="223">managed using kernel methods. Experiments with Support Vector Machines and our new kernels for paired trees show the validity of our interpretation. 1 Introduction Recently, a lot of valuable work on the recognition of textual entailment (RTE) has been carried out (Bar Haim et al., 2006). The aim is to detect implications between sentences like: T1 H1 T1 Wanadoo bought KStones H1 Wanadoo owns KStones where T1 and H1 stand for text and hypothesis, respectively. Several models, ranging from the simple lexical similarity between T and H to advanced Logic Form Representations, have been proposed (Corley and Mihalcea, 2005; Glickman and Dagan, 2004; de Salvo Braz et al., 2005; Bos and Markert, 2005). However, since a linguistic theory able to analytically show how to computationally solve the RTE problem has not been developed yet, to c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. design accurate systems, we should rely upon the application of machine learning. In this perspective, TE training examples have to be represented in terms of statistical feature distributions. These typic</context>
<context position="8412" citStr="Corley and Mihalcea, 2005" startWordPosition="1451" endWordPosition="1454">ovides a solution to both problems. i.e. an algorithm for placeholders assignments and for the computation of paired tree kernels which generates R and Rp representations. F \x10 VP V book NP D a N flight \x11 = n VP V NP D a N flight , VP V NP D N , NP D a N flight , NP D a N , NP D N flight , NP D N , N flight , . . . o Figure 1: A syntactic parse tree. 3 Kernels over Semantic Tree Pair-based Graphs The previous section has shown that placeholders enrich a tree-based graph with relational information, which, in turn, can be captured by means of word semantic similarities simw(wt, wh), e.g. (Corley and Mihalcea, 2005; Glickman et al., 2005). More specifically, we use a two-step greedy algorithm to anchor the content words (verbs, nouns, adjectives, and adverbs) in the hypothesis WH to words in the text WT . In the first step, each word wh in WH is connected to all words wt in WT that have the maximum similarity simw(wt, wh) with it (more than one wt can have the maximum similarity with wh). As result, we have a set of anchors A WT WH. simw(wt, wh) is computed by means of three techniques: 1. Two words are maximally similar if they have the same surface form wt = wh. 2. Otherwise, WordNet (Miller, 1995) si</context>
</contexts>
<marker>Corley, Mihalcea, 2005</marker>
<rawString>Corley, Courtney and Rada Mihalcea. 2005. Measuring the semantic similarity of texts. In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1318, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<date>2005</date>
<booktitle>The PASCAL RTE challenge. In PASCAL Challenges Workshop,</booktitle>
<location>Southampton, U.K.</location>
<contexts>
<context position="19781" citStr="Dagan et al., 2005" startWordPosition="3568" endWordPosition="3572">, c(H2)) \x01 , where c is a possible placeholder assignment which connects nodes from the first pair with those of the second pair and c() transforms trees according to c. Kpmx(e1, e2) = max cC Kt(c(T1), c(T2)) Kt(c(H1), c(H2)) \x01 . Note that Kmax is the kernel proposed in (Zanzotto and Moschitti, 2006) and Kpmx is a hybrid kernel based on the maximum Kp, which uses the space of tree fragment pairs. For all the above kernels, we set the default cost factor and trade-off parameters and we set to 0.4. To experiment with entailment relations, we used the data sets made available by the first (Dagan et al., 2005) and second (Bar Haim et al., 2006) Recognizing Textual Entailment Challenge. These corpora are divided in the development sets D1 and D2 and the test sets T1 and T2. D1 contains 567 examples whereas T1, D2 and T2 all have the same size, i.e. 800 instances. Each example is an ordered pair of texts for which the entailment relation has to be decided. 5.2 Evaluation and Discussion Table 1 shows the results of the above kernels on the split used for the RTE competitions. The first column reports the kernel model. The second and third columns illustrate the model accuracy for RTE1 whereas column 4</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Dagan, Ido, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL RTE challenge. In PASCAL Challenges Workshop, Southampton, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>de Salvo Braz</author>
<author>R Girju R</author>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>M Sammons</author>
</authors>
<title>An inference model for semantic entailment in natural language.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>16781679</pages>
<contexts>
<context position="1508" citStr="Braz et al., 2005" startWordPosition="230" endWordPosition="233">Machines and our new kernels for paired trees show the validity of our interpretation. 1 Introduction Recently, a lot of valuable work on the recognition of textual entailment (RTE) has been carried out (Bar Haim et al., 2006). The aim is to detect implications between sentences like: T1 H1 T1 Wanadoo bought KStones H1 Wanadoo owns KStones where T1 and H1 stand for text and hypothesis, respectively. Several models, ranging from the simple lexical similarity between T and H to advanced Logic Form Representations, have been proposed (Corley and Mihalcea, 2005; Glickman and Dagan, 2004; de Salvo Braz et al., 2005; Bos and Markert, 2005). However, since a linguistic theory able to analytically show how to computationally solve the RTE problem has not been developed yet, to c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. design accurate systems, we should rely upon the application of machine learning. In this perspective, TE training examples have to be represented in terms of statistical feature distributions. These typically consist in word sequences (along with their lexic</context>
</contexts>
<marker>Braz, R, Punyakanok, Roth, Sammons, 2005</marker>
<rawString>de Salvo Braz, R., R. Girju, V. Punyakanok, D. Roth, and M. Sammons. 2005. An inference model for semantic entailment in natural language. In Proceedings of AAAI, pages 16781679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Giuglea</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Semantic role labeling via framenet, verbnet and propbank.</title>
<date>2006</date>
<booktitle>In Proceedings of Coling-ACL,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="10149" citStr="Giuglea and Moschitti, 2006" startWordPosition="1769" endWordPosition="1773">rs hwt, whi and hw t, whi to be selected and a pair hst, shi already selected, considers word proximity (in terms of number of words) between wt and st and between w t and st; the nearest word will be chosen. Once the graph has been enriched with semantic information we need to represent it in the learning algorithm; for this purpose, an interesting approach is based on kernel methods. Since the considered graphs are composed by only two trees, we can carried out a simplified computation of a graph kernel based on tree kernel pairs. 3.1 Tree Kernels Tree Kernels (e.g. see NLP applications in (Giuglea and Moschitti, 2006; Zanzotto and Moschitti, 2006; Moschitti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. Rn. The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence &quot;book a flight&quot;. The main advantage of tree kernels is that, to compute the substructures shared by two trees 1 and 2, the whole fragment space is not used. In the following, we report</context>
</contexts>
<marker>Giuglea, Moschitti, 2006</marker>
<rawString>Giuglea, Ana-Maria and Alessandro Moschitti. 2006. Semantic role labeling via framenet, verbnet and propbank. In Proceedings of Coling-ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
</authors>
<title>Probabilistic textual entailment: Generic applied modeling of language variability.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Learning Methods for Text Understanding and Mining,</booktitle>
<location>Grenoble, France.</location>
<contexts>
<context position="1480" citStr="Glickman and Dagan, 2004" startWordPosition="224" endWordPosition="227">s. Experiments with Support Vector Machines and our new kernels for paired trees show the validity of our interpretation. 1 Introduction Recently, a lot of valuable work on the recognition of textual entailment (RTE) has been carried out (Bar Haim et al., 2006). The aim is to detect implications between sentences like: T1 H1 T1 Wanadoo bought KStones H1 Wanadoo owns KStones where T1 and H1 stand for text and hypothesis, respectively. Several models, ranging from the simple lexical similarity between T and H to advanced Logic Form Representations, have been proposed (Corley and Mihalcea, 2005; Glickman and Dagan, 2004; de Salvo Braz et al., 2005; Bos and Markert, 2005). However, since a linguistic theory able to analytically show how to computationally solve the RTE problem has not been developed yet, to c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. design accurate systems, we should rely upon the application of machine learning. In this perspective, TE training examples have to be represented in terms of statistical feature distributions. These typically consist in word seque</context>
</contexts>
<marker>Glickman, Dagan, 2004</marker>
<rawString>Glickman, Oren and Ido Dagan. 2004. Probabilistic textual entailment: Generic applied modeling of language variability. In Proceedings of the Workshop on Learning Methods for Text Understanding and Mining, Grenoble, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
<author>Moshe Koppel</author>
</authors>
<title>Web based probabilistic textual entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 1st Pascal Challenge Workshop,</booktitle>
<location>Southampton, UK.</location>
<contexts>
<context position="8436" citStr="Glickman et al., 2005" startWordPosition="1455" endWordPosition="1458">roblems. i.e. an algorithm for placeholders assignments and for the computation of paired tree kernels which generates R and Rp representations. F \x10 VP V book NP D a N flight \x11 = n VP V NP D a N flight , VP V NP D N , NP D a N flight , NP D a N , NP D N flight , NP D N , N flight , . . . o Figure 1: A syntactic parse tree. 3 Kernels over Semantic Tree Pair-based Graphs The previous section has shown that placeholders enrich a tree-based graph with relational information, which, in turn, can be captured by means of word semantic similarities simw(wt, wh), e.g. (Corley and Mihalcea, 2005; Glickman et al., 2005). More specifically, we use a two-step greedy algorithm to anchor the content words (verbs, nouns, adjectives, and adverbs) in the hypothesis WH to words in the text WT . In the first step, each word wh in WH is connected to all words wt in WT that have the maximum similarity simw(wt, wh) with it (more than one wt can have the maximum similarity with wh). As result, we have a set of anchors A WT WH. simw(wt, wh) is computed by means of three techniques: 1. Two words are maximally similar if they have the same surface form wt = wh. 2. Otherwise, WordNet (Miller, 1995) similarities (as in (Corle</context>
</contexts>
<marker>Glickman, Dagan, Koppel, 2005</marker>
<rawString>Glickman, Oren, Ido Dagan, and Moshe Koppel. 2005. Web based probabilistic textual entailment. In Proceedings of the 1st Pascal Challenge Workshop, Southampton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard \x0cHaasdonk</author>
</authors>
<title>Feature space interpretation of SVMs with indefinite kernels.</title>
<date>2005</date>
<journal>IEEE Trans Pattern Anal Mach Intell,</journal>
<volume>27</volume>
<issue>4</issue>
<marker>\x0cHaasdonk, 2005</marker>
<rawString>\x0cHaasdonk, Bernard. 2005. Feature space interpretation of SVMs with indefinite kernels. IEEE Trans Pattern Anal Mach Intell, 27(4):48292, Apr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale svm learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel MethodsSupport Vector Learning.</booktitle>
<editor>In Schlkopf, B., C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="18368" citStr="Joachims, 1999" startWordPosition="3282" endWordPosition="3283">his drawback, in (Moschitti and Zanzotto, 2007), it has been designed an algorithm to factorize the evaluation of tree subparts with respect to the different substitution. The resulting speed-up makes the application of such kernel feasible for datasets of ten of thousands of instances. 5 Experiments The aim of the experiments is to show that the space of tree fragment pairs is the most effective to represent Tree Pair-based Graphs for the design of Textual Entailment classifiers. 5.1 Experimental Setup To compare our model with previous work we implemented the following kernels in SVM-light (Joachims, 1999): Ks(e1, e2) = Kt(T1, T2) + Kt(H1, H2), where e1 = hT1, H1i and e2 = hT2, H2i are two text and hypothesis pairs and Kt is the syntactic tree kernel (Collins and Duffy, 2002) presented in the previous section. Kp(e1, e2) = Kt(T1, T2) Kt(H1, H2), which (as shown in the previous sections) en29 \x0cT1 H1 S NP X NNP X Wanadoo VP VBP bought NP Y NNP Y KStones S NP X NNP X Wanadoo VP VBP owns NP Y NNP Y KStones T2 H2 S NP 1 NP 1 DT the NN 1 president PP 2 IN of NP 2 NNP 2 Miramax VP VBP bought NP 3 DT a NN 3 castle S NP 1 NP 1 DT the NN 1 president PP 2 IN of NP 2 NNP 2 Miramax VP VBP own NP 3 DT a N</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Joachims, Thorsten. 1999. Making large-scale svm learning practical. In Schlkopf, B., C. Burges, and A. Smola, editors, Advances in Kernel MethodsSupport Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="9009" citStr="Miller, 1995" startWordPosition="1567" endWordPosition="1568">d Mihalcea, 2005; Glickman et al., 2005). More specifically, we use a two-step greedy algorithm to anchor the content words (verbs, nouns, adjectives, and adverbs) in the hypothesis WH to words in the text WT . In the first step, each word wh in WH is connected to all words wt in WT that have the maximum similarity simw(wt, wh) with it (more than one wt can have the maximum similarity with wh). As result, we have a set of anchors A WT WH. simw(wt, wh) is computed by means of three techniques: 1. Two words are maximally similar if they have the same surface form wt = wh. 2. Otherwise, WordNet (Miller, 1995) similarities (as in (Corley and Mihalcea, 2005)) and different relation between words such as verb entailment and derivational morphology are applied. 3. The edit distance measure is finally used to capture the similarity between words that are missed by the previous analysis (for misspelling errors or for the lack of derivational forms in WordNet). In the second step, we select the final anchor set A A, such that wt (or wh) !hwt, whi A. The selection is based on a simple greedy algorithm that given two pairs hwt, whi and hw t, whi to be selected and a pair hst, shi already selected, consider</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>Miller, George A. 1995. WordNet: A lexical database for English. Communications of the ACM, 38(11):3941, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Cosmin Adrian Bejan</author>
</authors>
<title>A semantic kernel for predicate argument classification.</title>
<date>2004</date>
<booktitle>In CoNLL-2004,</booktitle>
<location>USA.</location>
<contexts>
<context position="10255" citStr="Moschitti and Bejan, 2004" startWordPosition="1786" endWordPosition="1789"> terms of number of words) between wt and st and between w t and st; the nearest word will be chosen. Once the graph has been enriched with semantic information we need to represent it in the learning algorithm; for this purpose, an interesting approach is based on kernel methods. Since the considered graphs are composed by only two trees, we can carried out a simplified computation of a graph kernel based on tree kernel pairs. 3.1 Tree Kernels Tree Kernels (e.g. see NLP applications in (Giuglea and Moschitti, 2006; Zanzotto and Moschitti, 2006; Moschitti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. Rn. The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence &quot;book a flight&quot;. The main advantage of tree kernels is that, to compute the substructures shared by two trees 1 and 2, the whole fragment space is not used. In the following, we report the formal definition presented in (Collins and Duffy, 2002). Given the set of fragments {f1, f2, ..} = F</context>
</contexts>
<marker>Moschitti, Bejan, 2004</marker>
<rawString>Moschitti, Alessandro and Cosmin Adrian Bejan. 2004. A semantic kernel for predicate argument classification. In CoNLL-2004, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>F Zanzotto</author>
</authors>
<title>Fast and effective kernels for relational learning from texts.</title>
<date>2007</date>
<booktitle>Proceedings of the 24th Annual International Conference on Machine Learning (ICML</booktitle>
<editor>In Ghahramani, Zoubin, editor,</editor>
<contexts>
<context position="17800" citStr="Moschitti and Zanzotto, 2007" startWordPosition="3188" endWordPosition="3191"> (b) it is not a valid kernel as the max function does not in general produce valid kernels. However, in (Haasdonk, 2005), it is shown that when kernel functions are not positive semidefinite like in this case, SVMs still solve a data separation problem in pseudo Euclidean spaces. The drawback is that the solution may be only a local optimum. Nevertheless, such solution can still be valuable as the problem is modeled with a very rich feature space. Regarding the computational complexity, running the above kernel on a large training set may result very expensive. To overcome this drawback, in (Moschitti and Zanzotto, 2007), it has been designed an algorithm to factorize the evaluation of tree subparts with respect to the different substitution. The resulting speed-up makes the application of such kernel feasible for datasets of ten of thousands of instances. 5 Experiments The aim of the experiments is to show that the space of tree fragment pairs is the most effective to represent Tree Pair-based Graphs for the design of Textual Entailment classifiers. 5.1 Experimental Setup To compare our model with previous work we implemented the following kernels in SVM-light (Joachims, 1999): Ks(e1, e2) = Kt(T1, T2) + Kt(H</context>
</contexts>
<marker>Moschitti, Zanzotto, 2007</marker>
<rawString>Moschitti, A. and F. Zanzotto. 2007. Fast and effective kernels for relational learning from texts. In Ghahramani, Zoubin, editor, Proceedings of the 24th Annual International Conference on Machine Learning (ICML 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Semantic Role Labeling via Tree Kernel Joint Inference.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X.</booktitle>
<contexts>
<context position="10227" citStr="Moschitti et al., 2006" startWordPosition="1782" endWordPosition="1785">iders word proximity (in terms of number of words) between wt and st and between w t and st; the nearest word will be chosen. Once the graph has been enriched with semantic information we need to represent it in the learning algorithm; for this purpose, an interesting approach is based on kernel methods. Since the considered graphs are composed by only two trees, we can carried out a simplified computation of a graph kernel based on tree kernel pairs. 3.1 Tree Kernels Tree Kernels (e.g. see NLP applications in (Giuglea and Moschitti, 2006; Zanzotto and Moschitti, 2006; Moschitti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. Rn. The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence &quot;book a flight&quot;. The main advantage of tree kernels is that, to compute the substructures shared by two trees 1 and 2, the whole fragment space is not used. In the following, we report the formal definition presented in (Collins and Duffy, 2002). Given the set o</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2006</marker>
<rawString>Moschitti, Alessandro, Daniele Pighin, and Roberto Basili. 2006. Semantic Role Labeling via Tree Kernel Joint Inference. In Proceedings of CoNLL-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question answer classification.</title>
<date>2007</date>
<booktitle>In Proceedings ACL,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="10203" citStr="Moschitti et al., 2007" startWordPosition="1778" endWordPosition="1781">i already selected, considers word proximity (in terms of number of words) between wt and st and between w t and st; the nearest word will be chosen. Once the graph has been enriched with semantic information we need to represent it in the learning algorithm; for this purpose, an interesting approach is based on kernel methods. Since the considered graphs are composed by only two trees, we can carried out a simplified computation of a graph kernel based on tree kernel pairs. 3.1 Tree Kernels Tree Kernels (e.g. see NLP applications in (Giuglea and Moschitti, 2006; Zanzotto and Moschitti, 2006; Moschitti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. Rn. The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence &quot;book a flight&quot;. The main advantage of tree kernels is that, to compute the substructures shared by two trees 1 and 2, the whole fragment space is not used. In the following, we report the formal definition presented in (Collins and Duffy</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Moschitti, Alessandro, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question answer classification. In Proceedings ACL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In ECML06.</booktitle>
<contexts>
<context position="4111" citStr="Moschitti, 2006" startWordPosition="657" endWordPosition="658">uctures in learning algorithms. 1 V is larger than the actual space, which is the one of all possible subsequences with gaps, i.e. it only contains all possible concatenations of words respecting their order. 25 \x0cFor this purpose, kernel methods, and in particular tree kernels allow for representing trees in terms of all possible subtrees (Collins and Duffy, 2002). Unfortunately, the representation in entailment recognition problems requires the definition of kernels over graphs constituted by tree pairs, which are in general different from kernels applied to single trees. In (Zanzotto and Moschitti, 2006), this has been addressed by introducing semantic links (placeholders) between text and hypothesis parse trees and evaluating two distinct tree kernels for the trees of texts and for those of hypotheses. In order to make such disjoint kernel combination effective, all possible assignments between the placeholders of the first and the second entailment pair were generated causing a remarkable slowdown. In this paper, we describe the feature space of all possible tree fragment pairs and we show that it can be evaluated with a much simpler kernel than the one used in previous work, both in terms </context>
<context position="6516" citStr="Moschitti, 2006" startWordPosition="1084" endWordPosition="1085">erent degrees of generalization can be obtained. R = n h NP NNP , NP NNP i , h S NP VP , S NP VP i , h S NP NNP VP VBP bought NP NNP , S NP NNP VP VBP owns NP NNP i , h VP VBP bought NP NNP , VP VBP owns NP NNP i , .. o These features (relational pairs) generalize the entailment property, e.g. the pair h[VP [VBP bought] [NP]], [VP [VBP own] [NP]]i generalizes many word sequences, i.e. those external to the verbal phrases and internal to the NPs. We can improve this space by adding semantic links between the tree fragments. Such links or placeholders have been firstly proposed in (Zanzotto and Moschitti, 2006). A placeholder assigned to a node of t and a node of h states that such nodes dominate the same (or similar) information. In particular, placeholders are assigned to nodes whose words ti in T are equal, similar, or semantically dependent on words hj in H. Using placeholders, we obtain a richer fragment pair based representation that we call Rp, exemplified hereafter: n h S NP NNP X VP VBP bought NP NNP Y , S NP NNP X VP VBP owns NP NNP Y i , h S NP VP VBP bought NP NNP Y , S NP VP VBP owns NP NNP Y i , h S NP VP , S NP VP i , ... o The placeholders (or variables) indicated with X and Y specif</context>
<context position="10149" citStr="Moschitti, 2006" startWordPosition="1772" endWordPosition="1773">and hw t, whi to be selected and a pair hst, shi already selected, considers word proximity (in terms of number of words) between wt and st and between w t and st; the nearest word will be chosen. Once the graph has been enriched with semantic information we need to represent it in the learning algorithm; for this purpose, an interesting approach is based on kernel methods. Since the considered graphs are composed by only two trees, we can carried out a simplified computation of a graph kernel based on tree kernel pairs. 3.1 Tree Kernels Tree Kernels (e.g. see NLP applications in (Giuglea and Moschitti, 2006; Zanzotto and Moschitti, 2006; Moschitti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. Rn. The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence &quot;book a flight&quot;. The main advantage of tree kernels is that, to compute the substructures shared by two trees 1 and 2, the whole fragment space is not used. In the following, we report</context>
<context position="12050" citStr="Moschitti, 2006" startWordPosition="2142" endWordPosition="2143"> are pre-terminals symbols) then (n1, n2) = 1; 3. if the productions at n1 and n2 are the same, and n1 and n2 are not pre-terminals then (n1, n2) = nc(n1) Y j=1 (1 + (cj n1 , cj n2 )) (3) where nc(n1) is the number of the children of n1 and cj n is the j-th child of the node n. Note that since the productions are the same, nc(n1) = nc(n2). Additionally, we add the decay factor by modifying steps (2) and (3) as follows3: 2. (n1, n2) = , 3. (n1, n2) = nc(n1) Y j=1 (1 + (cj n1 , cj n2 )). The computational complexity of Eq. 2 is O(|N1 ||N2 |) although the average running time tends to be linear (Moschitti, 2006). 3.2 Tree-based Graph Kernels The above tree kernel function can be applied to the parse trees of two texts or those of the two hypotheses to measure their similarity in terms of the shared fragments. If we sum the contributions of the two kernels (for texts and for hypotheses) as proposed in (Zanzotto and Moschitti, 2006), we just obtain the feature space of the union of the fragments which is completely different from the space of the tree fragments pairs, i.e. R . Note that the union space is not useful to describe which 3 To have a similarity score between 0 and 1, we also apply the norma</context>
<context position="14320" citStr="Moschitti, 2006" startWordPosition="2562" endWordPosition="2563">ences is the kernel product, which corresponds to the set of all possible syntactic fragment pairs. Note that, such kernel can be also used to evaluate the space of fragment pairs for trees enriched with relational information, i.e. by placeholders. 4 Approximated Graph Kernel The feature space described in the previous section correctly encodes the fragment pairs. However, such huge space may result inadequate also for algorithms such as SVMs, which are in general robust to many irrelevant features. An approximation of the fragment pair space is given by the kernel described in (Zanzotto and Moschitti, 2006). Hereafter we illustrate its main points. First, tree kernels applied to two texts or two hypotheses match identical fragments. When placeholders are added to trees, the labeled fragments 28 \x0care matched only if the basic fragments and the assigned placeholders match. This means that we should use the same placeholders for all texts and all hypotheses of the corpus. Moreover, they should be assigned in a way that similar syntactic structures and similar relational information between two entailment pairs can be matched, i.e. same placeholders should be assigned to the potentially similar f</context>
<context position="19469" citStr="Moschitti, 2006" startWordPosition="3512" endWordPosition="3513">bought NP 3 DT a NN 3 castle S NP 1 NP 1 DT the NN 1 president PP 2 IN of NP 2 NNP 2 Miramax VP VBP own NP 3 DT a NN 3 castle Figure 2: The problem of finding the correct mapping between placeholders codes the tree fragment pairs with and without placeholders. Kmax(e1, e2) = max cC Kt(c(T1), c(T2))+ Kt(c(H1), c(H2)) \x01 , where c is a possible placeholder assignment which connects nodes from the first pair with those of the second pair and c() transforms trees according to c. Kpmx(e1, e2) = max cC Kt(c(T1), c(T2)) Kt(c(H1), c(H2)) \x01 . Note that Kmax is the kernel proposed in (Zanzotto and Moschitti, 2006) and Kpmx is a hybrid kernel based on the maximum Kp, which uses the space of tree fragment pairs. For all the above kernels, we set the default cost factor and trade-off parameters and we set to 0.4. To experiment with entailment relations, we used the data sets made available by the first (Dagan et al., 2005) and second (Bar Haim et al., 2006) Recognizing Textual Entailment Challenge. These corpora are divided in the development sets D1 and D2 and the test sets T1 and T2. D1 contains 567 examples whereas T1, D2 and T2 all have the same size, i.e. 800 instances. Each example is an ordered pai</context>
<context position="21439" citStr="Moschitti, 2006" startWordPosition="3862" endWordPosition="3864">ore effective for encoding the syntactic rules describing the entailment concept. Second, on RTE1, the introduction of placeholders does not improve Kp or Ks suggesting that for their correct exploitation an extension of the space of tree fragment pairs should be modeled. Third, on RTE2, the impact of placeholders seems more important but only Kmax and Ks are able to fully exploit their semantic contribution. A possible explanation is that in order to use the set of all possible assignments (required by Kmax), we needed to prune the too large syntactic trees as also suggested in (Zanzotto and Moschitti, 2006). This may have negatively biased the statistical distribution of tree fragment pairs. Finally, although we show that Kp is better 30 \x0cKernels RTE1 RTE2 P P P P Ks 54.9 50.0 52.6 59.5 Kp 58.9 55.5 53.5 56.0 Kmax - 58.25 - 61.0 Kpmx - 50.0 - 56.8 Table 1: Accuracy of different kernel models using (P) and not using ( P) placeholder information on RTE1 and RTE2. suited for RTE than the other kernels, its accuracy is lower than the state-of-the-art in RTE. This is because the latter uses additional models like the lexical similarity between text and hypothesis, which greatly improve accuracy. 6</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Moschitti, Alessandro. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In ECML06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Automatic learning of textual entailments with cross-pair similarities.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st Coling and 44th ACL,</booktitle>
<pages>401408</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="4111" citStr="Zanzotto and Moschitti, 2006" startWordPosition="655" endWordPosition="658">resenting structures in learning algorithms. 1 V is larger than the actual space, which is the one of all possible subsequences with gaps, i.e. it only contains all possible concatenations of words respecting their order. 25 \x0cFor this purpose, kernel methods, and in particular tree kernels allow for representing trees in terms of all possible subtrees (Collins and Duffy, 2002). Unfortunately, the representation in entailment recognition problems requires the definition of kernels over graphs constituted by tree pairs, which are in general different from kernels applied to single trees. In (Zanzotto and Moschitti, 2006), this has been addressed by introducing semantic links (placeholders) between text and hypothesis parse trees and evaluating two distinct tree kernels for the trees of texts and for those of hypotheses. In order to make such disjoint kernel combination effective, all possible assignments between the placeholders of the first and the second entailment pair were generated causing a remarkable slowdown. In this paper, we describe the feature space of all possible tree fragment pairs and we show that it can be evaluated with a much simpler kernel than the one used in previous work, both in terms </context>
<context position="6516" citStr="Zanzotto and Moschitti, 2006" startWordPosition="1082" endWordPosition="1085">t depth, different degrees of generalization can be obtained. R = n h NP NNP , NP NNP i , h S NP VP , S NP VP i , h S NP NNP VP VBP bought NP NNP , S NP NNP VP VBP owns NP NNP i , h VP VBP bought NP NNP , VP VBP owns NP NNP i , .. o These features (relational pairs) generalize the entailment property, e.g. the pair h[VP [VBP bought] [NP]], [VP [VBP own] [NP]]i generalizes many word sequences, i.e. those external to the verbal phrases and internal to the NPs. We can improve this space by adding semantic links between the tree fragments. Such links or placeholders have been firstly proposed in (Zanzotto and Moschitti, 2006). A placeholder assigned to a node of t and a node of h states that such nodes dominate the same (or similar) information. In particular, placeholders are assigned to nodes whose words ti in T are equal, similar, or semantically dependent on words hj in H. Using placeholders, we obtain a richer fragment pair based representation that we call Rp, exemplified hereafter: n h S NP NNP X VP VBP bought NP NNP Y , S NP NNP X VP VBP owns NP NNP Y i , h S NP VP VBP bought NP NNP Y , S NP VP VBP owns NP NNP Y i , h S NP VP , S NP VP i , ... o The placeholders (or variables) indicated with X and Y specif</context>
<context position="10179" citStr="Zanzotto and Moschitti, 2006" startWordPosition="1774" endWordPosition="1777">be selected and a pair hst, shi already selected, considers word proximity (in terms of number of words) between wt and st and between w t and st; the nearest word will be chosen. Once the graph has been enriched with semantic information we need to represent it in the learning algorithm; for this purpose, an interesting approach is based on kernel methods. Since the considered graphs are composed by only two trees, we can carried out a simplified computation of a graph kernel based on tree kernel pairs. 3.1 Tree Kernels Tree Kernels (e.g. see NLP applications in (Giuglea and Moschitti, 2006; Zanzotto and Moschitti, 2006; Moschitti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. Rn. The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence &quot;book a flight&quot;. The main advantage of tree kernels is that, to compute the substructures shared by two trees 1 and 2, the whole fragment space is not used. In the following, we report the formal definition present</context>
<context position="12375" citStr="Zanzotto and Moschitti, 2006" startWordPosition="2197" endWordPosition="2200">re the same, nc(n1) = nc(n2). Additionally, we add the decay factor by modifying steps (2) and (3) as follows3: 2. (n1, n2) = , 3. (n1, n2) = nc(n1) Y j=1 (1 + (cj n1 , cj n2 )). The computational complexity of Eq. 2 is O(|N1 ||N2 |) although the average running time tends to be linear (Moschitti, 2006). 3.2 Tree-based Graph Kernels The above tree kernel function can be applied to the parse trees of two texts or those of the two hypotheses to measure their similarity in terms of the shared fragments. If we sum the contributions of the two kernels (for texts and for hypotheses) as proposed in (Zanzotto and Moschitti, 2006), we just obtain the feature space of the union of the fragments which is completely different from the space of the tree fragments pairs, i.e. R . Note that the union space is not useful to describe which 3 To have a similarity score between 0 and 1, we also apply the normalization in the kernel space, i.e. K (1, 2) = T K(1,2) T K(1,1)T K(2,2) . grammatical and lexical property is at the same time held by T and H to trig the implication. Therefore to generate the space of the fragment pairs we need to define the kernel between two pairs of entailment examples hT1, H1i and hT2, H2i as Kp(hT1, </context>
<context position="14320" citStr="Zanzotto and Moschitti, 2006" startWordPosition="2560" endWordPosition="2563">tailment sentences is the kernel product, which corresponds to the set of all possible syntactic fragment pairs. Note that, such kernel can be also used to evaluate the space of fragment pairs for trees enriched with relational information, i.e. by placeholders. 4 Approximated Graph Kernel The feature space described in the previous section correctly encodes the fragment pairs. However, such huge space may result inadequate also for algorithms such as SVMs, which are in general robust to many irrelevant features. An approximation of the fragment pair space is given by the kernel described in (Zanzotto and Moschitti, 2006). Hereafter we illustrate its main points. First, tree kernels applied to two texts or two hypotheses match identical fragments. When placeholders are added to trees, the labeled fragments 28 \x0care matched only if the basic fragments and the assigned placeholders match. This means that we should use the same placeholders for all texts and all hypotheses of the corpus. Moreover, they should be assigned in a way that similar syntactic structures and similar relational information between two entailment pairs can be matched, i.e. same placeholders should be assigned to the potentially similar f</context>
<context position="19469" citStr="Zanzotto and Moschitti, 2006" startWordPosition="3510" endWordPosition="3513">ramax VP VBP bought NP 3 DT a NN 3 castle S NP 1 NP 1 DT the NN 1 president PP 2 IN of NP 2 NNP 2 Miramax VP VBP own NP 3 DT a NN 3 castle Figure 2: The problem of finding the correct mapping between placeholders codes the tree fragment pairs with and without placeholders. Kmax(e1, e2) = max cC Kt(c(T1), c(T2))+ Kt(c(H1), c(H2)) \x01 , where c is a possible placeholder assignment which connects nodes from the first pair with those of the second pair and c() transforms trees according to c. Kpmx(e1, e2) = max cC Kt(c(T1), c(T2)) Kt(c(H1), c(H2)) \x01 . Note that Kmax is the kernel proposed in (Zanzotto and Moschitti, 2006) and Kpmx is a hybrid kernel based on the maximum Kp, which uses the space of tree fragment pairs. For all the above kernels, we set the default cost factor and trade-off parameters and we set to 0.4. To experiment with entailment relations, we used the data sets made available by the first (Dagan et al., 2005) and second (Bar Haim et al., 2006) Recognizing Textual Entailment Challenge. These corpora are divided in the development sets D1 and D2 and the test sets T1 and T2. D1 contains 567 examples whereas T1, D2 and T2 all have the same size, i.e. 800 instances. Each example is an ordered pai</context>
<context position="21439" citStr="Zanzotto and Moschitti, 2006" startWordPosition="3860" endWordPosition="3864">t pairs are more effective for encoding the syntactic rules describing the entailment concept. Second, on RTE1, the introduction of placeholders does not improve Kp or Ks suggesting that for their correct exploitation an extension of the space of tree fragment pairs should be modeled. Third, on RTE2, the impact of placeholders seems more important but only Kmax and Ks are able to fully exploit their semantic contribution. A possible explanation is that in order to use the set of all possible assignments (required by Kmax), we needed to prune the too large syntactic trees as also suggested in (Zanzotto and Moschitti, 2006). This may have negatively biased the statistical distribution of tree fragment pairs. Finally, although we show that Kp is better 30 \x0cKernels RTE1 RTE2 P P P P Ks 54.9 50.0 52.6 59.5 Kp 58.9 55.5 53.5 56.0 Kmax - 58.25 - 61.0 Kpmx - 50.0 - 56.8 Table 1: Accuracy of different kernel models using (P) and not using ( P) placeholder information on RTE1 and RTE2. suited for RTE than the other kernels, its accuracy is lower than the state-of-the-art in RTE. This is because the latter uses additional models like the lexical similarity between text and hypothesis, which greatly improve accuracy. 6</context>
</contexts>
<marker>Zanzotto, Moschitti, 2006</marker>
<rawString>Zanzotto, Fabio Massimo and Alessandro Moschitti. 2006. Automatic learning of textual entailments with cross-pair similarities. In Proceedings of the 21st Coling and 44th ACL, pages 401408, Sydney, Australia, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>