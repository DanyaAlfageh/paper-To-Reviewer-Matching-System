This sort of scenario was used in CITATION for training an n-gram language model using the perceptron algorithm,,
CITATION and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dyna,,
ogramming methods (CITATION; CITATION) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model,,
CITATION and Collins and Duffy (2,,
CITATION and Collins and Duffy (2002) rerank the top N p,,
First, are there guarantees for the algorithm if the training data is not separable? Second, performance on a training sample is all very well, but what does this guarantee about how well the algorithm generalizes to newly drawn test examples? CITATION discuss how the theory for classification problems can be extended to deal with both of these questions; CITATION describes how these results apply to NLP problems,,
We then built a generative model with this feature set and the same tree transform, for use with the beam-search parser from CITATION to compare against our baseline perceptron model,,
CITATION and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming,,
(1999) and CITATION use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large.,,
This paper explores an alternative approach to parsing, based on the perceptron training algorithm introduced in CITATION,,
One way around this problem is to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of CITATION,,
The algorithm and theorems are based on the approach to classification problems described in CITATION,,
We implemented the perceptron approach with the same feature set as that of an existing generative model (CITATIONa), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search,,
We will briefly review the perceptron algorithm, and its convergence properties see CITATION for a full description,,
In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of CITATIONa) and CITATION,,
As a final note, following CITATION, we used the averaged parameters from the training algorithm in decoding test examples in our experiments,,
CITATION and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming methods (CITATION; CITATION) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model,,
2.1 Linear Models for NLP We follow the framework outlined in CITATION; 2004),,
For this paper, we used POS tags that were provided either by the Treebank itself (gold standard tags) or by the perceptron POS tagger3 presented in CITATION,,
Unlike in CITATIONa; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word,,
Before inducing the left-child chains and allowable triples from the treebank, the trees are transformed with a selective left-corner transformation CITATION that has been flattened as presented in CITATIONb),,
(1999) and CITATION use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large,,
CITATION and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming methods (Geman and Johnso,,
As a final note, following CITATION, we used the averaged pa,,
to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of CITATION,,
3 A full description of the parsing approach The parser is an incremental beam-search parser very similar to the sort described in CITATIONa; 2004), with some changes in the search strategy to accommodate the perceptron feature weights,,
CITATION originally proposed the averaged parameter method; i,,
Note that the allowable chains in our grammar are what CITATION call connection paths from the partial parse to the next word,,
Examples of such techniques are Markov Random Fields (CITATION; CITATION; Della CITATION; CITATION), and boosting or perceptron approaches to reranking (CITATION; CITATION; CITATION),,
All of the convergence and generalization results in CITATION depend on notions of separability rather than the size of GEN,,
When the FSLC has been applied and the set is restricted to those occurring more than once 2See CITATION for a presentation of the transform/detransform paradigm in parsing,,
