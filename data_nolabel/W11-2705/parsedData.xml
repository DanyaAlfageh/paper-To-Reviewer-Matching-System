<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.466565">
b&apos;Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 3338,
Edinburgh, Scotland, UK, July 31, 2011. c
</bodyText>
<figure confidence="0.633085666666667">
2011 Association for Computational Linguistics
Exploring linguistically-rich patterns for question generation
Sergio Curto
L2
F/INESC-ID Lisbon
sslc@l2f.inesc-id.pt
Ana Cristina Mendes
L2
F/INESC-ID Lisbon
IST, Tech. Univ. Lisbon
acbm@l2f.inesc-id.pt
Lusa Coheur
</figure>
<page confidence="0.608356">
L2
</page>
<sectionHeader confidence="0.801168" genericHeader="abstract">
F/INESC-ID Lisbon
</sectionHeader>
<bodyText confidence="0.248411">
IST, Tech. Univ. Lisbon
lcoheur@inesc-id.pt
</bodyText>
<sectionHeader confidence="0.950462" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.9969654">
Linguistic patterns reflect the regularities of
Natural Language and their applicability is
acknowledged in several Natural Language
Processing tasks. Particularly, in the task of
Question Generation, many systems depend
on patterns to generate questions from text.
The approach we follow relies on patterns
that convey lexical, syntactic and semantic in-
formation, automatically learned from large-
scale corpora.
In this paper we discuss the impact of varying
several parameters during pattern learning and
matching in the Question Generation task. In
particular, we introduce semantics (by means
of named entities) in our lexico-syntactic pat-
terns. We evaluate and compare the number
and quality of the learned patterns and the
matched text segments. Also, we detail the
influence of the patterns in the generation of
natural language questions.
</bodyText>
<sectionHeader confidence="0.998177" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99028474">
Natural Language (NL) is known for its variability
and expressiveness. There are hundreds of ways to
express an idea, to describe a fact. But language also
comprises several regularities, or patterns, that de-
note the presence of certain information. For exam-
ple, Paris is located in France is a common way to
say that Paris is in France, indicated by the words
located in.
The use of patterns is a widely accepted as an ef-
fective approach in the field of Natural Language
Processing (NLP), in tasks like Question-Answering
(QA) (Soubbotin, 2001; Ravichandran and Hovy,
2002) or Question Generation (QG) (Wyse and Pi-
wek, 2009; Mendes et al., 2011).
Particularly, QG aims at generating questions
from text and has became a vibrant line of re-
search. Generating questions (and answers), on one
hand, allows QA or Dialogue Systems to be easily
ported to different domains, by quickly providing
new questions to train the systems. On the other
hand, it is useful for knowledge assessment-related
tasks, by reducing the amount of time allocated for
the creation of tests by teachers (a time consuming
and tedious task if done manually), or by allowing
the self evaluation of knowledge acquired by learn-
ers.
Most systems dedicated to QG are based on hand-
crafted rules and rely on pattern matching to gener-
ate questions. For example, in (Chen et al., 2009),
after the identification of key points, a situation
model is built and question templates are used to
generate questions. The Ceist system (Wyse and Pi-
wek, 2009) uses syntactic patterns and the Tregex
tool (Levy and Andrew, 2006) that receives a set
of hand-crafted rules and matches the rules against
parsed text, generating, in this way, questions (and
answers). Kalady et al.(2010) bases the QG task
in Up-keys (significant phrases in documents), parse
tree manipulation and named entity recognition.
Our approach to QG also relies on linguistic pat-
terns, defined as a sequence of symbols that convey
lexical, syntactic and semantic information, reflect-
ing and expressing a regularity of the language. The
patterns associate a question to its answer and are
automatically learned from a set of seeds, based on
large-scale information corpora, shallow parsing and
named entities recognition. The generation of ques-
tions uses the learned patterns, as questions are cre-
ated from text segments found in free text after being
matched against the patterns.
</bodyText>
<page confidence="0.999156">
33
</page>
<bodyText confidence="0.9974345">
\x0cThis paper studies the impact on QG of vary-
ing linguistic parameters during pattern learning and
matching. It is organized as follows: in Sec. 2
we introduce our pattern-based approach to QG; in
Sec. 3 we show the experiments and discuss results;
in Sec. 4 we conclude and point to future work.
</bodyText>
<sectionHeader confidence="0.857329" genericHeader="method">
2 Linguistically-Rich Patterns for
</sectionHeader>
<subsectionHeader confidence="0.556373">
Question Generation
</subsectionHeader>
<bodyText confidence="0.990351805555556">
The generation of questions involves two phases: a
first offline phase pattern learning where pat-
terns are learned from a set of seeds; and a sec-
ond online phase pattern matching and question
generation where the learned patterns are matched
against a target document and the questions are gen-
erated. Next we describe these phases.
Pattern Learning Our approach to pattern learn-
ing is inspired by the work of Ravichandran and
Hovy (2002), who propose a method to learn pat-
terns based on a two-step technique: the first ac-
quires patterns from the Web given a set of seeds and
the second validates the patterns. Despite the sim-
ilarities, ours and Ravichandran and Hovys work
have some differences: our patterns also contain
syntactic and semantic information and are not vali-
dated. Moreover, our seeds are well formulated NL
questions and their respective correct answers (in-
stead of two entities), which allows to directly take
advantage of the test sets already built and made
available in evaluation campaigns for QA systems
(like Text REtrieval Conference (TREC) or Cross
Language Evaluation Forum (CLEF)).
We use a set of seeds, each composed by a NL
question and its correct answer. We start by classi-
fying each seed question into a semantic category,
in order to discover the type of information these
are seeking after: for example, the question Who
painted the Birth of Venus ? asks for a persons
name. Afterwards, we extract the phrase nodes of
each seed question (excluding the Wh-phrase), en-
close each in double quotes and submit them as a
query to a search engine. For instance, given the
seed Who painted the Birth of Venus ?/Botticelli
and the syntactic structure of its question [WHNP
Who] [VBD painted] [NP the Birth of Venus]1, we
</bodyText>
<page confidence="0.865504">
1
</page>
<bodyText confidence="0.996992133333333">
The Penn Treebank II Tags (Bies et al., 1995) are used.
build the query: &amp;quot;painted&amp;quot; &amp;quot;the Birth of
Venus&amp;quot; &amp;quot;Botticelli&amp;quot;.
We build patterns that associate the entities
in the question to the answer from the top re-
trieved documents. From the sentence The Birth
of Venus was painted around 1486 by Botti-
celli, retrieved as result to the above query, we
learn the pattern NP VBD[was] VBN PP[around
1486]:[Date] IN:[by] NP{ANSWER}2. The
syntactic labels without lexical information are re-
lated with the constituents of the question, while
those with {ANSWER} mark the answer.
By creating queries with the inflected forms of the
main verb of the question, we learn patterns where
the surface form of the verb is different to that of
the verb in the seed question (e.g., NP{ANSWER}
VBD[began] VBG NP is learned from the sen-
tence Botticelli began painting the Birth of Venus).
The patterns generated by verb inflection are IN-
FLECTED; the others are STRONG patterns.
Our patterns convey linguistic information ex-
tracted from the sentences in the documents where
all the constituents of the query exist. The pat-
tern is built with the words, their syntactic and se-
mantic classes, that constitute the segments where
those constituents are found. For that, we per-
form syntactic analysis and named entity recog-
nition in each sentence. In this paper, we ad-
dress the impact of adding semantic information
to the patterns, that is, the difference in hav-
ing a pattern NP VBD[was] VBN PP[around
1486]:[Date] IN:[by] NP{ANSWER} with or
without the named entity of type DATE, for instance.
Pattern Matching and Question Generation
The match of the patterns against a given free text
is done at the lexical, syntactic and semantic lev-
els. We have implemented a (recursive) algorithm
that explores the parsed tree of the text sentences in
a top-down, left-to-right, depth-first search, unifying
the text with the linguistic information in the pattern.
Also, we discard all matched segments in which
the answer does not agree with the semantic cate-
gory expected by the question.
The generation of questions from the matched text
</bodyText>
<page confidence="0.934103">
2
</page>
<bodyText confidence="0.999157333333333">
The patterns are more complex than the ones presented:
they are linked to the seed question by indexes, mapping the po-
sition of each of its components into the question constituents.
</bodyText>
<page confidence="0.993005">
34
</page>
<bodyText confidence="0.997009">
\x0csegments is straightforward, since we keep track of
the syntactic structure of the questions and the sen-
tences on the origin of the patterns. There is a di-
rect unification of all components of the text seg-
ment with the constituents of the pattern. In the
INFLECTED patterns, the verb is inflected with the
tense and person of the seed question and the auxil-
iary verb is also used.
</bodyText>
<sectionHeader confidence="0.998323" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.983199">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.963410125">
We used the 6 seeds shown in Table 1, chosen be-
cause the questions contain regular verbs and they
focus on known entities being so, it is probable
that there will be several texts in the Web referring to
them. However, understanding the characteristics of
a pair that makes it a good seed is an important and
pertinent question and a direction for future work.
GId: 1
</bodyText>
<table confidence="0.993378909090909">
Syntactic Structure: WHNP VBD NP
Semantic Category: HUMAN:INDIVIDUAL
Who wrote Hamlet?/Shakespeare
Who painted Guernica?/Picasso
Who painted The Starry Night?/Van Gogh
GId: 2
Syntactic Structure: WHADVP VBD NP VBN
Semantic Category: NUMERIC:DATE
When was Hamlet written?/1601
When was Guernica painted?/1937
When was The Starry Night painted?/1889
</table>
<tableCaption confidence="0.998665">
Table 1: Seeds used in the experiments.
</tableCaption>
<bodyText confidence="0.99637016">
The syntactic analysis of the questions was done
by the Berkeley Parser (Petrov and Klein, 2007)
trained on the QuestionBank (Judge et al., 2006).
For question classification, we used Li and Roth
(2002) taxonomy and a machine learning-based
classifier fed with features derived from a rule-based
classifier (Silva et al., 2011).
For the learning of patterns we used the top
64 documents retrieved by Google and to recog-
nize the named entities in the pattern we apply
several strategies, namely: 1) the Stanfords Con-
ditional Random-Field-based named entity recog-
nizer (Finkel et al., 2005) to detect entities of type
HUMAN; 2) regular expressions to detect NUMERIC
and DATE type entities; 3) gazetteers to detect enti-
ties of type LOCATION.
For the generation of questions we used the top 16
documents retrieved by the Google for 9 personali-
ties from several domains, like literature (e.g., Jane
Austen) and politics (e.g., Adolf Hitler). We do not
have influence on the content of the retrieved doc-
uments, nor perform any pre-processing (like text
simplification or anaphora resolution). The Berkeley
Parser (Petrov and Klein, 2007) was used to parse
the sentences, trained with the Wall Street Journal.
</bodyText>
<subsectionHeader confidence="0.999939">
3.2 Pattern Learning Results
</subsectionHeader>
<bodyText confidence="0.9619795">
A total of 272 patterns was learned, from which 212
are INFLECTED and the remaining are STRONG. On
average, each seed led to 46 patterns.
Table 2 shows the number of learned patterns of
types INFLECTED and STRONG according to each
group of seed questions. It indicates the number of
patterns in which at least one named entity was rec-
ognized (W) and the number of patterns which do not
contain any named entity (WO). Three main results
of the pattern learning phase are shown: 1) the num-
ber of learned INFLECTED patterns is much higher
than the number of learned STRONG patterns: nearly
80% of the patterns are INFLECTED; 2) most of the
patterns do not have named entities; and 3) the num-
ber of patterns learned from the questions of group
1 are nearly 70% of the total number of patterns.
</bodyText>
<table confidence="0.9447225">
INFLECTED STRONG
GId WO W WO W TOTAL
1 127 19 36 8
146 44 190
2 40 26 10 6
66 16 82
All 167 45 46 14
212 60 272
</table>
<tableCaption confidence="0.997359">
Table 2: Number of learned patterns.
</tableCaption>
<bodyText confidence="0.9842075">
The following are examples of patterns and the
actual sentences from where they were learned:
NP{ANSWER} VBZ NP: an INFLECTED pattern
learned from group 1, from the sentence 1601
</bodyText>
<figure confidence="0.552642090909091">
William Shakespeare writes Hamlet in London.,
without named entities;
NP VBD VBN IN[in] NP{ANSWER}: a
35
\x0cSTRONG pattern learned from group 2, from the
sentence (Guernica was painted in 1937.), without
named entities;
NNP VBZ[is] NP[a tragedy] ,[,]
VBN[believed] VBN IN[between]
NP[1599]:[NUMERIC COUNT,NUMERIC DATE]
CC[and] NP{ANSWER}: an INFLECTED pattern
</figure>
<bodyText confidence="0.964411">
learned from group 2, from the sentence William
Shakespeares Hamlet is a tragedy , believed written
between 1599 and 1601, with 1599 being recog-
nized as named entity of type NUMERIC COUNT
and NUMERIC DATE.
</bodyText>
<subsectionHeader confidence="0.818727">
3.3 Pattern Matching and Question
Generation Results
</subsectionHeader>
<bodyText confidence="0.877612">
Regarding the number of text segments matched in
the texts retrieved for the 9 personalities, Table 3
shows that, from the 272 learned patterns, only 30
(11%) were in fact effective (an effective pattern
matches at least one text segment). The most effec-
tive patterns were those from group 2, as 12 from 82
</bodyText>
<table confidence="0.826513">
(14.6%) matched at least one instance in the text.
GId INFLECTED STRONG TOTAL
1 13 5 18
2 9 3 (2 W) 12
All 22 8 30
</table>
<tableCaption confidence="0.999362">
Table 3: Matched patterns.
</tableCaption>
<bodyText confidence="0.988836275862069">
Regarding the patterns with named entities, only
those from group 2 matched instances in the texts.
The pattern that matched the most instances was
NP{ANSWER} VBD NP, learned from group 1.
In the evaluation of the questions, we use the
guidelines of Chen et al. (2009), who classify ques-
tions as plausible if they are grammatically correct
and if they make sense regarding the text from where
they were extracted and implausible (otherwise).
However, we split plausible questions in three cat-
egories: 1) Pa for plausible, anaphoric questions,
e.g., When was she awarded the Nobel Peace Prize?;
2) Pc for plausible questions that need a context to
be answered, e.g., When was the manuscript pub-
lished?; and 3) Pp, a plausible perfect question. If
a question can be marked both as PLa and PLc, we
mark it as PLa. Also, we split implausible questions
in: 1) IPi: for implausible questions due to incom-
pleteness, e.g., When was Bob Marley invited?; and
2) IP: for questions that make no sense, e.g., When
was December 1926 Agatha identified?.
A total of 447 questions was generated: 31 by
STRONG patterns, 269 by INFLECTED patterns and
147 by both STRONG and INFLECTED patterns. We
manually evaluated 100 questions, randomly se-
lected. Results are in Table 4, shown according
to the type (INFLECTED/STRONG) and presence of
named entities (W/WO) in the pattern that generated
them.
</bodyText>
<table confidence="0.998272">
Pa Pc Pp IPi IP Total
INFLECTED 57
WO 2 0 27 23 5
STRONG 13
W 1 0 1 0 1
WO 1 2 3 3 1
INFL/STR 30
WO 0 0 9 18 3
All 4 2 40 44 10 100
</table>
<tableCaption confidence="0.999455">
Table 4: Evaluation of the generated questions.
</tableCaption>
<bodyText confidence="0.991522916666667">
46 of the evaluated questions were considered
plausible and, from these, 40 can be used without
modifications. From the 54 implausible questions,
44 were due to lack of information in the question.
69% (9 in 13) of the questions originated in STRONG
patterns were plausible. This value is smaller for
questions generated by INFLECTED patterns: 50.8%
(29 in 57). Questions that had in their origin both a
STRONG and a INFLECTED pattern were mostly im-
plausible, only 9 in 30 were plausible (30%). The
presence of named entities led to an increase of
questions of only 3 (2 plausible and 1 implausible).
</bodyText>
<subsectionHeader confidence="0.978139">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.9956792">
The results concerning the transition from lexico-
syntactic to lexico-syntactic-semantic patterns were
not conclusive. There were 59 patterns with named
entities, but only 2 matched new text segments.
Only 3 questions were generated from patterns
with semantics. We think that this happened due to
two reasons: 1) not all of the named entities in the
patterns were detected; and 2) the patterns contained
lexical information that did not allow a match with
the text (e.g., NP{ANSWER} VBD[responded]
</bodyText>
<page confidence="0.93746">
36
</page>
<bodyText confidence="0.983809181818182">
\x0cPP[in 1937]:textit[Date] WHADVP[when]
NP[he] VBD NP requires the words responded,
when and he.)
From a small set of seeds, our approach learned
patterns that were later used to generate 447 ques-
tions from previously unseen text. In a sample of
100 questions, 46% were judged as plausible. Two
plausible questions are: Who had no real interest
in the former German African colonies?, When
was The Road to Resurgence published? and Who
launched a massive naval and land campaign de-
signed to seize New York?.
The presence of syntactic information (a differ-
ence between ours and Ravichandran and Hovys
work) allows to relax the patterns and to gener-
ate questions of various topics: e.g., the questions
Who invented the telegraph? and Who di-
rected the Titanic? can be generated from match-
ing the pattern NP VBD[was] VBN IN:[by]
NP{ANSWER} with the sentences The telegraph was
invented by Samuel Morse and The Titanic was di-
rected by James Cameron, respectively.
</bodyText>
<sectionHeader confidence="0.998236" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999915666666666">
We presented an approach to generating questions
based on linguistic patterns, automatically learned
from the Web from a set of seeds. We addressed the
impact of adding semantics to patterns in matching
text segments and generating new NL questions.
We did not detect any improvement when adding
semantics to the patterns, mostly because the pat-
terns with named entities did not match too many
text segments. Nevertheless, from a small set of 6
seeds, we generated 447 NL questions. From these,
we evaluated 100 and 46% were considered correct
at the lexical, syntactic and semantic levels.
In the future, we intend to pre-process the texts
against which the patterns are matched and from
which the questions are generated. Also, we are
experimenting this approach in another language.
We aim at using more complex questions as seeds,
studying its influence on the generation of questions.
</bodyText>
<sectionHeader confidence="0.943993" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.985742857142857">
This work was supported by FCT (INESC-ID mul-
tiannual funding) through the PIDDAC Program
funds, and also through the project FALACOMIGO
(ProjectoVII em co-promocao, QREN n 13449).
Ana Cristina Mendes is supported by a PhD fel-
lowship from Fundacao para a Ciencia e a Tecnolo-
gia (SFRH/BD/43487/2008).
</bodyText>
<sectionHeader confidence="0.981336" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999003307692307">
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
intyre. 1995. Bracketing Guidelines for Treebank II
Style Penn Treebank Project.
Wei Chen, Gregory Aist, , and Jack Mostow. 2009. Gen-
erating questions automatically from informational
text. In The 2nd Workshop on Question Generation.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proc. 43rd
Annual Meeting on Association
for Computational Linguistics, ACL 05, pages 363
370. ACL.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: creating a corpus of parse-annotated
questions. In ACL-44: Proc. 21st
Int. Conf. on Com-
putational Linguistics and the 44th
Annual Meeting of
the Association for Computational Linguistics, pages
497504. ACL.
Saidalavi Kalady, Ajeesh Elikkottil, and Rajarshi Das.
2010. Natural language question generation using
syntax and keywords. In The 3rd
Workshop on Ques-
tion Generation.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In LREC 2006.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proc. 19th
Int. Conf. on Computational Lin-
guistics, pages 17. ACL.
Ana Cristina Mendes, Sergio Curto, and Lusa Coheur.
2011. Bootstrapping multiple-choice tests with the-
mentor. In CICLing, 12th
International Conference
on Intelligent Text Processing and Computational Lin-
guistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proc. Main Conference, pages 404411. ACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering sys-
tem. In ACL 02: Proc. 40th
Annual Meeting on As-
sociation for Computational Linguistics, pages 4147.
ACL.
Joao Silva, Lusa Coheur, Ana Mendes, and Andreas
Wichert. 2011. From symbolic to sub-symbolic in-
</reference>
<page confidence="0.978869">
37
</page>
<reference confidence="0.977934444444445">
\x0cformation in question classification. Artificial Intelli-
gence Review, 35:137154.
M. M. Soubbotin. 2001. Patterns of potential answer
expressions as clues to the right answers. In Proc. 10th
Text REtrieval Conference (TREC), pages 293302.
Brendan Wyse and Paul Piwek. 2009. Generating ques-
tions from openlearn study units. In The 2nd
Workshop
on Question Generation.
</reference>
<page confidence="0.93076">
38
</page>
<figure confidence="0.291427">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.019531">
<note confidence="0.976725">b&apos;Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 3338, Edinburgh, Scotland, UK, July 31, 2011. c 2011 Association for Computational Linguistics</note>
<title confidence="0.965871">Exploring linguistically-rich patterns for question generation</title>
<author confidence="0.99617">Sergio Curto</author>
<pubnum confidence="0.395043">L2</pubnum>
<affiliation confidence="0.815254">F/INESC-ID Lisbon</affiliation>
<email confidence="0.735097">sslc@l2f.inesc-id.pt</email>
<author confidence="0.858378">Ana Cristina Mendes</author>
<email confidence="0.402819">L2</email>
<affiliation confidence="0.935975">F/INESC-ID Lisbon IST, Tech. Univ. Lisbon</affiliation>
<email confidence="0.765842">acbm@l2f.inesc-id.pt</email>
<author confidence="0.83594">Lusa Coheur</author>
<pubnum confidence="0.399226">L2</pubnum>
<affiliation confidence="0.878287">F/INESC-ID Lisbon IST, Tech. Univ. Lisbon</affiliation>
<email confidence="0.987302">lcoheur@inesc-id.pt</email>
<abstract confidence="0.999654">Linguistic patterns reflect the regularities of Natural Language and their applicability is acknowledged in several Natural Language Processing tasks. Particularly, in the task of Question Generation, many systems depend on patterns to generate questions from text. The approach we follow relies on patterns that convey lexical, syntactic and semantic information, automatically learned from largescale corpora. In this paper we discuss the impact of varying several parameters during pattern learning and matching in the Question Generation task. In particular, we introduce semantics (by means of named entities) in our lexico-syntactic patterns. We evaluate and compare the number and quality of the learned patterns and the matched text segments. Also, we detail the influence of the patterns in the generation of natural language questions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Robert Macintyre</author>
</authors>
<title>Bracketing Guidelines for Treebank II Style Penn Treebank Project.</title>
<date>1995</date>
<contexts>
<context position="5884" citStr="Bies et al., 1995" startWordPosition="937" endWordPosition="940">ct answer. We start by classifying each seed question into a semantic category, in order to discover the type of information these are seeking after: for example, the question Who painted the Birth of Venus ? asks for a persons name. Afterwards, we extract the phrase nodes of each seed question (excluding the Wh-phrase), enclose each in double quotes and submit them as a query to a search engine. For instance, given the seed Who painted the Birth of Venus ?/Botticelli and the syntactic structure of its question [WHNP Who] [VBD painted] [NP the Birth of Venus]1, we 1 The Penn Treebank II Tags (Bies et al., 1995) are used. build the query: &amp;quot;painted&amp;quot; &amp;quot;the Birth of Venus&amp;quot; &amp;quot;Botticelli&amp;quot;. We build patterns that associate the entities in the question to the answer from the top retrieved documents. From the sentence The Birth of Venus was painted around 1486 by Botticelli, retrieved as result to the above query, we learn the pattern NP VBD[was] VBN PP[around 1486]:[Date] IN:[by] NP{ANSWER}2. The syntactic labels without lexical information are related with the constituents of the question, while those with {ANSWER} mark the answer. By creating queries with the inflected forms of the main verb of the question</context>
</contexts>
<marker>Bies, Ferguson, Katz, Macintyre, 1995</marker>
<rawString>Ann Bies, Mark Ferguson, Karen Katz, and Robert Macintyre. 1995. Bracketing Guidelines for Treebank II Style Penn Treebank Project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Chen</author>
<author>Gregory Aist</author>
</authors>
<title>Generating questions automatically from informational text.</title>
<date>2009</date>
<booktitle>In The 2nd Workshop on Question Generation.</booktitle>
<marker>Chen, Aist, 2009</marker>
<rawString>Wei Chen, Gregory Aist, , and Jack Mostow. 2009. Generating questions automatically from informational text. In The 2nd Workshop on Question Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proc. 43rd Annual Meeting on Association for Computational Linguistics, ACL 05,</booktitle>
<pages>363--370</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="9895" citStr="Finkel et al., 2005" startWordPosition="1595" endWordPosition="1598"> Seeds used in the experiments. The syntactic analysis of the questions was done by the Berkeley Parser (Petrov and Klein, 2007) trained on the QuestionBank (Judge et al., 2006). For question classification, we used Li and Roth (2002) taxonomy and a machine learning-based classifier fed with features derived from a rule-based classifier (Silva et al., 2011). For the learning of patterns we used the top 64 documents retrieved by Google and to recognize the named entities in the pattern we apply several strategies, namely: 1) the Stanfords Conditional Random-Field-based named entity recognizer (Finkel et al., 2005) to detect entities of type HUMAN; 2) regular expressions to detect NUMERIC and DATE type entities; 3) gazetteers to detect entities of type LOCATION. For the generation of questions we used the top 16 documents retrieved by the Google for 9 personalities from several domains, like literature (e.g., Jane Austen) and politics (e.g., Adolf Hitler). We do not have influence on the content of the retrieved documents, nor perform any pre-processing (like text simplification or anaphora resolution). The Berkeley Parser (Petrov and Klein, 2007) was used to parse the sentences, trained with the Wall S</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proc. 43rd Annual Meeting on Association for Computational Linguistics, ACL 05, pages 363 370. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Judge</author>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Questionbank: creating a corpus of parse-annotated questions.</title>
<date>2006</date>
<booktitle>In ACL-44: Proc. 21st Int. Conf. on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>497504</pages>
<publisher>ACL.</publisher>
<marker>Judge, Cahill, van Genabith, 2006</marker>
<rawString>John Judge, Aoife Cahill, and Josef van Genabith. 2006. Questionbank: creating a corpus of parse-annotated questions. In ACL-44: Proc. 21st Int. Conf. on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 497504. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saidalavi Kalady</author>
<author>Ajeesh Elikkottil</author>
<author>Rajarshi Das</author>
</authors>
<title>Natural language question generation using syntax and keywords.</title>
<date>2010</date>
<booktitle>In The 3rd Workshop on Question Generation.</booktitle>
<marker>Kalady, Elikkottil, Das, 2010</marker>
<rawString>Saidalavi Kalady, Ajeesh Elikkottil, and Rajarshi Das. 2010. Natural language question generation using syntax and keywords. In The 3rd Workshop on Question Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Galen Andrew</author>
</authors>
<title>Tregex and tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>In LREC</booktitle>
<contexts>
<context position="2908" citStr="Levy and Andrew, 2006" startWordPosition="445" endWordPosition="448"> for knowledge assessment-related tasks, by reducing the amount of time allocated for the creation of tests by teachers (a time consuming and tedious task if done manually), or by allowing the self evaluation of knowledge acquired by learners. Most systems dedicated to QG are based on handcrafted rules and rely on pattern matching to generate questions. For example, in (Chen et al., 2009), after the identification of key points, a situation model is built and question templates are used to generate questions. The Ceist system (Wyse and Piwek, 2009) uses syntactic patterns and the Tregex tool (Levy and Andrew, 2006) that receives a set of hand-crafted rules and matches the rules against parsed text, generating, in this way, questions (and answers). Kalady et al.(2010) bases the QG task in Up-keys (significant phrases in documents), parse tree manipulation and named entity recognition. Our approach to QG also relies on linguistic patterns, defined as a sequence of symbols that convey lexical, syntactic and semantic information, reflecting and expressing a regularity of the language. The patterns associate a question to its answer and are automatically learned from a set of seeds, based on large-scale info</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>Roger Levy and Galen Andrew. 2006. Tregex and tsurgeon: tools for querying and manipulating tree data structures. In LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In Proc. 19th Int. Conf. on Computational Linguistics,</booktitle>
<pages>17</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="9509" citStr="Li and Roth (2002)" startWordPosition="1534" endWordPosition="1537">ion for future work. GId: 1 Syntactic Structure: WHNP VBD NP Semantic Category: HUMAN:INDIVIDUAL Who wrote Hamlet?/Shakespeare Who painted Guernica?/Picasso Who painted The Starry Night?/Van Gogh GId: 2 Syntactic Structure: WHADVP VBD NP VBN Semantic Category: NUMERIC:DATE When was Hamlet written?/1601 When was Guernica painted?/1937 When was The Starry Night painted?/1889 Table 1: Seeds used in the experiments. The syntactic analysis of the questions was done by the Berkeley Parser (Petrov and Klein, 2007) trained on the QuestionBank (Judge et al., 2006). For question classification, we used Li and Roth (2002) taxonomy and a machine learning-based classifier fed with features derived from a rule-based classifier (Silva et al., 2011). For the learning of patterns we used the top 64 documents retrieved by Google and to recognize the named entities in the pattern we apply several strategies, namely: 1) the Stanfords Conditional Random-Field-based named entity recognizer (Finkel et al., 2005) to detect entities of type HUMAN; 2) regular expressions to detect NUMERIC and DATE type entities; 3) gazetteers to detect entities of type LOCATION. For the generation of questions we used the top 16 documents re</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>Xin Li and Dan Roth. 2002. Learning question classifiers. In Proc. 19th Int. Conf. on Computational Linguistics, pages 17. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana Cristina Mendes</author>
<author>Sergio Curto</author>
<author>Lusa Coheur</author>
</authors>
<title>Bootstrapping multiple-choice tests with thementor.</title>
<date>2011</date>
<booktitle>In CICLing, 12th International Conference on Intelligent Text Processing and Computational Linguistics.</booktitle>
<contexts>
<context position="1976" citStr="Mendes et al., 2011" startWordPosition="289" endWordPosition="292">nown for its variability and expressiveness. There are hundreds of ways to express an idea, to describe a fact. But language also comprises several regularities, or patterns, that denote the presence of certain information. For example, Paris is located in France is a common way to say that Paris is in France, indicated by the words located in. The use of patterns is a widely accepted as an effective approach in the field of Natural Language Processing (NLP), in tasks like Question-Answering (QA) (Soubbotin, 2001; Ravichandran and Hovy, 2002) or Question Generation (QG) (Wyse and Piwek, 2009; Mendes et al., 2011). Particularly, QG aims at generating questions from text and has became a vibrant line of research. Generating questions (and answers), on one hand, allows QA or Dialogue Systems to be easily ported to different domains, by quickly providing new questions to train the systems. On the other hand, it is useful for knowledge assessment-related tasks, by reducing the amount of time allocated for the creation of tests by teachers (a time consuming and tedious task if done manually), or by allowing the self evaluation of knowledge acquired by learners. Most systems dedicated to QG are based on hand</context>
</contexts>
<marker>Mendes, Curto, Coheur, 2011</marker>
<rawString>Ana Cristina Mendes, Sergio Curto, and Lusa Coheur. 2011. Bootstrapping multiple-choice tests with thementor. In CICLing, 12th International Conference on Intelligent Text Processing and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proc. Main Conference,</booktitle>
<pages>404411</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="9403" citStr="Petrov and Klein, 2007" startWordPosition="1517" endWordPosition="1520">ing the characteristics of a pair that makes it a good seed is an important and pertinent question and a direction for future work. GId: 1 Syntactic Structure: WHNP VBD NP Semantic Category: HUMAN:INDIVIDUAL Who wrote Hamlet?/Shakespeare Who painted Guernica?/Picasso Who painted The Starry Night?/Van Gogh GId: 2 Syntactic Structure: WHADVP VBD NP VBN Semantic Category: NUMERIC:DATE When was Hamlet written?/1601 When was Guernica painted?/1937 When was The Starry Night painted?/1889 Table 1: Seeds used in the experiments. The syntactic analysis of the questions was done by the Berkeley Parser (Petrov and Klein, 2007) trained on the QuestionBank (Judge et al., 2006). For question classification, we used Li and Roth (2002) taxonomy and a machine learning-based classifier fed with features derived from a rule-based classifier (Silva et al., 2011). For the learning of patterns we used the top 64 documents retrieved by Google and to recognize the named entities in the pattern we apply several strategies, namely: 1) the Stanfords Conditional Random-Field-based named entity recognizer (Finkel et al., 2005) to detect entities of type HUMAN; 2) regular expressions to detect NUMERIC and DATE type entities; 3) gazet</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proc. Main Conference, pages 404411. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In ACL 02: Proc. 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>4147</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1904" citStr="Ravichandran and Hovy, 2002" startWordPosition="276" endWordPosition="279">eration of natural language questions. 1 Introduction Natural Language (NL) is known for its variability and expressiveness. There are hundreds of ways to express an idea, to describe a fact. But language also comprises several regularities, or patterns, that denote the presence of certain information. For example, Paris is located in France is a common way to say that Paris is in France, indicated by the words located in. The use of patterns is a widely accepted as an effective approach in the field of Natural Language Processing (NLP), in tasks like Question-Answering (QA) (Soubbotin, 2001; Ravichandran and Hovy, 2002) or Question Generation (QG) (Wyse and Piwek, 2009; Mendes et al., 2011). Particularly, QG aims at generating questions from text and has became a vibrant line of research. Generating questions (and answers), on one hand, allows QA or Dialogue Systems to be easily ported to different domains, by quickly providing new questions to train the systems. On the other hand, it is useful for knowledge assessment-related tasks, by reducing the amount of time allocated for the creation of tests by teachers (a time consuming and tedious task if done manually), or by allowing the self evaluation of knowle</context>
<context position="4527" citStr="Ravichandran and Hovy (2002)" startWordPosition="705" endWordPosition="708">ur pattern-based approach to QG; in Sec. 3 we show the experiments and discuss results; in Sec. 4 we conclude and point to future work. 2 Linguistically-Rich Patterns for Question Generation The generation of questions involves two phases: a first offline phase pattern learning where patterns are learned from a set of seeds; and a second online phase pattern matching and question generation where the learned patterns are matched against a target document and the questions are generated. Next we describe these phases. Pattern Learning Our approach to pattern learning is inspired by the work of Ravichandran and Hovy (2002), who propose a method to learn patterns based on a two-step technique: the first acquires patterns from the Web given a set of seeds and the second validates the patterns. Despite the similarities, ours and Ravichandran and Hovys work have some differences: our patterns also contain syntactic and semantic information and are not validated. Moreover, our seeds are well formulated NL questions and their respective correct answers (instead of two entities), which allows to directly take advantage of the test sets already built and made available in evaluation campaigns for QA systems (like Text </context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In ACL 02: Proc. 40th Annual Meeting on Association for Computational Linguistics, pages 4147. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joao Silva</author>
<author>Lusa Coheur</author>
<author>Ana Mendes</author>
<author>Andreas Wichert</author>
</authors>
<title>From symbolic to sub-symbolic in\x0cformation in question classification.</title>
<date>2011</date>
<journal>Artificial Intelligence Review,</journal>
<pages>35--137154</pages>
<contexts>
<context position="9634" citStr="Silva et al., 2011" startWordPosition="1552" endWordPosition="1555">re Who painted Guernica?/Picasso Who painted The Starry Night?/Van Gogh GId: 2 Syntactic Structure: WHADVP VBD NP VBN Semantic Category: NUMERIC:DATE When was Hamlet written?/1601 When was Guernica painted?/1937 When was The Starry Night painted?/1889 Table 1: Seeds used in the experiments. The syntactic analysis of the questions was done by the Berkeley Parser (Petrov and Klein, 2007) trained on the QuestionBank (Judge et al., 2006). For question classification, we used Li and Roth (2002) taxonomy and a machine learning-based classifier fed with features derived from a rule-based classifier (Silva et al., 2011). For the learning of patterns we used the top 64 documents retrieved by Google and to recognize the named entities in the pattern we apply several strategies, namely: 1) the Stanfords Conditional Random-Field-based named entity recognizer (Finkel et al., 2005) to detect entities of type HUMAN; 2) regular expressions to detect NUMERIC and DATE type entities; 3) gazetteers to detect entities of type LOCATION. For the generation of questions we used the top 16 documents retrieved by the Google for 9 personalities from several domains, like literature (e.g., Jane Austen) and politics (e.g., Adolf</context>
</contexts>
<marker>Silva, Coheur, Mendes, Wichert, 2011</marker>
<rawString>Joao Silva, Lusa Coheur, Ana Mendes, and Andreas Wichert. 2011. From symbolic to sub-symbolic in\x0cformation in question classification. Artificial Intelligence Review, 35:137154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Soubbotin</author>
</authors>
<title>Patterns of potential answer expressions as clues to the right answers.</title>
<date>2001</date>
<booktitle>In Proc. 10th Text REtrieval Conference (TREC),</booktitle>
<pages>293302</pages>
<contexts>
<context position="1874" citStr="Soubbotin, 2001" startWordPosition="274" endWordPosition="275">tterns in the generation of natural language questions. 1 Introduction Natural Language (NL) is known for its variability and expressiveness. There are hundreds of ways to express an idea, to describe a fact. But language also comprises several regularities, or patterns, that denote the presence of certain information. For example, Paris is located in France is a common way to say that Paris is in France, indicated by the words located in. The use of patterns is a widely accepted as an effective approach in the field of Natural Language Processing (NLP), in tasks like Question-Answering (QA) (Soubbotin, 2001; Ravichandran and Hovy, 2002) or Question Generation (QG) (Wyse and Piwek, 2009; Mendes et al., 2011). Particularly, QG aims at generating questions from text and has became a vibrant line of research. Generating questions (and answers), on one hand, allows QA or Dialogue Systems to be easily ported to different domains, by quickly providing new questions to train the systems. On the other hand, it is useful for knowledge assessment-related tasks, by reducing the amount of time allocated for the creation of tests by teachers (a time consuming and tedious task if done manually), or by allowing</context>
</contexts>
<marker>Soubbotin, 2001</marker>
<rawString>M. M. Soubbotin. 2001. Patterns of potential answer expressions as clues to the right answers. In Proc. 10th Text REtrieval Conference (TREC), pages 293302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan Wyse</author>
<author>Paul Piwek</author>
</authors>
<title>Generating questions from openlearn study units.</title>
<date>2009</date>
<booktitle>In The 2nd Workshop on Question Generation.</booktitle>
<contexts>
<context position="1954" citStr="Wyse and Piwek, 2009" startWordPosition="284" endWordPosition="288">ral Language (NL) is known for its variability and expressiveness. There are hundreds of ways to express an idea, to describe a fact. But language also comprises several regularities, or patterns, that denote the presence of certain information. For example, Paris is located in France is a common way to say that Paris is in France, indicated by the words located in. The use of patterns is a widely accepted as an effective approach in the field of Natural Language Processing (NLP), in tasks like Question-Answering (QA) (Soubbotin, 2001; Ravichandran and Hovy, 2002) or Question Generation (QG) (Wyse and Piwek, 2009; Mendes et al., 2011). Particularly, QG aims at generating questions from text and has became a vibrant line of research. Generating questions (and answers), on one hand, allows QA or Dialogue Systems to be easily ported to different domains, by quickly providing new questions to train the systems. On the other hand, it is useful for knowledge assessment-related tasks, by reducing the amount of time allocated for the creation of tests by teachers (a time consuming and tedious task if done manually), or by allowing the self evaluation of knowledge acquired by learners. Most systems dedicated t</context>
</contexts>
<marker>Wyse, Piwek, 2009</marker>
<rawString>Brendan Wyse and Paul Piwek. 2009. Generating questions from openlearn study units. In The 2nd Workshop on Question Generation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>