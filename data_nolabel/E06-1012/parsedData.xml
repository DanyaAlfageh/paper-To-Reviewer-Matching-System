<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.889642">
b&amp;quot;Statistical Dependency Parsing of Turkish
</title>
<author confidence="0.95781">
Gulsen Eryigit
</author>
<affiliation confidence="0.9912445">
Department of Computer Engineering
Istanbul Technical University
</affiliation>
<address confidence="0.880993">
Istanbul, 34469, Turkey
</address>
<email confidence="0.979224">
gulsen@cs.itu.edu.tr
</email>
<author confidence="0.808122">
Kemal Oflazer
</author>
<affiliation confidence="0.969513">
Faculty of Engineering and Natural Sciences
Sabanci University
</affiliation>
<address confidence="0.878926">
Istanbul, 34956, Turkey
</address>
<email confidence="0.996011">
oflazer@sabanciuniv.edu
</email>
<sectionHeader confidence="0.990801" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999821272727273">
This paper presents results from the first
statistical dependency parser for Turkish.
Turkish is a free-constituent order lan-
guage with complex agglutinative inflec-
tional and derivational morphology and
presents interesting challenges for statisti-
cal parsing, as in general, dependency re-
lations are between portions of words
called inflectional groups. We have
explored statistical models that use dif-
ferent representational units for parsing.
We have used the Turkish Dependency
Treebank to train and test our parser
but have limited this initial exploration
to that subset of the treebank sentences
with only left-to-right non-crossing depen-
dency links. Our results indicate that the
best accuracy in terms of the dependency
relations between inflectional groups is
obtained when we use inflectional groups
as units in parsing, and when contexts
around the dependent are employed.
</bodyText>
<sectionHeader confidence="0.998332" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998096266666667">
The availability of treebanks of various sorts have
fostered the development of statistical parsers
trained with the structural data in these tree-
banks. With the emergence of the important role
of word-to-word relations in parsing (Charniak,
2000; Collins, 1996), dependency grammars have
gained a certain popularity; e.g., Yamada and Mat-
sumoto (2003) for English, Kudo and Matsumoto
(2000; 2002), Sekine et al. (2000) for Japanese,
Chung and Rim (2004) for Korean, Nivre et al.
(2004) for Swedish, Nivre and Nilsson (2005) for
Czech, among others.
Dependency grammars represent the structure
of the sentences by positing binary dependency
relations between words. For instance, Figure 1
</bodyText>
<figureCaption confidence="0.956974">
Figure 1: Dependency Relations for a Turkish and
</figureCaption>
<bodyText confidence="0.997429214285714">
an English sentence
shows the dependency graph of a Turkish and
an English sentence where dependency labels are
shown annotating the arcs which extend from de-
pendents to heads.
Parsers employing CFG-backbones have been
found to be less effective for free-constituent-
order languages where constituents can easily
change their position in the sentence without
modifying the general meaning of the sentence.
Collins et al. (1999) applied the parser of Collins
(1997) developed for English, to Czech, and found
that the performance was substantially lower when
compared to the results for English.
</bodyText>
<sectionHeader confidence="0.997068" genericHeader="introduction">
2 Turkish
</sectionHeader>
<bodyText confidence="0.998854294117647">
Turkish is an agglutinative language where a se-
quence of inflectional and derivational morphemes
get affixed to a root (Oflazer, 1994). At the syntax
level, the unmarked constituent order is SOV, but
constituent order may vary freely as demanded by
the discourse context. Essentially all constituent
orders are possible, especially at the main sen-
tence level, with very minimal formal constraints.
In written text however, the unmarked order is
dominant at both the main sentence and embedded
clause level.
Turkish morphotactics is quite complicated: a
given word form may involve multiple derivations
and the number of word forms one can generate
from a nominal or verbal root is theoretically in-
finite. Derivations in Turkish are very produc-
tive, and the syntactic relations that a word is in-
</bodyText>
<page confidence="0.995622">
89
</page>
<bodyText confidence="0.976818230769231">
\x0cvolved in as a dependent or head element, are de-
termined by the inflectional properties of the one
or more (possibly intermediate) derived forms. In
this work, we assume that a Turkish word is rep-
resented as a sequence of inflectional groups (IGs
hereafter), separated by DBs, denoting derivation
boundaries, in the following general form:
root+IG1 + DB+IG2 + DB+ + DB+IGn.
Here each IGi denotes relevant inflectional fea-
tures including the part-of-speech for the root and
for any of the derived forms. For instance, the de-
rived modifier saglamlastrdgmzdaki1
would be represented as:2
</bodyText>
<equation confidence="0.5510598">
saglam(strong)+Adj
+DB+Verb+Become
+DB+Verb+Caus+Pos
+DB+Noun+PastPart+A3sg+P3sg+Loc
+DB+Adj+Rel
</equation>
<bodyText confidence="0.999250741935484">
The five IGs in this are the feature sequences sep-
arated by the DB marker. The first IG shows the
part-of-speech for the root which is its only inflec-
tional feature. The second IG indicates a deriva-
tion into a verb whose semantics is to become
the preceding adjective. The third IG indicates
that a causative verb with positive polarity is de-
rived from the previous verb. The fourth IG in-
dicates the derivation of a nominal form, a past
participle, with +Noun as the part-of-speech and
+PastPart, as the minor part-of-speech, with
some additional inflectional features. Finally, the
fifth IG indicates a derivation into a relativizer ad-
jective.
A sentence would then be represented as a se-
quence of the IGs making up the words. When a
word is considered as a sequence of IGs, linguis-
tically, the last IG of a word determines its role
as a dependent, so, syntactic relation links only
emanate from the last IG of a (dependent) word,
and land on one of the IGs of a (head) word on
the right (with minor exceptions), as exemplified
in Figure 2. And again with minor exceptions, the
dependency links between the IGs, when drawn
above the IG sequence, do not cross.3 Figure 3
from Oflazer (2003) shows a dependency tree for
a Turkish sentence laid on top of the words seg-
mented along IG boundaries.
With this view in mind, the dependency rela-
tions that are to be extracted by a parser should be
relations between certain inflectional groups and
</bodyText>
<page confidence="0.935648">
1
</page>
<bodyText confidence="0.976091">
Literally, (the thing existing) at the time we caused
(something) to become strong.
</bodyText>
<page confidence="0.981122">
2
</page>
<bodyText confidence="0.950544666666667">
The morphological features other than the obvious part-
of-speech features are: +Become: become verb, +Caus:
causative verb, +PastPart: Derived past participle,
+P3sg: 3sg possessive agreement, +A3sg: 3sg number-
person agreement, +Loc: Locative case, +Pos: Positive Po-
larity, +Rel: Relativizing Modifier.
</bodyText>
<page confidence="0.988209">
3
</page>
<figureCaption confidence="0.680049333333333">
Only 2.5% of the dependencies in the Turkish treebank
(Oflazer et al., 2003) actually cross another dependency link.
Figure 2: Dependency Links and IGs
</figureCaption>
<bodyText confidence="0.9992099">
not orthographic words. Since only the word-
final inflectional groups have out-going depen-
dency links to a head, there will be IGs which do
not have any outgoing links (e.g., the first IG of the
word buyumesi in Figure 3). We assume that such
IGs are implicitly linked to the next IG, but nei-
ther represent nor extract such relationships with
the parser, as it is the task of the morphological
analyzer to extract those. Thus the parsing mod-
els that we will present in subsequent sections all
aim to extract these surface relations between the
relevant IGs, and in line with this, we will employ
performance measures based on IGs and their re-
lationships, and not on orthographic words.
We use a model of sentence structure as de-
picted in Figure 4. In this figure, the top part repre-
sents the words in a sentence. After morphological
analysis and morphological disambiguation, each
word is represented with (the sequence of) its in-
flectional groups, shown in the middle of the fig-
ure. The inflectional groups are then reindexed
so that they are the units for the purposes of
parsing. The inflectional groups marked with
are those from which a dependency link will em-
anate from, to a head-word to the right. Please
note that the number of such marked inflectional
groups is the same as the number of words in the
sentence, and all of such IGs, (except one corre-
sponding to the distinguished head of the sentence
which will not have any links), will have outgoing
dependency links.
In the rest of this paper, we first give a very brief
overview a general model of statistical depen-
dency parsing and then introduce three models for
dependency parsing of Turkish. We then present
our results for these models and for some addi-
tional experiments for the best performing model.
We then close with a discussion on the results,
analysis of the errors the parser makes, and con-
clusions.
</bodyText>
<sectionHeader confidence="0.997207" genericHeader="method">
3 Parser
</sectionHeader>
<bodyText confidence="0.99736375">
Statistical dependency parsers first compute the
probabilities of the unit-to-unit dependencies, and
then find the most probable dependency tree T
among the set of possible dependency trees. This
</bodyText>
<page confidence="0.809038">
90
</page>
<figure confidence="0.994127534883721">
\x0cBu eski ev+de +ki gul+un boyle buyu +me+si herkes+i cok etkile+di
Mod
Det
Mod
Subj
Mod
Subj
Obj
Mod
bu
+Det
eski
+Adj
ev
+Noun
+A3sg
+Pnon
+Loc
+Adj gul
+Noun
+A3sg
+Pnon
+Gen
boyle
+Adv
buyu
+Verb
+Noun
+Inf
+A3sg
+P3sg
+Nom
herkes
+Pron
+A3pl
+Pnon
+Acc
cok
+Adv
etkile
+Verb
+Past
+A3sg
</figure>
<bodyText confidence="0.994905">
This old house-at+that-is rose&amp;apos;s such grow +ing everyone very impressed
Such growing of the rose in this old house impressed everyone very much.
+s indicate morpheme boundaries. The rounded rectangles show the words while the inflectional groups within
the words that have more than 1 IG are emphasized with the dashed rounded rectangles. The inflectional features
of each inflectional group as produced by the morphological analyzer are listed below.
</bodyText>
<figureCaption confidence="0.966446">
Figure 3: Dependency links in an example Turkish sentence.
</figureCaption>
<equation confidence="0.99332296969697">
w1
\x04\x04 \x0f\x0f ##
IG1
\x0f\x0f
IG2
\x0f\x0f
IG
g1
\x0f\x0f
IG1 IG2 IG
g1
w2
\x01\x01 \x0f\x0f $$
IG1
\x0f\x0f
IG2 IG
g2
\x0f\x0f
IGg1+1 IG
g1+g2
. . .
. . .
wn
\x04\x04 \x0f\x0f ##
IG1 IG2 IG
gn
\x0f\x0f
IG
n
i
=
Pi
k=1 gk
</equation>
<figureCaption confidence="0.924194">
Figure 4: Sentence Structure
can be formulated as
</figureCaption>
<equation confidence="0.9991406">
T
= argmax
T
P(T, S)
= argmax
T
n1
Y
i=1
P(dep (wi, wH(i))  |S)(1)
</equation>
<bodyText confidence="0.9863583">
where in our case S is a sequence of units (words,
IGs) and T, ranges over possible dependency
trees consisting of left-to-right dependency links
dep (wi, wH(i)) with wH(i) denoting the head unit
to which the dependent unit, wi, is linked to.
The distance between the dependent units plays
an important role in the computation of the depen-
dency probabilities. Collins (1996) employs this
distance i,H(i) in the computation of word-to-
word dependency probabilities
</bodyText>
<equation confidence="0.999096">
P(dep (wi, wH(i))  |S) (2)
P(link(wi, wH(i))  |i,H(i))
</equation>
<bodyText confidence="0.9778051">
suggesting that distance is a crucial variable when
deciding whether two words are related, along
with other features such as intervening punctua-
tion. Chung and Rim (2004) propose a different
method and introduce a new probability factor that
takes into account the distance between the depen-
dent and the head. The model in equation 3 takes
into account the contexts that the dependent and
head reside in and the distance between the head
and the dependent.
</bodyText>
<equation confidence="0.99650125">
P(dep (wi, wH(i))  |S) (3)
P(link(wi, wH(i)))  |i H(i))
P(wi links to some head
H(i) i away|i)
</equation>
<bodyText confidence="0.9939416">
Here i represents the context around the depen-
dent wi and H(i), represents the context around
the head word. P(dep (wi, wH(i))  |S) is the prob-
ability of the directed dependency relation be-
tween wi and wH(i) in the current sentence, while
P(link(wi, wH(i))  |i H(i)) is the probability of
seeing a similar dependency (with wi as the depen-
dent, wH(i) as the head in a similar context) in the
training treebank.
For the parsing models that will be described
below, the relevant statistical parameters needed
have been estimated from the Turkish treebank
(Oflazer et al., 2003). Since this treebank is rel-
atively smaller than the available treebanks for
other languages (e.g., Penn Treebank), we have
</bodyText>
<page confidence="0.985019">
91
</page>
<bodyText confidence="0.966000291666667">
\x0copted to model the bigram linkage probabilities
in an unlexicalized manner (that is, by just taking
certain morphosyntactic properties into account),
to avoid, to the extent possible, the data sparseness
problem which is especially acute for Turkish. We
have also been encouraged by the success of the
unlexicalized parsers reported recently (Klein and
Manning, 2003; Chung and Rim, 2004).
For parsing, we use a version of the Backward
Beam Search Algorithm (Sekine et al., 2000) de-
veloped for Japanese dependency analysis adapted
to our representations of the morphological struc-
ture of the words. This algorithm parses a sentence
by starting from the end and analyzing it towards
the beginning. By making the projectivity assump-
tion that the relations do not cross, this algorithm
considerably facilitates the analysis.
4 Details of the Parsing Models
In this section we detail three models that we have
experimented with for Turkish. All three models
are unlexicalized and differ either in the units used
for parsing or in the way contexts modeled. In
all three models, we use the probability model in
Equation 3.
</bodyText>
<subsectionHeader confidence="0.991053">
4.1 Simplifying IG Tags
</subsectionHeader>
<bodyText confidence="0.964280875">
Our morphological analyzer produces a rather rich
representation with a multitude of morphosyntac-
tic and morphosemantic features encoded in the
words. However, not all of these features are nec-
essarily relevant in all the tasks that these analyses
can be used in. Further, different subsets of these
features may be relevant depending on the func-
tion of a word. In the models discussed below, we
use a reduced representation of the IGs to unlex-
icalize the words:
1. For nominal IGs,4 we use two different tags
depending on whether the IG is used as a de-
pendent or as a head during (different stages
of ) parsing:
If the IG is used as a dependent, (and,
only word-final IGs can be dependents),
we represent that IG by a reduced tag
consisting of only the case marker, as
that essentially determines the syntactic
function of that IG as a dependent, and
only nominals have cases.
If the IG is used as a head, then we use
only part-of-speech and the possessive
agreement marker in the reduced tag.
</bodyText>
<page confidence="0.965928">
4
</page>
<bodyText confidence="0.991691895833333">
These are nouns, pronouns, and other derived forms that
inflect with the same paradigm as nouns, including infinitives,
past and future participles.
2. For adjective IGs with present/past/future
participles minor part-of-speech, we use the
part-of-speech when they are used as depen-
dents and the part-of-speech plus the the pos-
sessive agreement marker when used as a
head.
3. For other IGs, we reduce the IG to just the
part-of-speech.
Such a reduced representation also helps alleviate
the sparse data problem as statistics from many
word forms with only the relevant features are
conflated.
We modeled the second probability term on the
right-hand side of Equation 3 (involving the dis-
tance between the dependent and the head unit) in
the following manner. First, we collected statis-
tics over the treebank sentences, and noted that,
if we count words as units, then 90% of depen-
dency links link to a word that is less than 3 words
away. Similarly, if we count distance in terms of
IGs, then 90% of dependency links link to an IG
that is less than 4 IGs away to the right. Thus we
selected a parameter k = 4 for Models 1 and 3 be-
low, where distance is measured in terms of words,
and k = 5 for Model 2 where distance is measured
in terms of IGs, as a threshold value at and beyond
which a dependency is considered distant. Dur-
ing actual runs,
P(wi links to some head H(i) i away|i)
was computed by interpolating
P1(wi links to some head H(i) i away|i)
estimated from the training corpus, and
P2(wi links to some head H(i) i away)
the estimated probability for a length of a link
when no contexts are considered, again estimated
from the training corpus. When probabilities are
estimated from the training set, all distances larger
than k are assigned the same probability. If even
after interpolation, the probability is 0, then a very
small value is used. This is a modified version of
the backed-off smoothing used by Collins (1996)
to alleviate sparse data problems. A similar inter-
polation is used for the first component on the right
hand side of Equation 3 by removing the head and
the dependent contextual information all at once.
</bodyText>
<subsectionHeader confidence="0.9361765">
4.2 Model 1 Unlexicalized Word-based
Model
</subsectionHeader>
<bodyText confidence="0.975336">
In this model, we represent each word by a re-
duced representation of its last IG when used as a
dependent,5 and by concatenation of the reduced
</bodyText>
<page confidence="0.919886">
5
</page>
<bodyText confidence="0.9315365">
Remember that other IGs in a word, if any, do not have
any bearing on how this word links to its head word.
</bodyText>
<page confidence="0.987743">
92
</page>
<bodyText confidence="0.998910346153846">
\x0crepresentation of its IGs when used as a head.
Since a word can be both a dependent and a head
word, the reduced representation to be used is dy-
namically determined during parsing.
Parsing then proceeds with words as units rep-
resented in this manner. Once the parser links
these units, we remap these links back to IGs to
recover the actual IG-to-IG dependencies. We al-
ready know that any outgoing link from a depen-
dent will emanate from the last IG of that word.
For the head word, we assume that the link lands
on the first IG of that word.6
For the contexts, we use the following scheme.
A contextual element on the left is treated as a de-
pendent and is modeled with its last IG, while a
contextual element on the right is represented as
if it were a head using all its IGs. We ignore any
overlaps between contexts in this and the subse-
quent models.
In Figure 5 we show in a table the sample sen-
tence in Figure 3, the morphological analysis for
each word and the reduced tags for representing
the units for the three models. For each model, we
list the tags when the unit is used as a head and
when it is used as a dependent. For model 1, we
use the tags in rows 3 and 4.
</bodyText>
<subsectionHeader confidence="0.922874">
4.3 Model 2 - IG-based Model
</subsectionHeader>
<bodyText confidence="0.9968276">
In this model, we represent each IG with re-
duced representations in the manner above, but
do not concatenate them into a representation for
the word. So our units for parsing are IGs.
The parser directly establishes IG-to-IG links from
word-final IGs to some IG to the right. The con-
texts that are used in this model are the IGs to
the left (starting with the last IG of the preceding
word) and the right of the dependent and the head
IG.
The units and the tags we use in this model are
in rows 5 and 6 in the table in Figure 5. Note
that the empty cells in row 4 corresponds to IGs
which can not be syntactic dependents as they are
not word-final.
</bodyText>
<subsectionHeader confidence="0.9478125">
4.4 Model 3 IG-based Model with
Word-final IG Contexts
</subsectionHeader>
<bodyText confidence="0.999071833333333">
This model is almost exactly like Model 2 above.
The two differences are that (i) for contexts we
only use just the word-final IGs to the left and the
right ignoring any non-word-final IGs in between
(except for the case that the context and the head
overlap, where we use the tag of the head IG in-
</bodyText>
<page confidence="0.991229">
6
</page>
<bodyText confidence="0.999115">
This choice is based on the observation that in the tree-
bank, 85.6% of the dependency links land on the first (and
possibly the only) IG of the head word, while 14.4% of the
dependency links land on an IG other than the first one.
stead of the final IG); and (ii) the distance function
is computed in terms of words. The reason this
model is used is that it is the word final IGs that
determine the syntactic roles of the dependents.
</bodyText>
<sectionHeader confidence="0.99994" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.998283176470589">
Since in this study we are limited to parsing sen-
tences with only left-to-right dependency links7
which do not cross each other, we eliminated the
sentences having such dependencies (even if they
contain a single one) and used a subset of 3398
such sentences in the Turkish Treebank. The gold
standard part-of-speech tags are used in the exper-
iments. The sentences in the corpus ranged be-
tween 2 words to 40 words with an average of
about 8 words;8 90% of the sentences had less
than or equal to 15 words. In terms of IGs, the
sentences comprised 2 to 55 IGs with an average
of 10 IGs per sentence; 90% of the sentences had
less than or equal to 15 IGs. We partitioned this
set into training and test sets in 10 different ways
to obtain results with 10-fold cross-validation.
We implemented three baseline parsers:
</bodyText>
<footnote confidence="0.643813">
1. The first baseline parser links a word-final IG
to the first IG of the next word on the right.
2. The second baseline parser links a word-final
IG to the last IG of the next word on the
right.9
3. The third baseline parser is a deterministic
rule-based parser that links each word-final
</footnote>
<bodyText confidence="0.970041">
IG to an IG on the right based on the approach
of Nivre (2003). The parser uses 23 unlexi-
calized linking rules and a heuristic that links
any non-punctuation word not linked by the
parser to the last IG of the last word as a de-
pendent.
Table 1 shows the results from our experiments
with these baseline parsers and parsers that are
based on the three models above. The three mod-
els have been experimented with different contexts
around both the dependent unit and the head. In
each row, columns 3 and 4 show the percentage of
IGIG dependency relations correctly recovered
for all tokens, and just words excluding punctu-
ation from the statistics, while columns 5 and 6
show the percentage of test sentences for which
all dependency relations extracted agree with the
</bodyText>
<page confidence="0.993954">
7
</page>
<bodyText confidence="0.997408">
In 95% of the treebank dependencies, the head is the
right of the dependent.
</bodyText>
<page confidence="0.990842">
8
</page>
<bodyText confidence="0.996006666666667">
This is quite normal; the equivalents of function words
in English are embedded as morphemes (not IGs) into these
words.
</bodyText>
<page confidence="0.978183">
9
</page>
<bodyText confidence="0.9989035">
Note that for head words with a single IG, the first two
baselines behave the same.
</bodyText>
<page confidence="0.99509">
93
</page>
<bodyText confidence="0.9804566">
\x0cFigure 5: Tags used in the parsing models
relations in the treebank. Each entry presents the
average and the standard error of the results on the
test set, over the 10 iterations of the 10-fold cross-
validation. Our main goal is to improve the per-
centage of correctly determined IG-to-IG depen-
dency relations, shown in the fourth column of the
table. The best results in these experiments are ob-
tained with Model 3 using 1 unit on both sides of
the dependent. Although it is slightly better than
Model 2 with the same context size, the difference
between the means (0.40.2) for each 10 iterations
is statistically significant.
Since we have been using unlexicalized models,
we wanted to test out whether a smaller training
corpus would have a major impact for our current
models. Table 2 shows results for Model 3 with no
context and 1 unit on each side of the dependent,
obtained by using only a 1500 sentence subset of
the original treebank, again using 10-fold cross
validation. Remarkably the reduction in training
set size has a very small impact on the results.
Although all along, we have suggested that de-
termining word-to-word dependency relationships
is not the right approach for evaluating parser per-
formance for Turkish, we have nevertheless per-
formed word-to-word correctness evaluation so
that comparison with other word based approaches
can be made. In this evaluation, we assume that a
dependency link is correct if we correctly deter-
mine the head word (but not necessarily the cor-
rect IG). Table 3 shows the word based results for
the best cases of the models in Table 1.
We have also tested our parser with a pure word
model where both the dependent and the head are
represented by the concatenation of their IGs, that
is, by their full morphological analysis except the
root. The result for this case is given in the last row
of Table 3. This result is even lower than the rule-
based baseline.10 For this model, if we connect the
</bodyText>
<page confidence="0.99542">
10
</page>
<bodyText confidence="0.99918">
Also lower than Model 1 with no context (79.11.1)
dependent to the first IG of the head as we did in
Model 1, the IG-IG accuracy excluding punctua-
tions becomes 69.93.1, which is also lower than
baseline 3 (70.5%).
</bodyText>
<sectionHeader confidence="0.997785" genericHeader="evaluation">
6 Discussions
</sectionHeader>
<bodyText confidence="0.999736266666667">
Our results indicate that all of our models perform
better than the 3 baseline parsers, even when no
contexts around the dependent and head units are
used. We get our best results with Model 3, where
IGs are used as units for parsing and contexts are
comprised of word final IGs. The highest accuracy
in terms of percent of correctly extracted IG-to-IG
relations excluding punctuations (73.5%) was ob-
tained when one word is used as context on both
sides of the the dependent.11 We also noted that
using a smaller treebank to train our models did
not result in a significant reduction in our accu-
racy indicating that the unlexicalized models are
quite effective, but this also may hint that a larger
treebank with unlexicalized modeling may not be
useful for improving link accuracy.
A detailed look at the results from the best per-
forming model shown in in Table 4,12 indicates
that, accuracy decrases with the increasing sen-
tence length. For longer sentences, we should em-
ploy more sophisticated models possibly including
lexicalization.
A further analysis of the actual errors made by
the best performing model indicates almost 40%
of the errors are attachment problems: the de-
pendent IGs, especially verbal adjuncts and argu-
ments, link to the wrong IG but otherwise with the
same morphological features as the correct one ex-
cept for the root word. This indicates we may have
to model distance in a more sophisticated way and
</bodyText>
<page confidence="0.996277">
11
</page>
<bodyText confidence="0.999708">
We should also note that early experiments using differ-
ent sets of morphological features that we intuitively thought
should be useful, gave rather low accuracy results.
</bodyText>
<page confidence="0.987355">
12
</page>
<bodyText confidence="0.9743075">
These results are significantly higher than the best base-
line (rule based) for all the sentence length categories.
</bodyText>
<page confidence="0.975187">
94
</page>
<table confidence="0.999271428571429">
\x0cPercentage of IG-IG Percentage of Sentences
Relations Correct With ALL Relations Correct
Parsing Model Context Words+Punc Words only Words+Punc Words only
Baseline 1 NA 59.9 0.3 63.9 0.7 21.4 0.6 24.0 0.7
Baseline 2 NA 58.3 0.2 62.2 0.8 20.1 0.0 22.6 0.6
Baseline 3 NA 69.6 0.2 70.5 0.8 31.7 0.7 36.6 0.8
Model 1 None 69.8 0.4 71.0 1.3 32.7 0.6 36.2 0.7
(k=4) Dl=1 69.9 0.4 71.1 1.2 32.9 0.5 36.4 0.6
Dl=1 Dr=1 71.3 0.4 72.5 1.2 33.4 0.8 36.7 0.8
Hl=1 Hr=1 64.7 0.4 65.5 1.3 25.4 0.6 28.7 0.8
Both 71.4 0.4 72.6 1.1 34.2 0.7 37.2 0.6
Model 2 None 70.5 0.3 71.9 1.0 32.1 0.9 36.3 0.9
(k=5) Dl=1 71.3 0.3 72.7 0.9 33.8 0.8 37.4 0.7
Dl=1 Dr=1 71.9 0.3 73.1 0.9 34.8 0.7 38.0 0.7
Hl=1 Hr=1 57.4 0.3 57.6 0.7 23.5 0.6 25.8 0.6
Both 70.9 0.3 72.2 0.9 34.2 0.8 37.2 0.9
Model 3 None 71.2 0.3 72.6 0.9 34.4 0.7 38.1 0.7
(k=4) Dl=1 71.2 0.4 72.6 1.1 34.5 0.7 38.3 0.6
Dl=1 Dr=1 72.3 0.3 73.5 1.0 35.5 0.9 38.7 0.9
Hl=1 Hr=1 55.2 0.3 55.1 0.7 22.0 0.6 24.1 0.6
Both 71.1 0.3 72.4 0.9 35.5 0.8 38.4 0.9
</table>
<bodyText confidence="0.814222">
The Context column entries show the context around the dependent and the head unit. Dl=1 and Dr=1 indicate
the use of 1 unit left and the right of the dependent respectively. Hl=1 and Hr=1 indicate the use of 1 unit left and
the right of the head respectively. Both indicates both head and the dependent have 1 unit of context on both sides.
</bodyText>
<tableCaption confidence="0.995446">
Table 1: Results from parsing with the baseline parsers and statistical parsers based on Models 1-3.
</tableCaption>
<table confidence="0.9939844">
Percentage of IG-IG Percentage of Sentences
Relations Correct With ALL Relations Correct
Parsing Model Context Words+Punc Words only Words+Punc Words only
Model 3 None 71.0 0.6 72.2 1.5 34.4 1.0 38.1 1.1
(k=4, 1500 Sentences) Dl=1 Dr=1 71.6 0.4 72.6 1.1 35.1 1.3 38.4 1.5
</table>
<tableCaption confidence="0.664484">
Table 2: Results from using a smaller training corpus.
</tableCaption>
<table confidence="0.9979128">
Percentage of Word-Word
Relations Correct
Parsing Model Context Words only
Baseline 1 NA 72.1 0.5
Baseline 2 NA 72.1 0.5
Baseline 3 NA 80.3 0.7
Model 1 (k=4) Both 80.8 0.9
Model 2 (k=5) Dl=1 Dr=1 81.0 0.7
Model 3 (k=4) Dl=1 Dr=1 81.2 1.0
Pure Word Model None 77.7 3.5
</table>
<tableCaption confidence="0.993495">
Table 3: Results from word-to-word correctness evaluation.
</tableCaption>
<figure confidence="0.9496502">
Sentence Length l (IGs) % Accuracy
1 &amp;lt; l 10 80.2 0.5
10 &amp;lt; l 20 70.1 0.4
20 &amp;lt; l 30 64.6 1.0
30 &amp;lt; l 62.7 1.3
</figure>
<tableCaption confidence="0.991106">
Table 4: Accuracy over different length sentences.
</tableCaption>
<page confidence="0.992497">
95
</page>
<bodyText confidence="0.988888333333333">
\x0cperhaps use a limited lexicalization such as includ-
ing limited non-morphological information (e.g.,
verb valency) into the tags.
</bodyText>
<sectionHeader confidence="0.998573" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999845956521739">
We have presented our results from statistical de-
pendency parsing of Turkish with statistical mod-
els trained from the sentences in the Turkish tree-
bank. The dependency relations are between
sub-lexical units that we call inflectional groups
(IGs) and the parser recovers dependency rela-
tions between these IGs. Due to the modest size
of the treebank available to us, we have used
unlexicalized statistical models, representing IGs
by reduced representations of their morphological
properties. For the purposes of this work we have
limited ourselves to sentences with all left-to-right
dependency links that do not cross each other.
We get our best results (73.5% IG-to-IG link ac-
curacy) using a model where IGs are used as units
for parsing and we use as contexts, word final IGs
of the words before and after the dependent.
Future work involves a more detailed under-
standing of the nature of the errors and see how
limited lexicalization can help, as well as investi-
gation of more sophisticated models such as SVM
or memory-based techniques for correctly identi-
fying dependencies.
</bodyText>
<sectionHeader confidence="0.995075" genericHeader="acknowledgments">
8 Acknowledgement
</sectionHeader>
<bodyText confidence="0.43467825">
This research was supported in part by a research
grant from TUBITAK (The Scientific and Techni-
cal Research Council of Turkey) and from Istanbul
Technical University.
</bodyText>
<sectionHeader confidence="0.729162" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999599972222222">
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In 1st Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Seattle, Washington.
Hoojung Chung and Hae-Chang Rim. 2004. Un-
lexicalized dependency parser for variable word or-
der languages based on local contextual pattern.
In Computational Linguistics and Intelligent Text
Processing (CICLing-2004), Seoul, Korea. Lecture
Notes in Computer Science 2945.
Michael Collins, Jan Hajic, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 505518, University of Maryland.
Michael Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In Proceedings of
the 34th Annual Meeting of the Association for Com-
putational Linguistics, Santa Cruz, CA.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics and 8th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 1623, Madrid, Spain.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423430, Sapporo,
Japan.
Taku Kudo and Yuji Matsumoto. 2000. Japanese
dependency analysis based on support vector ma-
chines. In Joint Sigdat Conference On Empirical
Methods In Natural Language Processing and Very
Large Corpora, Hong Kong.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Sixth Conference on Natural Language Learning,
Taipei, Taiwan.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL05), pages 99106,
Ann Arbor, Michigan, June.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In 8th Confer-
ence on Computational Natural Language Learning,
Boston, Massachusetts.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of 8th
International Workshop on Parsing Technologies,
pages 2325, Nancy, France, April.
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tur,
and Gokhan Tur. 2003. Building a Turkish tree-
bank. In Anne Abeille, editor, Building and Exploit-
ing Syntactically-annotated Corpora. Kluwer Acad-
emic Publishers.
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Comput-
ing, 9(2).
Kemal Oflazer. 2003. Dependency parsing with an
extended finite-state approach. Computational Lin-
guistics, 29(4).
Satoshi Sekine, Kiyotaka Uchimoto, and Hitoshi Isa-
hara. 2000. Backward beam search algorithm for
dependency analysis of Japanese. In 17th Inter-
national Conference on Computational Linguistics,
pages 754 760, Saarbrucken, Germany.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In 8th International Workshop of Parsing
Technologies, Nancy, France.
</reference>
<page confidence="0.855631">
96
</page>
<figure confidence="0.341424">
\x0c&amp;quot;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.619019">
<title confidence="0.996534">b&amp;quot;Statistical Dependency Parsing of Turkish</title>
<author confidence="0.983041">Gulsen Eryigit</author>
<affiliation confidence="0.999581">Department of Computer Engineering Istanbul Technical University</affiliation>
<address confidence="0.999944">Istanbul, 34469, Turkey</address>
<email confidence="0.991701">gulsen@cs.itu.edu.tr</email>
<author confidence="0.644735">Kemal Oflazer</author>
<affiliation confidence="0.979691">Faculty of Engineering and Natural Sciences Sabanci University</affiliation>
<address confidence="0.999662">Istanbul, 34956, Turkey</address>
<email confidence="0.999858">oflazer@sabanciuniv.edu</email>
<abstract confidence="0.999716">This paper presents results from the first statistical dependency parser for Turkish. Turkish is a free-constituent order language with complex agglutinative inflectional and derivational morphology and presents interesting challenges for statistical parsing, as in general, dependency relations are between portions of words called inflectional groups. We have explored statistical models that use different representational units for parsing. We have used the Turkish Dependency Treebank to train and test our parser but have limited this initial exploration to that subset of the treebank sentences with only left-to-right non-crossing dependency links. Our results indicate that the best accuracy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In 1st Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Seattle, Washington.</location>
<contexts>
<context position="1448" citStr="Charniak, 2000" startWordPosition="200" endWordPosition="201">ited this initial exploration to that subset of the treebank sentences with only left-to-right non-crossing dependency links. Our results indicate that the best accuracy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels a</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In 1st Conference of the North American Chapter of the Association for Computational Linguistics, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoojung Chung</author>
<author>Hae-Chang Rim</author>
</authors>
<title>Unlexicalized dependency parser for variable word order languages based on local contextual pattern.</title>
<date>2004</date>
<contexts>
<context position="1655" citStr="Chung and Rim (2004)" startWordPosition="230" endWordPosition="233">tions between inflectional groups is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have been found to be less effective for free-constituentorder languages where constituents can easily chan</context>
<context position="9931" citStr="Chung and Rim (2004)" startWordPosition="1602" endWordPosition="1605">es over possible dependency trees consisting of left-to-right dependency links dep (wi, wH(i)) with wH(i) denoting the head unit to which the dependent unit, wi, is linked to. The distance between the dependent units plays an important role in the computation of the dependency probabilities. Collins (1996) employs this distance i,H(i) in the computation of word-toword dependency probabilities P(dep (wi, wH(i)) |S) (2) P(link(wi, wH(i)) |i,H(i)) suggesting that distance is a crucial variable when deciding whether two words are related, along with other features such as intervening punctuation. Chung and Rim (2004) propose a different method and introduce a new probability factor that takes into account the distance between the dependent and the head. The model in equation 3 takes into account the contexts that the dependent and head reside in and the distance between the head and the dependent. P(dep (wi, wH(i)) |S) (3) P(link(wi, wH(i))) |i H(i)) P(wi links to some head H(i) i away|i) Here i represents the context around the dependent wi and H(i), represents the context around the head word. P(dep (wi, wH(i)) |S) is the probability of the directed dependency relation between wi and wH(i) in the curren</context>
<context position="11402" citStr="Chung and Rim, 2004" startWordPosition="1844" endWordPosition="1847">nt statistical parameters needed have been estimated from the Turkish treebank (Oflazer et al., 2003). Since this treebank is relatively smaller than the available treebanks for other languages (e.g., Penn Treebank), we have 91 \x0copted to model the bigram linkage probabilities in an unlexicalized manner (that is, by just taking certain morphosyntactic properties into account), to avoid, to the extent possible, the data sparseness problem which is especially acute for Turkish. We have also been encouraged by the success of the unlexicalized parsers reported recently (Klein and Manning, 2003; Chung and Rim, 2004). For parsing, we use a version of the Backward Beam Search Algorithm (Sekine et al., 2000) developed for Japanese dependency analysis adapted to our representations of the morphological structure of the words. This algorithm parses a sentence by starting from the end and analyzing it towards the beginning. By making the projectivity assumption that the relations do not cross, this algorithm considerably facilitates the analysis. 4 Details of the Parsing Models In this section we detail three models that we have experimented with for Turkish. All three models are unlexicalized and differ eithe</context>
</contexts>
<marker>Chung, Rim, 2004</marker>
<rawString>Hoojung Chung and Hae-Chang Rim. 2004. Unlexicalized dependency parser for variable word order languages based on local contextual pattern.</rawString>
</citation>
<citation valid="false">
<booktitle>In Computational Linguistics and Intelligent Text Processing (CICLing-2004), Seoul, Korea. Lecture Notes in Computer Science</booktitle>
<pages>2945</pages>
<marker></marker>
<rawString>In Computational Linguistics and Intelligent Text Processing (CICLing-2004), Seoul, Korea. Lecture Notes in Computer Science 2945.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Jan Hajic</author>
<author>Lance Ramshaw</author>
<author>Christoph Tillmann</author>
</authors>
<title>A statistical parser for Czech.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>505518</pages>
<institution>University of Maryland.</institution>
<contexts>
<context position="2365" citStr="Collins et al. (1999)" startWordPosition="338" endWordPosition="341">thers. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have been found to be less effective for free-constituentorder languages where constituents can easily change their position in the sentence without modifying the general meaning of the sentence. Collins et al. (1999) applied the parser of Collins (1997) developed for English, to Czech, and found that the performance was substantially lower when compared to the results for English. 2 Turkish Turkish is an agglutinative language where a sequence of inflectional and derivational morphemes get affixed to a root (Oflazer, 1994). At the syntax level, the unmarked constituent order is SOV, but constituent order may vary freely as demanded by the discourse context. Essentially all constituent orders are possible, especially at the main sentence level, with very minimal formal constraints. In written text however,</context>
</contexts>
<marker>Collins, Hajic, Ramshaw, Tillmann, 1999</marker>
<rawString>Michael Collins, Jan Hajic, Lance Ramshaw, and Christoph Tillmann. 1999. A statistical parser for Czech. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 505518, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="1464" citStr="Collins, 1996" startWordPosition="202" endWordPosition="203">l exploration to that subset of the treebank sentences with only left-to-right non-crossing dependency links. Our results indicate that the best accuracy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotat</context>
<context position="9618" citStr="Collins (1996)" startWordPosition="1557" endWordPosition="1558">1 \x0f\x0f IG2 IG g2 \x0f\x0f IGg1+1 IG g1+g2 . . . . . . wn \x04\x04 \x0f\x0f ## IG1 IG2 IG gn \x0f\x0f IG n i = Pi k=1 gk Figure 4: Sentence Structure can be formulated as T = argmax T P(T, S) = argmax T n1 Y i=1 P(dep (wi, wH(i)) |S)(1) where in our case S is a sequence of units (words, IGs) and T, ranges over possible dependency trees consisting of left-to-right dependency links dep (wi, wH(i)) with wH(i) denoting the head unit to which the dependent unit, wi, is linked to. The distance between the dependent units plays an important role in the computation of the dependency probabilities. Collins (1996) employs this distance i,H(i) in the computation of word-toword dependency probabilities P(dep (wi, wH(i)) |S) (2) P(link(wi, wH(i)) |i,H(i)) suggesting that distance is a crucial variable when deciding whether two words are related, along with other features such as intervening punctuation. Chung and Rim (2004) propose a different method and introduce a new probability factor that takes into account the distance between the dependent and the head. The model in equation 3 takes into account the contexts that the dependent and head reside in and the distance between the head and the dependent. </context>
<context position="15087" citStr="Collins (1996)" startWordPosition="2486" endWordPosition="2487">t. During actual runs, P(wi links to some head H(i) i away|i) was computed by interpolating P1(wi links to some head H(i) i away|i) estimated from the training corpus, and P2(wi links to some head H(i) i away) the estimated probability for a length of a link when no contexts are considered, again estimated from the training corpus. When probabilities are estimated from the training set, all distances larger than k are assigned the same probability. If even after interpolation, the probability is 0, then a very small value is used. This is a modified version of the backed-off smoothing used by Collins (1996) to alleviate sparse data problems. A similar interpolation is used for the first component on the right hand side of Equation 3 by removing the head and the dependent contextual information all at once. 4.2 Model 1 Unlexicalized Word-based Model In this model, we represent each word by a reduced representation of its last IG when used as a dependent,5 and by concatenation of the reduced 5 Remember that other IGs in a word, if any, do not have any bearing on how this word links to its head word. 92 \x0crepresentation of its IGs when used as a head. Since a word can be both a dependent and a he</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>1623</pages>
<location>Madrid,</location>
<contexts>
<context position="2402" citStr="Collins (1997)" startWordPosition="346" endWordPosition="347">ucture of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have been found to be less effective for free-constituentorder languages where constituents can easily change their position in the sentence without modifying the general meaning of the sentence. Collins et al. (1999) applied the parser of Collins (1997) developed for English, to Czech, and found that the performance was substantially lower when compared to the results for English. 2 Turkish Turkish is an agglutinative language where a sequence of inflectional and derivational morphemes get affixed to a root (Oflazer, 1994). At the syntax level, the unmarked constituent order is SOV, but constituent order may vary freely as demanded by the discourse context. Essentially all constituent orders are possible, especially at the main sentence level, with very minimal formal constraints. In written text however, the unmarked order is dominant at bo</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 1623, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="11380" citStr="Klein and Manning, 2003" startWordPosition="1840" endWordPosition="1843">scribed below, the relevant statistical parameters needed have been estimated from the Turkish treebank (Oflazer et al., 2003). Since this treebank is relatively smaller than the available treebanks for other languages (e.g., Penn Treebank), we have 91 \x0copted to model the bigram linkage probabilities in an unlexicalized manner (that is, by just taking certain morphosyntactic properties into account), to avoid, to the extent possible, the data sparseness problem which is especially acute for Turkish. We have also been encouraged by the success of the unlexicalized parsers reported recently (Klein and Manning, 2003; Chung and Rim, 2004). For parsing, we use a version of the Backward Beam Search Algorithm (Sekine et al., 2000) developed for Japanese dependency analysis adapted to our representations of the morphological structure of the words. This algorithm parses a sentence by starting from the end and analyzing it towards the beginning. By making the projectivity assumption that the relations do not cross, this algorithm considerably facilitates the analysis. 4 Details of the Parsing Models In this section we detail three models that we have experimented with for Turkish. All three models are unlexica</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis based on support vector machines.</title>
<date>2000</date>
<booktitle>In Joint Sigdat Conference On Empirical Methods In Natural Language Processing and Very Large Corpora,</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="1591" citStr="Kudo and Matsumoto (2000" startWordPosition="219" endWordPosition="222">ults indicate that the best accuracy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have been found to be less effective for fr</context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000. Japanese dependency analysis based on support vector machines. In Joint Sigdat Conference On Empirical Methods In Natural Language Processing and Very Large Corpora, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Sixth Conference on Natural Language Learning,</booktitle>
<location>Taipei, Taiwan.</location>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Sixth Conference on Natural Language Learning, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudoprojective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05),</booktitle>
<pages>99106</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1725" citStr="Nivre and Nilsson (2005)" startWordPosition="242" endWordPosition="245">onal groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have been found to be less effective for free-constituentorder languages where constituents can easily change their position in the sentence without modifying the general meanin</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudoprojective dependency parsing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05), pages 99106, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In 8th Conference on Computational Natural Language Learning,</booktitle>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="1687" citStr="Nivre et al. (2004)" startWordPosition="236" endWordPosition="239"> is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have been found to be less effective for free-constituentorder languages where constituents can easily change their position in the sentenc</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In 8th Conference on Computational Natural Language Learning, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of 8th International Workshop on Parsing Technologies,</booktitle>
<pages>2325</pages>
<location>Nancy, France,</location>
<contexts>
<context position="19414" citStr="Nivre (2003)" startWordPosition="3306" endWordPosition="3307">IGs with an average of 10 IGs per sentence; 90% of the sentences had less than or equal to 15 IGs. We partitioned this set into training and test sets in 10 different ways to obtain results with 10-fold cross-validation. We implemented three baseline parsers: 1. The first baseline parser links a word-final IG to the first IG of the next word on the right. 2. The second baseline parser links a word-final IG to the last IG of the next word on the right.9 3. The third baseline parser is a deterministic rule-based parser that links each word-final IG to an IG on the right based on the approach of Nivre (2003). The parser uses 23 unlexicalized linking rules and a heuristic that links any non-punctuation word not linked by the parser to the last IG of the last word as a dependent. Table 1 shows the results from our experiments with these baseline parsers and parsers that are based on the three models above. The three models have been experimented with different contexts around both the dependent unit and the head. In each row, columns 3 and 4 show the percentage of IGIG dependency relations correctly recovered for all tokens, and just words excluding punctuation from the statistics, while columns 5 </context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of 8th International Workshop on Parsing Technologies, pages 2325, Nancy, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Bilge Say, Dilek Zeynep Hakkani-Tur, and Gokhan Tur.</title>
<date>2003</date>
<booktitle>Building and Exploiting Syntactically-annotated Corpora.</booktitle>
<editor>In Anne Abeille, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="5216" citStr="Oflazer (2003)" startWordPosition="806" endWordPosition="807">y, the fifth IG indicates a derivation into a relativizer adjective. A sentence would then be represented as a sequence of the IGs making up the words. When a word is considered as a sequence of IGs, linguistically, the last IG of a word determines its role as a dependent, so, syntactic relation links only emanate from the last IG of a (dependent) word, and land on one of the IGs of a (head) word on the right (with minor exceptions), as exemplified in Figure 2. And again with minor exceptions, the dependency links between the IGs, when drawn above the IG sequence, do not cross.3 Figure 3 from Oflazer (2003) shows a dependency tree for a Turkish sentence laid on top of the words segmented along IG boundaries. With this view in mind, the dependency relations that are to be extracted by a parser should be relations between certain inflectional groups and 1 Literally, (the thing existing) at the time we caused (something) to become strong. 2 The morphological features other than the obvious partof-speech features are: +Become: become verb, +Caus: causative verb, +PastPart: Derived past participle, +P3sg: 3sg possessive agreement, +A3sg: 3sg numberperson agreement, +Loc: Locative case, +Pos: Positive</context>
</contexts>
<marker>Oflazer, 2003</marker>
<rawString>Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tur, and Gokhan Tur. 2003. Building a Turkish treebank. In Anne Abeille, editor, Building and Exploiting Syntactically-annotated Corpora. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Two-level description of Turkish morphology.</title>
<date>1994</date>
<journal>Literary and Linguistic Computing,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="2677" citStr="Oflazer, 1994" startWordPosition="389" endWordPosition="390">ting the arcs which extend from dependents to heads. Parsers employing CFG-backbones have been found to be less effective for free-constituentorder languages where constituents can easily change their position in the sentence without modifying the general meaning of the sentence. Collins et al. (1999) applied the parser of Collins (1997) developed for English, to Czech, and found that the performance was substantially lower when compared to the results for English. 2 Turkish Turkish is an agglutinative language where a sequence of inflectional and derivational morphemes get affixed to a root (Oflazer, 1994). At the syntax level, the unmarked constituent order is SOV, but constituent order may vary freely as demanded by the discourse context. Essentially all constituent orders are possible, especially at the main sentence level, with very minimal formal constraints. In written text however, the unmarked order is dominant at both the main sentence and embedded clause level. Turkish morphotactics is quite complicated: a given word form may involve multiple derivations and the number of word forms one can generate from a nominal or verbal root is theoretically infinite. Derivations in Turkish are ve</context>
</contexts>
<marker>Oflazer, 1994</marker>
<rawString>Kemal Oflazer. 1994. Two-level description of Turkish morphology. Literary and Linguistic Computing, 9(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Dependency parsing with an extended finite-state approach.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="5216" citStr="Oflazer (2003)" startWordPosition="806" endWordPosition="807">y, the fifth IG indicates a derivation into a relativizer adjective. A sentence would then be represented as a sequence of the IGs making up the words. When a word is considered as a sequence of IGs, linguistically, the last IG of a word determines its role as a dependent, so, syntactic relation links only emanate from the last IG of a (dependent) word, and land on one of the IGs of a (head) word on the right (with minor exceptions), as exemplified in Figure 2. And again with minor exceptions, the dependency links between the IGs, when drawn above the IG sequence, do not cross.3 Figure 3 from Oflazer (2003) shows a dependency tree for a Turkish sentence laid on top of the words segmented along IG boundaries. With this view in mind, the dependency relations that are to be extracted by a parser should be relations between certain inflectional groups and 1 Literally, (the thing existing) at the time we caused (something) to become strong. 2 The morphological features other than the obvious partof-speech features are: +Become: become verb, +Caus: causative verb, +PastPart: Derived past participle, +P3sg: 3sg possessive agreement, +A3sg: 3sg numberperson agreement, +Loc: Locative case, +Pos: Positive</context>
</contexts>
<marker>Oflazer, 2003</marker>
<rawString>Kemal Oflazer. 2003. Dependency parsing with an extended finite-state approach. Computational Linguistics, 29(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Kiyotaka Uchimoto</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Backward beam search algorithm for dependency analysis of Japanese.</title>
<date>2000</date>
<booktitle>In 17th International Conference on Computational Linguistics,</booktitle>
<pages>754--760</pages>
<location>Saarbrucken, Germany.</location>
<contexts>
<context position="1620" citStr="Sekine et al. (2000)" startWordPosition="224" endWordPosition="227">acy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have been found to be less effective for free-constituentorder languages</context>
<context position="11493" citStr="Sekine et al., 2000" startWordPosition="1860" endWordPosition="1863">al., 2003). Since this treebank is relatively smaller than the available treebanks for other languages (e.g., Penn Treebank), we have 91 \x0copted to model the bigram linkage probabilities in an unlexicalized manner (that is, by just taking certain morphosyntactic properties into account), to avoid, to the extent possible, the data sparseness problem which is especially acute for Turkish. We have also been encouraged by the success of the unlexicalized parsers reported recently (Klein and Manning, 2003; Chung and Rim, 2004). For parsing, we use a version of the Backward Beam Search Algorithm (Sekine et al., 2000) developed for Japanese dependency analysis adapted to our representations of the morphological structure of the words. This algorithm parses a sentence by starting from the end and analyzing it towards the beginning. By making the projectivity assumption that the relations do not cross, this algorithm considerably facilitates the analysis. 4 Details of the Parsing Models In this section we detail three models that we have experimented with for Turkish. All three models are unlexicalized and differ either in the units used for parsing or in the way contexts modeled. In all three models, we use</context>
</contexts>
<marker>Sekine, Uchimoto, Isahara, 2000</marker>
<rawString>Satoshi Sekine, Kiyotaka Uchimoto, and Hitoshi Isahara. 2000. Backward beam search algorithm for dependency analysis of Japanese. In 17th International Conference on Computational Linguistics, pages 754 760, Saarbrucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In 8th International Workshop of Parsing Technologies,</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="1553" citStr="Yamada and Matsumoto (2003)" startWordPosition="212" endWordPosition="216">ht non-crossing dependency links. Our results indicate that the best accuracy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have </context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In 8th International Workshop of Parsing Technologies, Nancy, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>