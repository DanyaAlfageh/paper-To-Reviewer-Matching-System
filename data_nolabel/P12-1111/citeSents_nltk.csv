CITATION.,,
The word count number is further processed following CITATION, wc(w) = floor(log(count(w)) 5)/5.,,
1054 \x0c2 English POS tagging 2.1 Explore deterministic constraints Suppose that, following CITATION, we distinguish major lexical categories (Noun, Verb, Adjective and Preposition) by two binary features: + |N and + |V.,,
On the other hand, consider the annotation guideline of English Treebank CITATION instead.,,
We adopt the basic feature set used in CITATION and CITATION.,,
As discussed in CITATION, categorical information of neighbouring words on both sides of w0 help resolve POS ambiguity of w0.,,
In CITATION, lookahead features may be available for use during decoding since searching is bidirectional instead of left-to-right as in Viterbi decoding.,,
parameter estimation of , we use the averaged perceptron as described in CITATION.,,
As introduced in Section 2.2, we adopt a very compact feature set used in CITATION1.,,
This replicates the feature set B used in CITATION.,,
Based on experiments carried out 1 Our implementation of this feature set is basically the same as the version used in CITATION.,,
1059 \x0cCITATIONs feature Beam=1 Beam=5 raw 96.46%/3 97.16/1 constrained 96.80%/14 97.20/10 Feature B in CITATION CITATION 97.15% (Beam=3) constrained 97.03%/11 97.20/8 Table 9: POS tagging accuracy and speed.,,
The baseline for speed in all cases is the unconstrained tagger using CITATIONs feature and conducting a beam (=5) search.,,
Our baseline model, which uses CITATIONs feature set and conducts a beam (=5) search in the unconstrained,,
For example, CITATIONa) combine different levels of knowledge in an outside linear model of a twolayer cascaded model; CITATIONb) uses the forest re-ranking technique CITATION; and in CITATION, only known words in vocabulary are included in the hybrid lattice consisting of both character- and word-level nodes.,,
Character-based feature templates We adopt the non-lexical-target feature templates in (CITATIONa).,,
In early work, rule-based models find words one by one based on heuristics such as forward maximum match CITATION.,,
Exact search is possible with a Viterbi-style algorithm, but beamsearch decoding is more popular as used in CITATION and (CITATIONa).,,
For example, CITATIONa) combine different levels of knowledge in an outside linear model of a twolayer cascaded model; CITATIONb) uses the forest re-ranking technique CITATION; and in CITATION, only known words in vocabulary are included in the hybrid lattice consisting of both character- and word-level nodes.,,
CITATION uses this technique to merge different levels of predictors for word segmentation.,,
CITATION and CITATION, we divide this corpus into training set (sections 0-18), development set (sections 19-21) and the final test set (sections 22-24).,,
Following (CITATIONa), we divide this corpus into training set (chapters 1-260), development set (chapters 271-300) and the final test set (chapters 301-325).,,
We use the same Viterbi decoder as implemented for English POS tagging and use a noncommercial ILP solver included in GNU Linear Pro1060 \x0cprecision recall F-measure Viterbi 0.971 0.966 0.968 ILP 0.970 0.977 0.974 (CITATIONa), POS- 0.971 (CITATIONa), POS+ 0.973 Table 10: F-measure on Chinese word segmentation.,,
These results are compared to the core perceptron trained without POS in (CITATIONa).,,
Character-based feature templates We adopt the non-lexical-target feature templates in (CITATIONa).,,
In early work, rule-based models find words one by one based on heuristics such as forward maximum match CITATION.,,
Exact search is possible with a Viterbi-style algorithm, but beamsearch decoding is more popular as used in CITATION and (CITATIONa).,,
For example, CITATIONa) combine different levels of knowledge in an outside linear model of a twolayer cascaded model; CITATIONb) uses the forest re-ranking technique CITATION; and in CITATION, only known words in vocabulary are included in the hybrid lattice consisting of both character- and word-level nodes.,,
CITATION uses this technique to merge different levels of predictors for word segmentation.,,
CITATION and CITATION, we divide this corpus into training set (sections 0-18), development set (sections 19-21) and the final test set (sections 22-24).,,
Following (CITATIONa), we divide this corpus into training set (chapters 1-260), development set (chapters 271-300) and the final test set (chapters 301-325).,,
We use the same Viterbi decoder as implemented for English POS tagging and use a noncommercial ILP solver included in GNU Linear Pro1060 \x0cprecision recall F-measure Viterbi 0.971 0.966 0.968 ILP 0.970 0.977 0.974 (CITATIONa), POS- 0.971 (CITATIONa), POS+ 0.973 Table 10: F-measure on Chinese word segmentation.,,
These results are compared to the core perceptron trained without POS in (CITATIONa).,,
This type of constraint may come from human input solicited in interactive inference procedure CITATION.,,
For example, CITATIONa) combine different levels of knowledge in an outside linear model of a twolayer cascaded model; CITATIONb) uses the forest re-ranking technique CITATION; and in CITATION, only known words in vocabulary are included in the hybrid lattice consisting of both character- and word-level nodes.,,
These results are compared to the core perceptron trained without POS in (CITATIONa).,,
CITATION, (CITATIONa), and CITATION.,,
F-measure 0.978 by the cascaded model in (CITATIONa).,,
CITATION uses the stacked learning technique to merge different levels of predictors, obtaining a combined system that beats individual ones.,,
1054 \x0c2 English POS tagging 2.1 Explore deterministic constraints Suppose that, following CITATION, we distinguish major lexical categories (Noun, Verb, Adjective and Preposition) by two binary features: + |N and + |V.,,
On the other hand, consider the annotation guideline of English Treebank CITATION instead.,,
1 introduction In recent work, interesting results are reported for applications of integer linear programming (ILP) such as semantic role labeling (SRL) CITATION, dependency parsing CITATION and so on.,,
These results are compared to the core perceptron trained without POS in (CITATIONa).,,
CITATION, (CITATIONa), and CITATION.,,
F-measure 0.978 by the cascaded model in (CITATIONa).,,
CITATION uses the stacked learning technique to merge different levels of predictors, obtaining a combined system that beats individual ones.,,
We adopt the basic feature set used in CITATION and CITATION.,,
As discussed in CITATION, categorical information of neighbouring words on both sides of w0 help resolve POS ambiguity of w0.,,
In CITATION, lookahead features may be available for use during decoding since searching is bidirectional instead of left-to-right as in Viterbi decoding.,,
As introduced in Section 2.2, we adopt a very compact feature set used in CITATION1.,,
This replicates the feature set B used in CITATION.,,
Based on experiments carried out 1 Our implementation of this feature set is basically the same as the version used in CITATION.,,
1059 \x0cCITATIONs feature Beam=1 Beam=5 raw 96.46%/3 97.16/1 constrained 96.80%/14 97.20/,,
When beam-width is set to 5, tagging accuracy is not improved by the use of Feature B in CITATION; and because the size of the feature model grows, efficiency is hurt.,,
As compared to the constrained greedy tagger using CITATIONs feature set, with the additional use of three locally lookahead feature templates, tagging accuracy is increased from 96.80% to 97.02%.,,
When no further data is used other than training data, the bidirectional tagger described in CITATION achives an accuracy of 97.33%, using a much richer feature set (E) than feature set B, the one we compare with here.,,
1 introduction In recent work, interesting results are reported for applications of integer linear programming (ILP) such as semantic role labeling (SRL) CITATION, dependency parsing CITATION and so on.,,
We adopt the basic feature set used in CITATION and CITATION.,,
As discussed in CITATION, categorical information of neighbouring words on both sides of w0 help resolve POS ambiguity of w0.,,
In CITATION, lookahead features may be available for use during decoding since searching is bidirectional instead of left-to-right as in Viterbi decoding.,,
CITATION uses this technique to merge different levels of predictors for word segmentation.,,
CITATION and CITATION, we divide this corpus into training set (sections 0-18), development set (sections 19-21) and the final test set (sections 22-24).,,
Following (CITATIONa), we divide this corpus into training set (chapters 1-260), development set (chapters 271-300) and the final test set (chapters 301-325).,,
As introduced in Section 2.2, we adopt a very compact feature set used in CITATION1.,,
This replicates the feature set B used in CITATION.,,
Based on experiments carried out 1 Our implementation of this feature set is basically the same as the version used in CITATION.,,
1059 \x0cCITATIONs feature Beam=1 Beam=5 raw 96.46%/3 97.16/1 constrained 96.80%/14 97.20/10 Feature B in CITATION CITATION 97.15% (Beam=3) constrained 97.03%/11 97.20/8 Table 9: POS tagging accuracy and speed.,,
When beam-width is set to 5, tagging accuracy is not improved by the use of Feature B in CITATION; and because the size of the feature model grows, efficiency is hurt.,,
As compared to the constrained greedy tagger using CITATIONs feature set, with the additional use of three locally lookahead feature templates, tagging accuracy is increased from 96.80% to 97.02%.,,
When no further data is used other than training data, the bidirectional tagger described in CITATION achives an accuracy of 97.33%, using a much richer feature set (E) than feature set,,
In early work, rule-based models find words one by one based on heuristics such as forward maximum match CITATION.,,
Exact search is possible with a Viterbi-style algorithm, but beamsearch decoding is more popular as used in CITATION and (CITATIONa).,,
CITATION uses this technique to merge different levels of predictors for word segmentation.,,
CITATION and CITATION, we divide this corpus into training set (sections 0-18), development set (sections 19-21) and the final test set (sections 22-24).,,
Following (CITATIONa), we divide this corpus into training set (chapters 1-260), development set (chapters 2,,
CITATION, (CITATIONa), and CITATION.,,
F-measure 0.978 by the cascaded model in (CITATIONa).,,
CITATION uses the stacked learning technique to merge different levels of predictors, obtaining a combined system that beats individual ones.,,
CITATION uses this technique to merge different levels of predictors for word segmentation.,,
CITATION and CITATION, we divide this corpus into training set (sections 0-18), development set (sections 19-21) and the final test set (sections 22-24).,,
Following (CITATIONa), we divide this corpus into training set (chapters 1-260), development set (chapters 2,,
CITATION, (CITATIONa), and CITATION.,,
F-measure 0.978 by the cascaded model in (CITATIONa).,,
CITATION uses the stacked learning technique to merge different levels of predictors, obtaining a combined system that beats individual ones.,,
When no further data is used other than training data, the bidirectional tagger described in CITATION achives an accuracy of 97.33%, using a much richer feature set (E) than feature set B, the one we compare with here.,,
CITATION.,,
3 Chinese Word Segmentation (CWS) 3.1 Word segmentation as character tagging Considering the ambiguity problem that a Chinese character may appear in any relative position in a word and the out-of-vocabulary (OOV) problem that it is impossible to observe all words in training data, CWS is widely formulated as a character tagging problem CITATION.,,
In early work, rule-based models find words one by one based on heuristics such as forward maximum match CITATION.,,
Exact search is possible with a Viterbi-style algorithm, but beamsearch decoding is more popular as used in CITATION and (CITATIONa).,,
