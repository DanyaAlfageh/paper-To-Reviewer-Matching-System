1 Introduction Much recent work on coreference resolution, which is the task of deciding which noun phrases, or mentions, in a document refer to the same real world entity, builds on CITATION,,
Our feature set was simple, and included many features from CITATION, including the pronoun, string match, definite and demonstrative NP, number and gender agreement, proper name and appositive features,,
Our feature set was simple, and included many features from CITATION, including the pronoun, string match, definite and demonstrative NP, number an,,
More recently, CITATION utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers,,
45 \x0copposed to pairwise models) has included: CITATION who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree; CITATION who defined several conditional random field-based models; CITATION who took a reranking approach; and CITATION who use a probabilistic first-order logic model,,
3.1 Evaluation Metrics The MUC scorer CITATION is a popular coreference evaluation metric, but we found it to be fatally flawed,,
(2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree; CITATION who defined several conditional random field-based models; CITATION who took a reranking approach; and CITATION who use a probabilistic first-order logic model,,
Prior work (CITATION; CITATION) has generated training data for pairwise classifiers in the following manner,,
We also added part of speech (POS) tags to the data using the tagger of CITATION, and used the tags to decide if mentions were plural or singular,,
3.1 Evaluation Metrics The MUC scorer CITATION is a popular c,,
In addition to the MUC and b3 scorers, we also evaluate using cluster f-measure CITATION, which is the standard f-measure computed over true/false coreference decisions for pairs of 3 From http://lpsolve.sourceforge.net/ 4 Integer linear programming is, after all, NP-hard,,
mentions; the Rand index CITATION, which is pairwise accuracy of the clustering; and variation of information CITATION, which utilizes the entropy of th,,
For comparison, we also give the results of the COREFILP system of CITATION, which was also based on a nave pairwise classifier,,
As observed by CITATION, if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score significantly higher than any published system,,
Our D&B-STYLE baseline used the same test time method as CITATION, however at training time we created data for all mention pairs,,
mentions; the Rand index CITATION, which is pairwise accuracy of the clustering; and variation of information CITATION, which utilizes the entropy of the clusterings and their mutual information (and for which lower values are better),,
Ng and Cardie (2002a) and CITATION highlight the problem of determining whether or not common noun phrases are anaphoric,,
For comparison, we also give the results of the COREFILP system of CITATION, which was also based on a nave pair,,
The b3 scorer CITATION was proposed to overcome several shortcomings of the MUC scorer,,
This approach made sense for CITATION because testing proceeded in a similar manner: for each mention, work backwards until you find a previous mention which the classifier thinks is coreferent, add a link, and terminate the search,,
Our SOON-STYLE baseline used the same training and testing regimen as CITATION,,
In addition to the MUC and b3 scorers, we also evaluate using cluster f-measure CITATION, which is the standard f-measure computed over true/false coreference decisions for pairs of 3 Fro,,
The COREF-ILP model of CITATION took a different approach at test time: for each mention they would work backwards and add a link for all previous mentions which the classifier deemed coreferent,,
When describing our model, we build upon the notation used by CITATION,,
Much work that followed improved upon this strategy, by improving the features (CITATIONb), the type of classifier CITATION, and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (CITATIONb),,
e tagger of CITATION, and used the tags to decide if mentions were plural or singular,,
This corpus had a third portion, NPAPER, but we found that several documents where too long for lp solve to find a solution.4 We added named entity (NE) tags to the data using the tagger of CITATION,,
As observed by CITATION, if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score significantly higher than any published syste,,
a solution.4 We added named entity (NE) tags to the data using the tagger of CITATION,,
