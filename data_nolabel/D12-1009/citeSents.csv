in part-ofspeech clustering CITATION,,
5.2 Baseline Models Our work is motivated by the Bayesian HMM approach of CITATION the model we refer to as the block HMM (BHMM) and we consider this our primary baseline,,
The CNET corpus is annotated with twelve speech act classes: QUESTION and ANSWER, which are both broken down into multiple sub-classes, as well as RESOLUTION, REPRODUCTION, and OTHER CITATION,,
This is common practice and we will not go into detail; see CITATION for a general example on sampling switching variables,,
Specifically, we build off the Bayesian block HMMs used by CITATION for modeling Twitter conversations, which will be our primary baseline,,
If it is desired to have a large number of latent topics as is common in LDA, this model could be combined with a standard topic model without sequential dependencies, as explored by CITATION,,
Given only a flat structure, can we recover the reply structure of messages in the conversation? Previous work with BHMM found the optimal structure by computing the likelihood of all permutations of a thread or sequence (CITATION; CITATIONb),,
Extensions to the block HMM have incorporated mixed membership properties within blocks, notably the Markov Clustering Topic Model CITATION, which allows each HMM state to be associat,,
Addressing these concerns, there has been recent work with unsupervised models of Web conversations based on hidden Markov models CITATION, where each state corresponds to a conversational class or act,,
(See also CITATION for more details on Bayesian HMMs with Dirichlet priors.) We also compare against LDA, which makes latent assignments at the token-level, but blocks of text are independent of 5 Three messages in this corpus have multiple parents,,
CITATION applied HMMs as an unsupervised model of discourse,,
 CITATION), the variety of conversation domains on the Web motivates the use of unsupervised ap5 10 15 20 25 Number of classes 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Variation of Information (VI) Speech Act Clustering (CNET) Random LDA BHMM M4 Figure 3: The variation of information between the human-created speech act annotations of the CNET corpus and the latent class assignments by various models,,
In the case of conversation data, our hope is that some of the latent classes represent speech acts or dialog acts CITATION,,
stribution over words which is independent of the latent classes in a document; this was also done by CITATIONb),,
We instead handle this by extending our model to include a background distribution over words which is independent of the latent classes in a document; this was also done by CITATIONb),,
the latent permutation model of CITATION,,
This figure depicts the Bayesian variant of the block HMM CITATION where the transition distributions depend on a Dirichlet() prior,,
One topic model that imposes sequential dependencies between documents is Sequential LDA CITATION, which models a document as a sequence of segments (such as paragraphs) governed by a Pitman-Yor process, in which the latent topic distribution of one segment serves as the base distribution for the next segment,,
Much as LDA topics do not always correspond to what humans would judge to be semantic classes CITATION, the conversation classes inferred by unsupervised sequence models are similarly unlikely to be a perfect fit to human-assigned classes,,
into sentences) and constraint each segment to belong to one class or speech act; modifications along these lines have been applied to topic models as well CITATION,,
5.1 Data Sets First, we use a corpus of discussion threads from CNET forums CITATION, which are mostly technical discussion and support,,
Extensions to the block HMM have incorporated mixed membership properties within blocks, notably the Markov Clustering Topic Model CITATION, which allows each HMM state to be associated with its own distribution over topics in a topic model,,
This sampling distribution is very similar to that of LDA CITATION, but the distribution over topics is now a function of the previous block, which gives the leftmost term,,
More recently, CITATIONb) experimented with similar tasks using a related HMMbased model called the Structural Topic Model,,
An approach to unsupervised discourse modeling that does not use HMMs is 3 Incremental updates are justified under the generalized EM algorithm CITATION,,
mulations of latent class distributions4 are utilized in correlated topic models CITATION as a means of incorporating covariance structure among topic probabilities,,
justified under the generalized EM algorithm CITATION,,
In topic models, log-linear formulations of latent class distributions4 are utilized in correlated topic models CITATION as a means of incorporating covariance structure among topic probabilities,,
For example, topic models such as Latent Dirichlet Allocation (LDA) CITATION assume each document has its own distribution over multiple classes (often called topics),,
We would like to quantitatively measure how closely the latent states induced by our model match these annotations.7 We can measure this with variation of information CITATION, which has been used in recent years for unsupervised evaluation, e.g,,
Under the block HMM, as utilized by CITATION, messages in a conversation flow according to a Markov process, where the words of messages are generated according to language models ass,,
Standard topic model extensions such as n-gram models CITATION can straightforwardly be applied here, and indeed we already applied such an extension by incorporating background distributions in 5.3,,
Applying log-linear regression to potentially many features was combined with LDA by CITATION, who model the Dirichlet prior over topics as a function of document features,,
This corpus includes 321 threads and a total of 1309 messages, with an average message length of 78 tokens after preprocessing.5 Second, we use the Twitter data set created by CITATION,,
Decoupling HMM states from latent classes was considered by CITATION with the Factorial HMM, which uses factorized state representations,,
Compared to the naive approach of treating conversations as flat documents, models which include conversation structure have been shown to improve tasks such as forum search (CITATION; CITATION), question answering and expert finding (CITATION; CITATIONa), and interpersonal relationship identification CITATION,,
nversation structure have been shown to improve tasks such as forum search (CITATION; CITATION), question answering and expert finding (CITATION; CITATIONa), and interpersonal relationship identification CITATION,,
We show that M4 increases thread reconstruction accuracy by up to 15% compared to the HMM of CITATION, and we reduce variation of information against speech act annotations by an average of 18% from HMM and LDA baselines,,
CITATION extended this work by enriching the emission distributions and using additional features such as speaker and position information,,
While there is a body of work in supervised speech act classification (Cohen et al., 2004; Bangalore et al., 2006; CITATION; CITATION), the variety of conversation domains on the Web motivates the use of unsupervised ap5 10 15 20 25 Number of classes 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Variation of Information (VI) Speech Act Clustering (CNET) Random LDA BHMM M4 Figure 3: The variation of information between the human-created speech act annotations of the CNET corpus and the latent class assignments by various models,,
We use optimized asymmetric priors as described in 5.2, and we use a symmetric Dirichlet for the word distributions, following CITATION,,
In topic models, this is sometimes assumed to be Poisson CITATION,,
Unsupervised HMMs were applied to conversational data by CITATION who experimented with Twitter conversations,,
One topic model that imposes sequential dependencies between documents is Sequential LDA CITATION, which models a document as a sequence of segments (such as paragraphs) governed by a Pit,,
In the conversation domain, this corresponds to the task of thread reconstruction (CITATION; CITATIONc),,
Similar arguments are made by CITATION when designing supervised topic models,,
