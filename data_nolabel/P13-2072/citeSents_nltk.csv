Overall, even though the method shows some promise, we do not see the dramatic gains that have been seen for the web search ranking task CITATION,,
Some recent works have attempted to relax the linearity assumption on MT features CITATION, by defining non-parametric models on complete translation hypotheses, for use in an nbest re-ranking setting,,
The minimization is done approximately by a standard greedy tree-growing algorithm CITATION,,
In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (CITATION; Wu This research was conducted during the authors internship at Microsoft Research et a,,
We achieve this by applying gradient boosting machines CITATION to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU,,
2.3 Loss function We use a pair-wise ranking log-loss as in the PRO parameter tuning method CITATION,,
In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (CITATION; Wu This research was conducted during the authors internship at,,
We build on the work by CITATION which shows how to induce features to minimize any differentiable loss function,,
We report performance using the BLEU-SBP metric proposed in (CITATIONa),,
 researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; CITATION),,
When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; CITATION,,
ying gradient boosting machines CITATION to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU,,
This is a variant of BLEU CITATION with strict brevity penalty, where a long translation for one sentence can not be used to counteract the brevity penalty for another sentence with a short translation,,
The initial scores have the form F0(x) = l=1...L lfl(x).This is equivalent to using the CITATION method of parameter tuning for a fixed input feature set and a linear model,,
Recently, researchers have proposed a large number of additional features (CITATION; CITATION) and parameter tuning methods (CITATIONb; CITATION; CITATION) which are better able to scale to the larger parameter space,,
CITATION) has become the de-facto standard in the field,,
1 Introduction The linear model for machine translation CITATION has become the de-facto standard in the field,,
When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; CITATION),,
In our application the features are regression decision trees, and the loss function is the pairwise ranking log-loss from the PRO method for parameter tuning CITATION,,
et al., 2007; CITATION) and parameter tuning methods (CITATIONb; CITATION; CITATION) which are better able to scale to the larger parameter space,,
Recently, researchers have proposed a large number of additional features (CITATION; CITATION) and parameter tuning method,,
In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (CITATION; Wu This research was conducted during the authors internship at Microsoft Research et al., 2010),,
e loss function is the pairwise ranking log-loss from the PRO method for parameter tuning CITATION,,
In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (CITATION; Wu This resea,,
We apply the framework of gradient boosting for decision tree weak learners CITATION,,
