We use the open-source software package Alchemy CITATION for MLN inference, which allows computing the probability of a query literal given a set of weighted clauses as background knowledge and evidence,,
This approach can be viewed as a bridge between CITATIONs purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators or entailment,,
Existing approaches that derive phrasal inference rules from distributional similarity (CITATIONa; CITATION; CITATION) precompile large lists of inference rules,,
For the RTE task, systems were evaluated using both accuracy and confidence-weighted score (cws) as used by CITATION and the RTE1 challenge CITATION,,
CITATION; 2013) proposed an approach to RTE that uses MLNs to combine traditional logical representations with distributional information in order to support probabilistic textual inference,,
Specifically, we utilize Markov Logic Networks (MLNs) CITATION, which employ weighted formulas in first-order logic to compactly ,,
presented by CITATION and CITATION,,
This is possible because MLNs constitute a flexible programming language based on probabilistic logic CITATION that can be easily adapted to support multiple types of linguistically useful inference,,
This is the same technique used by CITATION except we used additive regression CITATION instead of linear regression since it gave better results,,
Of existing RTE approaches, the closest to ours is by CITATION, who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover,,
The corpus itself was built by asking annotators from Amazon Mechanical Turk to describe very short video fragments CITATION,,
Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 CITATION,,
Finally, we built an ensemble that combines the output of multiple systems using regression trained 17 \x0cMethod r AvgComb + no DIR 0.58 AvgComb + word DIR 0.60 AvgComb + phrase DIR 0.66 AvgComb w/o VarBind + no DIR 0.58 AvgComb w/o VarBind + word DIR 0.60 AvgComb w/o VarBind + phrase DIR 0.73 VS-Add 0.78 VS-Mul 0.58 VS-Pairwise 0.77 Ensemble (VS-Add + VS-Mul + VSPairwise) 0.83 Ensemble ([AvgComb + phrase DIR] + VS-Add + VS-Mul + VS-Pairwise) 0.85 Best MSR-Vid score in STS 2012 CITATION 0.87 Table 2: Results on the STS video dataset,,
CITATION introduced a tractable subset of Markov Logic (TML) for which a future software release is planned,,
In comparison to existing methods for creating textual inference rules (CITATIONb; CITATION), these rules are computed on the fly as needed, rather than pre-compiled,,
presenting the contexts in which they occur (CITATION; CITATION),,
We plan to incorporate directed similarity measures such as those of CITATION and CITATION,,
In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition CITATION or component-wise multiplication (CITATION; CITATION),,
Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els CITATION use contextual similarity to predict semantic similarity of words and phrases (CITATION; CITATION), and to model polysemy (CITATION; CITATION; CITATION),,
CITATION; 2013) propose a framework for combining logic and distributional models in which logical form is the prima,,
 the similarity of vectors representing the contexts in which they occur (CITATION; CITATION),,
To reduce the impact of misparsing, we plan to use a version of C&C that can produce the top-n parses together with parse re-ranking CITATION,,
CITATION recognize paraphrases using a tree of vectors, a phrase structure tree in which each constituent is associated,,
Finally, Markov Logic Networks CITATION (MLNs) are used to perform weighted inference on the resulting knowledge base,,
The best performer in 2012s competition was by CITATION, an ensemble system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics,,
CITATION recognize paraphrases using a tree of vectors, a phrase structure tree in which each constituent is associated with a vector, and overall sentence similarity is computed by a classifier that integrates all pairwise similarities,,
CITATION; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation,,
to use a version of C&C that can produce the top-n parses together with parse re-ranking CITATION,,
The distributional model we use contains all lemmas occurring at least 50 times in the Gigaword corpus CITATION except a list of stop words,,
The organizers of the STS 2012 task CITATION sampled video descriptions and asked Turkers to assign similarity scores (ranging from 0 to 5) to pairs of sentences, without ,,
It builds on the C&C CCG parser CITATION,,
In particular, they perform worse than strict 16 \x0cMethod acc cws Chance 0.50 0.50 Bos & Markert, strict 0.52 0.55 Best system in RTE-1 challenge CITATION 0.59 0.62 VS-Add 0.49 0.53 VS-Mul 0.51 0.52 VS-Pairwise 0.50 0.50 AvgComb w/o VarBind + phrase DIR 0.52 0.53 Deterministic AND + phrase DIR 0.57 0.57 Table 1: Results on the RTE-1 Test Set,,
We chose to use the average evidence combiner for MLNs introduced by CITATION,,
CITATION or by a component-wise product of word vectors (CITATION; CITATION),,
Specifically, we utilize Markov Logic Networks (MLNs) CITATION, which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models,,
Wide-coverage logic-based semantics Boxer CITATION is a s,,
CITATION achieves 0.89 accuracy and 0.88 cws on the combined RTE-2 and RTE-3 dataset,,
We plan to experiment with more sophisticated approaches to computing phrase vectors such as those recently presented by CITATION and CITATION,,
Specifically, we utilize Markov Logic Networks (MLNs) CITATION, which employ weighted formulas in first-order logic to compactly encode complex undirected probabili,,
collections (e.g., (CITATION; Chan et al., 2011)),,
AvgComb + word DIR 0.60 AvgComb + phrase DIR 0.66 AvgComb w/o VarBind + no DIR 0.58 AvgComb w/o VarBind + word DIR 0.60 AvgComb w/o VarBind + phrase DIR 0.73 VS-Add 0.78 VS-Mul 0.58 VS-Pairwise 0.77 Ensemble (VS-Add + VS-Mul + VSPairwise) 0.83 Ensemble ([AvgComb + phrase DIR] + VS-Add + VS-Mul + VS-Pairwise) 0.85 Best MSR-Vid score in STS 2012 CITATION 0.87 Table 2: Results on the STS video dataset,,
e vector by adding the vectors for the individual words CITATION or by a component-wise product of word vectors (CITATION; CITATION),,
This approach can be viewed as a bridge between CITATIONs purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators,,
Wide-coverage logic-based semantics Boxer CITATION is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures CITATION,,
We replace deterministic conjunction by an average combiner, which encodes causal independence CITATION,,
CITATION conceptualize,,
We plan to explore coarser logical representations such as Minimal Recursion Semantics (MRS) CITATION,,
2 Related work Distributional semantics Distributional models define the semantic relatedness of words as the similarity of vectors representing the contexts in which they occur (CITATION; CITATION),,
entailment from CITATION, a system that uses only logic,,
CITATION conceptualize textual entailment as tree rewriting of syntactic graphs, where some rewriting rules are distributional inference rules,,
4 Task 1: Recognizing Textual Entailment 4.1 Dataset In order to compare directly to the logic-based system of CITATION, we focus on the RTE-1 dataset CITATION, which includes 567 Text-Hypothesis (T-H) pairs in the development set and 800 pairs in the test set,,
Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (CITATION; CITATION),,
The best performer in 2012s competition was by CITATION, an ensemble system that integrates many techniques including string similarity, n-,,
This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (CITATION; CITATION), which encourages developing new techniques to combine them,,
CITATION; 2013) proposed an approach to RTE that uses MLNs to combine traditional logical representations with distributional information in order,,
As discussed by CITATION), Boxers output is mapped to logical form and augmented with additional information to handle a variety of semantic phenomena,,
The organizers of the STS 2012 task CITATION sampled video descriptions and asked Turkers to assign similarity scores (ranging from 0 to 5) to pairs of sentences, without access to the video,,
Weighted inference, and combined structuraldistributional representations One approach to weighted inference in NLP is that of CITATION, who proposed viewing natural language interpretation as abductive inference,,
stributional inference rule collections (e.g., (CITATION; Chan et al., 2011)),,
(This is in contrast to approaches like CITATION and CITATION, who do not offer a proposal for using vectors at multiple levels in a syntactic tree simultaneously.) 3 MLN system Our system extends that of CITATION; 2013) to support larger-scale evaluation on standard benchmarks for both RTE and STS,,
Logic-based representations (CITATION; CITATION) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms,,
The simplest models compute a phrase vector by adding the vectors for the individual words CITATION or by a component-wise product of word vectors (CITATION; CITATION),,
CITATION use this framework for RTE, deriving inference costs from WordNet similarity and properties of the syntactic parse,,
The third scales pairwise words similarities to the sentence level using weighted average where weights are inverse document frequencies idf as suggested by CITATION (VSPairwise),,
First, we intend to incorporate logical form translations of existing distributional inference rule collections (e.g., (CITATION; Chan et al., 2011)),,
hods that compute phrase vectors from word vectors and tensors (CITATION; CITATION),,
We then treat similarity as degree of entailment, a move that has a long tradition (e.g., (CITATIONb; CITATION; CITATION)),,
For example, pizza slice and slice of pizza are both mapped to the 2 It is customary to transform raw counts in a way that captures association between target words and dimensions, for example through point-wise mutual information CITATION,,
(This is in contrast to approaches like CITATION and CITATION, who do not offer a proposal for using vectors at multiple levels in a syntactic tree simultaneously.) 3 ML,,
