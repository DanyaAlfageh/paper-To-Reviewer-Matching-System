<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<figureCaption confidence="0.218497">
b&amp;apos;Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 1121, Atlanta, Georgia, June 13-14, 2013. c
</figureCaption>
<figure confidence="0.9280885">
2013 Association for Computational Linguistics
Montague Meets Markov: Deep Semantics with Probabilistic Logical Form
Islam Beltagy
, Cuong Chau
, Gemma Boleda
, Dan Garrette
, Katrin Erk
,
</figure>
<author confidence="0.677756">
Raymond Mooney
</author>
<affiliation confidence="0.988148">
Department of Computer Science
Department of Linguistics
The University of Texas at Austin
</affiliation>
<address confidence="0.926408">
Austin, Texas 78712
</address>
<email confidence="0.994306">
{beltagy,ckcuong,dhg,mooney}@cs.utexas.edu
gemma.boleda@utcompling.com,katrin.erk@mail.utexas.edu
</email>
<sectionHeader confidence="0.990626" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9977565">
We combine logical and distributional rep-
resentations of natural language meaning by
transforming distributional similarity judg-
ments into weighted inference rules using
Markov Logic Networks (MLNs). We show
that this framework supports both judg-
ing sentence similarity and recognizing tex-
tual entailment by appropriately adapting the
MLN implementation of logical connectives.
We also show that distributional phrase simi-
larity, used as textual inference rules created
on the fly, improves its performance.
</bodyText>
<sectionHeader confidence="0.998238" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997278052631579">
Tasks in natural language semantics are very diverse
and pose different requirements on the underlying
formalism for representing meaning. Some tasks
require a detailed representation of the structure of
complex sentences. Some tasks require the ability to
recognize near-paraphrases or degrees of similarity
between sentences. Some tasks require logical infer-
ence, either exact or approximate. Often it is neces-
sary to handle ambiguity and vagueness in meaning.
Finally, we frequently want to be able to learn rele-
vant knowledge automatically from corpus data.
There is no single representation for natural lan-
guage meaning at this time that fulfills all require-
ments. But there are representations that meet some
of the criteria. Logic-based representations (Mon-
tague, 1970; Kamp and Reyle, 1993) provide an
expressive and flexible formalism to express even
complex propositions, and they come with standard-
ized inference mechanisms. Distributional mod-
</bodyText>
<figure confidence="0.823906">
hamster(
gerbil(
sim(
#
hamster,
#
gerbil) = w
8x hamster(x) ! gerbil(x)  |f(w)
</figure>
<figureCaption confidence="0.9544255">
Figure 1: Turning distributional similarity into a
weighted inference rule
</figureCaption>
<bodyText confidence="0.944673727272727">
els (Turney and Pantel, 2010) use contextual sim-
ilarity to predict semantic similarity of words and
phrases (Landauer and Dumais, 1997; Mitchell and
Lapata, 2010), and to model polysemy (Schutze,
1998; Erk and Pado, 2008; Thater et al., 2010).
This suggests that distributional models and logic-
based representations of natural language meaning
are complementary in their strengths (Grefenstette
and Sadrzadeh, 2011; Garrette et al., 2011), which
encourages developing new techniques to combine
them.
Garrette et al. (2011; 2013) propose a framework
for combining logic and distributional models in
which logical form is the primary meaning repre-
sentation. Distributional similarity between pairs of
words is converted into weighted inference rules that
are added to the logical form, as illustrated in Fig-
ure 1. Finally, Markov Logic Networks (Richardson
and Domingos, 2006) (MLNs) are used to perform
weighted inference on the resulting knowledge base.
However, they only employed single-word distribu-
tional similarity rules, and only evaluated on a small
</bodyText>
<page confidence="0.999164">
11
</page>
<bodyText confidence="0.998558615384615">
\x0cset of short, hand-crafted test sentences.
In this paper, we extend Garrette et al.s approach
and adapt it to handle two existing semantic tasks:
recognizing textual entailment (RTE) and seman-
tic textual similarity (STS). We show how this sin-
gle semantic framework using probabilistic logical
form in Markov logic can be adapted to support both
of these important tasks. This is possible because
MLNs constitute a flexible programming language
based on probabilistic logic (Domingos and Lowd,
2009) that can be easily adapted to support multiple
types of linguistically useful inference.
At the word and short phrase level, our approach
model entailment through distributional similarity
(Figure 1). If X and Y occur in similar contexts, we
assume that they describe similar entities and thus
there is some degree of entailment between them. At
the sentence level, however, we hold that a stricter,
logic-based view of entailment is beneficial, and we
even model sentence similarity (in STS) as entail-
ment.
There are two main innovations in the formalism
that make it possible for us to work with naturally
occurring corpus data. First, we use more expres-
sive distributional inference rules based on the sim-
ilarity of phrases rather than just individual words.
In comparison to existing methods for creating tex-
tual inference rules (Lin and Pantel, 2001b; Szpek-
tor and Dagan, 2008), these rules are computed on
the fly as needed, rather than pre-compiled. Second,
we use more flexible probabilistic combinations of
evidence in order to compute degrees of sentence
similarity for STS and to help compensate for parser
errors. We replace deterministic conjunction by an
average combiner, which encodes causal indepen-
dence (Natarajan et al., 2010).
We show that our framework is able to han-
dle both sentence similarity (STS) and textual en-
tailment (RTE) by making some simple adapta-
tions to the MLN when switching between tasks.
The framework achieves reasonable results on both
tasks. On STS, we obtain a correlation of r = 0.66
with full logic, r = 0.73 in a system with weak-
ened variable binding, and r = 0.85 in an ensemble
model. On RTE-1 we obtain an accuracy of 0.57.
We show that the distributional inference rules ben-
efit both tasks and that more flexible probabilistic
combinations of evidence are crucial for STS. Al-
though other approaches could be adapted to handle
both RTE and STS, we do not know of any other
methods that have been explicitly tested on both
problems.
</bodyText>
<sectionHeader confidence="0.998598" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.985395975">
Distributional semantics Distributional models
define the semantic relatedness of words as the
similarity of vectors representing the contexts in
which they occur (Landauer and Dumais, 1997;
Lund and Burgess, 1996). Recently, such mod-
els have also been used to represent the meaning
of larger phrases. The simplest models compute
a phrase vector by adding the vectors for the indi-
vidual words (Landauer and Dumais, 1997) or by a
component-wise product of word vectors (Mitchell
and Lapata, 2008; Mitchell and Lapata, 2010).
Other approaches, in the emerging area of distribu-
tional compositional semantics, use more complex
methods that compute phrase vectors from word
vectors and tensors (Baroni and Zamparelli, 2010;
Grefenstette and Sadrzadeh, 2011).
Wide-coverage logic-based semantics Boxer
(Bos, 2008) is a software package for wide-coverage
semantic analysis that produces logical forms using
Discourse Representation Structures (Kamp and
Reyle, 1993). It builds on the C&amp;C CCG parser
(Clark and Curran, 2004).
Markov Logic In order to combine logical and
probabilistic information, we draw on existing work
in Statistical Relational AI (Getoor and Taskar,
2007). Specifically, we utilize Markov Logic Net-
works (MLNs) (Domingos and Lowd, 2009), which
employ weighted formulas in first-order logic to
compactly encode complex undirected probabilistic
graphical models. MLNs are well suited for our ap-
proach since they provide an elegant framework for
assigning weights to first-order logical rules, com-
bining a diverse set of inference rules and perform-
ing sound probabilistic inference.
An MLN consists of a set of weighted first-order
clauses. It provides a way of softening first-order
logic by allowing situations in which not all clauses
are satisfied. More specifically, they provide a
well-founded probability distribution across possi-
ble worlds by specifying that the probability of a
</bodyText>
<page confidence="0.99786">
12
</page>
<bodyText confidence="0.973794142857143">
\x0cworld increases exponentially with the total weight
of the logical clauses that it satisfies. While methods
exist for learning MLN weights directly from train-
ing data, since the appropriate training data is lack-
ing, our approach uses weights computed using dis-
tributional semantics. We use the open-source soft-
ware package Alchemy (Kok et al., 2005) for MLN
inference, which allows computing the probability
of a query literal given a set of weighted clauses as
background knowledge and evidence.
Tasks: RTE and STS Recognizing Textual En-
tailment (RTE) is the task of determining whether
one natural language text, the premise, implies an-
other, the hypothesis. Consider (1) below.
</bodyText>
<listItem confidence="0.763350333333333">
(1) p: Oracle had fought to keep the forms from
being released
h: Oracle released a confidential document
</listItem>
<bodyText confidence="0.999488345454546">
Here, h is not entailed. RTE directly tests whether
a system can construct semantic representations that
allow it to draw correct inferences. Of existing RTE
approaches, the closest to ours is by Bos and Mark-
ert (2005), who employ a purely logical approach
that uses Boxer to convert both the premise and hy-
pothesis into first-order logic and then checks for
entailment using a theorem prover. By contrast, our
approach uses Markov logic with probabilistic infer-
ence.
Semantic Textual Similarity (STS) is the task of
judging the similarity of two sentences on a scale
from 0 to 5 (Agirre et al., 2012). Gold standard
scores are averaged over multiple human annota-
tions. The best performer in 2012s competition was
by Bar et al. (2012), an ensemble system that inte-
grates many techniques including string similarity,
n-gram overlap, WordNet similarity, vector space
similarity and MT evaluation metrics.
Weighted inference, and combined structural-
distributional representations One approach to
weighted inference in NLP is that of Hobbs et al.
(1993), who proposed viewing natural language in-
terpretation as abductive inference. In this frame-
work, problems like reference resolution and syntac-
tic ambiguity resolution become inferences to best
explanations that are associated with costs. How-
ever, this leaves open the question of how costs are
assigned. Raina et al. (2005) use this framework for
RTE, deriving inference costs from WordNet simi-
larity and properties of the syntactic parse.
Garrette et al. (2011; 2013) proposed an approach
to RTE that uses MLNs to combine traditional log-
ical representations with distributional information
in order to support probabilistic textual inference.
This approach can be viewed as a bridge between
Bos and Markert (2005)s purely logical approach,
which relies purely on hard logical rules and the-
orem proving, and distributional approaches, which
support graded similarity between concepts but have
no notion of logical operators or entailment.
There are also other methods that combine dis-
tributional and structured representations. Stern et
al. (2011) conceptualize textual entailment as tree
rewriting of syntactic graphs, where some rewrit-
ing rules are distributional inference rules. Socher
et al. (2011) recognize paraphrases using a tree of
vectors, a phrase structure tree in which each con-
stituent is associated with a vector, and overall sen-
tence similarity is computed by a classifier that inte-
grates all pairwise similarities. (This is in contrast to
approaches like Baroni and Zamparelli (2010) and
Grefenstette and Sadrzadeh (2011), who do not of-
fer a proposal for using vectors at multiple levels in
a syntactic tree simultaneously.)
</bodyText>
<sectionHeader confidence="0.995401" genericHeader="method">
3 MLN system
</sectionHeader>
<bodyText confidence="0.944528">
Our system extends that of Garrette et al. (2011;
2013) to support larger-scale evaluation on standard
benchmarks for both RTE and STS. We conceptual-
ize both tasks as probabilistic entailment in Markov
logic, where STS is judged as the average degree of
mutual entailment, i.e. we compute the probability
</bodyText>
<listItem confidence="0.889863">
of both S1 |= S2 and S2 |= S1 and average the re-
sults. Below are some sentence pairs that we use as
examples in the discussion below:
(2) S1: A man is slicing a cucumber.
S2: A man is slicing a zucchini.
(3) S1: A boy is riding a bicycle.
S2: A little boy is riding a bike.
(4) S1: A man is driving.
S2: A man is driving a car.
</listItem>
<page confidence="0.998068">
13
</page>
<bodyText confidence="0.9771542">
\x0cSystem overview. To compute the probability of
an entailment S1 |= S2, the system first constructs
logical forms for each sentence using Boxer and
then translates them into MLN clauses. In example
(2) above, the logical form for S1:
</bodyText>
<equation confidence="0.985236333333333">
x0, e1, x2 man(x0) slice(e1) Agent(e1, x0)
cucumber(x2) Patient(e1, x2)
\x01
</equation>
<bodyText confidence="0.912115333333333">
is used as evidence, and the logical form for S2 is
turned into the following formula (by default, vari-
ables are assumed to be universally quantified):
</bodyText>
<equation confidence="0.998538">
man(x) slice(e) Agent(e, x)
zucchini(y) Patient(e, y) result()
</equation>
<bodyText confidence="0.988452888888889">
where result() is the query for which we have
Alchemy compute the probability.
However, S2 is not strictly entailed by S1 because
of the mismatch between cucumber and zuc-
chini, so with just the strict logical-form transla-
tions of S1 and S2, the probability of result() will
be zero. This is where we introduce distributional
similarity, in this case the similarity of cucumber
and zucchini, cos(
</bodyText>
<equation confidence="0.793963">
#
cucumber,
#
</equation>
<bodyText confidence="0.973032125">
zucchini). We cre-
ate inference rules from such similarities as a form
of background knowledge. We then treat similarity
as degree of entailment, a move that has a long tradi-
tion (e.g., (Lin and Pantel, 2001b; Raina et al., 2005;
Szpektor and Dagan, 2008)). In general, given two
words a and b, we transform their cosine similarity
into an inference-rule weight wt(a, b) using:
</bodyText>
<figure confidence="0.874209">
wt(a, b) = log(
cos(#
a ,
#
b )
1 cos(#
a ,
#
b )
) prior (5)
</figure>
<bodyText confidence="0.9837398">
Where prior is a negative weight used to initialize
all predicates, so that by default facts are assumed
to have very low probability. In our experiments,
we use prior = 3. In the case of sentence pair
(2), we generate the inference rule:
</bodyText>
<equation confidence="0.716498">
cucumber(x) zucchini(x)  |wt(cuc., zuc.)
</equation>
<bodyText confidence="0.6853495">
Such inference rules are generated for all pairs of
words (w1, w2) where w1 S1 and w2 S2.1
</bodyText>
<page confidence="0.674475">
1
</page>
<bodyText confidence="0.998529326530612">
We omit inference rules for words (a, b) where cos(a, b) &amp;lt;
for a threshold set to maximize performance on the training
data. Low-similarity pairs usually indicate dissimilar words.
This removes a sizeable number of rules for STS, while for RTE
the tuned threshold was near zero.
The distributional model we use contains all lem-
mas occurring at least 50 times in the Gigaword cor-
pus (Graff et al., 2007) except a list of stop words.
The dimensions are the 2,000 most frequent of these
words, and cell values are weighted with point-wise
mutual information. 2
Phrase-based inference rules. Garrette et al. only
considered distributional inference rules for pairs of
individual words. We extend their approach to dis-
tributional inference rules for pairs of phrases in or-
der to handle cases like (3). To properly estimate
the similarity between S1 and S2 in (3), we not only
need an inference rule linking bike to bicycle,
but also a rule estimating how similar boy is to
little boy. To do so, we make use of existing ap-
proaches that compute distributional representations
for phrases. In particular, we compute the vector for
a phrase from the vectors of the words in that phrase,
using either vector addition (Landauer and Dumais,
1997) or component-wise multiplication (Mitchell
and Lapata, 2008; Mitchell and Lapata, 2010). The
inference-rule weight, wt(p1, p2), for two phrases
p1 and p2 is then determined using Eq. (5) in the
same way as for words.
Existing approaches that derive phrasal inference
rules from distributional similarity (Lin and Pantel,
2001a; Szpektor and Dagan, 2008; Berant et al.,
2011) precompile large lists of inference rules. By
comparison, distributional phrase similarity can be
seen as a generator of inference rules on the fly,
as it is possible to compute distributional phrase
vectors for arbitrary phrases on demand as they are
needed for particular examples.
Inference rules are generated for all pairs of con-
stituents (c1, c2) where c1 S1 and c2 S2, a
constituent is a single word or a phrase. The log-
ical form provides a handy way to extract phrases,
as they are generally mapped to one of two logical
constructs. Either we have multiple single-variable
predicates operating on the same variable. For ex-
ample the phrase a little boy has the logical form
boy(x) little(x). Or we have two unary predi-
cates connected with a relation. For example, pizza
slice and slice of pizza are both mapped to the
</bodyText>
<page confidence="0.972706">
2
</page>
<bodyText confidence="0.974024333333333">
It is customary to transform raw counts in a way that cap-
tures association between target words and dimensions, for ex-
ample through point-wise mutual information (Lowe, 2001).
</bodyText>
<page confidence="0.998076">
14
</page>
<bodyText confidence="0.9838216">
\x0clogical form, slice(x0) of(x0, x1) pizza(x1).
We consider all binary predicates as relations.
Average Combiner to determine similarity in the
presence of missing phrases. The logical forms
for the sentences in (4): are
</bodyText>
<equation confidence="0.9456672">
S1: x0, e1 man(x0)agent(x0, e1)drive(e1)
\x01
S2: x0, e1, x2 man(x0) agent(x0, e1)
drive(e1) patient(e1, x2) car(x2)
\x01
</equation>
<bodyText confidence="0.997504777777778">
If we try to prove S1 |= S2, the probability of
the result() will be zero: There is no evidence for
a car, and the hypothesis predicates are conjoined
using a deterministic AND. For RTE, this makes
sense: If one of the hypothesis predicates is False,
the probability of entailment should be zero. For the
STS task, this should in principle be the same, at
least if the omitted facts are vital, but it seems that
annotators rated the data points in this task more for
overall similarity than for degrees of entailment. So
in STS, we want the similarity to be a function of
the number of elements in the hypothesis that are
inferable. Therefore, we need to replace the deter-
ministic AND with a different way of combining
evidence. We chose to use the average evidence
combiner for MLNs introduced by Natarajan et al.
(2010). To use the average combiner, the full logi-
cal form is divided into smaller clauses (which we
call mini-clauses), then the combiner averages their
probabilities. In case the formula is a list of con-
juncted predicates, a mini-clause is a conjunction
of a single-variable predicate with a relation predi-
cate(as in the example below). In case the logical
form contains a negated sub-formula, the negated
sub-formula is also a mini-clause. The hypothesis
above after dividing clauses for the average com-
biner looks like this:
</bodyText>
<equation confidence="0.99681625">
man(x0) agent(x0, e1) result(x0, e1, x2)  |w
drive(e1) agent(x0, e1) result(x0, e1, x2)  |w
drive(e1) patient(e1, x2) result(x0, e1, x2)  |w
car(x2) patient(e1, x2) result(x0, e1, x2)  |w
</equation>
<bodyText confidence="0.969840166666667">
where result is again the query predicate. Here,
result has all of the variables in the clause as argu-
ments in order to maintain the binding of variables
across all of the mini-clauses. The weights w are the
following function of n, the number of mini-clauses
(4 in the above example):
</bodyText>
<equation confidence="0.997010571428571">
w =
1
n
(log(
p
1 p
) prior) (6)
</equation>
<bodyText confidence="0.998032083333333">
where p is a value close to 1 that is set to maximize
performance on the training data, and prior is the
same negative weight as before. Setting w this way
produces a probability of p for the result() in cases
that satisfy the antecedents of all mini-clauses. For
the example above, the antecedents of the first two
mini-clauses are satisfied, while the antecedents of
the last two are not since the premise provides no
evidence for an object of the verb drive. The simi-
larity is then computed to be the maximum probabil-
ity of any grounding of the result predicate, which
in this case is around p
</bodyText>
<equation confidence="0.510828">
2 .3
</equation>
<bodyText confidence="0.9712685">
An interesting variation of the average combiner
is to omit variable bindings between the mini-
clauses. In this case, the hypothesis clauses look
like this for our example:
</bodyText>
<equation confidence="0.9957065">
man(x) agent(x, e) result()  |w
drive(e) agent(x, e) result()  |w
drive(e) patient(e, x) result()  |w
car(x) patient(e, x) result()  |w
</equation>
<bodyText confidence="0.998191380952381">
This implementation loses a lot of information,
for example it does not differentiate between A
man is walking and a woman is driving and A
man is driving and a woman is walking. In fact,
logical form without variable binding degrades to a
representation similar to a set of independent syn-
tactic dependencies, 4 while the average combiner
with variable binding retains all of the information
in the original logical form. Still, omitting variable
binding turns out to be useful for the STS task.
It is also worth commenting on the efficiency of
the inference algorithm when run on the three dif-
ferent approaches to combining evidence. The aver-
age combiner without variable binding is the fastest
and has the least memory requirements because all
cliques in the ground network are of limited size
(just 3 or 4 nodes). Deterministic AND is much
slower than the average combiner without variable
binding, because the maximum clique size depends
on the sentence. The average combiner with vari-
able binding is the most memory intensive since the
</bodyText>
<page confidence="0.971428">
3
</page>
<bodyText confidence="0.991274666666667">
One could also give mini-clauses different weights depend-
ing on their importance, but we have not experimented with this
so far.
</bodyText>
<page confidence="0.946149">
4
</page>
<bodyText confidence="0.8622865">
However, it is not completely the same since we do not
divide up formulas under negation into mini-clauses.
</bodyText>
<page confidence="0.964227">
15
</page>
<bodyText confidence="0.997386115384615">
\x0cnumber of arguments of the result() predicate can
become large (there is an argument for each individ-
ual and event in the sentence). Consequently, the
inference algorithm needs to consider a combinato-
rial number of possible groundings of the result()
predicate, making inference very slow.
Adaptation of the logical form. As discussed by
Garrette et al. (2011), Boxers output is mapped to
logical form and augmented with additional infor-
mation to handle a variety of semantic phenomena.
However, we do not use their additional rules for
handling implicatives and factives, as we wanted to
test the system without background knowledge be-
yond that supplied by the vector space.
Unfortunately, current MLN inference algorithms
are not able to efficiently handle complex formu-
las with nested quantifiers. For that reason, we re-
placed universal quantifiers in Boxers output with
existentials since they caused serious problems for
Alchemy. Although this is a radical change to the
semantics of the logical form, due to the nature of
the STS and RTE data, it only effects about 5% of
the sentences, and we found that most of the uni-
versal quantifiers in these cases were actually due
to parsing errors. We are currently exploring more
effective ways of dealing with this issue.
</bodyText>
<sectionHeader confidence="0.87375" genericHeader="method">
4 Task 1: Recognizing Textual Entailment
</sectionHeader>
<subsectionHeader confidence="0.909979">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.9877441">
In order to compare directly to the logic-based sys-
tem of Bos and Markert (2005), we focus on the
RTE-1 dataset (Dagan et al., 2005), which includes
567 Text-Hypothesis (T-H) pairs in the development
set and 800 pairs in the test set. The data covers a
wide range of issues in entailment, including lexical,
syntactic, logical, world knowledge, and combina-
tions of these at different levels of difficulty. In both
development and test sets, 50% of sentence pairs are
true entailments and 50% are not.
</bodyText>
<subsectionHeader confidence="0.995686">
4.2 Method
</subsectionHeader>
<bodyText confidence="0.992336975">
We run our system for different configurations of in-
ference rules and evidence combiners. For distri-
butional inference rules (DIR), three different lev-
els are tested: without inference rules (no DIR),
inference rules for individual words (word DIR),
and inference rules for words and phrases (phrase
DIR). Phrase vectors were built using vector addi-
tion, as point-wise multiplication performed slightly
worse. To combine evidence for the result() query,
three different options are available: without av-
erage combiner which is just using Deterministic
AND (Deterministic AND), average combiner with
variable binding (AvgComb) and average combiner
without variable binding (AvgComb w/o VarBind).
Different combinations of configurations are tested
according to its suitability for the task; RTE and
STS.
We also tested several distributional only sys-
tems. The first such system builds a vector represen-
tation for each sentence by adding its word vectors,
then computes the cosine similarity between the sen-
tence vectors for S1 and S2 (VS-Add). The second
uses point-wise multiplication instead of vector ad-
dition (VS-Mul). The third scales pairwise words
similarities to the sentence level using weighted av-
erage where weights are inverse document frequen-
cies idf as suggested by Mihalcea et al. (2006) (VS-
Pairwise).
For the RTE task, systems were evaluated using
both accuracy and confidence-weighted score (cws)
as used by Bos and Markert (2005) and the RTE-
1 challenge (Dagan et al., 2005). In order to map
a probability of entailment to a strict prediction of
True or False, we determined a threshold that op-
timizes performance on the development set. The
cws score rewards a systems ability to assign higher
confidence scores to correct predictions than incor-
rect ones. For cws, a systems predictions are sorted
in decreasing order of confidence and the score is
computed as:
</bodyText>
<equation confidence="0.999349875">
cws =
1
n
n
X
i=1
#correct-up-to-rank-i
i
</equation>
<bodyText confidence="0.998602">
where n is the number of the items in the test set,
and i ranges over the sorted items. In our systems,
we defined the confidence value for a T-H pair as
the distance between the computed probability for
the result() predicate and the threshold.
</bodyText>
<subsectionHeader confidence="0.911687">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.99986">
The results are shown in Table 1. They show
that the distributional only baselines perform very
poorly. In particular, they perform worse than strict
</bodyText>
<page confidence="0.994678">
16
</page>
<table confidence="0.995328384615385">
\x0cMethod acc cws
Chance 0.50 0.50
Bos &amp; Markert, strict 0.52 0.55
Best system in RTE-1 challenge
(Bayer et al., 2005)
0.59 0.62
VS-Add 0.49 0.53
VS-Mul 0.51 0.52
VS-Pairwise 0.50 0.50
AvgComb w/o VarBind + phrase
DIR
0.52 0.53
Deterministic AND + phrase DIR 0.57 0.57
</table>
<tableCaption confidence="0.970759">
Table 1: Results on the RTE-1 Test Set.
entailment from Bos and Markert (2005), a system
</tableCaption>
<bodyText confidence="0.983954555555555">
that uses only logic. This illustrates the important
role of logic-based representations for the entail-
ment task. Due to intractable memory demands of
Alchemy inference, our current system with deter-
ministic AND fails to execute on 118 of the 800 test
pairs, so, by default, the system classifies these cases
as False (non-entailing) with very low confidence.
Comparing the two configurations of our system,
using deterministic AND vs. the average combiner
without variable binding (last two lines in Table 1),
we see that for RTE, it is essential to retain the full
logical form.
Our system with deterministic AND obtains both
an accuracy and cws of 0.57. The best result in
the RTE-1 challenge by Bayer et al. (2005) ob-
tained an accuracy of 0.59 and cws of 0.62. 5 In
terms of both accuracy and cws, our system outper-
forms both distributional only systems and strict
logical entailment, showing again that integrating
both logical form and distributional inference rules
using MLNs is beneficial. Interestingly, the strict
entailment system of Bos and Markert incorporated
generic knowledge, lexical knowledge (from Word-
Net) and geographical knowledge that we do not
utilize. This demonstrates the advantage of us-
ing a model that operationalizes entailment between
words and phrases as distributional similarity.
</bodyText>
<page confidence="0.879999">
5
</page>
<table confidence="0.74297225">
On other RTE datasets there are higher previous results.
Hickl (2008) achieves 0.89 accuracy and 0.88 cws on the com-
bined RTE-2 and RTE-3 dataset.
5 Task 2: Semantic Textual Similarity
</table>
<subsectionHeader confidence="0.864229">
5.1 Dataset
</subsectionHeader>
<bodyText confidence="0.9827585">
The dataset we use in our experiments is the MSR
Video Paraphrase Corpus (MSR-Vid) subset of the
STS 2012 task, consisting of 1,500 sentence pairs.
The corpus itself was built by asking annotators
from Amazon Mechanical Turk to describe very
short video fragments (Chen and Dolan, 2011). The
organizers of the STS 2012 task (Agirre et al., 2012)
sampled video descriptions and asked Turkers to as-
sign similarity scores (ranging from 0 to 5) to pairs
of sentences, without access to the video. The gold
standard score is the average of the Turkers annota-
tions. In addition to the MSR Video Paraphrase Cor-
pus subset, the STS 2012 task involved data from
machine translation and sense descriptions. We do
not use these because they do not consist of full
grammatical sentences, which the parser does not
handle well. In addition, the STS 2012 data included
sentences from the MSR Paraphrase Corpus, which
we also do not currently use because some sentences
are long and create intractable MLN inference prob-
lems. This issue is discussed further in section 6.
Following STS standards, our evaluation compares
a systems similarity judgments to the gold standard
scores using Pearsons correlation coefficient r.
</bodyText>
<subsectionHeader confidence="0.999272">
5.2 Method
</subsectionHeader>
<bodyText confidence="0.997070315789474">
Our system can be tested for different configuration
of inference rules and evidence combiners which
are explained in section 4.2. The tested systems on
the STS task are listed in table 2. Out experiments
showed that using average combiner (AvgComb) is
very memory intensive and MLN inference for 28 of
the 1,500 pairs either ran out of memory or did not
finish in reasonable time. In such cases, we back off
to AvgComb w/o VarBind.
We compare to several baselines; our MLN
system without distributional inference rules
(AvgComb + no DIR), and distributional-only
systems (VS-Add, VS-Mul, VS-Pairwise). These
are the natural baselines for our system, since they
use only one of the two types of information that
we combine (i.e. logical form and distributional
representations).
Finally, we built an ensemble that combines the
output of multiple systems using regression trained
</bodyText>
<page confidence="0.99449">
17
</page>
<table confidence="0.766527">
\x0cMethod r
AvgComb + no DIR 0.58
AvgComb + word DIR 0.60
AvgComb + phrase DIR 0.66
AvgComb w/o VarBind + no DIR 0.58
AvgComb w/o VarBind + word DIR 0.60
AvgComb w/o VarBind + phrase DIR 0.73
VS-Add 0.78
VS-Mul 0.58
</table>
<figure confidence="0.9059931">
VS-Pairwise 0.77
Ensemble (VS-Add + VS-Mul + VS-
Pairwise)
0.83
Ensemble ([AvgComb + phrase DIR] +
VS-Add + VS-Mul + VS-Pairwise)
0.85
Best MSR-Vid score in STS 2012 (Bar
et al., 2012)
0.87
</figure>
<tableCaption confidence="0.991277">
Table 2: Results on the STS video dataset.
</tableCaption>
<bodyText confidence="0.9617685">
on the training data. We then compare the perfor-
mance of an ensemble with and without our sys-
tem. This is the same technique used by Bar et al.
(2012) except we used additive regression (Fried-
man, 2002) instead of linear regression since it gave
better results.
</bodyText>
<subsectionHeader confidence="0.584259">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.99877564516129">
Table 2 summarizes the results of our experiments.
They show that adding distributional information
improves results, as expected, and also that adding
phrase rules gives further improvement: Using only
word distributional inference rules improves results
from 0.58 to 0.6, and adding phrase inference rules
further improves them to 0.66. As for variable bind-
ing, note that although it provides more precise in-
formation, the STS scores actually improve when it
is dropped, from 0.66 to 0.73. We offer two expla-
nations for this result: First, this information is very
sensitive to parsing errors, and the C&amp;C parser, on
which Boxer is based, produces many errors on this
dataset, even for simple sentences. When the C&amp;C
CCG parse is wrong, the resulting logical form is
wrong, and the resulting similarity score is greatly
affected. Dropping variable binding makes the sys-
tems more robust to parsing errors. Second, in con-
trast to RTE, the STS dataset does not really test the
important role of syntax and logical form in deter-
mining meaning. This also explains why the dis-
tributional only baselines are actually doing better
than the MLN systems.
Although the MLN system on its own does not
perform better than the distributional compositional
models, it does provide complementary information,
as shown by the fact that ensembling it with the rest
of the models improves performance (0.85 with the
MLN system, compared to 0.83 without it). The per-
formance of this ensemble is close to the current best
result for this dataset (0.87).
</bodyText>
<sectionHeader confidence="0.997873" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999649529411765">
The approach presented in this paper constitutes a
step towards achieving the challenging goal of effec-
tively combining logical representations with dis-
tributional information automatically acquired from
text. In this section, we discuss some of limita-
tions of the current work and directions for future
research.
As noted before, parse errors are currently a sig-
nificant problem. We use Boxer to obtain a logi-
cal representation for a sentence, which in turn re-
lies on the C&amp;C parser. Unfortunately, C&amp;C mis-
parses many sentences, which leads to inaccurate
logical forms. To reduce the impact of misparsing,
we plan to use a version of C&amp;C that can produce
the top-n parses together with parse re-ranking (Ng
and Curran, 2012). As an alternative to re-ranking,
one could obtain logical forms for each of the top-
n parses, and create an MLN that integrates all of
them (together with their certainty) as an underspec-
ified meaning representation that could then be used
to directly support inferences such as STS and RTE.
We also plan to exploit a greater variety of dis-
tributional inference rules. First, we intend to in-
corporate logical form translations of existing dis-
tributional inference rule collections (e.g., (Berant
et al., 2011; Chan et al., 2011)). Another issue
is obtaining improved rule weights based on dis-
tributional phrase vectors. We plan to experiment
with more sophisticated approaches to computing
phrase vectors such as those recently presented by
Baroni and Zamparelli (2010) and Grefenstette and
Sadrzadeh (2011). Furthermore, we are currently
deriving symmetric similarity ratings between word
pairs or phrase pairs, when really what we need is di-
</bodyText>
<page confidence="0.987616">
18
</page>
<bodyText confidence="0.9984845">
\x0crectional similarity. We plan to incorporate directed
similarity measures such as those of Kotlerman et al.
(2010) and Clarke (2012).
A primary problem for our approach is the limita-
tions of existing MLN inference algorithms, which
do not effectively scale to large and complex MLNs.
We plan to explore coarser logical representa-
tions such as Minimal Recursion Semantics (MRS)
(Copestake et al., 2005). Another potential approach
to this problem is to trade expressivity for efficiency.
Domingos and Webb (2012) introduced a tractable
subset of Markov Logic (TML) for which a future
software release is planned. Formulating the infer-
ence problem in TML could potentially allow us to
run our system on longer and more complex sen-
tences.
</bodyText>
<sectionHeader confidence="0.996715" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.997818214285714">
In this paper we have used an approach that com-
bines logic-based and distributional representations
for natural language meaning. It uses logic as
the primary representation, transforms distributional
similarity judgments to weighted inference rules,
and uses Markov Logic Networks to perform in-
ferences over the weighted clauses. This approach
views textual entailment and sentence similarity as
degrees of logical entailment, while at the same
time using distributional similarity as an indicator
of entailment at the word and short phrase level. We
have evaluated the framework on two different tasks,
RTE and STS, finding that it is able to handle both
tasks given that we adapt the way evidence is com-
bined in the MLN. Even though other entailment
models could be applied to STS, given that similar-
ity can obviously be operationalized as a degree of
mutual entailment, this has not been done before to
our best knowledge. Our framework achieves rea-
sonable results on both tasks. On RTE-1 we obtain
an accuracy of 0.57. On STS, we obtain a correla-
tion of r = 0.66 with full logic, r = 0.73 in a system
with weakened variable binding, and r = 0.85 in an
ensemble model. We find that distributional word
and phrase similarity, used as textual inference rules
on the fly, leads to sizeable improvements on both
tasks. We also find that using more flexible proba-
bilistic combinations of evidence is crucial for STS.
</bodyText>
<sectionHeader confidence="0.937875" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.980956166666667">
This research was supported in part by the NSF CA-
REER grant IIS 0845925, by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026, by
MURI ARO grant W911NF-08-1-0242 and by an
NDSEG grant. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the author and do not necessarily reflect
the view of DARPA, AFRL, ARO, DoD or the US
government.
Some of our experiments were run on the
Mastodon Cluster supported by NSF Grant EIA-
0303609.
</bodyText>
<sectionHeader confidence="0.965915" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994386">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
SemEval.
Daniel Bar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In SemEval-2012.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
11831193, Cambridge, MA, October. Association for
Computational Linguistics.
Samuel Bayer, John Burger, Lisa Ferro, John Hender-
son, and Alexander Yeh. 2005. MITREs Submissions
to the EU Pascal RTE Challenge. In In Proceedings
of the PASCAL Challenges Workshop on Recognising
Textual Entailment, pages 4144.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceedings
of EMNLP 2005, pages 628635, Vancouver, B.C.,
Canada.
Johan Bos. 2008. Wide-coverage semantic analysis with
boxer. In Johan Bos and Rodolfo Delmonte, editors,
Semantics in Text Processing. STEP 2008 Conference
Proceedings, Research in Computational Semantics,
pages 277286. College Publications.
Tsz Ping Chan, Chris Callison-Burch, and Benjamin
Van Durme. 2011. Reranking bilingually extracted
</reference>
<page confidence="0.987048">
19
</page>
<reference confidence="0.998826336448599">
\x0cparaphrases using monolingual distributional similar-
ity. In Proceedings of the GEMS 2011 Workshop on
GEometrical Models of Natural Language Semantics,
pages 3342, Edinburgh, UK.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 190200,
Portland, Oregon, USA, June.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of ACL 2004, pages 104111, Barcelona, Spain.
Daoud Clarke. 2012. A context-theoretic framework for
compositionality in distributional semantics. Compu-
tational Linguistics, 38(1).
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A
Sag. 2005. Minimal recursion semantics: An intro-
duction. Research on Language and Computation,
3(2-3):281332.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL Recognising Textual Entailment
Challenge. In In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
pages 18.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Synthesis Lectures on Artificial Intelligence and Ma-
chine Learning. Morgan &amp; Claypool Publishers.
Pedro Domingos and W Austin Webb. 2012. A tractable
first-order probabilistic logic. In Proceedings of the
Twenty-Sixth National Conference on Artificial Intel-
ligence.
Katrin Erk and Sebastian Pado. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP 2008, pages 897906, Honolulu,
HI.
Jerome H Friedman. 2002. Stochastic gradient boosting.
Computational Statistics &amp; Data Analysis, 38(4):367
378.
Dan Garrette, Katrin Erk, and Raymond Mooney. 2011.
Integrating logical representations with probabilistic
information using markov logic. In Proceedings of
IWCS, Oxford, UK.
Dan Garrette, Katrin Erk, and Raymond Mooney. 2013.
A formal approach to linking logical form and vector-
space lexical semantics. In Harry Bunt, Johan Bos,
and Stephen Pulman, editors, Computing Meaning,
Vol. 4.
Lise Getoor and Ben Taskar, editors. 2007. Introduction
to Statistical Relational Learning. MIT Press, Cam-
bridge, MA.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edi-
tion. http://www.ldc.upenn.edu/
Catalog/CatalogEntry.jsp?catalogId=
LDC2007T07.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
EMNLP, Edinburgh, Scotland, UK.
Andrew Hickl. 2008. Using Discourse Commitments
to Recognize Textual Entailment. In Proceedings of
COLING 2008, pages 337344.
Jerry R. Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(12):69142.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic; An Introduction to Modeltheoretic Semantics of
Natural Language, Formal Logic and DRT. Kluwer,
Dordrecht.
Stanley Kok, Parag Singla, Matthew Richardson, and Pe-
dro Domingos. 2005. The Alchemy system for sta-
tistical relational AI. Technical report, Department
of Computer Science and Engineering, University
of Washington. http://www.cs.washington.
edu/ai/alchemy.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(04):359389.
Thomas Landauer and Susan Dumais. 1997. A solution
to Platos problem: the latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review, 104(2):211240.
Dekang Lin and Patrick Pantel. 2001a. DIRT - discovery
of inference rules from text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 323328.
Dekang Lin and Patrick Pantel. 2001b. Discovery of
inference rules for question answering. Natural Lan-
guage Engineering, 7(4):343360.
Will Lowe. 2001. Towards a theory of semantic space.
In Proceedings of the Cognitive Science Society, pages
576581.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28:203208.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the na-
tional conference on artificial intelligence, volume 21,
page 775. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236244.
</reference>
<page confidence="0.796661">
20
</page>
<reference confidence="0.999509434782609">
\x0cJeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):13881429.
Richard Montague. 1970. Universal grammar. Theoria,
36:373398. Reprinted in Thomason (1974), pp 7-27.
Sriraam Natarajan, Tushar Khot, Daniel Lowd, Prasad
Tadepalli, Kristian Kersting, and Jude Shavlik. 2010.
Exploiting causal independence in markov logic net-
works: Combining undirected and directed models.
In Proceedings of European Conference in Machine
Learning (ECML), Barcelona, Spain.
Dominick Ng and James R Curran. 2012. Dependency
hashing for n-best ccg parsing. In In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62:107
136.
Hinrich Schutze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1).
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew Ng,
and Christopher Manning. 2011. Dynamic pooling
and unfolding recursive autoencoders for paraphrase
detection. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett,
F.C.N. Pereira, and K.Q. Weinberger, editors, Pro-
ceedings of NIPS.
Asher Stern, Amnon Lotan, Shachar Mirkin, Eyal
Shnarch, Lili Kotlerman, Jonathan Berant, and Ido Da-
gan. 2011. Knowledge and tree-edits in learnable en-
tailment proofs. In TAC, Gathersburg, MD.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Stefan Thater, Hagen Furstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of ACL 2010, pages 948957, Uppsala, Sweden.
Richmond H. Thomason, editor. 1974. Formal Philoso-
phy. Selected Papers of Richard Montague. Yale Uni-
versity Press, New Haven.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141188.
</reference>
<page confidence="0.973731">
21
</page>
<figure confidence="0.255275">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.150161">
<note confidence="0.976261666666667">b&amp;apos;Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 1121, Atlanta, Georgia, June 13-14, 2013. c 2013 Association for Computational Linguistics</note>
<title confidence="0.631343">Montague Meets Markov: Deep Semantics with Probabilistic Logical Form</title>
<author confidence="0.8458158">Gemma Boleda</author>
<email confidence="0.468148">,</email>
<author confidence="0.997916">Raymond Mooney</author>
<affiliation confidence="0.999781666666667">Department of Computer Science Department of Linguistics The University of Texas at Austin</affiliation>
<address confidence="0.997711">Austin, Texas 78712</address>
<email confidence="0.9979255">beltagy@cs.utexas.edugemma.boleda@utcompling.com,katrin.erk@mail.utexas.edu</email>
<email confidence="0.9979255">ckcuong@cs.utexas.edugemma.boleda@utcompling.com,katrin.erk@mail.utexas.edu</email>
<email confidence="0.9979255">dhg@cs.utexas.edugemma.boleda@utcompling.com,katrin.erk@mail.utexas.edu</email>
<email confidence="0.9979255">mooney@cs.utexas.edugemma.boleda@utcompling.com,katrin.erk@mail.utexas.edu</email>
<abstract confidence="0.999401307692308">We combine logical and distributional representations of natural language meaning by transforming distributional similarity judgments into weighted inference rules using Markov Logic Networks (MLNs). We show that this framework supports both judging sentence similarity and recognizing textual entailment by appropriately adapting the MLN implementation of logical connectives. We also show that distributional phrase similarity, used as textual inference rules created on the fly, improves its performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<contexts>
<context position="9095" citStr="Agirre et al., 2012" startWordPosition="1390" endWordPosition="1393">fidential document Here, h is not entailed. RTE directly tests whether a system can construct semantic representations that allow it to draw correct inferences. Of existing RTE approaches, the closest to ours is by Bos and Markert (2005), who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover. By contrast, our approach uses Markov logic with probabilistic inference. Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations. The best performer in 2012s competition was by Bar et al. (2012), an ensemble system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics. Weighted inference, and combined structuraldistributional representations One approach to weighted inference in NLP is that of Hobbs et al. (1993), who proposed viewing natural language interpretation as abductive inference. In this framework, problems like reference resolution and syntactic ambig</context>
<context position="26977" citStr="Agirre et al., 2012" startWordPosition="4357" endWordPosition="4360">onalizes entailment between words and phrases as distributional similarity. 5 On other RTE datasets there are higher previous results. Hickl (2008) achieves 0.89 accuracy and 0.88 cws on the combined RTE-2 and RTE-3 dataset. 5 Task 2: Semantic Textual Similarity 5.1 Dataset The dataset we use in our experiments is the MSR Video Paraphrase Corpus (MSR-Vid) subset of the STS 2012 task, consisting of 1,500 sentence pairs. The corpus itself was built by asking annotators from Amazon Mechanical Turk to describe very short video fragments (Chen and Dolan, 2011). The organizers of the STS 2012 task (Agirre et al., 2012) sampled video descriptions and asked Turkers to assign similarity scores (ranging from 0 to 5) to pairs of sentences, without access to the video. The gold standard score is the average of the Turkers annotations. In addition to the MSR Video Paraphrase Corpus subset, the STS 2012 task involved data from machine translation and sense descriptions. We do not use these because they do not consist of full grammatical sentences, which the parser does not handle well. In addition, the STS 2012 data included sentences from the MSR Paraphrase Corpus, which we also do not currently use because some s</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Bar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>UKP: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In SemEval-2012.</booktitle>
<contexts>
<context position="9228" citStr="Bar et al. (2012)" startWordPosition="1412" endWordPosition="1415">draw correct inferences. Of existing RTE approaches, the closest to ours is by Bos and Markert (2005), who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover. By contrast, our approach uses Markov logic with probabilistic inference. Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations. The best performer in 2012s competition was by Bar et al. (2012), an ensemble system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics. Weighted inference, and combined structuraldistributional representations One approach to weighted inference in NLP is that of Hobbs et al. (1993), who proposed viewing natural language interpretation as abductive inference. In this framework, problems like reference resolution and syntactic ambiguity resolution become inferences to best explanations that are associated with costs. However, this leaves open the question of how </context>
<context position="29130" citStr="Bar et al., 2012" startWordPosition="4714" endWordPosition="4717">e only one of the two types of information that we combine (i.e. logical form and distributional representations). Finally, we built an ensemble that combines the output of multiple systems using regression trained 17 \x0cMethod r AvgComb + no DIR 0.58 AvgComb + word DIR 0.60 AvgComb + phrase DIR 0.66 AvgComb w/o VarBind + no DIR 0.58 AvgComb w/o VarBind + word DIR 0.60 AvgComb w/o VarBind + phrase DIR 0.73 VS-Add 0.78 VS-Mul 0.58 VS-Pairwise 0.77 Ensemble (VS-Add + VS-Mul + VSPairwise) 0.83 Ensemble ([AvgComb + phrase DIR] + VS-Add + VS-Mul + VS-Pairwise) 0.85 Best MSR-Vid score in STS 2012 (Bar et al., 2012) 0.87 Table 2: Results on the STS video dataset. on the training data. We then compare the performance of an ensemble with and without our system. This is the same technique used by Bar et al. (2012) except we used additive regression (Friedman, 2002) instead of linear regression since it gave better results. 5.3 Results Table 2 summarizes the results of our experiments. They show that adding distributional information improves results, as expected, and also that adding phrase rules gives further improvement: Using only word distributional inference rules improves results from 0.58 to 0.6, and</context>
</contexts>
<marker>Bar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel Bar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. UKP: Computing semantic textual similarity by combining multiple content similarity measures. In SemEval-2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>11831193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="6518" citStr="Baroni and Zamparelli, 2010" startWordPosition="989" endWordPosition="992"> the similarity of vectors representing the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&amp;C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly </context>
<context position="11045" citStr="Baroni and Zamparelli (2010)" startWordPosition="1682" endWordPosition="1685">n concepts but have no notion of logical operators or entailment. There are also other methods that combine distributional and structured representations. Stern et al. (2011) conceptualize textual entailment as tree rewriting of syntactic graphs, where some rewriting rules are distributional inference rules. Socher et al. (2011) recognize paraphrases using a tree of vectors, a phrase structure tree in which each constituent is associated with a vector, and overall sentence similarity is computed by a classifier that integrates all pairwise similarities. (This is in contrast to approaches like Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011), who do not offer a proposal for using vectors at multiple levels in a syntactic tree simultaneously.) 3 MLN system Our system extends that of Garrette et al. (2011; 2013) to support larger-scale evaluation on standard benchmarks for both RTE and STS. We conceptualize both tasks as probabilistic entailment in Markov logic, where STS is judged as the average degree of mutual entailment, i.e. we compute the probability of both S1 |= S2 and S2 |= S1 and average the results. Below are some sentence pairs that we use as examples in the discussion below: (2) S1</context>
<context position="32497" citStr="Baroni and Zamparelli (2010)" startWordPosition="5267" endWordPosition="5270">together with their certainty) as an underspecified meaning representation that could then be used to directly support inferences such as STS and RTE. We also plan to exploit a greater variety of distributional inference rules. First, we intend to incorporate logical form translations of existing distributional inference rule collections (e.g., (Berant et al., 2011; Chan et al., 2011)). Another issue is obtaining improved rule weights based on distributional phrase vectors. We plan to experiment with more sophisticated approaches to computing phrase vectors such as those recently presented by Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011). Furthermore, we are currently deriving symmetric similarity ratings between word pairs or phrase pairs, when really what we need is di18 \x0crectional similarity. We plan to incorporate directed similarity measures such as those of Kotlerman et al. (2010) and Clarke (2012). A primary problem for our approach is the limitations of existing MLN inference algorithms, which do not effectively scale to large and complex MLNs. We plan to explore coarser logical representations such as Minimal Recursion Semantics (MRS) (Copestake et al., 2005). Another potentia</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 11831193, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Bayer</author>
<author>John Burger</author>
<author>Lisa Ferro</author>
<author>John Henderson</author>
<author>Alexander Yeh</author>
</authors>
<title>MITREs Submissions to the EU Pascal RTE Challenge. In</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<pages>4144</pages>
<contexts>
<context position="24878" citStr="Bayer et al., 2005" startWordPosition="4015" endWordPosition="4018">er of confidence and the score is computed as: cws = 1 n n X i=1 #correct-up-to-rank-i i where n is the number of the items in the test set, and i ranges over the sorted items. In our systems, we defined the confidence value for a T-H pair as the distance between the computed probability for the result() predicate and the threshold. 4.3 Results The results are shown in Table 1. They show that the distributional only baselines perform very poorly. In particular, they perform worse than strict 16 \x0cMethod acc cws Chance 0.50 0.50 Bos &amp; Markert, strict 0.52 0.55 Best system in RTE-1 challenge (Bayer et al., 2005) 0.59 0.62 VS-Add 0.49 0.53 VS-Mul 0.51 0.52 VS-Pairwise 0.50 0.50 AvgComb w/o VarBind + phrase DIR 0.52 0.53 Deterministic AND + phrase DIR 0.57 0.57 Table 1: Results on the RTE-1 Test Set. entailment from Bos and Markert (2005), a system that uses only logic. This illustrates the important role of logic-based representations for the entailment task. Due to intractable memory demands of Alchemy inference, our current system with deterministic AND fails to execute on 118 of the 800 test pairs, so, by default, the system classifies these cases as False (non-entailing) with very low confidence. </context>
</contexts>
<marker>Bayer, Burger, Ferro, Henderson, Yeh, 2005</marker>
<rawString>Samuel Bayer, John Burger, Lisa Ferro, John Henderson, and Alexander Yeh. 2005. MITREs Submissions to the EU Pascal RTE Challenge. In In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment, pages 4144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Global learning of typed entailment rules.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Portland, OR.</location>
<contexts>
<context position="15195" citStr="Berant et al., 2011" startWordPosition="2399" endWordPosition="2402"> we make use of existing approaches that compute distributional representations for phrases. In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition (Landauer and Dumais, 1997) or component-wise multiplication (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). The inference-rule weight, wt(p1, p2), for two phrases p1 and p2 is then determined using Eq. (5) in the same way as for words. Existing approaches that derive phrasal inference rules from distributional similarity (Lin and Pantel, 2001a; Szpektor and Dagan, 2008; Berant et al., 2011) precompile large lists of inference rules. By comparison, distributional phrase similarity can be seen as a generator of inference rules on the fly, as it is possible to compute distributional phrase vectors for arbitrary phrases on demand as they are needed for particular examples. Inference rules are generated for all pairs of constituents (c1, c2) where c1 S1 and c2 S2, a constituent is a single word or a phrase. The logical form provides a handy way to extract phrases, as they are generally mapped to one of two logical constructs. Either we have multiple single-variable predicates operati</context>
<context position="32236" citStr="Berant et al., 2011" startWordPosition="5228" endWordPosition="5231">to use a version of C&amp;C that can produce the top-n parses together with parse re-ranking (Ng and Curran, 2012). As an alternative to re-ranking, one could obtain logical forms for each of the topn parses, and create an MLN that integrates all of them (together with their certainty) as an underspecified meaning representation that could then be used to directly support inferences such as STS and RTE. We also plan to exploit a greater variety of distributional inference rules. First, we intend to incorporate logical form translations of existing distributional inference rule collections (e.g., (Berant et al., 2011; Chan et al., 2011)). Another issue is obtaining improved rule weights based on distributional phrase vectors. We plan to experiment with more sophisticated approaches to computing phrase vectors such as those recently presented by Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011). Furthermore, we are currently deriving symmetric similarity ratings between word pairs or phrase pairs, when really what we need is di18 \x0crectional similarity. We plan to incorporate directed similarity measures such as those of Kotlerman et al. (2010) and Clarke (2012). A primary problem for ou</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2011</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Global learning of typed entailment rules. In Proceedings of ACL, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising textual entailment with logical inference.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP 2005,</booktitle>
<pages>628635</pages>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="8712" citStr="Bos and Markert (2005)" startWordPosition="1324" endWordPosition="1328">omputing the probability of a query literal given a set of weighted clauses as background knowledge and evidence. Tasks: RTE and STS Recognizing Textual Entailment (RTE) is the task of determining whether one natural language text, the premise, implies another, the hypothesis. Consider (1) below. (1) p: Oracle had fought to keep the forms from being released h: Oracle released a confidential document Here, h is not entailed. RTE directly tests whether a system can construct semantic representations that allow it to draw correct inferences. Of existing RTE approaches, the closest to ours is by Bos and Markert (2005), who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover. By contrast, our approach uses Markov logic with probabilistic inference. Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations. The best performer in 2012s competition was by Bar et al. (2012), an ensemble system that integrates many techniques including string similarity, n-</context>
<context position="10258" citStr="Bos and Markert (2005)" startWordPosition="1564" endWordPosition="1567">ork, problems like reference resolution and syntactic ambiguity resolution become inferences to best explanations that are associated with costs. However, this leaves open the question of how costs are assigned. Raina et al. (2005) use this framework for RTE, deriving inference costs from WordNet similarity and properties of the syntactic parse. Garrette et al. (2011; 2013) proposed an approach to RTE that uses MLNs to combine traditional logical representations with distributional information in order to support probabilistic textual inference. This approach can be viewed as a bridge between Bos and Markert (2005)s purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators or entailment. There are also other methods that combine distributional and structured representations. Stern et al. (2011) conceptualize textual entailment as tree rewriting of syntactic graphs, where some rewriting rules are distributional inference rules. Socher et al. (2011) recognize paraphrases using a tree of vectors, a phrase structure tree in which each constituent is associated</context>
<context position="21995" citStr="Bos and Markert (2005)" startWordPosition="3548" endWordPosition="3551">sted quantifiers. For that reason, we replaced universal quantifiers in Boxers output with existentials since they caused serious problems for Alchemy. Although this is a radical change to the semantics of the logical form, due to the nature of the STS and RTE data, it only effects about 5% of the sentences, and we found that most of the universal quantifiers in these cases were actually due to parsing errors. We are currently exploring more effective ways of dealing with this issue. 4 Task 1: Recognizing Textual Entailment 4.1 Dataset In order to compare directly to the logic-based system of Bos and Markert (2005), we focus on the RTE-1 dataset (Dagan et al., 2005), which includes 567 Text-Hypothesis (T-H) pairs in the development set and 800 pairs in the test set. The data covers a wide range of issues in entailment, including lexical, syntactic, logical, world knowledge, and combinations of these at different levels of difficulty. In both development and test sets, 50% of sentence pairs are true entailments and 50% are not. 4.2 Method We run our system for different configurations of inference rules and evidence combiners. For distributional inference rules (DIR), three different levels are tested: w</context>
<context position="23873" citStr="Bos and Markert (2005)" startWordPosition="3839" endWordPosition="3842">l distributional only systems. The first such system builds a vector representation for each sentence by adding its word vectors, then computes the cosine similarity between the sentence vectors for S1 and S2 (VS-Add). The second uses point-wise multiplication instead of vector addition (VS-Mul). The third scales pairwise words similarities to the sentence level using weighted average where weights are inverse document frequencies idf as suggested by Mihalcea et al. (2006) (VSPairwise). For the RTE task, systems were evaluated using both accuracy and confidence-weighted score (cws) as used by Bos and Markert (2005) and the RTE1 challenge (Dagan et al., 2005). In order to map a probability of entailment to a strict prediction of True or False, we determined a threshold that optimizes performance on the development set. The cws score rewards a systems ability to assign higher confidence scores to correct predictions than incorrect ones. For cws, a systems predictions are sorted in decreasing order of confidence and the score is computed as: cws = 1 n n X i=1 #correct-up-to-rank-i i where n is the number of the items in the test set, and i ranges over the sorted items. In our systems, we defined the confid</context>
<context position="25107" citStr="Bos and Markert (2005)" startWordPosition="4055" endWordPosition="4058">or a T-H pair as the distance between the computed probability for the result() predicate and the threshold. 4.3 Results The results are shown in Table 1. They show that the distributional only baselines perform very poorly. In particular, they perform worse than strict 16 \x0cMethod acc cws Chance 0.50 0.50 Bos &amp; Markert, strict 0.52 0.55 Best system in RTE-1 challenge (Bayer et al., 2005) 0.59 0.62 VS-Add 0.49 0.53 VS-Mul 0.51 0.52 VS-Pairwise 0.50 0.50 AvgComb w/o VarBind + phrase DIR 0.52 0.53 Deterministic AND + phrase DIR 0.57 0.57 Table 1: Results on the RTE-1 Test Set. entailment from Bos and Markert (2005), a system that uses only logic. This illustrates the important role of logic-based representations for the entailment task. Due to intractable memory demands of Alchemy inference, our current system with deterministic AND fails to execute on 118 of the 800 test pairs, so, by default, the system classifies these cases as False (non-entailing) with very low confidence. Comparing the two configurations of our system, using deterministic AND vs. the average combiner without variable binding (last two lines in Table 1), we see that for RTE, it is essential to retain the full logical form. Our syst</context>
</contexts>
<marker>Bos, Markert, 2005</marker>
<rawString>Johan Bos and Katja Markert. 2005. Recognising textual entailment with logical inference. In Proceedings of EMNLP 2005, pages 628635, Vancouver, B.C., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with boxer.</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics,</booktitle>
<pages>277286</pages>
<editor>In Johan Bos and Rodolfo Delmonte, editors,</editor>
<publisher>College Publications.</publisher>
<contexts>
<context position="6608" citStr="Bos, 2008" startWordPosition="1001" endWordPosition="1002">nd Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&amp;C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models. MLNs are well suited for our app</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-coverage semantic analysis with boxer. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics, pages 277286. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsz Ping Chan</author>
<author>Chris Callison-Burch</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Reranking bilingually extracted \x0cparaphrases using monolingual distributional similarity.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>3342</pages>
<location>Edinburgh, UK.</location>
<marker>Chan, Callison-Burch, Van Durme, 2011</marker>
<rawString>Tsz Ping Chan, Chris Callison-Burch, and Benjamin Van Durme. 2011. Reranking bilingually extracted \x0cparaphrases using monolingual distributional similarity. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 3342, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>William B Dolan</author>
</authors>
<title>Collecting highly parallel data for paraphrase evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>190200</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="26918" citStr="Chen and Dolan, 2011" startWordPosition="4346" endWordPosition="4349">his demonstrates the advantage of using a model that operationalizes entailment between words and phrases as distributional similarity. 5 On other RTE datasets there are higher previous results. Hickl (2008) achieves 0.89 accuracy and 0.88 cws on the combined RTE-2 and RTE-3 dataset. 5 Task 2: Semantic Textual Similarity 5.1 Dataset The dataset we use in our experiments is the MSR Video Paraphrase Corpus (MSR-Vid) subset of the STS 2012 task, consisting of 1,500 sentence pairs. The corpus itself was built by asking annotators from Amazon Mechanical Turk to describe very short video fragments (Chen and Dolan, 2011). The organizers of the STS 2012 task (Agirre et al., 2012) sampled video descriptions and asked Turkers to assign similarity scores (ranging from 0 to 5) to pairs of sentences, without access to the video. The gold standard score is the average of the Turkers annotations. In addition to the MSR Video Paraphrase Corpus subset, the STS 2012 task involved data from machine translation and sense descriptions. We do not use these because they do not consist of full grammatical sentences, which the parser does not handle well. In addition, the STS 2012 data included sentences from the MSR Paraphras</context>
</contexts>
<marker>Chen, Dolan, 2011</marker>
<rawString>David L. Chen and William B. Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 190200, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL 2004,</booktitle>
<pages>104111</pages>
<location>Barcelona,</location>
<contexts>
<context position="6817" citStr="Clark and Curran, 2004" startWordPosition="1030" endWordPosition="1033">(Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&amp;C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models. MLNs are well suited for our approach since they provide an elegant framework for assigning weights to first-order logical rules, combining a diverse set of inference rules and performing sound probabilistic inference. An MLN consists of a s</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCG and log-linear models. In Proceedings of ACL 2004, pages 104111, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>A context-theoretic framework for compositionality in distributional semantics.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="32810" citStr="Clarke (2012)" startWordPosition="5316" endWordPosition="5317">collections (e.g., (Berant et al., 2011; Chan et al., 2011)). Another issue is obtaining improved rule weights based on distributional phrase vectors. We plan to experiment with more sophisticated approaches to computing phrase vectors such as those recently presented by Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011). Furthermore, we are currently deriving symmetric similarity ratings between word pairs or phrase pairs, when really what we need is di18 \x0crectional similarity. We plan to incorporate directed similarity measures such as those of Kotlerman et al. (2010) and Clarke (2012). A primary problem for our approach is the limitations of existing MLN inference algorithms, which do not effectively scale to large and complex MLNs. We plan to explore coarser logical representations such as Minimal Recursion Semantics (MRS) (Copestake et al., 2005). Another potential approach to this problem is to trade expressivity for efficiency. Domingos and Webb (2012) introduced a tractable subset of Markov Logic (TML) for which a future software release is planned. Formulating the inference problem in TML could potentially allow us to run our system on longer and more complex sentenc</context>
</contexts>
<marker>Clarke, 2012</marker>
<rawString>Daoud Clarke. 2012. A context-theoretic framework for compositionality in distributional semantics. Computational Linguistics, 38(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2005</date>
<booktitle>Research on Language and Computation,</booktitle>
<pages>3--2</pages>
<contexts>
<context position="33079" citStr="Copestake et al., 2005" startWordPosition="5357" endWordPosition="5360">presented by Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011). Furthermore, we are currently deriving symmetric similarity ratings between word pairs or phrase pairs, when really what we need is di18 \x0crectional similarity. We plan to incorporate directed similarity measures such as those of Kotlerman et al. (2010) and Clarke (2012). A primary problem for our approach is the limitations of existing MLN inference algorithms, which do not effectively scale to large and complex MLNs. We plan to explore coarser logical representations such as Minimal Recursion Semantics (MRS) (Copestake et al., 2005). Another potential approach to this problem is to trade expressivity for efficiency. Domingos and Webb (2012) introduced a tractable subset of Markov Logic (TML) for which a future software release is planned. Formulating the inference problem in TML could potentially allow us to run our system on longer and more complex sentences. 7 Conclusion In this paper we have used an approach that combines logic-based and distributional representations for natural language meaning. It uses logic as the primary representation, transforms distributional similarity judgments to weighted inference rules, a</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A Sag. 2005. Minimal recursion semantics: An introduction. Research on Language and Computation, 3(2-3):281332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge. In</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<pages>18</pages>
<contexts>
<context position="22047" citStr="Dagan et al., 2005" startWordPosition="3558" endWordPosition="3561">l quantifiers in Boxers output with existentials since they caused serious problems for Alchemy. Although this is a radical change to the semantics of the logical form, due to the nature of the STS and RTE data, it only effects about 5% of the sentences, and we found that most of the universal quantifiers in these cases were actually due to parsing errors. We are currently exploring more effective ways of dealing with this issue. 4 Task 1: Recognizing Textual Entailment 4.1 Dataset In order to compare directly to the logic-based system of Bos and Markert (2005), we focus on the RTE-1 dataset (Dagan et al., 2005), which includes 567 Text-Hypothesis (T-H) pairs in the development set and 800 pairs in the test set. The data covers a wide range of issues in entailment, including lexical, syntactic, logical, world knowledge, and combinations of these at different levels of difficulty. In both development and test sets, 50% of sentence pairs are true entailments and 50% are not. 4.2 Method We run our system for different configurations of inference rules and evidence combiners. For distributional inference rules (DIR), three different levels are tested: without inference rules (no DIR), inference rules for</context>
<context position="23917" citStr="Dagan et al., 2005" startWordPosition="3848" endWordPosition="3851">ystem builds a vector representation for each sentence by adding its word vectors, then computes the cosine similarity between the sentence vectors for S1 and S2 (VS-Add). The second uses point-wise multiplication instead of vector addition (VS-Mul). The third scales pairwise words similarities to the sentence level using weighted average where weights are inverse document frequencies idf as suggested by Mihalcea et al. (2006) (VSPairwise). For the RTE task, systems were evaluated using both accuracy and confidence-weighted score (cws) as used by Bos and Markert (2005) and the RTE1 challenge (Dagan et al., 2005). In order to map a probability of entailment to a strict prediction of True or False, we determined a threshold that optimizes performance on the development set. The cws score rewards a systems ability to assign higher confidence scores to correct predictions than incorrect ones. For cws, a systems predictions are sorted in decreasing order of confidence and the score is computed as: cws = 1 n n X i=1 #correct-up-to-rank-i i where n is the number of the items in the test set, and i ranges over the sorted items. In our systems, we defined the confidence value for a T-H pair as the distance be</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL Recognising Textual Entailment Challenge. In In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment, pages 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
<author>Daniel Lowd</author>
</authors>
<title>Markov Logic: An Interface Layer for Artificial Intelligence.</title>
<date>2009</date>
<booktitle>Synthesis Lectures on Artificial Intelligence and Machine Learning.</booktitle>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="3804" citStr="Domingos and Lowd, 2009" startWordPosition="547" endWordPosition="550">knowledge base. However, they only employed single-word distributional similarity rules, and only evaluated on a small 11 \x0cset of short, hand-crafted test sentences. In this paper, we extend Garrette et al.s approach and adapt it to handle two existing semantic tasks: recognizing textual entailment (RTE) and semantic textual similarity (STS). We show how this single semantic framework using probabilistic logical form in Markov logic can be adapted to support both of these important tasks. This is possible because MLNs constitute a flexible programming language based on probabilistic logic (Domingos and Lowd, 2009) that can be easily adapted to support multiple types of linguistically useful inference. At the word and short phrase level, our approach model entailment through distributional similarity (Figure 1). If X and Y occur in similar contexts, we assume that they describe similar entities and thus there is some degree of entailment between them. At the sentence level, however, we hold that a stricter, logic-based view of entailment is beneficial, and we even model sentence similarity (in STS) as entailment. There are two main innovations in the formalism that make it possible for us to work with n</context>
<context position="7051" citStr="Domingos and Lowd, 2009" startWordPosition="1065" endWordPosition="1068">hods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&amp;C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models. MLNs are well suited for our approach since they provide an elegant framework for assigning weights to first-order logical rules, combining a diverse set of inference rules and performing sound probabilistic inference. An MLN consists of a set of weighted first-order clauses. It provides a way of softening first-order logic by allowing situations in which not all clauses are satisfied. More specifically, they provide a well-founded probability distribution across possibl</context>
</contexts>
<marker>Domingos, Lowd, 2009</marker>
<rawString>Pedro Domingos and Daniel Lowd. 2009. Markov Logic: An Interface Layer for Artificial Intelligence. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
<author>W Austin Webb</author>
</authors>
<title>A tractable first-order probabilistic logic.</title>
<date>2012</date>
<booktitle>In Proceedings of the Twenty-Sixth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="33189" citStr="Domingos and Webb (2012)" startWordPosition="5373" endWordPosition="5376">y deriving symmetric similarity ratings between word pairs or phrase pairs, when really what we need is di18 \x0crectional similarity. We plan to incorporate directed similarity measures such as those of Kotlerman et al. (2010) and Clarke (2012). A primary problem for our approach is the limitations of existing MLN inference algorithms, which do not effectively scale to large and complex MLNs. We plan to explore coarser logical representations such as Minimal Recursion Semantics (MRS) (Copestake et al., 2005). Another potential approach to this problem is to trade expressivity for efficiency. Domingos and Webb (2012) introduced a tractable subset of Markov Logic (TML) for which a future software release is planned. Formulating the inference problem in TML could potentially allow us to run our system on longer and more complex sentences. 7 Conclusion In this paper we have used an approach that combines logic-based and distributional representations for natural language meaning. It uses logic as the primary representation, transforms distributional similarity judgments to weighted inference rules, and uses Markov Logic Networks to perform inferences over the weighted clauses. This approach views textual ent</context>
</contexts>
<marker>Domingos, Webb, 2012</marker>
<rawString>Pedro Domingos and W Austin Webb. 2012. A tractable first-order probabilistic logic. In Proceedings of the Twenty-Sixth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pado</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>897906</pages>
<location>Honolulu, HI.</location>
<contexts>
<context position="2462" citStr="Erk and Pado, 2008" startWordPosition="347" endWordPosition="350">some of the criteria. Logic-based representations (Montague, 1970; Kamp and Reyle, 1993) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms. Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Schutze, 1998; Erk and Pado, 2008; Thater et al., 2010). This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form, as illustrated in Figure 1. Finally,</context>
</contexts>
<marker>Erk, Pado, 2008</marker>
<rawString>Katrin Erk and Sebastian Pado. 2008. A structured vector space model for word meaning in context. In Proceedings of EMNLP 2008, pages 897906, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome H Friedman</author>
</authors>
<title>Stochastic gradient boosting.</title>
<date>2002</date>
<journal>Computational Statistics &amp; Data Analysis,</journal>
<volume>38</volume>
<issue>4</issue>
<pages>378</pages>
<contexts>
<context position="29381" citStr="Friedman, 2002" startWordPosition="4762" endWordPosition="4764">AvgComb + word DIR 0.60 AvgComb + phrase DIR 0.66 AvgComb w/o VarBind + no DIR 0.58 AvgComb w/o VarBind + word DIR 0.60 AvgComb w/o VarBind + phrase DIR 0.73 VS-Add 0.78 VS-Mul 0.58 VS-Pairwise 0.77 Ensemble (VS-Add + VS-Mul + VSPairwise) 0.83 Ensemble ([AvgComb + phrase DIR] + VS-Add + VS-Mul + VS-Pairwise) 0.85 Best MSR-Vid score in STS 2012 (Bar et al., 2012) 0.87 Table 2: Results on the STS video dataset. on the training data. We then compare the performance of an ensemble with and without our system. This is the same technique used by Bar et al. (2012) except we used additive regression (Friedman, 2002) instead of linear regression since it gave better results. 5.3 Results Table 2 summarizes the results of our experiments. They show that adding distributional information improves results, as expected, and also that adding phrase rules gives further improvement: Using only word distributional inference rules improves results from 0.58 to 0.6, and adding phrase inference rules further improves them to 0.66. As for variable binding, note that although it provides more precise information, the STS scores actually improve when it is dropped, from 0.66 to 0.73. We offer two explanations for this r</context>
</contexts>
<marker>Friedman, 2002</marker>
<rawString>Jerome H Friedman. 2002. Stochastic gradient boosting. Computational Statistics &amp; Data Analysis, 38(4):367 378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Integrating logical representations with probabilistic information using markov logic.</title>
<date>2011</date>
<booktitle>In Proceedings of IWCS,</booktitle>
<location>Oxford, UK.</location>
<contexts>
<context position="2680" citStr="Garrette et al., 2011" startWordPosition="377" endWordPosition="380">mechanisms. Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Schutze, 1998; Erk and Pado, 2008; Thater et al., 2010). This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form, as illustrated in Figure 1. Finally, Markov Logic Networks (Richardson and Domingos, 2006) (MLNs) are used to perform weighted inference on the resulting knowledge base. However, they only employed single-word distributional similarity rules, and only ev</context>
<context position="10005" citStr="Garrette et al. (2011" startWordPosition="1526" endWordPosition="1529">ion metrics. Weighted inference, and combined structuraldistributional representations One approach to weighted inference in NLP is that of Hobbs et al. (1993), who proposed viewing natural language interpretation as abductive inference. In this framework, problems like reference resolution and syntactic ambiguity resolution become inferences to best explanations that are associated with costs. However, this leaves open the question of how costs are assigned. Raina et al. (2005) use this framework for RTE, deriving inference costs from WordNet similarity and properties of the syntactic parse. Garrette et al. (2011; 2013) proposed an approach to RTE that uses MLNs to combine traditional logical representations with distributional information in order to support probabilistic textual inference. This approach can be viewed as a bridge between Bos and Markert (2005)s purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators or entailment. There are also other methods that combine distributional and structured representations. Stern et al. (2011) conceptualize</context>
<context position="11248" citStr="Garrette et al. (2011" startWordPosition="1717" endWordPosition="1720"> tree rewriting of syntactic graphs, where some rewriting rules are distributional inference rules. Socher et al. (2011) recognize paraphrases using a tree of vectors, a phrase structure tree in which each constituent is associated with a vector, and overall sentence similarity is computed by a classifier that integrates all pairwise similarities. (This is in contrast to approaches like Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011), who do not offer a proposal for using vectors at multiple levels in a syntactic tree simultaneously.) 3 MLN system Our system extends that of Garrette et al. (2011; 2013) to support larger-scale evaluation on standard benchmarks for both RTE and STS. We conceptualize both tasks as probabilistic entailment in Markov logic, where STS is judged as the average degree of mutual entailment, i.e. we compute the probability of both S1 |= S2 and S2 |= S1 and average the results. Below are some sentence pairs that we use as examples in the discussion below: (2) S1: A man is slicing a cucumber. S2: A man is slicing a zucchini. (3) S1: A boy is riding a bicycle. S2: A little boy is riding a bike. (4) S1: A man is driving. S2: A man is driving a car. 13 \x0cSystem o</context>
<context position="20950" citStr="Garrette et al. (2011)" startWordPosition="3375" endWordPosition="3378">ive since the 3 One could also give mini-clauses different weights depending on their importance, but we have not experimented with this so far. 4 However, it is not completely the same since we do not divide up formulas under negation into mini-clauses. 15 \x0cnumber of arguments of the result() predicate can become large (there is an argument for each individual and event in the sentence). Consequently, the inference algorithm needs to consider a combinatorial number of possible groundings of the result() predicate, making inference very slow. Adaptation of the logical form. As discussed by Garrette et al. (2011), Boxers output is mapped to logical form and augmented with additional information to handle a variety of semantic phenomena. However, we do not use their additional rules for handling implicatives and factives, as we wanted to test the system without background knowledge beyond that supplied by the vector space. Unfortunately, current MLN inference algorithms are not able to efficiently handle complex formulas with nested quantifiers. For that reason, we replaced universal quantifiers in Boxers output with existentials since they caused serious problems for Alchemy. Although this is a radica</context>
</contexts>
<marker>Garrette, Erk, Mooney, 2011</marker>
<rawString>Dan Garrette, Katrin Erk, and Raymond Mooney. 2011. Integrating logical representations with probabilistic information using markov logic. In Proceedings of IWCS, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>A formal approach to linking logical form and vectorspace lexical semantics.</title>
<date>2013</date>
<journal>Computing Meaning,</journal>
<volume>4</volume>
<editor>In Harry Bunt, Johan Bos, and Stephen Pulman, editors,</editor>
<marker>Garrette, Erk, Mooney, 2013</marker>
<rawString>Dan Garrette, Katrin Erk, and Raymond Mooney. 2013. A formal approach to linking logical form and vectorspace lexical semantics. In Harry Bunt, Johan Bos, and Stephen Pulman, editors, Computing Meaning, Vol. 4.</rawString>
</citation>
<citation valid="true">
<title>Introduction to Statistical Relational Learning.</title>
<date>2007</date>
<editor>Lise Getoor and Ben Taskar, editors.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>2007</marker>
<rawString>Lise Getoor and Ben Taskar, editors. 2007. Introduction to Statistical Relational Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2007</date>
<booktitle>English Gigaword Third Edition. http://www.ldc.upenn.edu/ Catalog/CatalogEntry.jsp?catalogId= LDC2007T07.</booktitle>
<contexts>
<context position="13988" citStr="Graff et al., 2007" startWordPosition="2203" endWordPosition="2206">we use prior = 3. In the case of sentence pair (2), we generate the inference rule: cucumber(x) zucchini(x) |wt(cuc., zuc.) Such inference rules are generated for all pairs of words (w1, w2) where w1 S1 and w2 S2.1 1 We omit inference rules for words (a, b) where cos(a, b) &amp;lt; for a threshold set to maximize performance on the training data. Low-similarity pairs usually indicate dissimilar words. This removes a sizeable number of rules for STS, while for RTE the tuned threshold was near zero. The distributional model we use contains all lemmas occurring at least 50 times in the Gigaword corpus (Graff et al., 2007) except a list of stop words. The dimensions are the 2,000 most frequent of these words, and cell values are weighted with point-wise mutual information. 2 Phrase-based inference rules. Garrette et al. only considered distributional inference rules for pairs of individual words. We extend their approach to distributional inference rules for pairs of phrases in order to handle cases like (3). To properly estimate the similarity between S1 and S2 in (3), we not only need an inference rule linking bike to bicycle, but also a rule estimating how similar boy is to little boy. To do so, we make use </context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2007</marker>
<rawString>David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2007. English Gigaword Third Edition. http://www.ldc.upenn.edu/ Catalog/CatalogEntry.jsp?catalogId= LDC2007T07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="2656" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="373" endWordPosition="376"> come with standardized inference mechanisms. Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Schutze, 1998; Erk and Pado, 2008; Thater et al., 2010). This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form, as illustrated in Figure 1. Finally, Markov Logic Networks (Richardson and Domingos, 2006) (MLNs) are used to perform weighted inference on the resulting knowledge base. However, they only employed single-word distributional simil</context>
<context position="6553" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="993" endWordPosition="996">presenting the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&amp;C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly encode complex undirected probabili</context>
<context position="11083" citStr="Grefenstette and Sadrzadeh (2011)" startWordPosition="1687" endWordPosition="1690">logical operators or entailment. There are also other methods that combine distributional and structured representations. Stern et al. (2011) conceptualize textual entailment as tree rewriting of syntactic graphs, where some rewriting rules are distributional inference rules. Socher et al. (2011) recognize paraphrases using a tree of vectors, a phrase structure tree in which each constituent is associated with a vector, and overall sentence similarity is computed by a classifier that integrates all pairwise similarities. (This is in contrast to approaches like Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011), who do not offer a proposal for using vectors at multiple levels in a syntactic tree simultaneously.) 3 MLN system Our system extends that of Garrette et al. (2011; 2013) to support larger-scale evaluation on standard benchmarks for both RTE and STS. We conceptualize both tasks as probabilistic entailment in Markov logic, where STS is judged as the average degree of mutual entailment, i.e. we compute the probability of both S1 |= S2 and S2 |= S1 and average the results. Below are some sentence pairs that we use as examples in the discussion below: (2) S1: A man is slicing a cucumber. S2: A m</context>
<context position="32535" citStr="Grefenstette and Sadrzadeh (2011)" startWordPosition="5272" endWordPosition="5275"> an underspecified meaning representation that could then be used to directly support inferences such as STS and RTE. We also plan to exploit a greater variety of distributional inference rules. First, we intend to incorporate logical form translations of existing distributional inference rule collections (e.g., (Berant et al., 2011; Chan et al., 2011)). Another issue is obtaining improved rule weights based on distributional phrase vectors. We plan to experiment with more sophisticated approaches to computing phrase vectors such as those recently presented by Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011). Furthermore, we are currently deriving symmetric similarity ratings between word pairs or phrase pairs, when really what we need is di18 \x0crectional similarity. We plan to incorporate directed similarity measures such as those of Kotlerman et al. (2010) and Clarke (2012). A primary problem for our approach is the limitations of existing MLN inference algorithms, which do not effectively scale to large and complex MLNs. We plan to explore coarser logical representations such as Minimal Recursion Semantics (MRS) (Copestake et al., 2005). Another potential approach to this problem is to trade</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of EMNLP, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
</authors>
<title>Using Discourse Commitments to Recognize Textual Entailment.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>337344</pages>
<contexts>
<context position="26504" citStr="Hickl (2008)" startWordPosition="4279" endWordPosition="4280">s of both accuracy and cws, our system outperforms both distributional only systems and strict logical entailment, showing again that integrating both logical form and distributional inference rules using MLNs is beneficial. Interestingly, the strict entailment system of Bos and Markert incorporated generic knowledge, lexical knowledge (from WordNet) and geographical knowledge that we do not utilize. This demonstrates the advantage of using a model that operationalizes entailment between words and phrases as distributional similarity. 5 On other RTE datasets there are higher previous results. Hickl (2008) achieves 0.89 accuracy and 0.88 cws on the combined RTE-2 and RTE-3 dataset. 5 Task 2: Semantic Textual Similarity 5.1 Dataset The dataset we use in our experiments is the MSR Video Paraphrase Corpus (MSR-Vid) subset of the STS 2012 task, consisting of 1,500 sentence pairs. The corpus itself was built by asking annotators from Amazon Mechanical Turk to describe very short video fragments (Chen and Dolan, 2011). The organizers of the STS 2012 task (Agirre et al., 2012) sampled video descriptions and asked Turkers to assign similarity scores (ranging from 0 to 5) to pairs of sentences, without </context>
</contexts>
<marker>Hickl, 2008</marker>
<rawString>Andrew Hickl. 2008. Using Discourse Commitments to Recognize Textual Entailment. In Proceedings of COLING 2008, pages 337344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Mark Stickel</author>
<author>Douglas Appelt</author>
<author>Paul Martin</author>
</authors>
<title>Interpretation as abduction.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<volume>63</volume>
<issue>12</issue>
<contexts>
<context position="9543" citStr="Hobbs et al. (1993)" startWordPosition="1455" endWordPosition="1458">v logic with probabilistic inference. Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations. The best performer in 2012s competition was by Bar et al. (2012), an ensemble system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics. Weighted inference, and combined structuraldistributional representations One approach to weighted inference in NLP is that of Hobbs et al. (1993), who proposed viewing natural language interpretation as abductive inference. In this framework, problems like reference resolution and syntactic ambiguity resolution become inferences to best explanations that are associated with costs. However, this leaves open the question of how costs are assigned. Raina et al. (2005) use this framework for RTE, deriving inference costs from WordNet similarity and properties of the syntactic parse. Garrette et al. (2011; 2013) proposed an approach to RTE that uses MLNs to combine traditional logical representations with distributional information in order</context>
</contexts>
<marker>Hobbs, Stickel, Appelt, Martin, 1993</marker>
<rawString>Jerry R. Hobbs, Mark Stickel, Douglas Appelt, and Paul Martin. 1993. Interpretation as abduction. Artificial Intelligence, 63(12):69142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<title>From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT.</title>
<date>1993</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="1932" citStr="Kamp and Reyle, 1993" startWordPosition="267" endWordPosition="270">tation of the structure of complex sentences. Some tasks require the ability to recognize near-paraphrases or degrees of similarity between sentences. Some tasks require logical inference, either exact or approximate. Often it is necessary to handle ambiguity and vagueness in meaning. Finally, we frequently want to be able to learn relevant knowledge automatically from corpus data. There is no single representation for natural language meaning at this time that fulfills all requirements. But there are representations that meet some of the criteria. Logic-based representations (Montague, 1970; Kamp and Reyle, 1993) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms. Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Schutze, 1998; Erk and Pado, 2008; Thater et al., 2010). This suggests that distributional models and l</context>
<context position="6759" citStr="Kamp and Reyle, 1993" startWordPosition="1019" endWordPosition="1022">e vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&amp;C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models. MLNs are well suited for our approach since they provide an elegant framework for assigning weights to first-order logical rules, combining a diverse set of inference rules and perfor</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Hans Kamp and Uwe Reyle. 1993. From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Kok</author>
<author>Parag Singla</author>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>The Alchemy system for statistical relational AI.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science and Engineering, University of Washington.</institution>
<note>http://www.cs.washington. edu/ai/alchemy.</note>
<contexts>
<context position="8056" citStr="Kok et al., 2005" startWordPosition="1218" endWordPosition="1221">rder clauses. It provides a way of softening first-order logic by allowing situations in which not all clauses are satisfied. More specifically, they provide a well-founded probability distribution across possible worlds by specifying that the probability of a 12 \x0cworld increases exponentially with the total weight of the logical clauses that it satisfies. While methods exist for learning MLN weights directly from training data, since the appropriate training data is lacking, our approach uses weights computed using distributional semantics. We use the open-source software package Alchemy (Kok et al., 2005) for MLN inference, which allows computing the probability of a query literal given a set of weighted clauses as background knowledge and evidence. Tasks: RTE and STS Recognizing Textual Entailment (RTE) is the task of determining whether one natural language text, the premise, implies another, the hypothesis. Consider (1) below. (1) p: Oracle had fought to keep the forms from being released h: Oracle released a confidential document Here, h is not entailed. RTE directly tests whether a system can construct semantic representations that allow it to draw correct inferences. Of existing RTE appr</context>
</contexts>
<marker>Kok, Singla, Richardson, Domingos, 2005</marker>
<rawString>Stanley Kok, Parag Singla, Matthew Richardson, and Pedro Domingos. 2005. The Alchemy system for statistical relational AI. Technical report, Department of Computer Science and Engineering, University of Washington. http://www.cs.washington. edu/ai/alchemy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>04</issue>
<contexts>
<context position="32792" citStr="Kotlerman et al. (2010)" startWordPosition="5311" endWordPosition="5314">stributional inference rule collections (e.g., (Berant et al., 2011; Chan et al., 2011)). Another issue is obtaining improved rule weights based on distributional phrase vectors. We plan to experiment with more sophisticated approaches to computing phrase vectors such as those recently presented by Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011). Furthermore, we are currently deriving symmetric similarity ratings between word pairs or phrase pairs, when really what we need is di18 \x0crectional similarity. We plan to incorporate directed similarity measures such as those of Kotlerman et al. (2010) and Clarke (2012). A primary problem for our approach is the limitations of existing MLN inference algorithms, which do not effectively scale to large and complex MLNs. We plan to explore coarser logical representations such as Minimal Recursion Semantics (MRS) (Copestake et al., 2005). Another potential approach to this problem is to trade expressivity for efficiency. Domingos and Webb (2012) introduced a tractable subset of Markov Logic (TML) for which a future software release is planned. Formulating the inference problem in TML could potentially allow us to run our system on longer and mo</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(04):359389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Platos problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="2376" citStr="Landauer and Dumais, 1997" startWordPosition="333" endWordPosition="336">meaning at this time that fulfills all requirements. But there are representations that meet some of the criteria. Logic-based representations (Montague, 1970; Kamp and Reyle, 1993) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms. Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Schutze, 1998; Erk and Pado, 2008; Thater et al., 2010). This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation. Distributional similarity between pairs of words is converted into weighted in</context>
<context position="5990" citStr="Landauer and Dumais, 1997" startWordPosition="906" endWordPosition="909">ystem with weakened variable binding, and r = 0.85 in an ensemble model. On RTE-1 we obtain an accuracy of 0.57. We show that the distributional inference rules benefit both tasks and that more flexible probabilistic combinations of evidence are crucial for STS. Although other approaches could be adapted to handle both RTE and STS, we do not know of any other methods that have been explicitly tested on both problems. 2 Related work Distributional semantics Distributional models define the semantic relatedness of words as the similarity of vectors representing the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics</context>
<context position="14820" citStr="Landauer and Dumais, 1997" startWordPosition="2342" endWordPosition="2345"> considered distributional inference rules for pairs of individual words. We extend their approach to distributional inference rules for pairs of phrases in order to handle cases like (3). To properly estimate the similarity between S1 and S2 in (3), we not only need an inference rule linking bike to bicycle, but also a rule estimating how similar boy is to little boy. To do so, we make use of existing approaches that compute distributional representations for phrases. In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition (Landauer and Dumais, 1997) or component-wise multiplication (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). The inference-rule weight, wt(p1, p2), for two phrases p1 and p2 is then determined using Eq. (5) in the same way as for words. Existing approaches that derive phrasal inference rules from distributional similarity (Lin and Pantel, 2001a; Szpektor and Dagan, 2008; Berant et al., 2011) precompile large lists of inference rules. By comparison, distributional phrase similarity can be seen as a generator of inference rules on the fly, as it is possible to compute distributional phrase vectors for arbitrary ph</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to Platos problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT - discovery of inference rules from text. In</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>323328</pages>
<contexts>
<context position="4659" citStr="Lin and Pantel, 2001" startWordPosition="685" endWordPosition="688">xts, we assume that they describe similar entities and thus there is some degree of entailment between them. At the sentence level, however, we hold that a stricter, logic-based view of entailment is beneficial, and we even model sentence similarity (in STS) as entailment. There are two main innovations in the formalism that make it possible for us to work with naturally occurring corpus data. First, we use more expressive distributional inference rules based on the similarity of phrases rather than just individual words. In comparison to existing methods for creating textual inference rules (Lin and Pantel, 2001b; Szpektor and Dagan, 2008), these rules are computed on the fly as needed, rather than pre-compiled. Second, we use more flexible probabilistic combinations of evidence in order to compute degrees of sentence similarity for STS and to help compensate for parser errors. We replace deterministic conjunction by an average combiner, which encodes causal independence (Natarajan et al., 2010). We show that our framework is able to handle both sentence similarity (STS) and textual entailment (RTE) by making some simple adaptations to the MLN when switching between tasks. The framework achieves reas</context>
<context position="12981" citStr="Lin and Pantel, 2001" startWordPosition="2019" endWordPosition="2022">t() where result() is the query for which we have Alchemy compute the probability. However, S2 is not strictly entailed by S1 because of the mismatch between cucumber and zucchini, so with just the strict logical-form translations of S1 and S2, the probability of result() will be zero. This is where we introduce distributional similarity, in this case the similarity of cucumber and zucchini, cos( # cucumber, # zucchini). We create inference rules from such similarities as a form of background knowledge. We then treat similarity as degree of entailment, a move that has a long tradition (e.g., (Lin and Pantel, 2001b; Raina et al., 2005; Szpektor and Dagan, 2008)). In general, given two words a and b, we transform their cosine similarity into an inference-rule weight wt(a, b) using: wt(a, b) = log( cos(# a , # b ) 1 cos(# a , # b ) ) prior (5) Where prior is a negative weight used to initialize all predicates, so that by default facts are assumed to have very low probability. In our experiments, we use prior = 3. In the case of sentence pair (2), we generate the inference rule: cucumber(x) zucchini(x) |wt(cuc., zuc.) Such inference rules are generated for all pairs of words (w1, w2) where w1 S1 and w2 S2</context>
<context position="15146" citStr="Lin and Pantel, 2001" startWordPosition="2391" endWordPosition="2394">ating how similar boy is to little boy. To do so, we make use of existing approaches that compute distributional representations for phrases. In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition (Landauer and Dumais, 1997) or component-wise multiplication (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). The inference-rule weight, wt(p1, p2), for two phrases p1 and p2 is then determined using Eq. (5) in the same way as for words. Existing approaches that derive phrasal inference rules from distributional similarity (Lin and Pantel, 2001a; Szpektor and Dagan, 2008; Berant et al., 2011) precompile large lists of inference rules. By comparison, distributional phrase similarity can be seen as a generator of inference rules on the fly, as it is possible to compute distributional phrase vectors for arbitrary phrases on demand as they are needed for particular examples. Inference rules are generated for all pairs of constituents (c1, c2) where c1 S1 and c2 S2, a constituent is a single word or a phrase. The logical form provides a handy way to extract phrases, as they are generally mapped to one of two logical constructs. Either we</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001a. DIRT - discovery of inference rules from text. In In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 323328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="4659" citStr="Lin and Pantel, 2001" startWordPosition="685" endWordPosition="688">xts, we assume that they describe similar entities and thus there is some degree of entailment between them. At the sentence level, however, we hold that a stricter, logic-based view of entailment is beneficial, and we even model sentence similarity (in STS) as entailment. There are two main innovations in the formalism that make it possible for us to work with naturally occurring corpus data. First, we use more expressive distributional inference rules based on the similarity of phrases rather than just individual words. In comparison to existing methods for creating textual inference rules (Lin and Pantel, 2001b; Szpektor and Dagan, 2008), these rules are computed on the fly as needed, rather than pre-compiled. Second, we use more flexible probabilistic combinations of evidence in order to compute degrees of sentence similarity for STS and to help compensate for parser errors. We replace deterministic conjunction by an average combiner, which encodes causal independence (Natarajan et al., 2010). We show that our framework is able to handle both sentence similarity (STS) and textual entailment (RTE) by making some simple adaptations to the MLN when switching between tasks. The framework achieves reas</context>
<context position="12981" citStr="Lin and Pantel, 2001" startWordPosition="2019" endWordPosition="2022">t() where result() is the query for which we have Alchemy compute the probability. However, S2 is not strictly entailed by S1 because of the mismatch between cucumber and zucchini, so with just the strict logical-form translations of S1 and S2, the probability of result() will be zero. This is where we introduce distributional similarity, in this case the similarity of cucumber and zucchini, cos( # cucumber, # zucchini). We create inference rules from such similarities as a form of background knowledge. We then treat similarity as degree of entailment, a move that has a long tradition (e.g., (Lin and Pantel, 2001b; Raina et al., 2005; Szpektor and Dagan, 2008)). In general, given two words a and b, we transform their cosine similarity into an inference-rule weight wt(a, b) using: wt(a, b) = log( cos(# a , # b ) 1 cos(# a , # b ) ) prior (5) Where prior is a negative weight used to initialize all predicates, so that by default facts are assumed to have very low probability. In our experiments, we use prior = 3. In the case of sentence pair (2), we generate the inference rule: cucumber(x) zucchini(x) |wt(cuc., zuc.) Such inference rules are generated for all pairs of words (w1, w2) where w1 S1 and w2 S2</context>
<context position="15146" citStr="Lin and Pantel, 2001" startWordPosition="2391" endWordPosition="2394">ating how similar boy is to little boy. To do so, we make use of existing approaches that compute distributional representations for phrases. In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition (Landauer and Dumais, 1997) or component-wise multiplication (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). The inference-rule weight, wt(p1, p2), for two phrases p1 and p2 is then determined using Eq. (5) in the same way as for words. Existing approaches that derive phrasal inference rules from distributional similarity (Lin and Pantel, 2001a; Szpektor and Dagan, 2008; Berant et al., 2011) precompile large lists of inference rules. By comparison, distributional phrase similarity can be seen as a generator of inference rules on the fly, as it is possible to compute distributional phrase vectors for arbitrary phrases on demand as they are needed for particular examples. Inference rules are generated for all pairs of constituents (c1, c2) where c1 S1 and c2 S2, a constituent is a single word or a phrase. The logical form provides a handy way to extract phrases, as they are generally mapped to one of two logical constructs. Either we</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001b. Discovery of inference rules for question answering. Natural Language Engineering, 7(4):343360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Lowe</author>
</authors>
<title>Towards a theory of semantic space.</title>
<date>2001</date>
<booktitle>In Proceedings of the Cognitive Science Society,</booktitle>
<pages>576581</pages>
<contexts>
<context position="16197" citStr="Lowe, 2001" startWordPosition="2572" endWordPosition="2573">is a single word or a phrase. The logical form provides a handy way to extract phrases, as they are generally mapped to one of two logical constructs. Either we have multiple single-variable predicates operating on the same variable. For example the phrase a little boy has the logical form boy(x) little(x). Or we have two unary predicates connected with a relation. For example, pizza slice and slice of pizza are both mapped to the 2 It is customary to transform raw counts in a way that captures association between target words and dimensions, for example through point-wise mutual information (Lowe, 2001). 14 \x0clogical form, slice(x0) of(x0, x1) pizza(x1). We consider all binary predicates as relations. Average Combiner to determine similarity in the presence of missing phrases. The logical forms for the sentences in (4): are S1: x0, e1 man(x0)agent(x0, e1)drive(e1) \x01 S2: x0, e1, x2 man(x0) agent(x0, e1) drive(e1) patient(e1, x2) car(x2) \x01 If we try to prove S1 |= S2, the probability of the result() will be zero: There is no evidence for a car, and the hypothesis predicates are conjoined using a deterministic AND. For RTE, this makes sense: If one of the hypothesis predicates is False,</context>
</contexts>
<marker>Lowe, 2001</marker>
<rawString>Will Lowe. 2001. Towards a theory of semantic space. In Proceedings of the Cognitive Science Society, pages 576581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, and Computers,</journal>
<pages>28--203208</pages>
<contexts>
<context position="6015" citStr="Lund and Burgess, 1996" startWordPosition="910" endWordPosition="913">e binding, and r = 0.85 in an ensemble model. On RTE-1 we obtain an accuracy of 0.57. We show that the distributional inference rules benefit both tasks and that more flexible probabilistic combinations of evidence are crucial for STS. Although other approaches could be adapted to handle both RTE and STS, we do not know of any other methods that have been explicitly tested on both problems. 2 Related work Distributional semantics Distributional models define the semantic relatedness of words as the similarity of vectors representing the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a s</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, and Computers, 28:203208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the national conference on artificial intelligence,</booktitle>
<volume>21</volume>
<pages>775</pages>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<contexts>
<context position="23728" citStr="Mihalcea et al. (2006)" startWordPosition="3816" endWordPosition="3819">b w/o VarBind). Different combinations of configurations are tested according to its suitability for the task; RTE and STS. We also tested several distributional only systems. The first such system builds a vector representation for each sentence by adding its word vectors, then computes the cosine similarity between the sentence vectors for S1 and S2 (VS-Add). The second uses point-wise multiplication instead of vector addition (VS-Mul). The third scales pairwise words similarities to the sentence level using weighted average where weights are inverse document frequencies idf as suggested by Mihalcea et al. (2006) (VSPairwise). For the RTE task, systems were evaluated using both accuracy and confidence-weighted score (cws) as used by Bos and Markert (2005) and the RTE1 challenge (Dagan et al., 2005). In order to map a probability of entailment to a strict prediction of True or False, we determined a threshold that optimizes performance on the development set. The cws score rewards a systems ability to assign higher confidence scores to correct predictions than incorrect ones. For cws, a systems predictions are sorted in decreasing order of confidence and the score is computed as: cws = 1 n n X i=1 #cor</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the national conference on artificial intelligence, volume 21, page 775. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>236244</pages>
<contexts>
<context position="6295" citStr="Mitchell and Lapata, 2008" startWordPosition="957" endWordPosition="960"> to handle both RTE and STS, we do not know of any other methods that have been explicitly tested on both problems. 2 Related work Distributional semantics Distributional models define the semantic relatedness of words as the similarity of vectors representing the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&amp;C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we d</context>
<context position="14880" citStr="Mitchell and Lapata, 2008" startWordPosition="2349" endWordPosition="2352">idual words. We extend their approach to distributional inference rules for pairs of phrases in order to handle cases like (3). To properly estimate the similarity between S1 and S2 in (3), we not only need an inference rule linking bike to bicycle, but also a rule estimating how similar boy is to little boy. To do so, we make use of existing approaches that compute distributional representations for phrases. In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition (Landauer and Dumais, 1997) or component-wise multiplication (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). The inference-rule weight, wt(p1, p2), for two phrases p1 and p2 is then determined using Eq. (5) in the same way as for words. Existing approaches that derive phrasal inference rules from distributional similarity (Lin and Pantel, 2001a; Szpektor and Dagan, 2008; Berant et al., 2011) precompile large lists of inference rules. By comparison, distributional phrase similarity can be seen as a generator of inference rules on the fly, as it is possible to compute distributional phrase vectors for arbitrary phrases on demand as they are needed for particular examples. </context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL, pages 236244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cJeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="2404" citStr="Mitchell and Lapata, 2010" startWordPosition="337" endWordPosition="340">ulfills all requirements. But there are representations that meet some of the criteria. Logic-based representations (Montague, 1970; Kamp and Reyle, 1993) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms. Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Schutze, 1998; Erk and Pado, 2008; Thater et al., 2010). This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation. Distributional similarity between pairs of words is converted into weighted inference rules that are added</context>
<context position="6323" citStr="Mitchell and Lapata, 2010" startWordPosition="961" endWordPosition="964">, we do not know of any other methods that have been explicitly tested on both problems. 2 Related work Distributional semantics Distributional models define the semantic relatedness of words as the similarity of vectors representing the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&amp;C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Stat</context>
<context position="14908" citStr="Mitchell and Lapata, 2010" startWordPosition="2353" endWordPosition="2356">r approach to distributional inference rules for pairs of phrases in order to handle cases like (3). To properly estimate the similarity between S1 and S2 in (3), we not only need an inference rule linking bike to bicycle, but also a rule estimating how similar boy is to little boy. To do so, we make use of existing approaches that compute distributional representations for phrases. In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition (Landauer and Dumais, 1997) or component-wise multiplication (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). The inference-rule weight, wt(p1, p2), for two phrases p1 and p2 is then determined using Eq. (5) in the same way as for words. Existing approaches that derive phrasal inference rules from distributional similarity (Lin and Pantel, 2001a; Szpektor and Dagan, 2008; Berant et al., 2011) precompile large lists of inference rules. By comparison, distributional phrase similarity can be seen as a generator of inference rules on the fly, as it is possible to compute distributional phrase vectors for arbitrary phrases on demand as they are needed for particular examples. Inference rules are generate</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>\x0cJeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):13881429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>Universal grammar.</title>
<date>1970</date>
<tech>Theoria,</tech>
<pages>36--373398</pages>
<note>Reprinted in Thomason</note>
<contexts>
<context position="1909" citStr="Montague, 1970" startWordPosition="264" endWordPosition="266">etailed representation of the structure of complex sentences. Some tasks require the ability to recognize near-paraphrases or degrees of similarity between sentences. Some tasks require logical inference, either exact or approximate. Often it is necessary to handle ambiguity and vagueness in meaning. Finally, we frequently want to be able to learn relevant knowledge automatically from corpus data. There is no single representation for natural language meaning at this time that fulfills all requirements. But there are representations that meet some of the criteria. Logic-based representations (Montague, 1970; Kamp and Reyle, 1993) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms. Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Schutze, 1998; Erk and Pado, 2008; Thater et al., 2010). This suggests that dist</context>
</contexts>
<marker>Montague, 1970</marker>
<rawString>Richard Montague. 1970. Universal grammar. Theoria, 36:373398. Reprinted in Thomason (1974), pp 7-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sriraam Natarajan</author>
<author>Tushar Khot</author>
<author>Daniel Lowd</author>
<author>Prasad Tadepalli</author>
<author>Kristian Kersting</author>
<author>Jude Shavlik</author>
</authors>
<title>Exploiting causal independence in markov logic networks: Combining undirected and directed models.</title>
<date>2010</date>
<booktitle>In Proceedings of European Conference in Machine Learning (ECML),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="5050" citStr="Natarajan et al., 2010" startWordPosition="745" endWordPosition="748">us data. First, we use more expressive distributional inference rules based on the similarity of phrases rather than just individual words. In comparison to existing methods for creating textual inference rules (Lin and Pantel, 2001b; Szpektor and Dagan, 2008), these rules are computed on the fly as needed, rather than pre-compiled. Second, we use more flexible probabilistic combinations of evidence in order to compute degrees of sentence similarity for STS and to help compensate for parser errors. We replace deterministic conjunction by an average combiner, which encodes causal independence (Natarajan et al., 2010). We show that our framework is able to handle both sentence similarity (STS) and textual entailment (RTE) by making some simple adaptations to the MLN when switching between tasks. The framework achieves reasonable results on both tasks. On STS, we obtain a correlation of r = 0.66 with full logic, r = 0.73 in a system with weakened variable binding, and r = 0.85 in an ensemble model. On RTE-1 we obtain an accuracy of 0.57. We show that the distributional inference rules benefit both tasks and that more flexible probabilistic combinations of evidence are crucial for STS. Although other approac</context>
<context position="17368" citStr="Natarajan et al. (2010)" startWordPosition="2769" endWordPosition="2772">sense: If one of the hypothesis predicates is False, the probability of entailment should be zero. For the STS task, this should in principle be the same, at least if the omitted facts are vital, but it seems that annotators rated the data points in this task more for overall similarity than for degrees of entailment. So in STS, we want the similarity to be a function of the number of elements in the hypothesis that are inferable. Therefore, we need to replace the deterministic AND with a different way of combining evidence. We chose to use the average evidence combiner for MLNs introduced by Natarajan et al. (2010). To use the average combiner, the full logical form is divided into smaller clauses (which we call mini-clauses), then the combiner averages their probabilities. In case the formula is a list of conjuncted predicates, a mini-clause is a conjunction of a single-variable predicate with a relation predicate(as in the example below). In case the logical form contains a negated sub-formula, the negated sub-formula is also a mini-clause. The hypothesis above after dividing clauses for the average combiner looks like this: man(x0) agent(x0, e1) result(x0, e1, x2) |w drive(e1) agent(x0, e1) result(x0</context>
</contexts>
<marker>Natarajan, Khot, Lowd, Tadepalli, Kersting, Shavlik, 2010</marker>
<rawString>Sriraam Natarajan, Tushar Khot, Daniel Lowd, Prasad Tadepalli, Kristian Kersting, and Jude Shavlik. 2010. Exploiting causal independence in markov logic networks: Combining undirected and directed models. In Proceedings of European Conference in Machine Learning (ECML), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominick Ng</author>
<author>James R Curran</author>
</authors>
<title>Dependency hashing for n-best ccg parsing. In</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="31727" citStr="Ng and Curran, 2012" startWordPosition="5144" endWordPosition="5147">vely combining logical representations with distributional information automatically acquired from text. In this section, we discuss some of limitations of the current work and directions for future research. As noted before, parse errors are currently a significant problem. We use Boxer to obtain a logical representation for a sentence, which in turn relies on the C&amp;C parser. Unfortunately, C&amp;C misparses many sentences, which leads to inaccurate logical forms. To reduce the impact of misparsing, we plan to use a version of C&amp;C that can produce the top-n parses together with parse re-ranking (Ng and Curran, 2012). As an alternative to re-ranking, one could obtain logical forms for each of the topn parses, and create an MLN that integrates all of them (together with their certainty) as an underspecified meaning representation that could then be used to directly support inferences such as STS and RTE. We also plan to exploit a greater variety of distributional inference rules. First, we intend to incorporate logical form translations of existing distributional inference rule collections (e.g., (Berant et al., 2011; Chan et al., 2011)). Another issue is obtaining improved rule weights based on distributi</context>
</contexts>
<marker>Ng, Curran, 2012</marker>
<rawString>Dominick Ng and James R Curran. 2012. Dependency hashing for n-best ccg parsing. In In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Robust textual inference via learning and abductive reasoning.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="9867" citStr="Raina et al. (2005)" startWordPosition="1504" endWordPosition="1507"> that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics. Weighted inference, and combined structuraldistributional representations One approach to weighted inference in NLP is that of Hobbs et al. (1993), who proposed viewing natural language interpretation as abductive inference. In this framework, problems like reference resolution and syntactic ambiguity resolution become inferences to best explanations that are associated with costs. However, this leaves open the question of how costs are assigned. Raina et al. (2005) use this framework for RTE, deriving inference costs from WordNet similarity and properties of the syntactic parse. Garrette et al. (2011; 2013) proposed an approach to RTE that uses MLNs to combine traditional logical representations with distributional information in order to support probabilistic textual inference. This approach can be viewed as a bridge between Bos and Markert (2005)s purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators</context>
<context position="13002" citStr="Raina et al., 2005" startWordPosition="2023" endWordPosition="2026">he query for which we have Alchemy compute the probability. However, S2 is not strictly entailed by S1 because of the mismatch between cucumber and zucchini, so with just the strict logical-form translations of S1 and S2, the probability of result() will be zero. This is where we introduce distributional similarity, in this case the similarity of cucumber and zucchini, cos( # cucumber, # zucchini). We create inference rules from such similarities as a form of background knowledge. We then treat similarity as degree of entailment, a move that has a long tradition (e.g., (Lin and Pantel, 2001b; Raina et al., 2005; Szpektor and Dagan, 2008)). In general, given two words a and b, we transform their cosine similarity into an inference-rule weight wt(a, b) using: wt(a, b) = log( cos(# a , # b ) 1 cos(# a , # b ) ) prior (5) Where prior is a negative weight used to initialize all predicates, so that by default facts are assumed to have very low probability. In our experiments, we use prior = 3. In the case of sentence pair (2), we generate the inference rule: cucumber(x) zucchini(x) |wt(cuc., zuc.) Such inference rules are generated for all pairs of words (w1, w2) where w1 S1 and w2 S2.1 1 We omit inferenc</context>
</contexts>
<marker>Raina, Ng, Manning, 2005</marker>
<rawString>Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005. Robust textual inference via learning and abductive reasoning. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--107</pages>
<contexts>
<context position="3116" citStr="Richardson and Domingos, 2006" startWordPosition="442" endWordPosition="445">This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form, as illustrated in Figure 1. Finally, Markov Logic Networks (Richardson and Domingos, 2006) (MLNs) are used to perform weighted inference on the resulting knowledge base. However, they only employed single-word distributional similarity rules, and only evaluated on a small 11 \x0cset of short, hand-crafted test sentences. In this paper, we extend Garrette et al.s approach and adapt it to handle two existing semantic tasks: recognizing textual entailment (RTE) and semantic textual similarity (STS). We show how this single semantic framework using probabilistic logical form in Markov logic can be adapted to support both of these important tasks. This is possible because MLNs constitut</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine Learning, 62:107 136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="2442" citStr="Schutze, 1998" startWordPosition="345" endWordPosition="346">ions that meet some of the criteria. Logic-based representations (Montague, 1970; Kamp and Reyle, 1993) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms. Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Schutze, 1998; Erk and Pado, 2008; Thater et al., 2010). This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form, as illustrated i</context>
</contexts>
<marker>Schutze, 1998</marker>
<rawString>Hinrich Schutze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric Huang</author>
<author>Jeffrey Pennin</author>
<author>Andrew Ng</author>
<author>Christopher Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>Proceedings of NIPS.</booktitle>
<editor>In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors,</editor>
<contexts>
<context position="10747" citStr="Socher et al. (2011)" startWordPosition="1634" endWordPosition="1637">rmation in order to support probabilistic textual inference. This approach can be viewed as a bridge between Bos and Markert (2005)s purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators or entailment. There are also other methods that combine distributional and structured representations. Stern et al. (2011) conceptualize textual entailment as tree rewriting of syntactic graphs, where some rewriting rules are distributional inference rules. Socher et al. (2011) recognize paraphrases using a tree of vectors, a phrase structure tree in which each constituent is associated with a vector, and overall sentence similarity is computed by a classifier that integrates all pairwise similarities. (This is in contrast to approaches like Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011), who do not offer a proposal for using vectors at multiple levels in a syntactic tree simultaneously.) 3 MLN system Our system extends that of Garrette et al. (2011; 2013) to support larger-scale evaluation on standard benchmarks for both RTE and STS. We conceptu</context>
</contexts>
<marker>Socher, Huang, Pennin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric Huang, Jeffrey Pennin, Andrew Ng, and Christopher Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asher Stern</author>
<author>Amnon Lotan</author>
<author>Shachar Mirkin</author>
<author>Eyal Shnarch</author>
<author>Lili Kotlerman</author>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
</authors>
<title>Knowledge and tree-edits in learnable entailment proofs. In TAC,</title>
<date>2011</date>
<location>Gathersburg, MD.</location>
<contexts>
<context position="10591" citStr="Stern et al. (2011)" startWordPosition="1612" endWordPosition="1615">ctic parse. Garrette et al. (2011; 2013) proposed an approach to RTE that uses MLNs to combine traditional logical representations with distributional information in order to support probabilistic textual inference. This approach can be viewed as a bridge between Bos and Markert (2005)s purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators or entailment. There are also other methods that combine distributional and structured representations. Stern et al. (2011) conceptualize textual entailment as tree rewriting of syntactic graphs, where some rewriting rules are distributional inference rules. Socher et al. (2011) recognize paraphrases using a tree of vectors, a phrase structure tree in which each constituent is associated with a vector, and overall sentence similarity is computed by a classifier that integrates all pairwise similarities. (This is in contrast to approaches like Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011), who do not offer a proposal for using vectors at multiple levels in a syntactic tree simultaneously.) 3 ML</context>
</contexts>
<marker>Stern, Lotan, Mirkin, Shnarch, Kotlerman, Berant, Dagan, 2011</marker>
<rawString>Asher Stern, Amnon Lotan, Shachar Mirkin, Eyal Shnarch, Lili Kotlerman, Jonathan Berant, and Ido Dagan. 2011. Knowledge and tree-edits in learnable entailment proofs. In TAC, Gathersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="4687" citStr="Szpektor and Dagan, 2008" startWordPosition="689" endWordPosition="693">y describe similar entities and thus there is some degree of entailment between them. At the sentence level, however, we hold that a stricter, logic-based view of entailment is beneficial, and we even model sentence similarity (in STS) as entailment. There are two main innovations in the formalism that make it possible for us to work with naturally occurring corpus data. First, we use more expressive distributional inference rules based on the similarity of phrases rather than just individual words. In comparison to existing methods for creating textual inference rules (Lin and Pantel, 2001b; Szpektor and Dagan, 2008), these rules are computed on the fly as needed, rather than pre-compiled. Second, we use more flexible probabilistic combinations of evidence in order to compute degrees of sentence similarity for STS and to help compensate for parser errors. We replace deterministic conjunction by an average combiner, which encodes causal independence (Natarajan et al., 2010). We show that our framework is able to handle both sentence similarity (STS) and textual entailment (RTE) by making some simple adaptations to the MLN when switching between tasks. The framework achieves reasonable results on both tasks</context>
<context position="13029" citStr="Szpektor and Dagan, 2008" startWordPosition="2027" endWordPosition="2030">e have Alchemy compute the probability. However, S2 is not strictly entailed by S1 because of the mismatch between cucumber and zucchini, so with just the strict logical-form translations of S1 and S2, the probability of result() will be zero. This is where we introduce distributional similarity, in this case the similarity of cucumber and zucchini, cos( # cucumber, # zucchini). We create inference rules from such similarities as a form of background knowledge. We then treat similarity as degree of entailment, a move that has a long tradition (e.g., (Lin and Pantel, 2001b; Raina et al., 2005; Szpektor and Dagan, 2008)). In general, given two words a and b, we transform their cosine similarity into an inference-rule weight wt(a, b) using: wt(a, b) = log( cos(# a , # b ) 1 cos(# a , # b ) ) prior (5) Where prior is a negative weight used to initialize all predicates, so that by default facts are assumed to have very low probability. In our experiments, we use prior = 3. In the case of sentence pair (2), we generate the inference rule: cucumber(x) zucchini(x) |wt(cuc., zuc.) Such inference rules are generated for all pairs of words (w1, w2) where w1 S1 and w2 S2.1 1 We omit inference rules for words (a, b) wh</context>
<context position="15173" citStr="Szpektor and Dagan, 2008" startWordPosition="2395" endWordPosition="2398">s to little boy. To do so, we make use of existing approaches that compute distributional representations for phrases. In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition (Landauer and Dumais, 1997) or component-wise multiplication (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). The inference-rule weight, wt(p1, p2), for two phrases p1 and p2 is then determined using Eq. (5) in the same way as for words. Existing approaches that derive phrasal inference rules from distributional similarity (Lin and Pantel, 2001a; Szpektor and Dagan, 2008; Berant et al., 2011) precompile large lists of inference rules. By comparison, distributional phrase similarity can be seen as a generator of inference rules on the fly, as it is possible to compute distributional phrase vectors for arbitrary phrases on demand as they are needed for particular examples. Inference rules are generated for all pairs of constituents (c1, c2) where c1 S1 and c2 S2, a constituent is a single word or a phrase. The logical form provides a handy way to extract phrases, as they are generally mapped to one of two logical constructs. Either we have multiple single-varia</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen Furstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010,</booktitle>
<pages>948957</pages>
<location>Uppsala,</location>
<contexts>
<context position="2484" citStr="Thater et al., 2010" startWordPosition="351" endWordPosition="354">. Logic-based representations (Montague, 1970; Kamp and Reyle, 1993) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms. Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Schutze, 1998; Erk and Pado, 2008; Thater et al., 2010). This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form, as illustrated in Figure 1. Finally, Markov Logic Networks</context>
</contexts>
<marker>Thater, Furstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen Furstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of ACL 2010, pages 948957, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richmond H Thomason</author>
<author>editor</author>
</authors>
<title>Formal Philosophy. Selected Papers of Richard Montague.</title>
<date>1974</date>
<publisher>Yale University Press,</publisher>
<location>New Haven.</location>
<marker>Thomason, editor, 1974</marker>
<rawString>Richmond H. Thomason, editor. 1974. Formal Philosophy. Selected Papers of Richard Montague. Yale University Press, New Haven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141188</pages>
<contexts>
<context position="2271" citStr="Turney and Pantel, 2010" startWordPosition="317" endWordPosition="320">levant knowledge automatically from corpus data. There is no single representation for natural language meaning at this time that fulfills all requirements. But there are representations that meet some of the criteria. Logic-based representations (Montague, 1970; Kamp and Reyle, 1993) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms. Distributional modhamster( gerbil( sim( # hamster, # gerbil) = w 8x hamster(x) ! gerbil(x) |f(w) Figure 1: Turning distributional similarity into a weighted inference rule els (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Schutze, 1998; Erk and Pado, 2008; Thater et al., 2010). This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the prima</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141188.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>