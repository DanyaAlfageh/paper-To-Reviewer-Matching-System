in lexical semantics and are more versatile than word-based spaces CITATION,,
Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (CITATION; CITATION; CITATION),,
Thus, the results for synonym choice are less clear-cut: derivational smoothing can trade accuracy against Smoothing trigger Smoothing scheme Acc % Cov % DM.DE, unsmoothed CITATION 53.7 80.8 DM.DE, smooth always avgSim 46.0 86.6 maxSim 50.3 86.6 centSim 49.1 86.6 DM.DE, smooth if sim = 0 avgSim 52.6 86.6 maxSim 51.2 86.6 centSim 51.3 86.6 BOW baseline 56.9 98.5 Tabl,,
CITATION build models of selectional preferences that include morphological features such as capitalization and the presence of digits,,
c 2013 Association for Computational Linguistics Derivational Smoothing for Syntactic Distributional Semantics Sebastian Pado Jan Snajder Britta Zeller Heidelberg University, Institut fur Computerlinguistik 69120 Heidelberg, Germany University of Zagreb, Faculty of Electrical Engineering and Computing Unska 3, 10000 Zagreb, Croatia {pado, zeller}@cl.uni-heidelberg.de jan.snajder@fer.hr Abstract Syntax-based vector spaces are used widely in lexical semantics and are more versatile than word-based spaces CITATION,,
Many of the methods were first applied in Language Modeling to deal with unseen n-grams (CITATION; CITATION),,
The unsmoothed model achieves an accuracy of 53.7% and a coverage of 80.8%, as reported by CITATION,,
DM.DE was created on the basis of the 884M-token SDEWAC web corpus CITATION, lemmatized, tagged, and parsed with the German MATE toolkit CITATION,,
CITATION make use of morphology by building,,
Such vector spaces have been applied successfully to many problems in NLP (see Turney and Pantel (2010) or CITATION for current overviews),,
For synonym choice, we follow the method established by CITATION, measuring accuracy over covered items, with partial credit for ties,,
It is also able at least in principle to capture more fine-grained types of semantic similarity such as predicateargument plausibility CITATION,,
The second task is synonym choice on the German version of the Readers Digest WordPower dataset CITATION.2 This dataset, which we also lemmatized and POS-tagged, consists of 984 target words with four synonym candidates each (including phrases), one of which is correct,,
The words in these families are typically semantically similar, although the exact degree depends on the type of relation and idiosyncratic factors (bookN bookishA, CITATION),,
CITATION make use of mor,,
CITATION build models of sele,,
CITATION make use of morphology by building language models for stemming-based equivalence classes,,
Using bootstrap resampling CITATION, we established that the difference to the unsmoothed DM.DE model is not significant at p < 0.05,,
Similarly, distributional features support generalization in Named Entity Recognition CITATION,,
More specifically, we use the W LW matricization of DM.DE, the German version CITATION of Distributional Memory CITATION,,
We measure quality of the semantic similarity task as the Pearson correlation between the model predictions and the human judgments for covered items CITATION,,
1 Introduction Distributional semantics CITATION builds on the assumption that the semantic similarity of words is ,,
1 Introduction Distributional semantics CITATION builds on the assumption that the semantic similarity of words is strongly correlated to the overlap between their linguistic contexts,,
We lemmatized and POS-tagged the German GUR350 dataset CITATION, a set of 350 word pairs with human similarity judgments, created analogously to the well-known CITATION dataset for English.2 We predict 2 ,,
Thus, the results for synonym choice are less clear-cut: derivational smoothing can trade accuracy against Smoothing trigger Smoothing scheme Acc % Cov % DM.DE, unsmoothed CITATION 53.7 80.8 DM.DE, smooth always avgSim 46.0 86.6 maxSim 50.3 86.6 centSim 49.1 86.6 DM.DE, smooth if sim = 0 avgSim 52.6 86.6 maxSim 51.2 86.6 centSim 51.3 86.6 BOW baseline 56.9 98.5 Table 2: Results on the synonym choice task (Acc: Accuracy, Cov: Coverage) coverage but does not lead to a clear improvement,,
The second task is synonym choice on the German version of the Readers Digest WordPower dataset CITATION.2 This dataset, which we also lemmatized and POS-tagged, consists of 984 target words with four,,
o et al., 1998; CITATION),,
This makes them more versatile; the Distributional Memory framework by CITATION is applicable to a wide range of tasks,,
ector spaces have been applied successfully to many problems in NLP (see Turney and Pantel (2010) or CITATION for current overviews),,
(CITATION; CITATION; CITATION),,
Its higher coverage compared to CELEX CITATION and IMSLEX CITATION makes it particularly suitable for the use in smoothing, where the resource should include low-frequency lemmas,,
We use version 1.3 of DERIVBASE CITATION,1 a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 nonsingleton derivational families,,
We lemmatized and POS-tagged the German GUR350 dataset CITATION, a set of 350 word pairs with human similarity judgments, created analogously to the well-known CITATION dataset for English.2 We predict 2 Downloadable from: http://goo.gl/bFokI semantic similarity as cosine similarity,,
We lemmatized and POS-tagged the German GUR350 dataset CITATION, a set of 350 word pairs with human similarity judgments, created analogously to the well-known CITATION dataset for English.2 We predict 2 Downloadable from: http://goo.gl/bFokI semantic si,,
In lexical semantics, smoothing is often achieved by backing 731 \x0coff from words to semantic classes, either adopted from a resource such as WordNet CITATION or induced from data (CITATION; CITATION; CITATION),,
The best previous result is a GermaNet/Wikipedia-based model by CITATION,,
 (CITATION; CITATION),,
W LW matricization of DM.DE, the German version CITATION of Distributional Memory CITATION,,
