<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000214">
<bodyText confidence="0.3237075">
b&apos;Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 10511055,
Prague, June 2007. c
</bodyText>
<sectionHeader confidence="0.325516" genericHeader="method">
2007 Association for Computational Linguistics
</sectionHeader>
<table confidence="0.8813035">
Frustratingly Hard Domain Adaptation for Dependency Parsing
Mark Dredze1
and John Blitzer1
and Partha Pratim Talukdar1
and
Kuzman Ganchev1
and Joao V. Graca2
and Fernando Pereira1
</table>
<page confidence="0.794222">
1
</page>
<affiliation confidence="0.791733">
CIS Dept., University of Pennsylvania, Philadelphia, PA 19104
</affiliation>
<email confidence="0.933178">
{mdredze|blitzer|partha|kuzman|pereira}@seas.upenn.edu
</email>
<page confidence="0.6184835">
2
L2
</page>
<sectionHeader confidence="0.261198" genericHeader="method">
F INESC-ID Lisboa/IST, Rua Alves Redol 9, 1000-029, Lisboa, Portugal
</sectionHeader>
<email confidence="0.81967">
javg@l2f.inesc-id.pt
</email>
<sectionHeader confidence="0.978816" genericHeader="method">
Abstract
</sectionHeader>
<bodyText confidence="0.997340777777778">
We describe some challenges of adaptation
in the 2007 CoNLL Shared Task on Domain
Adaptation. Our error analysis for this task
suggests that a primary source of error is
differences in annotation guidelines between
treebanks. Our suspicions are supported by
the observation that no team was able to im-
prove target domain performance substan-
tially over a state of the art baseline.
</bodyText>
<sectionHeader confidence="0.997972" genericHeader="method">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999031255319149">
Dependency parsing, an important NLP task, can be
done with high levels of accuracy. However, adapt-
ing parsers to new domains without target domain
labeled training data remains an open problem. This
paper outlines our participation in the 2007 CoNLL
Shared Task on Domain Adaptation (Nivre et al.,
2007). The goal was to adapt a parser trained on
a single source domain to a new target domain us-
ing only unlabeled data. We were given around
15K sentences of labeled text from the Wall Street
Journal (WSJ) (Marcus et al., 1993; Johansson and
Nugues, 2007) as well as 200K unlabeled sentences.
The development data was 200 sentences of labeled
biomedical oncology text (BIO, the ONCO portion
of the Penn Biomedical Treebank), as well as 200K
unlabeled sentences (Kulick et al., 2004). The two
test domains were a collection of medline chem-
istry abstracts (pchem, the CYP portion of the Penn
Biomedical Treebank) and the Child Language Data
Exchange System corpus (CHILDES) (MacWhin-
ney, 2000; Brown, 1973). We used the second or-
der two stage parser and edge labeler of McDonald
et al. (2006), which achieved top results in the 2006
CoNLL-X shared task. Preliminary experiments in-
dicated that the edge labeler was fairly robust to do-
main adaptation, lowering accuracy by 3% in the de-
velopment domain as opposed to 2% in the source,
so we focused on unlabeled dependency parsing.
Our system did well, officially coming in 3rd
place out of 12 teams and within 1% of the top sys-
tem (Table 1). 1 In unlabeled parsing, we scored
1st and 2nd on CHILDES and pchem respectively.
However, our results were obtained without adap-
tation. Given our position in the ranking, this sug-
gests that no team was able to significantly improve
performance on either test domain beyond that of a
state-of-the-art parser.
After much effort in developing adaptation meth-
ods, it is critical to understand the causes of these
negative results. In what follows, we provide an er-
ror analysis that attributes domain loss for this task
to a difference in annotation guidelines between do-
mains. We then overview our attempts to improve
adaptation. While we were able to show limited
adaptation on reduced training data or with first-
order features, no modifications improved parsing
with all the training data and second-order features.
</bodyText>
<sectionHeader confidence="0.980349" genericHeader="method">
2 Parsing Challenges
</sectionHeader>
<bodyText confidence="0.998936571428572">
We begin with an error analysis for adaptation be-
tween WSJ and BIO. We divided the available WSJ
data into a train and test set, trained a parser on
the train set and compared errors on the test set
and BIO. Accuracy dropped from 90% on WSJ to
84% on BIO. We then computed the fraction of er-
rors involving each POS tag. For the most common
</bodyText>
<page confidence="0.933345">
1
</page>
<bodyText confidence="0.967584">
While only 8 teams participated in the closed track with us,
our score beat all of the teams in the open track.
</bodyText>
<page confidence="0.976443">
1051
</page>
<table confidence="0.9807264">
\x0cpchem l pchem ul childes ul bio ul
Ours 80.22 83.38 61.37 83.93
Best 81.06 83.42 61.37 -
Mean 73.03 76.42 57.89 -
Rank 3rd 2nd 1st -
</table>
<tableCaption confidence="0.998593">
Table 1: Official labeled (l) and other unlabeled (ul)
</tableCaption>
<bodyText confidence="0.998031305555555">
submitted results for the two test domains (pchem
and childes) and development data accuracy (bio).
The parser was trained on the provided WSJ data.
POS types, the loss (difference in source and tar-
get error) was: verbs (2%), conjunctions (5%), dig-
its (23%), prepositions (4%), adjectives (3%), de-
terminers (4%) and nouns (9%). 2 Two POS types
stand out: digits and nouns. Digits are less than
4% of the tokens in BIO. Errors result from the BIO
annotations for long sequences of digits which do
not appear in WSJ. Since these annotations are new
with respect to the WSJ guidelines, it is impossi-
ble to parse these without injecting knowledge of
the annotation guidelines. 3 Nouns are far more
common, comprising 33% of BIO and 30% of WSJ
tokens, the most popular POS tag by far. Addi-
tionally, other POS types listed above (adjectives,
prepositions, determiners, conjunctions) often attach
to nouns. To confirm that nouns were problem-
atic, we modified a first-order parser (no second or-
der features) by adding a feature indicating correct
noun-noun edges, forcing the parser to predict these
edges correctly. Adaptation performance rose on
BIO from 78% without the feature to 87% with the
feature. This indicates that most of the loss comes
from missing these edges.
The primary problem for nouns is the difference
between structures in each domain. The annota-
tion guidelines for the Penn Treebank flattened noun
phrases to simplify annotation (Marcus et al., 1993),
so there is no complex structure to NPs. Kubler
(2006) showed that it is difficult to compare the
Penn Treebank to other treebanks with more com-
plex noun structures, such as BIO. Consider the WSJ
phrase the New York State Insurance Department.
The annotation indicates a flat structure, where ev-
</bodyText>
<page confidence="0.972572">
2
</page>
<bodyText confidence="0.99796">
We measured these drops on several other dependency
parsers and found similar results.
</bodyText>
<page confidence="0.98969">
3
</page>
<bodyText confidence="0.99936926">
For example, the phrase (R = 28% (10/26); K=10% (3/29);
chi2 test: p=0.014).
ery token is headed by Department. In contrast,
a similar BIO phrase has a very different structure,
pursuant to the BIO guidelines. For the detoxi-
cation enzyme glutathione transferase P1-1, en-
zyme is the head of the NP, P1-1 is the head of
transferase, and transferase is the head of glu-
tathione. Since the guidelines differ, we observe no
corresponding structure in the WSJ. It is telling that
the parser labels this BIO example by attaching ev-
ery token to the final proper noun P1-1, exactly as
the WSJ guidelines indicate. Unlabeled data cannot
indicate that BIO uses a different standard.
Another problem concerns appositives. For ex-
ample, the phrase Howard Mosher, president and
chief executive officer, has Mosher as the head
of Howard and of the appositive NP delimited by
commas. While similar constructions occur in BIO,
there are no commas to indicate this. An example is
the above BIO NP, in which the phrase glutathione
transferase P1-1 is an appositive indicating which
enzyme is meant. However, since there are no
commas, the parser thinks P1-1 is the head. How-
ever, there are not many right to left attaching nouns.
In addition to a change in the annotation guide-
lines for NPs, we observed an important difference
in the distribution of POS tags. NN tags were almost
twice as likely in the BIO domain (14% in WSJ and
25% in BIO). NNP tags, which are close to 10% of
the tags in WSJ, are nonexistent in BIO (.24%). The
cause for this is clear when the annotation guide-
lines are considered. The proper nouns in WSJ are
names of companies, people and places, while in
BIO they are names of genes, proteins and chemi-
cals. However, for BIO these nouns are labeled NN
instead of NNP. This decision effectively removes
NNP from the BIO domain and renders all features
that depend on the NNP tag ineffective. In our above
BIO NP example, all nouns are labeled NN, whereas
the WSJ example contains NNP tags. The largest
tri-gram differences involve nouns, such as NN-NN-
NN, NNP-NNP-NNP, NN-IN-NN, and IN-NN-NN.
However, when we examine the coarse POS tags,
which do not distinguish between nouns, these dif-
ferences disappear. This indicates that while the
overall distribution of POS tags is similar between
the domains, the fine grained tags differ. These fine
grained tags provide more information than coarse
tags; experiments that removed fine grained tags
</bodyText>
<page confidence="0.952808">
1052
</page>
<bodyText confidence="0.999006666666667">
\x0churt WSJ performance but did not affect BIO.
Finally, we examined the effect of unknown
words. Not surprisingly, the most significant dif-
ferences in error rates concerned dependencies be-
tween words of which one or both were unknown
to the parser. For two words that were seen in the
training data loss was 4%, for a single unknown
word loss was 15%, and 26% when both words were
unknown. Both words were unknown only 5% of
the time in BIO, while one of the words being un-
known was more common, reflecting 27% of deci-
sions. Upon further investigation, the majority of
unknown words were nouns, which indicates that
unknown word errors were caused by the problems
discussed above.
Recent theoretical work on domain adapta-
tion (Ben-David et al., 2006) attributes adaptation
loss to two sources: the difference in the distribu-
tion between domains and the difference in label-
ing functions. Adaptation techniques focus on the
former since it is impossible to determine the lat-
ter without knowledge of the labeling function. In
parsing adaptation, the former corresponds to a dif-
ference between the features seen in each domain,
such as new words in the target domain. The de-
cision function corresponds to differences between
annotation guidelines between two domains. Our er-
ror analysis suggests that the primary cause of loss
from adaptation is from differences in the annotation
guidelines themselves. Therefore, significant im-
provements cannot be made without specific knowl-
edge of the target domains annotation standards. No
amount of source training data can help if no rele-
vant structure exists in the data. Given the results
for the domain adaptation track, it appears no team
successfully adapted a state-of-the-art parser.
</bodyText>
<sectionHeader confidence="0.976407" genericHeader="method">
3 Adaptation Approaches
</sectionHeader>
<bodyText confidence="0.993242666666667">
We survey the main approaches we explored for this
task. While some of these approaches provided a
modest performance boost to a simple parser (lim-
ited data and first-order features), no method added
any performance to our best parser (all data and
second-order features).
</bodyText>
<subsectionHeader confidence="0.982133">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.999468884615385">
A natural approach to improving parsing is to mod-
ify the feature set, both by removing features less
likely to transfer and by adding features that are
more likely to transfer. We began with the first ap-
proach and removed a large number of features that
we believed transfered poorly, such as most features
for noun-noun edges. We obtained a small improve-
ment in BIO performance on limited data only. We
then added several different types of features, specif-
ically designed to improve noun phrase construc-
tions, such as features based on the lexical position
of nouns (common position in NPs), frequency of
occurrence, and NP chunking information. For ex-
ample, trained on in-domain data, nouns that occur
more often tend to be heads. However, none of these
features transfered between domains.
A final type of feature we added was based on
the behavior of nouns, adjectives and verbs in each
domain. We constructed a feature representation
of words based on adjacent POS and words and
clustered words using an algorithm similar to that
of Saul and Pereira (1997). For example, our clus-
tering algorithm grouped first names in one group
and measurements in another. We then added the
cluster membership as a lexical feature to the parser.
None of the resulting features helped adaptation.
</bodyText>
<subsectionHeader confidence="0.998888">
3.2 Diversity
</subsectionHeader>
<bodyText confidence="0.999323166666667">
Training diversity may be an effective source for
adaptation. We began by adding information from
multiple different parsers, which has been shown
to improve in-domain parsing. We added features
indicating when an edge was predicted by another
parser and if an edge crossed a predicted edge, as
well as conjunctions with edge types. This failed
to improve BIO accuracy since these features were
less reliable at test time. Next, we tried instance
bagging (Breiman, 1996) to generate some diversity
among parsers. We selected with replacement 2000
training examples from the training data and trained
three parsers. Each parser then tagged the remain-
ing 13K sentences, yielding 39K parsed sentences.
We then shuffled these sentences and trained a final
parser. This failed to improve performance, possibly
because of conflicting annotations or because of lack
of sufficient diversity. To address conflicting annota-
</bodyText>
<page confidence="0.902805">
1053
</page>
<bodyText confidence="0.9996523">
\x0ctions, we added slack variables to the MIRA learn-
ing algorithm (Crammer et al., 2006) used to train
the parsers, without success. We measured diversity
by comparing the parses of each model. The dif-
ference in annotation agreement between the three
instance bagging parsers was about half the differ-
ence between these parsers and the gold annotations.
While we believe this is not enough diversity, it was
not feasible to repeat our experiment with a large
number of parsers.
</bodyText>
<subsectionHeader confidence="0.999309">
3.3 Target Focused Learning
</subsectionHeader>
<bodyText confidence="0.999424055555556">
Another approach to adaptation is to favor training
examples that are similar to the target. We first mod-
ified the weight given by the parser to each training
sentence based on the similarity of the sentence to
target domain sentences. This can be done by mod-
ifying the loss to limit updates in cases where the
sentence does not reflect the target domain. We tried
a number of criteria to weigh sentences without suc-
cess, including sentence length and number of verbs.
Next, we trained a discriminative model on the pro-
vided unlabeled data to predict the domain of each
sentence based on POS n-grams in the sentence.
Training sentences with a higher probability of be-
ing in the target domain received higher weights,
also without success. Further experiments showed
that any decrease in training data hurt parser perfor-
mance. It would seem that the parser has no dif-
ficulty learning important training sentences in the
presence of unimportant training examples.
A related idea focused on words, weighing highly
tokens that appeared frequently in the target domain.
We scaled the loss associated with a token by a fac-
tor proportional to its frequency in the target do-
main. We found certain scaling techniques obtained
tiny improvements on the target domain that, while
significant compared to competition results, are not
statistically significant. We also attempted a sim-
ilar approach on the feature level. A very predic-
tive source domain feature is not useful if it does
not appear in the target domain. However, limiting
the feature space to target domain features had no
effect. Instead, we scaled each features value by a
factor proportional to its frequency in the target do-
main and trained the parser on these scaled feature
values. We obtained small improvements on small
amounts of training data.
</bodyText>
<sectionHeader confidence="0.999509" genericHeader="method">
4 Future Directions
</sectionHeader>
<bodyText confidence="0.999926333333333">
Given our pessimistic analysis and the long list of
failed methods, one may wonder if parser adapta-
tion is possible at all. We believe that it is. First,
there may be room for adaptation with our domains
if a common annotation scheme is used. Second,
we have stressed that typical adaptation, modifying
a model trained on the source domain, will fail but
there may be unsupervised parsing techniques that
improve performance after adaptation, such as a rule
based NP parser for BIO based on knowledge of the
annotations. However, this approach is unsatisfying
as it does not allow general purpose adaptation.
</bodyText>
<sectionHeader confidence="0.999302" genericHeader="conclusions">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.942481416666667">
We thank Joel Wallenberg and Nikhil Dinesh for
their informative and helpful linguistic expertise,
Kevin Lerman for his edge labeler code, and Koby
Crammer for helpful conversations. Dredze is sup-
ported by a NDSEG fellowship; Ganchev and Taluk-
dar by NSF ITR EIA-0205448; and Blitzer by
DARPA under Contract No. NBCHD03001. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
author(s) and do not necessarily reflect the views of
the DARPA or the Department of Interior-National
Business Center (DOI-NBC).
</bodyText>
<sectionHeader confidence="0.914004" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994429888888889">
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In NIPS.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123140.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551585, Mar.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Sandra Kubler. 2006. How do treebank annotation
schemes influence parsing results? or how not to com-
pare apples and oranges. In RANLP.
</reference>
<page confidence="0.569402">
1054
</page>
<reference confidence="0.999601846153846">
\x0cS. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313330.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency parsing with a two-
stage discriminative parser. In Conference on Natural
Language Learning (CoNLL).
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
Lawrence Saul and Fernando Pereira. 1997. Aggre-
gate and mixed-order markov models for statistical
language modeling. In EMNLP.
</reference>
<page confidence="0.743507">
1055
</page>
<figure confidence="0.266599">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.017468">
<note confidence="0.897002">b&apos;Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 10511055, Prague, June 2007. c 2007 Association for Computational Linguistics</note>
<title confidence="0.983349">Frustratingly Hard Domain Adaptation for Dependency Parsing</title>
<author confidence="0.746435714285714">Mark Dredze</author>
<author confidence="0.746435714285714">John Blitzer</author>
<author confidence="0.746435714285714">Partha Pratim Talukdar</author>
<author confidence="0.746435714285714">Kuzman Ganchev</author>
<author confidence="0.746435714285714">Joao V Graca</author>
<author confidence="0.746435714285714">Fernando Pereira</author>
<date confidence="0.544821">1</date>
<address confidence="0.762608">CIS Dept., University of Pennsylvania, Philadelphia, PA 19104</address>
<email confidence="0.977777">{mdredze|blitzer|partha|kuzman|pereira}@seas.upenn.edu</email>
<note confidence="0.77844125">2 L2 F INESC-ID Lisboa/IST, Rua Alves Redol 9, 1000-029, Lisboa, Portugal javg@l2f.inesc-id.pt</note>
<abstract confidence="0.9871061">We describe some challenges of adaptation in the 2007 CoNLL Shared Task on Domain Adaptation. Our error analysis for this task suggests that a primary source of error is differences in annotation guidelines between treebanks. Our suspicions are supported by the observation that no team was able to improve target domain performance substantially over a state of the art baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Analysis of representations for domain adaptation.</title>
<date>2006</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="8977" citStr="Ben-David et al., 2006" startWordPosition="1501" endWordPosition="1504">nificant differences in error rates concerned dependencies between words of which one or both were unknown to the parser. For two words that were seen in the training data loss was 4%, for a single unknown word loss was 15%, and 26% when both words were unknown. Both words were unknown only 5% of the time in BIO, while one of the words being unknown was more common, reflecting 27% of decisions. Upon further investigation, the majority of unknown words were nouns, which indicates that unknown word errors were caused by the problems discussed above. Recent theoretical work on domain adaptation (Ben-David et al., 2006) attributes adaptation loss to two sources: the difference in the distribution between domains and the difference in labeling functions. Adaptation techniques focus on the former since it is impossible to determine the latter without knowledge of the labeling function. In parsing adaptation, the former corresponds to a difference between the features seen in each domain, such as new words in the target domain. The decision function corresponds to differences between annotation guidelines between two domains. Our error analysis suggests that the primary cause of loss from adaptation is from dif</context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Pereira, 2006</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. 2006. Analysis of representations for domain adaptation. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="12036" citStr="Breiman, 1996" startWordPosition="1996" endWordPosition="1997">other. We then added the cluster membership as a lexical feature to the parser. None of the resulting features helped adaptation. 3.2 Diversity Training diversity may be an effective source for adaptation. We began by adding information from multiple different parsers, which has been shown to improve in-domain parsing. We added features indicating when an edge was predicted by another parser and if an edge crossed a predicted edge, as well as conjunctions with edge types. This failed to improve BIO accuracy since these features were less reliable at test time. Next, we tried instance bagging (Breiman, 1996) to generate some diversity among parsers. We selected with replacement 2000 training examples from the training data and trained three parsers. Each parser then tagged the remaining 13K sentences, yielding 39K parsed sentences. We then shuffled these sentences and trained a final parser. This failed to improve performance, possibly because of conflicting annotations or because of lack of sufficient diversity. To address conflicting annota1053 \x0ctions, we added slack variables to the MIRA learning algorithm (Crammer et al., 2006) used to train the parsers, without success. We measured divers</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brown</author>
</authors>
<title>A First Language: The Early Stages.</title>
<date>1973</date>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="1953" citStr="Brown, 1973" startWordPosition="299" endWordPosition="300">n using only unlabeled data. We were given around 15K sentences of labeled text from the Wall Street Journal (WSJ) (Marcus et al., 1993; Johansson and Nugues, 2007) as well as 200K unlabeled sentences. The development data was 200 sentences of labeled biomedical oncology text (BIO, the ONCO portion of the Penn Biomedical Treebank), as well as 200K unlabeled sentences (Kulick et al., 2004). The two test domains were a collection of medline chemistry abstracts (pchem, the CYP portion of the Penn Biomedical Treebank) and the Child Language Data Exchange System corpus (CHILDES) (MacWhinney, 2000; Brown, 1973). We used the second order two stage parser and edge labeler of McDonald et al. (2006), which achieved top results in the 2006 CoNLL-X shared task. Preliminary experiments indicated that the edge labeler was fairly robust to domain adaptation, lowering accuracy by 3% in the development domain as opposed to 2% in the source, so we focused on unlabeled dependency parsing. Our system did well, officially coming in 3rd place out of 12 teams and within 1% of the top system (Table 1). 1 In unlabeled parsing, we scored 1st and 2nd on CHILDES and pchem respectively. However, our results were obtained </context>
</contexts>
<marker>Brown, 1973</marker>
<rawString>R. Brown. 1973. A First Language: The Early Stages. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551585</pages>
<contexts>
<context position="12573" citStr="Crammer et al., 2006" startWordPosition="2075" endWordPosition="2078">res were less reliable at test time. Next, we tried instance bagging (Breiman, 1996) to generate some diversity among parsers. We selected with replacement 2000 training examples from the training data and trained three parsers. Each parser then tagged the remaining 13K sentences, yielding 39K parsed sentences. We then shuffled these sentences and trained a final parser. This failed to improve performance, possibly because of conflicting annotations or because of lack of sufficient diversity. To address conflicting annota1053 \x0ctions, we added slack variables to the MIRA learning algorithm (Crammer et al., 2006) used to train the parsers, without success. We measured diversity by comparing the parses of each model. The difference in annotation agreement between the three instance bagging parsers was about half the difference between these parsers and the gold annotations. While we believe this is not enough diversity, it was not feasible to repeat our experiment with a large number of parsers. 3.3 Target Focused Learning Another approach to adaptation is to favor training examples that are similar to the target. We first modified the weight given by the parser to each training sentence based on the s</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research, 7:551585, Mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proc. of the 16th Nordic Conference on Computational Linguistics (NODALIDA).</booktitle>
<contexts>
<context position="1505" citStr="Johansson and Nugues, 2007" startWordPosition="226" endWordPosition="229">ormance substantially over a state of the art baseline. 1 Introduction Dependency parsing, an important NLP task, can be done with high levels of accuracy. However, adapting parsers to new domains without target domain labeled training data remains an open problem. This paper outlines our participation in the 2007 CoNLL Shared Task on Domain Adaptation (Nivre et al., 2007). The goal was to adapt a parser trained on a single source domain to a new target domain using only unlabeled data. We were given around 15K sentences of labeled text from the Wall Street Journal (WSJ) (Marcus et al., 1993; Johansson and Nugues, 2007) as well as 200K unlabeled sentences. The development data was 200 sentences of labeled biomedical oncology text (BIO, the ONCO portion of the Penn Biomedical Treebank), as well as 200K unlabeled sentences (Kulick et al., 2004). The two test domains were a collection of medline chemistry abstracts (pchem, the CYP portion of the Penn Biomedical Treebank) and the Child Language Data Exchange System corpus (CHILDES) (MacWhinney, 2000; Brown, 1973). We used the second order two stage parser and edge labeler of McDonald et al. (2006), which achieved top results in the 2006 CoNLL-X shared task. Prel</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>R. Johansson and P. Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proc. of the 16th Nordic Conference on Computational Linguistics (NODALIDA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Kubler</author>
</authors>
<title>How do treebank annotation schemes influence parsing results? or how not to compare apples and oranges.</title>
<date>2006</date>
<booktitle>In RANLP.</booktitle>
<contexts>
<context position="5444" citStr="Kubler (2006)" startWordPosition="901" endWordPosition="902">rm that nouns were problematic, we modified a first-order parser (no second order features) by adding a feature indicating correct noun-noun edges, forcing the parser to predict these edges correctly. Adaptation performance rose on BIO from 78% without the feature to 87% with the feature. This indicates that most of the loss comes from missing these edges. The primary problem for nouns is the difference between structures in each domain. The annotation guidelines for the Penn Treebank flattened noun phrases to simplify annotation (Marcus et al., 1993), so there is no complex structure to NPs. Kubler (2006) showed that it is difficult to compare the Penn Treebank to other treebanks with more complex noun structures, such as BIO. Consider the WSJ phrase the New York State Insurance Department. The annotation indicates a flat structure, where ev2 We measured these drops on several other dependency parsers and found similar results. 3 For example, the phrase (R = 28% (10/26); K=10% (3/29); chi2 test: p=0.014). ery token is headed by Department. In contrast, a similar BIO phrase has a very different structure, pursuant to the BIO guidelines. For the detoxication enzyme glutathione transferase P1-1, </context>
</contexts>
<marker>Kubler, 2006</marker>
<rawString>Sandra Kubler. 2006. How do treebank annotation schemes influence parsing results? or how not to compare apples and oranges. In RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bies Kulick</author>
<author>M Liberman</author>
<author>M Mandel</author>
<author>R McDonald</author>
<author>M Palmer</author>
<author>A Schein</author>
<author>L Ungar</author>
</authors>
<title>Integrated annotation for biomedical information extraction.</title>
<date>2004</date>
<booktitle>In Proc. of the Human Language Technology Conference and the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL).</booktitle>
<contexts>
<context position="1732" citStr="Kulick et al., 2004" startWordPosition="262" endWordPosition="265">g data remains an open problem. This paper outlines our participation in the 2007 CoNLL Shared Task on Domain Adaptation (Nivre et al., 2007). The goal was to adapt a parser trained on a single source domain to a new target domain using only unlabeled data. We were given around 15K sentences of labeled text from the Wall Street Journal (WSJ) (Marcus et al., 1993; Johansson and Nugues, 2007) as well as 200K unlabeled sentences. The development data was 200 sentences of labeled biomedical oncology text (BIO, the ONCO portion of the Penn Biomedical Treebank), as well as 200K unlabeled sentences (Kulick et al., 2004). The two test domains were a collection of medline chemistry abstracts (pchem, the CYP portion of the Penn Biomedical Treebank) and the Child Language Data Exchange System corpus (CHILDES) (MacWhinney, 2000; Brown, 1973). We used the second order two stage parser and edge labeler of McDonald et al. (2006), which achieved top results in the 2006 CoNLL-X shared task. Preliminary experiments indicated that the edge labeler was fairly robust to domain adaptation, lowering accuracy by 3% in the development domain as opposed to 2% in the source, so we focused on unlabeled dependency parsing. Our sy</context>
</contexts>
<marker>Kulick, Liberman, Mandel, McDonald, Palmer, Schein, Ungar, 2004</marker>
<rawString>\x0cS. Kulick, A. Bies, M. Liberman, M. Mandel, R. McDonald, M. Palmer, A. Schein, and L. Ungar. 2004. Integrated annotation for biomedical information extraction. In Proc. of the Human Language Technology Conference and the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for Analyzing Talk. Lawrence Erlbaum.</title>
<date>2000</date>
<contexts>
<context position="1939" citStr="MacWhinney, 2000" startWordPosition="296" endWordPosition="298">a new target domain using only unlabeled data. We were given around 15K sentences of labeled text from the Wall Street Journal (WSJ) (Marcus et al., 1993; Johansson and Nugues, 2007) as well as 200K unlabeled sentences. The development data was 200 sentences of labeled biomedical oncology text (BIO, the ONCO portion of the Penn Biomedical Treebank), as well as 200K unlabeled sentences (Kulick et al., 2004). The two test domains were a collection of medline chemistry abstracts (pchem, the CYP portion of the Penn Biomedical Treebank) and the Child Language Data Exchange System corpus (CHILDES) (MacWhinney, 2000; Brown, 1973). We used the second order two stage parser and edge labeler of McDonald et al. (2006), which achieved top results in the 2006 CoNLL-X shared task. Preliminary experiments indicated that the edge labeler was fairly robust to domain adaptation, lowering accuracy by 3% in the development domain as opposed to 2% in the source, so we focused on unlabeled dependency parsing. Our system did well, officially coming in 3rd place out of 12 teams and within 1% of the top system (Table 1). 1 In unlabeled parsing, we scored 1st and 2nd on CHILDES and pchem respectively. However, our results </context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>B. MacWhinney. 2000. The CHILDES Project: Tools for Analyzing Talk. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1476" citStr="Marcus et al., 1993" startWordPosition="222" endWordPosition="225">ve target domain performance substantially over a state of the art baseline. 1 Introduction Dependency parsing, an important NLP task, can be done with high levels of accuracy. However, adapting parsers to new domains without target domain labeled training data remains an open problem. This paper outlines our participation in the 2007 CoNLL Shared Task on Domain Adaptation (Nivre et al., 2007). The goal was to adapt a parser trained on a single source domain to a new target domain using only unlabeled data. We were given around 15K sentences of labeled text from the Wall Street Journal (WSJ) (Marcus et al., 1993; Johansson and Nugues, 2007) as well as 200K unlabeled sentences. The development data was 200 sentences of labeled biomedical oncology text (BIO, the ONCO portion of the Penn Biomedical Treebank), as well as 200K unlabeled sentences (Kulick et al., 2004). The two test domains were a collection of medline chemistry abstracts (pchem, the CYP portion of the Penn Biomedical Treebank) and the Child Language Data Exchange System corpus (CHILDES) (MacWhinney, 2000; Brown, 1973). We used the second order two stage parser and edge labeler of McDonald et al. (2006), which achieved top results in the 2</context>
<context position="5388" citStr="Marcus et al., 1993" startWordPosition="889" endWordPosition="892">ons, determiners, conjunctions) often attach to nouns. To confirm that nouns were problematic, we modified a first-order parser (no second order features) by adding a feature indicating correct noun-noun edges, forcing the parser to predict these edges correctly. Adaptation performance rose on BIO from 78% without the feature to 87% with the feature. This indicates that most of the loss comes from missing these edges. The primary problem for nouns is the difference between structures in each domain. The annotation guidelines for the Penn Treebank flattened noun phrases to simplify annotation (Marcus et al., 1993), so there is no complex structure to NPs. Kubler (2006) showed that it is difficult to compare the Penn Treebank to other treebanks with more complex noun structures, such as BIO. Consider the WSJ phrase the New York State Insurance Department. The annotation indicates a flat structure, where ev2 We measured these drops on several other dependency parsers and found similar results. 3 For example, the phrase (R = 28% (10/26); K=10% (3/29); chi2 test: p=0.014). ery token is headed by Department. In contrast, a similar BIO phrase has a very different structure, pursuant to the BIO guidelines. Fo</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual dependency parsing with a twostage discriminative parser.</title>
<date>2006</date>
<booktitle>In Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="2039" citStr="McDonald et al. (2006)" startWordPosition="314" endWordPosition="317">ext from the Wall Street Journal (WSJ) (Marcus et al., 1993; Johansson and Nugues, 2007) as well as 200K unlabeled sentences. The development data was 200 sentences of labeled biomedical oncology text (BIO, the ONCO portion of the Penn Biomedical Treebank), as well as 200K unlabeled sentences (Kulick et al., 2004). The two test domains were a collection of medline chemistry abstracts (pchem, the CYP portion of the Penn Biomedical Treebank) and the Child Language Data Exchange System corpus (CHILDES) (MacWhinney, 2000; Brown, 1973). We used the second order two stage parser and edge labeler of McDonald et al. (2006), which achieved top results in the 2006 CoNLL-X shared task. Preliminary experiments indicated that the edge labeler was fairly robust to domain adaptation, lowering accuracy by 3% in the development domain as opposed to 2% in the source, so we focused on unlabeled dependency parsing. Our system did well, officially coming in 3rd place out of 12 teams and within 1% of the top system (Table 1). 1 In unlabeled parsing, we scored 1st and 2nd on CHILDES and pchem respectively. However, our results were obtained without adaptation. Given our position in the ranking, this suggests that no team was </context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency parsing with a twostage discriminative parser. In Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S Kubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proc.</booktitle>
<contexts>
<context position="1253" citStr="Nivre et al., 2007" startWordPosition="180" endWordPosition="183"> Adaptation. Our error analysis for this task suggests that a primary source of error is differences in annotation guidelines between treebanks. Our suspicions are supported by the observation that no team was able to improve target domain performance substantially over a state of the art baseline. 1 Introduction Dependency parsing, an important NLP task, can be done with high levels of accuracy. However, adapting parsers to new domains without target domain labeled training data remains an open problem. This paper outlines our participation in the 2007 CoNLL Shared Task on Domain Adaptation (Nivre et al., 2007). The goal was to adapt a parser trained on a single source domain to a new target domain using only unlabeled data. We were given around 15K sentences of labeled text from the Wall Street Journal (WSJ) (Marcus et al., 1993; Johansson and Nugues, 2007) as well as 200K unlabeled sentences. The development data was 200 sentences of labeled biomedical oncology text (BIO, the ONCO portion of the Penn Biomedical Treebank), as well as 200K unlabeled sentences (Kulick et al., 2004). The two test domains were a collection of medline chemistry abstracts (pchem, the CYP portion of the Penn Biomedical Tr</context>
</contexts>
<marker>Nivre, Hall, Kubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proc.</rawString>
</citation>
<citation valid="true">
<title>of the CoNLL</title>
<date>2007</date>
<booktitle>Shared Task. Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL).</booktitle>
<marker>2007</marker>
<rawString>of the CoNLL 2007 Shared Task. Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Saul</author>
<author>Fernando Pereira</author>
</authors>
<title>Aggregate and mixed-order markov models for statistical language modeling.</title>
<date>1997</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="11327" citStr="Saul and Pereira (1997)" startWordPosition="1881" endWordPosition="1884">of features, specifically designed to improve noun phrase constructions, such as features based on the lexical position of nouns (common position in NPs), frequency of occurrence, and NP chunking information. For example, trained on in-domain data, nouns that occur more often tend to be heads. However, none of these features transfered between domains. A final type of feature we added was based on the behavior of nouns, adjectives and verbs in each domain. We constructed a feature representation of words based on adjacent POS and words and clustered words using an algorithm similar to that of Saul and Pereira (1997). For example, our clustering algorithm grouped first names in one group and measurements in another. We then added the cluster membership as a lexical feature to the parser. None of the resulting features helped adaptation. 3.2 Diversity Training diversity may be an effective source for adaptation. We began by adding information from multiple different parsers, which has been shown to improve in-domain parsing. We added features indicating when an edge was predicted by another parser and if an edge crossed a predicted edge, as well as conjunctions with edge types. This failed to improve BIO a</context>
</contexts>
<marker>Saul, Pereira, 1997</marker>
<rawString>Lawrence Saul and Fernando Pereira. 1997. Aggregate and mixed-order markov models for statistical language modeling. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>