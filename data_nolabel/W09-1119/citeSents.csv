It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance (CITATION; CITATIONa; CITATION; CITATION),,
ically viewed as a sequential prediction problem, the typical models include HMM CITATION, CRF CITATION, and sequential application of Perceptron or Winnow CITATION,,
In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing CITATION, Chinese word segmentation CITATION and NER CITATION,,
Given these findings, several approaches have been proposed to automatically extract comprehensive gazetteers from the web and from large collections of unlabeled text (CITATION; CITATION) with limited impact on NER,,
NER is typically viewed as a sequential prediction problem, the typical models include HMM CITATION, CRF CITATION, and sequential application of Perceptron or Winnow CITATION,,
The technique is based on word class models, pioneered by CITATION, which hierarchically 151 \x0cCoNLL03 CoNLL03 MUC7 MUC7 Web Component Test data Dev data Dev Test pages 1) Baseline 83.65 89.25 74.72 71.28 71.41 2) (1) + Gazetteer Match 87.22 91.61 85.83 80.43 74.46 3) (1) + Word Class Model 86.82 90.85 80.25 79.88 72.26 4) All External Knowledge 88.55 92.49 84.50 83.23 74.44 Table 4: Utility of external knowledge,,
The technique is based on word class models, pioneered by CITATION, which hierarchically 151 \x0cCoNLL03 CoNLL03 MUC7 MUC7 Web Component Test data Dev data Dev Test pages 1) Baseline 83.65 89.25 74.72 71.28 71.41 ,,
Related works include voting between several representation schemes CITATION, lexicalizing the schemes CITATION and automatically searching for best encoding CITATION,,
Recently, (CITATION; CITATIONa) have successfully constructed high quality and high coverage gazetteers from Wikipedia,,
CITATION used the intuition that some instances of a token appear in easily-identifiable contexts,,
Surprisingly, the greedy policy performs well, this phenmenon was also observed in the POS tagging task (CITATION; CITATION),,
5.1 Context aggregation CITATION used features that aggregate, for each document, the context tokens appear in,,
6.1 Unlabeled Text Recent successful semi-supervised systems (CITATION; CITATION) have illustrated that unlabeled text can be used to improve the performance of NER systems,,
Our baseline NER system uses a regularized averaged perceptron CITATION,,
NER proves to be a knowledgeintensive task, and it was reassuring to observe that System Resources Used F1 + LBJ-NER Wikipedia, Nonlocal Features, Word-class Model 90.80 - CITATION Semi-supervised on 1Gword unlabeled data 89.92 - CITATION Semi-supervised on 27Mword unlabeled data 89.31 - (CITATIONa) Wikipedia 88.02 - CITATION Non-local Features 87.24 - (CITATIONb) Non-local Features 87.17 + CITATION Non-local Features 86.86 Table 7: Results for CoNLL03 data reported in the literature,,
The results we obtained on the CoNLL03 test set were consistent with what was reported in CITATION,,
Both CITATION and (CITATIONa) used the free-text description of the Wikipedia entity to reason about the entity type,,
Systems based on perceptron have been shown to be competitive in NER and text chunking (CITATIONb; CITATION; CITATION) We specify the model and the features with the LBJ CITATION modeling language,,
This conditional probability distribution is estimated in NER using the following baseline set of features CITATION: (1) previous two predictions yi1 and yi2 (2) current word xi (3) xi word type (all-capitalized, is-capitalized, all-digits, alphanumeric, etc.) (4) prefixes and suffix,,
The technique is based on word class models, pioneered by CITATION, which hierarchically 151 \x0cCoNLL03 CoNLL03 MUC7 MUC7 Web Component Test data Dev data Dev Test pages 1) Baseline 83.65 89.25 74.72 71.28 71.41 2) (1) + Gazetteer Match 87,,
Another important question that has been studied extensively in the context of shallow parsing and was somewhat overlooked in the NER literature is the representation of text segments CITATION,,
We implemented the token-majority and the entity-majority features discussed in CITATION; however, instead of document and corpus majority tags, we used relative frequency of the tags in a 1000 token window,,
This conditional probability distribution is estimated in NER using the following baseline set of features CITATION: (1) previous two predictions yi1 and yi2 (2) current word xi (3) xi word type (all-capitalized, is-,,
This conditional probability distribution is estimated in NER using the following baseline set of features CITATION: (1) previous two predictions yi1 and yi2 (2) current word xi (3) xi word type (all-capitalized, is-capitalized, all-digits, alphanumeric, etc.) (4) prefixes and suffixes of xi (5) tokens in the window c = (xi2, xi1, xi, xi+1, xi+2) (6) capitalization pattern in the window c (7) conjunction of c and yi1,,
This conditional probability distribution is estimated in NER using the following baseline set of features CITATION: (1) previous two predictions yi1 and yi2 (2) current word xi (3) xi wo,,
