Features We model syntactic features, following CITATION,,
Otherwise, the problem would reduce to wrapper learning CITATION,,
encode information typically used in NER, including content and contextual patterns, as well as lookups in available dictionaries (Finkel et al., 2005; CITATION),,
Mainly, predetermined word-level dependencies were represented as links in the underlying graphical model (CITATION; Finkel et al., 2005),,
We have annotated this dataset with two additional attributes: date and title.2 We consider this corpus as an example of semi-structured text, where some of the field values appear in the email header, in a tabular structure, or using special formatting (CITATION; CITATION).3 We used a set of rules to extract candidate named entities per the types specified in Figure 2.4 The rules encode informat,,
In order to compensate for parsing errors, shallow syntactic features were added, representing the values of neighboring verbs and prepositions CITATION,,
We report experimental evaluations on two benchmark datasets in different genres, the CMU seminar announcements and corporate acquisitions CITATION,,
Recently, CITATION presented a generative semi-supervised approach for template filling,,
In open IE, entities and relations may be inferred jointly (CITATION; CITATION),,
We demonstrate empirically that these challenges can be solved with a combination of greedy beam decoding, performed directly in the joint space of possible mention clusters and field assignments, and structured Perceptronstyle learning algorithm CITATION,,
Unfortunately, we can not directly compare against a generative joint model evaluated on this dataset CITATION.7 The best results per attribute are shown in boldface,,
Table 2 provides a comparison of the full model 850 \x0cDate Stime Etime Location Speaker Title CITATION - 96.7 97.2 88.1 80.4 - (Finkel et al., 2005) - 97.1 97.9 90.0 84.2 - Full model 95.4 97.1 97.9 97.0 86.5 75.5 Table 3: Seminar extraction results: Token-level F1 against previous state-of-the-art results,,
849 \x0cDate Stime Etime Location Speaker Title Full model 96.1 99.3 98.7 96.4 87.5 69.5 No structural features 94.9 99.1 98.0 96.1 83.8 65.1 No semantic features 96.1 98.7 95.4 96.4 87.5 69.5 No unification 87.2 97.0 95.1 94.5 76.0 62.7 Individual fields 96.5 97.2 - 96.4 86.8 64.5 Table 1: Seminar extraction results (5-fold CV): Field-level F1 Date Stime Etime Location Speaker Title SNOW CITATION - 99.6 96.3 75.2 73.8 - BIEN CITATION - 96.0 98.8 87.1 76.9 - Elie CITATION - 98.5 96.4 86.5 88.5 - TIE CITATION - 99.3 97.1 81.7 85.4 - Full model 96.3 99.1 98.0 96.9 85.8 67.7 Table 2: Seminar extraction results (5-fold CV, trained on 50% of corpus): Field-level F1 day-of-week,,
It has been previously shown that the structure available in semi-structured documents such as email messages is useful for information extraction (CITATION; CITATION),,
Table 4 shows results of our full model in terms of field-level F1, compared against TIE, a state-of-theart discriminative system CITATION,,
Template filling is an IE task where the goal is to populate the fields of a target relation, for example to extract the attributes of a job posting CITATION or to recover the details of a corporate acquisition event from a news story CITATION,,
We have annotated this dataset with two additional attributes: date and title.2 We consider this corpus as an example of semi-structured text, where some of the field values appear in the email header, in a tabular structure, or using special formatting (CITATION; CITATION).3 We used a set of rules to extract candidate named entities per the types specified in Figure 2.4 The rules encode information typically used in NER, including content and contextual patterns, as well as lookups in available dictionaries (Finkel et al., 2005; CITATION),,
In evaluating the template filling task, only exact matches are accepted as true positives, where partial matches are counted as errors CITATION,,
For example, Figure 1 shows an extraction from CMU seminar announcement corpus CITATION,,
Following Collins, we employ the averaged Perceptron online algorithm (CITATION; CITATION) for weight learning,,
In addition, extracted relations may be required to be consistent with an existing ontology CITATION,,
In order to increase recall further, additional candidates were extracted based on document structure CITATION,,
Lexical features of this form are commonly used in NER (Finkel et al., 2005; CITATION),,
4.3 Beam Search Unfortunately, optimal local decoding algorithms (such as the Viterbi algorithm in tagging problems CITATION) can not be applied to our problem,,
Documents were pre-processed to extract noun phrases, similarly to CITATION,,
les to extract candidate named entities per the types specified in Figure 2.4 The rules encode information typically used in NER, including content and contextual patterns, as well as lookups in available dictionaries (Finkel et al., 2005; CITATION),,
Typically, these values are extracted based on local evidence, where the most likely entity is assigned to each slot (CITATION; CITATION),,
4 The rule language used is based on cascaded finite state machines CITATION,,
5 Seminar Extraction Task Dataset The CMU seminar announcement dataset CITATION includes 485 emails containing seminar announcements,,
ere some of the field values appear in the email header, in a tabular structure, or using special formatting (CITATION; CITATION).3 We used a set of rules to extract candidate named entities per the types specified in Figure 2.4 The rules encode information typically used in NER, including content and contextual patterns, as well as lookups in available dictionaries (Finkel et al., 2005; CITATION),,
4.2 Learning We employ a discriminative learning algorithm, following CITATION,,
