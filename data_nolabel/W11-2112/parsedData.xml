<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<bodyText confidence="0.547971">
b&apos;Proceedings of the 6th Workshop on Statistical Machine Translation, pages 116122,
Edinburgh, Scotland, UK, July 3031, 2011. c
</bodyText>
<sectionHeader confidence="0.475883" genericHeader="abstract">
2011 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.636696">
TINE: A Metric to Assess MT Adequacy
</title>
<author confidence="0.950916">
Miguel Rios, Wilker Aziz and Lucia Specia
</author>
<affiliation confidence="0.909262">
Research Group in Computational Linguistics
University of Wolverhampton
</affiliation>
<address confidence="0.990891">
Stafford Street, Wolverhampton, WV1 1SB, UK
</address>
<email confidence="0.988746">
{m.rios, w.aziz, l.specia}@wlv.ac.uk
</email>
<sectionHeader confidence="0.990779" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.998777260869565">
We describe TINE, a new automatic evalua-
tion metric for Machine Translation that aims
at assessing segment-level adequacy. Lexical
similarity and shallow-semantics are used as
indicators of adequacy between machine and
reference translations. The metric is based on
the combination of a lexical matching com-
ponent and an adequacy component. Lexi-
cal matching is performed comparing bags-
of-words without any linguistic annotation.
The adequacy component consists in: i) us-
ing ontologies to align predicates (verbs), ii)
using semantic roles to align predicate argu-
ments (core arguments and modifiers), and
iii) matching predicate arguments using dis-
tributional semantics. TINEs performance
is comparable to that of previous metrics
at segment level for several language pairs,
with average Kendalls tau correlation from
0.26 to 0.29. We show that the addition of
the shallow-semantic component improves the
performance of simple lexical matching strate-
gies and metrics such as BLEU.
</bodyText>
<sectionHeader confidence="0.998159" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999714386363636">
The automatic evaluation of Machine Translation
(MT) is a long-standing problem. A number of met-
rics have been proposed in the last two decades,
mostly measuring some form of matching between
the MT output (hypothesis) and one or more human
(reference) translations. However, most of these
metrics focus on fluency aspects, as opposed to ad-
equacy. Therefore, measuring whether the meaning
of the hypothesis and reference translation are the
same or similar is still an understudied problem.
The most commonly used metrics, BLEU (Pap-
ineni et al., 2002) and alike, perform simple exact
matching of n-grams between hypothesis and refer-
ence translations. Such a simple matching proce-
dure has well known limitations, including that the
matching of non-content words counts as much as
the matching of content words, that variations of
words with the same meaning are disregarded, and
that a perfect matching can happen even if the order
of sequences of n-grams in the hypothesis and ref-
erence translation are very different, changing com-
pletely the meaning of the translation.
A number of other metrics have been proposed
to address these limitations, for example, by allow-
ing for the matching of synonyms or paraphrases
of content words, such as in METEOR (Denkowski
and Lavie, 2010). Other attempts have been made
to capture whether the reference translation and hy-
pothesis translations share the same meaning us-
ing shallow semantics, i.e., Semantic Role Labeling
(Gimenez and Marquez, 2007). However, these are
limited to the exact matching of semantic roles and
their fillers.
We propose TINE, a new metric that comple-
ments lexical matching with a shallow semantic
component to better address adequacy. The main
contribution of such a metric is to provide a more
flexible way of measuring the overlap between shal-
low semantic representations that considers both the
semantic structure of the sentence and the content
of the semantic elements. The metric uses SRLs
such as in (Gimenez and Marquez, 2007). However,
it analyses the content of predicates and arguments
seeking for either exact or similar matches. The
</bodyText>
<page confidence="0.999255">
116
</page>
<bodyText confidence="0.991862583333333">
\x0cinexact matching is based on the use of ontologies
such as VerbNet (Schuler, 2006) and distributional
semantics similarity metrics, such as Dekang Lins
thesaurus (Lin, 1998) .
In the remainder of this paper we describe some
related work (Section 2), present our metric - TINE
- (Section 3) and its performance compared to pre-
vious work (Section 4) as well as some further im-
provements. We then provide an analysis of these
results and discuss the limitations of the metric (Sec-
tion 5) and present conclusions and future work
(Section 6).
</bodyText>
<sectionHeader confidence="0.999608" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998396454545455">
A few metrics have been proposed in recent years
to address the problem of measuring whether a hy-
pothesis and a reference translation share the same
meaning. The most well-know metric is probably
METEOR (Banerjee and Lavie, 2005; Denkowski
and Lavie, 2010). METEOR is based on a general-
ized concept of unigram matching between the hy-
pothesis and the reference translation. Alignments
are based on exact, stem, synonym, and paraphrase
matches between words and phrases. However, the
structure of the sentences is not considered.
Wong and Kit (2010) measure word choice and
word order by the matching of words based on
surface forms, stems, senses and semantic similar-
ity. The informativeness of matched and unmatched
words is also weighted.
Liu et al. (2010) propose to match bags of uni-
grams, bigrams and trigrams considering both recall
and precision and F-measure giving more impor-
tance to recall, but also using WordNet synonyms.
Tratz and Hovy (2008) use transformations in or-
der to match short syntactic units defined as Ba-
sic Elements (BE). The BE are minimal-length
syntactically well defined units. For example,
nouns, verbs, adjectives and adverbs can be con-
sidered BE-Unigrams, while a BE-Bigram could be
formed from a syntactic relation (e.g. subject+verb,
verb+object). BEs can be lexically different, but se-
mantically similar.
Pado et al. (2009) uses Textual Entailment fea-
tures extracted from the Standford Entailment Rec-
ognizer (MacCartney et al., 2006). The Textual En-
tailment Recognizer computes matching and mis-
matching features over dependency parses. The met-
ric then predicts the MT quality with a regression
model. The alignment is improved using ontologies.
He et al. (2010) measure the similarity between
hypothesis and reference translation in terms of
the Lexical Functional Grammar (LFG) represen-
tation. The representation uses dependency graphs
to generate unordered sets of dependency triples.
Calculating precision, recall, and F-score on the
sets of triples corresponding to the hypothesis and
reference segments allows measuring similarity at
the lexical and syntactic levels. The measure also
matches WordNet synonyms.
The closest related metric to the one proposed in
this paper is that by Gimenez and Marquez (2007)
and Gimenez et al. (2010), which also uses shallow
semantic representations. Such a metric combines a
number of components, including lexical matching
metrics like BLEU and METEOR, as well as com-
ponents that compute the matching of constituent
and dependency parses, named entities, discourse
representations and semantic roles. However, the se-
mantic role matching is based on exact matching of
roles and role fillers. Moreover, it is not clear what
the contribution of this specific information is for the
overall performance of the metric.
We propose a metric that uses a lexical similar-
ity component and a semantic component in order
to deal with both word choice and semantic struc-
ture. The semantic component is based on seman-
tic roles, but instead of simply matching the surface
forms (i.e. arguments and predicates) it is able to
match similar words.
</bodyText>
<sectionHeader confidence="0.994192" genericHeader="method">
3 Metric Description
</sectionHeader>
<bodyText confidence="0.999538833333334">
The rationale behind TINE is that an adequacy-
oriented metric should go beyond measuring the
matching of lexical items to incorporate information
about the semantic structure of the sentence, as in
(Gimenez et al., 2010). However, the metric should
also be flexible to consider inexact matches of se-
mantic components, similar to what is done with lex-
ical metrics like METEOR (Denkowski and Lavie,
2010). We experiment with TINE having English
as target language because of the availability of lin-
guistic processing tools for this language. The met-
ric is particularly dependent on semantic role label-
</bodyText>
<page confidence="0.997064">
117
</page>
<bodyText confidence="0.998665538461538">
\x0cing systems, which have reached satisfactory perfor-
mance for English (Carreras and Marquez, 2005).
TINE uses semantic role labels (SRL) and lexical se-
mantics to fulfill two requirements by: (i) compare
both the semantic structure and its content across
matching arguments in the hypothesis and refer-
ence translations; and (ii) propose alternative ways
of measuring inexact matches for both predicates
and role fillers. Additionally, it uses an exact lexi-
cal matching component to reward hypotheses that
present the same lexical choices as the reference
translation. The overall score s is defined using the
simple weighted average model in Equation (1):
</bodyText>
<equation confidence="0.983253142857143">
s(H, R) = max
\x1a
L(H, R) + A(H, R)
+
\x1b
RR
(1)
</equation>
<bodyText confidence="0.997661444444444">
where H represents the hypothesis translation, R
represents a reference translation contained in the set
of available references R; L defines the (exact) lex-
ical match component in Equation (2), A defines the
adequacy component in Equation (3); and and
are tunable weights for these two components. If
multiple references are provided, the score of the
segment is the maximum score achieved by compar-
ing the segment to each available reference.
</bodyText>
<equation confidence="0.989839">
L(H, R) =
|H
T
R|
p
|H ||R|
</equation>
<bodyText confidence="0.97670612">
(2)
The lexical match component measures the over-
lap between the two representations in terms of the
cosine similarity metric. A segment, either a hypoth-
esis or a reference, is represented as a bag of tokens
extracted from an unstructured representation, that
is, bag of unigrams (words or stems). Cosine sim-
ilarity was chosen, as opposed to simply checking
the percentage of overlapping words (POW) because
cosine does not penalize differences in the length of
the hypothesis and reference translation as much as
POW. Cosine similarity normalizes the cardinality
of the intersection |H R |using the geometric mean
p
|H ||R |instead of the union |HR|. This is par-
ticularly important for the matching of arguments -
which is also based on cosine similarity. If an hy-
pothesized argument has the same meaning as its
reference translation, but differs from it in length,
cosine will penalize less the matching than POW.
That is specially interesting when core arguments
get merged with modifiers due to bad semantic role
labeling (e.g. [A0 I] [T bought] [A1 something to eat
yesterday] instead of [A0 I] [T bought] [A1 some-
thing to eat] [AM-TMP yesterday]).
</bodyText>
<equation confidence="0.9825742">
A(H, R) =
P
vV verb score(Hv, Rv)
|Vr|
(3)
</equation>
<bodyText confidence="0.998322285714286">
In the adequacy component, V is the set of verbs
aligned between H and R, and |Vr |is the number of
verbs in R. Hereafter the indexes h and r stand for
hypothesis and reference translations, respectively.
Verbs are aligned using VerbNet (Schuler, 2006) and
VerbOcean (Chklovski and Pantel, 2004). A verb in
the hypothesis vh is aligned to a verb in the refer-
ence vr if they are related according to the follow-
ing heuristics: (i) the pair of verbs share at least one
class in VerbNet; or (ii) the pair of verbs holds a re-
lation in VerbOcean.
For example, in VerbNet the verbs spook and ter-
rify share the same class amuse-31.1, and in VerbO-
cean the verb dress is related to the verb wear.
</bodyText>
<equation confidence="0.9925125">
verb score(Hv, Rv) =
P
aArAt
arg score(Ha, Ra)
|Ar|
(4)
</equation>
<bodyText confidence="0.999579285714286">
The similarity between the arguments of a verb
pair (vh, vr) in V is measured as defined in Equa-
tion (4), where Ah and At are the sets of labeled
arguments of the hypothesis and the reference re-
spectively and |Ar |is the number of arguments of
the verb in R. In other words, we only measure the
similarity of arguments in a pair of sentences that are
annotated with the same role. This ensures that the
structure of the sentence is taken into account (for
example, an argument in the role of agent would not
be compared against an argument in a role of experi-
encer). Additionally, by restricting the comparison
to arguments of a given verb pair, we avoid argument
confusion in sentences with multiple verbs.
The arg score(Ha, Ra) computation is based on
the cosine similarity as in Equation (2). We treat
the tokens in the argument as a bag-of-words. How-
ever, in this case we change the representation of
the segments. If the two sets do not match exactly,
we expand both of them by adding similar words.
For every mismatch in a segment, we retrieve the
</bodyText>
<page confidence="0.993853">
118
</page>
<bodyText confidence="0.9158371">
\x0c20-most similar words from Dekang Lins distribu-
tional thesaurus (Lin, 1998), resulting in sets with
richer lexical variety.
The following example shows how the computa-
tion of A(H, R) is performed, considering the fol-
lowing hypothesis and reference translations:
H: The lack of snow discourages people from ordering
ski stays in hotels and boarding houses.
R: The lack of snow is putting people off booking ski
holidays in hotels and guest houses.
</bodyText>
<listItem confidence="0.9995116">
1. extract verbs from H: Vh = {discourages, ordering}
2. extract verbs from R: Vr = {putting, booking}
3. similar verbs aligned with VerbNet (shared class
get-13.5.1): V = {(vh = order,vr = book)}
4. compare arguments of (vh = order,vr = book):
</listItem>
<equation confidence="0.8134476">
Ah = {A0, A1, AM-LOC}
Ar = {A0, A1, AM-LOC}
5. Ah Ar = {A0, A1, AM-LOC}
6. exact matches:
HA0 = {people} and RA0 = {people}
</equation>
<bodyText confidence="0.427857">
argument score = 1
7. different word forms: expand the representation:
</bodyText>
<equation confidence="0.9155524">
HA1 = {ski, stays} and RA1 = {ski, holidays}
expand to:
HA1 = {{ski},{stays, remain... journey...}}
RA1 = {{ski},{holidays, vacations, trips... jour-
ney...}}
</equation>
<listItem confidence="0.76086575">
argument score = 0.5
8. similarly to HAMLOC and RAMLOC
argument score = 0.72
9. verb score (order, book) = 1+0.5+0.72
</listItem>
<equation confidence="0.848878333333333">
3 = 0.74
10. A(H, R) = 0.74
2 = 0.37
</equation>
<bodyText confidence="0.9904602">
Different from previous work, we have not used
WordNet to measure lexical similarity for two main
reasons: problems with lexical ambiguity and lim-
ited coverage in WordNet (instances of named enti-
ties are not in WordNet, e.g. Barack Obama). For
example, in WordNet the aligned verbs (order/book)
from the previous hypothesis and reference trans-
lations have: 9 senses - order (e.g. give instruc-
tions to or direct somebody to do something with
authority, make a request for something, etc.) - and
4 senses - book (engage for a performance, arrange
for and reserve (something for someone else) in ad-
vance, etc.). Thus, a WordNet-based similarity mea-
sure would require disambiguating segments, an ad-
ditional step and a possible source of errors. Second,
a thresholds would need to be set to determine when
a pair of verbs is aligned. In contrast, the structure of
VerbNet (i.e. clusters of verbs) allows a binary deci-
sion, although the VerbNet heuristic results in some
errors, as we discuss in Section 5.
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.983201285714286">
We set the weights and by experimental test-
ing to = 1 and = 0.25. The lexical component
weight is prioritized because it has shown a good av-
erage Kendalls tau correlation (0.23) on a develop-
ment dataset (Callison-Burch et al., 2010). Table 1
shows the correlation of the lexical component with
human judgments for a number of language pairs.
</bodyText>
<tableCaption confidence="0.982408">
Table 1: Kendalls tau segment-level correlation of the
</tableCaption>
<table confidence="0.696999">
lexical component with human judgments
Metric cz-en fr-en de-en es-en avg
Lexical 0.27 0.21 0.26 0.19 0.23
</table>
<bodyText confidence="0.982848714285714">
We use the SENNA1 SRL system to tag the
dataset with semantic roles. SENNA has shown to
have achieved an F-measure of 75.79% for tagging
semantic roles over the CoNLL 2005 2 benchmark.
We compare our metric against standard BLEU
(Papineni et al., 2002), METEOR (Denkowski and
Lavie, 2010) and other previous metrics reported in
(Callison-Burch et al., 2010) which also claim to use
some form of semantic information (see Section 2
for their description). The comparison is made in
terms of Kendalls tau correlation against the human
judgments at a segment-level. For our submission to
the shared evaluation task, system-level scores are
obtained by averaging the segment-level scores.
TINE achieves the same average correlation with
BLUE, but outperforms it for some language pairs.
Additionally, TINE outperforms some of the previ-
ous which use WordNet to deal with synonyms as
part of the lexical matching.
The closest metric to TINE (Gimenez et al.,
2010), which also uses semantic roles as one of its
</bodyText>
<equation confidence="0.3308975">
1
http://ml.nec-labs.com/senna/
2
http://www.lsi.upc.edu/ srlconll/
</equation>
<page confidence="0.994243">
119
</page>
<tableCaption confidence="0.951176">
\x0cTable 2: Comparison with previous semantically-
oriented metrics using segment-level Kendalls tau cor-
</tableCaption>
<table confidence="0.980990333333333">
relation with human judgments
Metric cz-en fr-en de-en es-en avg
(Liu et al.,
2010)
0.34 0.34 0.38 0.34 0.35
(Gimenez
et al., 2010)
0.34 0.33 0.34 0.33 0.33
(Wong and
Kit, 2010)
0.33 0.27 0.37 0.32 0.32
METEOR 0.33 0.27 0.36 0.33 0.32
TINE 0.28 0.25 0.30 0.22 0.26
BLEU 0.26 0.22 0.27 0.28 0.26
(He et al.,
2010)
0.15 0.14 0.17 0.21 0.17
(Tratz
and Hovy,
2008)
0.05 0.0 0.12 0.05 0.05
</table>
<bodyText confidence="0.99691725">
components, achieves better performance. However,
this metric is a rather complex combination of a
number of other metrics to deal with different lin-
guistic phenomena.
</bodyText>
<subsectionHeader confidence="0.996063">
4.1 Further Improvements
</subsectionHeader>
<bodyText confidence="0.995229111111111">
As an additional experiment, we use BLEU as the
lexical component L(H, R) in order to test if the
shallow-semantic component can contribute to the
performance of this standard evaluation metric. Ta-
ble 3 shows the results of the combination of BLEU
and the shallow-semantic component using the same
parameter configuration as in Section 4. The addi-
tion of the shallow-semantic component increased
the average correlation of BLEU from 0.26 to 0.28.
</bodyText>
<tableCaption confidence="0.994126">
Table 3: TINE-B: Combination of BLEU and the
</tableCaption>
<table confidence="0.785293333333333">
shallow-semantic component
Metric cz-en fr-en de-en es-en avg
TINE-B 0.27 0.25 0.30 0.30 0.28
</table>
<bodyText confidence="0.98155165">
Finally, we improve the tuning of the weights of
the components ( and parameters) by using a
simple genetic algorithm (Back et al., 1999) to se-
lect the weights that maximize the correlation with
human scores on a development set (we use the de-
velopment sets from WMT10 (Callison-Burch et al.,
2010)). The configuration of the genetic algorithm
is as follows:
Fitness function: Kendalls tau correlation
Chromosome: two real numbers, and
Number of individuals: 80
Number of generations: 100
Selection method: roulette
Crossover probability: 0.9
Mutation probability: 0.01
Table 4 shows the parameter values obtaining
from tuning for each language pair and the corre-
lation achieved by the metric with such parameters.
With such an optimization step the average correla-
tion of the metric increases to 0.29.
</bodyText>
<tableCaption confidence="0.878456">
Table 4: Optimized values of the parameters using a ge-
netic algorithm and Kendalls tau and final correlation of
the metric on the test sets
</tableCaption>
<table confidence="0.977479333333333">
Language pair Correlation
cz-en 0.28 0.62 0.02
fr-en 0.25 0.91 0.03
de-en 0.30 0.72 0.1
es-en 0.31 0.57 0.02
avg 0.29
</table>
<sectionHeader confidence="0.986389" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.940488882352941">
In what follows we discuss with a few examples
some of the common errors made by TINE. Over-
all, we consider the following categories of errors:
1. Lack of coverage of the ontologies.
R: This year, women were awarded the Nobel Prize in all
fields except physics
H: This year the women received the Nobel prizes in all
categories less physical
The lack of coverage in VerbNet prevented the
detection of the similarity between receive and
award.
2. Matching of unrelated verbs.
R: If snow falls on the slopes this week, Christmas will
sell out too, says Schiefert.
H: If the roads remain snowfall during the week, the dates
of Christmas will dry up, said Schiefert.
In VerbOcean remain and say are incorrectly
</bodyText>
<page confidence="0.889837">
120
</page>
<bodyText confidence="0.9762408">
\x0csaid to be related. VerbOcean was cre-
ated by a semi-automatic extraction algorithm
(Chklovski and Pantel, 2004) with an average
accuracy of 65.5%.
3. Incorrect tagging of the semantic roles by
SENNA.
R: Colder weather is forecast for Thursday, so if anything
falls, it should be snow.
H: On Thursday , must fall temperatures and, if there is
rain, in the mountains should.
The position of the predicates affects the SRL
tagging. The predicate fall has the following
roles (A1, V, and S-A1) in the reference, and
the following roles (AM-ADV, A0, AM-MOD,
and AM-DIS) in the hypothesis. As a con-
sequence, the metric cannot attempt to match
the fillers. Also, SRL systems do not detect
phrasal verbs such as in the example of Section
3, where the action putting people off is similar
to discourages.
</bodyText>
<sectionHeader confidence="0.996963" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99988064516129">
We have presented an MT evaluation metric based
on the alignment of semantic roles and flexible
matching of role fillers between hypothesis and ref-
erence translations. To deal with inexact matches,
the metric uses ontologies and distributional seman-
tics, as opposed to lexical databases like WordNet,
in order to minimize ambiguity and lack of cover-
age. The metric also uses an exact lexical matching
component to reward hypotheses that present lexical
choices similar to those of the reference translation.
Given the simplicity of the metric, it has achieved
competitive results. We have shown that the addition
of the shallow-semantic component into a lexical
component yields absolute improvements in the cor-
relation of 3%-6% on average, depending on the lex-
ical component used (cosine similarity or BLEU).
In future work, in order to improve the perfor-
mance of the metric we plan to add components to
address a few other linguistic phenomena such as
in (Gimenez and Marquez, 2007; Gimenez et al.,
2010). In order to deal with the coverage problem
of an ontology, we plan to use distributional seman-
tics (i.e. word space models) also to align the pred-
icates. We consider using a backoff model for the
shallow-semantic component to deal with the very
frequent cases where there are no comparable pred-
icates between the reference and hypothesis transla-
tions, which result in a 0 score from the semantic
component. Finally, we plan to improve the lexical
component to better tackle fluency, for example, by
adding information about the word order.
</bodyText>
<sectionHeader confidence="0.987288" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997616720930232">
Thomas Back, David B. Fogel, and Zbigniew
Michalewicz, editors. 1999. Evolutionary Com-
putation 1, Basic Algorithms and Operators. IOP
Publishing Ltd., Bristol, UK, 1st edition.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 6572, Ann Arbor, Michigan, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 1753, Uppsala, Sweden, July.
Xavier Carreras and Llus Marquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling.
In Proceedings of the 9th Conference on Natural Lan-
guage Learning, CoNLL-2005, Ann Arbor, MI USA.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic Verb
Relations. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 3340, Barcelona,
Spain, July.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: Improved evaluation
support for five target languages. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 339342, July.
Jesus Gimenez and Llus Marquez. 2007. Linguistic fea-
tures for automatic evaluation of heterogenous mt sys-
tems. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, StatMT 07, pages 256
264, Stroudsburg, PA, USA.
Jesus Gimenez, Llus Marquez, Elisabet Comelles, Irene
Castellon, and Victoria Arranz. 2010. Document-
level automatic mt evaluation based on discourse rep-
resentations. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, WMT 10, pages 333338, Stroudsburg, PA,
USA.
</reference>
<page confidence="0.937113">
121
</page>
<reference confidence="0.999789934782609">
\x0cYifan He, Jinhua Du, Andy Way, and Josef van Gen-
abith. 2010. The dcu dependency-based metric in
wmt-metricsmatr 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, WMT 10, pages 349353, Strouds-
burg, PA, USA.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL 98, pages 768
774, Stroudsburg, PA, USA.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010.
Tesla: translation evaluation of sentences with linear-
programming-based analysis. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, WMT 10, pages 354359,
Stroudsburg, PA, USA.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the Human Language
Technology Conference of the NAACL, pages 4148,
New York City, USA, June.
Sebastian Pado, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23:181193, September.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL 02, pages 311318, Strouds-
burg, PA, USA.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
University of Pennsylvania.
Stephen Tratz and Eduard Hovy. 2008. Summarisation
evaluation using transformed basic elements. In Pro-
ceedings TAC 2008.
Billy T.-M. Wong and Chunyu Kit. 2010. The parameter-
optimized atec metric for mt evaluation. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, WMT 10, pages 360
364, Stroudsburg, PA, USA.
</reference>
<page confidence="0.963036">
122
</page>
<figure confidence="0.260741">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.685199">
<note confidence="0.972508333333333">b&apos;Proceedings of the 6th Workshop on Statistical Machine Translation, pages 116122, Edinburgh, Scotland, UK, July 3031, 2011. c 2011 Association for Computational Linguistics</note>
<title confidence="0.83496">TINE: A Metric to Assess MT Adequacy</title>
<author confidence="0.999839">Miguel Rios</author>
<author confidence="0.999839">Wilker Aziz</author>
<author confidence="0.999839">Lucia Specia</author>
<affiliation confidence="0.9990515">Research Group in Computational Linguistics University of Wolverhampton</affiliation>
<address confidence="0.997179">Stafford Street, Wolverhampton, WV1 1SB, UK</address>
<email confidence="0.997563">m.rios@wlv.ac.uk</email>
<email confidence="0.997563">w.aziz@wlv.ac.uk</email>
<email confidence="0.997563">l.specia@wlv.ac.uk</email>
<abstract confidence="0.995366208333333">We describe TINE, a new automatic evaluation metric for Machine Translation that aims at assessing segment-level adequacy. Lexical similarity and shallow-semantics are used as indicators of adequacy between machine and reference translations. The metric is based on the combination of a lexical matching component and an adequacy component. Lexical matching is performed comparing bagsof-words without any linguistic annotation. The adequacy component consists in: i) using ontologies to align predicates (verbs), ii) using semantic roles to align predicate arguments (core arguments and modifiers), and iii) matching predicate arguments using distributional semantics. TINEs performance is comparable to that of previous metrics at segment level for several language pairs, with average Kendalls tau correlation from 0.26 to 0.29. We show that the addition of the shallow-semantic component improves the performance of simple lexical matching strategies and metrics such as BLEU.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thomas Back</author>
<author>David B Fogel</author>
<author>Zbigniew Michalewicz</author>
<author>editors</author>
</authors>
<title>edition.</title>
<date>1999</date>
<booktitle>Evolutionary Computation 1, Basic Algorithms and Operators. IOP Publishing Ltd.,</booktitle>
<location>Bristol, UK,</location>
<contexts>
<context position="17194" citStr="Back et al., 1999" startWordPosition="2808" endWordPosition="2811">-semantic component can contribute to the performance of this standard evaluation metric. Table 3 shows the results of the combination of BLEU and the shallow-semantic component using the same parameter configuration as in Section 4. The addition of the shallow-semantic component increased the average correlation of BLEU from 0.26 to 0.28. Table 3: TINE-B: Combination of BLEU and the shallow-semantic component Metric cz-en fr-en de-en es-en avg TINE-B 0.27 0.25 0.30 0.30 0.28 Finally, we improve the tuning of the weights of the components ( and parameters) by using a simple genetic algorithm (Back et al., 1999) to select the weights that maximize the correlation with human scores on a development set (we use the development sets from WMT10 (Callison-Burch et al., 2010)). The configuration of the genetic algorithm is as follows: Fitness function: Kendalls tau correlation Chromosome: two real numbers, and Number of individuals: 80 Number of generations: 100 Selection method: roulette Crossover probability: 0.9 Mutation probability: 0.01 Table 4 shows the parameter values obtaining from tuning for each language pair and the correlation achieved by the metric with such parameters. With such an optimizat</context>
</contexts>
<marker>Back, Fogel, Michalewicz, editors, 1999</marker>
<rawString>Thomas Back, David B. Fogel, and Zbigniew Michalewicz, editors. 1999. Evolutionary Computation 1, Basic Algorithms and Operators. IOP Publishing Ltd., Bristol, UK, 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>6572</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="4313" citStr="Banerjee and Lavie, 2005" startWordPosition="669" endWordPosition="672">urus (Lin, 1998) . In the remainder of this paper we describe some related work (Section 2), present our metric - TINE - (Section 3) and its performance compared to previous work (Section 4) as well as some further improvements. We then provide an analysis of these results and discuss the limitations of the metric (Section 5) and present conclusions and future work (Section 6). 2 Related Work A few metrics have been proposed in recent years to address the problem of measuring whether a hypothesis and a reference translation share the same meaning. The most well-know metric is probably METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). METEOR is based on a generalized concept of unigram matching between the hypothesis and the reference translation. Alignments are based on exact, stem, synonym, and paraphrase matches between words and phrases. However, the structure of the sentences is not considered. Wong and Kit (2010) measure word choice and word order by the matching of words based on surface forms, stems, senses and semantic similarity. The informativeness of matched and unmatched words is also weighted. Liu et al. (2010) propose to match bags of unigrams, bigrams and trigrams considering bo</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Kay Peterson</author>
<author>Mark Przybocki</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>1753</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="14442" citStr="Callison-Burch et al., 2010" startWordPosition="2364" endWordPosition="2367">a WordNet-based similarity measure would require disambiguating segments, an additional step and a possible source of errors. Second, a thresholds would need to be set to determine when a pair of verbs is aligned. In contrast, the structure of VerbNet (i.e. clusters of verbs) allows a binary decision, although the VerbNet heuristic results in some errors, as we discuss in Section 5. 4 Results We set the weights and by experimental testing to = 1 and = 0.25. The lexical component weight is prioritized because it has shown a good average Kendalls tau correlation (0.23) on a development dataset (Callison-Burch et al., 2010). Table 1 shows the correlation of the lexical component with human judgments for a number of language pairs. Table 1: Kendalls tau segment-level correlation of the lexical component with human judgments Metric cz-en fr-en de-en es-en avg Lexical 0.27 0.21 0.26 0.19 0.23 We use the SENNA1 SRL system to tag the dataset with semantic roles. SENNA has shown to have achieved an F-measure of 75.79% for tagging semantic roles over the CoNLL 2005 2 benchmark. We compare our metric against standard BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2010) and other previous metrics reported in </context>
<context position="17355" citStr="Callison-Burch et al., 2010" startWordPosition="2836" endWordPosition="2839">shallow-semantic component using the same parameter configuration as in Section 4. The addition of the shallow-semantic component increased the average correlation of BLEU from 0.26 to 0.28. Table 3: TINE-B: Combination of BLEU and the shallow-semantic component Metric cz-en fr-en de-en es-en avg TINE-B 0.27 0.25 0.30 0.30 0.28 Finally, we improve the tuning of the weights of the components ( and parameters) by using a simple genetic algorithm (Back et al., 1999) to select the weights that maximize the correlation with human scores on a development set (we use the development sets from WMT10 (Callison-Burch et al., 2010)). The configuration of the genetic algorithm is as follows: Fitness function: Kendalls tau correlation Chromosome: two real numbers, and Number of individuals: 80 Number of generations: 100 Selection method: roulette Crossover probability: 0.9 Mutation probability: 0.01 Table 4 shows the parameter values obtaining from tuning for each language pair and the correlation achieved by the metric with such parameters. With such an optimization step the average correlation of the metric increases to 0.29. Table 4: Optimized values of the parameters using a genetic algorithm and Kendalls tau and fina</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, and Omar Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 1753, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llus Marquez</author>
</authors>
<title>Introduction to the conll-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<contexts>
<context position="7917" citStr="Carreras and Marquez, 2005" startWordPosition="1233" endWordPosition="1236">d measuring the matching of lexical items to incorporate information about the semantic structure of the sentence, as in (Gimenez et al., 2010). However, the metric should also be flexible to consider inexact matches of semantic components, similar to what is done with lexical metrics like METEOR (Denkowski and Lavie, 2010). We experiment with TINE having English as target language because of the availability of linguistic processing tools for this language. The metric is particularly dependent on semantic role label117 \x0cing systems, which have reached satisfactory performance for English (Carreras and Marquez, 2005). TINE uses semantic role labels (SRL) and lexical semantics to fulfill two requirements by: (i) compare both the semantic structure and its content across matching arguments in the hypothesis and reference translations; and (ii) propose alternative ways of measuring inexact matches for both predicates and role fillers. Additionally, it uses an exact lexical matching component to reward hypotheses that present the same lexical choices as the reference translation. The overall score s is defined using the simple weighted average model in Equation (1): s(H, R) = max \x1a L(H, R) + A(H, R) + \x1b</context>
</contexts>
<marker>Carreras, Marquez, 2005</marker>
<rawString>Xavier Carreras and Llus Marquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the 9th Conference on Natural Language Learning, CoNLL-2005,</booktitle>
<location>Ann Arbor, MI USA.</location>
<marker></marker>
<rawString>In Proceedings of the 9th Conference on Natural Language Learning, CoNLL-2005, Ann Arbor, MI USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>3340</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="10490" citStr="Chklovski and Pantel, 2004" startWordPosition="1664" endWordPosition="1667">, cosine will penalize less the matching than POW. That is specially interesting when core arguments get merged with modifiers due to bad semantic role labeling (e.g. [A0 I] [T bought] [A1 something to eat yesterday] instead of [A0 I] [T bought] [A1 something to eat] [AM-TMP yesterday]). A(H, R) = P vV verb score(Hv, Rv) |Vr| (3) In the adequacy component, V is the set of verbs aligned between H and R, and |Vr |is the number of verbs in R. Hereafter the indexes h and r stand for hypothesis and reference translations, respectively. Verbs are aligned using VerbNet (Schuler, 2006) and VerbOcean (Chklovski and Pantel, 2004). A verb in the hypothesis vh is aligned to a verb in the reference vr if they are related according to the following heuristics: (i) the pair of verbs share at least one class in VerbNet; or (ii) the pair of verbs holds a relation in VerbOcean. For example, in VerbNet the verbs spook and terrify share the same class amuse-31.1, and in VerbOcean the verb dress is related to the verb wear. verb score(Hv, Rv) = P aArAt arg score(Ha, Ra) |Ar| (4) The similarity between the arguments of a verb pair (vh, vr) in V is measured as defined in Equation (4), where Ah and At are the sets of labeled argume</context>
<context position="18957" citStr="Chklovski and Pantel, 2004" startWordPosition="3100" endWordPosition="3103">men were awarded the Nobel Prize in all fields except physics H: This year the women received the Nobel prizes in all categories less physical The lack of coverage in VerbNet prevented the detection of the similarity between receive and award. 2. Matching of unrelated verbs. R: If snow falls on the slopes this week, Christmas will sell out too, says Schiefert. H: If the roads remain snowfall during the week, the dates of Christmas will dry up, said Schiefert. In VerbOcean remain and say are incorrectly 120 \x0csaid to be related. VerbOcean was created by a semi-automatic extraction algorithm (Chklovski and Pantel, 2004) with an average accuracy of 65.5%. 3. Incorrect tagging of the semantic roles by SENNA. R: Colder weather is forecast for Thursday, so if anything falls, it should be snow. H: On Thursday , must fall temperatures and, if there is rain, in the mountains should. The position of the predicates affects the SRL tagging. The predicate fall has the following roles (A1, V, and S-A1) in the reference, and the following roles (AM-ADV, A0, AM-MOD, and AM-DIS) in the hypothesis. As a consequence, the metric cannot attempt to match the fillers. Also, SRL systems do not detect phrasal verbs such as in the </context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 3340, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor-next and the meteor paraphrase tables: Improved evaluation support for five target languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>339342</pages>
<contexts>
<context position="2690" citStr="Denkowski and Lavie, 2010" startWordPosition="404" endWordPosition="407">uch a simple matching procedure has well known limitations, including that the matching of non-content words counts as much as the matching of content words, that variations of words with the same meaning are disregarded, and that a perfect matching can happen even if the order of sequences of n-grams in the hypothesis and reference translation are very different, changing completely the meaning of the translation. A number of other metrics have been proposed to address these limitations, for example, by allowing for the matching of synonyms or paraphrases of content words, such as in METEOR (Denkowski and Lavie, 2010). Other attempts have been made to capture whether the reference translation and hypothesis translations share the same meaning using shallow semantics, i.e., Semantic Role Labeling (Gimenez and Marquez, 2007). However, these are limited to the exact matching of semantic roles and their fillers. We propose TINE, a new metric that complements lexical matching with a shallow semantic component to better address adequacy. The main contribution of such a metric is to provide a more flexible way of measuring the overlap between shallow semantic representations that considers both the semantic struc</context>
<context position="4341" citStr="Denkowski and Lavie, 2010" startWordPosition="673" endWordPosition="676">remainder of this paper we describe some related work (Section 2), present our metric - TINE - (Section 3) and its performance compared to previous work (Section 4) as well as some further improvements. We then provide an analysis of these results and discuss the limitations of the metric (Section 5) and present conclusions and future work (Section 6). 2 Related Work A few metrics have been proposed in recent years to address the problem of measuring whether a hypothesis and a reference translation share the same meaning. The most well-know metric is probably METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). METEOR is based on a generalized concept of unigram matching between the hypothesis and the reference translation. Alignments are based on exact, stem, synonym, and paraphrase matches between words and phrases. However, the structure of the sentences is not considered. Wong and Kit (2010) measure word choice and word order by the matching of words based on surface forms, stems, senses and semantic similarity. The informativeness of matched and unmatched words is also weighted. Liu et al. (2010) propose to match bags of unigrams, bigrams and trigrams considering both recall and precision and </context>
<context position="7615" citStr="Denkowski and Lavie, 2010" startWordPosition="1187" endWordPosition="1190"> word choice and semantic structure. The semantic component is based on semantic roles, but instead of simply matching the surface forms (i.e. arguments and predicates) it is able to match similar words. 3 Metric Description The rationale behind TINE is that an adequacyoriented metric should go beyond measuring the matching of lexical items to incorporate information about the semantic structure of the sentence, as in (Gimenez et al., 2010). However, the metric should also be flexible to consider inexact matches of semantic components, similar to what is done with lexical metrics like METEOR (Denkowski and Lavie, 2010). We experiment with TINE having English as target language because of the availability of linguistic processing tools for this language. The metric is particularly dependent on semantic role label117 \x0cing systems, which have reached satisfactory performance for English (Carreras and Marquez, 2005). TINE uses semantic role labels (SRL) and lexical semantics to fulfill two requirements by: (i) compare both the semantic structure and its content across matching arguments in the hypothesis and reference translations; and (ii) propose alternative ways of measuring inexact matches for both predi</context>
<context position="15002" citStr="Denkowski and Lavie, 2010" startWordPosition="2456" endWordPosition="2459">tion (0.23) on a development dataset (Callison-Burch et al., 2010). Table 1 shows the correlation of the lexical component with human judgments for a number of language pairs. Table 1: Kendalls tau segment-level correlation of the lexical component with human judgments Metric cz-en fr-en de-en es-en avg Lexical 0.27 0.21 0.26 0.19 0.23 We use the SENNA1 SRL system to tag the dataset with semantic roles. SENNA has shown to have achieved an F-measure of 75.79% for tagging semantic roles over the CoNLL 2005 2 benchmark. We compare our metric against standard BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2010) and other previous metrics reported in (Callison-Burch et al., 2010) which also claim to use some form of semantic information (see Section 2 for their description). The comparison is made in terms of Kendalls tau correlation against the human judgments at a segment-level. For our submission to the shared evaluation task, system-level scores are obtained by averaging the segment-level scores. TINE achieves the same average correlation with BLUE, but outperforms it for some language pairs. Additionally, TINE outperforms some of the previous which use WordNet to deal with synonyms as part of th</context>
</contexts>
<marker>Denkowski, Lavie, 2010</marker>
<rawString>Michael Denkowski and Alon Lavie. 2010. Meteor-next and the meteor paraphrase tables: Improved evaluation support for five target languages. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 339342, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesus Gimenez</author>
<author>Llus Marquez</author>
</authors>
<title>Linguistic features for automatic evaluation of heterogenous mt systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT 07,</booktitle>
<pages>256--264</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2899" citStr="Gimenez and Marquez, 2007" startWordPosition="435" endWordPosition="438">disregarded, and that a perfect matching can happen even if the order of sequences of n-grams in the hypothesis and reference translation are very different, changing completely the meaning of the translation. A number of other metrics have been proposed to address these limitations, for example, by allowing for the matching of synonyms or paraphrases of content words, such as in METEOR (Denkowski and Lavie, 2010). Other attempts have been made to capture whether the reference translation and hypothesis translations share the same meaning using shallow semantics, i.e., Semantic Role Labeling (Gimenez and Marquez, 2007). However, these are limited to the exact matching of semantic roles and their fillers. We propose TINE, a new metric that complements lexical matching with a shallow semantic component to better address adequacy. The main contribution of such a metric is to provide a more flexible way of measuring the overlap between shallow semantic representations that considers both the semantic structure of the sentence and the content of the semantic elements. The metric uses SRLs such as in (Gimenez and Marquez, 2007). However, it analyses the content of predicates and arguments seeking for either exact</context>
<context position="6334" citStr="Gimenez and Marquez (2007)" startWordPosition="980" endWordPosition="983">ssion model. The alignment is improved using ontologies. He et al. (2010) measure the similarity between hypothesis and reference translation in terms of the Lexical Functional Grammar (LFG) representation. The representation uses dependency graphs to generate unordered sets of dependency triples. Calculating precision, recall, and F-score on the sets of triples corresponding to the hypothesis and reference segments allows measuring similarity at the lexical and syntactic levels. The measure also matches WordNet synonyms. The closest related metric to the one proposed in this paper is that by Gimenez and Marquez (2007) and Gimenez et al. (2010), which also uses shallow semantic representations. Such a metric combines a number of components, including lexical matching metrics like BLEU and METEOR, as well as components that compute the matching of constituent and dependency parses, named entities, discourse representations and semantic roles. However, the semantic role matching is based on exact matching of roles and role fillers. Moreover, it is not clear what the contribution of this specific information is for the overall performance of the metric. We propose a metric that uses a lexical similarity compon</context>
</contexts>
<marker>Gimenez, Marquez, 2007</marker>
<rawString>Jesus Gimenez and Llus Marquez. 2007. Linguistic features for automatic evaluation of heterogenous mt systems. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT 07, pages 256 264, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesus Gimenez</author>
<author>Llus Marquez</author>
<author>Elisabet Comelles</author>
<author>Irene Castellon</author>
<author>Victoria Arranz</author>
</authors>
<title>Documentlevel automatic mt evaluation based on discourse representations.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT 10,</booktitle>
<pages>333338</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6360" citStr="Gimenez et al. (2010)" startWordPosition="985" endWordPosition="988">mproved using ontologies. He et al. (2010) measure the similarity between hypothesis and reference translation in terms of the Lexical Functional Grammar (LFG) representation. The representation uses dependency graphs to generate unordered sets of dependency triples. Calculating precision, recall, and F-score on the sets of triples corresponding to the hypothesis and reference segments allows measuring similarity at the lexical and syntactic levels. The measure also matches WordNet synonyms. The closest related metric to the one proposed in this paper is that by Gimenez and Marquez (2007) and Gimenez et al. (2010), which also uses shallow semantic representations. Such a metric combines a number of components, including lexical matching metrics like BLEU and METEOR, as well as components that compute the matching of constituent and dependency parses, named entities, discourse representations and semantic roles. However, the semantic role matching is based on exact matching of roles and role fillers. Moreover, it is not clear what the contribution of this specific information is for the overall performance of the metric. We propose a metric that uses a lexical similarity component and a semantic compone</context>
<context position="15671" citStr="Gimenez et al., 2010" startWordPosition="2561" endWordPosition="2564">n-Burch et al., 2010) which also claim to use some form of semantic information (see Section 2 for their description). The comparison is made in terms of Kendalls tau correlation against the human judgments at a segment-level. For our submission to the shared evaluation task, system-level scores are obtained by averaging the segment-level scores. TINE achieves the same average correlation with BLUE, but outperforms it for some language pairs. Additionally, TINE outperforms some of the previous which use WordNet to deal with synonyms as part of the lexical matching. The closest metric to TINE (Gimenez et al., 2010), which also uses semantic roles as one of its 1 http://ml.nec-labs.com/senna/ 2 http://www.lsi.upc.edu/ srlconll/ 119 \x0cTable 2: Comparison with previous semanticallyoriented metrics using segment-level Kendalls tau correlation with human judgments Metric cz-en fr-en de-en es-en avg (Liu et al., 2010) 0.34 0.34 0.38 0.34 0.35 (Gimenez et al., 2010) 0.34 0.33 0.34 0.33 0.33 (Wong and Kit, 2010) 0.33 0.27 0.37 0.32 0.32 METEOR 0.33 0.27 0.36 0.33 0.32 TINE 0.28 0.25 0.30 0.22 0.26 BLEU 0.26 0.22 0.27 0.28 0.26 (He et al., 2010) 0.15 0.14 0.17 0.21 0.17 (Tratz and Hovy, 2008) 0.05 0.0 0.12 0.0</context>
</contexts>
<marker>Gimenez, Marquez, Comelles, Castellon, Arranz, 2010</marker>
<rawString>Jesus Gimenez, Llus Marquez, Elisabet Comelles, Irene Castellon, and Victoria Arranz. 2010. Documentlevel automatic mt evaluation based on discourse representations. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT 10, pages 333338, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cYifan He</author>
<author>Jinhua Du</author>
<author>Andy Way</author>
<author>Josef van Genabith</author>
</authors>
<title>The dcu dependency-based metric in wmt-metricsmatr 2010.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT 10,</booktitle>
<pages>349353</pages>
<location>Stroudsburg, PA, USA.</location>
<marker>He, Du, Way, van Genabith, 2010</marker>
<rawString>\x0cYifan He, Jinhua Du, Andy Way, and Josef van Genabith. 2010. The dcu dependency-based metric in wmt-metricsmatr 2010. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT 10, pages 349353, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL 98,</booktitle>
<pages>768--774</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3705" citStr="Lin, 1998" startWordPosition="565" endWordPosition="566">address adequacy. The main contribution of such a metric is to provide a more flexible way of measuring the overlap between shallow semantic representations that considers both the semantic structure of the sentence and the content of the semantic elements. The metric uses SRLs such as in (Gimenez and Marquez, 2007). However, it analyses the content of predicates and arguments seeking for either exact or similar matches. The 116 \x0cinexact matching is based on the use of ontologies such as VerbNet (Schuler, 2006) and distributional semantics similarity metrics, such as Dekang Lins thesaurus (Lin, 1998) . In the remainder of this paper we describe some related work (Section 2), present our metric - TINE - (Section 3) and its performance compared to previous work (Section 4) as well as some further improvements. We then provide an analysis of these results and discuss the limitations of the metric (Section 5) and present conclusions and future work (Section 6). 2 Related Work A few metrics have been proposed in recent years to address the problem of measuring whether a hypothesis and a reference translation share the same meaning. The most well-know metric is probably METEOR (Banerjee and Lav</context>
<context position="12075" citStr="Lin, 1998" startWordPosition="1958" endWordPosition="1959"> an argument in a role of experiencer). Additionally, by restricting the comparison to arguments of a given verb pair, we avoid argument confusion in sentences with multiple verbs. The arg score(Ha, Ra) computation is based on the cosine similarity as in Equation (2). We treat the tokens in the argument as a bag-of-words. However, in this case we change the representation of the segments. If the two sets do not match exactly, we expand both of them by adding similar words. For every mismatch in a segment, we retrieve the 118 \x0c20-most similar words from Dekang Lins distributional thesaurus (Lin, 1998), resulting in sets with richer lexical variety. The following example shows how the computation of A(H, R) is performed, considering the following hypothesis and reference translations: H: The lack of snow discourages people from ordering ski stays in hotels and boarding houses. R: The lack of snow is putting people off booking ski holidays in hotels and guest houses. 1. extract verbs from H: Vh = {discourages, ordering} 2. extract verbs from R: Vr = {putting, booking} 3. similar verbs aligned with VerbNet (shared class get-13.5.1): V = {(vh = order,vr = book)} 4. compare arguments of (vh = o</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL 98, pages 768 774, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Liu</author>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Tesla: translation evaluation of sentences with linearprogramming-based analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT 10,</booktitle>
<pages>354359</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4842" citStr="Liu et al. (2010)" startWordPosition="754" endWordPosition="757">the same meaning. The most well-know metric is probably METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). METEOR is based on a generalized concept of unigram matching between the hypothesis and the reference translation. Alignments are based on exact, stem, synonym, and paraphrase matches between words and phrases. However, the structure of the sentences is not considered. Wong and Kit (2010) measure word choice and word order by the matching of words based on surface forms, stems, senses and semantic similarity. The informativeness of matched and unmatched words is also weighted. Liu et al. (2010) propose to match bags of unigrams, bigrams and trigrams considering both recall and precision and F-measure giving more importance to recall, but also using WordNet synonyms. Tratz and Hovy (2008) use transformations in order to match short syntactic units defined as Basic Elements (BE). The BE are minimal-length syntactically well defined units. For example, nouns, verbs, adjectives and adverbs can be considered BE-Unigrams, while a BE-Bigram could be formed from a syntactic relation (e.g. subject+verb, verb+object). BEs can be lexically different, but semantically similar. Pado et al. (2009</context>
<context position="15976" citStr="Liu et al., 2010" startWordPosition="2603" endWordPosition="2606"> by averaging the segment-level scores. TINE achieves the same average correlation with BLUE, but outperforms it for some language pairs. Additionally, TINE outperforms some of the previous which use WordNet to deal with synonyms as part of the lexical matching. The closest metric to TINE (Gimenez et al., 2010), which also uses semantic roles as one of its 1 http://ml.nec-labs.com/senna/ 2 http://www.lsi.upc.edu/ srlconll/ 119 \x0cTable 2: Comparison with previous semanticallyoriented metrics using segment-level Kendalls tau correlation with human judgments Metric cz-en fr-en de-en es-en avg (Liu et al., 2010) 0.34 0.34 0.38 0.34 0.35 (Gimenez et al., 2010) 0.34 0.33 0.34 0.33 0.33 (Wong and Kit, 2010) 0.33 0.27 0.37 0.32 0.32 METEOR 0.33 0.27 0.36 0.33 0.32 TINE 0.28 0.25 0.30 0.22 0.26 BLEU 0.26 0.22 0.27 0.28 0.26 (He et al., 2010) 0.15 0.14 0.17 0.21 0.17 (Tratz and Hovy, 2008) 0.05 0.0 0.12 0.05 0.05 components, achieves better performance. However, this metric is a rather complex combination of a number of other metrics to deal with different linguistic phenomena. 4.1 Further Improvements As an additional experiment, we use BLEU as the lexical component L(H, R) in order to test if the shallow</context>
</contexts>
<marker>Liu, Dahlmeier, Ng, 2010</marker>
<rawString>Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010. Tesla: translation evaluation of sentences with linearprogramming-based analysis. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT 10, pages 354359, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to recognize features of valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL,</booktitle>
<pages>4148</pages>
<location>New York City, USA,</location>
<marker>MacCartney, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D. Manning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of the Human Language Technology Conference of the NAACL, pages 4148, New York City, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Daniel Cer</author>
<author>Michel Galley</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Textual Entailment Features for Machine Translation Evaluation</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--181193</pages>
<contexts>
<context position="5443" citStr="Pado et al. (2009)" startWordPosition="848" endWordPosition="851">Liu et al. (2010) propose to match bags of unigrams, bigrams and trigrams considering both recall and precision and F-measure giving more importance to recall, but also using WordNet synonyms. Tratz and Hovy (2008) use transformations in order to match short syntactic units defined as Basic Elements (BE). The BE are minimal-length syntactically well defined units. For example, nouns, verbs, adjectives and adverbs can be considered BE-Unigrams, while a BE-Bigram could be formed from a syntactic relation (e.g. subject+verb, verb+object). BEs can be lexically different, but semantically similar. Pado et al. (2009) uses Textual Entailment features extracted from the Standford Entailment Recognizer (MacCartney et al., 2006). The Textual Entailment Recognizer computes matching and mismatching features over dependency parses. The metric then predicts the MT quality with a regression model. The alignment is improved using ontologies. He et al. (2010) measure the similarity between hypothesis and reference translation in terms of the Lexical Functional Grammar (LFG) representation. The representation uses dependency graphs to generate unordered sets of dependency triples. Calculating precision, recall, and F</context>
</contexts>
<marker>Pado, Cer, Galley, Jurafsky, Manning, 2009</marker>
<rawString>Sebastian Pado, Daniel Cer, Michel Galley, Dan Jurafsky, and Christopher D. Manning. 2009. Measuring machine translation quality as semantic equivalence: A metric based on entailment features. Machine Translation, 23:181193, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL 02,</booktitle>
<pages>311318</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1963" citStr="Papineni et al., 2002" startWordPosition="285" endWordPosition="289">e lexical matching strategies and metrics such as BLEU. 1 Introduction The automatic evaluation of Machine Translation (MT) is a long-standing problem. A number of metrics have been proposed in the last two decades, mostly measuring some form of matching between the MT output (hypothesis) and one or more human (reference) translations. However, most of these metrics focus on fluency aspects, as opposed to adequacy. Therefore, measuring whether the meaning of the hypothesis and reference translation are the same or similar is still an understudied problem. The most commonly used metrics, BLEU (Papineni et al., 2002) and alike, perform simple exact matching of n-grams between hypothesis and reference translations. Such a simple matching procedure has well known limitations, including that the matching of non-content words counts as much as the matching of content words, that variations of words with the same meaning are disregarded, and that a perfect matching can happen even if the order of sequences of n-grams in the hypothesis and reference translation are very different, changing completely the meaning of the translation. A number of other metrics have been proposed to address these limitations, for e</context>
<context position="14966" citStr="Papineni et al., 2002" startWordPosition="2451" endWordPosition="2454">ood average Kendalls tau correlation (0.23) on a development dataset (Callison-Burch et al., 2010). Table 1 shows the correlation of the lexical component with human judgments for a number of language pairs. Table 1: Kendalls tau segment-level correlation of the lexical component with human judgments Metric cz-en fr-en de-en es-en avg Lexical 0.27 0.21 0.26 0.19 0.23 We use the SENNA1 SRL system to tag the dataset with semantic roles. SENNA has shown to have achieved an F-measure of 75.79% for tagging semantic roles over the CoNLL 2005 2 benchmark. We compare our metric against standard BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2010) and other previous metrics reported in (Callison-Burch et al., 2010) which also claim to use some form of semantic information (see Section 2 for their description). The comparison is made in terms of Kendalls tau correlation against the human judgments at a segment-level. For our submission to the shared evaluation task, system-level scores are obtained by averaging the segment-level scores. TINE achieves the same average correlation with BLUE, but outperforms it for some language pairs. Additionally, TINE outperforms some of the previous which use WordNet</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL 02, pages 311318, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper Schuler</author>
</authors>
<title>VerbNet: A BroadCoverage, Comprehensive Verb Lexicon.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3614" citStr="Schuler, 2006" startWordPosition="553" endWordPosition="554">NE, a new metric that complements lexical matching with a shallow semantic component to better address adequacy. The main contribution of such a metric is to provide a more flexible way of measuring the overlap between shallow semantic representations that considers both the semantic structure of the sentence and the content of the semantic elements. The metric uses SRLs such as in (Gimenez and Marquez, 2007). However, it analyses the content of predicates and arguments seeking for either exact or similar matches. The 116 \x0cinexact matching is based on the use of ontologies such as VerbNet (Schuler, 2006) and distributional semantics similarity metrics, such as Dekang Lins thesaurus (Lin, 1998) . In the remainder of this paper we describe some related work (Section 2), present our metric - TINE - (Section 3) and its performance compared to previous work (Section 4) as well as some further improvements. We then provide an analysis of these results and discuss the limitations of the metric (Section 5) and present conclusions and future work (Section 6). 2 Related Work A few metrics have been proposed in recent years to address the problem of measuring whether a hypothesis and a reference transla</context>
<context position="10447" citStr="Schuler, 2006" startWordPosition="1660" endWordPosition="1661"> but differs from it in length, cosine will penalize less the matching than POW. That is specially interesting when core arguments get merged with modifiers due to bad semantic role labeling (e.g. [A0 I] [T bought] [A1 something to eat yesterday] instead of [A0 I] [T bought] [A1 something to eat] [AM-TMP yesterday]). A(H, R) = P vV verb score(Hv, Rv) |Vr| (3) In the adequacy component, V is the set of verbs aligned between H and R, and |Vr |is the number of verbs in R. Hereafter the indexes h and r stand for hypothesis and reference translations, respectively. Verbs are aligned using VerbNet (Schuler, 2006) and VerbOcean (Chklovski and Pantel, 2004). A verb in the hypothesis vh is aligned to a verb in the reference vr if they are related according to the following heuristics: (i) the pair of verbs share at least one class in VerbNet; or (ii) the pair of verbs holds a relation in VerbOcean. For example, in VerbNet the verbs spook and terrify share the same class amuse-31.1, and in VerbOcean the verb dress is related to the verb wear. verb score(Hv, Rv) = P aArAt arg score(Ha, Ra) |Ar| (4) The similarity between the arguments of a verb pair (vh, vr) in V is measured as defined in Equation (4), whe</context>
</contexts>
<marker>Schuler, 2006</marker>
<rawString>Karin Kipper Schuler. 2006. VerbNet: A BroadCoverage, Comprehensive Verb Lexicon. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>Summarisation evaluation using transformed basic elements.</title>
<date>2008</date>
<booktitle>In Proceedings TAC</booktitle>
<contexts>
<context position="5039" citStr="Tratz and Hovy (2008)" startWordPosition="786" endWordPosition="789">pothesis and the reference translation. Alignments are based on exact, stem, synonym, and paraphrase matches between words and phrases. However, the structure of the sentences is not considered. Wong and Kit (2010) measure word choice and word order by the matching of words based on surface forms, stems, senses and semantic similarity. The informativeness of matched and unmatched words is also weighted. Liu et al. (2010) propose to match bags of unigrams, bigrams and trigrams considering both recall and precision and F-measure giving more importance to recall, but also using WordNet synonyms. Tratz and Hovy (2008) use transformations in order to match short syntactic units defined as Basic Elements (BE). The BE are minimal-length syntactically well defined units. For example, nouns, verbs, adjectives and adverbs can be considered BE-Unigrams, while a BE-Bigram could be formed from a syntactic relation (e.g. subject+verb, verb+object). BEs can be lexically different, but semantically similar. Pado et al. (2009) uses Textual Entailment features extracted from the Standford Entailment Recognizer (MacCartney et al., 2006). The Textual Entailment Recognizer computes matching and mismatching features over de</context>
<context position="16253" citStr="Tratz and Hovy, 2008" startWordPosition="2657" endWordPosition="2660">t metric to TINE (Gimenez et al., 2010), which also uses semantic roles as one of its 1 http://ml.nec-labs.com/senna/ 2 http://www.lsi.upc.edu/ srlconll/ 119 \x0cTable 2: Comparison with previous semanticallyoriented metrics using segment-level Kendalls tau correlation with human judgments Metric cz-en fr-en de-en es-en avg (Liu et al., 2010) 0.34 0.34 0.38 0.34 0.35 (Gimenez et al., 2010) 0.34 0.33 0.34 0.33 0.33 (Wong and Kit, 2010) 0.33 0.27 0.37 0.32 0.32 METEOR 0.33 0.27 0.36 0.33 0.32 TINE 0.28 0.25 0.30 0.22 0.26 BLEU 0.26 0.22 0.27 0.28 0.26 (He et al., 2010) 0.15 0.14 0.17 0.21 0.17 (Tratz and Hovy, 2008) 0.05 0.0 0.12 0.05 0.05 components, achieves better performance. However, this metric is a rather complex combination of a number of other metrics to deal with different linguistic phenomena. 4.1 Further Improvements As an additional experiment, we use BLEU as the lexical component L(H, R) in order to test if the shallow-semantic component can contribute to the performance of this standard evaluation metric. Table 3 shows the results of the combination of BLEU and the shallow-semantic component using the same parameter configuration as in Section 4. The addition of the shallow-semantic compon</context>
</contexts>
<marker>Tratz, Hovy, 2008</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2008. Summarisation evaluation using transformed basic elements. In Proceedings TAC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy T-M Wong</author>
<author>Chunyu Kit</author>
</authors>
<title>The parameteroptimized atec metric for mt evaluation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT 10,</booktitle>
<pages>360--364</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4632" citStr="Wong and Kit (2010)" startWordPosition="719" endWordPosition="722"> 5) and present conclusions and future work (Section 6). 2 Related Work A few metrics have been proposed in recent years to address the problem of measuring whether a hypothesis and a reference translation share the same meaning. The most well-know metric is probably METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). METEOR is based on a generalized concept of unigram matching between the hypothesis and the reference translation. Alignments are based on exact, stem, synonym, and paraphrase matches between words and phrases. However, the structure of the sentences is not considered. Wong and Kit (2010) measure word choice and word order by the matching of words based on surface forms, stems, senses and semantic similarity. The informativeness of matched and unmatched words is also weighted. Liu et al. (2010) propose to match bags of unigrams, bigrams and trigrams considering both recall and precision and F-measure giving more importance to recall, but also using WordNet synonyms. Tratz and Hovy (2008) use transformations in order to match short syntactic units defined as Basic Elements (BE). The BE are minimal-length syntactically well defined units. For example, nouns, verbs, adjectives an</context>
<context position="16070" citStr="Wong and Kit, 2010" startWordPosition="2621" endWordPosition="2624">, but outperforms it for some language pairs. Additionally, TINE outperforms some of the previous which use WordNet to deal with synonyms as part of the lexical matching. The closest metric to TINE (Gimenez et al., 2010), which also uses semantic roles as one of its 1 http://ml.nec-labs.com/senna/ 2 http://www.lsi.upc.edu/ srlconll/ 119 \x0cTable 2: Comparison with previous semanticallyoriented metrics using segment-level Kendalls tau correlation with human judgments Metric cz-en fr-en de-en es-en avg (Liu et al., 2010) 0.34 0.34 0.38 0.34 0.35 (Gimenez et al., 2010) 0.34 0.33 0.34 0.33 0.33 (Wong and Kit, 2010) 0.33 0.27 0.37 0.32 0.32 METEOR 0.33 0.27 0.36 0.33 0.32 TINE 0.28 0.25 0.30 0.22 0.26 BLEU 0.26 0.22 0.27 0.28 0.26 (He et al., 2010) 0.15 0.14 0.17 0.21 0.17 (Tratz and Hovy, 2008) 0.05 0.0 0.12 0.05 0.05 components, achieves better performance. However, this metric is a rather complex combination of a number of other metrics to deal with different linguistic phenomena. 4.1 Further Improvements As an additional experiment, we use BLEU as the lexical component L(H, R) in order to test if the shallow-semantic component can contribute to the performance of this standard evaluation metric. Tabl</context>
</contexts>
<marker>Wong, Kit, 2010</marker>
<rawString>Billy T.-M. Wong and Chunyu Kit. 2010. The parameteroptimized atec metric for mt evaluation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT 10, pages 360 364, Stroudsburg, PA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>