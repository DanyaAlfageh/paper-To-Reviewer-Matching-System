CITATION presented an unlexicalized parser that eliminated all lexicalized parameters,,
Moreover, it was argued in CITATION that dependency based eva,,
There have been several previous approaches to parsing Chinese with the Penn Chinese Treebank (e.g., CITATION; CITATION),,
In CITATION, the bigram probability P(w2|w1) is computed as the weighted average of the conditional probability of w2 given similar words of w1,,
Secondly, in CITATION, the distribution P(|w1) may itself be sparsely observed,,
CITATION also computes a conditional probability of dependency structures,,
We used the same data split as CITATION: Sections 1-270 and 400-931 as 156 \x0cthe training set, Sections 271-300 as testing and Sections 301-325 as the development set,,
The generation process according to the canonical order is similar to the head outward generation process in CITATION, except that it is bottom-up whereas Collins models are top-down,,
CITATION pr,,
Similarity-based smoothing was used in CITATION to estimate word co-occurrence probabilities,,
A difference between similarity-based smoothing in CITATION and our approach is that our model only computes probability distributions of binary variables,,
In the DMV model CITATION, the probability of a dependency link is partly conditioned on whether or not there is a head word of the link already has a,,
In many dependency parsing models such as CITATION and (MacDonald et al., 2005), the score of a dependency tree is the sum of the scores of the dependency links, which are computed independently of other links,,
Moreover, it was argued in CITATION that dependency based evaluation is much more mean,,
Moreover, it was argued in CITATION that dependency based evaluation is much more meaningful for the applications that use parse trees, since the semantic relationships are generally embedded in the dependency relationships,,
We can compute the probability of T as follows: ( ) ( ) ( ) = = = N i i i N G G S G P S G G G P S T P 1 1 1 2 1 ,..., , | | ,..., , | Following CITATION, we require that the creation of a dependency link from head h to modifier m be preceded by placing a left STOP and a right STOP around the modifier m and STOP between h and m,,
The probability ( ) 1 1,..., , | i i G G S G P can be computed as: ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) L v L L v L v R u R u L u L u i L L v R u L u i i C v u v u link P C v E P C u E P C u E P G G S v u link E E E P G G S G P , , | , , | 1 , | , | ,..., , | , , , , ,..., , | 1 1 1 1 = = The events R w E and L w E correspond to the STOP events in CITATION and CITATION,,
was found in CITATION that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output,,
1 Introduction There has been a great deal of progress in statistical parsing in the past decade (CITATION; CITATION; Chaniak, 2000),,
In the DMV model CITATION, the probability of a dependency link is partly conditioned on whether or not there is a head word of the link already has a modifier,,
Performance of Alternative Models 157 \x0c5 Related Work Previous parsing models (e.g., CITATION; CITATION) maximize the joint probability P(S, T) of a sentence S and its parse tree T,,
It was found in CITATION that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output,,
Many methods have been proposed to compute distributional similarity between words (CITATION; CITATION; CITATION; CITATION),,
ebank (e.g., CITATION; CITATION),,
This is known as the Distributional Hypothesis in linguistics CITATION,,
CITATION observed that the bi-lexical statistics accounted for only 1.49% of the bigram statistics used by the parser,,
Firstly, when a context contains two words, we are able to use the cross product of the similar words, whereas CITATION can only use the similar words of one of the words,,
We converted them to dependency trees using the same method and the head table as CITATION,,
The similarity-based smoothing method in CITATION uses the similar words of one of the words in a bigram,,
