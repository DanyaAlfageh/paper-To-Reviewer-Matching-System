<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.344404">
b&apos;Dependency Parsing Schemata and Mildly
Non-Projective Dependency Parsing
</title>
<author confidence="0.379768">
Carlos Gomez-Rodrguez
</author>
<affiliation confidence="0.347968">
Universidade da Coruna, Spain
</affiliation>
<author confidence="0.761801">
John Carroll
</author>
<affiliation confidence="0.94659">
University of Sussex, UK
</affiliation>
<author confidence="0.902204">
David Weir
</author>
<affiliation confidence="0.988053">
University of Sussex, UK
</affiliation>
<bodyText confidence="0.999759916666667">
We introduce dependency parsing schemata, a formal framework based on Sikkels parsing
schemata for constituency parsers, which can be used to describe, analyze, and compare depen-
dency parsing algorithms. We use this framework to describe several well-known projective and
non-projective dependency parsers, build correctness proofs, and establish formal relationships
between them. We then use the framework to define new polynomial-time parsing algorithms
for various mildly non-projective dependency formalisms, including well-nested structures with
their gap degree bounded by a constant k in time O(n5+2k
), and a new class that includes all
gap degree k structures present in several natural language treebanks (which we call mildly
ill-nested structures for gap degree k) in time O(n4+3k
). Finally, we illustrate how the parsing
schema framework can be applied to Link Grammar, a dependency-related formalism.
</bodyText>
<sectionHeader confidence="0.994209" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.916344789473684">
Dependency parsing involves finding the structure of a sentence as expressed by a set
of directed links (called dependencies) between individual words. Dependency for-
malisms have attracted considerable interest in recent years, having been successfully
applied to tasks such as machine translation (Ding and Palmer 2005; Shen, Xu, and
Weischedel 2008), textual entailment recognition (Herrera, Penas, and Verdejo 2005),
relation extraction (Culotta and Sorensen 2004; Fundel, Kuffner, and Zimmer 2006),
and question answering (Cui et al. 2005). Key characteristics of the dependency parsing
approach are that dependency structures specify headmodifier and headcomplement
relationships, which form the basis of predicateargument structure, but are not rep-
resented explicitly in constituency trees; there is no need for dependency parsers to
postulate the existence of non-lexical nodes; and some variants of dependency parsers
Facultade de Informatica, Universidade da Coruna Campus de Elvina, s/n, 15071 A Coruna, Spain.
E-mail: cgomezr@udc.es.
School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK.
E-mail: J.A.Carroll@sussex.ac.uk.
School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK.
E-mail: D.J.Weir@sussex.ac.uk.
Submission received: 21 October 2009; revised submission received: 23 December 2010; accepted for
publication: 29 January 2011.
</bodyText>
<sectionHeader confidence="0.748637" genericHeader="keywords">
2011 Association for Computational Linguistics
</sectionHeader>
<bodyText confidence="0.995630095238096">
\x0cComputational Linguistics Volume 37, Number 3
are able to represent non-projective structures (McDonald et al. 2005), which is impor-
tant when parsing free word order languages where discontinuous constituents are
common.
The formalism of parsing schemata, introduced by Sikkel (1997), is a useful tool for
the study of constituency parsers, supporting precise, high-level descriptions of parsing
algorithms. Potential applications of parsing schemata include devising correctness
proofs, extending our understanding of relationships between different algorithms,
deriving new variants of existing algorithms, and obtaining efficient implementations
automatically (Gomez-Rodrguez, Vilares, and Alonso 2009). The formalism was origi-
nally defined for context-free grammars (CFG) and since then has been applied to other
constituency-based formalisms, such as tree-adjoining grammars (Alonso et al. 1999).
This article considers the application of parsing schemata to the task of dependency
parsing. The contributions of this article are as follows.
r We introduce dependency parsing schemata, a novel adaptation of the
original parsing schemata framework (see Section 2).
r We use the dependency parsing schemata to define and compare a number
of existing dependency parsers (projective parsers are presented in
Section 3, and their formal properties discussed in Sections 4 and 5; a
number of non-projective parsers are presented in Section 6).
r We present parsing algorithms for several sets of mildly non-projective
dependency structures, including a parser for a new class of structures we
call mildly ill-nested, which encompasses all the structures in a number of
existing dependency treebanks (see Section 7).
r We adapt the dependency parsing schema framework to the formalism of
Link Grammar (Sleator and Temperley 1991, 1993) (see Section 8).
Although some of these contributions have been published previously, this article
presents them in a thorough and consistent way. The definition of dependency parsing
schemata was first published by Gomez-Rodrguez, Carroll, and Weir (2008), along with
some of the projective schemata presented here and their associated proofs. The results
concerning mildly non-projective parsing in Section 7 were first published by Gomez-
Rodrguez, Weir, and Carroll (2008, 2009). On the other hand, the material on Nivre
and Covingtons projective parsers, as well as all the non-projective parsers and the
application of the formalism to Link Grammar, are entirely new contributions of this
article.
The notion of a parsing schema comes from considering parsing as a deduction
process which generates intermediate results called items. In particular, items in parsing
schemata are sets of partial constituency trees taken from the set of all partial parse
trees that do not violate the constraints imposed by a grammar. A parsing schema can
be used to obtain a working implementation of a parser by using deductive engines
such as the ones described by Shieber et al. (1995) and Gomez-Rodrguez, Vilares, and
Alonso (2009), or the Dyna language (Eisner, Goldlust, and Smith 2005).
</bodyText>
<sectionHeader confidence="0.980339" genericHeader="introduction">
2. Dependency Parsing Schemata
</sectionHeader>
<bodyText confidence="0.996755">
Although parsing schemata were originally defined for CFG parsers, they have since
been adapted to other constituency-based grammar formalisms. This involves finding
</bodyText>
<page confidence="0.993213">
542
</page>
<bodyText confidence="0.995889784313725">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
a suitable definition of the set of structures contained in items, and a way to define
deduction steps that captures the formalisms composition rules (Alonso et al. 1999).
Although it is less clear how to adapt parsing schemata to dependency parsing, a num-
ber of dependency parsers have the key property of being constructive: They proceed by
combining smaller structures to form larger ones, terminating when a complete parse
for the input sentence is found. We show that this makes it possible to define a variant
of the traditional parsing schemata framework, where the encodings of intermediate
dependency structures are defined as items, and the operations used to combine them
are expressed as inference rules. We begin by addressing a number of preliminary
issues.
Traditional parsing schemata are used to define grammar-driven parsers, in which
the parsing process is guided by some set of rules which are used to license deduction
steps. For example, an Earley PREDICTOR step is tied to a particular grammar rule, and
can only be executed if such a rule exists. Some dependency parsers are also grammar-
driven. For example, those described by Lombardo and Lesmo (1996), Barbero et al.
(1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of depen-
dency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However,
many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally
considered to be grammar-driven, because they do not use an explicit formal grammar;
decisions about which dependencies to create are taken individually, using probabilis-
tic models (Eisner 1996) or classifiers (Yamada and Matsumoto 2003). These are called
data-driven parsers. To express such algorithms as deduction systems, we use the
notion of D-rules (Covington 1990). D-rules have the form (a, i) (b, j), which speci-
fies that a word b located at position j in the input string can have the word a in
position i as a dependent. Deduction steps in data-driven parsers can be associated with
the D-rules corresponding to the links they create, so that parsing schemata for such
parsers are defined using grammars of D-rules. In this way, we obtain a representation
of some of the declarative aspects of these parsing strategies that is independent of the
particular model used to make the decisions associated with each D-rule. Note that
this representation is useful for designing control structures or probabilistic models for
the parsers, because it makes explicit the choice points where the models will have
to make probabilistic decisions, as well as the information available at each of those
choice points. Additionally, D-rules allow us to use an uniform description that is valid
for both data-driven and grammar-driven parsers, because D-rules can function like
grammatical rules.
The fundamental structures in dependency parsing are dependency trees. There-
fore, just as items for constituency parsers encode sets of partial constituency trees,
items for dependency parsers can be defined using partial dependency trees. How-
ever, dependency trees cannot express the fact that a particular structure has been
predicted, but not yet built; this is required for grammar-based algorithms such as
those of Lombardo and Lesmo (1996) and Kahane, Nasr, and Rambow (1998). The
formalism can be made general enough to include these parsers by using a novel way of
representing intermediate states of dependency parsers based on a form of dependency
trees that include nodes labelled with preterminals and terminals (Gomez-Rodrguez,
Carroll, and Weir 2008; Gomez-Rodrguez 2009). For simplicity of presentation, we will
only use this representation (called extended dependency trees) in the grammar-based
algorithms that need it, and we will define the formalism and the rest of the algo-
rithms with simple dependency trees. Some existing dependency parsing algorithms,
for example, the algorithm of Eisner (1996), involve steps that connect spans which
can represent disconnected dependency graphs. Such spans cannot be represented by
</bodyText>
<page confidence="0.995349">
543
</page>
<bodyText confidence="0.985949085106383">
\x0cComputational Linguistics Volume 37, Number 3
a single dependency tree. Therefore, our formalism allows items to be sets of forests of
partial dependency trees, rather than sets of trees.
We are now ready to define the concepts needed to specify item sets for dependency
parsers.
Definition 1
An interval (with endpoints i and j) is a set of natural numbers of the form [i..j] = {k |
i k j}. We will use the notation i..j for the ordered list of the numbers in [i..j]. A
dependency graph for a string w = w1 . . . wn is a graph G = (V, E), where V [1..n]
and E V V.
The edge (i, j) is written i j, and each such edge is called a dependency link,
encoding the fact that the word wi is a syntactic dependent (or child) of wj or, conversely,
that wj is the parent, governor, or head of wi. We write i \x02
j to denote that there exists
a (possibly empty) path from i to j. The projection of a node i, denoted \x06i\x07, is the set of
reflexive-transitive dependents of i, that is, \x06i\x07 = {j V  |j \x02
i}. In contexts where we
refer to different dependency graphs, we use the notation \x06i\x07G to specify the projection
of a node i in the graph G.
Definition 2
A dependency graph T for a string w1 . . . wn is called a dependency tree for that string
if it contains no cycles and all of its nodes have exactly one parent, except for one node
that has none and is called the root or head of the tree T, denoted head(T). The yield of
a dependency tree T, denoted yield(T), is the ordered list of its nodes. We will use the
term dependency forest to refer to a set of dependency trees for the same string,1
and
the generic term dependency structure to refer to a dependency tree or forest.
A dependency tree is said to be a parse tree for a string w1 . . . wn if its yield is 1..n.
Definition 3
We say that a dependency graph G = (V, E) for a string w1 . . . wn is projective if \x06i\x07 is an
interval for every i V.
Definition 4
Let (G) be the set of dependency trees which are syntactically well-formed according
to a given grammar G (which may be a grammar of D-rules or of CFG-like rules, as
explained previously). We define an item set for dependency parsing as a set I ,
where is a partition of the power set, ((G)), of the set (G). Each element of I,
called an item, is a set of dependency forests for strings. For example, each member of
the item [1, 5] in the item set of the parser by Yamada and Matsumoto (2003) that will
be explained in Section 3.4 is a dependency forest with two projective trees, one with
head 1 and the other with head 5, and such that the concatenation of their yields is 1..5.
Figure 1 shows the three dependency forests that constitute the contents of this item
under a specific grammar of D-rules.
Following Sikkel (1997), items are sets of syntactic structures and tuples are a short-
hand notation for such sets, as seen in the previous example. An alternative approach,
1 Note that the trees in a dependency forest can have different yields, because the node set of a dependency
tree for a string w1 . . . wn can be any subset of [1..n]. In fact, all the forests used by the parsers in this
article contain trees with non-overlapping yields, although this is not required by the definition.
</bodyText>
<page confidence="0.997869">
544
</page>
<bodyText confidence="0.971077307692308">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
Figure 1
Contents of the item [1, 5] from the Yamada and Matsumoto (2003) parsing schema under a
grammar of D-rules {(w2, 2) (w1, 1) , (w3, 3) (w1, 1) , (w3, 3) (w5, 5) , (w3, 3) (w4, 4) ,
(w4, 4) (w5, 5)}.
following Shieber, Schabes, and Pereira (1995), would be to define items as tuples
that denote sets of syntactic structures. Although the latter approach provides more
flexibility, this makes defining the relationships between parsers less straightforward.
In any case, because tuple notation is used to write schemata under both approaches,
the schemata we provide are compatible with both interpretations.
Having defined an item set for dependency parsing, the remaining definitions are
analogous to those in Sikkels theory of constituency parsing (Sikkel 1997), and are not
presented in full detail. A dependency parsing system is a deduction system (I, H, D)
where I is a dependency item set as defined here, H is a set containing initial items
or hypotheses (not necessarily contained in I), and D ( (H I) I) is a set of
deduction steps defining an inference relation
.
Final items in this formalism will be those containing some forest F containing a
parse tree for some string w1 . . . wn. In parsers for general non-projective structures, any
item containing such a tree will be called a coherent final item for w1 . . . wn. In schemata
for parsers that are constrained to a more restrictive class T of dependency trees, such as
projective parsers, coherent final items will be those containing parse trees for w1 . . . wn
that are in T . For example, because we expect correct projective parsers to produce only
projective structures, coherent final items for projective parsers will be those containing
projective parse trees for w1 . . . wn. Correctness proofs typically define a set of coherent
items, such that its intersection with final items produces the set of coherent final items.
The definition of coherent items depends on each particular proof.2
For each input string, a parsing schemas deduction steps allow us to infer a set of
items, called derivable items, for that string.3
A parsing schema is said to be sound if all
derivable final items it produces for any arbitrary string are coherent for that string. A
parsing schema is said to be complete if all coherent final items are derivable. A correct
parsing schema is one which is both sound and complete.
Parsing schemata are located at a higher abstraction level than parsing algorithms,
and formalize declarative aspects of their logic: A parsing schema specifies a set of
intermediate results that are obtained by the algorithm (items) and a set of operations
that can be used to obtain new such results from existing ones (deduction steps); but it
makes no claim about the order in which to execute the operations or the data structures
to use for storing the results.
</bodyText>
<sectionHeader confidence="0.937836" genericHeader="method">
3. Projective Schemata
</sectionHeader>
<bodyText confidence="0.9629465">
In this section, we show how dependency parsing schemata can be used to describe
several existing projective dependency parsers.
</bodyText>
<footnote confidence="0.6922045">
2 Coherent (final) items are called correct (final) items in the original formulation by Sikkel (1997).
3 Derivable items are called valid items in the original formulation by Sikkel (1997).
</footnote>
<page confidence="0.969854">
545
</page>
<figureCaption confidence="0.4571875">
\x0cComputational Linguistics Volume 37, Number 3
Figure 2
</figureCaption>
<bodyText confidence="0.9268455">
Representation of the [i, j, h] item in Collinss parser, together with one of the dependency
structures contained in it (left side); and of the antecedents and consequents of an L-LINK step
(right side). White rectangles in an item represent intervals of nodes that have been assigned a
head by the parser, and dark squares represent nodes that have no head.
</bodyText>
<subsectionHeader confidence="0.987683">
3.1 Collins (1996)
</subsectionHeader>
<bodyText confidence="0.956580857142857">
One of the most straightforward projective dependency parsing strategies was intro-
duced by Collins (1996), and is based on the CYK bottomup parsing strategy (Kasami
1965; Younger 1967). Collinss parser works with dependency trees which are linked to
each other by creating links between their heads. The schema for this parser maps every
set of D-rules G and input string w1 . . . wn to an instantiated dependency parsing system
(ICol96, H, DCol96) such that:
Item set: The item set is defined as ICol96 = {[i, j, h]  |1 i h j n}, where item [i, j, h]
is defined as the set of forests containing a single projective dependency tree T of (G)
such that yield(T) is of the form i..j and head(T) = h (see Figure 2, left side). From now
on, we will implicitly assume that the dependency trees appearing in items of a parsing
schema for a grammar G are taken from the set (G) of syntactically well-formed trees
according to G.
This means that Collinss parser will be able to infer an item [i, j, h] in the presence
of an input string w1 . . . wn if, using our set of D-rules, it is possible to build a projective
dependency tree headed at h that spans the substring wi . . . wj of the input.
Hypotheses: For an input string w1 . . . wn, the set of hypotheses is H = {[i, i, i]  |0 i
n + 1}, where each item contains a forest with a single dependency tree having only
one node i. This same set of hypotheses is used for all the parsers, so is not repeated
for subsequent schemata.4
Note that the nodes 0 and n + 1 used in the definition do not
correspond to actual input wordsthese are dummy nodes that we call beginning-of-
sentence and end-of-sentence markers, respectively, and will be needed by several of
the parsers described subsequently.
Final items: The set of final items is {[1, n, h]  |1 h n}. These items trivially represent
parse trees for the input sentence whose head is some node h, expressing that the word
wh is the sentences syntactic head.
4 Although in the parsers described in Section 7 we use a different notation for the hypotheses, they still
are the same, as explained later.
</bodyText>
<page confidence="0.979348">
546
</page>
<bodyText confidence="0.373839">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
Deduction steps: The set of deduction steps, DCol96, is the union of the following:
R-LINK:
</bodyText>
<equation confidence="0.961314166666667">
[i, j, h1]
[j + 1, k, h2]
[i, k, h2]
(wh1
, h1) (wh2
, h2) L-LINK:
[i, j, h1]
[j + 1, k, h2]
[i, k, h1]
(wh2
, h2) (wh1
, h1)
</equation>
<bodyText confidence="0.99975645">
allowing us to join two contiguous trees by linking their heads with a rightward or
leftward link, respectively. Figure 2 (right side) shows a graphical representation of
how trees are joined by the L-LINK step. Note that, because this parsing strategy is
data-driven, D-rules are used as side conditions for the parsers deduction steps. Side
conditions restrict the inference relation by specifying which combinations of values are
permissible for the variables appearing in the antecedents and consequent of deduction
steps.
This parsing schema specifies a recognizer: Given a set of D-rules and an input
string w1 . . . wn, the sentence can be parsed (projectively) under those D-rules if and
only if the deduction system infers a coherent final item. When executing this schema
with a deductive engine, the parse forest can be recovered by following back pointers,
as in constituency parsers (Billot and Lang 1989).
This schema formalizes a parsing logic which is independent of the order and the
way linking decisions are taken. Statistical models can be used to determine whether
a step linking words a and b in positions i and ji.e., having (a, i) (b, j) as a side
conditionis executed or not, and probabilities can be attached to items in order to as-
sign different weights to different analyses of the sentence. The side conditions provide
an explicit representation of the choice points where probabilistic decisions are made by
the control mechanism that is executing the schema. The same principle applies to all
D-rule-based parsers described in this article.
</bodyText>
<subsectionHeader confidence="0.90537">
3.2 Eisner (1996)
</subsectionHeader>
<bodyText confidence="0.962718947368421">
Based on the number of free variables used in deduction steps of Collinss parser, it is
apparent that its time complexity is O(n5
): There are O(n5
) combinations of index values
with which each of its LINK steps can be executed.5
This complexity arises because a
parentless word (head) may appear in any position in the items generated by the parser;
the complexity can be reduced to O(n3
) by ensuring that parentless words only appear
at the first or last position of an item. This is the idea behind the parser defined by
Eisner (1996), which is still in wide use today (McDonald, Crammer, and Pereira 2005;
Corston-Oliver et al. 2006). The parsing schema for this algorithm is defined as follows.
Item set: The item set is
IEis96 = {[i, j, True, False]  |0 i j n} {[i, j, False, True]  |0 i j n}
{[i, j, False, False]  |0 i j n},
where item [i, j, True, False] corresponds to [i, j, j] ICol96, item [i, j, False, True] corre-
sponds to item [i, j, i] ICol96, and item [i, j, False, False] is defined as the set of forests
5 For this and the rest of the complexity results in this article, we assume that the linking decision
associated with a D-rule can be made in constant time.
</bodyText>
<page confidence="0.992688">
547
</page>
<bodyText confidence="0.9495749">
\x0cComputational Linguistics Volume 37, Number 3
of the form {T1, T2} such that T1, T2 are projective, head(T1) = i, head(T2) = j, and there
is some k (i k &amp;lt; j) such that yield(T1) = i..k and yield(T2) = k + 1..j.
The flags b, c in [i, j, b, c] indicate whether the nodes i and j, respectively, have a
parent in the item. Items with one of the flags set to True represent dependency trees
where the node i or j is the head, whereas items with both flags set to False represent
pairs of trees headed at nodes i and j which jointly dominate the substring wi . . . wj.
Items of this kind correspond to disconnected dependency graphs.
Deduction steps: The set of deduction steps is as follows:6
INITTER:
</bodyText>
<equation confidence="0.863935357142857">
[i, i, i]
[i + 1, i + 1, i + 1]
[i, i + 1, False, False]
COMBINESPANS:
[i, j, b, c]
[j, k, not(c), d]
[i, k, b, d]
R-LINK:
[i, j, False, False]
[i, j, True, False]
(wi, i) (wj, j) L-LINK:
[i, j, False, False]
[i, j, False, True]
(wj, j) (wi, i)
</equation>
<bodyText confidence="0.9978487">
where the R-LINK and L-LINK steps establish a dependency link between the heads of
an item containing two trees (i.e., having both flags set to False), producing a new item
containing a single tree. The COMBINESPANS step is used to join two items that overlap
at a single word, which must have a parent in only one of the items, so that the result
of joining trees coming from both items (without creating any dependency link) is a
well-formed dependency tree.
Final items: The set of final items is {[0, n, False, True]}. Note that these items represent de-
pendency trees rooted at the beginning-of-sentence marker 0, which acts as a dummy
head for the sentence. In order for the algorithm to parse sentences correctly, we need
to define D-rules to allow the real sentence head to be linked to the node 0.
</bodyText>
<subsectionHeader confidence="0.995134">
3.3 Eisner and Satta (1999)
</subsectionHeader>
<bodyText confidence="0.97817175">
Eisner and Satta (1999) define an O(n3
) parser for split head automaton grammars
which can be used for dependency parsing. This algorithm is conceptually simpler than
Eisners (1996) algorithm, because it only uses items representing single dependency
trees, avoiding items of the form [i, j, False, False].
Item set: The item set is IES99 = {[i, j, i]  |0 i j n} {[i, j, j]  |0 i j n}, where
items are defined as in Collinss parsing schema.
Deduction steps: The deduction steps for this parser are the following:
</bodyText>
<construct confidence="0.767661588235294">
R-LINK:
[i, j, i]
[j + 1, k, k]
[i, k, k]
(wi, i) (wk, k) L-LINK:
[i, j, i]
[j + 1, k, k]
[i, k, i]
(wk, k) (wi, i)
R-COMBINER:
[i, j, i]
[j, k, j]
[i, k, i]
L-COMBINER:
[i, j, j]
[j, k, k]
[i, k, k]
</construct>
<sectionHeader confidence="0.438334" genericHeader="method">
6 We could have used items [i, i + 1, False, False] as hypotheses for this parsing schema, and not require
</sectionHeader>
<bodyText confidence="0.9218325">
an Initter step, but we prefer a standard set of hypotheses valid for all parsers as it facilitates more
straightforward proofs of the relations between schemata.
</bodyText>
<page confidence="0.994512">
548
</page>
<bodyText confidence="0.996334133333333">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
where LINK steps create a dependency link between two dependency trees spanning
adjacent segments of the input, and COMBINER steps join two overlapping trees by a
graph union operation that does not create new links. COMBINER steps follow the same
mechanism as those in the algorithm of Eisner (1996), and LINK steps work analogously
to those of Collins (1996), so this schema can be seen as being intermediate between
those two algorithms. These relationships will be formally described in Section 4.
Final items: The set of final items is {[0, n, 0]}. By convention, parse trees have the
beginning-of-sentence marker 0 as their head, as in the previous algorithm.
When described for head automaton grammars (Eisner and Satta 1999), this algo-
rithm appears to be more complex to understand and implement than the previous
one, requiring four different kinds of items to keep track of the state of the automata
used by the grammars. However, this abstract representation of its underlying seman-
tics reveals that this parsing strategy is, in fact, conceptually simpler for dependency
parsing.
</bodyText>
<subsectionHeader confidence="0.981872">
3.4 Yamada and Matsumoto (2003)
</subsectionHeader>
<bodyText confidence="0.963359761904762">
Yamada and Matsumoto (2003) define a deterministic, shift-reduce dependency parser
guided by support vector machines, which achieves over 90% dependency accuracy on
Section 23 of the Wall Street Journal Penn Treebank. Parsing schemata cannot specify
control strategies that guide deterministic parsers; schemata work at an abstraction
level, defining a set of operations without procedural constraints on the order in
which they are applied. However, deterministic parsers can be viewed as optimizations
of underlying nondeterministic algorithms, and we can represent the actions of the
underlying parser as deduction steps, abstracting away from the deterministic im-
plementation details, obtaining a potentially interesting nondeterministic dependency
parser.
Actions in Yamada and Matsumotos parser create links between two target nodes,
which act as heads of neighboring dependency trees. One of the actions creates a link
where the left target node becomes a child of the right one, and the head of a tree
located directly to the left of the target nodes becomes the new left target node. The
other action is symmetric, performing the same operation with a right-to-left link. An
O(n3
) nondeterministic parser generalizing this behavior can be defined as follows.
Item set: The item set is IYM03 = {[i, j]  |0 i j n + 1}, where each item [i, j] corre-
sponds to the item [i, j, False, False] in IEis96.
Deduction steps: The deduction steps are as follows:
INITTER:
</bodyText>
<equation confidence="0.976350083333334">
[i, i, i]
[i + 1, i + 1, i + 1]
[i, i + 1]
R-LINK:
[i, j]
[j, k]
[i, k]
(wj, j) (wk, k) L-LINK:
[i, j]
[j, k]
[i, k]
(wj, j) (wi, i)
</equation>
<bodyText confidence="0.985333857142857">
where a LINK step joins a pair of items containing forests with two trees each and
overlapping at a head node, and creates a dependency link from their common head to
one of the peripheral heads. Note that this is analogous to performing an Eisner LINK
step immediately followed by an Eisner COMBINE step, as will be further analyzed in
Section 4.
Final items: The set of final items is {[0, n + 1]}. For this set to be well-defined, the
grammar must not have D-rules of the form (wi, i) (wn+1, n + 1), that is, it must not
</bodyText>
<page confidence="0.997193">
549
</page>
<bodyText confidence="0.91504575">
\x0cComputational Linguistics Volume 37, Number 3
Figure 3
Grounded extended dependency tree and associated dependency structure.
allow the end-of-sentence marker to govern any words. If the grammar satisfies this
condition, it is trivial to see that every forest in an item of the form [0, n + 1] must contain
a parse tree rooted at the beginning-of-sentence marker and with yield 0..n.
As can be seen from the schema, this algorithm requires less bookkeeping than the
other parsers described here.
</bodyText>
<subsectionHeader confidence="0.541782">
3.5 Lombardo and Lesmo (1996) and Other Earley-Based Parsers
</subsectionHeader>
<bodyText confidence="0.977539961538462">
The algorithms presented so far are based on making individual decisions about de-
pendency links, represented by D-rules. Other parsers, such as that of Lombardo and
Lesmo (1996), use grammars with CFG-like rules which encode the preferred order of
dependents for each given governor. For example, a rule of the form N(Det PP) is used
to allow N to have Det as left dependent and PP as right dependent. The algorithm
by Lombardo and Lesmo is a version of Earleys CFG parser (Earley 1970) that uses
Gaifmans dependency grammar (Gaifman 1965).
As this algorithm predicts dependency relations before building them, item sets
contain extended dependency trees, trees that have two kinds of nodes: preterminal
nodes and terminal nodes. Depending on whether all the preterminal nodes have been
linked to terminals, extended dependency trees can either be grounded, in which case
they are isomorphic to traditional dependency graphs (see Figure 3), or ungrounded, as
in Figure 4, in which case they capture parser states in which some structure has been
predicted, but not yet found. Note that a dependency graph can always be extracted
from such a tree, but in the ungrounded case different extended trees can be associated
with the same graph. Gomez-Rodrguez, Carroll, and Weir (2008) present extended
dependency trees in more detail.
Item set: The item set is ILomLes = {[A( ), i, j]  |A() P 1 i j + 1 n} where
and are strings; P is a set of CFG-like rules;7
and each item [A( ), i, j] represents
the set of projective extended dependency trees rooted at A, where the direct children of
A are , and the subtrees rooted at have yield i..j. Note that Lombardo and Lesmos
parser uses both grounded trees (in items [A(), i, j]) and non-grounded trees (in items
7 A CFG-like rule A( ) rewrites a preterminal A to strings x over terminals and preterminals,
where , are strings of preterminals and x is a terminal of category A (the head of the rule). A special
rule (S) is used to state that the preterminal S can act as the root of an extended dependency tree.
</bodyText>
<page confidence="0.984592">
550
</page>
<figureCaption confidence="0.7201005">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
Figure 4
</figureCaption>
<bodyText confidence="0.987782857142857">
Non-grounded extended dependency tree: A determiner and adjective have been found and,
according to the grammar, we expect a noun that will act as their common head. As this head
has not been read, no dependency links have been established.
[A( ), i, j], where is nonempty). Items in this parser can represent infinite sets of
extended dependency trees, as in Earleys CFG parser but unlike items in D-rule-based
parsers, which are finite sets.
Deduction steps: The deduction steps for this parsing schema are as follows:
</bodyText>
<figure confidence="0.597754714285714">
INITTER:
[(S), 1, 0]
(S) P PREDICTOR:
[A( B), i, j]
[B(), j + 1, j]
B() P
SCANNER:
</figure>
<equation confidence="0.781899571428571">
[A( \x02), i, h 1]
[h, h, h]
[A( \x02 ), i, h]
wh IS A COMPLETER:
[A( B), i, j]
[B(), j + 1, k]
[A(B ), i, k]
</equation>
<bodyText confidence="0.981615666666667">
Final items: The final item set is {[(S), 1, n]}.
The schema for Lombardo and Lesmos parser is a variant of the Earley constituency
parser (cf. Sikkel 1997), with minor changes to adapt it to dependency grammar (for
example, the SCANNER always moves the dot over the head symbol , rather than
over a terminal symbol). Analogously, other dependency parsing schemata based on
CFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997):
The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and
the parser described by Courtin and Genthial (1998) is a variant of the head-corner
parser.
</bodyText>
<subsectionHeader confidence="0.577725">
3.6 Nivre (2003)
</subsectionHeader>
<bodyText confidence="0.998212454545454">
Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing,
later extended by Nivre, Hall, and Nilsson (2004). With linear-time performance and
competitive parsing accuracy (Nivre et al. 2006; Nivre and McDonald 2008), it is one
of the parsers included in the MaltParser system (Nivre et al. 2007), which is currently
widely used (e.g., Nivre et al. 2007; Surdeanu et al. 2008).
The parser proceeds by reading the sentence from left to right, using a stack and four
different kinds of transitions between configurations. The transition system defined by
all the possible configurations and transitions is nondeterministic, and machine learning
techniques are used to train a mechanism that produces a deterministic parser.
A deduction system describing the transitions of the parser is defined by Nivre,
Hall, and Nilsson (2004), with the following set of rules that describes transitions
</bodyText>
<page confidence="0.99399">
551
</page>
<bodyText confidence="0.957114333333333">
\x0cComputational Linguistics Volume 37, Number 3
between configurations (we use the symbol for a stack and the notation :: h for the
stack resulting from pushing h into , and i to represent a buffer of the form wi . . . wn):
</bodyText>
<equation confidence="0.989399722222222">
Initter
(\x0c
, 0, )
Shift
(, f , V)
( :: f, f+1, V)
Reduce
( :: l, f , V)
(, f , V)
al ak V
L-Link
( :: l, f , V)
( :: l :: f, f+1, V {af al})
af al  |\x02af ak V
R-Link
( :: l, f , V)
(, f , V {al af })
al af  |\x02al ak V
</equation>
<bodyText confidence="0.9928913">
This set of inference rules is not a parsing schema, however, because the entities it
works with are not items. Although the antecedents and consequents in this deduction
system are parser configurations, they do not correspond to disjoint sets of dependency
structures (several configurations may correspond to the same dependency structures),
and therefore do not conform to the definition of an item set. It would be possible
to define parsing schemata in a different way with a weaker definition of item sets
allowing these configurations as items, but this would make it harder to formalize
relations between parsers, because they rely on the properties of item sets.
A parsing schema for Nivres parser can be obtained by abstracting away the
rules in the system that are implementing control structures, however, and expressing
only declarative aspects of the parsers tree building logic. To do this, we first obtain
a simplified version of the deduction system. This version of the parser is obtained
by storing an index f rather than the full buffer f in each configuration, and then
grouping configurations that share common features, making them equivalent for the
side conditions of the system: Instead of storing the full set of dependency links that the
algorithm has constructed up to a given point (denoted by V), we only keep track of
whether elements in the stack have been assigned a head or not; and we represent this
by using a stack of pairs (l, b), where l is the position of a word in the string and b is a
flag which is True if the corresponding node has been assigned a head or False if it has
not:
</bodyText>
<equation confidence="0.971448466666667">
Initter
(\x0c
, 0)
Shift
(, f )
( :: ( f, False), f + 1)
Reduce
( :: (l, True), f )
(, f )
L-Link
( :: (l, h), f )
( :: (l, h) :: ( f, True), f + 1)
(wf , f ) (wl, l) R-Link
( :: (l, False), f )
(, f )
</equation>
<bodyText confidence="0.964932416666667">
(wl, l) (wf , f )
To obtain a parsing schema from this deduction system, we retain only rules per-
taining to the way in which the parser joins dependency structures and builds links
between them. In particular, the Reduce step is just a mechanism to select which of a set
of possible linkable words to link to the word currently being read. Two different
configurations corresponding to the same dependency structure may have different
lists of words in the stack depending on which Reduce steps have been executed. In
the parsing schema, these configurations must correspond to the same item, as they
involve the same dependency structures. To define an item set for this parser, we must
establish which words could be on the stack at each configuration.
A node in a dependency graph T is right-linkable if it is not a dependent of any
node situated to its right, and is not covered by any dependency link (j is covered by
</bodyText>
<page confidence="0.995816">
552
</page>
<bodyText confidence="0.981101055555555">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
the link i k if i &amp;lt; j &amp;lt; k or i &amp;gt; j &amp;gt; k). A link cannot be created between a non-right-
linkable node and any node to the right of T without violating the projectivity property.
When the parser is reading a particular word at position f, the following properties hold
for all nodes to the left of f (nodes 0 . . . f 1):
r If the node i is not right-linkable, then it cannot be on the stack.
r If the node i does not have a head, then it must be on the stack. (Note that
nodes that do not have a head assigned are always right-linkable.)
r If the node i has a head and it is right-linkable, then it may or may not be
on the stack, depending on the transitions that we have executed.
A dependency parsing schema represents items with lists (instead of stacks) containing
all the nodes found so far which are right-linkable, and a flag associated with each node
indicating whether it has been assigned a head or not. Instead of using Reduce steps to
decide which node to choose as a head of the one corresponding to the currently-read
word, we allow any node in the list that does not have a headless node to its right to
be the head; this is equivalent to performing several Reduce transitions followed by an
L-link transition.
Item set: The item set is
</bodyText>
<equation confidence="0.6587855">
INiv = {[i, \x0c(i1, b1), . . . , (ik, bk)
]  |0 i n + 1 0 i1 . . . ik n bj {False, True}}
</equation>
<bodyText confidence="0.818569777777778">
where an item [i, L] represents the set of forests of projective trees of the form F =
{T1, . . . , Tw} (w &amp;gt; 0) satisfying the following:
r The concatenation of the yields of T1, . . . , Tw is 0..i,
r The heads of the trees T1, . . . , Tw1 are the nodes j where (j, False) L;
and the head of the tree Tw is the node i,
r The right-linkable nodes in the dependency graph corresponding to
the union of the trees in F are the nodes j where (j, b) L, with
b {False, True}.
Final items: The set of final items is {[n + 1, \x0c(0, False), (v1, True), . . . , (vk, True)
</bodyText>
<equation confidence="0.817839">
]  |1 vj
</equation>
<bodyText confidence="0.97928375">
n}, the set of items containing a forest with a single projective dependency tree T headed
at the dummy node 0, whose yield spans the whole input string, and which contains
any set of right-linkable words.
Deduction steps: The deduction steps are as follows:
</bodyText>
<figure confidence="0.39236975">
INITTER:
[0, \x0c
]
ADVANCE:
</figure>
<equation confidence="0.9933690625">
[i, \x0c(i1, b1), . . . , (ik, bk)
]
[i + 1, \x0c(i1, b1), . . . , (ik, bk), (i, False)
]
L-LINK:
[i, \x0c(i1, b1), . . . , (ik, bk), (l, b), (v1, True), . . . , (vr, True)
]
[i + 1, \x0c(i1, b1), . . . , (ik, bk), (l, b), (i, True)
]
(wi, i) (wl, l)
R-LINK:
[i, \x0c(i1, b1), . . . , (ik, bk), (h, False), (v1, True), . . . , (vr, True)
]
[i, \x0c(i1, b1), . . . , (ik, bk)
]
(wh, h) (wi, i)
</equation>
<page confidence="0.984471">
553
</page>
<bodyText confidence="0.971305">
\x0cComputational Linguistics Volume 37, Number 3
Note that a naive nondeterministic implementation of this schema in a generic
deductive engine would have exponential complexity. The linear complexity in Nivres
algorithm is achieved by using a control strategy that deterministically selects a single
transition at each state.
</bodyText>
<subsectionHeader confidence="0.978938">
3.7 Covingtons (2001) Projective Parser
</subsectionHeader>
<bodyText confidence="0.996204692307692">
Covington (2001) defines a non-projective dependency parser, and a projective vari-
ant called Algorithm LSUP (for List-based Search with Uniqueness and Projectivity).
Unfortunately, the algorithm presented in Covington (2001) is not complete: It does
not parse all projective dependency structures, because when creating leftward links
it assumes that the head of a node i must be a reflexive-transitive head of the node i 1,
which is not always the case. For instance, the structure shown in Figure 5 cannot be
parsed because the constraints imposed by the algorithm prevent it from finding the
head of 4.
The MaltParser system (Nivre et al. 2007) includes an implementation of a complete
variant of Covingtons LSUP parser where these constraints have been relaxed. This
implementation has the same tree building logic as the parser described by Nivre (2003),
differing from it only with respect to the control structure. Thus, it can be seen as a
different realization of the schema shown in Section 3.6.
</bodyText>
<sectionHeader confidence="0.883387" genericHeader="method">
4. Relations Between Dependency Parsers
</sectionHeader>
<bodyText confidence="0.977754777777778">
The parsing schemata framework can be exploited to establish how different algorithms
are related, improving our understanding of the features of these parsers, and poten-
tially exposing new algorithms that combine characteristics of existing parsers in novel
ways. Sikkel (1994) defines various relations between schemata that fall into two cate-
gories: generalization relations, which are used to obtain more fine-grained versions of
parsers, and filtering relations, which can be seen as the converse of generalization and
are used to reduce the number of items and/or steps needed for parsing. Informally, a
parsing schema can be generalized from another via the following transformations:
r Item refinement: P2 is an item refinement of P1, written P1
</bodyText>
<equation confidence="0.594485">
ir
P2, if there
</equation>
<bodyText confidence="0.901377">
is a mapping between items in both parsers such that single items in P1
are mapped into multiple items in P2 and individual deductions are
preserved.
</bodyText>
<equation confidence="0.774526666666667">
r Step refinement: P1
sr
P2 if the item set of P1 is a subset of that of P2
</equation>
<bodyText confidence="0.9964045">
and every deduction step in P1 can be emulated by a sequence of steps
in P2.
Figure 5
A projective dependency structure that cannot be parsed with Covingtons LSUP algorithm.
</bodyText>
<page confidence="0.996159">
554
</page>
<bodyText confidence="0.9367155">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
A schema can be obtained from another by filtering in the following ways:
</bodyText>
<equation confidence="0.752627666666667">
r Static/dynamic filtering: P1
sf/df
P2 if the item set of P2 is a subset of that
</equation>
<bodyText confidence="0.94321225">
of P1 and P2 allows a subset of the direct inferences in P1. Sikkel (1994)
explains the distinction between static and dynamic filtering, which is not
used here.
r Item contraction: The inverse of item refinement: P1
</bodyText>
<equation confidence="0.983131888888889">
ic
P2 if P2
ir
P1.
r Step contraction: The inverse of step refinement: P1
sc
P2 if P2
sr
P1.
</equation>
<bodyText confidence="0.826678416666667">
Many of the parsing schemata described in Section 3 can be related (see Figure 6), but
for space reasons we sketch proofs for only the more interesting cases.
Theorem 1
Yamada and Matsumoto (2003)
sr
Eisner (1996).
Proof 1
It is easy to see from the schema definitions that IYM03 IEis96. We must verify that
every deduction step in the Yamada and Matsumoto (2003) schema can be emulated by
a sequence of inferences in the Eisner (1996) schema. For the INITTER step this is trivial
as the INITTERs of both parsers are equivalent. Expressing the R-LINK step of Yamada
and Matsumotos parser in the notation used for Eisner items gives:
</bodyText>
<equation confidence="0.423967818181818">
R-Link
[i, j, False, False] [j, k, False, False]
[i, k, False, False]
(wj, j) (wk, k)
This can be emulated in Eisners parser by an R-LINK step followed by a
COMBINESPANS step:
[j, k, False, False]
[j, k, True, False] (by R-LINK)
[j, k, True, False], [i, j, False, False]
[i, k, False, False] (by COMBINESPANS)
Figure 6
</equation>
<bodyText confidence="0.997681666666667">
Relating several well-known dependency parsers. Arrows pointing up correspond to
generalization relations, while those pointing down correspond to filtering. The specific subtype
of relation is shown in each arrows label, following the notation in Section 4.
</bodyText>
<page confidence="0.989765">
555
</page>
<bodyText confidence="0.471493666666667">
\x0cComputational Linguistics Volume 37, Number 3
Symmetrically, the L-LINK step in Yamada and Matsumotos parser can be emu-
lated by an L-LINK followed by a COMBINESPANS in Eisners. \x02
</bodyText>
<figure confidence="0.542598428571429">
Theorem 2
Eisner and Satta (1999)
sr
Eisner (1996).
Proof 2
Writing R-LINK in Eisner and Sattas parser in the notation used for Eisner items gives
R-LINK:
</figure>
<construct confidence="0.672417818181818">
[i, j, False, True] [j + 1, k, True, False]
[i, k, True, False]
(wi, i) (wk, k)
This inference can be emulated in Eisners parser as follows:
[j, j + 1, False, False] (by INITTER)
[i, j, False, True], [j, j + 1, False, False]
[i, j + 1, False, False] (by COMBINESPANS)
[i, j + 1, False, False], [j + 1, k, True, False]
[i, k, False, False] (by COMBINESPANS)
[i, k, False, False]
[i, k, True, False] (by R-LINK)
</construct>
<bodyText confidence="0.99753071875">
The proof corresponding to the L-LINK step is symmetric. As for the R-COMBINER and
L-COMBINER steps in Eisner and Sattas parser, it is easy to see that they are particular
cases of the COMBINESPANS step in Eisners, and therefore can be emulated by a single
application of COMBINESPANS. \x02
Note that, in practice, these two relations mean that the parsers by Eisner and Satta
(1999) and Yamada and Matsumoto (2003) are more efficient, at the schema level, than
that of Eisner (1996), in that they generate fewer items and need fewer steps to perform
the same deductions. These two parsers also have the interesting property that they
use disjoint item sets (one uses items representing trees while the other uses items
representing pairs of trees); and the union of these disjoint sets is the item set used
by Eisners parser. The optimization in Yamada and Matsumotos parser comes from
contracting deductions in Eisners parser so that linking operations are immediately
followed by combining operations; whereas Eisner and Sattas parser does the opposite,
forcing combining operations to be followed by linking operations.
By generalizing the linking steps in Eisner and Sattas parser so that the head of
each item can be in any position, we obtain an O(n5
) parser which can be filtered into
the parser of Collins (1996) by eliminating the COMBINER steps. From Collinss parser,
we obtain an O(n5
) head-corner parser based on CFG-like rules by an item refinement
in which each Collins item [i, j, h] is split into a set of items [A( ), i, j, h]. The
refinement relation between these parsers only holds if for every D-rule B A there is
a corresponding CFG-like rule A . . . B . . . in the grammar used by the head-corner
parser. Although this parser uses three indices i, j, h, using CFG-like rules to guide link-
ing decisions makes the h indices redundant. This simplification is an item contraction
which results in an O(n3
) head-corner parser. From here, we can follow the procedure
described by Sikkel (1994) to relate this head-corner algorithm to parsers analogous
to other algorithms for CFGs. In this way, we can refine the head-corner parser to a
variant of the algorithm by de Vreught and Honig (1989) (Sikkel 1997), and by successive
filters we reach a left-corner parser which is equivalent to the one described by Barbero
et al. (1998), and a step contraction of the Earley-based dependency parser by Lombardo
</bodyText>
<page confidence="0.995095">
556
</page>
<bodyText confidence="0.978803111111111">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
and Lesmo (1996). The proofs for these relations are the same as those given by Sikkel
(1994), except that the dependency variants of each algorithm are simpler (due to the
absence of epsilon rules and the fact that the rules are lexicalized). The names used
for schemata dVH1, dVH2, dVH3, and buLC shown in Figure 6 come from Sikkel (1994,
1997). These dependency parsing schemata are versions of the homonymous schemata
whose complete description can be found in Sikkel (1997), adapted for dependency
parsing. Gomez-Rodrguez (2009) gives a more thorough explanation of these relations
and schemata.
</bodyText>
<sectionHeader confidence="0.95737" genericHeader="method">
5. Proving Correctness
</sectionHeader>
<bodyText confidence="0.994746666666667">
Another use of the parsing schemata framework is that it is helpful in establishing
the correctness of a parser. Furthermore, relations between schemata can be used to
establish the correctness of one schema from that of related ones. In this section, we
show that the schemata for Yamada and Matsumoto (2003) and Eisner and Satta (1999)
are correct, and use this to prove the correctness of the schema for Eisner (1996).
Theorem 3
The Eisner and Satta (1999) parsing schema is correct.
Proof 3
To prove correctness, we must show both soundness and completeness. To verify
soundness we need to check that every individual deduction step in the parser infers a
coherent consequent item when applied to coherent antecedents (i.e., in this case, that
steps always generate non-empty items that conform to the definition in Section 3.3).
This is shown by checking that, given two antecedents of a deduction step that contain
a tree licensed by a set of D-rules G, the consequent of the step also contains such a tree.
The tree for the consequent is built from the trees corresponding to the antecedents: by
a graph union operation, in the case of COMBINER steps; or by linking the heads of both
trees with a dependency relation licensed by G, in the case of LINK steps.
To prove completeness we prove that all coherent final items are derivable by
proving the stronger result that all coherent items are derivable. We show this by strong
induction on the length of items, where the length of an item = [i, k, h] is defined as
length() = k i + 1. Coherent items of length 1 are the hypotheses of the schema (of the
form [i, i, i]) which are trivially derivable. We show that, if all coherent items of length m
are derivable for all 1 m &amp;lt; l, then items of length l are also derivable.
Let [i, k, i] be an item of length l in IES99 (thus, l = k i + 1). If this item is coher-
ent, it contains a dependency tree T such that yield(T) = i..k and head(t) = i. By con-
struction, the root of T is labelled i. Let j be the rightmost daughter of i in T. Because T
is projective, we know that the yield of j must be of the form l..k, where i &amp;lt; l j k. If
l &amp;lt; j, then l is the leftmost transitive dependent of j in T, and if k &amp;gt; j, then we know that
k is the rightmost transitive dependent of j in T.
Let Tj be the subtree of T rooted at j, T1 be the tree obtained from removing Tj by
</bodyText>
<page confidence="0.6548">
T,8
</page>
<bodyText confidence="0.90029225">
T2 be the tree obtained by removing all the nodes to the right of j from Tj, and T3
be the tree obtained by removing all the nodes to the left of j from Tj. By construction,
8 Removing a subtree from a dependency tree involves removing all the nodes in the subtree from its
vertex set, and all the outgoing links from nodes in the subtree from its edge set.
</bodyText>
<page confidence="0.99014">
557
</page>
<bodyText confidence="0.970065232558139">
\x0cComputational Linguistics Volume 37, Number 3
T1 belongs to a coherent item [i, l 1, i], T2 belongs to a coherent item [l, j, j], and T3
belongs to a coherent item [j, k, j]. Because these three items have a length strictly less
than l, by the inductive hypothesis, they are derivable. Thus the item [i, k, i] is also
derivable, as it can be obtained from these derivable items by the following inferences:
[i, l 1, i], [l, j, j]
[i, j, i] (by the L-LINK step)
[i, j, i], [j, k, j]
[i, k, i] (by the L-COMBINER step)
This proves that all coherent items of length l which are of the form [i, k, i] are derivable
under the induction hypothesis. The same can be shown for items of the form [i, k, k] by
symmetric reasoning. \x02
Theorem 4
The Yamada and Matsumoto (2003) parsing schema is correct.
Proof 4
Soundness is verified by building forests for the consequents of steps from those cor-
responding to the antecedents. To prove completeness we use strong induction on the
length of items, where the length of an item [i, j] is defined as j i + 1. The induction
step involves considering any coherent item [i, k] of length l &amp;gt; 2 (l = 2 is the base case
here because items of length 2 are generated by the Initter step) and showing that it
can be inferred from derivable antecedents of length less than l, so it is derivable. If
l &amp;gt; 2, either i has at least one right dependent or k has at least one left dependent in
the item. Suppose i has a right dependent; if T1 and T2 are the trees rooted at i and
k in a forest in [i, k], we call j the rightmost daughter of i and consider the following
trees:
r V = the subtree of T1 rooted at j,
r U1 = the tree obtained by removing V from T1,
r U2 = the tree obtained by removing all nodes to the right of j from V,
r U3 = the tree obtained by removing all nodes to the left of j from V.
The forest {U1, U2} belongs to the coherent item [i, j], and {U3, T2} belongs to the co-
herent item [j, k]. From these two items, we can obtain [i, k] by using the L-LINK step.
Symmetric reasoning can be applied if i has no right dependents but k has at least one
left dependent, analogously to the case of the previous parser. \x02
Theorem 5
The Eisner (1996) parsing schema is correct.
Proof 5
By using the previous proofs and the relationships between schemata established ear-
lier, we show that the parser of Eisner (1996) is correct: Soundness is straightforward,
and completeness can be shown by using the properties of other algorithms. Because
the set of final items in the Eisner (1996) and Eisner and Satta (1999) schemata are the
same, and the former is a step refinement of the latter, the completeness of Eisner and
Sattas parser directly implies the completeness of Eisners parser. Alternatively, we can
use Yamada and Matsumotos parser to prove the correctness of Eisners parser if we
</bodyText>
<page confidence="0.989488">
558
</page>
<bodyText confidence="0.983627">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
redefine the set of final items in the latter to be items of the form [0, n + 1, False, False],
which are equally valid as final items since they always contain parse trees. This idea
can be applied to transfer proofs of completeness across any refinement relation. \x02
</bodyText>
<sectionHeader confidence="0.990409" genericHeader="method">
6. Non-Projective Schemata
</sectionHeader>
<bodyText confidence="0.999651">
The parsing schemata presented so far define parsers that are restricted to projective
dependency structures, that is, structures in which the set of reflexive-transitive de-
pendents of each node forms a contiguous substring of the input. We now show that
the dependency parsing schema formalism can also describe various non-projective
parsers.
</bodyText>
<subsectionHeader confidence="0.99898">
6.1 Pseudo-Projectivity
</subsectionHeader>
<bodyText confidence="0.993065714285714">
Pseudo-projective parsers generate non-projective analyses in polynomial time by using
a projective parsing strategy and postprocessing the results to establish non-projective
links. This projective parsing strategy can be represented by dependency parsing
schemata such as those seen in Section 3. For example, the algorithm of Kahane, Nasr,
and Rambow (1998) uses a strategy similar to Lombardo and Lesmo (1996), but with the
following initializer step instead of the INITTER and PREDICTOR:
INITTER:
[A(), i, i 1]
A() P 1 i n
The initialization step specified by Kahane, Nasr, and Rambow (1998) differs from this
(directly consuming a nonterminal from the input) but this gives an incomplete algo-
rithm. The problem can be fixed either by using the step shown here instead (bottomup
Earley strategy) or by adding an additional step turning it into a bottomup left-corner
parser.
</bodyText>
<subsectionHeader confidence="0.976308">
6.2 Attardi (2006)
</subsectionHeader>
<bodyText confidence="0.9972629375">
The non-projective parser of Attardi (2006) extends the algorithm of Yamada and
Matsumoto (2003), adding additional shift and reduce actions to handle non-projective
dependency structures. These extra actions allow the parser to link to nodes that are
several positions deep in the stack, creating non-projective links. In particular, Attardi
uses six non-projective actions: two actions to link to nodes that are two positions
deep, another two actions for nodes that are three positions deep, and a third pair
of actions that generalizes the previous ones to n positions deep for any n. Thus, the
maximum depth in the stack to which links can be created can be configured according
to the actions allowed. We use Attd for the variant of the algorithm that allows links
only up to depth d, and Att for the original, unrestricted algorithm with unlimited
depth actions. A nondeterministic version of the algorithm Attd can be described as
follows.
Item set: The item set is IAtt = {[h1, h2, . . . , hm]  |0 h1 &amp;lt; . . . &amp;lt; hm n + 1} where [h1,
h2, . . . , hm] is the set of dependency forests of the form {T1, T2, . . . , Tm} such that:
head(Ti) = hi for each i [1..m]; and the projections of the nodes h1, h2, . . . , hm are pair-
wise disjoint, and their union is [h1..hm].
</bodyText>
<page confidence="0.995169">
559
</page>
<construct confidence="0.3392374375">
\x0cComputational Linguistics Volume 37, Number 3
Deduction steps: The set of deduction steps for Attd is the following:
INITTER:
[i, i, i]
[i + 1, i + 1, i + 1]
[i, i + 1]
COMBINE:
[h1, h2, . . . , hm]
[hm, hm+1, . . . , hp]
[h1, h2, . . . , hp]
LINK:
[h1, h2, . . . , hm]
[h1, h2, . . . , hi1, hi+1, . . . , hm]
(whi
, hi) (whj
, hj), 1 &amp;lt; i &amp;lt; m, 1 j m, j \x10= i,  |j i  |d
</construct>
<bodyText confidence="0.972260947368421">
Deduction steps for Att are obtained by removing the constraint  |j i  |d from this
set (this restriction corresponds to the maximum stack depth to which dependency links
can be created).
Final items: The set of final items is {[0, n + 1]}. Although similar to the final item set for
Yamada and Matsumotos parser, they differ in that an Attardi item of the form [0, n + 1]
may contain forests with non-projective dependency trees.
Given the number of indices manipulated in the schema, a nondeterministic im-
plementation of Attd has exponential complexity with respect to input length (though
in the implementation of Attardi [2006], control structures determinize the algorithm).
Soundness of the algorithm Att is shown as in the previous algorithms, and complete-
ness can be shown by reasoning that every coherent final item [0, n + 1] can be obtained
by first performing n + 1 INITTER steps to obtain items [i, i + 1] for each 0 i n,
then using n COMBINERs to join all of these items into [0, 1, . . . , n, n + 1], and then
performing the LINK steps corresponding to the links in a tree contained in [0, n + 1]
to obtain this final item. The algorithm Attd where d is finite is not correct with respect
to the set of non-projective dependency structures, because it only parses a restricted
subset of them (Attardi 2006). Note that the algorithm Attd is a static filter of Attd+1
for every natural number d, since the set of deduction steps of Attd is a subset of that
of Attd+1.
</bodyText>
<subsectionHeader confidence="0.9707">
6.3 The MHk Parser
</subsectionHeader>
<bodyText confidence="0.94954675">
We now define a novel variant of Attardis parser with polynomial complexity by lim-
iting the number of trees in each forest contained in an item (rather than limiting stack
depth), producing a parsing schema MHk (standing for multi-headed with at most k
heads per item). Its item set is IMHk
= {[h1, h2, . . . , hm]  |0 h1 &amp;lt; . . . &amp;lt; hm n + 1 2
m k} where [h1, h2, . . . , hm] is defined as in IAtt, and the deduction steps are the
following:
INITTER:
</bodyText>
<equation confidence="0.957031642857143">
[i, i, i]
[i + 1, i + 1, i + 1]
[i, i + 1]
COMBINE:
[h1, h2, . . . , hm]
[hm, hm+1, . . . , hp]
[h1, h2, . . . , hp]
p k
LINK:
[h1, h2, . . . , hm]
[h1, h2, . . . , hi1, hi+1, . . . , hm]
(whi
, hi) (whj
, hj), 1 &amp;lt; i &amp;lt; m, 1 j m, j \x10= i
</equation>
<bodyText confidence="0.99629825">
As with the Attd parser, MHk parses a restricted subset of non-projective dependency
structures, such that the set of structures parsed by MHk is always a subset of those
parsed by MHk+1. The MH parser, obtained by assuming that the number of trees
per forest is unbounded, is equivalent to Att, and therefore correct with respect to
</bodyText>
<page confidence="0.946169">
560
</page>
<bodyText confidence="0.952579333333333">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
the set of non-projective dependency structures. For finite values of k, MHd+2 is a
static filter of Attd, because its sets of items and deduction steps are subsets of those
of Attd. Therefore, the set of structures parsed by MHd+2 is also a subset of those parsed
by Attd.
The complexity of the MHk parser is O(nk
). For k = 3, MH3 is a step refinement
of the parser by Yamada and Matsumoto (2003) that parses projective structures only,
but by modifying the bound k we can define polynomial-time algorithms that parse
larger sets of non-projective dependency structures. The MHk parser has the property
of being able to parse any possible dependency structure as long as we make k large
enough.
</bodyText>
<subsectionHeader confidence="0.954843">
6.4 MST Parser (McDonald et al. 2005)
</subsectionHeader>
<bodyText confidence="0.998125681818182">
McDonald et al. (2005) describe a parser which finds a nonprojective analysis for
a sentence in O(n2
) time under a strong independence assumption called an edge-
factored model: Each dependency decision is assumed to be independent of all the
others (McDonald and Satta 2007). Despite the restrictiveness of this model, this max-
imum spanning tree (MST) parser achieves state-of-the-art performance for projective
and non-projective structures (Che et al. 2008; Nivre and McDonald 2008; Surdeanu
et al. 2008). The parser considers the weighted graph formed by all the possible de-
pendencies between pairs of input words, and applies an MST algorithm to find a
dependency tree covering all the words in the sentence and maximizing the sum of
weights.
The MST algorithm for directed graphs suggested by McDonald et al. (2005) is
not fully constructive: It does not work by building structures and combining them
into large structures until it finds the solution. Instead, the algorithm works by using
a greedy strategy to select a candidate set of edges for the spanning tree, potentially
creating cycles and forming an illegal dependency tree. A cycle elimination procedure
is iteratively applied to this graph until a legal dependency tree is obtained. It is still
possible to express declarative aspects of the parser with a parsing schema, although
the importance of the control mechanism in eliminating cycles makes this schema
less informative than other cases we have considered, and we will not discuss it in
detail here. Gomez-Rodrguez (2009) gives a complete description and discussion of the
schema for the MST parser.
</bodyText>
<subsectionHeader confidence="0.878401">
6.5 Covingtons (1990, 2001) Non-Projective Parser
</subsectionHeader>
<bodyText confidence="0.988110571428571">
Covingtons non-projective parsing algorithm (Covington 1990, 2001) reads the input
from left to right, establishing dependency links between the current word and previous
words in the input. The parser maintains two lists: one with all the words encountered
so far, and one with those that do not yet have a head assigned. A new word can be
linked as a dependent of any of the words in the first list, and as a head of any of the
words in the second list. The following parsing schema expresses this algorithm.
Item set: The item set is ICovNP = {[i, \x0ch1, h2, . . . , hk
</bodyText>
<equation confidence="0.844432">
]  |1 h1 . . . hk i n} where
</equation>
<bodyText confidence="0.96049275">
an item [i, \x0ch1, h2, . . . , hk
] represents the set of forests of the form F = {T1, T2, . . . , Tk}
such that head(Tj) = hj for every Tj in F; the projections of the nodes h1, h2, . . . , hk are
pairwise disjoint, and their union is [1..i].
</bodyText>
<page confidence="0.973768">
561
</page>
<table confidence="0.656084666666667">
\x0cComputational Linguistics Volume 37, Number 3
Deduction steps: The set of deduction steps is as follows:
INITTER:
</table>
<equation confidence="0.884942227272727">
[1, \x0c1
]
R-LINK:
[i, \x0ch1, . . . , hj1, hj, hj+1, . . . , hk
]
[i, \x0ch1, . . . , hj1, hj+1, . . . , hk
]
(whj
, hj) (wi, i)(j &amp;lt; i)
ADVANCE:
[i, \x0ch1, . . . , hk
]
[i + 1, \x0ch1, . . . , hk, i + 1
]
L-LINK:
[i, \x0ch1, . . . , hk, i
]
[i, \x0ch1, . . . , hk
]
(wi, i) (wj, j)(j &amp;lt; i)
Final items: The set of final items is {[n, \x0ch
]  |1 h n}, the set of items containing a
</equation>
<bodyText confidence="0.970723864864865">
forest with a single dependency tree T headed at an arbitrary position h of the string,
and whose yield spans the whole input string. The time complexity of the algorithm is
exponential in the input length n.
Note that this parsing schema is not correct, because Covingtons algorithm does
not prevent the generation of cycles in the dependency graphs it produces. Quoting
Covington (2001, page 99),
Because the parser operates one word at a time, unity can only be checked at the end
of the whole process: did it produce a tree with a single root that comprises all of the
words?
Therefore, a postprocessing mechanism is needed to determine which of the gener-
ated structures are, in fact, valid trees. In the parsing schema, this is reflected by the fact
that the schema is complete but not sound. Nivre (2007) uses a variant of this algorithm
in which cycle detection is used to avoid generating incorrect structures.
Other non-projective parsers not covered here can also be represented under the
parsing schema framework. For example, Kuhlmann (2010) presents a deduction sys-
tem for a non-projective parser which uses a grammar formalism called regular de-
pendency grammars. This deduction system can easily be converted into a parsing
schema by associating adequate semantics with items. However, we do not show this
here for space reasons, because we would first have to explain the formalism of regular
dependency grammars.
7. Mildly Non-Projective Dependency Parsing
For reasons of computational efficiency, many practical implementations of dependency
parsing are restricted to projective structures. However, some natural language sen-
tences appear to have non-projective syntactic structure, something that arises in many
languages (Havelka 2007), and is particularly common in free word order languages
such as Czech. Parsing without the projectivity constraint is computationally complex:
Although it is possible to parse non-projective structures in quadratic time with respect
to input length under a model in which each dependency decision is independent of
all the others (as in the parser of McDonald et al. [2005], discussed in Section 6.4), the
problem is intractable in the absence of this assumption (McDonald and Satta 2007).
Nivre and Nilsson (2005) observe that most non-projective dependency structures
appearing in practice contain only small proportions of non-projective arcs. This has
led to the study of sub-classes of the class of all non-projective dependency struc-
tures (Kuhlmann and Nivre 2006; Havelka 2007). Kuhlmann (2010) investigates sev-
eral such classes, based on well-nestedness and gap degree constraints (Bodirsky,
Kuhlmann, and Mohl 2005), relating them to lexicalized constituency grammar for-
malisms. Specifically, Kuhlmann shows that linear context-free rewriting systems
</bodyText>
<page confidence="0.98474">
562
</page>
<bodyText confidence="0.963540869565217">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
(LCFRS) with fan-out k (Vijay-Shanker, Weir, and Joshi 1987; Satta 1992) induce the
set of dependency structures with gap degree at most k 1; coupled CFG in which
the maximal rank of a nonterminal is k (Hotz and Pitsch 1996) induces the set of well-
nested dependency structures with gap degree at most k 1; and finally, LTAG (Joshi
and Schabes 1997) induces the set of well-nested dependency structures with gap degree
at most 1.
These results establish that there are polynomial-time dependency parsing algo-
rithms for well-nested structures with bounded gap degree, because such parsers exist
for their corresponding lexicalized constituency-based formalisms. Developing efficient
dependency parsing strategies for these sets of structures has considerable practical
interest, in particular, making it possible to parse directly with dependencies in a
data-driven manner rather than indirectly by constructing intermediate constituency
grammars and extracting dependencies from constituency parses. In this section, we
make four contributions to this enterprise.
Firstly, we define a parser for well-nested structures of gap degree 1, and prove
its correctness. The parser runs in time O(n7
), the same complexity as the best existing
algorithms for LTAG (Eisner and Satta 2000), and can be optimized to O(n6
) in the non-
lexicalized case. Secondly, we generalize our algorithm to any well-nested dependency
structure with gap degree at most k, resulting in an algorithm with time complexity
O(n5+2k
). Thirdly, we generalize the previous parsers in order to include ill-nested
structures with gap degree at most k satisfying certain constraints, giving a parser that
runs in time O(n4+3k
). Note that parsing unrestricted ill-nested structures, even when
the gap degree is bounded, is NP-complete: These structures are equivalent to LCFRS
for which the recognition problem is NP-complete (Satta 1992). Finally, we characterize
the set of structures covered by this parser, which we call mildly ill-nested structures,
and show that it includes all the trees present in a number of dependency treebanks.
We now define the concepts of gap degree and well-nestedness (Kuhlmann and
Nivre 2006). Let T be a dependency tree for the string w1 . . . wn:
Definition 5
The gap degree of a node k in T is the minimum g (N {0}) such that \x06k\x07 (the projec-
tion of the node k) can be written as the union of g + 1 intervals, that is, the number of
discontinuities in \x06k\x07. The gap degree of the dependency tree T is the maximum of the
gap degrees of its nodes.
Note that T has gap degree 0 if and only if T is projective.
Definition 6
The subtree induced by the node u in a dependency tree T is the tree Tu = (\x06u\x07, Eu)
where Eu = {i j E  |j \x06u\x07}. The subtrees induced by nodes p and q are interleaved
if \x06p\x07 \x06q\x07 = and there are nodes i, j \x06p\x07 and k, l \x06q\x07 such that i &amp;lt; k &amp;lt; j &amp;lt; l. A
dependency tree T is well-nested if it does not contain two interleaved subtrees, and
a tree that is not well-nested is said to be ill-nested.
Projective trees are always well-nested, but well-nested trees are not always projective.
</bodyText>
<subsectionHeader confidence="0.957746">
7.1 The WG1 Parser
</subsectionHeader>
<bodyText confidence="0.999677">
We now define WG1, a polynomial-time parser for well-nested dependency structures
of gap degree at most 1. In this and subsequent schemata, each dependency forest in
</bodyText>
<page confidence="0.988401">
563
</page>
<bodyText confidence="0.966303666666667">
\x0cComputational Linguistics Volume 37, Number 3
an item is a singleton set containing a dependency tree, so we will not make explicit
mention of these forests, referring directly to their trees instead. Also note that in the
parsers in this section we use D-rules to express parsing decisions, so dependency trees
are assumed to be taken from the set of trees licensed by a given set of D-rules. The
schema for the WG1 parser is defined as follows:
Item set: The item set is IWG1 = I1 I2, with
I1 = {[i, j, h, \x12, \x12]  |i, j, h N, 1 h n, 1 i j n, h \x10= j, h \x10= i 1}
where each item of the form [i, j, h, \x12, \x12] represents the set of all well-nested dependency
trees with gap degree at most 1, rooted at h, and such that \x06h\x07 = {h} [i..j], and
I2 = {[i, j, h, l, r]  |i, j, h, l, r N, 1 h n, 1 i &amp;lt; l r &amp;lt; j n, h \x10= j, h \x10= i 1, h \x10= l 1, h \x10= r}
where each item of the form [i, j, h, l, r] represents the set of all well-nested dependency
trees rooted at h such that \x06h\x07 = {h} ([i..j] \\ [l..r]), and all the nodes (except possibly
h) have gap degree at most 1. We call items of this form gapped items, and the interval
[l..r] the gap of the item. Figure 7 shows two WG1 items, one from I1 and the other from
I2, together with one of the trees contained in each of them. Note that the constraints
h \x10= j, h \x10= i + 1, h \x10= l 1, h \x10= r are added to items to avoid redundancy in the item set.
Because the result of the expression {h} ([i..j] \\ [l..r]) for a given head can be the same
for different sets of values of i, j, l, r, we restrict these values so that we cannot get two
different items representing the same dependency structures. Items violating these
constraints always have an alternative representation that does not violate them, which
we can express with a normalizing function nm() as follows:
nm([i, j, j, l, r]) = [i, j 1, j, l, r] (if r j 1 or r = \x12), or [i, l 1, j, \x12, \x12] (if r = j 1).
nm([i, j, l 1, l, r]) = [i, j, l 1, l 1, r](if l &amp;gt; i + 1), or [r + 1, j, l 1, \x12, \x12] (if l = i + 1).
nm([i, j, i 1, l, r]) = [i 1, j, i 1, l, r].
nm([i, j, r, l, r]) = [i, j, r, l, r 1] (if l &amp;lt; r), or [i, j, r, \x12, \x12] (if l = r).
nm([i, j, h, l, r]) = [i, j, h, l, r] for all other items.
When defining the deduction steps for this and other parsers, we assume that they
always produce normalized items. For clarity, we do not explicitly write this in the
deduction steps, writing instead of nm() as antecedents and consequents of steps.
</bodyText>
<figureCaption confidence="0.97325">
Figure 7
</figureCaption>
<bodyText confidence="0.890759">
Representation of the WG1 items [i, j, h, \x12, \x12] and [i, j, h, l, r], each together with one of the
dependency structures contained in it.
</bodyText>
<page confidence="0.993258">
564
</page>
<bodyText confidence="0.8363276">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
Initial items: The set of initial items (hypotheses) is defined as the set
H = {[h, h, h, \x12, \x12]  |h [1..n]}
where each item [h, h, h, \x12, \x12] represents the set containing the trivial dependency tree
consisting of a single node h and no links. This is the same set of hypotheses used by
the parsers defined in previous sections, but we use the notation [h, h, h, \x12, \x12] rather than
[h, h, h] here for convenience when defining deduction steps. The same set of hypotheses
is used for all the mildly non-projective parsers, so we do not make it explicit for
subsequent schemata. Note that initial items are separate from the item set IWG1 and
not subject to its constraints, so they do not require normalization.
Final items: The set of final items for strings of length n in WG1 is defined as the set
F = {[1, n, h, \x12, \x12]  |h [1..n]},
which is the set of the items in IWG1 containing dependency trees for the complete input
string (from position 1 to n), with their head at any position h.
Deduction steps: The deduction steps of the WG1 parser are the following:
</bodyText>
<equation confidence="0.94307628">
LINK UNGAPPED:
[h1, h1, h1, \x12, \x12]
[i2, j2, h2, \x12, \x12]
[i2, j2, h1, \x12, \x12]
(wh2
, h2) (wh1
, h1)
such that h2 [i2..j2] h1 /
[i2..j2]
LINK GAPPED:
[h1, h1, h1, \x12, \x12]
[i2, j2, h2, l2, r2]
[i2, j2, h1, l2, r2]
(wh2
, h2) (wh1
, h1)
such that h2 [i2..j2] \\ [l2..r2] h1 /
[i2..j2] \\ [l2..r2]
COMBINE UNGAPPED:
[i, j, h, \x12, \x12]
[j + 1, k, h, \x12, \x12]
[i, k, h, \x12, \x12]
COMBINE OPENING GAP:
[i, j, h, \x12, \x12]
[k, l, h, \x12, \x12]
</equation>
<construct confidence="0.837763545454546">
[i, l, h, j + 1, k 1]
j &amp;lt; k 1
COMBINE KEEPING GAP LEFT:
[i, j, h, l, r]
[j + 1, k, h, \x12, \x12]
[i, k, h, l, r]
COMBINE KEEPING GAP RIGHT:
[i, j, h, \x12, \x12]
[j + 1, k, h, l, r]
[i, k, h, l, r]
COMBINE CLOSING GAP:
[i, j, h, l, r]
[l, r, h, \x12, \x12]
[i, j, h, \x12, \x12]
COMBINE SHRINKING GAP CENTRE:
[i, j, h, l, r]
[l, r, h, l2, r2]
[i, j, h, l2, r2]
COMBINE SHRINKING GAP LEFT:
[i, j, h, l, r]
[l, k, h, \x12, \x12]
[i, j, h, k + 1, r]
</construct>
<sectionHeader confidence="0.295887" genericHeader="method">
COMBINE SHRINKING GAP RIGHT:
</sectionHeader>
<bodyText confidence="0.7676606">
[i, j, h, l, r]
[k, r, h, \x12, \x12]
[i, j, h, l, k 1]
The WG1 parser proceeds bottomup, building dependency subtrees and combining
them into larger subtrees, until a complete dependency tree for the input sentence is
</bodyText>
<page confidence="0.984287">
565
</page>
<bodyText confidence="0.949536">
\x0cComputational Linguistics Volume 37, Number 3
found. The parser logic specifies how items corresponding to the subtree induced by a
particular node are inferred, given the items for the subtrees induced by the direct de-
pendents of that node. Suppose that, in a complete dependency analysis for a sentence
</bodyText>
<equation confidence="0.8249865">
w1 . . . wn, the node h has d1 . . . dp as direct dependents (i.e., we have dependency links
d1 h, . . . , dp h). The item corresponding to the subtree induced by h is obtained
</equation>
<bodyText confidence="0.98080275">
from the ones corresponding to the subtrees induced by d1 . . . dp as follows.
First, apply the LINK UNGAPPED or LINK GAPPED step to each of the items cor-
responding to the subtrees induced by the direct dependents, and to the hypothesis
[h, h, h, \x12, \x12]. We infer p items representing the result of linking each of the dependent
subtrees to the new head h. Second, apply the various COMBINE steps to join all items
obtained in the previous step into a single item. The COMBINE steps perform a union
operation between subtrees. Therefore, the result is a dependency tree containing all the
dependent subtrees, and with all of them linked to hthis is the subtree induced by h.
This process is applied repeatedly to build larger subtrees, until, if the parsing process is
successful, a final item is found containing a dependency tree for the complete sentence.
A graphical representation of this process is shown in Figure 8.
Figure 8
Example WG1 parse, following the notation of Figure 7. LINK steps link an item to a new
head, while COMBINE steps are used to join a pair of items sharing the same head. Different
COMBINE steps correspond to different relative positions of items that can be joined and
their gaps.
</bodyText>
<page confidence="0.990058">
566
</page>
<bodyText confidence="0.990675857142857">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
The WG1 schema provides an abstract mechanism for finding all the dependency
structures in the class of well-nested structures of gap degree at most 1, for an input
string under a set of D-rules. Concrete implementations of the schema may use proba-
bilistic models or machine learning techniques to make the linking decisions associated
with the D-rules, as explained in Section 3.1. The definition of such statistical models
for guiding the execution of schemata falls outside the scope of this article.
</bodyText>
<subsectionHeader confidence="0.999628">
7.2 Proof of Correctness for WG1
</subsectionHeader>
<bodyText confidence="0.994404888888889">
We define a set of coherent items for the schema, in such a way that final items in
this set satisfy the general definition of coherent final items; and then prove the stronger
claims that all derivable items are coherent and all coherent items are derivable. The full
correctness proof has previously been published (Gomez-Rodrguez, Weir, and Carroll
2008; Gomez-Rodrguez 2009), so for reasons of space we only sketch the proof here.
To define the set of coherent items for WG1, we provide a definition of the trees that
these items must contain. Let T be a well-nested dependency tree headed at a node h,
with all its edges licensed by our set of D-rules. We call such a tree a properly formed
tree for the algorithm WG1 if it satisfies the following conditions.
</bodyText>
<listItem confidence="0.915075">
1. \x06h\x07 is either of the form {h} [i..j] or {h} ([i..j] \\ [l..r]).
2. All the nodes in T have gap degree at most 1 except for h, which can have
gap degree up to 2.
</listItem>
<bodyText confidence="0.9399804">
An item [i, j, h, l, r] IWG1 is coherent if it contains a properly formed tree headed at
h, such that \x06h\x07 = {h} ([i..j] \\ [l..r]). Similarily for items of the form [i, j, h, \x12, \x12], where
\x06h\x07 = {h} [i..j]. A coherent final item [1, n, h, \x12, \x12] for an input string contains at least
one well-nested parse of gap degree 1 for that string. With these sets of coherent and
coherent final items, we prove the soundness and completeness of WG1.
</bodyText>
<equation confidence="0.674247666666667">
Theorem 6
WG1 is sound.
Proof 6
</equation>
<bodyText confidence="0.991856714285714">
Proving the soundness of the WG1 parser involves showing that all derivable final items
are coherent. We do this by proving the stronger claim that all derivable items are
coherent. As in previous proofs, this is done by showing that each deduction step in
the parser infers a coherent consequent item when applied to coherent antecedents. We
proceed step by step, showing that if each of the antecedents of a given step contains at
least one properly formed tree, we obtain a properly formed tree that is an element of the
corresponding consequent. In the case of LINK steps, this properly formed consequent
tree is obtained by creating a dependency link between the heads of the properly formed
antecedent trees; for COMBINE steps, it is obtained from the union of the antecedent
trees. To prove that these consequent trees are properly formed, we show that they are
well-nested, have a projection corresponding to the indices in the consequent item, and
satisfy the gap degree constraint 2 required for the trees to be properly formed. Each
of these properties is proven individually, based on the properties of the antecedent
trees. \x02
</bodyText>
<page confidence="0.995388">
567
</page>
<table confidence="0.9015944">
\x0cComputational Linguistics Volume 37, Number 3
Theorem 7
WG1 is complete.
Proof 7
Proving completeness of the WG1 parser involves proving that all coherent final items
</table>
<bodyText confidence="0.62398625">
in WG1 are derivable. We show this by proving the following, stronger claim.
Lemma 1
If T is a dependency tree headed at a node h, which is a properly formed tree for WG1,
then:
</bodyText>
<listItem confidence="0.95352425">
1. If \x06h\x07 = {h} [i..j], then the item [i, j, h, \x12, \x12] containing T is a derivable
item in the WG1 parser.
2. If \x06h\x07 = {h} ([i..j] \\ [l..r]), then the item [i, j, h, l, r] containing T is a
derivable item in the WG1 parser.
</listItem>
<bodyText confidence="0.997009833333333">
This implies that all coherent final items are derivable, and therefore that WG1 is
complete. The lemma is proven by strong induction on the number of elements in \x06h\x07,
which we denote #(\x06h\x07).
The base case of the induction is trivial, because the case #(\x06h\x07) = 1 corresponds to
a tree contained in an initial item, which is derivable by definition. For the induction
step, we take T to be a properly formed dependency tree rooted at a node h, such that
</bodyText>
<equation confidence="0.522179">
#(\x06h\x07) = N for some N &amp;gt; 1. Lemma 1 holds for T if it holds for every properly formed
dependency tree T\x03
rooted at h\x03
</equation>
<bodyText confidence="0.980914954545454">
such that #(\x06h\x03
\x07) &amp;lt; N. Let p be the number of direct
children of h in the tree T. We have p 1, because by hypothesis #(\x06h\x07) &amp;gt; 1. With this,
the induction step proof is divided into two cases, according to whether p = 1 or p &amp;gt; 1.
When p = 1, the item that Lemma 1 associates with the subtree of T induced by the
single direct dependent of h is known to be derivable by the induction hypothesis. It
can be shown case by case that the item corresponding to h by Lemma 1 can be inferred
using LINK steps, thus completing the case for p = 1. For p &amp;gt; 1, we use the concept of
order annotations (Kuhlmann and Mohl 2007; Kuhlmann 2010). Order annotations are
strings that encode the precedence relation between the nodes of a dependency tree. The
order annotation for a given node encodes the shape (with respect to this precedence
relation) of the projection of each of the children of that node, that is, the number of
intervals in each projection, the number of gaps, and the way in which intervals and
gaps are interleaved. The concepts of projectivity, gap degree, and well-nestedness are
associated with particular constraints on order annotations.
The completeness proof for p &amp;gt; 1 is divided into cases according to the order
annotation of the head h. The fact that the tree T is properly formed imposes constraints
on the form of this order annotation. With this information, we divide the possible
order annotations into a number of cases. Using the induction hypotheses and some
relevant properties of order annotations we find that, for each of this cases, we can find
a sequence of COMBINE steps to infer the item corresponding to T from smaller coherent
items. \x02
</bodyText>
<subsectionHeader confidence="0.999878">
7.3 Computational Complexity of WG1
</subsectionHeader>
<bodyText confidence="0.997049">
The time complexity of WG1 is O(n7
), as the step COMBINE SHRINKING GAP CENTRE
works with seven free string positions. This complexity with respect to the length of the
</bodyText>
<page confidence="0.987551">
568
</page>
<bodyText confidence="0.9912883">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
input is as expected for this set of structures, because Kuhlmann (2010) shows that they
are equivalent to LTAG, and the best existing parsers for this formalism also perform
in O(n7
) (Eisner and Satta 2000).9
Note that the COMBINE step that is the bottleneck
only uses seven indices, not any additional entities such as D-rules. Hence, the O(n7
)
complexity does not involve additional factors relating to grammar size.
Given unlexicalized D-rules specifying the possibility of dependencies between
pairs of categories rather than pairs of words, a variant of this parser can be constructed
with time complexity O(n6
), as with parsers for unlexicalized TAG. We expand the
item set with unlexicalized items of the form [i, j, C, l, r], where C is a category, distinct
from the existing items [i, j, h, l, r]. Steps in the parser are duplicated, to work both with
lexicalized and unlexicalized items, except for the LINK steps, which always work with
a lexicalized item and an unlexicalized hypothesis to produce an unlexicalized item,
and the COMBINE SHRINKING GAP steps, which work only with unlexicalized items.
Steps are added to obtain lexicalized items from their unlexicalized equivalents by
binding the head to particular string positions. Finally, we need certain variants of the
</bodyText>
<sectionHeader confidence="0.566032" genericHeader="method">
COMBINE SHRINKING GAP steps that take two unlexicalized antecedents and produce
</sectionHeader>
<bodyText confidence="0.798882">
a lexicalized consequent; an example is the following:
</bodyText>
<equation confidence="0.6345285">
COMBINE SHRINKING GAP CENTRE L:
[i, j, C, l, r] [l + 1, r, C, l2, r2]
[i, j, l, l2, r2]
cat(wl)=C
</equation>
<bodyText confidence="0.99876575">
Although this version of the algorithm reduces time complexity to O(n6
), it also adds
a factor related to the number of categories, as well as constant factors due to having
more kinds of items and steps than the original WG1 algorithm.
</bodyText>
<subsectionHeader confidence="0.935464">
7.4 The WGk Parser
</subsectionHeader>
<bodyText confidence="0.9881066">
The WG1 parsing schema can be generalized to obtain a parser for all well-nested
dependency structures with gap degree bounded by a constant k (k 1), which we call
the WGk parser. We extend the item set so that it contains items with up to k gaps, and
modify the deduction steps to work with these multi-gapped items.
Item set: The item set for the WGk parsing schema is
</bodyText>
<equation confidence="0.660580625">
IWGk = {[i, j, h, \x0c(l1, r1), . . . , (lg, rg)
]}
where i, j, h (N {0}), 0 g k, 1 h n, 1 i j n, h \x10= j, h \x10= i 1; and for each
p {1, 2, . . . , g}: lp, rp N, i &amp;lt; lp rp &amp;lt; j, rp &amp;lt; lp+1 1, h \x10= lp 1, h \x10= rp. An item [i, j, h,
\x0c(l1, r1), . . . , (lg, rg)
] represents the set of all well-nested dependency trees rooted at h
such that \x06h\x07 = {h} ([i..j] \\
\x02g
</equation>
<bodyText confidence="0.9976325">
p=1[lp..rp]), where each interval [lp..rp] is called a gap.
The constraints h \x10= j, h \x10= i + 1, h \x10= lp 1, h \x10= rp are added to avoid redundancy, and
</bodyText>
<listItem confidence="0.75319275">
normalization is defined as in WG1.
Final items: The set of final items is defined as the set F = {[1, n, h, \x0c
]  |h [1..n]}. Note
that this set is the same as in WG1, as these are the items that we denoted [1, n, h, \x12, \x12] in
that parser.
9 Although standard TAG parsing algorithms run in time O(n6) with respect to the input length, they also
have a complexity factor related to grammar size. Eisner and Satta (2000) show that, in the case of
lexicalized TAG, this factor is a function of the input length n; hence the additional complexity.
</listItem>
<page confidence="0.993011">
569
</page>
<table confidence="0.791397">
\x0cComputational Linguistics Volume 37, Number 3
Deduction steps: The parser has the following deduction steps:
LINK:
</table>
<equation confidence="0.995778015151515">
[h1, h1, h1, \x0c
] [i2, j2, h2, \x0c(l1, r1), . . . , (lg, rg)
]
[i2, j2, h1, \x0c(l1, r1), . . . , (lg, rg)
]
(wh2
, h2) (wh1
, h1)
such that h2 [i2..j2] \\
g
\x03
p=1
[lp..rp] h1 /
[i2..j2] \\
g
\x03
p=1
[lp..rp]
COMBINE OPENING GAP:
[i, lq 1, h, \x0c(l1, r1), . . . , (lq1, rq1)
]
[rq + 1, m, h, \x0c(lq+1, rq+1), . . . , (lg, rg)
]
[i, m, h, \x0c(l1, r1), . . . , (lg, rg)
]
g k lq rq
COMBINE KEEPING GAPS:
[i, j, h, \x0c(l1, r1), . . . , (lq, rq)
]
[j + 1, m, h, \x0c(lq+1, rq+1), . . . , (lg, rg)
]
[i, m, h, \x0c(l1, r1), . . . , (lg, rg)
]
g k
COMBINE SHRINKING GAP LEFT:
[i, j, h, \x0c(l1, r1), . . . , (lq, rq), (l\x03
, rs), (ls+1, rs+1), . . . , (lg, rg)
]
[l\x03
, ls 1, h, \x0c(lq+1, rq+1), . . . , (ls1, rs1)
]
[i, j, h, \x0c(l1, r1), . . . , (lg, rg)
]
g k
COMBINE SHRINKING GAP RIGHT:
[i, j, h, \x0c(l1, r1), . . . , (lq1, rq1), (lq, r\x03
), (ls, rs), . . . , (lg, rg)
]
[rq + 1, r\x03
, h, \x0c(lq+1, rq+1), . . . , (ls1, rs1)
]
[i, j, h, \x0c(l1, r1), . . . , (lg, rg)
]
g k
COMBINE SHRINKING GAP CENTRE:
[i, j, h, \x0c(l1, r1), . . . , (lq, rq), (l\x03
, r\x03
), (ls, rs), . . . , (lg, rg)
]
[l\x03
, r\x03
, h, \x0c(lq+1, rq+1), . . . , (ls1, rs1)
]
[i, j, h, \x0c(l1, r1), . . . , (lg, rg)
]
g k
</equation>
<bodyText confidence="0.999683166666667">
As expected, the WG1 parser corresponds to WGk for k = 1. WGk works in the same
way as WG1, except that COMBINE steps can create items with more than one gap. In all
the parsers described in this section, COMBINE steps may be applied in different orders
to produce the same result, causing spurious ambiguity. In WG1 and WGk, this can be
avoided when implementing the schemata by adding flags to items so as to impose a
particular order on the execution of these steps.
</bodyText>
<subsectionHeader confidence="0.987546">
7.5 Proof of Correctness for WGk
</subsectionHeader>
<bodyText confidence="0.954092333333333">
The proof of correctness for WGk is analogous to that of WG1, but generalizing the
definition of properly formed trees to a higher gap degree. A properly formed tree in
WGk is a dependency tree T, headed at node h, such that the following hold.
</bodyText>
<footnote confidence="0.94072925">
1. \x06h\x07 is of the form {h} ([i..j] \\
\x02g
p=1[lp..rp]), with 0 g k.
2. All the nodes in T have gap degree at most k except for h, which can have
</footnote>
<bodyText confidence="0.9455305">
gap degree up to k + 1.
With this, we define coherent items and coherent final items as for WG1. Soundness
is shown as for WG1, changing the constraints on nodes so that any node can have
gap degree up to k and the head of a properly formed tree can have gap degree k + 1.
</bodyText>
<page confidence="0.966138">
570
</page>
<bodyText confidence="0.987235">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
Completeness is shown by induction on #(\x06h\x07). The base case is the same as for WG1,
and for the induction step, we consider the direct children d1 . . . dp of h. The case where
p = 1 is proven by using LINK steps just as in WG1. In the case for p 1, we also base
our proof on the order annotation for h, but we have to take into account that the set of
possible annotations is larger when we allow the gap degree to be greater than 1, so we
must consider more cases in this part of the proof.
</bodyText>
<subsectionHeader confidence="0.99982">
7.6 Computational Complexity of WGk
</subsectionHeader>
<bodyText confidence="0.998375866666667">
The WGk parser runs in time O(n5+2k
). As in the case of WG1, the step with most free
variables is COMBINE SHRINKING GAP CENTRE with 5 + 2k free indices. Again, this
complexity result is in line with what could be expected from previous research in
constituency parsing: Kuhlmann (2010) shows that the set of well-nested dependency
structures with gap degree at most k is closely related to coupled CFG in which the
maximal rank of a nonterminal is k + 1. The constituency parser defined by Hotz and
Pitsch (1996) for these grammars also adds an n2
factor for each unit increment of k. Note
that a small value of k appears to be sufficient to account for the vast majority of the
non-projective sentences found in natural language treebanks. For instance, the Prague
Dependency Treebank (Hajic et al. 2006) contains no structures with gap degree greater
than 4. Thus, a WG4 parser would be able to analyze all the well-nested structures in
this treebank, which represent 99.89% of the total (WG1 would be able to parse 99.49%).
Increasing k beyond 4 would not produce further improvements in coverage.
</bodyText>
<subsectionHeader confidence="0.995804">
7.7 Parsing Ill-Nested Structures: MG1 and MGk
</subsectionHeader>
<bodyText confidence="0.99902316">
The WGk parser analyzes dependency structures with bounded gap degree as long as
they are well-nested. Although this covers the vast majority of the structures that occur
in natural language treebanks (Kuhlmann and Nivre 2006), a significant number of sen-
tences contain ill-nested structures. Maier and Lichte (2011) provide examples of some
linguistic phenomena that cause ill-nestedness. Unfortunately, the general problem of
parsing ill-nested structures is NP-complete, even when the gap degree is bounded.
This set of structures is closely related to LCFRS with bounded fan-out and unbounded
production length, and parsing in this formalism is known to be NP-complete (Satta
1992). The reason for this complexity is the problem of unrestricted crossing configura-
tions, appearing when dependency subtrees are allowed to interleave in every possi-
ble way.
Ill-nested structures can be parsed in polynomial time with bounds on the gap
degree and the number of dependents allowed per node: Kuhlmann (2010) presents
a parser based on this idea, using a kind of grammar that resembles LCFRS, called
regular dependency grammar. This parser is exponential in the gap degree, as well as in
the maximum number of dependents allowed per node: Its complexity is O(nk(m+1)
),
where k is the maximum gap degree and m is the maximum number of dependents per
node. In contrast, the parsers presented here are data-driven and thus do not need an
explicit grammar. Furthermore, they are able to parse dependency structures with any
number of dependents per node, and their computational complexity is independent of
this parameter m.
In line with the observation that most non-projective structures appearing in prac-
tice are only slightly non-projective (Nivre and Nilsson 2005), we characterize a
sense in which the structures appearing in treebanks are only slightly ill-nested. We
</bodyText>
<page confidence="0.985493">
571
</page>
<bodyText confidence="0.990846043478261">
\x0cComputational Linguistics Volume 37, Number 3
generalize the algorithms WG1 and WGk to parse a proper superset of the set of well-
nested structures in polynomial time, and give a characterization of this new set of
structures, which includes all the structures in several dependency treebanks.
The WGk parser for well-nested structures presented previously is based on a
bottomup process, where LINK steps are used to link completed subtrees to a head,
and COMBINE steps are used to join subtrees governed by a common head to obtain a
larger structure. As WGk is a parser for well-nested structures of gap degree up to k,
its COMBINER steps correspond to all the ways in which we can join two sets of sibling
subtrees meeting these constraints, and having a common head, into another. Therefore,
this parser does not use COMBINER steps that produce interleaved subtrees, because
these would generate items corresponding to ill-nested structures.
We obtain a polynomial parser for a larger set of structures of gap degree at most
k, including some ill-nested ones, by having COMBINER steps representing all ways in
which two sets of sibling subtrees of gap degree at most k with a common head can be
joined into another, including those producing interleaved subtrees. This does not mean
that we build every possible ill-nested structure. Some structures with complex crossed
configurations have gap degree k, but cannot be built by combining two structures of
that gap degree. More specifically, this algorithm will parse a dependency structure
(well-nested or not) if there exists a binarization of that structure that has gap degree at
most k. The parser works by implicitly finding such a binarization, because COMBINE
steps are always applied to two items and no intermediate item generated by them can
exceed gap degree k (not counting the position of the head in the projection).
</bodyText>
<subsectionHeader confidence="0.461303">
Definition 7
</subsectionHeader>
<bodyText confidence="0.715839">
Let w1 . . . wn be a string, and T a dependency tree headed at a node h. A binarization of
</bodyText>
<listItem confidence="0.8256504">
T is a tree B in which each node has at most two children, such that:
1. Each node in B can be unlabelled, or labelled with a word position i.
Several nodes may have the same label (in contrast to the dependency
graphs, where a word occurrence cannot appear twice in the graph).
2. A node labelled i is a descendant of j in B if and only if i \x02
</listItem>
<bodyText confidence="0.966140357142857">
j in T.
The projection of a node in a binarization is the set of reflexive-transitive children of
that node. With this, condition (2) of Definition 7 can be rewritten i \x06j\x07B i \x06j\x07T,
and the gap degree of a binarization can be defined as with a dependency structure,
allowing us to define mildly ill-nested structures as follows.
Definition 8
A dependency structure is mildly ill-nested for gap degree k if it has at least one
binarization of gap degree k. Otherwise, it is strongly ill-nested for gap degree k.
The set of mildly ill-nested structures for gap degree k includes all well-nested structures
with gap degree up to k. We define MG1, a parser for mildly ill-nested structures for gap
degree 1, as follows.
Item set and final item set: The item set and the final item set are the same as for WG1,
except that items can contain any mildly ill-nested structures for gap degree 1, instead
of being restricted to well-nested structures.
</bodyText>
<page confidence="0.994446">
572
</page>
<construct confidence="0.791025130434783">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
Deduction steps: Deduction steps include those in WG1, plus the following.
COMBINE INTERLEAVING:
[i, j, h, l, r]
[l, k, h, r + 1, j]
[i, k, h, \x12, \x12]
COMBINE INTERLEAVING
GAP C:
[i, j, h, l, r]
[l, k, h, m, j]
[i, k, h, m, r]
m &amp;lt; r + 1
COMBINE INTERLEAVING
GAP L:
[i, j, h, l, r]
[l, k, h, r + 1, u]
[i, k, h, j + 1, u]
u &amp;gt; j COMBINE INTERLEAVING
GAP R:
[i, j, h, l, r]
[k, m, h, r + 1, j]
[i, m, h, l, k 1]
k &amp;gt; l
</construct>
<bodyText confidence="0.975759625">
These extra COMBINE steps allow the parser to combine interleaved subtrees with
simple crossing configurations. The MG1 parser still runs in O(n7
), as these new steps
do not use more than seven string positions. To generalize this algorithm to mildly
ill-nested structures for gap degree k, we add a COMBINE step for every possible
way of joining two structures of gap degree at most k into another. This is done in a
systematic way by considering a set of strings over an alphabet of three symbols: a
and b to represent intervals of words in the projection of each of the structures, and
g to represent intervals that are not in the projection of either of the structures and
will correspond to gaps in the joined structure. The legal combinations of structures
for gap degree k will correspond to strings where symbols a and b each appear at most
k + 1 times, g appears at most k times and is not the first or last symbol, and there is
no more than one consecutive appearance of any symbol. Given a string of this form,
of length n, with as located at positions a1 . . . ap(1 a1 &amp;lt; . . . &amp;lt; ap n), bs at positions
b1 . . . bq(1 b1 &amp;lt; . . . &amp;lt; bq n), and gs at positions g1 . . . gr(2 g1 &amp;lt; . . . &amp;lt; gr n 1),
such that p + q + r = n, the corresponding COMBINE step is as follows.
</bodyText>
<equation confidence="0.975061071428571">
[ia1
, iap+1 1, h, \x0c(ia1+1, ia2
1), . . . , (iap1+1, iap
1)
]
[ib1
, ibq+1 1, h, \x0c(ib1+1, ib2
1), . . . , (ibq1+1, ibq
1)
]
[imin(a1,b1), imax(ap+1,bq+1) 1, h, \x0c(ig1
, ig1+1 1), . . . , (igr
, igr+1 1)
]
</equation>
<bodyText confidence="0.999606833333333">
For example, the COMBINE INTERLEAVING GAP C step in MG1 is obtained from the
string abgab. Therefore, we define the parsing schema for MGk, a parser for mildly ill-
nested structures for gap degree k, as the schema where the item set is the same as that
of WGk, except that items now contain mildly ill-nested structures for gap degree k; and
the set of deduction steps consists of the LINK step in WGk, plus a set of COMBINE steps
obtained as explained herein.
</bodyText>
<subsectionHeader confidence="0.999822">
7.8 Computational Complexity of MGk
</subsectionHeader>
<bodyText confidence="0.997589666666667">
Because the string used to generate a COMBINER step can have length at most 3k + 2,
and the resulting step contains an index for each symbol of the string plus two extra
indices, the MGk parser has complexity O(n3k+4
) with respect to the length of the
input. Note that this expression denotes the complexity with respect to n of the MGk
parser obtained for a given k: Taking k to be a variable would add an additional O(33k
)
complexity factor, because the number of different COMBINER steps that can be applied
to a given item grows exponentially with k.
</bodyText>
<page confidence="0.994753">
573
</page>
<table confidence="0.299116">
\x0cComputational Linguistics Volume 37, Number 3
</table>
<subsectionHeader confidence="0.970732">
7.9 Proof of Correctness for MGk
</subsectionHeader>
<bodyText confidence="0.937110555555555">
As for previous parsers, we only show here a sketch of the proof that MGk is correct.
The detailed proof has been published previously (Gomez-Rodrguez, Weir, and Carroll
2008; Gomez-Rodrguez 2009).
Theorem 8
MGk is correct.
Proof 8
As with WGk, we define the sets of properly formed trees and coherent items for this
algorithm. Let T be a dependency tree headed at a node h. We call such a tree a properly
formed tree for the algorithm MGk if it satisfies the following.
</bodyText>
<footnote confidence="0.9519635">
1. \x06h\x07 is of the form {h} ([i..j] \\
\x02g
p=1[lp..rp]), with 0 g k.
2. There is a binarization of T such that all the nodes in it have gap degree at
</footnote>
<bodyText confidence="0.9764837">
most k except for its root node, which can have gap degree up to k + 1.
The sets of coherent and coherent final items are defined as in previous proofs. Sound-
ness is shown as for previous algorithms, where we show that consequent trees are
properly formed by building a binarization for them from the binarizations obtained
from antecedent items. This part of the proof involves imposing additional constraints
on binarizations, which are useful to provide a suitable way of combining binarizations
obtained from antecedents of steps. Completeness is proven by showing the following,
stronger claim.
Proposition 1
Let T be a dependency tree headed at node h, and properly formed for MGk. Then, if
</bodyText>
<equation confidence="0.9041835">
\x06h\x07 = {h} ([i..j] \\
\x02g
</equation>
<bodyText confidence="0.98033405">
p=1[lp..rp]), for g k, the item [i, j, h, \x0c(l1, r1), . . . , (lg, rg)
] containing
T is derivable under this parser.
To prove this, we say that a binarization of a properly formed tree is a well-formed
binarization for MGk if each of its nodes has gap degree k except possibly the head,
which can have gap degree k + 1. We then reduce the proof to establishing the following
lemma.
Lemma 2
Let B be a well-formed binarization of a dependency tree T, headed at a node h and
properly formed for MGk. If the projection of h in T is \x06h\x07T = \x06h\x07B = {h} ([i..j] \\
\x02g
p=1[lp..rp]), for g k, the item [i, j, h, \x0c(l1, r1), . . . , (lg, rg)
] containing T is derivable
under this parser.
The lemma is shown by strong induction on the number of nodes of B (denoted #B).
The base case where #B = 1 is trivial. For the induction step, we consider different cases
depending on the number and type of children of the head node h of B. When h has a
single child, we obtain the item corresponding to T from a smaller item, shown to be
derivable by the induction hypothesis, by using a LINK step. Where h has two children
in B, the relevant item is obtained by using a COMBINER step. \x02
</bodyText>
<page confidence="0.999119">
574
</page>
<figureCaption confidence="0.6387344">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
Figure 9
A structure which is strongly ill-nested for gap degree 1, but only mildly ill-nested for gap
degree 2.
7.10 Mildly Ill-Nested Dependency Structures
</figureCaption>
<bodyText confidence="0.914679777777778">
The MGk algorithm parses mildly ill-nested structures for gap degree k in polynomial
time. The mildly ill-nested structures for gap degree k are those with a binarization of
gap degree k. Because a binarization of a dependency structure cannot have lower gap
degree than the original structure, the mildly ill-nested structures for gap degree k all
have gap degree k. Given the relation between MGk and WGk, we know they contain
all well-nested structures with gap degree k. Figure 9 shows a structure with gap
degree 1, but which is strongly ill-nested for gap degree 1. For all trees up to 10 nodes
(excluding the dummy root node at position 0) all structures of gap degree k with
length smaller than 10 are well-nested or only mildly ill-nested for that gap degree.
Even if T is strongly ill-nested for a gap degree, there is always an m N such that
T is mildly ill-nested for m (every structure can be binarized, and binarizations have
finite gap degree). For example, the structure in Figure 9 is mildly ill-nested for gap
degree 2. Therefore, MGk parsers have the property of being able to parse any arbitrary
dependency structure as long as we make k large enough. Structures like the one in
Figure 9 do not arise in dependency treebanks. None of the treebanks for nine different
languages10
contain structures that are strongly ill-nested for their gap degree (Table 1).
Therefore, in any of these treebanks, the MGk parser can parse every sentence with gap
degree at most k in time O(n3k+4
).
8. Link Grammar Schemata
Link Grammar (LG), introduced by Sleator and Temperley (1991, 1993), is a theory
of syntax whose structural representation of sentences is closely related to projective
dependency representations, but with some important differences.11
Undirected links: Like dependency formalisms, LG represents the structure of sentences
as a set of links between words. However, whereas dependency links are directed,
the links used in LG are undirected: There is no distinction made between heads and
</bodyText>
<listItem confidence="0.857074166666667">
dependents.
10 Arabic (Hajic et al. 2004), Czech (Hajic et al. 2006), Danish (Kromann 2003), Dutch (van der Beek et al.
2002), Latin (Bamman and Crane 2006), Portuguese (Afonso et al. 2002), Slovene (Dzeroski et al. 2006),
Swedish (Nilsson, Hall, and Nivre 2005), and Turkish (Atalay, Oflazer, and Say 2003; Oflazer et al. 2003).
11 A complete treatment of LG is beyond the scope of this article: Schneider (1998) gives a detailed
comparison of Link Grammar and dependency formalisms.
</listItem>
<page confidence="0.990386">
575
</page>
<tableCaption confidence="0.457895">
\x0cComputational Linguistics Volume 37, Number 3
Table 1
</tableCaption>
<bodyText confidence="0.896986">
Counts of dependency structures in treebanks for several languages, classified by projectivity,
gap degree, and mild and strong ill-nestedness (for their gap degree).
</bodyText>
<figure confidence="0.972957782608696">
Language Structures
Total Nonprojective
Total By gap degree By nestedness
Gap
deg
1
Gap
deg
2
Gap
deg
3
Gap
deg
&amp;gt; 3
Well-
nested
Mildly
ill-
nested
Strongly
ill-
nested
</figure>
<table confidence="0.993639">
Arabic 2,995 205 189 13 2 1 204 1 0
Czech 87,889 20,353 19,989 359 4 1 20,257 96 0
Danish 5,430 864 854 10 0 0 856 8 0
Dutch 13,349 4,865 4,425 427 13 0 4,850 15 0
Latin 3,473 1,743 1,543 188 10 2 1,552 191 0
Portuguese 9,071 1,718 1,302 351 51 14 1,711 7 0
Slovene 1,998 555 443 81 21 10 550 5 0
Swedish 11,042 1,079 1,048 19 7 5 1,008 71 0
Turkish 5,583 685 656 29 0 0 665 20 0
</table>
<bodyText confidence="0.97891716">
Cycles: The sets of links representing the structure of sentences in LG may contain cycles,
in contrast to dependency structures.
LG is a grammar-based formalism in which a grammar G consists of a set of words,
each of which is associated with a set of linking requirements. Given a link grammar G,
a set of labelled links between the words of a sentence w1 . . . wn is said to be a linkage
for that sentence if it satisfies the following conditions: planarity (the links do not cross
when drawn above the words), connectivity (the undirected graph defined by links is
connected), and satisfaction (the links satisfy the linking requirements of all the words in
the input). An input sentence is considered grammatical with respect to a link grammar
G if it is possible to build a linkage for the sentence with the grammar G.
The linking requirements of a word are expressed as a set of rules specifying the
labels of the links that can be established between that word and other words located
to its left or to its right. Linking requirements can include constraints on the order of
the links, for example, a requirement can specify that a word w can be linked to two
words located to its left in such a way that the link to the farthest (leftmost) word has a
particular label L2 and the link to the closest word has a label L1.
We use the disjunctive form notation (Sleator and Temperley 1991) to denote
linking requirements: The requirements of words are expressed as a set of disjuncts.
Each disjunct corresponds to one way of satisfying the requirements of the word. We
represent a disjunct for a word w as a pair of strings = (R1R2 . . . Rq, L1L2 . . . Lp) where
L1, L2, . . . Lp are the labels of the links that must connect w to words located to the left
of w, which must be monotonically increasing in distance from w (e.g., Lp links to the
leftmost word that is directly linked to w), and R1, R2, . . . Rp are the labels of the links
that must connect w to words to its right, also monotonically increasing in distance from
w (e.g., Rq links to the rightmost word that is directly connected to w).
</bodyText>
<page confidence="0.99284">
576
</page>
<bodyText confidence="0.884008233333333">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
Parsing schemata for LG parsers follow the same principles used for constituency
and dependency formalisms. Item sets for LG parsing schemata are defined as sets of
partial syntactic structures, which in this case are partial linkages:
Definition 9
Given a link grammar G and a string w1 . . . wn, a partial linkage is any edge-labeled
undirected graph H such that the following conditions hold.
r The graph H has n vertices {v1, . . . , vn}, where each vertex vi is a tuple
(wi, i, i) such that i is a disjunct for wi in the grammar G.
r The graph H is connected and satisfies the planarity requirement with
respect to the order v1, . . . , vn of vertices (i.e., if we draw vertices in that
order, with the given links, the links do not cross).
r Given a vertex vi = (wi, i, i) such that i = (R1R2 . . . Rq, L1L2 . . . Lp), the
following conditions are satisfied:
Every edge {vi, vj} with j &amp;lt; i must be labelled Ls for some 1 s p.
For every pair of edges {vi, vj}, {vi, vk} such that k &amp;lt; j &amp;lt; i, we have
that {vi, vj} is labelled Ls1
, {vi, vk} is labelled Ls2
, and s1 &amp;lt; s2.
Every edge {vi, vj} with j &amp;gt; i must be labelled Rt for some 1 t q.
For every pair of edges {vi, vj}, {vi, vk} such that k &amp;gt; j &amp;gt; i, we have
that {vi, vj} is labelled Rt1
, {vi, vk} is labelled Rt2
, and t1 &amp;lt; t2.
Informally, a partial linkage is the result of choosing a particular disjunct from those
available for each word in the input string, and then adding labelled links between
words that are compatible with the requirements of the disjunct. Compatibility means
that, for each word wi associated with a disjunct i = (R1R2 . . . Rq, L1L2 . . . Lp), the list
of labels of links connecting vi to words to its right, ordered from the leftmost to the
rightmost such word, is of the form Ri1
</bodyText>
<equation confidence="0.882377">
, Ri2
, . . . Rir
, with 0 &amp;lt; i1 &amp;lt; i2 &amp;lt; . . . &amp;lt; ir q and,
</equation>
<bodyText confidence="0.970339">
symmetrically, the list of labels of links connecting vi to words to its left, ordered from
the rightmost to the leftmost, is of the form Lj1
</bodyText>
<equation confidence="0.782935">
, Lj2
, . . . Ljl
, with 0 &amp;lt; j1 &amp;lt; j2 &amp;lt; . . . &amp;lt; jl p.
</equation>
<bodyText confidence="0.709528">
Given such a linkage, the right linking requirements Ri1
</bodyText>
<equation confidence="0.535855">
, Ri2
, . . . Rir
</equation>
<bodyText confidence="0.841052">
of the word wi are
satisfied, and the same for the left linking requirements Lj1
</bodyText>
<equation confidence="0.682097">
, Lj2
, . . . Ljl
</equation>
<bodyText confidence="0.991138">
of wi. Linking
requirements that are not satisfied (e.g., the requirement of a link Rk in the disjunct asso-
ciated with word wi, with 0 &amp;lt; k q, such that k /
{i1, . . . , ir}) are said to be unsatisfied.
The definition of item sets for LG resembles those for dependency parsers (Defini-
tion 4), where items come from a partition of the set of partial linkages for a given link
grammar G. With these item sets, LG parsing schemata are analogous to the depen-
dency and constituency cases. As an example of an LG parsing schema, we describe the
original LG parser by Sleator and Temperley (1991), and show how projective parsing
schemata, such as those seen in Section 3, can be adapted to obtain new LG parsers.
</bodyText>
<subsectionHeader confidence="0.967392">
8.1 Sleator and Temperleys LG Parser
</subsectionHeader>
<bodyText confidence="0.978794666666667">
The LG parser of Sleator and Temperley (1991) is a dynamic programming algorithm
that builds linkages topdown: A link between vi and vk is always added before links
between vi and vj or between vj and vk, if i &amp;lt; j &amp;lt; k. This contrasts with many of the
</bodyText>
<page confidence="0.985279">
577
</page>
<bodyText confidence="0.91730765625">
\x0cComputational Linguistics Volume 37, Number 3
dependency parsers seen in previous sections (Eisner 1996; Eisner and Satta 1999;
Yamada and Matsumoto 2003), which build dependency graphs bottomup.
Item set: The item set for Sleator and Temperleys parser is
ISlT = {[i, j, , , B, C]  |0 i j n + 1
B, C {True, False} and , , , are strings of link labels}
where an item [i, j, , , B, C] represents the set of partial linkages over the sub-
string wi . . . wj of the input, wi is linked to words in that substring by links labelled
and has right linking requirements unsatisfied, wj is linked to words in the substring
by links labelled and has left linking requirements unsatisfied, B is True if and only
if there is a direct link between wi and wj, and C is True if and only if all the inner words
in the span are transitively reflexively linked to one of the end words wi or wj, and have
all of their linking requirements satisfied.
String positions referenced by the items in ISlT range from 0 to n + 1. Position 0
corresponds to an artificial word w0 (the wall) that the LG formalism inserts at the
beginning of every input sentence (Sleator and Temperley 1991). Therefore, we assume
that strings are extended with this symbol. On the other hand, position n + 1 corre-
sponds to a dummy word wn+1 that must not be linkable to any other, and is used by
the parser for convenience, as in the schema for Yamada and Matsumotos dependency
parser (Section 3.4).
We use the notation [i, , ] as shorthand for the item [i, i, , , False, True], which
is an item used to select a particular disjunct for a word wi.
Deduction steps: The set of deduction steps is the following:
SELECTDISJUNCT:
[i, RqRq1 . . . R1, LpLp1 . . . L1]
such that wi has a disjunct = (R1R2 . . . Rq, L1L2 . . . Lp)
INITTER:
[0, , ] [n + 1, \x08, \x08]
[0, n + 1, , \x08, False, False]
LEFTPREDICT:
[i, j, , , B1, False] [z, , ]
[i, z, , , False, (z i = 1)]
</bodyText>
<equation confidence="0.655704333333333">
i &amp;lt; z &amp;lt; j
LEFTLINKPREDICT (vi
b
</equation>
<bodyText confidence="0.779494333333333">
vz):
[i, j, b, , B1, False] [z, , b]
[i, z, b , b , True, (z i = 1)]
</bodyText>
<equation confidence="0.712846">
i &amp;lt; z &amp;lt; j
</equation>
<bodyText confidence="0.631675333333333">
RIGHTPREDICT:
[i, j, , , B1, False] [z, , ]
[z, j, , , False, (j z = 1)]
</bodyText>
<equation confidence="0.779344333333333">
i &amp;lt; z &amp;lt; j
RIGHTLINKPREDICT (vz
b
</equation>
<bodyText confidence="0.761602">
vj):
[i, j, , b, B1, False] [z, b, ]
[z, j, b , b , True, (j z = 1)]
</bodyText>
<equation confidence="0.716458">
i &amp;lt; z &amp;lt; j
</equation>
<bodyText confidence="0.761974222222222">
COMPLETER:
[i, j, , , B1, False]
[i, z, , , B2, True] [z, j, , , B3, True] [z, , ]
[i, j, , , B1, True]
B2 B3
An annotation of the form (vi
b
vj) near the name of a step in this and subsequent
LG schemata indicates that the corresponding step adds a link labelled b between nodes
</bodyText>
<page confidence="0.985702">
578
</page>
<bodyText confidence="0.9968586">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
vi and vj, and can be used to recover a set of complete linkages contained in a final
item from each sequence of deduction steps that generates it. The SELECTDISJUNCT
step chooses one of the available disjuncts for a given word wi. The INITTER step starts
the topdown process by constructing a linkage that spans the whole string w1 . . . wn,
but where no links have been constructed yet. Then, the PREDICT and LINKPREDICT
steps repeatedly divide the problem of finding a linkage for a substring wi . . . wj into
the smaller subproblems of finding linkages for wi . . . wz and wz . . . wj, with i &amp;lt; z &amp;lt; j. In
particular, the LEFTPREDICT step poses the subproblem of finding a linkage for wi . . . wz
in which wi is not directly linked to wz, and LEFTLINKPREDICT poses the same problem
while building a direct link from wi to wz. RIGHTPREDICT and RIGHTLINKPREDICT
proceed analogously for the substring wz . . . wj. After these two smaller linkages have
been found, they are combined by a COMPLETER step into a larger linkage; the flags
b and c in items are used by the COMPLETER step to ensure that its resulting item
will contain a valid linkage satisfying the connectivity constraint. An example of this
process, where a particular substring is parsed by using the LEFTLINKPREDICT and
RIGHTPREDICT steps to divide it into smaller substrings, is shown in Figure 10. The
algorithm runs in time O(n3
) with respect to the length of the input, because none of its
deduction steps uses more than three independent string position indices.
Final items: The set of final items is {[0, n + 1, , , B, True]}. Items of this form contain
full valid linkages for the string w0 . . . wn, because having the second boolean flag set to
True implies that their linkages for w0 . . . wn+1 have at most two connected components,
and we have assumed that the word wn+1 cannot be linked to any other, so one of the
components must link w0 . . . wn.
</bodyText>
<subsectionHeader confidence="0.967247">
8.2 Adapting Projective Dependency Parsers to Link Grammar
</subsectionHeader>
<bodyText confidence="0.998033909090909">
We now exploit similarities between LG linkages and projective dependency structures
to adapt projective dependency parsers to the LG formalism. As an example we present
an LG version of the parser by Eisner (1996), but the same principles can be applied to
other parsers: schemata for LG versions of the parsers by Eisner and Satta (1999) and
Yamada and Matsumoto (2003) can be found in Gomez-Rodrguez (2009).
Item sets from dependency parsers are adapted to LG parsers by considering the
forests contained in each dependency item. The corresponding LG items contain link-
ages with the same structure as these forests. For example, because each forest in an item
of the form [i, j, False, False] in Eisners dependency parsing schema contains two trees
Figure 10
An example of LG parsing with the schema for Sleator and Temperleys parser.
</bodyText>
<page confidence="0.97153">
579
</page>
<bodyText confidence="0.995540021276596">
\x0cComputational Linguistics Volume 37, Number 3
headed at the words wi and wj, the analogous item in the corresponding LG parsing
schema will contain linkages with two connected components, one containing the word
wi and the other containing wj. The notion of a head is lost in the conversion because
the undirected LG linkages do not make distinctions between heads and dependents.
This simplifies the notation used to denote items in some cases: For instance, we do not
need to make a distinction between Eisner items of the form [i, j, True, False] and those
of the form [i, j, False, True], because their structure is the same other than the direction
of the links. Therefore, items in the LG version of Eisners parser will use a single flag,
indicating whether linkages contained in them have one or two connected components.
The combining and linking steps of the dependency parsers are directly translated
to LG. If the original dependency steps always produce items containing projective
dependency forests, the resulting LG steps produce items with planar linkages. When
the original dependency steps have constraints related to the position of the head in
items (like combiner steps in Eisners parser, where we can combine [i, j, True, False] with
[j, k, True, False] but not with [j, k, False, True]), we ignore these constraints, allowing any
word in a linkage to be its head for the purpose of linking it to other linkages.
Because projective dependency parsers do not allow cyclic structures, we add steps
or remove constraints to allow cycles, so that the parsers are able to link two words that
are already in the same connected component of a linkage. In the schema obtained from
Eisners parser, this is done by allowing LINK steps to be applied to items representing
fully connected linkages; in the schema corresponding to Eisner and Sattas parser we
allow COMBINER steps to create a link in addition to joining two linkages; and in the
schema for Yamada and Matsumotos parser we add a step that creates two links at the
same time, combining the functionality of the L-LINK and R-LINK steps.
Finally, because LG is a grammar-based formalism where the set of valid linkages
is constrained by disjuncts associated with words, we include disjunct information in
items in order to ensure that only grammatical linkages are constructed. This is similar
to the schema for Sleator and Temperleys parser, but in this case items need to specify
both left and right linking requirements for each of their end words: These bottom
up parsers establish links from end words of an item to words outside the items span
(which can be to the left or to the right of the span) rather than to words inside the span
(which are always to the right of the left end word, and to the left of the right end word).
Based on this, the following is an LG variant of the projective dependency parser of
Eisner (1996).
Item set: The item set is
IEisLG = {[i, j, 1 1, 2 2, 3, 4, B]  |0 i j n
B {True, False} and 1, 1, 2, 2, 3, 4 are strings of link labels}
where an item of the form [i, j, 1 1, 2 2, 3, 4, B] represents the set of partial
linkages over the substring wi . . . wj of the input, satisfying the following conditions.
r All words in positions k, such that i &amp;lt; k &amp;lt; j, have all linking requirements
satisfied.
r The word in position i has left linking requirements 3 not satisfied, and
right linking requirements 11, where the requirements 1 are satisfied
by links to words within the items span, and the requirements 1 are not
satisfied. Requirements appear in 3 and 11 in increasing order of link
distance.
</bodyText>
<page confidence="0.898951">
580
</page>
<bodyText confidence="0.9841294">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
r The word in position j has right linking requirements 4 not satisfied, and
left linking requirements 22, where the requirements 2 are satisfied by
links to words within the items span, and the requirements 2 are not
satisfied. Requirements appear in 4 and 22 in increasing order of link
distance.
r The partial linkage is connected if B equals True, or has exactly two
connected components (one containing the node vi and the other
containing vj) if B equals False.
Deduction steps: The set of deduction steps for this parser is as follows:
</bodyText>
<figure confidence="0.655390857142857">
INITTER:
[i, i + 1, R, L, L, R, False]
0 i n 1
such that wi has a disjunct i = (R, L) and wi+1 has a disjunct i+1 = (R, L).
LINK (vi
b
vj):
</figure>
<bodyText confidence="0.91424619047619">
[i, j, 1 b1, 2 b2, 3, 4, B]
[i, j, 1b 1, 2b 2, 3, 4, True]
COMBINE:
[i, j, 1 1, 2, 3, 4, B1]
[j, k, 4, 2 2, 2, 4, B2]
[i, k, 1 1, 2 2, 3, 4, B1 B2]
B1 B2
These steps resemble those in the schema for Eisners dependency parser, with the
exception that the LINK step is able to build links on items that contain fully connected
linkages (equivalent to the [i, j, True, False] and [i, j, False, True] items of the dependency
parser). A version of the parser restricted to acyclic linkages can be obtained by adding
the constraint that B must equal False in the LINK step.
Final items: The set of final items is {[0, n, , , \x08, \x08, True]}, corresponding to the set of
items containing fully connected linkages for the whole input string.
LG parsing schemata based on the parsers of Eisner and Satta (1999) and Yamada
and Matsumoto (2003) are not shown here for space reasons, but are presented by
Gomez-Rodrguez (2009). The relationships between these three LG parsing schemata
are the same as the corresponding dependency parsing schemata, that is, the LG vari-
ants of Eisner and Sattas and Yamada and Matsumotos dependency parsers are step
contractions of the LG variant of Eisners parser. As with the algorithm of Sleator and
Temperley, these bottomup LG parsers run in cubic time with respect to input length.
</bodyText>
<sectionHeader confidence="0.957686" genericHeader="conclusions">
9. Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999543">
The parsing schemata formalism of Sikkel (1997) has previously been used to define,
analyze, and compare algorithms for constituency-based parsing. We have shown how
to extend the formalism to dependency parsers, as well as the related Link Grammar
formalism.
Deductive approaches have been used in the past to describe individual depen-
dency parsers: In Kuhlmann (2007, 2010) a grammatical deduction system was used to
define a parser for regular dependency grammars.
</bodyText>
<page confidence="0.961328">
581
</page>
<bodyText confidence="0.993965">
\x0cComputational Linguistics Volume 37, Number 3
McDonald and Nivre (2007) give an alternative framework for dependency parsers,
viewing them as transition systems. That model is based on parser configurations and
transitions, and has no clear relationship to the approach described here.
To demonstrate the theoretical uses of dependency parsing schemata, we have used
them to describe a wide range of existing projective and non-projective dependency
parsers. We have also clarified various relations between parsers which were origi-
nally formulated very differentlyfor example, establishing the relation between the
dynamic programming algorithm of Eisner (1996) and the transition-based parser of
Yamada and Matsumoto (2003). We have also used the parsing schemata framework as
a formal tool to verify the correctness of parsing algorithms.
Not only are dependency parsing schemata useful when describing and extending
existing parsing algorithms, they can be used to define new parsers. We have presented
an algorithm that can parse any well-nested dependency structure with gap degree
bounded by a constant k with time complexity O(n2k+5
), and additionally, have defined
a wider set of structures that we call mildly ill-nested for a given gap degree k, and
presented an algorithm that can parse these in time O(n3k+4
). The practical relevance
of this set of structures can be seen in the data obtained from several dependency
treebanks, showing that all the sentences contained in them are mildly ill-nested for
their gap degree, and thus they are parsable with this algorithm. The strategy used
by this algorithm for parsing mildly ill-nested structures has been adapted to solve
the problem of finding minimal fan-out binarizations of LCFRS to improve parsing
efficiency (see Gomez-Rodrguez et al. 2009).
An interesting line of future work would be to provide implementations of the
mildly non-projective dependency parsers presented here, using probabilistic models to
guide their linking decisions, and compare their practical performance and accuracy to
those of other non-projective dependency parsers. Additionally, our definition of mildly
ill-nested structures is closely related to the way the corresponding parser works. It
would be interesting to find a more grammar-oriented definition that would provide
linguistic insight into this set of structures.
An alternative generalization of the concept of well-nestedness has recently been
introduced by Maier and Lichte (2011). The definition of this property of structures,
called k-ill-nestedness, is more declarative than that of mildly ill-nestedness. However,
it is based on properties that are not local to projections or subtrees, and there is no
evidence that k-ill-nested structures are parsable in polynomial time.
Finally, we observe that that some well-known parsing algorithms discussed here
(Nivre 2003; McDonald et al. 2005) rely on statistically-driven control mechanisms that
fall below the abstraction level of parsing schemata. Therefore, it would be useful to
have an extension of parsing schemata allowing the description and comparison of
these control structures in a general way.
</bodyText>
<sectionHeader confidence="0.89583" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<table confidence="0.2735904">
This work was partially supported by
MEC and FEDER (HUM2007-66607-C04)
and Xunta de Galicia
(PGIDIT07SIN005206PR,
INCITE08E1R104022ES,
INCITE08ENA305025ES,
INCITE08PXIB302179PR, Rede Galega de
Proc. da Linguaxe e Recup. de Informacion,
Rede Galega de Lingustica de Corpus,
Bolsas Estadas INCITE/FSE cofinanced).
</table>
<sectionHeader confidence="0.636308" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9217147">
Afonso, Susana, Eckhard Bick, Renato Haber,
and Diana Santos. 2002. Floresta
sinta(c)tica: A treebank for Portuguese. In
Proceedings of the 3rd International
Conference on Language Resources and
Evaluation (LREC 2002), pages 19681703,
Las Palmas.
Alonso, Miguel A., David Cabrero, Eric
Villemonte de la Clergerie, and Manuel
Vilares. 1999. Tabular algorithms for TAG
</reference>
<page confidence="0.96182">
582
</page>
<reference confidence="0.998304781512605">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
parsing. In Proceedings of the Ninth
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL-99), pages 150157, Bergen.
Atalay, Nart B., Kemal Oflazer, and Bilge Say.
2003. The annotation process in the
Turkish treebank. In Proceedings of EACL
Workshop on Linguistically Interpreted
Corpora (LINC-03), pages 243246,
Budapest.
Attardi, Giuseppe. 2006. Experiments with a
multilanguage non-projective dependency
parser. In Proceedings of the 10th Conference
on Computational Natural Language Learning
(CoNLL-X), pages 166170, New York, NY.
Bamman, David and Gregory Crane. 2006.
The design and use of a Latin dependency
treebank. In Proceedings of the Fifth
Workshop on Treebanks and Linguistic
Theories (TLT 2006), pages 6778, Prague.
Barbero, Cristina, Leonardo Lesmo,
Vincenzo Lombardo, and Paola Merlo.
1998. Integration of syntactic and
lexical information in a hierarchical
dependency grammar. In Proceedings of
COLING-ACL 98 Workshop on Processing of
Dependency-Based Grammars, pages 5867,
Montreal.
van der Beek, Leonoor, Gosse Bouma, Robert
Malouf, and Gertjan van Noord. 2002. The
Alpino dependency treebank. In Language
and Computers, Computational Linguistics in
the Netherlands 2001. Selected Papers from
the Twelfth CLIN Meeting, pages 822,
Amsterdam.
Billot, Sylvie and Bernard Lang. 1989. The
structure of shared forests in ambiguous
parsing. In Proceedings of the 27th Annual
Meeting of the Association for Computational
Linguistics (ACL89), pages 143151,
Montreal.
Bodirsky, Manuel, Marco Kuhlmann, and
Mathias Mohl. 2005. Well-nested drawings
as models of syntactic structure (extended
version). Technical report, Saarland
University.
Che, Wanxiang, Zhenghua Li, Yuxuan Hu,
Yongqiang Li, Bing Qin, Ting Liu, and
Sheng Li. 2008. A cascaded syntactic and
semantic dependency parsing system.
In Proceedings of the 12th Conference on
Computational Natural Language Learning
(CoNLL 2008), pages 238242, Manchester.
Collins, Michael John. 1996. A new statistical
parser based on bigram lexical
dependencies. In Proceedings of the 34th
Annual Meeting of the Association for
Computational Linguistics (ACL96),
pages 184191, Santa Cruz, CA.
Corston-Oliver, Simon, Anthony Aue, Kevin
Duh, and Eric Ringger. 2006. Multilingual
dependency parsing using Bayes Point
Machines. In Proceedings of the Human
Language Technology Conference of the North
American Chapter of the Association for
Computational Linguistics (NAACL HLT
2006), pages 160167, New York, NY.
Courtin, Jacques and Damien Genthial.
1998. Parsing with dependency relations
and robust parsing. In Proceedings of
COLING-ACL 98 Workshop on Processing of
Dependency-Based Grammars, pages 8894,
Montreal.
Covington, Michael A. 1990. A dependency
parser for variable-word-order languages.
Technical Report AI-1990-01, University
of Georgia, Athens, GA.
Covington, Michael A. 2001. A fundamental
algorithm for dependency parsing. In
Proceedings of the 39th Annual ACM
Southeast Conference, pages 95102,
Athens, GA.
Cui, Hang, Renxu Sun, Keya Li, Min-Yen
Kan, and Tat-Seng Chua. 2005. Question
answering passage retrieval using
dependency relations. In SIGIR 05:
Proceedings of the 28th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval,
pages 400407, Salvador.
Culotta, Aron and Jeffrey Sorensen. 2004.
Dependency tree kernels for relation
extraction. In ACL 04: Proceedings of the
42nd Annual Meeting of the Association for
Computational Linguistics, pages 423429,
Barcelona.
Ding, Yuan and Martha Palmer. 2005.
Machine translation using probabilistic
synchronous dependency insertion
grammars. In ACL 05: Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics, pages 541548,
Ann Arbor, MI.
Dzeroski, Saso, Tomaz Erjavec,
Nina Ledinek, Petr Pajas, Zdenek
Zabokrtsky, and Andreja Zele. 2006.
Towards a Slovene dependency treebank.
In Proceedings of the 5th International
Conference on Language Resources and
Evaluation (LREC 2006), pages 13881391,
Genoa.
Earley, Jay. 1970. An efficient context-free
parsing algorithm. Communications of the
ACM, 13(2):94102.
Eisner, Jason. 1996. Three new probabilistic
models for dependency parsing: An
exploration. In Proceedings of the 16th
International Conference on Computational
</reference>
<page confidence="0.97885">
583
</page>
<reference confidence="0.998598075630252">
\x0cComputational Linguistics Volume 37, Number 3
Linguistics (COLING-96), pages 340345,
Copenhagen.
Eisner, Jason, Eric Goldlust, and Noah A.
Smith. 2005. Compiling comp ling:
Weighted dynamic programming
and the Dyna language. In Proceedings
of Human Language Technology
Conference and Conference on Empirical
Methods in Natural Language Processing
(HLT-EMNLP 2005), pages 281290,
Vancouver.
Eisner, Jason and Giorgio Satta. 1999.
Efficient parsing for bilexical context-free
grammars and head automaton grammars.
In Proceedings of the 37th Annual Meeting
of the Association for Computational
Linguistics (ACL99), pages 457464,
College Park, MD.
Eisner, Jason and Giorgio Satta. 2000. A
faster parsing algorithm for lexicalized
tree-adjoining grammars. In Proceedings
of the 5th Workshop on Tree-Adjoining
Grammars and Related Formalisms (TAG+5),
pages 1419, Paris.
Fundel, Katrin, Robert Kuffner, and Ralf
Zimmer. 2006. RelExRelation extraction
using dependency parse trees.
Bioinformatics, 23(3):365371.
Gaifman, Haim. 1965. Dependency systems
and phrase-structure systems. Information
and Control, 8:304337.
Gomez-Rodrguez, Carlos. 2009. Parsing
Schemata for Practical Text Analysis. Ph.D.
thesis, Universidade da Coruna, Spain.
Gomez-Rodrguez, Carlos, John Carroll, and
David Weir. 2008. A deductive approach to
dependency parsing. In Proceedings of the
46th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies (ACL08:HLT), pages 968976,
Columbus, OH.
Gomez-Rodrguez, Carlos, Marco
Kuhlmann, Giorgio Satta, and David Weir.
2009. Optimal reduction of rule length in
linear context-free rewriting systems. In
Proceedings of NAACL HLT 2009: the
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 539547, Boulder, CO.
Gomez-Rodrguez, Carlos, Jesus Vilares, and
Miguel A. Alonso. 2009. A compiler for
parsing schemata. Software: Practice and
Experience, 39(5):441470.
Gomez-Rodrguez, Carlos, David Weir, and
John Carroll. 2008. Parsing mildly
non-projective dependency structures
(extended version). Technical Report
CSRP 600, Department of Informatics,
University of Sussex.
Gomez-Rodrguez, Carlos, David Weir, and
John Carroll. 2009. Parsing mildly
non-projective dependency structures. In
Proceedings of the 12th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL-09),
pages 291299, Athens.
Hajic, Jan, Jarmila Panevova, Eva Hajicova,
Jarmila Panevova, Petr Sgall, Petr Pajas,
Jan Stepanek, Jir Havelka, and Marie
Mikulova. 2006. Prague Dependency
Treebank 2.0. CDROM CAT: LDC2006T01,
ISBN 1-58563-370-4. Linguistic Data
Consortium, University of Pennsylvania.
Hajic, Jan, Otakar Smrz, Petr Zemanek, Jan
Snaidauf, and Emanuel Beska. 2004.
Prague Arabic dependency treebank:
Development in data and tools. In
Proceedings of the NEMLAR International
Conference on Arabic Language Resources and
Tools, pages 110117, Cairo.
Havelka, Jir. 2007. Beyond projectivity:
Multilingual evaluation of constraints
and measures on non-projective
structures. In ACL 2007: Proceedings of the
45th Annual Meeting of the Association for
Computational Linguistics, pages 608615,
Prague.
Hays, David. 1964. Dependency theory: a
formalism and some observations.
Language, 40:511525.
Herrera, Jesus, Anselmo Penas, and Felisa
Verdejo. 2005. Textual entailment
recognition based on dependency analysis
and WordNet. In J. Quinonero-Camdela,
I. Dagan, B. Magnini, and F. dAlche-Buc,
editors, Machine Learning Challenges.
Lecture Notes in Computer Science,
vol. 3944. Springer-Verlag, Berlin-
Heidelberg-New York, pages 231239.
Hotz, Gunter and Gisela Pitsch. 1996. On
parsing coupled-context-free languages.
Theoretical Computer Science,
161(1-2):205233.
Joshi, Aravind K. and Yves Schabes. 1997.
Tree-adjoining grammars. In G. Rozenberg
and A. Salomaa, editors, Handbook of
Formal Languages, vol. 3: Beyond Words,
Springer-Verlag, New York, NY,
pages 69123.
Kahane, Sylvain, Alexis Nasr, and Owen
Rambow. 1998. Pseudo-projectivity: A
polynomially parsable non-projective
dependency grammar. In Proceedings of the
36th Annual Meeting of the Association for
Computational Linguistics and the 17th
International Conference on Computational
Linguistics (COLING-ACL98),
pages 646652, San Francisco, CA.
</reference>
<page confidence="0.97612">
584
</page>
<reference confidence="0.999772436974789">
\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata
Kasami, Tadao. 1965. An efficient recognition
and syntax algorithm for context-free
languages. Scientific Report
AFCRL-65-758, Air Force Cambridge
Research Lab, Bedford, MA.
Kromann, Matthias T. 2003. The Danish
dependency treebank and the underlying
linguistic theory. In Proceedings of the 2nd
Workshop on Treebanks and Linguistic
Theories (TLT), pages 217220, Vaxjo.
Kuhlmann, Marco. 2007. Dependency
Structures and Lexicalized Grammars. D. Phil
dissertation, Saarland University,
Saarbrucken, Germany.
Kuhlmann, Marco. 2010. Dependency
Structures and Lexicalized Grammars: An
Algebraic Approach. Lecture Notes in
Computer Science, vol. 6270. Springer,
New York, NY.
Kuhlmann, Marco and Mathias Mohl. 2007.
Mildly context-sensitive dependency
languages. In Proceedings of the 45th Annual
Meeting of the Association for Computational
Linguistics (ACL 2007), pages 160167,
Prague.
Kuhlmann, Marco and Joakim Nivre. 2006.
Mildly non-projective dependency
structures. In Proceedings of the
COLING/ACL 2006 Main Conference Poster
Sessions, pages 507514, Montreal.
Lombardo, Vincenzo and Leonardo Lesmo.
1996. An Earley-type recognizer for
dependency grammar. In Proceedings of
the 16th International Conference on
Computational Linguistics (COLING 96),
pages 723728, San Francisco, CA.
Maier, Wolfgang and Timm Lichte. 2011.
Characterizing discontinuity in
constituent treebanks. In P. de Grook,
M. Egg, and L. Kallmeyer, editors,
Formal Grammar, volume 5591 of
Lecture Notes in Computer Science.
Springer-Verlag, Berlin-Heidelberg-
New York, pages 164179.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In ACL 05: Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics, pages 9198,
Ann Arbor, MI.
McDonald, Ryan and Joakim Nivre. 2007.
Characterizing the errors of data-driven
dependency parsing models. In Proceedings
of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL 2007), pages 122131,
Prague.
McDonald, Ryan, Fernando Pereira, Kiril
Ribarov, and Jan Hajic. 2005. Non-
projective dependency parsing using
spanning tree algorithms. In HLT/EMNLP
2005: Proceedings of the Conference on
Human Language Technology and Empirical
Methods in Natural Language Processing,
pages 523530, Vancouver.
McDonald, Ryan and Giorgio Satta. 2007.
On the complexity of non-projective
data-driven dependency parsing. In IWPT
2007: Proceedings of the 10th International
Conference on Parsing Technologies,
pages 121132, Prague.
Nilsson, Jens, Johan Hall, and Joakim
Nivre. 2005. MAMBA meets TIGER:
Reconstructing a Swedish treebank from
antiquity. In Proceedings of NODALIDA
2005 Special Session on Treebanks,
pages 119132, Joensuu.
Nivre, Joakim. 2003. An efficient algorithm
for projective dependency parsing. In
Proceedings of the 8th International Workshop
on Parsing Technologies (IWPT 03),
pages 149160, Nancy.
Nivre, Joakim. 2007. Incremental
non-projective dependency parsing. In
Proceedings of NAACL HLT 2007: The
Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 396403, Rochester, NY.
Nivre, Joakim, Johan Hall, Sandra Kubler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007. The CoNLL
2007 shared task on dependency parsing.
In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007,
pages 915932, Prague.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency
parsing. In Proceedings of the 8th Conference
on Computational Natural Language
Learning (CoNLL-2004), pages 4956,
Boston, MA.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Atanas Chanev, Gulsen Eryigit, Sandra
Kubler, Stetoslav Marinov, and Erwin
Marsi. 2007. MaltParser: A language-
independent system for data-driven
dependency parsing. Natural Language
Engineering, 13(2):99135.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Gulsen Eryigit, and Stetoslav Marinov.
2006. Labeled pseudo-projective
dependency parsing with support vector
machines. In Proceedings of the 10th
Conference on Computational Natural
Language Learning (CoNLL-X),
pages 221225, Sydney.
</reference>
<page confidence="0.944305">
585
</page>
<reference confidence="0.998669499999999">
\x0cComputational Linguistics Volume 37, Number 3
Nivre, Joakim and Ryan McDonald.
2008. Integrating graph-based and
transition-based dependency parsers. In
Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics:
Human Language Technologies (ACL-08:
HLT), pages 950958, Columbus, OH.
Nivre, Joakim and Jens Nilsson. 2005.
Pseudo-projective dependency parsing. In
ACL 05: Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 99106, Ann Arbor, MI.
Oflazer, Kemal, Bilge Say, Dilek Zeynep
Hakkani-Tur, and Gokhan Tur. 2003.
Building a Turkish treebank. In A. Abeille,
editor, Building and Exploiting Syntactically-
annotated Corpora. Kluwer, Dordrecht,
pages 261277.
Satta, Giorgio. 1992. Recognition of linear
context-free rewriting systems. In
Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics
(ACL92), pages 8995, Newark, DE.
Schneider, Gerold. 1998. A linguistic
comparison of constituency, dependency,
and link grammar. M.Sc. thesis, University
of Zurich, Switzerland.
Shen, Libin, Jinxi Xu, and Ralph Weischedel.
2008. A new string-to-dependency
machine translation algorithm with a
target dependency language model. In
Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics:
Human Language Technologies (ACL-08:
HLT), pages 577585, Columbus, OH.
Shieber, Stuart M., Yves Schabes, and
Fernando C. N. Pereira. 1995. Principles
and implementation of deductive parsing.
Journal of Logic Programming, 24:336.
Sikkel, Klaas. 1994. How to compare the
structure of parsing algorithms. In
Proceedings of ASMICS Workshop on Parsing
Theory, pages 2139, Milano.
Sikkel, Klaas. 1997. Parsing Schemata A
Framework for Specification and Analysis of
Parsing Algorithms. Texts in Theoretical
Computer Science An EATCS Series.
Springer-Verlag, Berlin-Heidelberg-
New York.
Sleator, Daniel and Davy Temperley. 1991.
Parsing English with a Link Grammar.
Technical report CMU-CS-91-196, Carnegie
Mellon University, Pittsburgh, PA.
Sleator, Daniel and Davy Temperley. 1993.
Parsing English with a Link Grammar.
In Proceedings of the Third International
Workshop on Parsing Technologies (IWPT93),
pages 277292, Tilburg.
Surdeanu, Mihai, Richard Johansson, Adam
Meyers, Llus Marquez, and Joakim Nivre.
2008. The CoNLL-2008 shared task on
joint parsing of syntactic and semantic
dependencies. In Proceedings of the 12th
Conference on Computational Natural
Language Learning (CoNLL-2008),
pages 159177, Manchester.
Vijay-Shanker, K., David J. Weir, and
Aravind K. Joshi. 1987. Characterizing
structural descriptions produced by
various grammatical formalisms. In
Proceedings of the 25th Annual Meeting of the
Association for Computational Linguistics
(ACL87), pages 104111, Stanford, CA.
de Vreught, J. P. M. and H. J. Honig. 1989.
A tabular bottomup recognizer.
Report 89-78, Delft University of
Technology, Delft, the Netherlands.
Yamada, Hiroyasu and Yuji Matsumoto.
2003. Statistical dependency analysis with
support vector machines. In Proceedings
of 8th International Workshop on Parsing
Technologies (IWPT 2003), pages 195206,
Nancy.
Younger, Daniel H. 1967. Recognition and
parsing of context-free languages in time
</reference>
<page confidence="0.683348">
n3
</page>
<reference confidence="0.819882">
. Information and Control, 10(2):189208.
</reference>
<page confidence="0.976255">
586
</page>
<figure confidence="0.255906">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.459929">
<title confidence="0.998416">b&apos;Dependency Parsing Schemata and Mildly Non-Projective Dependency Parsing</title>
<author confidence="0.787494333333333">Carlos Gomez-Rodrguez Universidade da Coruna</author>
<author confidence="0.787494333333333">Spain John Carroll</author>
<affiliation confidence="0.984961">University of Sussex, UK</affiliation>
<author confidence="0.993996">David Weir</author>
<affiliation confidence="0.993522">University of Sussex, UK</affiliation>
<abstract confidence="0.990283416666667">We introduce dependency parsing schemata, a formal framework based on Sikkels parsing schemata for constituency parsers, which can be used to describe, analyze, and compare dependency parsing algorithms. We use this framework to describe several well-known projective and non-projective dependency parsers, build correctness proofs, and establish formal relationships between them. We then use the framework to define new polynomial-time parsing algorithms for various mildly non-projective dependency formalisms, including well-nested structures with their gap degree bounded by a constant k in time O(n5+2k ), and a new class that includes all gap degree k structures present in several natural language treebanks (which we call mildly ill-nested structures for gap degree k) in time O(n4+3k ). Finally, we illustrate how the parsing schema framework can be applied to Link Grammar, a dependency-related formalism.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Susana Afonso</author>
<author>Eckhard Bick</author>
<author>Renato Haber</author>
<author>Diana Santos</author>
</authors>
<title>Floresta sinta(c)tica: A treebank for Portuguese.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>19681703</pages>
<location>Las Palmas.</location>
<contexts>
<context position="102486" citStr="Afonso et al. 2002" startWordPosition="18254" endWordPosition="18257">91, 1993), is a theory of syntax whose structural representation of sentences is closely related to projective dependency representations, but with some important differences.11 Undirected links: Like dependency formalisms, LG represents the structure of sentences as a set of links between words. However, whereas dependency links are directed, the links used in LG are undirected: There is no distinction made between heads and dependents. 10 Arabic (Hajic et al. 2004), Czech (Hajic et al. 2006), Danish (Kromann 2003), Dutch (van der Beek et al. 2002), Latin (Bamman and Crane 2006), Portuguese (Afonso et al. 2002), Slovene (Dzeroski et al. 2006), Swedish (Nilsson, Hall, and Nivre 2005), and Turkish (Atalay, Oflazer, and Say 2003; Oflazer et al. 2003). 11 A complete treatment of LG is beyond the scope of this article: Schneider (1998) gives a detailed comparison of Link Grammar and dependency formalisms. 575 \x0cComputational Linguistics Volume 37, Number 3 Table 1 Counts of dependency structures in treebanks for several languages, classified by projectivity, gap degree, and mild and strong ill-nestedness (for their gap degree). Language Structures Total Nonprojective Total By gap degree By nestedness G</context>
</contexts>
<marker>Afonso, Bick, Haber, Santos, 2002</marker>
<rawString>Afonso, Susana, Eckhard Bick, Renato Haber, and Diana Santos. 2002. Floresta sinta(c)tica: A treebank for Portuguese. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC 2002), pages 19681703, Las Palmas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel A Alonso</author>
<author>David Cabrero</author>
</authors>
<title>Eric Villemonte de la Clergerie, and Manuel Vilares.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics (EACL-99),</booktitle>
<pages>150157</pages>
<location>Bergen.</location>
<marker>Alonso, Cabrero, 1999</marker>
<rawString>Alonso, Miguel A., David Cabrero, Eric Villemonte de la Clergerie, and Manuel Vilares. 1999. Tabular algorithms for TAG \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata parsing. In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics (EACL-99), pages 150157, Bergen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nart B Atalay</author>
<author>Kemal Oflazer</author>
<author>Bilge Say</author>
</authors>
<title>The annotation process in the Turkish treebank.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL Workshop on Linguistically Interpreted Corpora (LINC-03),</booktitle>
<pages>243246</pages>
<location>Budapest.</location>
<marker>Atalay, Oflazer, Say, 2003</marker>
<rawString>Atalay, Nart B., Kemal Oflazer, and Bilge Say. 2003. The annotation process in the Turkish treebank. In Proceedings of EACL Workshop on Linguistically Interpreted Corpora (LINC-03), pages 243246, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
</authors>
<title>Experiments with a multilanguage non-projective dependency parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>166170</pages>
<location>New York, NY.</location>
<contexts>
<context position="54463" citStr="Attardi (2006)" startWordPosition="9449" endWordPosition="9450"> in Section 3. For example, the algorithm of Kahane, Nasr, and Rambow (1998) uses a strategy similar to Lombardo and Lesmo (1996), but with the following initializer step instead of the INITTER and PREDICTOR: INITTER: [A(), i, i 1] A() P 1 i n The initialization step specified by Kahane, Nasr, and Rambow (1998) differs from this (directly consuming a nonterminal from the input) but this gives an incomplete algorithm. The problem can be fixed either by using the step shown here instead (bottomup Earley strategy) or by adding an additional step turning it into a bottomup left-corner parser. 6.2 Attardi (2006) The non-projective parser of Attardi (2006) extends the algorithm of Yamada and Matsumoto (2003), adding additional shift and reduce actions to handle non-projective dependency structures. These extra actions allow the parser to link to nodes that are several positions deep in the stack, creating non-projective links. In particular, Attardi uses six non-projective actions: two actions to link to nodes that are two positions deep, another two actions for nodes that are three positions deep, and a third pair of actions that generalizes the previous ones to n positions deep for any n. Thus, the </context>
<context position="57431" citStr="Attardi 2006" startWordPosition="10020" endWordPosition="10021">tt is shown as in the previous algorithms, and completeness can be shown by reasoning that every coherent final item [0, n + 1] can be obtained by first performing n + 1 INITTER steps to obtain items [i, i + 1] for each 0 i n, then using n COMBINERs to join all of these items into [0, 1, . . . , n, n + 1], and then performing the LINK steps corresponding to the links in a tree contained in [0, n + 1] to obtain this final item. The algorithm Attd where d is finite is not correct with respect to the set of non-projective dependency structures, because it only parses a restricted subset of them (Attardi 2006). Note that the algorithm Attd is a static filter of Attd+1 for every natural number d, since the set of deduction steps of Attd is a subset of that of Attd+1. 6.3 The MHk Parser We now define a novel variant of Attardis parser with polynomial complexity by limiting the number of trees in each forest contained in an item (rather than limiting stack depth), producing a parsing schema MHk (standing for multi-headed with at most k heads per item). Its item set is IMHk = {[h1, h2, . . . , hm] |0 h1 &amp;lt; . . . &amp;lt; hm n + 1 2 m k} where [h1, h2, . . . , hm] is defined as in IAtt, and the deduction steps </context>
</contexts>
<marker>Attardi, 2006</marker>
<rawString>Attardi, Giuseppe. 2006. Experiments with a multilanguage non-projective dependency parser. In Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 166170, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Gregory Crane</author>
</authors>
<date>2006</date>
<contexts>
<context position="102453" citStr="Bamman and Crane 2006" startWordPosition="18249" endWordPosition="18252">roduced by Sleator and Temperley (1991, 1993), is a theory of syntax whose structural representation of sentences is closely related to projective dependency representations, but with some important differences.11 Undirected links: Like dependency formalisms, LG represents the structure of sentences as a set of links between words. However, whereas dependency links are directed, the links used in LG are undirected: There is no distinction made between heads and dependents. 10 Arabic (Hajic et al. 2004), Czech (Hajic et al. 2006), Danish (Kromann 2003), Dutch (van der Beek et al. 2002), Latin (Bamman and Crane 2006), Portuguese (Afonso et al. 2002), Slovene (Dzeroski et al. 2006), Swedish (Nilsson, Hall, and Nivre 2005), and Turkish (Atalay, Oflazer, and Say 2003; Oflazer et al. 2003). 11 A complete treatment of LG is beyond the scope of this article: Schneider (1998) gives a detailed comparison of Link Grammar and dependency formalisms. 575 \x0cComputational Linguistics Volume 37, Number 3 Table 1 Counts of dependency structures in treebanks for several languages, classified by projectivity, gap degree, and mild and strong ill-nestedness (for their gap degree). Language Structures Total Nonprojective To</context>
</contexts>
<marker>Bamman, Crane, 2006</marker>
<rawString>Bamman, David and Gregory Crane. 2006.</rawString>
</citation>
<citation valid="true">
<title>The design and use of a Latin dependency treebank.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth Workshop on Treebanks and Linguistic Theories (TLT</booktitle>
<pages>6778</pages>
<location>Prague.</location>
<contexts>
<context position="56741" citStr="[2006]" startWordPosition="9889" endWordPosition="9889"> are obtained by removing the constraint |j i |d from this set (this restriction corresponds to the maximum stack depth to which dependency links can be created). Final items: The set of final items is {[0, n + 1]}. Although similar to the final item set for Yamada and Matsumotos parser, they differ in that an Attardi item of the form [0, n + 1] may contain forests with non-projective dependency trees. Given the number of indices manipulated in the schema, a nondeterministic implementation of Attd has exponential complexity with respect to input length (though in the implementation of Attardi [2006], control structures determinize the algorithm). Soundness of the algorithm Att is shown as in the previous algorithms, and completeness can be shown by reasoning that every coherent final item [0, n + 1] can be obtained by first performing n + 1 INITTER steps to obtain items [i, i + 1] for each 0 i n, then using n COMBINERs to join all of these items into [0, 1, . . . , n, n + 1], and then performing the LINK steps corresponding to the links in a tree contained in [0, n + 1] to obtain this final item. The algorithm Attd where d is finite is not correct with respect to the set of non-projectiv</context>
</contexts>
<marker>2006</marker>
<rawString>The design and use of a Latin dependency treebank. In Proceedings of the Fifth Workshop on Treebanks and Linguistic Theories (TLT 2006), pages 6778, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Barbero</author>
<author>Leonardo Lesmo</author>
<author>Vincenzo Lombardo</author>
<author>Paola Merlo</author>
</authors>
<title>Integration of syntactic and lexical information in a hierarchical dependency grammar.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL 98 Workshop on Processing of Dependency-Based Grammars,</booktitle>
<pages>5867</pages>
<location>Montreal.</location>
<contexts>
<context position="7155" citStr="Barbero et al. (1998)" startWordPosition="1039" endWordPosition="1042">ings of intermediate dependency structures are defined as items, and the operations used to combine them are expressed as inference rules. We begin by addressing a number of preliminary issues. Traditional parsing schemata are used to define grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley PREDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classifiers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we use the notion of D-rule</context>
<context position="32123" citStr="Barbero et al. (1998)" startWordPosition="5421" endWordPosition="5424"> B() P SCANNER: [A( \x02), i, h 1] [h, h, h] [A( \x02 ), i, h] wh IS A COMPLETER: [A( B), i, j] [B(), j + 1, k] [A(B ), i, k] Final items: The final item set is {[(S), 1, n]}. The schema for Lombardo and Lesmos parser is a variant of the Earley constituency parser (cf. Sikkel 1997), with minor changes to adapt it to dependency grammar (for example, the SCANNER always moves the dot over the head symbol , rather than over a terminal symbol). Analogously, other dependency parsing schemata based on CFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997): The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and the parser described by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Nivre (2003) Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing, later extended by Nivre, Hall, and Nilsson (2004). With linear-time performance and competitive parsing accuracy (Nivre et al. 2006; Nivre and McDonald 2008), it is one of the parsers included in the MaltParser system (Nivre et al. 2007), which is currently widely used (e.g., Nivre et al. 2007; Surdeanu et al. 2008). The parser proceeds by reading the sent</context>
<context position="46498" citStr="Barbero et al. (1998)" startWordPosition="8009" endWordPosition="8012">. Although this parser uses three indices i, j, h, using CFG-like rules to guide linking decisions makes the h indices redundant. This simplification is an item contraction which results in an O(n3 ) head-corner parser. From here, we can follow the procedure described by Sikkel (1994) to relate this head-corner algorithm to parsers analogous to other algorithms for CFGs. In this way, we can refine the head-corner parser to a variant of the algorithm by de Vreught and Honig (1989) (Sikkel 1997), and by successive filters we reach a left-corner parser which is equivalent to the one described by Barbero et al. (1998), and a step contraction of the Earley-based dependency parser by Lombardo 556 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata and Lesmo (1996). The proofs for these relations are the same as those given by Sikkel (1994), except that the dependency variants of each algorithm are simpler (due to the absence of epsilon rules and the fact that the rules are lexicalized). The names used for schemata dVH1, dVH2, dVH3, and buLC shown in Figure 6 come from Sikkel (1994, 1997). These dependency parsing schemata are versions of the homonymous schemata whose complete description can be</context>
</contexts>
<marker>Barbero, Lesmo, Lombardo, Merlo, 1998</marker>
<rawString>Barbero, Cristina, Leonardo Lesmo, Vincenzo Lombardo, and Paola Merlo. 1998. Integration of syntactic and lexical information in a hierarchical dependency grammar. In Proceedings of COLING-ACL 98 Workshop on Processing of Dependency-Based Grammars, pages 5867, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonoor van der Beek</author>
<author>Gosse Bouma</author>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>The Alpino dependency treebank.</title>
<date>2002</date>
<booktitle>In Language and Computers, Computational Linguistics in the Netherlands 2001. Selected Papers from the Twelfth CLIN Meeting,</booktitle>
<pages>822</pages>
<location>Amsterdam.</location>
<marker>van der Beek, Bouma, Malouf, van Noord, 2002</marker>
<rawString>van der Beek, Leonoor, Gosse Bouma, Robert Malouf, and Gertjan van Noord. 2002. The Alpino dependency treebank. In Language and Computers, Computational Linguistics in the Netherlands 2001. Selected Papers from the Twelfth CLIN Meeting, pages 822, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvie Billot</author>
<author>Bernard Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL89),</booktitle>
<pages>143151</pages>
<location>Montreal.</location>
<contexts>
<context position="20285" citStr="Billot and Lang 1989" startWordPosition="3326" endWordPosition="3329">ditions for the parsers deduction steps. Side conditions restrict the inference relation by specifying which combinations of values are permissible for the variables appearing in the antecedents and consequent of deduction steps. This parsing schema specifies a recognizer: Given a set of D-rules and an input string w1 . . . wn, the sentence can be parsed (projectively) under those D-rules if and only if the deduction system infers a coherent final item. When executing this schema with a deductive engine, the parse forest can be recovered by following back pointers, as in constituency parsers (Billot and Lang 1989). This schema formalizes a parsing logic which is independent of the order and the way linking decisions are taken. Statistical models can be used to determine whether a step linking words a and b in positions i and ji.e., having (a, i) (b, j) as a side conditionis executed or not, and probabilities can be attached to items in order to assign different weights to different analyses of the sentence. The side conditions provide an explicit representation of the choice points where probabilistic decisions are made by the control mechanism that is executing the schema. The same principle applies t</context>
</contexts>
<marker>Billot, Lang, 1989</marker>
<rawString>Billot, Sylvie and Bernard Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL89), pages 143151, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel Bodirsky</author>
<author>Marco Kuhlmann</author>
<author>Mathias Mohl</author>
</authors>
<title>Well-nested drawings as models of syntactic structure (extended version).</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Saarland University.</institution>
<marker>Bodirsky, Kuhlmann, Mohl, 2005</marker>
<rawString>Bodirsky, Manuel, Marco Kuhlmann, and Mathias Mohl. 2005. Well-nested drawings as models of syntactic structure (extended version). Technical report, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Yuxuan Hu</author>
<author>Yongqiang Li</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>A cascaded syntactic and semantic dependency parsing system.</title>
<date>2008</date>
<contexts>
<context position="59895" citStr="Che et al. 2008" startWordPosition="10489" endWordPosition="10492">Hk parser has the property of being able to parse any possible dependency structure as long as we make k large enough. 6.4 MST Parser (McDonald et al. 2005) McDonald et al. (2005) describe a parser which finds a nonprojective analysis for a sentence in O(n2 ) time under a strong independence assumption called an edgefactored model: Each dependency decision is assumed to be independent of all the others (McDonald and Satta 2007). Despite the restrictiveness of this model, this maximum spanning tree (MST) parser achieves state-of-the-art performance for projective and non-projective structures (Che et al. 2008; Nivre and McDonald 2008; Surdeanu et al. 2008). The parser considers the weighted graph formed by all the possible dependencies between pairs of input words, and applies an MST algorithm to find a dependency tree covering all the words in the sentence and maximizing the sum of weights. The MST algorithm for directed graphs suggested by McDonald et al. (2005) is not fully constructive: It does not work by building structures and combining them into large structures until it finds the solution. Instead, the algorithm works by using a greedy strategy to select a candidate set of edges for the s</context>
</contexts>
<marker>Che, Li, Hu, Li, Qin, Liu, Li, 2008</marker>
<rawString>Che, Wanxiang, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, and Sheng Li. 2008. A cascaded syntactic and semantic dependency parsing system.</rawString>
</citation>
<citation valid="true">
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL</booktitle>
<pages>238242</pages>
<location>Manchester.</location>
<contexts>
<context position="4663" citStr="(2008)" startWordPosition="651" endWordPosition="651">al sets of mildly non-projective dependency structures, including a parser for a new class of structures we call mildly ill-nested, which encompasses all the structures in a number of existing dependency treebanks (see Section 7). r We adapt the dependency parsing schema framework to the formalism of Link Grammar (Sleator and Temperley 1991, 1993) (see Section 8). Although some of these contributions have been published previously, this article presents them in a thorough and consistent way. The definition of dependency parsing schemata was first published by Gomez-Rodrguez, Carroll, and Weir (2008), along with some of the projective schemata presented here and their associated proofs. The results concerning mildly non-projective parsing in Section 7 were first published by GomezRodrguez, Weir, and Carroll (2008, 2009). On the other hand, the material on Nivre and Covingtons projective parsers, as well as all the non-projective parsers and the application of the formalism to Link Grammar, are entirely new contributions of this article. The notion of a parsing schema comes from considering parsing as a deduction process which generates intermediate results called items. In particular, ite</context>
<context position="30063" citStr="(2008)" startWordPosition="5046" endWordPosition="5046">two kinds of nodes: preterminal nodes and terminal nodes. Depending on whether all the preterminal nodes have been linked to terminals, extended dependency trees can either be grounded, in which case they are isomorphic to traditional dependency graphs (see Figure 3), or ungrounded, as in Figure 4, in which case they capture parser states in which some structure has been predicted, but not yet found. Note that a dependency graph can always be extracted from such a tree, but in the ungrounded case different extended trees can be associated with the same graph. Gomez-Rodrguez, Carroll, and Weir (2008) present extended dependency trees in more detail. Item set: The item set is ILomLes = {[A( ), i, j] |A() P 1 i j + 1 n} where and are strings; P is a set of CFG-like rules;7 and each item [A( ), i, j] represents the set of projective extended dependency trees rooted at A, where the direct children of A are , and the subtrees rooted at have yield i..j. Note that Lombardo and Lesmos parser uses both grounded trees (in items [A(), i, j]) and non-grounded trees (in items 7 A CFG-like rule A( ) rewrites a preterminal A to strings x over terminals and preterminals, where , are strings of pretermina</context>
</contexts>
<marker>2008</marker>
<rawString>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL 2008), pages 238242, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL96),</booktitle>
<pages>184191</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="16987" citStr="Collins (1996)" startWordPosition="2732" endWordPosition="2733">) items are called correct (final) items in the original formulation by Sikkel (1997). 3 Derivable items are called valid items in the original formulation by Sikkel (1997). 545 \x0cComputational Linguistics Volume 37, Number 3 Figure 2 Representation of the [i, j, h] item in Collinss parser, together with one of the dependency structures contained in it (left side); and of the antecedents and consequents of an L-LINK step (right side). White rectangles in an item represent intervals of nodes that have been assigned a head by the parser, and dark squares represent nodes that have no head. 3.1 Collins (1996) One of the most straightforward projective dependency parsing strategies was introduced by Collins (1996), and is based on the CYK bottomup parsing strategy (Kasami 1965; Younger 1967). Collinss parser works with dependency trees which are linked to each other by creating links between their heads. The schema for this parser maps every set of D-rules G and input string w1 . . . wn to an instantiated dependency parsing system (ICol96, H, DCol96) such that: Item set: The item set is defined as ICol96 = {[i, j, h] |1 i h j n}, where item [i, j, h] is defined as the set of forests containing a si</context>
<context position="25333" citStr="Collins (1996)" startWordPosition="4258" endWordPosition="4259">ema, and not require an Initter step, but we prefer a standard set of hypotheses valid for all parsers as it facilitates more straightforward proofs of the relations between schemata. 548 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata where LINK steps create a dependency link between two dependency trees spanning adjacent segments of the input, and COMBINER steps join two overlapping trees by a graph union operation that does not create new links. COMBINER steps follow the same mechanism as those in the algorithm of Eisner (1996), and LINK steps work analogously to those of Collins (1996), so this schema can be seen as being intermediate between those two algorithms. These relationships will be formally described in Section 4. Final items: The set of final items is {[0, n, 0]}. By convention, parse trees have the beginning-of-sentence marker 0 as their head, as in the previous algorithm. When described for head automaton grammars (Eisner and Satta 1999), this algorithm appears to be more complex to understand and implement than the previous one, requiring four different kinds of items to keep track of the state of the automata used by the grammars. However, this abstract repre</context>
<context position="45470" citStr="Collins (1996)" startWordPosition="7828" endWordPosition="7829"> uses items representing pairs of trees); and the union of these disjoint sets is the item set used by Eisners parser. The optimization in Yamada and Matsumotos parser comes from contracting deductions in Eisners parser so that linking operations are immediately followed by combining operations; whereas Eisner and Sattas parser does the opposite, forcing combining operations to be followed by linking operations. By generalizing the linking steps in Eisner and Sattas parser so that the head of each item can be in any position, we obtain an O(n5 ) parser which can be filtered into the parser of Collins (1996) by eliminating the COMBINER steps. From Collinss parser, we obtain an O(n5 ) head-corner parser based on CFG-like rules by an item refinement in which each Collins item [i, j, h] is split into a set of items [A( ), i, j, h]. The refinement relation between these parsers only holds if for every D-rule B A there is a corresponding CFG-like rule A . . . B . . . in the grammar used by the head-corner parser. Although this parser uses three indices i, j, h, using CFG-like rules to guide linking decisions makes the h indices redundant. This simplification is an item contraction which results in an </context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Collins, Michael John. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL96), pages 184191, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Anthony Aue</author>
<author>Kevin Duh</author>
<author>Eric Ringger</author>
</authors>
<title>Multilingual dependency parsing using Bayes Point Machines.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT</booktitle>
<pages>160167</pages>
<location>New York, NY.</location>
<contexts>
<context position="21596" citStr="Corston-Oliver et al. 2006" startWordPosition="3550" endWordPosition="3553"> number of free variables used in deduction steps of Collinss parser, it is apparent that its time complexity is O(n5 ): There are O(n5 ) combinations of index values with which each of its LINK steps can be executed.5 This complexity arises because a parentless word (head) may appear in any position in the items generated by the parser; the complexity can be reduced to O(n3 ) by ensuring that parentless words only appear at the first or last position of an item. This is the idea behind the parser defined by Eisner (1996), which is still in wide use today (McDonald, Crammer, and Pereira 2005; Corston-Oliver et al. 2006). The parsing schema for this algorithm is defined as follows. Item set: The item set is IEis96 = {[i, j, True, False] |0 i j n} {[i, j, False, True] |0 i j n} {[i, j, False, False] |0 i j n}, where item [i, j, True, False] corresponds to [i, j, j] ICol96, item [i, j, False, True] corresponds to item [i, j, i] ICol96, and item [i, j, False, False] is defined as the set of forests 5 For this and the rest of the complexity results in this article, we assume that the linking decision associated with a D-rule can be made in constant time. 547 \x0cComputational Linguistics Volume 37, Number 3 of th</context>
</contexts>
<marker>Corston-Oliver, Aue, Duh, Ringger, 2006</marker>
<rawString>Corston-Oliver, Simon, Anthony Aue, Kevin Duh, and Eric Ringger. 2006. Multilingual dependency parsing using Bayes Point Machines. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2006), pages 160167, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Courtin</author>
<author>Damien Genthial</author>
</authors>
<title>Parsing with dependency relations and robust parsing.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL 98 Workshop on Processing of Dependency-Based Grammars,</booktitle>
<pages>8894</pages>
<location>Montreal.</location>
<contexts>
<context position="32224" citStr="Courtin and Genthial (1998)" startWordPosition="5437" endWordPosition="5440"> [B(), j + 1, k] [A(B ), i, k] Final items: The final item set is {[(S), 1, n]}. The schema for Lombardo and Lesmos parser is a variant of the Earley constituency parser (cf. Sikkel 1997), with minor changes to adapt it to dependency grammar (for example, the SCANNER always moves the dot over the head symbol , rather than over a terminal symbol). Analogously, other dependency parsing schemata based on CFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997): The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and the parser described by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Nivre (2003) Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing, later extended by Nivre, Hall, and Nilsson (2004). With linear-time performance and competitive parsing accuracy (Nivre et al. 2006; Nivre and McDonald 2008), it is one of the parsers included in the MaltParser system (Nivre et al. 2007), which is currently widely used (e.g., Nivre et al. 2007; Surdeanu et al. 2008). The parser proceeds by reading the sentence from left to right, using a stack and four different kinds of transitions between configurations</context>
</contexts>
<marker>Courtin, Genthial, 1998</marker>
<rawString>Courtin, Jacques and Damien Genthial. 1998. Parsing with dependency relations and robust parsing. In Proceedings of COLING-ACL 98 Workshop on Processing of Dependency-Based Grammars, pages 8894, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>A dependency parser for variable-word-order languages.</title>
<date>1990</date>
<contexts>
<context position="7773" citStr="Covington 1990" startWordPosition="1133" endWordPosition="1134">nd Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classifiers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we use the notion of D-rules (Covington 1990). D-rules have the form (a, i) (b, j), which specifies that a word b located at position j in the input string can have the word a in position i as a dependent. Deduction steps in data-driven parsers can be associated with the D-rules corresponding to the links they create, so that parsing schemata for such parsers are defined using grammars of D-rules. In this way, we obtain a representation of some of the declarative aspects of these parsing strategies that is independent of the particular model used to make the decisions associated with each D-rule. Note that this representation is useful f</context>
<context position="61171" citStr="Covington 1990" startWordPosition="10692" endWordPosition="10693">al dependency tree. A cycle elimination procedure is iteratively applied to this graph until a legal dependency tree is obtained. It is still possible to express declarative aspects of the parser with a parsing schema, although the importance of the control mechanism in eliminating cycles makes this schema less informative than other cases we have considered, and we will not discuss it in detail here. Gomez-Rodrguez (2009) gives a complete description and discussion of the schema for the MST parser. 6.5 Covingtons (1990, 2001) Non-Projective Parser Covingtons non-projective parsing algorithm (Covington 1990, 2001) reads the input from left to right, establishing dependency links between the current word and previous words in the input. The parser maintains two lists: one with all the words encountered so far, and one with those that do not yet have a head assigned. A new word can be linked as a dependent of any of the words in the first list, and as a head of any of the words in the second list. The following parsing schema expresses this algorithm. Item set: The item set is ICovNP = {[i, \x0ch1, h2, . . . , hk ] |1 h1 . . . hk i n} where an item [i, \x0ch1, h2, . . . , hk ] represents the set o</context>
</contexts>
<marker>Covington, 1990</marker>
<rawString>Covington, Michael A. 1990. A dependency parser for variable-word-order languages.</rawString>
</citation>
<citation valid="false">
<tech>Technical Report AI-1990-01,</tech>
<institution>University of Georgia,</institution>
<location>Athens, GA.</location>
<marker></marker>
<rawString>Technical Report AI-1990-01, University of Georgia, Athens, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>A fundamental algorithm for dependency parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual ACM Southeast Conference,</booktitle>
<pages>95102</pages>
<location>Athens, GA.</location>
<contexts>
<context position="39446" citStr="Covington (2001)" startWordPosition="6812" endWordPosition="6813">, True) ] [i + 1, \x0c(i1, b1), . . . , (ik, bk), (l, b), (i, True) ] (wi, i) (wl, l) R-LINK: [i, \x0c(i1, b1), . . . , (ik, bk), (h, False), (v1, True), . . . , (vr, True) ] [i, \x0c(i1, b1), . . . , (ik, bk) ] (wh, h) (wi, i) 553 \x0cComputational Linguistics Volume 37, Number 3 Note that a naive nondeterministic implementation of this schema in a generic deductive engine would have exponential complexity. The linear complexity in Nivres algorithm is achieved by using a control strategy that deterministically selects a single transition at each state. 3.7 Covingtons (2001) Projective Parser Covington (2001) defines a non-projective dependency parser, and a projective variant called Algorithm LSUP (for List-based Search with Uniqueness and Projectivity). Unfortunately, the algorithm presented in Covington (2001) is not complete: It does not parse all projective dependency structures, because when creating leftward links it assumes that the head of a node i must be a reflexive-transitive head of the node i 1, which is not always the case. For instance, the structure shown in Figure 5 cannot be parsed because the constraints imposed by the algorithm prevent it from finding the head of 4. The MaltPa</context>
<context position="62851" citStr="Covington (2001" startWordPosition="11048" endWordPosition="11049">h1, . . . , hk ] [i + 1, \x0ch1, . . . , hk, i + 1 ] L-LINK: [i, \x0ch1, . . . , hk, i ] [i, \x0ch1, . . . , hk ] (wi, i) (wj, j)(j &amp;lt; i) Final items: The set of final items is {[n, \x0ch ] |1 h n}, the set of items containing a forest with a single dependency tree T headed at an arbitrary position h of the string, and whose yield spans the whole input string. The time complexity of the algorithm is exponential in the input length n. Note that this parsing schema is not correct, because Covingtons algorithm does not prevent the generation of cycles in the dependency graphs it produces. Quoting Covington (2001, page 99), Because the parser operates one word at a time, unity can only be checked at the end of the whole process: did it produce a tree with a single root that comprises all of the words? Therefore, a postprocessing mechanism is needed to determine which of the generated structures are, in fact, valid trees. In the parsing schema, this is reflected by the fact that the schema is complete but not sound. Nivre (2007) uses a variant of this algorithm in which cycle detection is used to avoid generating incorrect structures. Other non-projective parsers not covered here can also be represente</context>
</contexts>
<marker>Covington, 2001</marker>
<rawString>Covington, Michael A. 2001. A fundamental algorithm for dependency parsing. In Proceedings of the 39th Annual ACM Southeast Conference, pages 95102, Athens, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Renxu Sun</author>
<author>Keya Li</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Question answering passage retrieval using dependency relations.</title>
<date>2005</date>
<booktitle>In SIGIR 05: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>400407</pages>
<location>Salvador.</location>
<contexts>
<context position="1677" citStr="Cui et al. 2005" startWordPosition="233" endWordPosition="236">applied to Link Grammar, a dependency-related formalism. 1. Introduction Dependency parsing involves finding the structure of a sentence as expressed by a set of directed links (called dependencies) between individual words. Dependency formalisms have attracted considerable interest in recent years, having been successfully applied to tasks such as machine translation (Ding and Palmer 2005; Shen, Xu, and Weischedel 2008), textual entailment recognition (Herrera, Penas, and Verdejo 2005), relation extraction (Culotta and Sorensen 2004; Fundel, Kuffner, and Zimmer 2006), and question answering (Cui et al. 2005). Key characteristics of the dependency parsing approach are that dependency structures specify headmodifier and headcomplement relationships, which form the basis of predicateargument structure, but are not represented explicitly in constituency trees; there is no need for dependency parsers to postulate the existence of non-lexical nodes; and some variants of dependency parsers Facultade de Informatica, Universidade da Coruna Campus de Elvina, s/n, 15071 A Coruna, Spain. E-mail: cgomezr@udc.es. School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK. E-mail: J.A.Carroll@sus</context>
</contexts>
<marker>Cui, Sun, Li, Kan, Chua, 2005</marker>
<rawString>Cui, Hang, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng Chua. 2005. Question answering passage retrieval using dependency relations. In SIGIR 05: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 400407, Salvador.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In ACL 04: Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423429</pages>
<location>Barcelona.</location>
<contexts>
<context position="1600" citStr="Culotta and Sorensen 2004" startWordPosition="221" endWordPosition="224"> k) in time O(n4+3k ). Finally, we illustrate how the parsing schema framework can be applied to Link Grammar, a dependency-related formalism. 1. Introduction Dependency parsing involves finding the structure of a sentence as expressed by a set of directed links (called dependencies) between individual words. Dependency formalisms have attracted considerable interest in recent years, having been successfully applied to tasks such as machine translation (Ding and Palmer 2005; Shen, Xu, and Weischedel 2008), textual entailment recognition (Herrera, Penas, and Verdejo 2005), relation extraction (Culotta and Sorensen 2004; Fundel, Kuffner, and Zimmer 2006), and question answering (Cui et al. 2005). Key characteristics of the dependency parsing approach are that dependency structures specify headmodifier and headcomplement relationships, which form the basis of predicateargument structure, but are not represented explicitly in constituency trees; there is no need for dependency parsers to postulate the existence of non-lexical nodes; and some variants of dependency parsers Facultade de Informatica, Universidade da Coruna Campus de Elvina, s/n, 15071 A Coruna, Spain. E-mail: cgomezr@udc.es. School of Informatics</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Culotta, Aron and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In ACL 04: Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 423429, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In ACL 05: Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>541548</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="1453" citStr="Ding and Palmer 2005" startWordPosition="202" endWordPosition="205"> includes all gap degree k structures present in several natural language treebanks (which we call mildly ill-nested structures for gap degree k) in time O(n4+3k ). Finally, we illustrate how the parsing schema framework can be applied to Link Grammar, a dependency-related formalism. 1. Introduction Dependency parsing involves finding the structure of a sentence as expressed by a set of directed links (called dependencies) between individual words. Dependency formalisms have attracted considerable interest in recent years, having been successfully applied to tasks such as machine translation (Ding and Palmer 2005; Shen, Xu, and Weischedel 2008), textual entailment recognition (Herrera, Penas, and Verdejo 2005), relation extraction (Culotta and Sorensen 2004; Fundel, Kuffner, and Zimmer 2006), and question answering (Cui et al. 2005). Key characteristics of the dependency parsing approach are that dependency structures specify headmodifier and headcomplement relationships, which form the basis of predicateargument structure, but are not represented explicitly in constituency trees; there is no need for dependency parsers to postulate the existence of non-lexical nodes; and some variants of dependency p</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Ding, Yuan and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In ACL 05: Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 541548, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saso Dzeroski</author>
</authors>
<title>Tomaz Erjavec, Nina Ledinek, Petr Pajas, Zdenek Zabokrtsky, and Andreja Zele.</title>
<date>2006</date>
<marker>Dzeroski, 2006</marker>
<rawString>Dzeroski, Saso, Tomaz Erjavec, Nina Ledinek, Petr Pajas, Zdenek Zabokrtsky, and Andreja Zele. 2006. Towards a Slovene dependency treebank.</rawString>
</citation>
<citation valid="true">
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>13881391</pages>
<location>Genoa.</location>
<contexts>
<context position="56741" citStr="[2006]" startWordPosition="9889" endWordPosition="9889"> are obtained by removing the constraint |j i |d from this set (this restriction corresponds to the maximum stack depth to which dependency links can be created). Final items: The set of final items is {[0, n + 1]}. Although similar to the final item set for Yamada and Matsumotos parser, they differ in that an Attardi item of the form [0, n + 1] may contain forests with non-projective dependency trees. Given the number of indices manipulated in the schema, a nondeterministic implementation of Attd has exponential complexity with respect to input length (though in the implementation of Attardi [2006], control structures determinize the algorithm). Soundness of the algorithm Att is shown as in the previous algorithms, and completeness can be shown by reasoning that every coherent final item [0, n + 1] can be obtained by first performing n + 1 INITTER steps to obtain items [i, i + 1] for each 0 i n, then using n COMBINERs to join all of these items into [0, 1, . . . , n, n + 1], and then performing the LINK steps corresponding to the links in a tree contained in [0, n + 1] to obtain this final item. The algorithm Attd where d is finite is not correct with respect to the set of non-projectiv</context>
</contexts>
<marker>2006</marker>
<rawString>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006), pages 13881391, Genoa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="29271" citStr="Earley 1970" startWordPosition="4922" endWordPosition="4923">requires less bookkeeping than the other parsers described here. 3.5 Lombardo and Lesmo (1996) and Other Earley-Based Parsers The algorithms presented so far are based on making individual decisions about dependency links, represented by D-rules. Other parsers, such as that of Lombardo and Lesmo (1996), use grammars with CFG-like rules which encode the preferred order of dependents for each given governor. For example, a rule of the form N(Det PP) is used to allow N to have Det as left dependent and PP as right dependent. The algorithm by Lombardo and Lesmo is a version of Earleys CFG parser (Earley 1970) that uses Gaifmans dependency grammar (Gaifman 1965). As this algorithm predicts dependency relations before building them, item sets contain extended dependency trees, trees that have two kinds of nodes: preterminal nodes and terminal nodes. Depending on whether all the preterminal nodes have been linked to terminals, extended dependency trees can either be grounded, in which case they are isomorphic to traditional dependency graphs (see Figure 3), or ungrounded, as in Figure 4, in which case they capture parser states in which some structure has been predicted, but not yet found. Note that </context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational \x0cComputational Linguistics Volume 37, Number 3 Linguistics (COLING-96),</booktitle>
<pages>340345</pages>
<location>Copenhagen.</location>
<contexts>
<context position="7351" citStr="Eisner 1996" startWordPosition="1072" endWordPosition="1073">parsing schemata are used to define grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley PREDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classifiers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we use the notion of D-rules (Covington 1990). D-rules have the form (a, i) (b, j), which specifies that a word b located at position j in the input string can have the word a in position i as a dependent. Deduction steps i</context>
<context position="9915" citStr="Eisner (1996)" startWordPosition="1467" endWordPosition="1468">de general enough to include these parsers by using a novel way of representing intermediate states of dependency parsers based on a form of dependency trees that include nodes labelled with preterminals and terminals (Gomez-Rodrguez, Carroll, and Weir 2008; Gomez-Rodrguez 2009). For simplicity of presentation, we will only use this representation (called extended dependency trees) in the grammar-based algorithms that need it, and we will define the formalism and the rest of the algorithms with simple dependency trees. Some existing dependency parsing algorithms, for example, the algorithm of Eisner (1996), involve steps that connect spans which can represent disconnected dependency graphs. Such spans cannot be represented by 543 \x0cComputational Linguistics Volume 37, Number 3 a single dependency tree. Therefore, our formalism allows items to be sets of forests of partial dependency trees, rather than sets of trees. We are now ready to define the concepts needed to specify item sets for dependency parsers. Definition 1 An interval (with endpoints i and j) is a set of natural numbers of the form [i..j] = {k | i k j}. We will use the notation i..j for the ordered list of the numbers in [i..j]. </context>
<context position="20956" citStr="Eisner (1996)" startWordPosition="3439" endWordPosition="3440">ent of the order and the way linking decisions are taken. Statistical models can be used to determine whether a step linking words a and b in positions i and ji.e., having (a, i) (b, j) as a side conditionis executed or not, and probabilities can be attached to items in order to assign different weights to different analyses of the sentence. The side conditions provide an explicit representation of the choice points where probabilistic decisions are made by the control mechanism that is executing the schema. The same principle applies to all D-rule-based parsers described in this article. 3.2 Eisner (1996) Based on the number of free variables used in deduction steps of Collinss parser, it is apparent that its time complexity is O(n5 ): There are O(n5 ) combinations of index values with which each of its LINK steps can be executed.5 This complexity arises because a parentless word (head) may appear in any position in the items generated by the parser; the complexity can be reduced to O(n3 ) by ensuring that parentless words only appear at the first or last position of an item. This is the idea behind the parser defined by Eisner (1996), which is still in wide use today (McDonald, Crammer, and P</context>
<context position="25273" citStr="Eisner (1996)" startWordPosition="4248" endWordPosition="4249">[i, i + 1, False, False] as hypotheses for this parsing schema, and not require an Initter step, but we prefer a standard set of hypotheses valid for all parsers as it facilitates more straightforward proofs of the relations between schemata. 548 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata where LINK steps create a dependency link between two dependency trees spanning adjacent segments of the input, and COMBINER steps join two overlapping trees by a graph union operation that does not create new links. COMBINER steps follow the same mechanism as those in the algorithm of Eisner (1996), and LINK steps work analogously to those of Collins (1996), so this schema can be seen as being intermediate between those two algorithms. These relationships will be formally described in Section 4. Final items: The set of final items is {[0, n, 0]}. By convention, parse trees have the beginning-of-sentence marker 0 as their head, as in the previous algorithm. When described for head automaton grammars (Eisner and Satta 1999), this algorithm appears to be more complex to understand and implement than the previous one, requiring four different kinds of items to keep track of the state of the</context>
<context position="42395" citStr="Eisner (1996)" startWordPosition="7302" endWordPosition="7303">g ways: r Static/dynamic filtering: P1 sf/df P2 if the item set of P2 is a subset of that of P1 and P2 allows a subset of the direct inferences in P1. Sikkel (1994) explains the distinction between static and dynamic filtering, which is not used here. r Item contraction: The inverse of item refinement: P1 ic P2 if P2 ir P1. r Step contraction: The inverse of step refinement: P1 sc P2 if P2 sr P1. Many of the parsing schemata described in Section 3 can be related (see Figure 6), but for space reasons we sketch proofs for only the more interesting cases. Theorem 1 Yamada and Matsumoto (2003) sr Eisner (1996). Proof 1 It is easy to see from the schema definitions that IYM03 IEis96. We must verify that every deduction step in the Yamada and Matsumoto (2003) schema can be emulated by a sequence of inferences in the Eisner (1996) schema. For the INITTER step this is trivial as the INITTERs of both parsers are equivalent. Expressing the R-LINK step of Yamada and Matsumotos parser in the notation used for Eisner items gives: R-Link [i, j, False, False] [j, k, False, False] [i, k, False, False] (wj, j) (wk, k) This can be emulated in Eisners parser by an R-LINK step followed by a COMBINESPANS step: [j, </context>
<context position="43633" citStr="Eisner (1996)" startWordPosition="7509" endWordPosition="7510">rue, False] (by R-LINK) [j, k, True, False], [i, j, False, False] [i, k, False, False] (by COMBINESPANS) Figure 6 Relating several well-known dependency parsers. Arrows pointing up correspond to generalization relations, while those pointing down correspond to filtering. The specific subtype of relation is shown in each arrows label, following the notation in Section 4. 555 \x0cComputational Linguistics Volume 37, Number 3 Symmetrically, the L-LINK step in Yamada and Matsumotos parser can be emulated by an L-LINK followed by a COMBINESPANS in Eisners. \x02 Theorem 2 Eisner and Satta (1999) sr Eisner (1996). Proof 2 Writing R-LINK in Eisner and Sattas parser in the notation used for Eisner items gives R-LINK: [i, j, False, True] [j + 1, k, True, False] [i, k, True, False] (wi, i) (wk, k) This inference can be emulated in Eisners parser as follows: [j, j + 1, False, False] (by INITTER) [i, j, False, True], [j, j + 1, False, False] [i, j + 1, False, False] (by COMBINESPANS) [i, j + 1, False, False], [j + 1, k, True, False] [i, k, False, False] (by COMBINESPANS) [i, k, False, False] [i, k, True, False] (by R-LINK) The proof corresponding to the L-LINK step is symmetric. As for the R-COMBINER and L-</context>
<context position="47687" citStr="Eisner (1996)" startWordPosition="8200" endWordPosition="8201">plete description can be found in Sikkel (1997), adapted for dependency parsing. Gomez-Rodrguez (2009) gives a more thorough explanation of these relations and schemata. 5. Proving Correctness Another use of the parsing schemata framework is that it is helpful in establishing the correctness of a parser. Furthermore, relations between schemata can be used to establish the correctness of one schema from that of related ones. In this section, we show that the schemata for Yamada and Matsumoto (2003) and Eisner and Satta (1999) are correct, and use this to prove the correctness of the schema for Eisner (1996). Theorem 3 The Eisner and Satta (1999) parsing schema is correct. Proof 3 To prove correctness, we must show both soundness and completeness. To verify soundness we need to check that every individual deduction step in the parser infers a coherent consequent item when applied to coherent antecedents (i.e., in this case, that steps always generate non-empty items that conform to the definition in Section 3.3). This is shown by checking that, given two antecedents of a deduction step that contain a tree licensed by a set of D-rules G, the consequent of the step also contains such a tree. The tr</context>
<context position="52189" citStr="Eisner (1996)" startWordPosition="9092" endWordPosition="9093">er the following trees: r V = the subtree of T1 rooted at j, r U1 = the tree obtained by removing V from T1, r U2 = the tree obtained by removing all nodes to the right of j from V, r U3 = the tree obtained by removing all nodes to the left of j from V. The forest {U1, U2} belongs to the coherent item [i, j], and {U3, T2} belongs to the coherent item [j, k]. From these two items, we can obtain [i, k] by using the L-LINK step. Symmetric reasoning can be applied if i has no right dependents but k has at least one left dependent, analogously to the case of the previous parser. \x02 Theorem 5 The Eisner (1996) parsing schema is correct. Proof 5 By using the previous proofs and the relationships between schemata established earlier, we show that the parser of Eisner (1996) is correct: Soundness is straightforward, and completeness can be shown by using the properties of other algorithms. Because the set of final items in the Eisner (1996) and Eisner and Satta (1999) schemata are the same, and the former is a step refinement of the latter, the completeness of Eisner and Sattas parser directly implies the completeness of Eisners parser. Alternatively, we can use Yamada and Matsumotos parser to prove t</context>
<context position="109031" citStr="Eisner 1996" startWordPosition="19528" endWordPosition="19529">le of an LG parsing schema, we describe the original LG parser by Sleator and Temperley (1991), and show how projective parsing schemata, such as those seen in Section 3, can be adapted to obtain new LG parsers. 8.1 Sleator and Temperleys LG Parser The LG parser of Sleator and Temperley (1991) is a dynamic programming algorithm that builds linkages topdown: A link between vi and vk is always added before links between vi and vj or between vj and vk, if i &amp;lt; j &amp;lt; k. This contrasts with many of the 577 \x0cComputational Linguistics Volume 37, Number 3 dependency parsers seen in previous sections (Eisner 1996; Eisner and Satta 1999; Yamada and Matsumoto 2003), which build dependency graphs bottomup. Item set: The item set for Sleator and Temperleys parser is ISlT = {[i, j, , , B, C] |0 i j n + 1 B, C {True, False} and , , , are strings of link labels} where an item [i, j, , , B, C] represents the set of partial linkages over the substring wi . . . wj of the input, wi is linked to words in that substring by links labelled and has right linking requirements unsatisfied, wj is linked to words in the substring by links labelled and has left linking requirements unsatisfied, B is True if and only if th</context>
<context position="113693" citStr="Eisner (1996)" startWordPosition="20453" endWordPosition="20454">B, True]}. Items of this form contain full valid linkages for the string w0 . . . wn, because having the second boolean flag set to True implies that their linkages for w0 . . . wn+1 have at most two connected components, and we have assumed that the word wn+1 cannot be linked to any other, so one of the components must link w0 . . . wn. 8.2 Adapting Projective Dependency Parsers to Link Grammar We now exploit similarities between LG linkages and projective dependency structures to adapt projective dependency parsers to the LG formalism. As an example we present an LG version of the parser by Eisner (1996), but the same principles can be applied to other parsers: schemata for LG versions of the parsers by Eisner and Satta (1999) and Yamada and Matsumoto (2003) can be found in Gomez-Rodrguez (2009). Item sets from dependency parsers are adapted to LG parsers by considering the forests contained in each dependency item. The corresponding LG items contain linkages with the same structure as these forests. For example, because each forest in an item of the form [i, j, False, False] in Eisners dependency parsing schema contains two trees Figure 10 An example of LG parsing with the schema for Sleator</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Eisner, Jason. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th International Conference on Computational \x0cComputational Linguistics Volume 37, Number 3 Linguistics (COLING-96), pages 340345, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Eric Goldlust</author>
<author>Noah A Smith</author>
</authors>
<title>Compiling comp ling: Weighted dynamic programming and the Dyna language.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP</booktitle>
<pages>281290</pages>
<location>Vancouver.</location>
<marker>Eisner, Goldlust, Smith, 2005</marker>
<rawString>Eisner, Jason, Eric Goldlust, and Noah A. Smith. 2005. Compiling comp ling: Weighted dynamic programming and the Dyna language. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP 2005), pages 281290, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL99),</booktitle>
<pages>457464</pages>
<location>College Park, MD.</location>
<contexts>
<context position="23920" citStr="Eisner and Satta (1999)" startWordPosition="4000" endWordPosition="4003">SPANS step is used to join two items that overlap at a single word, which must have a parent in only one of the items, so that the result of joining trees coming from both items (without creating any dependency link) is a well-formed dependency tree. Final items: The set of final items is {[0, n, False, True]}. Note that these items represent dependency trees rooted at the beginning-of-sentence marker 0, which acts as a dummy head for the sentence. In order for the algorithm to parse sentences correctly, we need to define D-rules to allow the real sentence head to be linked to the node 0. 3.3 Eisner and Satta (1999) Eisner and Satta (1999) define an O(n3 ) parser for split head automaton grammars which can be used for dependency parsing. This algorithm is conceptually simpler than Eisners (1996) algorithm, because it only uses items representing single dependency trees, avoiding items of the form [i, j, False, False]. Item set: The item set is IES99 = {[i, j, i] |0 i j n} {[i, j, j] |0 i j n}, where items are defined as in Collinss parsing schema. Deduction steps: The deduction steps for this parser are the following: R-LINK: [i, j, i] [j + 1, k, k] [i, k, k] (wi, i) (wk, k) L-LINK: [i, j, i] [j + 1, k, </context>
<context position="25705" citStr="Eisner and Satta 1999" startWordPosition="4316" endWordPosition="4319">, and COMBINER steps join two overlapping trees by a graph union operation that does not create new links. COMBINER steps follow the same mechanism as those in the algorithm of Eisner (1996), and LINK steps work analogously to those of Collins (1996), so this schema can be seen as being intermediate between those two algorithms. These relationships will be formally described in Section 4. Final items: The set of final items is {[0, n, 0]}. By convention, parse trees have the beginning-of-sentence marker 0 as their head, as in the previous algorithm. When described for head automaton grammars (Eisner and Satta 1999), this algorithm appears to be more complex to understand and implement than the previous one, requiring four different kinds of items to keep track of the state of the automata used by the grammars. However, this abstract representation of its underlying semantics reveals that this parsing strategy is, in fact, conceptually simpler for dependency parsing. 3.4 Yamada and Matsumoto (2003) Yamada and Matsumoto (2003) define a deterministic, shift-reduce dependency parser guided by support vector machines, which achieves over 90% dependency accuracy on Section 23 of the Wall Street Journal Penn T</context>
<context position="43616" citStr="Eisner and Satta (1999)" startWordPosition="7504" endWordPosition="7507">, k, False, False] [j, k, True, False] (by R-LINK) [j, k, True, False], [i, j, False, False] [i, k, False, False] (by COMBINESPANS) Figure 6 Relating several well-known dependency parsers. Arrows pointing up correspond to generalization relations, while those pointing down correspond to filtering. The specific subtype of relation is shown in each arrows label, following the notation in Section 4. 555 \x0cComputational Linguistics Volume 37, Number 3 Symmetrically, the L-LINK step in Yamada and Matsumotos parser can be emulated by an L-LINK followed by a COMBINESPANS in Eisners. \x02 Theorem 2 Eisner and Satta (1999) sr Eisner (1996). Proof 2 Writing R-LINK in Eisner and Sattas parser in the notation used for Eisner items gives R-LINK: [i, j, False, True] [j + 1, k, True, False] [i, k, True, False] (wi, i) (wk, k) This inference can be emulated in Eisners parser as follows: [j, j + 1, False, False] (by INITTER) [i, j, False, True], [j, j + 1, False, False] [i, j + 1, False, False] (by COMBINESPANS) [i, j + 1, False, False], [j + 1, k, True, False] [i, k, False, False] (by COMBINESPANS) [i, k, False, False] [i, k, True, False] (by R-LINK) The proof corresponding to the L-LINK step is symmetric. As for the </context>
<context position="47604" citStr="Eisner and Satta (1999)" startWordPosition="8183" endWordPosition="8186">4, 1997). These dependency parsing schemata are versions of the homonymous schemata whose complete description can be found in Sikkel (1997), adapted for dependency parsing. Gomez-Rodrguez (2009) gives a more thorough explanation of these relations and schemata. 5. Proving Correctness Another use of the parsing schemata framework is that it is helpful in establishing the correctness of a parser. Furthermore, relations between schemata can be used to establish the correctness of one schema from that of related ones. In this section, we show that the schemata for Yamada and Matsumoto (2003) and Eisner and Satta (1999) are correct, and use this to prove the correctness of the schema for Eisner (1996). Theorem 3 The Eisner and Satta (1999) parsing schema is correct. Proof 3 To prove correctness, we must show both soundness and completeness. To verify soundness we need to check that every individual deduction step in the parser infers a coherent consequent item when applied to coherent antecedents (i.e., in this case, that steps always generate non-empty items that conform to the definition in Section 3.3). This is shown by checking that, given two antecedents of a deduction step that contain a tree licensed </context>
<context position="52551" citStr="Eisner and Satta (1999)" startWordPosition="9149" endWordPosition="9152"> [j, k]. From these two items, we can obtain [i, k] by using the L-LINK step. Symmetric reasoning can be applied if i has no right dependents but k has at least one left dependent, analogously to the case of the previous parser. \x02 Theorem 5 The Eisner (1996) parsing schema is correct. Proof 5 By using the previous proofs and the relationships between schemata established earlier, we show that the parser of Eisner (1996) is correct: Soundness is straightforward, and completeness can be shown by using the properties of other algorithms. Because the set of final items in the Eisner (1996) and Eisner and Satta (1999) schemata are the same, and the former is a step refinement of the latter, the completeness of Eisner and Sattas parser directly implies the completeness of Eisners parser. Alternatively, we can use Yamada and Matsumotos parser to prove the correctness of Eisners parser if we 558 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata redefine the set of final items in the latter to be items of the form [0, n + 1, False, False], which are equally valid as final items since they always contain parse trees. This idea can be applied to transfer proofs of completeness across any refineme</context>
<context position="109054" citStr="Eisner and Satta 1999" startWordPosition="19530" endWordPosition="19533">arsing schema, we describe the original LG parser by Sleator and Temperley (1991), and show how projective parsing schemata, such as those seen in Section 3, can be adapted to obtain new LG parsers. 8.1 Sleator and Temperleys LG Parser The LG parser of Sleator and Temperley (1991) is a dynamic programming algorithm that builds linkages topdown: A link between vi and vk is always added before links between vi and vj or between vj and vk, if i &amp;lt; j &amp;lt; k. This contrasts with many of the 577 \x0cComputational Linguistics Volume 37, Number 3 dependency parsers seen in previous sections (Eisner 1996; Eisner and Satta 1999; Yamada and Matsumoto 2003), which build dependency graphs bottomup. Item set: The item set for Sleator and Temperleys parser is ISlT = {[i, j, , , B, C] |0 i j n + 1 B, C {True, False} and , , , are strings of link labels} where an item [i, j, , , B, C] represents the set of partial linkages over the substring wi . . . wj of the input, wi is linked to words in that substring by links labelled and has right linking requirements unsatisfied, wj is linked to words in the substring by links labelled and has left linking requirements unsatisfied, B is True if and only if there is a direct link be</context>
<context position="113818" citStr="Eisner and Satta (1999)" startWordPosition="20473" endWordPosition="20476">lean flag set to True implies that their linkages for w0 . . . wn+1 have at most two connected components, and we have assumed that the word wn+1 cannot be linked to any other, so one of the components must link w0 . . . wn. 8.2 Adapting Projective Dependency Parsers to Link Grammar We now exploit similarities between LG linkages and projective dependency structures to adapt projective dependency parsers to the LG formalism. As an example we present an LG version of the parser by Eisner (1996), but the same principles can be applied to other parsers: schemata for LG versions of the parsers by Eisner and Satta (1999) and Yamada and Matsumoto (2003) can be found in Gomez-Rodrguez (2009). Item sets from dependency parsers are adapted to LG parsers by considering the forests contained in each dependency item. The corresponding LG items contain linkages with the same structure as these forests. For example, because each forest in an item of the form [i, j, False, False] in Eisners dependency parsing schema contains two trees Figure 10 An example of LG parsing with the schema for Sleator and Temperleys parser. 579 \x0cComputational Linguistics Volume 37, Number 3 headed at the words wi and wj, the analogous it</context>
<context position="119486" citStr="Eisner and Satta (1999)" startWordPosition="21495" endWordPosition="21498">in the schema for Eisners dependency parser, with the exception that the LINK step is able to build links on items that contain fully connected linkages (equivalent to the [i, j, True, False] and [i, j, False, True] items of the dependency parser). A version of the parser restricted to acyclic linkages can be obtained by adding the constraint that B must equal False in the LINK step. Final items: The set of final items is {[0, n, , , \x08, \x08, True]}, corresponding to the set of items containing fully connected linkages for the whole input string. LG parsing schemata based on the parsers of Eisner and Satta (1999) and Yamada and Matsumoto (2003) are not shown here for space reasons, but are presented by Gomez-Rodrguez (2009). The relationships between these three LG parsing schemata are the same as the corresponding dependency parsing schemata, that is, the LG variants of Eisner and Sattas and Yamada and Matsumotos dependency parsers are step contractions of the LG variant of Eisners parser. As with the algorithm of Sleator and Temperley, these bottomup LG parsers run in cubic time with respect to input length. 9. Conclusions and Future Work The parsing schemata formalism of Sikkel (1997) has previousl</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Eisner, Jason and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL99), pages 457464, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>A faster parsing algorithm for lexicalized tree-adjoining grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the 5th Workshop on Tree-Adjoining Grammars and Related Formalisms (TAG+5),</booktitle>
<pages>1419</pages>
<location>Paris.</location>
<contexts>
<context position="66628" citStr="Eisner and Satta 2000" startWordPosition="11614" endWordPosition="11617">ms. Developing efficient dependency parsing strategies for these sets of structures has considerable practical interest, in particular, making it possible to parse directly with dependencies in a data-driven manner rather than indirectly by constructing intermediate constituency grammars and extracting dependencies from constituency parses. In this section, we make four contributions to this enterprise. Firstly, we define a parser for well-nested structures of gap degree 1, and prove its correctness. The parser runs in time O(n7 ), the same complexity as the best existing algorithms for LTAG (Eisner and Satta 2000), and can be optimized to O(n6 ) in the nonlexicalized case. Secondly, we generalize our algorithm to any well-nested dependency structure with gap degree at most k, resulting in an algorithm with time complexity O(n5+2k ). Thirdly, we generalize the previous parsers in order to include ill-nested structures with gap degree at most k satisfying certain constraints, giving a parser that runs in time O(n4+3k ). Note that parsing unrestricted ill-nested structures, even when the gap degree is bounded, is NP-complete: These structures are equivalent to LCFRS for which the recognition problem is NP</context>
<context position="81910" citStr="Eisner and Satta 2000" startWordPosition="14460" endWordPosition="14463">at, for each of this cases, we can find a sequence of COMBINE steps to infer the item corresponding to T from smaller coherent items. \x02 7.3 Computational Complexity of WG1 The time complexity of WG1 is O(n7 ), as the step COMBINE SHRINKING GAP CENTRE works with seven free string positions. This complexity with respect to the length of the 568 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata input is as expected for this set of structures, because Kuhlmann (2010) shows that they are equivalent to LTAG, and the best existing parsers for this formalism also perform in O(n7 ) (Eisner and Satta 2000).9 Note that the COMBINE step that is the bottleneck only uses seven indices, not any additional entities such as D-rules. Hence, the O(n7 ) complexity does not involve additional factors relating to grammar size. Given unlexicalized D-rules specifying the possibility of dependencies between pairs of categories rather than pairs of words, a variant of this parser can be constructed with time complexity O(n6 ), as with parsers for unlexicalized TAG. We expand the item set with unlexicalized items of the form [i, j, C, l, r], where C is a category, distinct from the existing items [i, j, h, l, r</context>
<context position="84821" citStr="Eisner and Satta (2000)" startWordPosition="15007" endWordPosition="15010">at \x06h\x07 = {h} ([i..j] \\ \x02g p=1[lp..rp]), where each interval [lp..rp] is called a gap. The constraints h \x10= j, h \x10= i + 1, h \x10= lp 1, h \x10= rp are added to avoid redundancy, and normalization is defined as in WG1. Final items: The set of final items is defined as the set F = {[1, n, h, \x0c ] |h [1..n]}. Note that this set is the same as in WG1, as these are the items that we denoted [1, n, h, \x12, \x12] in that parser. 9 Although standard TAG parsing algorithms run in time O(n6) with respect to the input length, they also have a complexity factor related to grammar size. Eisner and Satta (2000) show that, in the case of lexicalized TAG, this factor is a function of the input length n; hence the additional complexity. 569 \x0cComputational Linguistics Volume 37, Number 3 Deduction steps: The parser has the following deduction steps: LINK: [h1, h1, h1, \x0c ] [i2, j2, h2, \x0c(l1, r1), . . . , (lg, rg) ] [i2, j2, h1, \x0c(l1, r1), . . . , (lg, rg) ] (wh2 , h2) (wh1 , h1) such that h2 [i2..j2] \\ g \x03 p=1 [lp..rp] h1 / [i2..j2] \\ g \x03 p=1 [lp..rp] COMBINE OPENING GAP: [i, lq 1, h, \x0c(l1, r1), . . . , (lq1, rq1) ] [rq + 1, m, h, \x0c(lq+1, rq+1), . . . , (lg, rg) ] [i, m, h, \x0c</context>
</contexts>
<marker>Eisner, Satta, 2000</marker>
<rawString>Eisner, Jason and Giorgio Satta. 2000. A faster parsing algorithm for lexicalized tree-adjoining grammars. In Proceedings of the 5th Workshop on Tree-Adjoining Grammars and Related Formalisms (TAG+5), pages 1419, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Fundel</author>
<author>Robert Kuffner</author>
<author>Ralf Zimmer</author>
</authors>
<title>RelExRelation extraction using dependency parse trees.</title>
<date>2006</date>
<journal>Bioinformatics,</journal>
<volume>23</volume>
<issue>3</issue>
<marker>Fundel, Kuffner, Zimmer, 2006</marker>
<rawString>Fundel, Katrin, Robert Kuffner, and Ralf Zimmer. 2006. RelExRelation extraction using dependency parse trees. Bioinformatics, 23(3):365371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haim Gaifman</author>
</authors>
<title>Dependency systems and phrase-structure systems.</title>
<date>1965</date>
<journal>Information and Control,</journal>
<pages>8--304337</pages>
<contexts>
<context position="7305" citStr="Gaifman (1965)" startWordPosition="1065" endWordPosition="1066">sing a number of preliminary issues. Traditional parsing schemata are used to define grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley PREDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classifiers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we use the notion of D-rules (Covington 1990). D-rules have the form (a, i) (b, j), which specifies that a word b located at position j in the input string can have the word a i</context>
<context position="29324" citStr="Gaifman 1965" startWordPosition="4929" endWordPosition="4930">cribed here. 3.5 Lombardo and Lesmo (1996) and Other Earley-Based Parsers The algorithms presented so far are based on making individual decisions about dependency links, represented by D-rules. Other parsers, such as that of Lombardo and Lesmo (1996), use grammars with CFG-like rules which encode the preferred order of dependents for each given governor. For example, a rule of the form N(Det PP) is used to allow N to have Det as left dependent and PP as right dependent. The algorithm by Lombardo and Lesmo is a version of Earleys CFG parser (Earley 1970) that uses Gaifmans dependency grammar (Gaifman 1965). As this algorithm predicts dependency relations before building them, item sets contain extended dependency trees, trees that have two kinds of nodes: preterminal nodes and terminal nodes. Depending on whether all the preterminal nodes have been linked to terminals, extended dependency trees can either be grounded, in which case they are isomorphic to traditional dependency graphs (see Figure 3), or ungrounded, as in Figure 4, in which case they capture parser states in which some structure has been predicted, but not yet found. Note that a dependency graph can always be extracted from such </context>
</contexts>
<marker>Gaifman, 1965</marker>
<rawString>Gaifman, Haim. 1965. Dependency systems and phrase-structure systems. Information and Control, 8:304337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Gomez-Rodrguez</author>
</authors>
<title>Parsing Schemata for Practical Text Analysis.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Universidade da Coruna,</institution>
<contexts>
<context position="9581" citStr="Gomez-Rodrguez 2009" startWordPosition="1417" endWordPosition="1418">tems for dependency parsers can be defined using partial dependency trees. However, dependency trees cannot express the fact that a particular structure has been predicted, but not yet built; this is required for grammar-based algorithms such as those of Lombardo and Lesmo (1996) and Kahane, Nasr, and Rambow (1998). The formalism can be made general enough to include these parsers by using a novel way of representing intermediate states of dependency parsers based on a form of dependency trees that include nodes labelled with preterminals and terminals (Gomez-Rodrguez, Carroll, and Weir 2008; Gomez-Rodrguez 2009). For simplicity of presentation, we will only use this representation (called extended dependency trees) in the grammar-based algorithms that need it, and we will define the formalism and the rest of the algorithms with simple dependency trees. Some existing dependency parsing algorithms, for example, the algorithm of Eisner (1996), involve steps that connect spans which can represent disconnected dependency graphs. Such spans cannot be represented by 543 \x0cComputational Linguistics Volume 37, Number 3 a single dependency tree. Therefore, our formalism allows items to be sets of forests of </context>
<context position="47176" citStr="Gomez-Rodrguez (2009)" startWordPosition="8116" endWordPosition="8117">arser by Lombardo 556 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata and Lesmo (1996). The proofs for these relations are the same as those given by Sikkel (1994), except that the dependency variants of each algorithm are simpler (due to the absence of epsilon rules and the fact that the rules are lexicalized). The names used for schemata dVH1, dVH2, dVH3, and buLC shown in Figure 6 come from Sikkel (1994, 1997). These dependency parsing schemata are versions of the homonymous schemata whose complete description can be found in Sikkel (1997), adapted for dependency parsing. Gomez-Rodrguez (2009) gives a more thorough explanation of these relations and schemata. 5. Proving Correctness Another use of the parsing schemata framework is that it is helpful in establishing the correctness of a parser. Furthermore, relations between schemata can be used to establish the correctness of one schema from that of related ones. In this section, we show that the schemata for Yamada and Matsumoto (2003) and Eisner and Satta (1999) are correct, and use this to prove the correctness of the schema for Eisner (1996). Theorem 3 The Eisner and Satta (1999) parsing schema is correct. Proof 3 To prove corre</context>
<context position="60983" citStr="Gomez-Rodrguez (2009)" startWordPosition="10667" endWordPosition="10668">res until it finds the solution. Instead, the algorithm works by using a greedy strategy to select a candidate set of edges for the spanning tree, potentially creating cycles and forming an illegal dependency tree. A cycle elimination procedure is iteratively applied to this graph until a legal dependency tree is obtained. It is still possible to express declarative aspects of the parser with a parsing schema, although the importance of the control mechanism in eliminating cycles makes this schema less informative than other cases we have considered, and we will not discuss it in detail here. Gomez-Rodrguez (2009) gives a complete description and discussion of the schema for the MST parser. 6.5 Covingtons (1990, 2001) Non-Projective Parser Covingtons non-projective parsing algorithm (Covington 1990, 2001) reads the input from left to right, establishing dependency links between the current word and previous words in the input. The parser maintains two lists: one with all the words encountered so far, and one with those that do not yet have a head assigned. A new word can be linked as a dependent of any of the words in the first list, and as a head of any of the words in the second list. The following p</context>
<context position="76351" citStr="Gomez-Rodrguez 2009" startWordPosition="13466" endWordPosition="13467">to make the linking decisions associated with the D-rules, as explained in Section 3.1. The definition of such statistical models for guiding the execution of schemata falls outside the scope of this article. 7.2 Proof of Correctness for WG1 We define a set of coherent items for the schema, in such a way that final items in this set satisfy the general definition of coherent final items; and then prove the stronger claims that all derivable items are coherent and all coherent items are derivable. The full correctness proof has previously been published (Gomez-Rodrguez, Weir, and Carroll 2008; Gomez-Rodrguez 2009), so for reasons of space we only sketch the proof here. To define the set of coherent items for WG1, we provide a definition of the trees that these items must contain. Let T be a well-nested dependency tree headed at a node h, with all its edges licensed by our set of D-rules. We call such a tree a properly formed tree for the algorithm WG1 if it satisfies the following conditions. 1. \x06h\x07 is either of the form {h} [i..j] or {h} ([i..j] \\ [l..r]). 2. All the nodes in T have gap degree at most 1 except for h, which can have gap degree up to 2. An item [i, j, h, l, r] IWG1 is coherent if</context>
<context position="97701" citStr="Gomez-Rodrguez 2009" startWordPosition="17413" endWordPosition="17414">ith respect to the length of the input. Note that this expression denotes the complexity with respect to n of the MGk parser obtained for a given k: Taking k to be a variable would add an additional O(33k ) complexity factor, because the number of different COMBINER steps that can be applied to a given item grows exponentially with k. 573 \x0cComputational Linguistics Volume 37, Number 3 7.9 Proof of Correctness for MGk As for previous parsers, we only show here a sketch of the proof that MGk is correct. The detailed proof has been published previously (Gomez-Rodrguez, Weir, and Carroll 2008; Gomez-Rodrguez 2009). Theorem 8 MGk is correct. Proof 8 As with WGk, we define the sets of properly formed trees and coherent items for this algorithm. Let T be a dependency tree headed at a node h. We call such a tree a properly formed tree for the algorithm MGk if it satisfies the following. 1. \x06h\x07 is of the form {h} ([i..j] \\ \x02g p=1[lp..rp]), with 0 g k. 2. There is a binarization of T such that all the nodes in it have gap degree at most k except for its root node, which can have gap degree up to k + 1. The sets of coherent and coherent final items are defined as in previous proofs. Soundness is sho</context>
<context position="113888" citStr="Gomez-Rodrguez (2009)" startWordPosition="20486" endWordPosition="20487"> at most two connected components, and we have assumed that the word wn+1 cannot be linked to any other, so one of the components must link w0 . . . wn. 8.2 Adapting Projective Dependency Parsers to Link Grammar We now exploit similarities between LG linkages and projective dependency structures to adapt projective dependency parsers to the LG formalism. As an example we present an LG version of the parser by Eisner (1996), but the same principles can be applied to other parsers: schemata for LG versions of the parsers by Eisner and Satta (1999) and Yamada and Matsumoto (2003) can be found in Gomez-Rodrguez (2009). Item sets from dependency parsers are adapted to LG parsers by considering the forests contained in each dependency item. The corresponding LG items contain linkages with the same structure as these forests. For example, because each forest in an item of the form [i, j, False, False] in Eisners dependency parsing schema contains two trees Figure 10 An example of LG parsing with the schema for Sleator and Temperleys parser. 579 \x0cComputational Linguistics Volume 37, Number 3 headed at the words wi and wj, the analogous item in the corresponding LG parsing schema will contain linkages with t</context>
<context position="119599" citStr="Gomez-Rodrguez (2009)" startWordPosition="21515" endWordPosition="21516">that contain fully connected linkages (equivalent to the [i, j, True, False] and [i, j, False, True] items of the dependency parser). A version of the parser restricted to acyclic linkages can be obtained by adding the constraint that B must equal False in the LINK step. Final items: The set of final items is {[0, n, , , \x08, \x08, True]}, corresponding to the set of items containing fully connected linkages for the whole input string. LG parsing schemata based on the parsers of Eisner and Satta (1999) and Yamada and Matsumoto (2003) are not shown here for space reasons, but are presented by Gomez-Rodrguez (2009). The relationships between these three LG parsing schemata are the same as the corresponding dependency parsing schemata, that is, the LG variants of Eisner and Sattas and Yamada and Matsumotos dependency parsers are step contractions of the LG variant of Eisners parser. As with the algorithm of Sleator and Temperley, these bottomup LG parsers run in cubic time with respect to input length. 9. Conclusions and Future Work The parsing schemata formalism of Sikkel (1997) has previously been used to define, analyze, and compare algorithms for constituency-based parsing. We have shown how to exten</context>
</contexts>
<marker>Gomez-Rodrguez, 2009</marker>
<rawString>Gomez-Rodrguez, Carlos. 2009. Parsing Schemata for Practical Text Analysis. Ph.D. thesis, Universidade da Coruna, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Gomez-Rodrguez</author>
<author>John Carroll</author>
<author>David Weir</author>
</authors>
<title>A deductive approach to dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL08:HLT),</booktitle>
<pages>968976</pages>
<location>Columbus, OH.</location>
<marker>Gomez-Rodrguez, Carroll, Weir, 2008</marker>
<rawString>Gomez-Rodrguez, Carlos, John Carroll, and David Weir. 2008. A deductive approach to dependency parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL08:HLT), pages 968976, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Gomez-Rodrguez</author>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
<author>David Weir</author>
</authors>
<title>Optimal reduction of rule length in linear context-free rewriting systems.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL HLT 2009: the Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>539547</pages>
<location>Boulder, CO.</location>
<contexts>
<context position="122312" citStr="Gomez-Rodrguez et al. 2009" startWordPosition="21930" endWordPosition="21933">et of structures that we call mildly ill-nested for a given gap degree k, and presented an algorithm that can parse these in time O(n3k+4 ). The practical relevance of this set of structures can be seen in the data obtained from several dependency treebanks, showing that all the sentences contained in them are mildly ill-nested for their gap degree, and thus they are parsable with this algorithm. The strategy used by this algorithm for parsing mildly ill-nested structures has been adapted to solve the problem of finding minimal fan-out binarizations of LCFRS to improve parsing efficiency (see Gomez-Rodrguez et al. 2009). An interesting line of future work would be to provide implementations of the mildly non-projective dependency parsers presented here, using probabilistic models to guide their linking decisions, and compare their practical performance and accuracy to those of other non-projective dependency parsers. Additionally, our definition of mildly ill-nested structures is closely related to the way the corresponding parser works. It would be interesting to find a more grammar-oriented definition that would provide linguistic insight into this set of structures. An alternative generalization of the co</context>
</contexts>
<marker>Gomez-Rodrguez, Kuhlmann, Satta, Weir, 2009</marker>
<rawString>Gomez-Rodrguez, Carlos, Marco Kuhlmann, Giorgio Satta, and David Weir. 2009. Optimal reduction of rule length in linear context-free rewriting systems. In Proceedings of NAACL HLT 2009: the Conference of the North American Chapter of the Association for Computational Linguistics, pages 539547, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Gomez-Rodrguez</author>
<author>Jesus Vilares</author>
<author>Miguel A Alonso</author>
</authors>
<title>A compiler for parsing schemata.</title>
<date>2009</date>
<journal>Software: Practice and Experience,</journal>
<volume>39</volume>
<issue>5</issue>
<contexts>
<context position="122312" citStr="Gomez-Rodrguez et al. 2009" startWordPosition="21930" endWordPosition="21933">et of structures that we call mildly ill-nested for a given gap degree k, and presented an algorithm that can parse these in time O(n3k+4 ). The practical relevance of this set of structures can be seen in the data obtained from several dependency treebanks, showing that all the sentences contained in them are mildly ill-nested for their gap degree, and thus they are parsable with this algorithm. The strategy used by this algorithm for parsing mildly ill-nested structures has been adapted to solve the problem of finding minimal fan-out binarizations of LCFRS to improve parsing efficiency (see Gomez-Rodrguez et al. 2009). An interesting line of future work would be to provide implementations of the mildly non-projective dependency parsers presented here, using probabilistic models to guide their linking decisions, and compare their practical performance and accuracy to those of other non-projective dependency parsers. Additionally, our definition of mildly ill-nested structures is closely related to the way the corresponding parser works. It would be interesting to find a more grammar-oriented definition that would provide linguistic insight into this set of structures. An alternative generalization of the co</context>
</contexts>
<marker>Gomez-Rodrguez, Vilares, Alonso, 2009</marker>
<rawString>Gomez-Rodrguez, Carlos, Jesus Vilares, and Miguel A. Alonso. 2009. A compiler for parsing schemata. Software: Practice and Experience, 39(5):441470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Gomez-Rodrguez</author>
<author>David Weir</author>
<author>John Carroll</author>
</authors>
<title>Parsing mildly non-projective dependency structures (extended version).</title>
<date>2008</date>
<tech>Technical Report CSRP 600,</tech>
<institution>Department of Informatics, University of Sussex.</institution>
<marker>Gomez-Rodrguez, Weir, Carroll, 2008</marker>
<rawString>Gomez-Rodrguez, Carlos, David Weir, and John Carroll. 2008. Parsing mildly non-projective dependency structures (extended version). Technical Report CSRP 600, Department of Informatics, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Gomez-Rodrguez</author>
<author>David Weir</author>
<author>John Carroll</author>
</authors>
<title>Parsing mildly non-projective dependency structures.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL-09),</booktitle>
<pages>291299</pages>
<location>Athens.</location>
<contexts>
<context position="122312" citStr="Gomez-Rodrguez et al. 2009" startWordPosition="21930" endWordPosition="21933">et of structures that we call mildly ill-nested for a given gap degree k, and presented an algorithm that can parse these in time O(n3k+4 ). The practical relevance of this set of structures can be seen in the data obtained from several dependency treebanks, showing that all the sentences contained in them are mildly ill-nested for their gap degree, and thus they are parsable with this algorithm. The strategy used by this algorithm for parsing mildly ill-nested structures has been adapted to solve the problem of finding minimal fan-out binarizations of LCFRS to improve parsing efficiency (see Gomez-Rodrguez et al. 2009). An interesting line of future work would be to provide implementations of the mildly non-projective dependency parsers presented here, using probabilistic models to guide their linking decisions, and compare their practical performance and accuracy to those of other non-projective dependency parsers. Additionally, our definition of mildly ill-nested structures is closely related to the way the corresponding parser works. It would be interesting to find a more grammar-oriented definition that would provide linguistic insight into this set of structures. An alternative generalization of the co</context>
</contexts>
<marker>Gomez-Rodrguez, Weir, Carroll, 2009</marker>
<rawString>Gomez-Rodrguez, Carlos, David Weir, and John Carroll. 2009. Parsing mildly non-projective dependency structures. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL-09), pages 291299, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajic</author>
<author>Jarmila Panevova</author>
<author>Eva Hajicova</author>
<author>Jarmila Panevova</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
<author>Jan Stepanek</author>
<author>Jir Havelka</author>
<author>Marie Mikulova</author>
</authors>
<date>2006</date>
<booktitle>Prague Dependency Treebank 2.0. CDROM CAT: LDC2006T01, ISBN</booktitle>
<pages>1--58563</pages>
<institution>Linguistic Data Consortium, University of Pennsylvania.</institution>
<contexts>
<context position="88875" citStr="Hajic et al. 2006" startWordPosition="15840" endWordPosition="15843">n line with what could be expected from previous research in constituency parsing: Kuhlmann (2010) shows that the set of well-nested dependency structures with gap degree at most k is closely related to coupled CFG in which the maximal rank of a nonterminal is k + 1. The constituency parser defined by Hotz and Pitsch (1996) for these grammars also adds an n2 factor for each unit increment of k. Note that a small value of k appears to be sufficient to account for the vast majority of the non-projective sentences found in natural language treebanks. For instance, the Prague Dependency Treebank (Hajic et al. 2006) contains no structures with gap degree greater than 4. Thus, a WG4 parser would be able to analyze all the well-nested structures in this treebank, which represent 99.89% of the total (WG1 would be able to parse 99.49%). Increasing k beyond 4 would not produce further improvements in coverage. 7.7 Parsing Ill-Nested Structures: MG1 and MGk The WGk parser analyzes dependency structures with bounded gap degree as long as they are well-nested. Although this covers the vast majority of the structures that occur in natural language treebanks (Kuhlmann and Nivre 2006), a significant number of sente</context>
<context position="102365" citStr="Hajic et al. 2006" startWordPosition="18234" endWordPosition="18237"> degree at most k in time O(n3k+4 ). 8. Link Grammar Schemata Link Grammar (LG), introduced by Sleator and Temperley (1991, 1993), is a theory of syntax whose structural representation of sentences is closely related to projective dependency representations, but with some important differences.11 Undirected links: Like dependency formalisms, LG represents the structure of sentences as a set of links between words. However, whereas dependency links are directed, the links used in LG are undirected: There is no distinction made between heads and dependents. 10 Arabic (Hajic et al. 2004), Czech (Hajic et al. 2006), Danish (Kromann 2003), Dutch (van der Beek et al. 2002), Latin (Bamman and Crane 2006), Portuguese (Afonso et al. 2002), Slovene (Dzeroski et al. 2006), Swedish (Nilsson, Hall, and Nivre 2005), and Turkish (Atalay, Oflazer, and Say 2003; Oflazer et al. 2003). 11 A complete treatment of LG is beyond the scope of this article: Schneider (1998) gives a detailed comparison of Link Grammar and dependency formalisms. 575 \x0cComputational Linguistics Volume 37, Number 3 Table 1 Counts of dependency structures in treebanks for several languages, classified by projectivity, gap degree, and mild and </context>
</contexts>
<marker>Hajic, Panevova, Hajicova, Panevova, Sgall, Pajas, Stepanek, Havelka, Mikulova, 2006</marker>
<rawString>Hajic, Jan, Jarmila Panevova, Eva Hajicova, Jarmila Panevova, Petr Sgall, Petr Pajas, Jan Stepanek, Jir Havelka, and Marie Mikulova. 2006. Prague Dependency Treebank 2.0. CDROM CAT: LDC2006T01, ISBN 1-58563-370-4. Linguistic Data Consortium, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajic</author>
<author>Otakar Smrz</author>
<author>Petr Zemanek</author>
<author>Jan Snaidauf</author>
<author>Emanuel Beska</author>
</authors>
<title>Prague Arabic dependency treebank: Development in data and tools.</title>
<date>2004</date>
<booktitle>In Proceedings of the NEMLAR International Conference on Arabic Language Resources and Tools,</booktitle>
<pages>110117</pages>
<location>Cairo.</location>
<contexts>
<context position="102338" citStr="Hajic et al. 2004" startWordPosition="18229" endWordPosition="18232">rse every sentence with gap degree at most k in time O(n3k+4 ). 8. Link Grammar Schemata Link Grammar (LG), introduced by Sleator and Temperley (1991, 1993), is a theory of syntax whose structural representation of sentences is closely related to projective dependency representations, but with some important differences.11 Undirected links: Like dependency formalisms, LG represents the structure of sentences as a set of links between words. However, whereas dependency links are directed, the links used in LG are undirected: There is no distinction made between heads and dependents. 10 Arabic (Hajic et al. 2004), Czech (Hajic et al. 2006), Danish (Kromann 2003), Dutch (van der Beek et al. 2002), Latin (Bamman and Crane 2006), Portuguese (Afonso et al. 2002), Slovene (Dzeroski et al. 2006), Swedish (Nilsson, Hall, and Nivre 2005), and Turkish (Atalay, Oflazer, and Say 2003; Oflazer et al. 2003). 11 A complete treatment of LG is beyond the scope of this article: Schneider (1998) gives a detailed comparison of Link Grammar and dependency formalisms. 575 \x0cComputational Linguistics Volume 37, Number 3 Table 1 Counts of dependency structures in treebanks for several languages, classified by projectivity</context>
</contexts>
<marker>Hajic, Smrz, Zemanek, Snaidauf, Beska, 2004</marker>
<rawString>Hajic, Jan, Otakar Smrz, Petr Zemanek, Jan Snaidauf, and Emanuel Beska. 2004. Prague Arabic dependency treebank: Development in data and tools. In Proceedings of the NEMLAR International Conference on Arabic Language Resources and Tools, pages 110117, Cairo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jir Havelka</author>
</authors>
<title>Beyond projectivity: Multilingual evaluation of constraints and measures on non-projective structures.</title>
<date>2007</date>
<booktitle>In ACL 2007: Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>608615</pages>
<location>Prague.</location>
<contexts>
<context position="64218" citStr="Havelka 2007" startWordPosition="11261" endWordPosition="11262">called regular dependency grammars. This deduction system can easily be converted into a parsing schema by associating adequate semantics with items. However, we do not show this here for space reasons, because we would first have to explain the formalism of regular dependency grammars. 7. Mildly Non-Projective Dependency Parsing For reasons of computational efficiency, many practical implementations of dependency parsing are restricted to projective structures. However, some natural language sentences appear to have non-projective syntactic structure, something that arises in many languages (Havelka 2007), and is particularly common in free word order languages such as Czech. Parsing without the projectivity constraint is computationally complex: Although it is possible to parse non-projective structures in quadratic time with respect to input length under a model in which each dependency decision is independent of all the others (as in the parser of McDonald et al. [2005], discussed in Section 6.4), the problem is intractable in the absence of this assumption (McDonald and Satta 2007). Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice contai</context>
</contexts>
<marker>Havelka, 2007</marker>
<rawString>Havelka, Jir. 2007. Beyond projectivity: Multilingual evaluation of constraints and measures on non-projective structures. In ACL 2007: Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 608615, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hays</author>
</authors>
<title>Dependency theory: a formalism and some observations.</title>
<date>1964</date>
<journal>Language,</journal>
<pages>40--511525</pages>
<contexts>
<context position="7286" citStr="Hays (1964)" startWordPosition="1062" endWordPosition="1063"> begin by addressing a number of preliminary issues. Traditional parsing schemata are used to define grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley PREDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classifiers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we use the notion of D-rules (Covington 1990). D-rules have the form (a, i) (b, j), which specifies that a word b located at position j in the input string ca</context>
</contexts>
<marker>Hays, 1964</marker>
<rawString>Hays, David. 1964. Dependency theory: a formalism and some observations. Language, 40:511525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesus Herrera</author>
<author>Anselmo Penas</author>
<author>Felisa Verdejo</author>
</authors>
<title>Textual entailment recognition based on dependency analysis and WordNet.</title>
<date>2005</date>
<booktitle>Machine Learning Challenges. Lecture Notes in Computer Science,</booktitle>
<volume>3944</volume>
<pages>231239</pages>
<editor>In J. Quinonero-Camdela, I. Dagan, B. Magnini, and F. dAlche-Buc, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>BerlinHeidelberg-New York,</location>
<marker>Herrera, Penas, Verdejo, 2005</marker>
<rawString>Herrera, Jesus, Anselmo Penas, and Felisa Verdejo. 2005. Textual entailment recognition based on dependency analysis and WordNet. In J. Quinonero-Camdela, I. Dagan, B. Magnini, and F. dAlche-Buc, editors, Machine Learning Challenges. Lecture Notes in Computer Science, vol. 3944. Springer-Verlag, BerlinHeidelberg-New York, pages 231239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunter Hotz</author>
<author>Gisela Pitsch</author>
</authors>
<title>On parsing coupled-context-free languages.</title>
<date>1996</date>
<journal>Theoretical Computer Science,</journal>
<pages>161--1</pages>
<contexts>
<context position="65574" citStr="Hotz and Pitsch 1996" startWordPosition="11462" endWordPosition="11465"> structures (Kuhlmann and Nivre 2006; Havelka 2007). Kuhlmann (2010) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky, Kuhlmann, and Mohl 2005), relating them to lexicalized constituency grammar formalisms. Specifically, Kuhlmann shows that linear context-free rewriting systems 562 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata (LCFRS) with fan-out k (Vijay-Shanker, Weir, and Joshi 1987; Satta 1992) induce the set of dependency structures with gap degree at most k 1; coupled CFG in which the maximal rank of a nonterminal is k (Hotz and Pitsch 1996) induces the set of wellnested dependency structures with gap degree at most k 1; and finally, LTAG (Joshi and Schabes 1997) induces the set of well-nested dependency structures with gap degree at most 1. These results establish that there are polynomial-time dependency parsing algorithms for well-nested structures with bounded gap degree, because such parsers exist for their corresponding lexicalized constituency-based formalisms. Developing efficient dependency parsing strategies for these sets of structures has considerable practical interest, in particular, making it possible to parse dire</context>
<context position="88582" citStr="Hotz and Pitsch (1996)" startWordPosition="15790" endWordPosition="15793">reater than 1, so we must consider more cases in this part of the proof. 7.6 Computational Complexity of WGk The WGk parser runs in time O(n5+2k ). As in the case of WG1, the step with most free variables is COMBINE SHRINKING GAP CENTRE with 5 + 2k free indices. Again, this complexity result is in line with what could be expected from previous research in constituency parsing: Kuhlmann (2010) shows that the set of well-nested dependency structures with gap degree at most k is closely related to coupled CFG in which the maximal rank of a nonterminal is k + 1. The constituency parser defined by Hotz and Pitsch (1996) for these grammars also adds an n2 factor for each unit increment of k. Note that a small value of k appears to be sufficient to account for the vast majority of the non-projective sentences found in natural language treebanks. For instance, the Prague Dependency Treebank (Hajic et al. 2006) contains no structures with gap degree greater than 4. Thus, a WG4 parser would be able to analyze all the well-nested structures in this treebank, which represent 99.89% of the total (WG1 would be able to parse 99.49%). Increasing k beyond 4 would not produce further improvements in coverage. 7.7 Parsing</context>
</contexts>
<marker>Hotz, Pitsch, 1996</marker>
<rawString>Hotz, Gunter and Gisela Pitsch. 1996. On parsing coupled-context-free languages. Theoretical Computer Science, 161(1-2):205233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Tree-adjoining grammars.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>69123</pages>
<editor>In G. Rozenberg and A. Salomaa, editors,</editor>
<publisher>Beyond Words, Springer-Verlag,</publisher>
<location>New York, NY,</location>
<contexts>
<context position="65698" citStr="Joshi and Schabes 1997" startWordPosition="11484" endWordPosition="11487">edness and gap degree constraints (Bodirsky, Kuhlmann, and Mohl 2005), relating them to lexicalized constituency grammar formalisms. Specifically, Kuhlmann shows that linear context-free rewriting systems 562 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata (LCFRS) with fan-out k (Vijay-Shanker, Weir, and Joshi 1987; Satta 1992) induce the set of dependency structures with gap degree at most k 1; coupled CFG in which the maximal rank of a nonterminal is k (Hotz and Pitsch 1996) induces the set of wellnested dependency structures with gap degree at most k 1; and finally, LTAG (Joshi and Schabes 1997) induces the set of well-nested dependency structures with gap degree at most 1. These results establish that there are polynomial-time dependency parsing algorithms for well-nested structures with bounded gap degree, because such parsers exist for their corresponding lexicalized constituency-based formalisms. Developing efficient dependency parsing strategies for these sets of structures has considerable practical interest, in particular, making it possible to parse directly with dependencies in a data-driven manner rather than indirectly by constructing intermediate constituency grammars and</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Joshi, Aravind K. and Yves Schabes. 1997. Tree-adjoining grammars. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages, vol. 3: Beyond Words, Springer-Verlag, New York, NY, pages 69123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Kahane</author>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
</authors>
<title>Pseudo-projectivity: A polynomially parsable non-projective dependency grammar.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics (COLING-ACL98),</booktitle>
<pages>646652</pages>
<location>San Francisco, CA.</location>
<marker>Kahane, Nasr, Rambow, 1998</marker>
<rawString>Kahane, Sylvain, Alexis Nasr, and Owen Rambow. 1998. Pseudo-projectivity: A polynomially parsable non-projective dependency grammar. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics (COLING-ACL98), pages 646652, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carroll \x0cGomez-Rodrguez</author>
</authors>
<title>and Weir Dependency Parsing Schemata Kasami, Tadao.</title>
<date>1965</date>
<location>Bedford, MA.</location>
<marker>\x0cGomez-Rodrguez, 1965</marker>
<rawString>\x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata Kasami, Tadao. 1965. An efficient recognition and syntax algorithm for context-free languages. Scientific Report AFCRL-65-758, Air Force Cambridge Research Lab, Bedford, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias T Kromann</author>
</authors>
<title>The Danish dependency treebank and the underlying linguistic theory.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2nd Workshop on Treebanks and Linguistic Theories (TLT),</booktitle>
<pages>217220</pages>
<contexts>
<context position="102388" citStr="Kromann 2003" startWordPosition="18239" endWordPosition="18240">n3k+4 ). 8. Link Grammar Schemata Link Grammar (LG), introduced by Sleator and Temperley (1991, 1993), is a theory of syntax whose structural representation of sentences is closely related to projective dependency representations, but with some important differences.11 Undirected links: Like dependency formalisms, LG represents the structure of sentences as a set of links between words. However, whereas dependency links are directed, the links used in LG are undirected: There is no distinction made between heads and dependents. 10 Arabic (Hajic et al. 2004), Czech (Hajic et al. 2006), Danish (Kromann 2003), Dutch (van der Beek et al. 2002), Latin (Bamman and Crane 2006), Portuguese (Afonso et al. 2002), Slovene (Dzeroski et al. 2006), Swedish (Nilsson, Hall, and Nivre 2005), and Turkish (Atalay, Oflazer, and Say 2003; Oflazer et al. 2003). 11 A complete treatment of LG is beyond the scope of this article: Schneider (1998) gives a detailed comparison of Link Grammar and dependency formalisms. 575 \x0cComputational Linguistics Volume 37, Number 3 Table 1 Counts of dependency structures in treebanks for several languages, classified by projectivity, gap degree, and mild and strong ill-nestedness (</context>
</contexts>
<marker>Kromann, 2003</marker>
<rawString>Kromann, Matthias T. 2003. The Danish dependency treebank and the underlying linguistic theory. In Proceedings of the 2nd Workshop on Treebanks and Linguistic Theories (TLT), pages 217220, Vaxjo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
</authors>
<title>Dependency Structures and Lexicalized Grammars.</title>
<date>2007</date>
<institution>D. Phil dissertation, Saarland University,</institution>
<location>Saarbrucken, Germany.</location>
<contexts>
<context position="120393" citStr="Kuhlmann (2007" startWordPosition="21640" endWordPosition="21641">and Matsumotos dependency parsers are step contractions of the LG variant of Eisners parser. As with the algorithm of Sleator and Temperley, these bottomup LG parsers run in cubic time with respect to input length. 9. Conclusions and Future Work The parsing schemata formalism of Sikkel (1997) has previously been used to define, analyze, and compare algorithms for constituency-based parsing. We have shown how to extend the formalism to dependency parsers, as well as the related Link Grammar formalism. Deductive approaches have been used in the past to describe individual dependency parsers: In Kuhlmann (2007, 2010) a grammatical deduction system was used to define a parser for regular dependency grammars. 581 \x0cComputational Linguistics Volume 37, Number 3 McDonald and Nivre (2007) give an alternative framework for dependency parsers, viewing them as transition systems. That model is based on parser configurations and transitions, and has no clear relationship to the approach described here. To demonstrate the theoretical uses of dependency parsing schemata, we have used them to describe a wide range of existing projective and non-projective dependency parsers. We have also clarified various re</context>
</contexts>
<marker>Kuhlmann, 2007</marker>
<rawString>Kuhlmann, Marco. 2007. Dependency Structures and Lexicalized Grammars. D. Phil dissertation, Saarland University, Saarbrucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
</authors>
<title>Dependency Structures and Lexicalized Grammars: An Algebraic Approach.</title>
<date>2010</date>
<journal>Lecture Notes in Computer Science,</journal>
<volume>6270</volume>
<publisher>Springer,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="63517" citStr="Kuhlmann (2010)" startWordPosition="11161" endWordPosition="11162"> a time, unity can only be checked at the end of the whole process: did it produce a tree with a single root that comprises all of the words? Therefore, a postprocessing mechanism is needed to determine which of the generated structures are, in fact, valid trees. In the parsing schema, this is reflected by the fact that the schema is complete but not sound. Nivre (2007) uses a variant of this algorithm in which cycle detection is used to avoid generating incorrect structures. Other non-projective parsers not covered here can also be represented under the parsing schema framework. For example, Kuhlmann (2010) presents a deduction system for a non-projective parser which uses a grammar formalism called regular dependency grammars. This deduction system can easily be converted into a parsing schema by associating adequate semantics with items. However, we do not show this here for space reasons, because we would first have to explain the formalism of regular dependency grammars. 7. Mildly Non-Projective Dependency Parsing For reasons of computational efficiency, many practical implementations of dependency parsing are restricted to projective structures. However, some natural language sentences appe</context>
<context position="65021" citStr="Kuhlmann (2010)" startWordPosition="11383" endWordPosition="11384">tive structures in quadratic time with respect to input length under a model in which each dependency decision is independent of all the others (as in the parser of McDonald et al. [2005], discussed in Section 6.4), the problem is intractable in the absence of this assumption (McDonald and Satta 2007). Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice contain only small proportions of non-projective arcs. This has led to the study of sub-classes of the class of all non-projective dependency structures (Kuhlmann and Nivre 2006; Havelka 2007). Kuhlmann (2010) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky, Kuhlmann, and Mohl 2005), relating them to lexicalized constituency grammar formalisms. Specifically, Kuhlmann shows that linear context-free rewriting systems 562 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata (LCFRS) with fan-out k (Vijay-Shanker, Weir, and Joshi 1987; Satta 1992) induce the set of dependency structures with gap degree at most k 1; coupled CFG in which the maximal rank of a nonterminal is k (Hotz and Pitsch 1996) induces the set of wellnested dependency struc</context>
<context position="80383" citStr="Kuhlmann 2010" startWordPosition="14210" endWordPosition="14211"> N. Let p be the number of direct children of h in the tree T. We have p 1, because by hypothesis #(\x06h\x07) &amp;gt; 1. With this, the induction step proof is divided into two cases, according to whether p = 1 or p &amp;gt; 1. When p = 1, the item that Lemma 1 associates with the subtree of T induced by the single direct dependent of h is known to be derivable by the induction hypothesis. It can be shown case by case that the item corresponding to h by Lemma 1 can be inferred using LINK steps, thus completing the case for p = 1. For p &amp;gt; 1, we use the concept of order annotations (Kuhlmann and Mohl 2007; Kuhlmann 2010). Order annotations are strings that encode the precedence relation between the nodes of a dependency tree. The order annotation for a given node encodes the shape (with respect to this precedence relation) of the projection of each of the children of that node, that is, the number of intervals in each projection, the number of gaps, and the way in which intervals and gaps are interleaved. The concepts of projectivity, gap degree, and well-nestedness are associated with particular constraints on order annotations. The completeness proof for p &amp;gt; 1 is divided into cases according to the order an</context>
<context position="81774" citStr="Kuhlmann (2010)" startWordPosition="14438" endWordPosition="14439">r annotations into a number of cases. Using the induction hypotheses and some relevant properties of order annotations we find that, for each of this cases, we can find a sequence of COMBINE steps to infer the item corresponding to T from smaller coherent items. \x02 7.3 Computational Complexity of WG1 The time complexity of WG1 is O(n7 ), as the step COMBINE SHRINKING GAP CENTRE works with seven free string positions. This complexity with respect to the length of the 568 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata input is as expected for this set of structures, because Kuhlmann (2010) shows that they are equivalent to LTAG, and the best existing parsers for this formalism also perform in O(n7 ) (Eisner and Satta 2000).9 Note that the COMBINE step that is the bottleneck only uses seven indices, not any additional entities such as D-rules. Hence, the O(n7 ) complexity does not involve additional factors relating to grammar size. Given unlexicalized D-rules specifying the possibility of dependencies between pairs of categories rather than pairs of words, a variant of this parser can be constructed with time complexity O(n6 ), as with parsers for unlexicalized TAG. We expand t</context>
<context position="88355" citStr="Kuhlmann (2010)" startWordPosition="15751" endWordPosition="15752">g LINK steps just as in WG1. In the case for p 1, we also base our proof on the order annotation for h, but we have to take into account that the set of possible annotations is larger when we allow the gap degree to be greater than 1, so we must consider more cases in this part of the proof. 7.6 Computational Complexity of WGk The WGk parser runs in time O(n5+2k ). As in the case of WG1, the step with most free variables is COMBINE SHRINKING GAP CENTRE with 5 + 2k free indices. Again, this complexity result is in line with what could be expected from previous research in constituency parsing: Kuhlmann (2010) shows that the set of well-nested dependency structures with gap degree at most k is closely related to coupled CFG in which the maximal rank of a nonterminal is k + 1. The constituency parser defined by Hotz and Pitsch (1996) for these grammars also adds an n2 factor for each unit increment of k. Note that a small value of k appears to be sufficient to account for the vast majority of the non-projective sentences found in natural language treebanks. For instance, the Prague Dependency Treebank (Hajic et al. 2006) contains no structures with gap degree greater than 4. Thus, a WG4 parser would</context>
<context position="90223" citStr="Kuhlmann (2010)" startWordPosition="16050" endWordPosition="16051">Unfortunately, the general problem of parsing ill-nested structures is NP-complete, even when the gap degree is bounded. This set of structures is closely related to LCFRS with bounded fan-out and unbounded production length, and parsing in this formalism is known to be NP-complete (Satta 1992). The reason for this complexity is the problem of unrestricted crossing configurations, appearing when dependency subtrees are allowed to interleave in every possible way. Ill-nested structures can be parsed in polynomial time with bounds on the gap degree and the number of dependents allowed per node: Kuhlmann (2010) presents a parser based on this idea, using a kind of grammar that resembles LCFRS, called regular dependency grammar. This parser is exponential in the gap degree, as well as in the maximum number of dependents allowed per node: Its complexity is O(nk(m+1) ), where k is the maximum gap degree and m is the maximum number of dependents per node. In contrast, the parsers presented here are data-driven and thus do not need an explicit grammar. Furthermore, they are able to parse dependency structures with any number of dependents per node, and their computational complexity is independent of thi</context>
</contexts>
<marker>Kuhlmann, 2010</marker>
<rawString>Kuhlmann, Marco. 2010. Dependency Structures and Lexicalized Grammars: An Algebraic Approach. Lecture Notes in Computer Science, vol. 6270. Springer, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Mathias Mohl</author>
</authors>
<date>2007</date>
<contexts>
<context position="80367" citStr="Kuhlmann and Mohl 2007" startWordPosition="14206" endWordPosition="14209">that #(\x06h\x03 \x07) &amp;lt; N. Let p be the number of direct children of h in the tree T. We have p 1, because by hypothesis #(\x06h\x07) &amp;gt; 1. With this, the induction step proof is divided into two cases, according to whether p = 1 or p &amp;gt; 1. When p = 1, the item that Lemma 1 associates with the subtree of T induced by the single direct dependent of h is known to be derivable by the induction hypothesis. It can be shown case by case that the item corresponding to h by Lemma 1 can be inferred using LINK steps, thus completing the case for p = 1. For p &amp;gt; 1, we use the concept of order annotations (Kuhlmann and Mohl 2007; Kuhlmann 2010). Order annotations are strings that encode the precedence relation between the nodes of a dependency tree. The order annotation for a given node encodes the shape (with respect to this precedence relation) of the projection of each of the children of that node, that is, the number of intervals in each projection, the number of gaps, and the way in which intervals and gaps are interleaved. The concepts of projectivity, gap degree, and well-nestedness are associated with particular constraints on order annotations. The completeness proof for p &amp;gt; 1 is divided into cases according</context>
</contexts>
<marker>Kuhlmann, Mohl, 2007</marker>
<rawString>Kuhlmann, Marco and Mathias Mohl. 2007.</rawString>
</citation>
<citation valid="true">
<title>Mildly context-sensitive dependency languages.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>160167</pages>
<location>Prague.</location>
<contexts>
<context position="63274" citStr="(2007)" startWordPosition="11126" endWordPosition="11126">h n. Note that this parsing schema is not correct, because Covingtons algorithm does not prevent the generation of cycles in the dependency graphs it produces. Quoting Covington (2001, page 99), Because the parser operates one word at a time, unity can only be checked at the end of the whole process: did it produce a tree with a single root that comprises all of the words? Therefore, a postprocessing mechanism is needed to determine which of the generated structures are, in fact, valid trees. In the parsing schema, this is reflected by the fact that the schema is complete but not sound. Nivre (2007) uses a variant of this algorithm in which cycle detection is used to avoid generating incorrect structures. Other non-projective parsers not covered here can also be represented under the parsing schema framework. For example, Kuhlmann (2010) presents a deduction system for a non-projective parser which uses a grammar formalism called regular dependency grammars. This deduction system can easily be converted into a parsing schema by associating adequate semantics with items. However, we do not show this here for space reasons, because we would first have to explain the formalism of regular de</context>
<context position="120400" citStr="(2007, 2010)" startWordPosition="21641" endWordPosition="21642">motos dependency parsers are step contractions of the LG variant of Eisners parser. As with the algorithm of Sleator and Temperley, these bottomup LG parsers run in cubic time with respect to input length. 9. Conclusions and Future Work The parsing schemata formalism of Sikkel (1997) has previously been used to define, analyze, and compare algorithms for constituency-based parsing. We have shown how to extend the formalism to dependency parsers, as well as the related Link Grammar formalism. Deductive approaches have been used in the past to describe individual dependency parsers: In Kuhlmann (2007, 2010) a grammatical deduction system was used to define a parser for regular dependency grammars. 581 \x0cComputational Linguistics Volume 37, Number 3 McDonald and Nivre (2007) give an alternative framework for dependency parsers, viewing them as transition systems. That model is based on parser configurations and transitions, and has no clear relationship to the approach described here. To demonstrate the theoretical uses of dependency parsing schemata, we have used them to describe a wide range of existing projective and non-projective dependency parsers. We have also clarified various relations</context>
</contexts>
<marker>2007</marker>
<rawString>Mildly context-sensitive dependency languages. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007), pages 160167, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Joakim Nivre</author>
</authors>
<date>2006</date>
<contexts>
<context position="64989" citStr="Kuhlmann and Nivre 2006" startWordPosition="11377" endWordPosition="11380">hough it is possible to parse non-projective structures in quadratic time with respect to input length under a model in which each dependency decision is independent of all the others (as in the parser of McDonald et al. [2005], discussed in Section 6.4), the problem is intractable in the absence of this assumption (McDonald and Satta 2007). Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice contain only small proportions of non-projective arcs. This has led to the study of sub-classes of the class of all non-projective dependency structures (Kuhlmann and Nivre 2006; Havelka 2007). Kuhlmann (2010) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky, Kuhlmann, and Mohl 2005), relating them to lexicalized constituency grammar formalisms. Specifically, Kuhlmann shows that linear context-free rewriting systems 562 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata (LCFRS) with fan-out k (Vijay-Shanker, Weir, and Joshi 1987; Satta 1992) induce the set of dependency structures with gap degree at most k 1; coupled CFG in which the maximal rank of a nonterminal is k (Hotz and Pitsch 1996) induces the se</context>
<context position="67538" citStr="Kuhlmann and Nivre 2006" startWordPosition="11757" endWordPosition="11760">e ill-nested structures with gap degree at most k satisfying certain constraints, giving a parser that runs in time O(n4+3k ). Note that parsing unrestricted ill-nested structures, even when the gap degree is bounded, is NP-complete: These structures are equivalent to LCFRS for which the recognition problem is NP-complete (Satta 1992). Finally, we characterize the set of structures covered by this parser, which we call mildly ill-nested structures, and show that it includes all the trees present in a number of dependency treebanks. We now define the concepts of gap degree and well-nestedness (Kuhlmann and Nivre 2006). Let T be a dependency tree for the string w1 . . . wn: Definition 5 The gap degree of a node k in T is the minimum g (N {0}) such that \x06k\x07 (the projection of the node k) can be written as the union of g + 1 intervals, that is, the number of discontinuities in \x06k\x07. The gap degree of the dependency tree T is the maximum of the gap degrees of its nodes. Note that T has gap degree 0 if and only if T is projective. Definition 6 The subtree induced by the node u in a dependency tree T is the tree Tu = (\x06u\x07, Eu) where Eu = {i j E |j \x06u\x07}. The subtrees induced by nodes p and </context>
<context position="89444" citStr="Kuhlmann and Nivre 2006" startWordPosition="15931" endWordPosition="15934">nce, the Prague Dependency Treebank (Hajic et al. 2006) contains no structures with gap degree greater than 4. Thus, a WG4 parser would be able to analyze all the well-nested structures in this treebank, which represent 99.89% of the total (WG1 would be able to parse 99.49%). Increasing k beyond 4 would not produce further improvements in coverage. 7.7 Parsing Ill-Nested Structures: MG1 and MGk The WGk parser analyzes dependency structures with bounded gap degree as long as they are well-nested. Although this covers the vast majority of the structures that occur in natural language treebanks (Kuhlmann and Nivre 2006), a significant number of sentences contain ill-nested structures. Maier and Lichte (2011) provide examples of some linguistic phenomena that cause ill-nestedness. Unfortunately, the general problem of parsing ill-nested structures is NP-complete, even when the gap degree is bounded. This set of structures is closely related to LCFRS with bounded fan-out and unbounded production length, and parsing in this formalism is known to be NP-complete (Satta 1992). The reason for this complexity is the problem of unrestricted crossing configurations, appearing when dependency subtrees are allowed to in</context>
</contexts>
<marker>Kuhlmann, Nivre, 2006</marker>
<rawString>Kuhlmann, Marco and Joakim Nivre. 2006.</rawString>
</citation>
<citation valid="false">
<title>Mildly non-projective dependency structures.</title>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>507514</pages>
<location>Montreal.</location>
<marker></marker>
<rawString>Mildly non-projective dependency structures. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 507514, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincenzo Lombardo</author>
<author>Leonardo Lesmo</author>
</authors>
<title>An Earley-type recognizer for dependency grammar.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING 96),</booktitle>
<pages>723728</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="7132" citStr="Lombardo and Lesmo (1996)" startWordPosition="1035" endWordPosition="1038"> framework, where the encodings of intermediate dependency structures are defined as items, and the operations used to combine them are expressed as inference rules. We begin by addressing a number of preliminary issues. Traditional parsing schemata are used to define grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley PREDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classifiers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we u</context>
<context position="9241" citStr="Lombardo and Lesmo (1996)" startWordPosition="1364" endWordPosition="1367">s. Additionally, D-rules allow us to use an uniform description that is valid for both data-driven and grammar-driven parsers, because D-rules can function like grammatical rules. The fundamental structures in dependency parsing are dependency trees. Therefore, just as items for constituency parsers encode sets of partial constituency trees, items for dependency parsers can be defined using partial dependency trees. However, dependency trees cannot express the fact that a particular structure has been predicted, but not yet built; this is required for grammar-based algorithms such as those of Lombardo and Lesmo (1996) and Kahane, Nasr, and Rambow (1998). The formalism can be made general enough to include these parsers by using a novel way of representing intermediate states of dependency parsers based on a form of dependency trees that include nodes labelled with preterminals and terminals (Gomez-Rodrguez, Carroll, and Weir 2008; Gomez-Rodrguez 2009). For simplicity of presentation, we will only use this representation (called extended dependency trees) in the grammar-based algorithms that need it, and we will define the formalism and the rest of the algorithms with simple dependency trees. Some existing </context>
<context position="28753" citStr="Lombardo and Lesmo (1996)" startWordPosition="4832" endWordPosition="4835">, the grammar must not have D-rules of the form (wi, i) (wn+1, n + 1), that is, it must not 549 \x0cComputational Linguistics Volume 37, Number 3 Figure 3 Grounded extended dependency tree and associated dependency structure. allow the end-of-sentence marker to govern any words. If the grammar satisfies this condition, it is trivial to see that every forest in an item of the form [0, n + 1] must contain a parse tree rooted at the beginning-of-sentence marker and with yield 0..n. As can be seen from the schema, this algorithm requires less bookkeeping than the other parsers described here. 3.5 Lombardo and Lesmo (1996) and Other Earley-Based Parsers The algorithms presented so far are based on making individual decisions about dependency links, represented by D-rules. Other parsers, such as that of Lombardo and Lesmo (1996), use grammars with CFG-like rules which encode the preferred order of dependents for each given governor. For example, a rule of the form N(Det PP) is used to allow N to have Det as left dependent and PP as right dependent. The algorithm by Lombardo and Lesmo is a version of Earleys CFG parser (Earley 1970) that uses Gaifmans dependency grammar (Gaifman 1965). As this algorithm predicts </context>
<context position="53978" citStr="Lombardo and Lesmo (1996)" startWordPosition="9366" endWordPosition="9369">ve-transitive dependents of each node forms a contiguous substring of the input. We now show that the dependency parsing schema formalism can also describe various non-projective parsers. 6.1 Pseudo-Projectivity Pseudo-projective parsers generate non-projective analyses in polynomial time by using a projective parsing strategy and postprocessing the results to establish non-projective links. This projective parsing strategy can be represented by dependency parsing schemata such as those seen in Section 3. For example, the algorithm of Kahane, Nasr, and Rambow (1998) uses a strategy similar to Lombardo and Lesmo (1996), but with the following initializer step instead of the INITTER and PREDICTOR: INITTER: [A(), i, i 1] A() P 1 i n The initialization step specified by Kahane, Nasr, and Rambow (1998) differs from this (directly consuming a nonterminal from the input) but this gives an incomplete algorithm. The problem can be fixed either by using the step shown here instead (bottomup Earley strategy) or by adding an additional step turning it into a bottomup left-corner parser. 6.2 Attardi (2006) The non-projective parser of Attardi (2006) extends the algorithm of Yamada and Matsumoto (2003), adding additiona</context>
</contexts>
<marker>Lombardo, Lesmo, 1996</marker>
<rawString>Lombardo, Vincenzo and Leonardo Lesmo. 1996. An Earley-type recognizer for dependency grammar. In Proceedings of the 16th International Conference on Computational Linguistics (COLING 96), pages 723728, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
<author>Timm Lichte</author>
</authors>
<title>Characterizing discontinuity in constituent treebanks.</title>
<date>2011</date>
<journal>Formal Grammar,</journal>
<booktitle>of Lecture Notes in Computer Science.</booktitle>
<volume>5591</volume>
<pages>164179</pages>
<editor>In P. de Grook, M. Egg, and L. Kallmeyer, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin-HeidelbergNew York,</location>
<contexts>
<context position="89534" citStr="Maier and Lichte (2011)" startWordPosition="15944" endWordPosition="15947">ree greater than 4. Thus, a WG4 parser would be able to analyze all the well-nested structures in this treebank, which represent 99.89% of the total (WG1 would be able to parse 99.49%). Increasing k beyond 4 would not produce further improvements in coverage. 7.7 Parsing Ill-Nested Structures: MG1 and MGk The WGk parser analyzes dependency structures with bounded gap degree as long as they are well-nested. Although this covers the vast majority of the structures that occur in natural language treebanks (Kuhlmann and Nivre 2006), a significant number of sentences contain ill-nested structures. Maier and Lichte (2011) provide examples of some linguistic phenomena that cause ill-nestedness. Unfortunately, the general problem of parsing ill-nested structures is NP-complete, even when the gap degree is bounded. This set of structures is closely related to LCFRS with bounded fan-out and unbounded production length, and parsing in this formalism is known to be NP-complete (Satta 1992). The reason for this complexity is the problem of unrestricted crossing configurations, appearing when dependency subtrees are allowed to interleave in every possible way. Ill-nested structures can be parsed in polynomial time wit</context>
<context position="122992" citStr="Maier and Lichte (2011)" startWordPosition="22025" endWordPosition="22028">implementations of the mildly non-projective dependency parsers presented here, using probabilistic models to guide their linking decisions, and compare their practical performance and accuracy to those of other non-projective dependency parsers. Additionally, our definition of mildly ill-nested structures is closely related to the way the corresponding parser works. It would be interesting to find a more grammar-oriented definition that would provide linguistic insight into this set of structures. An alternative generalization of the concept of well-nestedness has recently been introduced by Maier and Lichte (2011). The definition of this property of structures, called k-ill-nestedness, is more declarative than that of mildly ill-nestedness. However, it is based on properties that are not local to projections or subtrees, and there is no evidence that k-ill-nested structures are parsable in polynomial time. Finally, we observe that that some well-known parsing algorithms discussed here (Nivre 2003; McDonald et al. 2005) rely on statistically-driven control mechanisms that fall below the abstraction level of parsing schemata. Therefore, it would be useful to have an extension of parsing schemata allowing</context>
</contexts>
<marker>Maier, Lichte, 2011</marker>
<rawString>Maier, Wolfgang and Timm Lichte. 2011. Characterizing discontinuity in constituent treebanks. In P. de Grook, M. Egg, and L. Kallmeyer, editors, Formal Grammar, volume 5591 of Lecture Notes in Computer Science. Springer-Verlag, Berlin-HeidelbergNew York, pages 164179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In ACL 05: Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>9198</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2689" citStr="McDonald et al. 2005" startWordPosition="365" endWordPosition="368">e de Informatica, Universidade da Coruna Campus de Elvina, s/n, 15071 A Coruna, Spain. E-mail: cgomezr@udc.es. School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK. E-mail: J.A.Carroll@sussex.ac.uk. School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK. E-mail: D.J.Weir@sussex.ac.uk. Submission received: 21 October 2009; revised submission received: 23 December 2010; accepted for publication: 29 January 2011. 2011 Association for Computational Linguistics \x0cComputational Linguistics Volume 37, Number 3 are able to represent non-projective structures (McDonald et al. 2005), which is important when parsing free word order languages where discontinuous constituents are common. The formalism of parsing schemata, introduced by Sikkel (1997), is a useful tool for the study of constituency parsers, supporting precise, high-level descriptions of parsing algorithms. Potential applications of parsing schemata include devising correctness proofs, extending our understanding of relationships between different algorithms, deriving new variants of existing algorithms, and obtaining efficient implementations automatically (Gomez-Rodrguez, Vilares, and Alonso 2009). The forma</context>
<context position="59436" citStr="McDonald et al. 2005" startWordPosition="10419" endWordPosition="10422">e its sets of items and deduction steps are subsets of those of Attd. Therefore, the set of structures parsed by MHd+2 is also a subset of those parsed by Attd. The complexity of the MHk parser is O(nk ). For k = 3, MH3 is a step refinement of the parser by Yamada and Matsumoto (2003) that parses projective structures only, but by modifying the bound k we can define polynomial-time algorithms that parse larger sets of non-projective dependency structures. The MHk parser has the property of being able to parse any possible dependency structure as long as we make k large enough. 6.4 MST Parser (McDonald et al. 2005) McDonald et al. (2005) describe a parser which finds a nonprojective analysis for a sentence in O(n2 ) time under a strong independence assumption called an edgefactored model: Each dependency decision is assumed to be independent of all the others (McDonald and Satta 2007). Despite the restrictiveness of this model, this maximum spanning tree (MST) parser achieves state-of-the-art performance for projective and non-projective structures (Che et al. 2008; Nivre and McDonald 2008; Surdeanu et al. 2008). The parser considers the weighted graph formed by all the possible dependencies between pai</context>
<context position="123405" citStr="McDonald et al. 2005" startWordPosition="22086" endWordPosition="22089">r-oriented definition that would provide linguistic insight into this set of structures. An alternative generalization of the concept of well-nestedness has recently been introduced by Maier and Lichte (2011). The definition of this property of structures, called k-ill-nestedness, is more declarative than that of mildly ill-nestedness. However, it is based on properties that are not local to projections or subtrees, and there is no evidence that k-ill-nested structures are parsable in polynomial time. Finally, we observe that that some well-known parsing algorithms discussed here (Nivre 2003; McDonald et al. 2005) rely on statistically-driven control mechanisms that fall below the abstraction level of parsing schemata. Therefore, it would be useful to have an extension of parsing schemata allowing the description and comparison of these control structures in a general way. Acknowledgments This work was partially supported by MEC and FEDER (HUM2007-66607-C04) and Xunta de Galicia (PGIDIT07SIN005206PR, INCITE08E1R104022ES, INCITE08ENA305025ES, INCITE08PXIB302179PR, Rede Galega de Proc. da Linguaxe e Recup. de Informacion, Rede Galega de Lingustica de Corpus, Bolsas Estadas INCITE/FSE cofinanced). Referen</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>McDonald, Ryan, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In ACL 05: Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 9198, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<date>2007</date>
<contexts>
<context position="120572" citStr="McDonald and Nivre (2007)" startWordPosition="21664" endWordPosition="21667">un in cubic time with respect to input length. 9. Conclusions and Future Work The parsing schemata formalism of Sikkel (1997) has previously been used to define, analyze, and compare algorithms for constituency-based parsing. We have shown how to extend the formalism to dependency parsers, as well as the related Link Grammar formalism. Deductive approaches have been used in the past to describe individual dependency parsers: In Kuhlmann (2007, 2010) a grammatical deduction system was used to define a parser for regular dependency grammars. 581 \x0cComputational Linguistics Volume 37, Number 3 McDonald and Nivre (2007) give an alternative framework for dependency parsers, viewing them as transition systems. That model is based on parser configurations and transitions, and has no clear relationship to the approach described here. To demonstrate the theoretical uses of dependency parsing schemata, we have used them to describe a wide range of existing projective and non-projective dependency parsers. We have also clarified various relations between parsers which were originally formulated very differentlyfor example, establishing the relation between the dynamic programming algorithm of Eisner (1996) and the </context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>McDonald, Ryan and Joakim Nivre. 2007.</rawString>
</citation>
<citation valid="true">
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>122131</pages>
<location>Prague.</location>
<contexts>
<context position="63274" citStr="(2007)" startWordPosition="11126" endWordPosition="11126">h n. Note that this parsing schema is not correct, because Covingtons algorithm does not prevent the generation of cycles in the dependency graphs it produces. Quoting Covington (2001, page 99), Because the parser operates one word at a time, unity can only be checked at the end of the whole process: did it produce a tree with a single root that comprises all of the words? Therefore, a postprocessing mechanism is needed to determine which of the generated structures are, in fact, valid trees. In the parsing schema, this is reflected by the fact that the schema is complete but not sound. Nivre (2007) uses a variant of this algorithm in which cycle detection is used to avoid generating incorrect structures. Other non-projective parsers not covered here can also be represented under the parsing schema framework. For example, Kuhlmann (2010) presents a deduction system for a non-projective parser which uses a grammar formalism called regular dependency grammars. This deduction system can easily be converted into a parsing schema by associating adequate semantics with items. However, we do not show this here for space reasons, because we would first have to explain the formalism of regular de</context>
<context position="120400" citStr="(2007, 2010)" startWordPosition="21641" endWordPosition="21642">motos dependency parsers are step contractions of the LG variant of Eisners parser. As with the algorithm of Sleator and Temperley, these bottomup LG parsers run in cubic time with respect to input length. 9. Conclusions and Future Work The parsing schemata formalism of Sikkel (1997) has previously been used to define, analyze, and compare algorithms for constituency-based parsing. We have shown how to extend the formalism to dependency parsers, as well as the related Link Grammar formalism. Deductive approaches have been used in the past to describe individual dependency parsers: In Kuhlmann (2007, 2010) a grammatical deduction system was used to define a parser for regular dependency grammars. 581 \x0cComputational Linguistics Volume 37, Number 3 McDonald and Nivre (2007) give an alternative framework for dependency parsers, viewing them as transition systems. That model is based on parser configurations and transitions, and has no clear relationship to the approach described here. To demonstrate the theoretical uses of dependency parsing schemata, we have used them to describe a wide range of existing projective and non-projective dependency parsers. We have also clarified various relations</context>
</contexts>
<marker>2007</marker>
<rawString>Characterizing the errors of data-driven dependency parsing models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007), pages 122131, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Nonprojective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP 2005: Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>523530</pages>
<location>Vancouver.</location>
<contexts>
<context position="2689" citStr="McDonald et al. 2005" startWordPosition="365" endWordPosition="368">e de Informatica, Universidade da Coruna Campus de Elvina, s/n, 15071 A Coruna, Spain. E-mail: cgomezr@udc.es. School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK. E-mail: J.A.Carroll@sussex.ac.uk. School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK. E-mail: D.J.Weir@sussex.ac.uk. Submission received: 21 October 2009; revised submission received: 23 December 2010; accepted for publication: 29 January 2011. 2011 Association for Computational Linguistics \x0cComputational Linguistics Volume 37, Number 3 are able to represent non-projective structures (McDonald et al. 2005), which is important when parsing free word order languages where discontinuous constituents are common. The formalism of parsing schemata, introduced by Sikkel (1997), is a useful tool for the study of constituency parsers, supporting precise, high-level descriptions of parsing algorithms. Potential applications of parsing schemata include devising correctness proofs, extending our understanding of relationships between different algorithms, deriving new variants of existing algorithms, and obtaining efficient implementations automatically (Gomez-Rodrguez, Vilares, and Alonso 2009). The forma</context>
<context position="59436" citStr="McDonald et al. 2005" startWordPosition="10419" endWordPosition="10422">e its sets of items and deduction steps are subsets of those of Attd. Therefore, the set of structures parsed by MHd+2 is also a subset of those parsed by Attd. The complexity of the MHk parser is O(nk ). For k = 3, MH3 is a step refinement of the parser by Yamada and Matsumoto (2003) that parses projective structures only, but by modifying the bound k we can define polynomial-time algorithms that parse larger sets of non-projective dependency structures. The MHk parser has the property of being able to parse any possible dependency structure as long as we make k large enough. 6.4 MST Parser (McDonald et al. 2005) McDonald et al. (2005) describe a parser which finds a nonprojective analysis for a sentence in O(n2 ) time under a strong independence assumption called an edgefactored model: Each dependency decision is assumed to be independent of all the others (McDonald and Satta 2007). Despite the restrictiveness of this model, this maximum spanning tree (MST) parser achieves state-of-the-art performance for projective and non-projective structures (Che et al. 2008; Nivre and McDonald 2008; Surdeanu et al. 2008). The parser considers the weighted graph formed by all the possible dependencies between pai</context>
<context position="123405" citStr="McDonald et al. 2005" startWordPosition="22086" endWordPosition="22089">r-oriented definition that would provide linguistic insight into this set of structures. An alternative generalization of the concept of well-nestedness has recently been introduced by Maier and Lichte (2011). The definition of this property of structures, called k-ill-nestedness, is more declarative than that of mildly ill-nestedness. However, it is based on properties that are not local to projections or subtrees, and there is no evidence that k-ill-nested structures are parsable in polynomial time. Finally, we observe that that some well-known parsing algorithms discussed here (Nivre 2003; McDonald et al. 2005) rely on statistically-driven control mechanisms that fall below the abstraction level of parsing schemata. Therefore, it would be useful to have an extension of parsing schemata allowing the description and comparison of these control structures in a general way. Acknowledgments This work was partially supported by MEC and FEDER (HUM2007-66607-C04) and Xunta de Galicia (PGIDIT07SIN005206PR, INCITE08E1R104022ES, INCITE08ENA305025ES, INCITE08PXIB302179PR, Rede Galega de Proc. da Linguaxe e Recup. de Informacion, Rede Galega de Lingustica de Corpus, Bolsas Estadas INCITE/FSE cofinanced). Referen</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Nonprojective dependency parsing using spanning tree algorithms. In HLT/EMNLP 2005: Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 523530, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Giorgio Satta</author>
</authors>
<date>2007</date>
<contexts>
<context position="59711" citStr="McDonald and Satta 2007" startWordPosition="10464" endWordPosition="10467">sumoto (2003) that parses projective structures only, but by modifying the bound k we can define polynomial-time algorithms that parse larger sets of non-projective dependency structures. The MHk parser has the property of being able to parse any possible dependency structure as long as we make k large enough. 6.4 MST Parser (McDonald et al. 2005) McDonald et al. (2005) describe a parser which finds a nonprojective analysis for a sentence in O(n2 ) time under a strong independence assumption called an edgefactored model: Each dependency decision is assumed to be independent of all the others (McDonald and Satta 2007). Despite the restrictiveness of this model, this maximum spanning tree (MST) parser achieves state-of-the-art performance for projective and non-projective structures (Che et al. 2008; Nivre and McDonald 2008; Surdeanu et al. 2008). The parser considers the weighted graph formed by all the possible dependencies between pairs of input words, and applies an MST algorithm to find a dependency tree covering all the words in the sentence and maximizing the sum of weights. The MST algorithm for directed graphs suggested by McDonald et al. (2005) is not fully constructive: It does not work by buildi</context>
<context position="64708" citStr="McDonald and Satta 2007" startWordPosition="11336" endWordPosition="11339">e natural language sentences appear to have non-projective syntactic structure, something that arises in many languages (Havelka 2007), and is particularly common in free word order languages such as Czech. Parsing without the projectivity constraint is computationally complex: Although it is possible to parse non-projective structures in quadratic time with respect to input length under a model in which each dependency decision is independent of all the others (as in the parser of McDonald et al. [2005], discussed in Section 6.4), the problem is intractable in the absence of this assumption (McDonald and Satta 2007). Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice contain only small proportions of non-projective arcs. This has led to the study of sub-classes of the class of all non-projective dependency structures (Kuhlmann and Nivre 2006; Havelka 2007). Kuhlmann (2010) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky, Kuhlmann, and Mohl 2005), relating them to lexicalized constituency grammar formalisms. Specifically, Kuhlmann shows that linear context-free rewriting systems 562 \x0cGomez-Rodrguez, Carr</context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>McDonald, Ryan and Giorgio Satta. 2007.</rawString>
</citation>
<citation valid="false">
<title>On the complexity of non-projective data-driven dependency parsing.</title>
<booktitle>In IWPT 2007: Proceedings of the 10th International Conference on Parsing Technologies,</booktitle>
<pages>121132</pages>
<location>Prague.</location>
<marker></marker>
<rawString>On the complexity of non-projective data-driven dependency parsing. In IWPT 2007: Proceedings of the 10th International Conference on Parsing Technologies, pages 121132, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Nilsson</author>
<author>Johan Hall</author>
<author>Joakim Nivre</author>
</authors>
<title>MAMBA meets TIGER: Reconstructing a Swedish treebank from antiquity.</title>
<date>2005</date>
<booktitle>In Proceedings of NODALIDA 2005 Special Session on Treebanks,</booktitle>
<pages>119132</pages>
<location>Joensuu.</location>
<marker>Nilsson, Hall, Nivre, 2005</marker>
<rawString>Nilsson, Jens, Johan Hall, and Joakim Nivre. 2005. MAMBA meets TIGER: Reconstructing a Swedish treebank from antiquity. In Proceedings of NODALIDA 2005 Special Session on Treebanks, pages 119132, Joensuu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03),</booktitle>
<pages>149160</pages>
<location>Nancy.</location>
<contexts>
<context position="32281" citStr="Nivre (2003)" startWordPosition="5449" endWordPosition="5450"> 1, n]}. The schema for Lombardo and Lesmos parser is a variant of the Earley constituency parser (cf. Sikkel 1997), with minor changes to adapt it to dependency grammar (for example, the SCANNER always moves the dot over the head symbol , rather than over a terminal symbol). Analogously, other dependency parsing schemata based on CFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997): The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and the parser described by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Nivre (2003) Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing, later extended by Nivre, Hall, and Nilsson (2004). With linear-time performance and competitive parsing accuracy (Nivre et al. 2006; Nivre and McDonald 2008), it is one of the parsers included in the MaltParser system (Nivre et al. 2007), which is currently widely used (e.g., Nivre et al. 2007; Surdeanu et al. 2008). The parser proceeds by reading the sentence from left to right, using a stack and four different kinds of transitions between configurations. The transition system defined by all the possible confi</context>
<context position="40288" citStr="Nivre (2003)" startWordPosition="6945" endWordPosition="6946">oes not parse all projective dependency structures, because when creating leftward links it assumes that the head of a node i must be a reflexive-transitive head of the node i 1, which is not always the case. For instance, the structure shown in Figure 5 cannot be parsed because the constraints imposed by the algorithm prevent it from finding the head of 4. The MaltParser system (Nivre et al. 2007) includes an implementation of a complete variant of Covingtons LSUP parser where these constraints have been relaxed. This implementation has the same tree building logic as the parser described by Nivre (2003), differing from it only with respect to the control structure. Thus, it can be seen as a different realization of the schema shown in Section 3.6. 4. Relations Between Dependency Parsers The parsing schemata framework can be exploited to establish how different algorithms are related, improving our understanding of the features of these parsers, and potentially exposing new algorithms that combine characteristics of existing parsers in novel ways. Sikkel (1994) defines various relations between schemata that fall into two categories: generalization relations, which are used to obtain more fin</context>
<context position="123382" citStr="Nivre 2003" startWordPosition="22084" endWordPosition="22085"> more grammar-oriented definition that would provide linguistic insight into this set of structures. An alternative generalization of the concept of well-nestedness has recently been introduced by Maier and Lichte (2011). The definition of this property of structures, called k-ill-nestedness, is more declarative than that of mildly ill-nestedness. However, it is based on properties that are not local to projections or subtrees, and there is no evidence that k-ill-nested structures are parsable in polynomial time. Finally, we observe that that some well-known parsing algorithms discussed here (Nivre 2003; McDonald et al. 2005) rely on statistically-driven control mechanisms that fall below the abstraction level of parsing schemata. Therefore, it would be useful to have an extension of parsing schemata allowing the description and comparison of these control structures in a general way. Acknowledgments This work was partially supported by MEC and FEDER (HUM2007-66607-C04) and Xunta de Galicia (PGIDIT07SIN005206PR, INCITE08E1R104022ES, INCITE08ENA305025ES, INCITE08PXIB302179PR, Rede Galega de Proc. da Linguaxe e Recup. de Informacion, Rede Galega de Lingustica de Corpus, Bolsas Estadas INCITE/F</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Nivre, Joakim. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03), pages 149160, Nancy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incremental non-projective dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT 2007: The Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>396403</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="63274" citStr="Nivre (2007)" startWordPosition="11125" endWordPosition="11126"> length n. Note that this parsing schema is not correct, because Covingtons algorithm does not prevent the generation of cycles in the dependency graphs it produces. Quoting Covington (2001, page 99), Because the parser operates one word at a time, unity can only be checked at the end of the whole process: did it produce a tree with a single root that comprises all of the words? Therefore, a postprocessing mechanism is needed to determine which of the generated structures are, in fact, valid trees. In the parsing schema, this is reflected by the fact that the schema is complete but not sound. Nivre (2007) uses a variant of this algorithm in which cycle detection is used to avoid generating incorrect structures. Other non-projective parsers not covered here can also be represented under the parsing schema framework. For example, Kuhlmann (2010) presents a deduction system for a non-projective parser which uses a grammar formalism called regular dependency grammars. This deduction system can easily be converted into a parsing schema by associating adequate semantics with items. However, we do not show this here for space reasons, because we would first have to explain the formalism of regular de</context>
<context position="120572" citStr="Nivre (2007)" startWordPosition="21666" endWordPosition="21667">ime with respect to input length. 9. Conclusions and Future Work The parsing schemata formalism of Sikkel (1997) has previously been used to define, analyze, and compare algorithms for constituency-based parsing. We have shown how to extend the formalism to dependency parsers, as well as the related Link Grammar formalism. Deductive approaches have been used in the past to describe individual dependency parsers: In Kuhlmann (2007, 2010) a grammatical deduction system was used to define a parser for regular dependency grammars. 581 \x0cComputational Linguistics Volume 37, Number 3 McDonald and Nivre (2007) give an alternative framework for dependency parsers, viewing them as transition systems. That model is based on parser configurations and transitions, and has no clear relationship to the approach described here. To demonstrate the theoretical uses of dependency parsing schemata, we have used them to describe a wide range of existing projective and non-projective dependency parsers. We have also clarified various relations between parsers which were originally formulated very differentlyfor example, establishing the relation between the dynamic programming algorithm of Eisner (1996) and the </context>
</contexts>
<marker>Nivre, 2007</marker>
<rawString>Nivre, Joakim. 2007. Incremental non-projective dependency parsing. In Proceedings of NAACL HLT 2007: The Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 396403, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra Kubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>shared task on dependency parsing.</title>
<date>2007</date>
<booktitle>The CoNLL</booktitle>
<contexts>
<context position="32602" citStr="Nivre et al. 2007" startWordPosition="5495" endWordPosition="5498">schemata based on CFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997): The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and the parser described by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Nivre (2003) Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing, later extended by Nivre, Hall, and Nilsson (2004). With linear-time performance and competitive parsing accuracy (Nivre et al. 2006; Nivre and McDonald 2008), it is one of the parsers included in the MaltParser system (Nivre et al. 2007), which is currently widely used (e.g., Nivre et al. 2007; Surdeanu et al. 2008). The parser proceeds by reading the sentence from left to right, using a stack and four different kinds of transitions between configurations. The transition system defined by all the possible configurations and transitions is nondeterministic, and machine learning techniques are used to train a mechanism that produces a deterministic parser. A deduction system describing the transitions of the parser is defined by Nivre, Hall, and Nilsson (2004), with the following set of rules that describes transitions 551 \x0c</context>
<context position="40077" citStr="Nivre et al. 2007" startWordPosition="6911" endWordPosition="6914">n-projective dependency parser, and a projective variant called Algorithm LSUP (for List-based Search with Uniqueness and Projectivity). Unfortunately, the algorithm presented in Covington (2001) is not complete: It does not parse all projective dependency structures, because when creating leftward links it assumes that the head of a node i must be a reflexive-transitive head of the node i 1, which is not always the case. For instance, the structure shown in Figure 5 cannot be parsed because the constraints imposed by the algorithm prevent it from finding the head of 4. The MaltParser system (Nivre et al. 2007) includes an implementation of a complete variant of Covingtons LSUP parser where these constraints have been relaxed. This implementation has the same tree building logic as the parser described by Nivre (2003), differing from it only with respect to the control structure. Thus, it can be seen as a different realization of the schema shown in Section 3.6. 4. Relations Between Dependency Parsers The parsing schemata framework can be exploited to establish how different algorithms are related, improving our understanding of the features of these parsers, and potentially exposing new algorithms </context>
</contexts>
<marker>Nivre, Hall, Kubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Nivre, Joakim, Johan Hall, Sandra Kubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>915932</pages>
<location>Prague.</location>
<marker></marker>
<rawString>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915932, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL-2004),</booktitle>
<pages>4956</pages>
<location>Boston, MA.</location>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Nivre, Joakim, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL-2004), pages 4956, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, Gulsen Eryigit, Sandra Kubler, Stetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="32602" citStr="Nivre et al. 2007" startWordPosition="5495" endWordPosition="5498">schemata based on CFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997): The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and the parser described by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Nivre (2003) Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing, later extended by Nivre, Hall, and Nilsson (2004). With linear-time performance and competitive parsing accuracy (Nivre et al. 2006; Nivre and McDonald 2008), it is one of the parsers included in the MaltParser system (Nivre et al. 2007), which is currently widely used (e.g., Nivre et al. 2007; Surdeanu et al. 2008). The parser proceeds by reading the sentence from left to right, using a stack and four different kinds of transitions between configurations. The transition system defined by all the possible configurations and transitions is nondeterministic, and machine learning techniques are used to train a mechanism that produces a deterministic parser. A deduction system describing the transitions of the parser is defined by Nivre, Hall, and Nilsson (2004), with the following set of rules that describes transitions 551 \x0c</context>
<context position="40077" citStr="Nivre et al. 2007" startWordPosition="6911" endWordPosition="6914">n-projective dependency parser, and a projective variant called Algorithm LSUP (for List-based Search with Uniqueness and Projectivity). Unfortunately, the algorithm presented in Covington (2001) is not complete: It does not parse all projective dependency structures, because when creating leftward links it assumes that the head of a node i must be a reflexive-transitive head of the node i 1, which is not always the case. For instance, the structure shown in Figure 5 cannot be parsed because the constraints imposed by the algorithm prevent it from finding the head of 4. The MaltParser system (Nivre et al. 2007) includes an implementation of a complete variant of Covingtons LSUP parser where these constraints have been relaxed. This implementation has the same tree building logic as the parser described by Nivre (2003), differing from it only with respect to the control structure. Thus, it can be seen as a different realization of the schema shown in Section 3.6. 4. Relations Between Dependency Parsers The parsing schemata framework can be exploited to establish how different algorithms are related, improving our understanding of the features of these parsers, and potentially exposing new algorithms </context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas Chanev, Gulsen Eryigit, Sandra Kubler, Stetoslav Marinov, and Erwin Marsi. 2007. MaltParser: A languageindependent system for data-driven dependency parsing. Natural Language Engineering, 13(2):99135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Gulsen Eryigit</author>
<author>Stetoslav Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>221225</pages>
<location>Sydney.</location>
<contexts>
<context position="32496" citStr="Nivre et al. 2006" startWordPosition="5476" endWordPosition="5479">he dot over the head symbol , rather than over a terminal symbol). Analogously, other dependency parsing schemata based on CFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997): The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and the parser described by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Nivre (2003) Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing, later extended by Nivre, Hall, and Nilsson (2004). With linear-time performance and competitive parsing accuracy (Nivre et al. 2006; Nivre and McDonald 2008), it is one of the parsers included in the MaltParser system (Nivre et al. 2007), which is currently widely used (e.g., Nivre et al. 2007; Surdeanu et al. 2008). The parser proceeds by reading the sentence from left to right, using a stack and four different kinds of transitions between configurations. The transition system defined by all the possible configurations and transitions is nondeterministic, and machine learning techniques are used to train a mechanism that produces a deterministic parser. A deduction system describing the transitions of the parser is defin</context>
</contexts>
<marker>Nivre, Hall, Nilsson, Eryigit, Marinov, 2006</marker>
<rawString>Nivre, Joakim, Johan Hall, Jens Nilsson, Gulsen Eryigit, and Stetoslav Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 221225, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>x0cComputational Linguistics Volume 37, Number</booktitle>
<volume>3</volume>
<pages>950958</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="32522" citStr="Nivre and McDonald 2008" startWordPosition="5480" endWordPosition="5483">d symbol , rather than over a terminal symbol). Analogously, other dependency parsing schemata based on CFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997): The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and the parser described by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Nivre (2003) Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing, later extended by Nivre, Hall, and Nilsson (2004). With linear-time performance and competitive parsing accuracy (Nivre et al. 2006; Nivre and McDonald 2008), it is one of the parsers included in the MaltParser system (Nivre et al. 2007), which is currently widely used (e.g., Nivre et al. 2007; Surdeanu et al. 2008). The parser proceeds by reading the sentence from left to right, using a stack and four different kinds of transitions between configurations. The transition system defined by all the possible configurations and transitions is nondeterministic, and machine learning techniques are used to train a mechanism that produces a deterministic parser. A deduction system describing the transitions of the parser is defined by Nivre, Hall, and Nil</context>
<context position="59920" citStr="Nivre and McDonald 2008" startWordPosition="10493" endWordPosition="10496"> property of being able to parse any possible dependency structure as long as we make k large enough. 6.4 MST Parser (McDonald et al. 2005) McDonald et al. (2005) describe a parser which finds a nonprojective analysis for a sentence in O(n2 ) time under a strong independence assumption called an edgefactored model: Each dependency decision is assumed to be independent of all the others (McDonald and Satta 2007). Despite the restrictiveness of this model, this maximum spanning tree (MST) parser achieves state-of-the-art performance for projective and non-projective structures (Che et al. 2008; Nivre and McDonald 2008; Surdeanu et al. 2008). The parser considers the weighted graph formed by all the possible dependencies between pairs of input words, and applies an MST algorithm to find a dependency tree covering all the words in the sentence and maximizing the sum of weights. The MST algorithm for directed graphs suggested by McDonald et al. (2005) is not fully constructive: It does not work by building structures and combining them into large structures until it finds the solution. Instead, the algorithm works by using a greedy strategy to select a candidate set of edges for the spanning tree, potentially</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>\x0cComputational Linguistics Volume 37, Number 3 Nivre, Joakim and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT), pages 950958, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudo-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In ACL 05: Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>99106</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="64734" citStr="Nivre and Nilsson (2005)" startWordPosition="11340" endWordPosition="11343">es appear to have non-projective syntactic structure, something that arises in many languages (Havelka 2007), and is particularly common in free word order languages such as Czech. Parsing without the projectivity constraint is computationally complex: Although it is possible to parse non-projective structures in quadratic time with respect to input length under a model in which each dependency decision is independent of all the others (as in the parser of McDonald et al. [2005], discussed in Section 6.4), the problem is intractable in the absence of this assumption (McDonald and Satta 2007). Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice contain only small proportions of non-projective arcs. This has led to the study of sub-classes of the class of all non-projective dependency structures (Kuhlmann and Nivre 2006; Havelka 2007). Kuhlmann (2010) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky, Kuhlmann, and Mohl 2005), relating them to lexicalized constituency grammar formalisms. Specifically, Kuhlmann shows that linear context-free rewriting systems 562 \x0cGomez-Rodrguez, Carroll, and Weir Dependency P</context>
<context position="90982" citStr="Nivre and Nilsson 2005" startWordPosition="16171" endWordPosition="16174">xponential in the gap degree, as well as in the maximum number of dependents allowed per node: Its complexity is O(nk(m+1) ), where k is the maximum gap degree and m is the maximum number of dependents per node. In contrast, the parsers presented here are data-driven and thus do not need an explicit grammar. Furthermore, they are able to parse dependency structures with any number of dependents per node, and their computational complexity is independent of this parameter m. In line with the observation that most non-projective structures appearing in practice are only slightly non-projective (Nivre and Nilsson 2005), we characterize a sense in which the structures appearing in treebanks are only slightly ill-nested. We 571 \x0cComputational Linguistics Volume 37, Number 3 generalize the algorithms WG1 and WGk to parse a proper superset of the set of wellnested structures in polynomial time, and give a characterization of this new set of structures, which includes all the structures in several dependency treebanks. The WGk parser for well-nested structures presented previously is based on a bottomup process, where LINK steps are used to link completed subtrees to a head, and COMBINE steps are used to join</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Nivre, Joakim and Jens Nilsson. 2005. Pseudo-projective dependency parsing. In ACL 05: Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 99106, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Bilge Say, Dilek Zeynep Hakkani-Tur, and Gokhan Tur.</title>
<date>2003</date>
<booktitle>Building and Exploiting Syntacticallyannotated Corpora.</booktitle>
<pages>261277</pages>
<editor>In A. Abeille, editor,</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht,</location>
<marker>Oflazer, 2003</marker>
<rawString>Oflazer, Kemal, Bilge Say, Dilek Zeynep Hakkani-Tur, and Gokhan Tur. 2003. Building a Turkish treebank. In A. Abeille, editor, Building and Exploiting Syntacticallyannotated Corpora. Kluwer, Dordrecht, pages 261277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giorgio Satta</author>
</authors>
<title>Recognition of linear context-free rewriting systems.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics (ACL92),</booktitle>
<pages>8995</pages>
<location>Newark, DE.</location>
<contexts>
<context position="65422" citStr="Satta 1992" startWordPosition="11435" endWordPosition="11436"> contain only small proportions of non-projective arcs. This has led to the study of sub-classes of the class of all non-projective dependency structures (Kuhlmann and Nivre 2006; Havelka 2007). Kuhlmann (2010) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky, Kuhlmann, and Mohl 2005), relating them to lexicalized constituency grammar formalisms. Specifically, Kuhlmann shows that linear context-free rewriting systems 562 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata (LCFRS) with fan-out k (Vijay-Shanker, Weir, and Joshi 1987; Satta 1992) induce the set of dependency structures with gap degree at most k 1; coupled CFG in which the maximal rank of a nonterminal is k (Hotz and Pitsch 1996) induces the set of wellnested dependency structures with gap degree at most k 1; and finally, LTAG (Joshi and Schabes 1997) induces the set of well-nested dependency structures with gap degree at most 1. These results establish that there are polynomial-time dependency parsing algorithms for well-nested structures with bounded gap degree, because such parsers exist for their corresponding lexicalized constituency-based formalisms. Developing e</context>
<context position="67250" citStr="Satta 1992" startWordPosition="11713" endWordPosition="11714">e optimized to O(n6 ) in the nonlexicalized case. Secondly, we generalize our algorithm to any well-nested dependency structure with gap degree at most k, resulting in an algorithm with time complexity O(n5+2k ). Thirdly, we generalize the previous parsers in order to include ill-nested structures with gap degree at most k satisfying certain constraints, giving a parser that runs in time O(n4+3k ). Note that parsing unrestricted ill-nested structures, even when the gap degree is bounded, is NP-complete: These structures are equivalent to LCFRS for which the recognition problem is NP-complete (Satta 1992). Finally, we characterize the set of structures covered by this parser, which we call mildly ill-nested structures, and show that it includes all the trees present in a number of dependency treebanks. We now define the concepts of gap degree and well-nestedness (Kuhlmann and Nivre 2006). Let T be a dependency tree for the string w1 . . . wn: Definition 5 The gap degree of a node k in T is the minimum g (N {0}) such that \x06k\x07 (the projection of the node k) can be written as the union of g + 1 intervals, that is, the number of discontinuities in \x06k\x07. The gap degree of the dependency </context>
<context position="89903" citStr="Satta 1992" startWordPosition="16000" endWordPosition="16001">ong as they are well-nested. Although this covers the vast majority of the structures that occur in natural language treebanks (Kuhlmann and Nivre 2006), a significant number of sentences contain ill-nested structures. Maier and Lichte (2011) provide examples of some linguistic phenomena that cause ill-nestedness. Unfortunately, the general problem of parsing ill-nested structures is NP-complete, even when the gap degree is bounded. This set of structures is closely related to LCFRS with bounded fan-out and unbounded production length, and parsing in this formalism is known to be NP-complete (Satta 1992). The reason for this complexity is the problem of unrestricted crossing configurations, appearing when dependency subtrees are allowed to interleave in every possible way. Ill-nested structures can be parsed in polynomial time with bounds on the gap degree and the number of dependents allowed per node: Kuhlmann (2010) presents a parser based on this idea, using a kind of grammar that resembles LCFRS, called regular dependency grammar. This parser is exponential in the gap degree, as well as in the maximum number of dependents allowed per node: Its complexity is O(nk(m+1) ), where k is the max</context>
</contexts>
<marker>Satta, 1992</marker>
<rawString>Satta, Giorgio. 1992. Recognition of linear context-free rewriting systems. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics (ACL92), pages 8995, Newark, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerold Schneider</author>
</authors>
<title>A linguistic comparison of constituency, dependency, and link grammar. M.Sc. thesis,</title>
<date>1998</date>
<institution>University of Zurich, Switzerland.</institution>
<contexts>
<context position="102710" citStr="Schneider (1998)" startWordPosition="18293" endWordPosition="18294">represents the structure of sentences as a set of links between words. However, whereas dependency links are directed, the links used in LG are undirected: There is no distinction made between heads and dependents. 10 Arabic (Hajic et al. 2004), Czech (Hajic et al. 2006), Danish (Kromann 2003), Dutch (van der Beek et al. 2002), Latin (Bamman and Crane 2006), Portuguese (Afonso et al. 2002), Slovene (Dzeroski et al. 2006), Swedish (Nilsson, Hall, and Nivre 2005), and Turkish (Atalay, Oflazer, and Say 2003; Oflazer et al. 2003). 11 A complete treatment of LG is beyond the scope of this article: Schneider (1998) gives a detailed comparison of Link Grammar and dependency formalisms. 575 \x0cComputational Linguistics Volume 37, Number 3 Table 1 Counts of dependency structures in treebanks for several languages, classified by projectivity, gap degree, and mild and strong ill-nestedness (for their gap degree). Language Structures Total Nonprojective Total By gap degree By nestedness Gap deg 1 Gap deg 2 Gap deg 3 Gap deg &amp;gt; 3 Wellnested Mildly illnested Strongly illnested Arabic 2,995 205 189 13 2 1 204 1 0 Czech 87,889 20,353 19,989 359 4 1 20,257 96 0 Danish 5,430 864 854 10 0 0 856 8 0 Dutch 13,349 4,86</context>
</contexts>
<marker>Schneider, 1998</marker>
<rawString>Schneider, Gerold. 1998. A linguistic comparison of constituency, dependency, and link grammar. M.Sc. thesis, University of Zurich, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT),</booktitle>
<pages>577585</pages>
<location>Columbus, OH.</location>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Shen, Libin, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT), pages 577585, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--336</pages>
<contexts>
<context position="5583" citStr="Shieber et al. (1995)" startWordPosition="795" endWordPosition="798">s, as well as all the non-projective parsers and the application of the formalism to Link Grammar, are entirely new contributions of this article. The notion of a parsing schema comes from considering parsing as a deduction process which generates intermediate results called items. In particular, items in parsing schemata are sets of partial constituency trees taken from the set of all partial parse trees that do not violate the constraints imposed by a grammar. A parsing schema can be used to obtain a working implementation of a parser by using deductive engines such as the ones described by Shieber et al. (1995) and Gomez-Rodrguez, Vilares, and Alonso (2009), or the Dyna language (Eisner, Goldlust, and Smith 2005). 2. Dependency Parsing Schemata Although parsing schemata were originally defined for CFG parsers, they have since been adapted to other constituency-based grammar formalisms. This involves finding 542 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata a suitable definition of the set of structures contained in items, and a way to define deduction steps that captures the formalisms composition rules (Alonso et al. 1999). Although it is less clear how to adapt parsing schemata</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Shieber, Stuart M., Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24:336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaas Sikkel</author>
</authors>
<title>How to compare the structure of parsing algorithms.</title>
<date>1994</date>
<booktitle>In Proceedings of ASMICS Workshop on Parsing Theory,</booktitle>
<pages>2139</pages>
<location>Milano.</location>
<contexts>
<context position="40754" citStr="Sikkel (1994)" startWordPosition="7016" endWordPosition="7017">LSUP parser where these constraints have been relaxed. This implementation has the same tree building logic as the parser described by Nivre (2003), differing from it only with respect to the control structure. Thus, it can be seen as a different realization of the schema shown in Section 3.6. 4. Relations Between Dependency Parsers The parsing schemata framework can be exploited to establish how different algorithms are related, improving our understanding of the features of these parsers, and potentially exposing new algorithms that combine characteristics of existing parsers in novel ways. Sikkel (1994) defines various relations between schemata that fall into two categories: generalization relations, which are used to obtain more fine-grained versions of parsers, and filtering relations, which can be seen as the converse of generalization and are used to reduce the number of items and/or steps needed for parsing. Informally, a parsing schema can be generalized from another via the following transformations: r Item refinement: P2 is an item refinement of P1, written P1 ir P2, if there is a mapping between items in both parsers such that single items in P1 are mapped into multiple items in P2</context>
<context position="46162" citStr="Sikkel (1994)" startWordPosition="7954" endWordPosition="7955">ead-corner parser based on CFG-like rules by an item refinement in which each Collins item [i, j, h] is split into a set of items [A( ), i, j, h]. The refinement relation between these parsers only holds if for every D-rule B A there is a corresponding CFG-like rule A . . . B . . . in the grammar used by the head-corner parser. Although this parser uses three indices i, j, h, using CFG-like rules to guide linking decisions makes the h indices redundant. This simplification is an item contraction which results in an O(n3 ) head-corner parser. From here, we can follow the procedure described by Sikkel (1994) to relate this head-corner algorithm to parsers analogous to other algorithms for CFGs. In this way, we can refine the head-corner parser to a variant of the algorithm by de Vreught and Honig (1989) (Sikkel 1997), and by successive filters we reach a left-corner parser which is equivalent to the one described by Barbero et al. (1998), and a step contraction of the Earley-based dependency parser by Lombardo 556 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata and Lesmo (1996). The proofs for these relations are the same as those given by Sikkel (1994), except that the dependen</context>
</contexts>
<marker>Sikkel, 1994</marker>
<rawString>Sikkel, Klaas. 1994. How to compare the structure of parsing algorithms. In Proceedings of ASMICS Workshop on Parsing Theory, pages 2139, Milano.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaas Sikkel</author>
</authors>
<title>Parsing Schemata A Framework for Specification and Analysis of Parsing Algorithms.</title>
<date>1997</date>
<booktitle>Texts in Theoretical Computer Science An EATCS Series.</booktitle>
<publisher>Springer-Verlag,</publisher>
<location>Berlin-HeidelbergNew York.</location>
<contexts>
<context position="2856" citStr="Sikkel (1997)" startWordPosition="391" endWordPosition="392">1 9QJ, UK. E-mail: J.A.Carroll@sussex.ac.uk. School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK. E-mail: D.J.Weir@sussex.ac.uk. Submission received: 21 October 2009; revised submission received: 23 December 2010; accepted for publication: 29 January 2011. 2011 Association for Computational Linguistics \x0cComputational Linguistics Volume 37, Number 3 are able to represent non-projective structures (McDonald et al. 2005), which is important when parsing free word order languages where discontinuous constituents are common. The formalism of parsing schemata, introduced by Sikkel (1997), is a useful tool for the study of constituency parsers, supporting precise, high-level descriptions of parsing algorithms. Potential applications of parsing schemata include devising correctness proofs, extending our understanding of relationships between different algorithms, deriving new variants of existing algorithms, and obtaining efficient implementations automatically (Gomez-Rodrguez, Vilares, and Alonso 2009). The formalism was originally defined for context-free grammars (CFG) and since then has been applied to other constituency-based formalisms, such as tree-adjoining grammars (Al</context>
<context position="12805" citStr="Sikkel (1997)" startWordPosition="2023" endWordPosition="2024"> dependency parsing as a set I , where is a partition of the power set, ((G)), of the set (G). Each element of I, called an item, is a set of dependency forests for strings. For example, each member of the item [1, 5] in the item set of the parser by Yamada and Matsumoto (2003) that will be explained in Section 3.4 is a dependency forest with two projective trees, one with head 1 and the other with head 5, and such that the concatenation of their yields is 1..5. Figure 1 shows the three dependency forests that constitute the contents of this item under a specific grammar of D-rules. Following Sikkel (1997), items are sets of syntactic structures and tuples are a shorthand notation for such sets, as seen in the previous example. An alternative approach, 1 Note that the trees in a dependency forest can have different yields, because the node set of a dependency tree for a string w1 . . . wn can be any subset of [1..n]. In fact, all the forests used by the parsers in this article contain trees with non-overlapping yields, although this is not required by the definition. 544 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata Figure 1 Contents of the item [1, 5] from the Yamada and Ma</context>
<context position="14116" citStr="Sikkel 1997" startWordPosition="2242" endWordPosition="2243">3) (w5, 5) , (w3, 3) (w4, 4) , (w4, 4) (w5, 5)}. following Shieber, Schabes, and Pereira (1995), would be to define items as tuples that denote sets of syntactic structures. Although the latter approach provides more flexibility, this makes defining the relationships between parsers less straightforward. In any case, because tuple notation is used to write schemata under both approaches, the schemata we provide are compatible with both interpretations. Having defined an item set for dependency parsing, the remaining definitions are analogous to those in Sikkels theory of constituency parsing (Sikkel 1997), and are not presented in full detail. A dependency parsing system is a deduction system (I, H, D) where I is a dependency item set as defined here, H is a set containing initial items or hypotheses (not necessarily contained in I), and D ( (H I) I) is a set of deduction steps defining an inference relation . Final items in this formalism will be those containing some forest F containing a parse tree for some string w1 . . . wn. In parsers for general non-projective structures, any item containing such a tree will be called a coherent final item for w1 . . . wn. In schemata for parsers that a</context>
<context position="16458" citStr="Sikkel (1997)" startWordPosition="2645" endWordPosition="2646"> aspects of their logic: A parsing schema specifies a set of intermediate results that are obtained by the algorithm (items) and a set of operations that can be used to obtain new such results from existing ones (deduction steps); but it makes no claim about the order in which to execute the operations or the data structures to use for storing the results. 3. Projective Schemata In this section, we show how dependency parsing schemata can be used to describe several existing projective dependency parsers. 2 Coherent (final) items are called correct (final) items in the original formulation by Sikkel (1997). 3 Derivable items are called valid items in the original formulation by Sikkel (1997). 545 \x0cComputational Linguistics Volume 37, Number 3 Figure 2 Representation of the [i, j, h] item in Collinss parser, together with one of the dependency structures contained in it (left side); and of the antecedents and consequents of an L-LINK step (right side). White rectangles in an item represent intervals of nodes that have been assigned a head by the parser, and dark squares represent nodes that have no head. 3.1 Collins (1996) One of the most straightforward projective dependency parsing strategi</context>
<context position="31784" citStr="Sikkel 1997" startWordPosition="5368" endWordPosition="5369">re is nonempty). Items in this parser can represent infinite sets of extended dependency trees, as in Earleys CFG parser but unlike items in D-rule-based parsers, which are finite sets. Deduction steps: The deduction steps for this parsing schema are as follows: INITTER: [(S), 1, 0] (S) P PREDICTOR: [A( B), i, j] [B(), j + 1, j] B() P SCANNER: [A( \x02), i, h 1] [h, h, h] [A( \x02 ), i, h] wh IS A COMPLETER: [A( B), i, j] [B(), j + 1, k] [A(B ), i, k] Final items: The final item set is {[(S), 1, n]}. The schema for Lombardo and Lesmos parser is a variant of the Earley constituency parser (cf. Sikkel 1997), with minor changes to adapt it to dependency grammar (for example, the SCANNER always moves the dot over the head symbol , rather than over a terminal symbol). Analogously, other dependency parsing schemata based on CFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997): The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and the parser described by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Nivre (2003) Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing, later extended by N</context>
<context position="46375" citStr="Sikkel 1997" startWordPosition="7990" endWordPosition="7991">ry D-rule B A there is a corresponding CFG-like rule A . . . B . . . in the grammar used by the head-corner parser. Although this parser uses three indices i, j, h, using CFG-like rules to guide linking decisions makes the h indices redundant. This simplification is an item contraction which results in an O(n3 ) head-corner parser. From here, we can follow the procedure described by Sikkel (1994) to relate this head-corner algorithm to parsers analogous to other algorithms for CFGs. In this way, we can refine the head-corner parser to a variant of the algorithm by de Vreught and Honig (1989) (Sikkel 1997), and by successive filters we reach a left-corner parser which is equivalent to the one described by Barbero et al. (1998), and a step contraction of the Earley-based dependency parser by Lombardo 556 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata and Lesmo (1996). The proofs for these relations are the same as those given by Sikkel (1994), except that the dependency variants of each algorithm are simpler (due to the absence of epsilon rules and the fact that the rules are lexicalized). The names used for schemata dVH1, dVH2, dVH3, and buLC shown in Figure 6 come from Sikke</context>
<context position="120072" citStr="Sikkel (1997)" startWordPosition="21591" endWordPosition="21592"> of Eisner and Satta (1999) and Yamada and Matsumoto (2003) are not shown here for space reasons, but are presented by Gomez-Rodrguez (2009). The relationships between these three LG parsing schemata are the same as the corresponding dependency parsing schemata, that is, the LG variants of Eisner and Sattas and Yamada and Matsumotos dependency parsers are step contractions of the LG variant of Eisners parser. As with the algorithm of Sleator and Temperley, these bottomup LG parsers run in cubic time with respect to input length. 9. Conclusions and Future Work The parsing schemata formalism of Sikkel (1997) has previously been used to define, analyze, and compare algorithms for constituency-based parsing. We have shown how to extend the formalism to dependency parsers, as well as the related Link Grammar formalism. Deductive approaches have been used in the past to describe individual dependency parsers: In Kuhlmann (2007, 2010) a grammatical deduction system was used to define a parser for regular dependency grammars. 581 \x0cComputational Linguistics Volume 37, Number 3 McDonald and Nivre (2007) give an alternative framework for dependency parsers, viewing them as transition systems. That mode</context>
</contexts>
<marker>Sikkel, 1997</marker>
<rawString>Sikkel, Klaas. 1997. Parsing Schemata A Framework for Specification and Analysis of Parsing Algorithms. Texts in Theoretical Computer Science An EATCS Series. Springer-Verlag, Berlin-HeidelbergNew York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing English with a Link Grammar.</title>
<date>1991</date>
<tech>Technical report CMU-CS-91-196,</tech>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="4399" citStr="Sleator and Temperley 1991" startWordPosition="610" endWordPosition="613"> schemata to define and compare a number of existing dependency parsers (projective parsers are presented in Section 3, and their formal properties discussed in Sections 4 and 5; a number of non-projective parsers are presented in Section 6). r We present parsing algorithms for several sets of mildly non-projective dependency structures, including a parser for a new class of structures we call mildly ill-nested, which encompasses all the structures in a number of existing dependency treebanks (see Section 7). r We adapt the dependency parsing schema framework to the formalism of Link Grammar (Sleator and Temperley 1991, 1993) (see Section 8). Although some of these contributions have been published previously, this article presents them in a thorough and consistent way. The definition of dependency parsing schemata was first published by Gomez-Rodrguez, Carroll, and Weir (2008), along with some of the projective schemata presented here and their associated proofs. The results concerning mildly non-projective parsing in Section 7 were first published by GomezRodrguez, Weir, and Carroll (2008, 2009). On the other hand, the material on Nivre and Covingtons projective parsers, as well as all the non-projective </context>
<context position="101869" citStr="Sleator and Temperley (1991" startWordPosition="18160" endWordPosition="18163">For example, the structure in Figure 9 is mildly ill-nested for gap degree 2. Therefore, MGk parsers have the property of being able to parse any arbitrary dependency structure as long as we make k large enough. Structures like the one in Figure 9 do not arise in dependency treebanks. None of the treebanks for nine different languages10 contain structures that are strongly ill-nested for their gap degree (Table 1). Therefore, in any of these treebanks, the MGk parser can parse every sentence with gap degree at most k in time O(n3k+4 ). 8. Link Grammar Schemata Link Grammar (LG), introduced by Sleator and Temperley (1991, 1993), is a theory of syntax whose structural representation of sentences is closely related to projective dependency representations, but with some important differences.11 Undirected links: Like dependency formalisms, LG represents the structure of sentences as a set of links between words. However, whereas dependency links are directed, the links used in LG are undirected: There is no distinction made between heads and dependents. 10 Arabic (Hajic et al. 2004), Czech (Hajic et al. 2006), Danish (Kromann 2003), Dutch (van der Beek et al. 2002), Latin (Bamman and Crane 2006), Portuguese (Af</context>
<context position="104948" citStr="Sleator and Temperley 1991" startWordPosition="18706" endWordPosition="18709">e to build a linkage for the sentence with the grammar G. The linking requirements of a word are expressed as a set of rules specifying the labels of the links that can be established between that word and other words located to its left or to its right. Linking requirements can include constraints on the order of the links, for example, a requirement can specify that a word w can be linked to two words located to its left in such a way that the link to the farthest (leftmost) word has a particular label L2 and the link to the closest word has a label L1. We use the disjunctive form notation (Sleator and Temperley 1991) to denote linking requirements: The requirements of words are expressed as a set of disjuncts. Each disjunct corresponds to one way of satisfying the requirements of the word. We represent a disjunct for a word w as a pair of strings = (R1R2 . . . Rq, L1L2 . . . Lp) where L1, L2, . . . Lp are the labels of the links that must connect w to words located to the left of w, which must be monotonically increasing in distance from w (e.g., Lp links to the leftmost word that is directly linked to w), and R1, R2, . . . Rp are the labels of the links that must connect w to words to its right, also mon</context>
<context position="108514" citStr="Sleator and Temperley (1991)" startWordPosition="19435" endWordPosition="19438">king requirements Lj1 , Lj2 , . . . Ljl of wi. Linking requirements that are not satisfied (e.g., the requirement of a link Rk in the disjunct associated with word wi, with 0 &amp;lt; k q, such that k / {i1, . . . , ir}) are said to be unsatisfied. The definition of item sets for LG resembles those for dependency parsers (Definition 4), where items come from a partition of the set of partial linkages for a given link grammar G. With these item sets, LG parsing schemata are analogous to the dependency and constituency cases. As an example of an LG parsing schema, we describe the original LG parser by Sleator and Temperley (1991), and show how projective parsing schemata, such as those seen in Section 3, can be adapted to obtain new LG parsers. 8.1 Sleator and Temperleys LG Parser The LG parser of Sleator and Temperley (1991) is a dynamic programming algorithm that builds linkages topdown: A link between vi and vk is always added before links between vi and vj or between vj and vk, if i &amp;lt; j &amp;lt; k. This contrasts with many of the 577 \x0cComputational Linguistics Volume 37, Number 3 dependency parsers seen in previous sections (Eisner 1996; Eisner and Satta 1999; Yamada and Matsumoto 2003), which build dependency graphs </context>
<context position="110086" citStr="Sleator and Temperley 1991" startWordPosition="19731" endWordPosition="19734"> right linking requirements unsatisfied, wj is linked to words in the substring by links labelled and has left linking requirements unsatisfied, B is True if and only if there is a direct link between wi and wj, and C is True if and only if all the inner words in the span are transitively reflexively linked to one of the end words wi or wj, and have all of their linking requirements satisfied. String positions referenced by the items in ISlT range from 0 to n + 1. Position 0 corresponds to an artificial word w0 (the wall) that the LG formalism inserts at the beginning of every input sentence (Sleator and Temperley 1991). Therefore, we assume that strings are extended with this symbol. On the other hand, position n + 1 corresponds to a dummy word wn+1 that must not be linkable to any other, and is used by the parser for convenience, as in the schema for Yamada and Matsumotos dependency parser (Section 3.4). We use the notation [i, , ] as shorthand for the item [i, i, , , False, True], which is an item used to select a particular disjunct for a word wi. Deduction steps: The set of deduction steps is the following: SELECTDISJUNCT: [i, RqRq1 . . . R1, LpLp1 . . . L1] such that wi has a disjunct = (R1R2 . . . Rq,</context>
</contexts>
<marker>Sleator, Temperley, 1991</marker>
<rawString>Sleator, Daniel and Davy Temperley. 1991. Parsing English with a Link Grammar. Technical report CMU-CS-91-196, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing English with a Link Grammar.</title>
<date>1993</date>
<booktitle>In Proceedings of the Third International Workshop on Parsing Technologies (IWPT93),</booktitle>
<pages>277292</pages>
<location>Tilburg.</location>
<marker>Sleator, Temperley, 1993</marker>
<rawString>Sleator, Daniel and Davy Temperley. 1993. Parsing English with a Link Grammar. In Proceedings of the Third International Workshop on Parsing Technologies (IWPT93), pages 277292, Tilburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llus Marquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008),</booktitle>
<pages>159177</pages>
<location>Manchester.</location>
<contexts>
<context position="32682" citStr="Surdeanu et al. 2008" startWordPosition="5509" endWordPosition="5512">emata of Sikkel (1997): The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and the parser described by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Nivre (2003) Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing, later extended by Nivre, Hall, and Nilsson (2004). With linear-time performance and competitive parsing accuracy (Nivre et al. 2006; Nivre and McDonald 2008), it is one of the parsers included in the MaltParser system (Nivre et al. 2007), which is currently widely used (e.g., Nivre et al. 2007; Surdeanu et al. 2008). The parser proceeds by reading the sentence from left to right, using a stack and four different kinds of transitions between configurations. The transition system defined by all the possible configurations and transitions is nondeterministic, and machine learning techniques are used to train a mechanism that produces a deterministic parser. A deduction system describing the transitions of the parser is defined by Nivre, Hall, and Nilsson (2004), with the following set of rules that describes transitions 551 \x0cComputational Linguistics Volume 37, Number 3 between configurations (we use the</context>
<context position="59943" citStr="Surdeanu et al. 2008" startWordPosition="10497" endWordPosition="10500">o parse any possible dependency structure as long as we make k large enough. 6.4 MST Parser (McDonald et al. 2005) McDonald et al. (2005) describe a parser which finds a nonprojective analysis for a sentence in O(n2 ) time under a strong independence assumption called an edgefactored model: Each dependency decision is assumed to be independent of all the others (McDonald and Satta 2007). Despite the restrictiveness of this model, this maximum spanning tree (MST) parser achieves state-of-the-art performance for projective and non-projective structures (Che et al. 2008; Nivre and McDonald 2008; Surdeanu et al. 2008). The parser considers the weighted graph formed by all the possible dependencies between pairs of input words, and applies an MST algorithm to find a dependency tree covering all the words in the sentence and maximizing the sum of weights. The MST algorithm for directed graphs suggested by McDonald et al. (2005) is not fully constructive: It does not work by building structures and combining them into large structures until it finds the solution. Instead, the algorithm works by using a greedy strategy to select a candidate set of edges for the spanning tree, potentially creating cycles and fo</context>
</contexts>
<marker>Surdeanu, Johansson, Meyers, Marquez, Nivre, 2008</marker>
<rawString>Surdeanu, Mihai, Richard Johansson, Adam Meyers, Llus Marquez, and Joakim Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008), pages 159177, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
<author>Aravind K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics (ACL87),</booktitle>
<pages>104111</pages>
<location>Stanford, CA.</location>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>Vijay-Shanker, K., David J. Weir, and Aravind K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics (ACL87), pages 104111, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P M de Vreught</author>
<author>H J Honig</author>
</authors>
<title>A tabular bottomup recognizer.</title>
<date>1989</date>
<tech>Report 89-78,</tech>
<institution>Delft University of Technology,</institution>
<location>Delft, the Netherlands.</location>
<marker>de Vreught, Honig, 1989</marker>
<rawString>de Vreught, J. P. M. and H. J. Honig. 1989. A tabular bottomup recognizer. Report 89-78, Delft University of Technology, Delft, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of 8th International Workshop on Parsing Technologies (IWPT</booktitle>
<pages>195206</pages>
<location>Nancy.</location>
<contexts>
<context position="7379" citStr="Yamada and Matsumoto 2003" startWordPosition="1074" endWordPosition="1077">ata are used to define grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley PREDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classifiers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we use the notion of D-rules (Covington 1990). D-rules have the form (a, i) (b, j), which specifies that a word b located at position j in the input string can have the word a in position i as a dependent. Deduction steps in data-driven parsers can be</context>
<context position="12470" citStr="Yamada and Matsumoto (2003)" startWordPosition="1963" endWordPosition="1966"> say that a dependency graph G = (V, E) for a string w1 . . . wn is projective if \x06i\x07 is an interval for every i V. Definition 4 Let (G) be the set of dependency trees which are syntactically well-formed according to a given grammar G (which may be a grammar of D-rules or of CFG-like rules, as explained previously). We define an item set for dependency parsing as a set I , where is a partition of the power set, ((G)), of the set (G). Each element of I, called an item, is a set of dependency forests for strings. For example, each member of the item [1, 5] in the item set of the parser by Yamada and Matsumoto (2003) that will be explained in Section 3.4 is a dependency forest with two projective trees, one with head 1 and the other with head 5, and such that the concatenation of their yields is 1..5. Figure 1 shows the three dependency forests that constitute the contents of this item under a specific grammar of D-rules. Following Sikkel (1997), items are sets of syntactic structures and tuples are a shorthand notation for such sets, as seen in the previous example. An alternative approach, 1 Note that the trees in a dependency forest can have different yields, because the node set of a dependency tree f</context>
<context position="26095" citStr="Yamada and Matsumoto (2003)" startWordPosition="4378" endWordPosition="4381">ion 4. Final items: The set of final items is {[0, n, 0]}. By convention, parse trees have the beginning-of-sentence marker 0 as their head, as in the previous algorithm. When described for head automaton grammars (Eisner and Satta 1999), this algorithm appears to be more complex to understand and implement than the previous one, requiring four different kinds of items to keep track of the state of the automata used by the grammars. However, this abstract representation of its underlying semantics reveals that this parsing strategy is, in fact, conceptually simpler for dependency parsing. 3.4 Yamada and Matsumoto (2003) Yamada and Matsumoto (2003) define a deterministic, shift-reduce dependency parser guided by support vector machines, which achieves over 90% dependency accuracy on Section 23 of the Wall Street Journal Penn Treebank. Parsing schemata cannot specify control strategies that guide deterministic parsers; schemata work at an abstraction level, defining a set of operations without procedural constraints on the order in which they are applied. However, deterministic parsers can be viewed as optimizations of underlying nondeterministic algorithms, and we can represent the actions of the underlying p</context>
<context position="42378" citStr="Yamada and Matsumoto (2003)" startWordPosition="7297" endWordPosition="7300">er by filtering in the following ways: r Static/dynamic filtering: P1 sf/df P2 if the item set of P2 is a subset of that of P1 and P2 allows a subset of the direct inferences in P1. Sikkel (1994) explains the distinction between static and dynamic filtering, which is not used here. r Item contraction: The inverse of item refinement: P1 ic P2 if P2 ir P1. r Step contraction: The inverse of step refinement: P1 sc P2 if P2 sr P1. Many of the parsing schemata described in Section 3 can be related (see Figure 6), but for space reasons we sketch proofs for only the more interesting cases. Theorem 1 Yamada and Matsumoto (2003) sr Eisner (1996). Proof 1 It is easy to see from the schema definitions that IYM03 IEis96. We must verify that every deduction step in the Yamada and Matsumoto (2003) schema can be emulated by a sequence of inferences in the Eisner (1996) schema. For the INITTER step this is trivial as the INITTERs of both parsers are equivalent. Expressing the R-LINK step of Yamada and Matsumotos parser in the notation used for Eisner items gives: R-Link [i, j, False, False] [j, k, False, False] [i, k, False, False] (wj, j) (wk, k) This can be emulated in Eisners parser by an R-LINK step followed by a COMBIN</context>
<context position="44563" citStr="Yamada and Matsumoto (2003)" startWordPosition="7677" endWordPosition="7680">, [j, j + 1, False, False] [i, j + 1, False, False] (by COMBINESPANS) [i, j + 1, False, False], [j + 1, k, True, False] [i, k, False, False] (by COMBINESPANS) [i, k, False, False] [i, k, True, False] (by R-LINK) The proof corresponding to the L-LINK step is symmetric. As for the R-COMBINER and L-COMBINER steps in Eisner and Sattas parser, it is easy to see that they are particular cases of the COMBINESPANS step in Eisners, and therefore can be emulated by a single application of COMBINESPANS. \x02 Note that, in practice, these two relations mean that the parsers by Eisner and Satta (1999) and Yamada and Matsumoto (2003) are more efficient, at the schema level, than that of Eisner (1996), in that they generate fewer items and need fewer steps to perform the same deductions. These two parsers also have the interesting property that they use disjoint item sets (one uses items representing trees while the other uses items representing pairs of trees); and the union of these disjoint sets is the item set used by Eisners parser. The optimization in Yamada and Matsumotos parser comes from contracting deductions in Eisners parser so that linking operations are immediately followed by combining operations; whereas Ei</context>
<context position="47576" citStr="Yamada and Matsumoto (2003)" startWordPosition="8178" endWordPosition="8181">n Figure 6 come from Sikkel (1994, 1997). These dependency parsing schemata are versions of the homonymous schemata whose complete description can be found in Sikkel (1997), adapted for dependency parsing. Gomez-Rodrguez (2009) gives a more thorough explanation of these relations and schemata. 5. Proving Correctness Another use of the parsing schemata framework is that it is helpful in establishing the correctness of a parser. Furthermore, relations between schemata can be used to establish the correctness of one schema from that of related ones. In this section, we show that the schemata for Yamada and Matsumoto (2003) and Eisner and Satta (1999) are correct, and use this to prove the correctness of the schema for Eisner (1996). Theorem 3 The Eisner and Satta (1999) parsing schema is correct. Proof 3 To prove correctness, we must show both soundness and completeness. To verify soundness we need to check that every individual deduction step in the parser infers a coherent consequent item when applied to coherent antecedents (i.e., in this case, that steps always generate non-empty items that conform to the definition in Section 3.3). This is shown by checking that, given two antecedents of a deduction step t</context>
<context position="50768" citStr="Yamada and Matsumoto (2003)" startWordPosition="8802" endWordPosition="8805">ongs to a coherent item [j, k, j]. Because these three items have a length strictly less than l, by the inductive hypothesis, they are derivable. Thus the item [i, k, i] is also derivable, as it can be obtained from these derivable items by the following inferences: [i, l 1, i], [l, j, j] [i, j, i] (by the L-LINK step) [i, j, i], [j, k, j] [i, k, i] (by the L-COMBINER step) This proves that all coherent items of length l which are of the form [i, k, i] are derivable under the induction hypothesis. The same can be shown for items of the form [i, k, k] by symmetric reasoning. \x02 Theorem 4 The Yamada and Matsumoto (2003) parsing schema is correct. Proof 4 Soundness is verified by building forests for the consequents of steps from those corresponding to the antecedents. To prove completeness we use strong induction on the length of items, where the length of an item [i, j] is defined as j i + 1. The induction step involves considering any coherent item [i, k] of length l &amp;gt; 2 (l = 2 is the base case here because items of length 2 are generated by the Initter step) and showing that it can be inferred from derivable antecedents of length less than l, so it is derivable. If l &amp;gt; 2, either i has at least one right d</context>
<context position="54560" citStr="Yamada and Matsumoto (2003)" startWordPosition="9461" endWordPosition="9464">strategy similar to Lombardo and Lesmo (1996), but with the following initializer step instead of the INITTER and PREDICTOR: INITTER: [A(), i, i 1] A() P 1 i n The initialization step specified by Kahane, Nasr, and Rambow (1998) differs from this (directly consuming a nonterminal from the input) but this gives an incomplete algorithm. The problem can be fixed either by using the step shown here instead (bottomup Earley strategy) or by adding an additional step turning it into a bottomup left-corner parser. 6.2 Attardi (2006) The non-projective parser of Attardi (2006) extends the algorithm of Yamada and Matsumoto (2003), adding additional shift and reduce actions to handle non-projective dependency structures. These extra actions allow the parser to link to nodes that are several positions deep in the stack, creating non-projective links. In particular, Attardi uses six non-projective actions: two actions to link to nodes that are two positions deep, another two actions for nodes that are three positions deep, and a third pair of actions that generalizes the previous ones to n positions deep for any n. Thus, the maximum depth in the stack to which links can be created can be configured according to the actio</context>
<context position="59100" citStr="Yamada and Matsumoto (2003)" startWordPosition="10365" endWordPosition="10368">ed by MHk+1. The MH parser, obtained by assuming that the number of trees per forest is unbounded, is equivalent to Att, and therefore correct with respect to 560 \x0cGomez-Rodrguez, Carroll, and Weir Dependency Parsing Schemata the set of non-projective dependency structures. For finite values of k, MHd+2 is a static filter of Attd, because its sets of items and deduction steps are subsets of those of Attd. Therefore, the set of structures parsed by MHd+2 is also a subset of those parsed by Attd. The complexity of the MHk parser is O(nk ). For k = 3, MH3 is a step refinement of the parser by Yamada and Matsumoto (2003) that parses projective structures only, but by modifying the bound k we can define polynomial-time algorithms that parse larger sets of non-projective dependency structures. The MHk parser has the property of being able to parse any possible dependency structure as long as we make k large enough. 6.4 MST Parser (McDonald et al. 2005) McDonald et al. (2005) describe a parser which finds a nonprojective analysis for a sentence in O(n2 ) time under a strong independence assumption called an edgefactored model: Each dependency decision is assumed to be independent of all the others (McDonald and </context>
<context position="109082" citStr="Yamada and Matsumoto 2003" startWordPosition="19534" endWordPosition="19537">ibe the original LG parser by Sleator and Temperley (1991), and show how projective parsing schemata, such as those seen in Section 3, can be adapted to obtain new LG parsers. 8.1 Sleator and Temperleys LG Parser The LG parser of Sleator and Temperley (1991) is a dynamic programming algorithm that builds linkages topdown: A link between vi and vk is always added before links between vi and vj or between vj and vk, if i &amp;lt; j &amp;lt; k. This contrasts with many of the 577 \x0cComputational Linguistics Volume 37, Number 3 dependency parsers seen in previous sections (Eisner 1996; Eisner and Satta 1999; Yamada and Matsumoto 2003), which build dependency graphs bottomup. Item set: The item set for Sleator and Temperleys parser is ISlT = {[i, j, , , B, C] |0 i j n + 1 B, C {True, False} and , , , are strings of link labels} where an item [i, j, , , B, C] represents the set of partial linkages over the substring wi . . . wj of the input, wi is linked to words in that substring by links labelled and has right linking requirements unsatisfied, wj is linked to words in the substring by links labelled and has left linking requirements unsatisfied, B is True if and only if there is a direct link between wi and wj, and C is Tr</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of 8th International Workshop on Parsing Technologies (IWPT 2003), pages 195206, Nancy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time .</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>2</issue>
<marker>Younger, 1967</marker>
<rawString>Younger, Daniel H. 1967. Recognition and parsing of context-free languages in time . Information and Control, 10(2):189208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>