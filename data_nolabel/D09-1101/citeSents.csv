These features have largely been employed by state-of-the-art learning-based coreference systems (e.g., CITATION, CITATIONb), CITATION), and are computed automatically,,
onastic its have been detected using heuristics (e.g., CITATION) and learning-based techniques such as rule learning (e.g., CITATION), kernels (e.g., CITATION), and distributional methods (e.g., CITATION),,
Motivated by previous work (CITATION; CITATION; CITATION), we create cluster-level features from mention-pair features using four predicates: NONE, MOST-FALSE, MOST-TRUE, and ALL,,
Equally importantly, cluster rankers are conceptually simple and easy to implement and do not rely on sophisticated training and inference procedures to make coreference decisions in dependent relation to each other, unlike relational coreference models (see CITATION),,
The notion of ranking candidate antecedents can be traced back to centering algorithms, many of which use grammatical roles to rank forward-looking centers (see CITATION, CITATION, and CITATION),,
Given an active mention mk, we follow CITATION and use an independently-trained classifier to determine whether mk is discourse-new,,
Non-anaphoric definite descriptions have been detected using heuristics (e.g., CITATION) and unsupervised methods (e.g., CITATION),,
Grammatical (1): The part-of-speech (POS) tag of wi obtained using the Stanford log-linear POS tagger CITATION,,
Pleonastic its have been detected using heuristics (e.g., CITATION) and learning-based techniques such as rule learning (e.g., CITATION), kernels (e.g.,,
e, CITATION train a mention-ranking model,,
s of NPs have been built using heuristics (e.g., CITATION) and modeled generatively (e.g., CITATION) and discriminatively (e.g., CITATION),,
Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., CITATION, CITATIONb), CITATION, CITATION),,
There have also been attempts to perform joint inference for discourse-new detection and coreference resolution using integer linear programming (ILP), where a discourse-new classifier and a coreference classifier are trained independently of each other, and then ILP is applied as a post-processing step to jointly infer discourse-new and coreference decisions so that they are consistent with each other (e.g., CITATION),,
For both types of mentions, the improvements over the corresponding results for the entity-mention baseline 7 We use Approximate Randomization CITATION for testing statistical significance, with p set to 0.05,,
The discourse-new classifier used in the resolution step is trained with 26 of the 37 features2 described in CITATIONa) that are deemed useful for distinguishing between anaphoric and non-anaphoric mentions,,
Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer CITATION,,
More recently, CITATION have proposed another entity-mention model trained by inductive logic programming,,
To train a mention-pair classifier, we use the SVM learning algorithm from the SVMlight package CITATION, converting all multi-valued features into an equivalent set of binary-valued features,,
If the response is generated using true mentions, then every mention in the response is mapped to some mention in the key and vice versa; in other words, there are no twinless (i.e., unmapped) mentions CITATION,,
Like many heuristic-based pronoun resolvers (e.g., CITATION), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors,,
As a result, errors in discourse-new detection could be propagated to the resolver, possibly leading to a deterioration of coreference performance (see CITATIONa)),,
Following CITATION, we recast mention extraction as a sequence labeling task, where we assign to each token in a test text a label that indicates whether it begins a mention, is inside a mention, or is outside a mention,,
General discourse-new detectors that are applicable to different types of NPs have been built using heuristics (e.g., CITATION) and modeled generatively (e.g., CITATION) and discriminatively (e.g., CITATION),,
We train our first baseline, the mention-pair coreference classifier, using the SVM learning algorithm as implemented in the SVMlight package CITATION.6 Results of this baseline using true mentions and system mentions, shown in row 1 of Tables 3 and 4, are reported in terms of recall (R), precision (P), and F-score (F) provided by the three scoring pro6 For this and subsequent uses of the SVM learner in our experiments, we set all parameters to their default values,,
ors that capture the salience of a cluster, and can therefore be viewed as a simple model of attentional state (see CITATION) realized by coreference clusters,,
While the insignificant performance difference is somewhat surprising given the improved expressiveness of entitymention models over mention-pair models, similar trends have been reported by CITATION,,
It is worth noting that researchers typically adopt a pipeline coreference architecture, performing discourse-new detection prior to coreference resolution and using the resulting information to prevent a coreference system from resolving mentions that are determined to be discourse-new (see CITATION for an overview),,
To score the output of a coreference model, we employ three scoring programs: MUC CITATION, B3 CITATION, and 3-CEAF CITATION,,
As mentioned before, CITATION train a mention-ranking model,,
Pleonastic its have been detected using heuristics (e.g., CITATION) and learning-based techniques such as rule learning (e.g., CITATION), kernels (e.g., CITATION), and distributional methods (e.g., CITATION),,
ennedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., CITATION), kernels (e.g., CITATION), and distributional methods (e.g., CITATION),,
To address the second weakness, researchers have investigated the acquisition of entity-mention coreference models (e.g., CITATION, CITATION),,
CITATION represent one of the earliest attempts to investigate learning-based entity-mention models,,
As a result, their cluster-ranking model employs only factors that capture the salience of a cluster, and can therefore be viewed as a simple model of attentional state (see CITATION) realized by coreference clusters,,
oring programs: MUC CITATION, B3 CITATION, and 3-CEAF CITATION,,
To address the first weakness, researchers have attempted to train a mention-ranking model for determining which candidate antecedent is most probable given an active mention (e.g., CITATION),,
As mentioned previously, the work most related to ours is CITATION, whose goal is to perform pronoun resolution by assigning an anaphoric pronoun to the highest-scored preceding cluster,,
Following CITATION, we create (1) a positive instance for each discourse-old mention mk and its closest antecedent mj; and (2) a negative instance for mk paired with each of the intervening mentions, mj+1, mj+2, ,,
(1999) and CITATION, as described below,,
While entity-mention models have previously been shown to be worse or at best marginally better than their mention-pair counterparts (CITATION; CITATION), our cluster-ranking models, which are a natural extension of entity-mention models, significantly outperformed all competing approaches,,
