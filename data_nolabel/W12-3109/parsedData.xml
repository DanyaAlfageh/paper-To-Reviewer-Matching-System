<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.5696975">
b&amp;apos;Proceedings of the 7th Workshop on Statistical Machine Translation, pages 9195,
Montreal, Canada, June 7-8, 2012. c
</bodyText>
<sectionHeader confidence="0.410119" genericHeader="abstract">
2012 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.834439">
Black Box Features for the WMT 2012 Quality Estimation Shared Task
</title>
<author confidence="0.985934">
Christian Buck
</author>
<affiliation confidence="0.9926285">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.955794">
Edinburgh, UK, EH8 9AB
</address>
<email confidence="0.995374">
christian.buck@ed.ac.uk
</email>
<sectionHeader confidence="0.988631" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.989300375">
In this paper we introduce a number of new
features for quality estimation in machine
translation that were developed for the WMT
2012 quality estimation shared task. We find
that very simple features such as indicators of
certain characters are able to outperform com-
plex features that aim to model the connection
between two languages.
</bodyText>
<sectionHeader confidence="0.958018" genericHeader="introduction">
1 Introduction and Task
</sectionHeader>
<bodyText confidence="0.996986">
This paper describes the features and setup used in
our submission to the WMT 2012 quality estimation
(QE) shared task. Given a machine translation (MT)
system and a corpus of its translations which have
been rated by humans, the task is to build a predic-
tor that can accurately estimate the quality of fur-
ther translations. The human ratings range from 1
(incomprehensible) to 5 (perfect translation) and are
given as the mean rating of three different judges.
Formally we are presented with a source sentence
</bodyText>
<footnote confidence="0.588786666666667">
fJ
1 and a translation eI
1 and we need to assign a score
</footnote>
<equation confidence="0.3032975">
S(fJ
1 , eI
</equation>
<bodyText confidence="0.712239">
1) [1, 5] or, in the ranking task, order the
source-translation pairs by expected quality.
</bodyText>
<sectionHeader confidence="0.996718" genericHeader="method">
2 Resources
</sectionHeader>
<bodyText confidence="0.99041475">
The organizers have made available a baseline QE
system that consists of a number of well established
features (Blatz et al., 2004) and serves as a starting
point for development. Furthermore the MT system
that generated the translations is available along with
its training data. Compared to the large training cor-
pus of the MT engine, the QE system is based on a
much smaller training set as detailed in Table 1.
</bodyText>
<figure confidence="0.1746975">
# sentences
europarl-nc 1,714,385
train 1,832
test 422
</figure>
<tableCaption confidence="0.996104">
Table 1: Corpus statistics
</tableCaption>
<sectionHeader confidence="0.999338" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.945007086956522">
In the literature (Blatz et al., 2004) a large number
of features have been considered for confidence es-
timation. These can be grouped into four general
categories:
1. Source features make a statement about the
source sentence, assessing the difficulty of
translating a particular sentence with the sys-
tem at hand. Some sentences may be very easy
to translate, e.g. short and common phrases,
while long and complex sentences are still be-
yond the systems capabilities.
2. Translation features model the connection be-
tween source and target. While this is very
closely related to the general problem of ma-
chine translation, the advantage in confidence
estimation is that we can exercise unconstruc-
tive criticism, i.e. point out errors without of-
fering a better translation. In addition, there is
no need for an efficient search algorithm, thus
allowing for more complex models.
3. Target features judge the translation of the sys-
tem without regarding in which way it was
produced. They often resemble the language
</bodyText>
<page confidence="0.989368">
91
</page>
<bodyText confidence="0.9768552">
\x0cmodel used in the noisy channel formulation
(Brown et al., 1993) but can also pinpoint more
specific issues. In practice, the same features as
for the source side can be used; the interpreta-
tion however is different.
4. Engine features are often referred to as glass
box features (Specia et al., 2009). They de-
scribe the process which produced the transla-
tion in question and usually rely on the inner
workings of the MT system. Examples include
model scores and word posterior probabilities
(WPP) (Ueffing et al., 2003).
In this work we focus on the first three categories
and ignore the particular system that produced the
translations. Such features are commonly referred
to as black box features. While some glass box fea-
tures, e.g. word posterior probabilities, have led to
promising results in the past, we chose to explore
new features potentially applicable to translations
from any source, e.g. translations found on the web.
</bodyText>
<subsectionHeader confidence="0.999361">
3.1 Binary Indicators
</subsectionHeader>
<bodyText confidence="0.9972764">
MTranslatability (Bernth and Gdaniec, 2001) gives a
notion of the structural complexity of a sentence that
relates to the quality of the produced translation. In
the literature, several characteristics that may hin-
der proper translation have been identified, among
them poor grammar and misplaced punctuation. As
a very simple approximation we implement binary
indicators that detect clauses by looking for quota-
tion marks, hyphens, commas, etc. Another binary
feature marks numbers and uppercase words.
</bodyText>
<subsectionHeader confidence="0.997377">
3.2 Named Entities
</subsectionHeader>
<bodyText confidence="0.931788642857143">
Another aspect that might pose a potential problem
to MT is the occurrence of words that were only ob-
served a few times or in very particular contexts, as
it is often the case for Named Entities. We used the
Stanford NER Tagger (Finkel et al., 2005) to detect
words that belong to one of four groups: Person, Lo-
cation, Organization and Misc. Each group is repre-
sented by a binary feature.
Counts are given in Table 2. The test set has sig-
nificantly less support for the Misc category, possi-
bly hinting that this data was taken from a different
source or document. To avoid the danger of biasing
train (src) test (src)
abs rel abs rel
</bodyText>
<table confidence="0.99806975">
Person 623 34% 141 33%
Location 479 26% 99 23%
Organization 505 28% 110 26%
Misc 428 23% 53 13%
</table>
<tableCaption confidence="0.998704">
Table 2: Distribution of Named Entities. The counts are
</tableCaption>
<bodyText confidence="0.99241425">
based on a binary features, i.e. multiple occurrences are
treated as a single one.
the classifier we decided not to use the Misc indica-
tor in our experiments.
</bodyText>
<subsectionHeader confidence="0.99922">
3.3 Backoff Behavior
</subsectionHeader>
<bodyText confidence="0.99941325">
In related work (Raybaud et al., 2011) the backoff
behavior of a 3-gram LM was found to be the most
powerful feature for word level QE. We compute for
each word the longest seen n-gram (up to n = 4)
and take the average length as a feature. N-grams at
the beginning of a sentence are extended with &amp;lt;s&amp;gt;
tokens to avoid penalizing short sentences. This is
done on both the source and target side.
</bodyText>
<subsectionHeader confidence="0.882069">
3.4 Discriminative Word Lexicon
</subsectionHeader>
<bodyText confidence="0.8004945">
Following the approach of Mauser et al. (2009) we
train log-linear binary classifiers that directly model
</bodyText>
<figure confidence="0.8977339375">
p(e|fJ
1 ) for each word e eI
1:
p(e|fJ
1 ) =
exp
\x10P
ffJ
1
e,f
\x11
1 + exp
\x10P
ffJ
1
e,f
</figure>
<equation confidence="0.899883">
\x11 (1)
</equation>
<bodyText confidence="0.987311">
where e,f are the trained model weights. Please
note that this introduces a global dependence on the
source sentence so that every source word may influ-
ence the choice of all words in eI
1 as opposed to the
local dependencies found in the underlying phrase-
based MT system.
Assuming independence among the words in the
translated sentence we could compute the probabil-
ity of the sentence pair as:
</bodyText>
<equation confidence="0.716936125">
p(eI
1|fJ
1 ) =
Y
eeI
1
p(e|fJ
1 )
Y
e/
eI
1
1 p(e|fJ
1 )
\x01
. (2)
</equation>
<bodyText confidence="0.981647">
In practice the second part of Equation (2) is too
noisy to be useful given the large number of words
</bodyText>
<page confidence="0.976716">
92
</page>
<bodyText confidence="0.6132565">
\x0csource resumption of the session
target reanudacion del perodo de sesiones
</bodyText>
<tableCaption confidence="0.974544">
Table 3: Example entry of filtered training corpus.
</tableCaption>
<bodyText confidence="0.921488333333333">
that do not appear in the sentence at hand. We there-
fore focus on the observed words and use the geo-
metric mean of their individual probabilities:
</bodyText>
<equation confidence="0.9968568">
xDWL(fJ
1 , eI
1) =
Y
eeI
1
p(e|fJ
1 )
1/I
. (3)
</equation>
<bodyText confidence="0.9916585">
We also compute the probability of the lowest
scoring word as an additional feature:
</bodyText>
<equation confidence="0.867726428571429">
xDWLmin(fJ
1 , eI
1) = min
eeI
1
p(e|fJ
1 ). (4)
</equation>
<subsectionHeader confidence="0.606042">
3.5 Neural Networks
</subsectionHeader>
<bodyText confidence="0.939215">
We seek to directly predict the words in eI
</bodyText>
<sectionHeader confidence="0.746455" genericHeader="method">
1 using
</sectionHeader>
<bodyText confidence="0.999060153846154">
a neural network. In order to do so, both source
and target sentence are encoded as high dimensional
vectors in which positive entries mark the occur-
rence of words. This representation is commonly
referred to as the vector space model and has been
successfully used for information retrieval.
The dimension of the vector representation is de-
termined by the respective sizes of the source and
target vocabulary. Without further pre-processing
we would need to learn a mapping from a 90k (|Vf |)
to a 170k (|Ve|) dimensional space. Even though our
implementation is specifically tailored to exploit the
sparsity of the data, such high dimensionality makes
training prohibitively expensive.
Two approaches to reduce dimensionality are ex-
plored in this work. First, we simply remove all
words that never occur in the QE data of 2,254 sen-
tences from the corpus leaving 8,365 input and 9,000
output nodes. This reduces the estimated training
time from 11 days to less than 6 hours per iteration1.
Standard stochastic gradient decent on a three-layer
feed-forward network is used.
As shown in Table 3 the filtering can lead to arti-
facts in which case an erroneous mapping is learned.
Moreover the filtering approach does not scale well
as the QE corpus and thereby the vocabulary grows.
</bodyText>
<page confidence="0.842863">
1
</page>
<bodyText confidence="0.999681606060606">
using a 2.66 GHz Intel Xeon and 2 threads
Our second approach to reduce dimensionality
uses the hashing trick (Weinberger et al., 2009): a
hash function is applied to each word and the sen-
tence is represented by the hashed values which
are again transformed using vector space model as
above. The dimensionality reduction is due to the
fact that there are less possible hash values than
words in the vocabulary. To reduce the loss of infor-
mation due to collisions, several different hash func-
tions are used. The resulting vector representation
closely resembles a Bloom Filter (Bloom, 1970).
This approach scales well but introduces two new
parameters: the number of hash functions to use
and the dimensionality of the resulting space. In
our experiments we have used SHA-1 hashes with
three different salts of which we used the first 12
bits, thereby mapping the sentences into a 4096-
dimensional space.
The results presented in Section 4 based on net-
works with 500 hidden nodes which were trained for
at least 10 iterations. The networks are not trained
until convergence due to time constraints; additional
training iterations will likely result in better per-
formance. Experiments using 250 or 1000 hidden
nodes showed very similar results.
After the models are trained we compare the pre-
dicted and the observed target vectors and derive
two features: (i) the euclidean distance, denoted as
NNdist and HNNdist for the filtered and hashed ver-
sions respectively and (ii) the geometric mean of
those dimensions where we expect a positive value,
denoted as NNprop+ and HNNprob+ in Table 5.
</bodyText>
<subsectionHeader confidence="0.922536">
3.6 Edit Distance
Using Levenshtein Distance we computed the dis-
</subsectionHeader>
<bodyText confidence="0.999092888888889">
tance to the closest entry in the training corpus. The
idea is that a sentence that was already seen almost
identically would be easier to translate. Likewise,
a translation that is very close to an element of the
corpus is likely to be a good translation. This was
performed for both source and target side and on
character as well as on word level giving a total of
four (EDIT) scores. The scores are normalized by
the length of the respective lines.
</bodyText>
<page confidence="0.995894">
93
</page>
<table confidence="0.99387025">
\x0csource corpus &amp;quot;
europarl-nc 37 227 25,637
train 0 0 641
test 78 76 100
</table>
<tableCaption confidence="0.999416">
Table 4: Counts of different quotation mark characters.
</tableCaption>
<sectionHeader confidence="0.998774" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999152166666667">
In this work we focus on the prediction of human
assessment of translation quality, i.e. the regression
task of the WMT12 QE shared task. Our submission
for the ranking task is derived from the order implied
by the predicted scores without further re-ranking.
In general our efforts were directed towards fea-
ture engineering and not to the machine learning as-
pects. Therefore, we apply a standard pipeline and
use neural networks for regression. All parameter
tuning is performed using 5-fold cross validation on
the baseline set of 17 features as provided by the or-
ganizers.
</bodyText>
<subsectionHeader confidence="0.999666">
4.1 Preprocessing and Analysis
</subsectionHeader>
<bodyText confidence="0.998492434782609">
To avoid including our own judgment, no more than
the first ten lines of the test data were visually in-
spected in order to ensure that the training and test
data was preprocessed in the same manner. Further-
more, the distribution of individual characters was
investigated. As shown in Table 4, the test data dif-
fers from the training corpus in treatment of quo-
tation marks. Hence, we replaced all typographi-
cal quotation marks ( , ) with the standard double
quote symbol (&amp;quot;).
Prior to computation of the features described in
Subsections 3.3, 3.4 and 3.5 all numbers are re-
placed with a special $number token.
Baseline features are used without further scal-
ing; experiments where all features were scaled to
the [0, 1] range showed a drop in accuracy.
While we implemented the training ourselves for
the features presented in Subsection 3.5, the open
source neural network library FANN2 is used for
all experiments in this section. As the performance
of individual classifiers shows a high variance, pre-
sumably due to local minima, all experiments are
conducted using ensembles on 500 networks trained
</bodyText>
<page confidence="0.941761">
2
</page>
<table confidence="0.9892168">
http://leenissen.dk/fann/wp/
Feature (Section) MAE RMSE |PCC|
BACKOFF (3.3) 0.0 0.0
INDICATORS (3.1) +0.5 +0.7
NER (3.2) +0.5 +0.4
DWLmin (3.4) 0.1 0.1 0.19
DWL (3.4) 0.0 0.1 0.36
EDIT (3.6) - tgt words 0.0 0.0 0.32
EDIT (3.6) - tgt chars 0.1 0.0 0.27
EDIT (3.6) - src words 0.0 0.0 0.36
EDIT (3.6) - src chars +0.2 +0.1 0.37
NNdist (3.5) 0.0 0.0 0.35
NNprob+ (3.5) +0.1 +0.2 0.35
HNNdist (3.5) 0.0 0.0 0.37
HNNprob+ (3.5) +0.1 +0.1 0.35
</table>
<tableCaption confidence="0.995939">
Table 5: Analysis of individual features using 5-fold
</tableCaption>
<bodyText confidence="0.938455">
cross-validation. Positive values indicate improvement
over a baseline of MAE 57.7% and RMSE 72.7%; e.g.
including the DWL feature actually worsens RMSE from
72.7% to 72.8%.
The last column gives the Pearson correlation coefficient
between the feature and the score if the feature is a single
column. This information was not used in feature selec-
tion as it is not based on cross validation.
with random initialization. Their consensus is com-
puted as the average of the individual predictions.
</bodyText>
<subsectionHeader confidence="0.983074">
4.2 Feature Evaluation
</subsectionHeader>
<bodyText confidence="0.999697263157895">
To evaluate the contribution of individual features,
each feature is tested in conjunction with all base-
line features, using the parameters that were opti-
mized on the baseline set. This slightly favors the
baseline features but we still expect that expressive
additional features lead to a noticeable performance
gain. The results are detailed in Table 5. In addi-
tion to the main evaluation metrics, mean average
error (MAE) and root mean squared error (RMSE),
we report the Pearson correlation coefficient (PCC)
as a measure of predictive strength of a single fea-
ture. Because features are not used alone this does
not directly translate into overall performance. Still,
it can be observed that our proposed features show
good correlation to the target variable. For compari-
son, among the baseline features only 2 of 17 reach
a PCC of over 0.3.
While the results generally remain inconclusive,
some very simple features that indicate difficulties
</bodyText>
<page confidence="0.988542">
94
</page>
<bodyText confidence="0.997792066666667">
\x0cfor the translation engine show good performance.
In particular binary markers of named entities and
and the indicator features introduced in Subsection
3.1 perform well. Further experiments with the latter
show their contribution to the systems performance
can be attributed to a single feature: the indicator of
the genitive case, i.e. occurrences of s or s.
Testing more combinations of simple and com-
plex features may lead to improvements at the risk
of over-fitting on the cross validation setup. As a
simple remedy several feature sets were created at
random, always combining all baseline features and
several new features presented in this paper. Averag-
ing of the individual results of all sets that performed
better than the baseline resulted in our submission.
</bodyText>
<subsectionHeader confidence="0.901265">
4.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999816071428572">
Of all the features detailed only a few lead to a con-
siderable improvement. This is also reflected by our
results on the test data which are nearly indistin-
guishable from the performance of the baseline sys-
tem. While this is disappointing, our more complex
features introduce a number of free parameters and
further experimentation will be needed to conclu-
sively assess their usefulness. In particular, features
based on neural networks can be further optimized
and tested in other settings.
Even though the machine learning aspects of this
task are not the focus of this work we are confident
that the proposed setup is sound and can be reused
in further evaluations.
</bodyText>
<sectionHeader confidence="0.998381" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999695642857143">
We described a number of new features that can be
used to predict human judgment of translation qual-
ity. Results suggest pointing out sentences that are
hard to translate, e.g. because they are too complex,
is a promising approach.
We presented a detailed evaluation of the utility
of individual features and a solid baseline setup for
further experimentation. The system, based on an
ensemble of neural networks, is insensitive to pa-
rameter settings and yields competitive results.
Our new features can potentially be applied for a
multitude of applications and may deliver insights
into the fundamental problems that cause translation
errors, thus aiding the progress in MT research.
</bodyText>
<sectionHeader confidence="0.978199" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.952711333333333">
This work was supported by the MateCAT project,
which is funded by the EC under the 7th Framework
Programme.
</bodyText>
<sectionHeader confidence="0.977801" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999575808510638">
Arendse Bernth and Claudia Gdaniec. 2001. Mtranslata-
bility. Machine Translation, 16(3):175218, Septem-
ber.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation for
machine translation. In Proceedings of Coling 2004,
pages 315321, Geneva, Switzerland, August.
Burton H. Bloom. 1970. Space/time trade-offs in hash
coding with allowable errors. Communications of the
ACM, 13(7):422426, July.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263312.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL 2005, pages
363370, Stroudsburg, PA, USA.
Arne Mauser, Sasa Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 210217, Singapore, August.
Sylvain Raybaud, David Langlois, and Kamel Smali.
2011. this sentence is wrong. detecting errors in
machine-translated sentences. Machine Translation,
25(1):134, March.
Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco
Turchi, and Nello Cristianini. 2009. Estimating
the sentence-level quality of machine translation sys-
tems. In Proceedings of the 13th Annual Conference
of the European Association for Machine Translation,
EAMT-2009, pages 2835, Barcelona, Spain, May.
Nicola Ueffing, Klaus Macherey, and Hermann Ney.
2003. Confidence measures for statistical machine
translation. In Machine Translation Summit, pages
394401, New Orleans, LA, September.
Kilian Q. Weinberger, Anirban Dasgupta, John Langford,
Alexander J. Smola, and Josh Attenberg. 2009. Fea-
ture hashing for large scale multitask learning. In
ICML 09: Proceedings of the 26th Annual Interna-
tional Conference on Machine Learning, ACM Inter-
national Conference Proceeding Series, pages 1113
1120, Montreal, Quebec, Canada, June.
</reference>
<page confidence="0.805047">
95
</page>
<figure confidence="0.375473">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.301991">
<note confidence="0.831579666666667">b&amp;apos;Proceedings of the 7th Workshop on Statistical Machine Translation, pages 9195, Montreal, Canada, June 7-8, 2012. c 2012 Association for Computational Linguistics</note>
<title confidence="0.436338">Black Box Features for the WMT 2012 Quality Estimation Shared Task</title>
<author confidence="0.999986">Christian Buck</author>
<affiliation confidence="0.999885">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.980736">Edinburgh, UK, EH8 9AB</address>
<email confidence="0.997105">christian.buck@ed.ac.uk</email>
<abstract confidence="0.998848888888889">In this paper we introduce a number of new features for quality estimation in machine translation that were developed for the WMT 2012 quality estimation shared task. We find that very simple features such as indicators of certain characters are able to outperform complex features that aim to model the connection between two languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arendse Bernth</author>
<author>Claudia Gdaniec</author>
</authors>
<date>2001</date>
<journal>Mtranslatability. Machine Translation,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="3922" citStr="Bernth and Gdaniec, 2001" startWordPosition="636" endWordPosition="639"> usually rely on the inner workings of the MT system. Examples include model scores and word posterior probabilities (WPP) (Ueffing et al., 2003). In this work we focus on the first three categories and ignore the particular system that produced the translations. Such features are commonly referred to as black box features. While some glass box features, e.g. word posterior probabilities, have led to promising results in the past, we chose to explore new features potentially applicable to translations from any source, e.g. translations found on the web. 3.1 Binary Indicators MTranslatability (Bernth and Gdaniec, 2001) gives a notion of the structural complexity of a sentence that relates to the quality of the produced translation. In the literature, several characteristics that may hinder proper translation have been identified, among them poor grammar and misplaced punctuation. As a very simple approximation we implement binary indicators that detect clauses by looking for quotation marks, hyphens, commas, etc. Another binary feature marks numbers and uppercase words. 3.2 Named Entities Another aspect that might pose a potential problem to MT is the occurrence of words that were only observed a few times </context>
</contexts>
<marker>Bernth, Gdaniec, 2001</marker>
<rawString>Arendse Bernth and Claudia Gdaniec. 2001. Mtranslatability. Machine Translation, 16(3):175218, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>315321</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="1526" citStr="Blatz et al., 2004" startWordPosition="245" endWordPosition="248">ch have been rated by humans, the task is to build a predictor that can accurately estimate the quality of further translations. The human ratings range from 1 (incomprehensible) to 5 (perfect translation) and are given as the mean rating of three different judges. Formally we are presented with a source sentence fJ 1 and a translation eI 1 and we need to assign a score S(fJ 1 , eI 1) [1, 5] or, in the ranking task, order the source-translation pairs by expected quality. 2 Resources The organizers have made available a baseline QE system that consists of a number of well established features (Blatz et al., 2004) and serves as a starting point for development. Furthermore the MT system that generated the translations is available along with its training data. Compared to the large training corpus of the MT engine, the QE system is based on a much smaller training set as detailed in Table 1. # sentences europarl-nc 1,714,385 train 1,832 test 422 Table 1: Corpus statistics 3 Features In the literature (Blatz et al., 2004) a large number of features have been considered for confidence estimation. These can be grouped into four general categories: 1. Source features make a statement about the source sente</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence estimation for machine translation. In Proceedings of Coling 2004, pages 315321, Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burton H Bloom</author>
</authors>
<title>Space/time trade-offs in hash coding with allowable errors.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>7</issue>
<contexts>
<context position="8980" citStr="Bloom, 1970" startWordPosition="1536" endWordPosition="1537">e vocabulary grows. 1 using a 2.66 GHz Intel Xeon and 2 threads Our second approach to reduce dimensionality uses the hashing trick (Weinberger et al., 2009): a hash function is applied to each word and the sentence is represented by the hashed values which are again transformed using vector space model as above. The dimensionality reduction is due to the fact that there are less possible hash values than words in the vocabulary. To reduce the loss of information due to collisions, several different hash functions are used. The resulting vector representation closely resembles a Bloom Filter (Bloom, 1970). This approach scales well but introduces two new parameters: the number of hash functions to use and the dimensionality of the resulting space. In our experiments we have used SHA-1 hashes with three different salts of which we used the first 12 bits, thereby mapping the sentences into a 4096- dimensional space. The results presented in Section 4 based on networks with 500 hidden nodes which were trained for at least 10 iterations. The networks are not trained until convergence due to time constraints; additional training iterations will likely result in better performance. Experiments using</context>
</contexts>
<marker>Bloom, 1970</marker>
<rawString>Burton H. Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Communications of the ACM, 13(7):422426, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2986" citStr="Brown et al., 1993" startWordPosition="485" endWordPosition="488">es. 2. Translation features model the connection between source and target. While this is very closely related to the general problem of machine translation, the advantage in confidence estimation is that we can exercise unconstructive criticism, i.e. point out errors without offering a better translation. In addition, there is no need for an efficient search algorithm, thus allowing for more complex models. 3. Target features judge the translation of the system without regarding in which way it was produced. They often resemble the language 91 \x0cmodel used in the noisy channel formulation (Brown et al., 1993) but can also pinpoint more specific issues. In practice, the same features as for the source side can be used; the interpretation however is different. 4. Engine features are often referred to as glass box features (Specia et al., 2009). They describe the process which produced the translation in question and usually rely on the inner workings of the MT system. Examples include model scores and word posterior probabilities (WPP) (Ueffing et al., 2003). In this work we focus on the first three categories and ignore the particular system that produced the translations. Such features are commonl</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<contexts>
<context position="4651" citStr="Finkel et al., 2005" startWordPosition="755" endWordPosition="758">ation. In the literature, several characteristics that may hinder proper translation have been identified, among them poor grammar and misplaced punctuation. As a very simple approximation we implement binary indicators that detect clauses by looking for quotation marks, hyphens, commas, etc. Another binary feature marks numbers and uppercase words. 3.2 Named Entities Another aspect that might pose a potential problem to MT is the occurrence of words that were only observed a few times or in very particular contexts, as it is often the case for Named Entities. We used the Stanford NER Tagger (Finkel et al., 2005) to detect words that belong to one of four groups: Person, Location, Organization and Misc. Each group is represented by a binary feature. Counts are given in Table 2. The test set has significantly less support for the Misc category, possibly hinting that this data was taken from a different source or document. To avoid the danger of biasing train (src) test (src) abs rel abs rel Person 623 34% 141 33% Location 479 26% 99 23% Organization 505 28% 110 26% Misc 428 23% 53 13% Table 2: Distribution of Named Entities. The counts are based on a binary features, i.e. multiple occurrences are treat</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling.</rawString>
</citation>
<citation valid="true">
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL</booktitle>
<pages>363370</pages>
<location>Stroudsburg, PA, USA.</location>
<marker>2005</marker>
<rawString>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL 2005, pages 363370, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Sasa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Extending statistical machine translation with discriminative and trigger-based lexicon models.</title>
<date>2009</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>210217</pages>
<location>Singapore,</location>
<contexts>
<context position="5841" citStr="Mauser et al. (2009)" startWordPosition="973" endWordPosition="976">multiple occurrences are treated as a single one. the classifier we decided not to use the Misc indicator in our experiments. 3.3 Backoff Behavior In related work (Raybaud et al., 2011) the backoff behavior of a 3-gram LM was found to be the most powerful feature for word level QE. We compute for each word the longest seen n-gram (up to n = 4) and take the average length as a feature. N-grams at the beginning of a sentence are extended with &amp;lt;s&amp;gt; tokens to avoid penalizing short sentences. This is done on both the source and target side. 3.4 Discriminative Word Lexicon Following the approach of Mauser et al. (2009) we train log-linear binary classifiers that directly model p(e|fJ 1 ) for each word e eI 1: p(e|fJ 1 ) = exp \x10P ffJ 1 e,f \x11 1 + exp \x10P ffJ 1 e,f \x11 (1) where e,f are the trained model weights. Please note that this introduces a global dependence on the source sentence so that every source word may influence the choice of all words in eI 1 as opposed to the local dependencies found in the underlying phrasebased MT system. Assuming independence among the words in the translated sentence we could compute the probability of the sentence pair as: p(eI 1|fJ 1 ) = Y eeI 1 p(e|fJ 1 ) Y e/ </context>
</contexts>
<marker>Mauser, Hasan, Ney, 2009</marker>
<rawString>Arne Mauser, Sasa Hasan, and Hermann Ney. 2009. Extending statistical machine translation with discriminative and trigger-based lexicon models. In Conference on Empirical Methods in Natural Language Processing, pages 210217, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Raybaud</author>
<author>David Langlois</author>
<author>Kamel Smali</author>
</authors>
<title>this sentence is wrong. detecting errors in machine-translated sentences.</title>
<date>2011</date>
<journal>Machine Translation,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="5406" citStr="Raybaud et al., 2011" startWordPosition="893" endWordPosition="896">ture. Counts are given in Table 2. The test set has significantly less support for the Misc category, possibly hinting that this data was taken from a different source or document. To avoid the danger of biasing train (src) test (src) abs rel abs rel Person 623 34% 141 33% Location 479 26% 99 23% Organization 505 28% 110 26% Misc 428 23% 53 13% Table 2: Distribution of Named Entities. The counts are based on a binary features, i.e. multiple occurrences are treated as a single one. the classifier we decided not to use the Misc indicator in our experiments. 3.3 Backoff Behavior In related work (Raybaud et al., 2011) the backoff behavior of a 3-gram LM was found to be the most powerful feature for word level QE. We compute for each word the longest seen n-gram (up to n = 4) and take the average length as a feature. N-grams at the beginning of a sentence are extended with &amp;lt;s&amp;gt; tokens to avoid penalizing short sentences. This is done on both the source and target side. 3.4 Discriminative Word Lexicon Following the approach of Mauser et al. (2009) we train log-linear binary classifiers that directly model p(e|fJ 1 ) for each word e eI 1: p(e|fJ 1 ) = exp \x10P ffJ 1 e,f \x11 1 + exp \x10P ffJ 1 e,f \x11 (1) w</context>
</contexts>
<marker>Raybaud, Langlois, Smali, 2011</marker>
<rawString>Sylvain Raybaud, David Langlois, and Kamel Smali. 2011. this sentence is wrong. detecting errors in machine-translated sentences. Machine Translation, 25(1):134, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
<author>Marco Turchi</author>
<author>Nello Cristianini</author>
</authors>
<title>Estimating the sentence-level quality of machine translation systems.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Annual Conference of the European Association for Machine Translation, EAMT-2009,</booktitle>
<pages>2835</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3223" citStr="Specia et al., 2009" startWordPosition="526" endWordPosition="529">criticism, i.e. point out errors without offering a better translation. In addition, there is no need for an efficient search algorithm, thus allowing for more complex models. 3. Target features judge the translation of the system without regarding in which way it was produced. They often resemble the language 91 \x0cmodel used in the noisy channel formulation (Brown et al., 1993) but can also pinpoint more specific issues. In practice, the same features as for the source side can be used; the interpretation however is different. 4. Engine features are often referred to as glass box features (Specia et al., 2009). They describe the process which produced the translation in question and usually rely on the inner workings of the MT system. Examples include model scores and word posterior probabilities (WPP) (Ueffing et al., 2003). In this work we focus on the first three categories and ignore the particular system that produced the translations. Such features are commonly referred to as black box features. While some glass box features, e.g. word posterior probabilities, have led to promising results in the past, we chose to explore new features potentially applicable to translations from any source, e.</context>
</contexts>
<marker>Specia, Cancedda, Dymetman, Turchi, Cristianini, 2009</marker>
<rawString>Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009. Estimating the sentence-level quality of machine translation systems. In Proceedings of the 13th Annual Conference of the European Association for Machine Translation, EAMT-2009, pages 2835, Barcelona, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Klaus Macherey</author>
<author>Hermann Ney</author>
</authors>
<title>Confidence measures for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Machine Translation Summit,</booktitle>
<pages>394401</pages>
<location>New Orleans, LA,</location>
<contexts>
<context position="3442" citStr="Ueffing et al., 2003" startWordPosition="562" endWordPosition="565">f the system without regarding in which way it was produced. They often resemble the language 91 \x0cmodel used in the noisy channel formulation (Brown et al., 1993) but can also pinpoint more specific issues. In practice, the same features as for the source side can be used; the interpretation however is different. 4. Engine features are often referred to as glass box features (Specia et al., 2009). They describe the process which produced the translation in question and usually rely on the inner workings of the MT system. Examples include model scores and word posterior probabilities (WPP) (Ueffing et al., 2003). In this work we focus on the first three categories and ignore the particular system that produced the translations. Such features are commonly referred to as black box features. While some glass box features, e.g. word posterior probabilities, have led to promising results in the past, we chose to explore new features potentially applicable to translations from any source, e.g. translations found on the web. 3.1 Binary Indicators MTranslatability (Bernth and Gdaniec, 2001) gives a notion of the structural complexity of a sentence that relates to the quality of the produced translation. In t</context>
</contexts>
<marker>Ueffing, Macherey, Ney, 2003</marker>
<rawString>Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003. Confidence measures for statistical machine translation. In Machine Translation Summit, pages 394401, New Orleans, LA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Q Weinberger</author>
<author>Anirban Dasgupta</author>
<author>John Langford</author>
<author>Alexander J Smola</author>
<author>Josh Attenberg</author>
</authors>
<title>Feature hashing for large scale multitask learning.</title>
<date>2009</date>
<booktitle>In ICML 09: Proceedings of the 26th Annual International Conference on Machine Learning, ACM International Conference Proceeding Series,</booktitle>
<pages>1113--1120</pages>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="8525" citStr="Weinberger et al., 2009" startWordPosition="1458" endWordPosition="1461"> never occur in the QE data of 2,254 sentences from the corpus leaving 8,365 input and 9,000 output nodes. This reduces the estimated training time from 11 days to less than 6 hours per iteration1. Standard stochastic gradient decent on a three-layer feed-forward network is used. As shown in Table 3 the filtering can lead to artifacts in which case an erroneous mapping is learned. Moreover the filtering approach does not scale well as the QE corpus and thereby the vocabulary grows. 1 using a 2.66 GHz Intel Xeon and 2 threads Our second approach to reduce dimensionality uses the hashing trick (Weinberger et al., 2009): a hash function is applied to each word and the sentence is represented by the hashed values which are again transformed using vector space model as above. The dimensionality reduction is due to the fact that there are less possible hash values than words in the vocabulary. To reduce the loss of information due to collisions, several different hash functions are used. The resulting vector representation closely resembles a Bloom Filter (Bloom, 1970). This approach scales well but introduces two new parameters: the number of hash functions to use and the dimensionality of the resulting space.</context>
</contexts>
<marker>Weinberger, Dasgupta, Langford, Smola, Attenberg, 2009</marker>
<rawString>Kilian Q. Weinberger, Anirban Dasgupta, John Langford, Alexander J. Smola, and Josh Attenberg. 2009. Feature hashing for large scale multitask learning. In ICML 09: Proceedings of the 26th Annual International Conference on Machine Learning, ACM International Conference Proceeding Series, pages 1113 1120, Montreal, Quebec, Canada, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>