<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<bodyText confidence="0.595302">
b&apos;Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 683692,
</bodyText>
<address confidence="0.433642">
Portland, Oregon, June 19-24, 2011. c
</address>
<title confidence="0.606866">
2011 Association for Computational Linguistics
Shift-Reduce CCG Parsing
</title>
<author confidence="0.971163">
Yue Zhang
</author>
<affiliation confidence="0.980462">
University of Cambridge
Computer Laboratory
</affiliation>
<email confidence="0.991047">
yue.zhang@cl.cam.ac.uk
</email>
<author confidence="0.988539">
Stephen Clark
</author>
<affiliation confidence="0.9823405">
University of Cambridge
Computer Laboratory
</affiliation>
<email confidence="0.994347">
stephen.clark@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.990646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999468">
CCGs are directly compatible with binary-
branching bottom-up parsing algorithms, in
particular CKY and shift-reduce algorithms.
While the chart-based approach has been the
dominant approach for CCG, the shift-reduce
method has been little explored. In this paper,
we develop a shift-reduce CCG parser using
a discriminative model and beam search, and
compare its strengths and weaknesses with the
chart-based C&amp;C parser. We study different
errors made by the two parsers, and show that
the shift-reduce parser gives competitive accu-
racies compared to C&amp;C. Considering our use
of a small beam, and given the high ambigu-
ity levels in an automatically-extracted gram-
mar and the amount of information in the CCG
lexical categories which form the shift actions,
this is a surprising result.
</bodyText>
<sectionHeader confidence="0.998024" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998800510638298">
Combinatory Categorial Grammar (CCG; Steedman
(2000)) is a lexicalised theory of grammar which has
been successfully applied to a range of problems in
NLP, including treebank creation (Hockenmaier and
Steedman, 2007), syntactic parsing (Hockenmaier,
2003; Clark and Curran, 2007), logical form con-
struction (Bos et al., 2004) and surface realization
(White and Rajkumar, 2009). From a parsing per-
spective, the C&amp;C parser (Clark and Curran, 2007)
has been shown to be competitive with state-of-the-
art statistical parsers on a variety of test suites, in-
cluding those consisting of grammatical relations
(Clark and Curran, 2007), Penn Treebank phrase-
structure trees (Clark and Curran, 2009), and un-
bounded dependencies (Rimell et al., 2009).
The binary branching nature of CCG means that
it is naturally compatible with bottom-up parsing al-
gorithms such as shift-reduce and CKY (Ades and
Steedman, 1982; Steedman, 2000). However, the
parsing work by Clark and Curran (2007), and also
Hockenmaier (2003) and Fowler and Penn (2010),
has only considered chart-parsing. In this paper we
fill a gap in the CCG literature by developing a shift-
reduce parser for CCG.
Shift-reduce parsers have become popular for de-
pendency parsing, building on the initial work of Ya-
mada and Matsumoto (2003) and Nivre and Scholz
(2004). One advantage of shift-reduce parsers is that
the scoring model can be defined over actions, al-
lowing highly efficient parsing by using a greedy
algorithm in which the highest scoring action (or a
small number of possible actions) is taken at each
step. In addition, high accuracy can be maintained
by using a model which utilises a rich set of features
for making each local decision (Nivre et al., 2006).
Following recent work applying global discrim-
inative models to large-scale structured prediction
problems (Collins and Roark, 2004; Miyao and
Tsujii, 2005; Clark and Curran, 2007; Finkel et
al., 2008), we build our shift-reduce parser using a
global linear model, and compare it with the chart-
based C&amp;C parser. Using standard development
and test sets from CCGbank, our shift-reduce parser
gives a labeled F-measure of 85.53%, which is com-
petitive with the 85.45% F-measure of the C&amp;C
parser on recovery of predicate-argument dependen-
cies from CCGbank. Hence our work shows that
</bodyText>
<page confidence="0.997378">
683
</page>
<bodyText confidence="0.999113342857143">
\x0ctransition-based parsing can be successfully applied
to CCG, improving on earlier attempts such as Has-
san et al. (2008). Detailed analysis shows that our
shift-reduce parser yields a higher precision, lower
recall and higher F-score on most of the common
CCG dependency types compared to C&amp;C.
One advantage of the shift-reduce parser is that
it easily handles sentences for which it is difficult
to find a spanning analysis, which can happen with
CCG because the lexical categories at the leaves of a
derivation place strong contraints on the set of possi-
ble derivations, and the supertagger which provides
the lexical categories sometimes makes mistakes.
Unlike the C&amp;C parser, the shift-reduce parser nat-
urally produces fragmentary analyses when appro-
priate (Nivre et al., 2006), and can produce sensible
local structures even when a full spanning analysis
cannot be found.1
Finally, considering this work in the wider pars-
ing context, it provides an interesting comparison
between heuristic beam search using a rich set of
features, and optimal dynamic programming search
where the feature range is restricted. We are able to
perform this comparison because the use of the CCG
supertagger means that the C&amp;C parser is able to
build the complete chart, from which it can find the
optimal derivation, with no pruning whatsoever at
the parsing stage. In contrast, the shift-reduce parser
uses a simple beam search with a relatively small
beam. Perhaps surprisingly, given the ambiguity lev-
els in an automatically-extracted grammar, and the
amount of information in the CCG lexical categories
which form the shift actions, the shift-reduce parser
using heuristic beam search is able to outperform the
chart-based parser.
</bodyText>
<sectionHeader confidence="0.991943" genericHeader="introduction">
2 CCG Parsing
</sectionHeader>
<bodyText confidence="0.998775142857143">
CCG, and the application of CCG to wide-coverage
parsing, is described in detail elsewhere (Steedman,
2000; Hockenmaier, 2003; Clark and Curran, 2007).
Here we provide only a short description.
During CCG parsing, adjacent categories are com-
bined using CCGs combinatory rules. For example,
a verb phrase in English (S\\NP) can combine with
</bodyText>
<page confidence="0.937709">
1
</page>
<bodyText confidence="0.999232666666667">
See e.g. Riezler et al. (2002) and Zhang et al. (2007) for chart-
based parsers which can produce fragmentary analyses.
an NP to its left using function application:
</bodyText>
<sectionHeader confidence="0.974036" genericHeader="method">
NP S\\NP S
</sectionHeader>
<bodyText confidence="0.948538083333333">
Categories can also combine using function
composition, allowing the combination of may
((S\\NP)/(S\\NP)) and like ((S\\NP)/NP) in
coordination examples such as John may like but
may detest Mary:
(S\\NP)/(S\\NP) (S\\NP)/NP (S\\NP)/NP
In addition to binary rules, such as function appli-
cation and composition, there are also unary rules
which operate on a single category in order to
change its type. For example, forward type-raising
can change a subject NP into a complex category
looking to the right for a verb phrase:
</bodyText>
<sectionHeader confidence="0.842392" genericHeader="method">
NP S/(S\\NP)
</sectionHeader>
<bodyText confidence="0.999649535714286">
An example CCG derivation is given in Section 3.
The resource used for building wide-coverage
CCG parsers of English is CCGbank (Hockenmaier
and Steedman, 2007), a version of the Penn Tree-
bank in which each phrase-structure tree has been
transformed into a normal-form CCG derivation.
There are two ways to extract a grammar from this
resource. One approach is to extract a lexicon,
i.e. a mapping from words to sets of lexical cat-
egories, and then manually define the combinatory
rule schemas, such as functional application and
composition, which combine the categories together.
The derivations in the treebank are then used to pro-
vide training data for the statistical disambiguation
model. This is the method used in the C&amp;C parser.2
The second approach is to read the complete
grammar from the derivations, by extracting combi-
natory rule instances from the local trees consisting
of a parent category and one or two child categories,
and applying only those instances during parsing.
(These rule instances also include rules to deal with
punctuation and unary type-changing rules, in addi-
tion to instances of the combinatory rule schemas.)
This is the method used by Hockenmaier (2003) and
is the method we adopt in this paper.
Fowler and Penn (2010) demonstrate that the sec-
ond extraction method results in a context-free ap-
proximation to the grammar resulting from the first
</bodyText>
<page confidence="0.98076">
2
</page>
<bodyText confidence="0.996292333333333">
Although the C&amp;C default mode applies a restriction for effi-
ciency reasons in which only rule instances seen in CCGbank
can be applied, making the grammar of the second type.
</bodyText>
<page confidence="0.998884">
684
</page>
<bodyText confidence="0.999430571428571">
\x0cmethod, which has the potential to produce a mildly-
context sensitive grammar (given the existence of
certain combinatory rules) (Weir, 1988). However,
it is important to note that the advantages of CCG, in
particular the tight relationship between syntax and
semantic interpretation, are still maintained with the
second approach, as Fowler and Penn (2010) argue.
</bodyText>
<sectionHeader confidence="0.995419" genericHeader="method">
3 The Shift-reduce CCG Parser
</sectionHeader>
<bodyText confidence="0.992440052631579">
Given an input sentence, our parser uses a stack of
partial derivations, a queue of incoming words, and
a series of actionsderived from the rule instances
in CCGbankto build a derivation tree. Following
Clark and Curran (2007), we assume that each input
word has been assigned a POS-tag (from the Penn
Treebank tagset) and a set of CCG lexical categories.
We use the same maximum entropy POS-tagger and
supertagger as the C&amp;C parser. The derivation tree
can be transformed into CCG dependencies or gram-
matical relations by a post-processing step, which
essentially runs the C&amp;C parser deterministically
over the derivation, interpreting the derivation and
generating the required output.
The configuration of the parser, at each step of
the parsing process, is shown in part (a) of Figure 1,
where the stack holds the partial derivation trees that
have been built, and the queue contains the incoming
words that have not been processed. In the figure,
S(H) represents a category S on the stack with head
word H, while Qi represents a word in the incoming
queue.
The set of action types used by the parser is as
follows: {SHIFT, COMBINE, UNARY, FINISH}.
Each action type represents a set of possible actions
available to the parser at each step in the process.
The SHIFT-X action pushes the next incoming
word onto the stack, and assigns the lexical category
X to the word (Figure 1(b)). The label X can be any
lexical category from the set assigned to the word
being shifted by the supertagger. Hence the shift ac-
tion performs lexical category disambiguation. This
is in contrast to a shift-reduce dependency parser in
which a shift action typically just pushes a word onto
the stack.
The COMBINE-X action pops the top two nodes
off the stack, and combines them into a new node,
which is pushed back on the stack. The category of
</bodyText>
<figureCaption confidence="0.984827">
Figure 1: The parser configuration and set of actions.
</figureCaption>
<bodyText confidence="0.999402818181818">
the new node is X. A COMBINE action corresponds
to a combinatory rule in the CCG grammar (or one of
the additional punctuation or type-changing rules),
which is applied to the categories of the top two
nodes on the stack.
The UNARY-X action pops the top of the stack,
transforms it into a new node with category X, and
pushes the new node onto the stack. A UNARY ac-
tion corresponds to a unary type-changing or type-
raising rule in the CCG grammar, which is applied to
the category on top of the stack.
The FINISH action terminates the parsing pro-
cess; it can be applied when all input words have
been shifted onto the stack. Note that the FINISH
action can be applied when the stack contains more
than one node, in which case the parser produces
a set of partial derivation trees, each corresponding
to a node on the stack. This sometimes happens
when a full derivation tree cannot be built due to su-
pertagging errors, and provides a graceful solution
to the problem of producing high-quality fragmen-
tary parses when necessary.
</bodyText>
<page confidence="0.998988">
685
</page>
<figureCaption confidence="0.8153255">
\x0cFigure 2: An example parsing process.
Figure 2 shows the shift-reduce parsing process
</figureCaption>
<bodyText confidence="0.997615655172414">
for the example sentence IBM bought Lotus. First
the word IBM is shifted onto the stack as an NP;
then bought is shifted as a transitive verb look-
ing for its object NP on the right and subject NP on
the left ((S[dcl]\\NP)/NP); and then Lotus is shifted
as an NP. Then bought is combined with its ob-
ject Lotus resulting in a verb phrase looking for its
subject on the left (S[dcl]\\NP). Finally, the resulting
verb phrase is combined with its subject, resulting in
a declarative sentence (S[dcl]).
A key difference with previous work on shift-
reduce dependency (Nivre et al., 2006) and CFG
(Sagae and Lavie, 2006b) parsing is that, for CCG,
there are many more shift actions a shift action for
each word-lexical category pair. Given the amount
of syntactic information in the lexical categories, the
choice of correct category, from those supplied by
the supertagger, is often a difficult one, and often
a choice best left to the parsing model. The C&amp;C
parser solves this problem by building the complete
packed chart consistent with the lexical categories
supplied by the supertagger, leaving the selection of
the lexical categories to the Viterbi algorithm. For
the shift-reduce parser the choice is also left to the
parsing model, but in contrast to C&amp;C the correct
lexical category could be lost at any point in the
heuristic search process. Hence it is perhaps sur-
prising that we are able to achieve a high parsing ac-
curacy of 85.5%, given a relatively small beam size.
</bodyText>
<sectionHeader confidence="0.996477" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.999150857142857">
Greedy local search (Yamada and Matsumoto, 2003;
Sagae and Lavie, 2005; Nivre and Scholz, 2004)
has typically been used for decoding in shift-reduce
parsers, while beam-search has recently been ap-
plied as an alternative to reduce error-propagation
(Johansson and Nugues, 2007; Zhang and Clark,
2008; Zhang and Clark, 2009; Huang et al., 2009).
Both greedy local search and beam-search have lin-
ear time complexity. We use beam-search in our
CCG parser.
To formulate the decoding algorithm, we define a
candidate item as a tuple hS, Q, Fi, where S repre-
sents the stack with partial derivations that have been
built, Q represents the queue of incoming words that
have not been processed, and F is a boolean value
that represents whether the candidate item has been
finished. A candidate item is finished if and only if
the FINISH action has been applied to it, and no
more actions can be applied to a candidate item af-
ter it reaches the finished status. Given an input sen-
tence, we define the start item as the unfinished item
with an empty stack and the whole input sentence as
the incoming words. A derivation is built from the
start item by repeated applications of actions until
the item is finished.
To apply beam-search, an agenda is used to hold
the N-best partial (unfinished) candidate items at
each parsing step. A separate candidate output is
</bodyText>
<page confidence="0.99287">
686
</page>
<equation confidence="0.8185523">
\x0cfunction DECODE(input, agenda, list, N,
grammar, candidate output):
agenda.clear()
agenda.insert(GETSTARTITEM(input))
candidate output = NONE
while not agenda.empty():
list.clear()
for item in agenda:
for action in grammar.getActions(item):
item = item.apply(action)
</equation>
<bodyText confidence="0.841421666666667">
if item.F == TRUE:
if candidate output == NONE or
item.score &gt; candidate output.score:
</bodyText>
<figure confidence="0.8541558">
candidate output = item
else:
list.append(item)
agenda.clear()
agenda.insert(list.best(N))
</figure>
<figureCaption confidence="0.999873">
Figure 3: The decoding algorithm; N is the agenda size
</figureCaption>
<bodyText confidence="0.964077384615385">
used to record the current best finished item that has
been found, since candidate items can be finished at
different steps. Initially the agenda contains only the
start item, and the candidate output is set to none. At
each step during parsing, each candidate item from
the agenda is extended in all possible ways by apply-
ing one action according to the grammar, and a num-
ber of new candidate items are generated. If a newly
generated candidate is finished, it is compared with
the current candidate output. If the candidate output
is none or the score of the newly generated candi-
date is higher than the score of the candidate output,
the candidate output is replaced with the newly gen-
erated item; otherwise the newly generated item is
discarded. If the newly generated candidate is un-
finished, it is appended to a list of newly generated
partial candidates. After all candidate items from the
agenda have been processed, the agenda is cleared
and the N-best items from the list are put on the
agenda. Then the list is cleared and the parser moves
on to the next step. This process repeats until the
agenda is empty (which means that no new items
have been generated in the previous step), and the
candidate output is the final derivation. Pseudocode
for the algorithm is shown in Figure 3.
feature templates
</bodyText>
<equation confidence="0.897976272727273">
1 S0wp, S0c, S0pc, S0wc,
S1wp, S1c, S1pc, S1wc,
S2pc, S2wc,
S3pc, S3wc,
2 Q0wp, Q1wp, Q2wp, Q3wp,
3 S0Lpc, S0Lwc, S0Rpc, S0Rwc,
S0Upc, S0Uwc,
S1Lpc, S1Lwc, S1Rpc, S1Rwc,
S1Upc, S1Uwc,
4 S0wcS1wc, S0cS1w, S0wS1c, S0cS1c,
S0wcQ0wp, S0cQ0wp, S0wcQ0p, S0cQ0p,
S1wcQ0wp, S1cQ0wp, S1wcQ0p, S1cQ0p,
5 S0wcS1cQ0p, S0cS1wcQ0p, S0cS1cQ0wp,
S0cS1cQ0p, S0pS1pQ0p,
S0wcQ0pQ1p, S0cQ0wpQ1p, S0cQ0pQ1wp,
S0cQ0pQ1p, S0pQ0pQ1p,
S0wcS1cS2c, S0cS1wcS2c, S0cS1cS2wc,
S0cS1cS2c, S0pS1pS2p,
6 S0cS0HcS0Lc, S0cS0HcS0Rc,
S1cS1HcS1Rc,
S0cS0RcQ0p, S0cS0RcQ0w,
S0cS0LcS1c, S0cS0LcS1w,
</equation>
<tableCaption confidence="0.6396435">
S0cS1cS1Rc, S0wS1cS1Rc.
Table 1: Feature templates.
</tableCaption>
<sectionHeader confidence="0.90573" genericHeader="method">
5 Model and Training
</sectionHeader>
<bodyText confidence="0.995624590909091">
We use a global linear model to score candidate
items, trained discriminatively with the averaged
perceptron (Collins, 2002). Features for a (finished
or partial) candidate are extracted from each ac-
tion that have been applied to build the candidate.
Following Collins and Roark (2004), we apply the
early update strategy to perceptron training: at any
step during decoding, if neither the candidate out-
put nor any item in the agenda is correct, decoding
is stopped and the parameters are updated using the
current highest scored item in the agenda or the can-
didate output, whichever has the higher score.
Table 1 shows the feature templates used by the
parser. The symbols S0, S1, S2 and S3 in the ta-
ble represent the top four nodes on the stack (if ex-
istent), and Q0, Q1, Q2 and Q3 represent the front
four words in the incoming queue (if existent). S0H
and S1H represent the subnodes of S0 and S1 that
have the lexical head of S0 and S1, respectively. S0L
represents the left subnode of S0, when the lexical
head is from the right subnode. S0R and S1R rep-
resent the right subnode of S0 and S1, respectively,
</bodyText>
<page confidence="0.988493">
687
</page>
<bodyText confidence="0.99882">
\x0cwhen the lexical head is from the left subnode. If S0
is built by a UNARY action, S0U represents the only
subnode of S0. The symbols w, p and c represent the
word, the POS, and the CCG category, respectively.
These rich feature templates produce a large num-
ber of features: 36 million after the first training it-
eration, compared to around 0.5 million in the C&amp;C
parser.
</bodyText>
<sectionHeader confidence="0.999102" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.993862058823529">
Our experiments were performed using CCGBank
(Hockenmaier and Steedman, 2007), which was
split into three subsets for training (Sections 0221),
development testing (Section 00) and the final test
(Section 23). Extracted from the training data, the
CCG grammar used by our parser consists of 3070
binary rule instances and 191 unary rule instances.
We compute F-scores over labeled CCG depen-
dencies and also lexical category accuracy. CCG de-
pendencies are defined in terms of lexical categories,
by numbering each argument slot in a complex cat-
egory. For example, the first NP in a transitive verb
category is a CCG dependency relation, correspond-
ing to the subject of the verb. Clark and Curran
(2007) gives a more precise definition. We use the
generate script from the C&amp;C tools3 to transform
derivations into CCG dependencies.
There is a mismatch between the grammar that
generate uses, which is the same grammar as the
C&amp;C parser, and the grammar we extract from CCG-
bank, which contains more rule instances. Hence
generate is unable to produce dependencies for
some of the derivations our shift-reduce parser pro-
duces. In order to allow generate to process all
derivations from the shift-reduce parser, we repeat-
edly removed rules that the generate script can-
not handle from our grammar, until all derivations
in the development data could be dealt with. In
fact, this procedure potentially reduces the accuracy
of the shift-reduce parser, but the effect is compar-
atively small because only about 4% of the devel-
opment and test sentences contain rules that are not
handled by the generate script.
All experiments were performed using automati-
</bodyText>
<page confidence="0.952655">
3
</page>
<bodyText confidence="0.9895522">
Available at http://svn.ask.it.usyd.edu.au/trac/candc/wiki; we
used the generate and evaluate scripts, as well as the
C&amp;C parser, for evaluation and comparison.
cally assigned POS-tags, with 10-fold cross valida-
tion used to assign POS-tags and lexical categories
to the training data. At the supertagging stage, mul-
tiple lexical categories are assigned to each word in
the input. For each word, the supertagger assigns all
lexical categories whose forward-backward proba-
bility is above max, where max is the highest
lexical category probability for the word, and is a
threshold parameter. To give the parser a reasonable
freedom in lexical category disambiguation, we used
a small value of 0.0001, which results in 3.6 lexi-
cal categories being assigned to each word on aver-
age in the training data. For training, but not testing,
we also added the correct lexical category to the list
of lexical categories for a word in cases when it was
not provided by the supertagger.
Increasing the size of the beam in the parser beam
search leads to higher accuracies but slower running
time. In our development experiments, the accu-
racy improvement became small when the beam size
reached 16, and so we set the size of the beam to 16
for the remainder of the experiments.
</bodyText>
<subsectionHeader confidence="0.999041">
6.1 Development test accuracies
</subsectionHeader>
<bodyText confidence="0.998787043478261">
Table 2 shows the labeled precision (lp), recall (lr),
F-score (lf), sentence-level accuracy (lsent) and lex-
ical category accuracy (cats) of our parser and the
C&amp;C parser on the development data. We ran the
C&amp;C parser using the normal-form model (we re-
produced the numbers reported in Clark and Cur-
ran (2007)), and copied the results of the hybrid
model from Clark and Curran (2007), since the hy-
brid model is not part of the public release.
The accuracy of our parser is much better when
evaluated on all sentences, partly because C&amp;C
failed on 0.94% of the data due to the failure to pro-
duce a spanning analysis. Our shift-reduce parser
does not suffer from this problem because it pro-
duces fragmentary analyses for those cases. When
evaluated on only those sentences that C&amp;C could
analyze, our parser gave 0.29% higher F-score. Our
shift-reduce parser also gave higher accuracies on
lexical category assignment. The sentence accuracy
of our shift-reduce parser is also higher than C&amp;C,
which confirms that our shift-reduce parser produces
reasonable sentence-level analyses, despite the pos-
sibility for fragmentary analysis.
</bodyText>
<page confidence="0.994174">
688
</page>
<bodyText confidence="0.794286">
\x0clp. lr. lf. lsent. cats. evaluated on
</bodyText>
<table confidence="0.9878368">
shift-reduce 87.15% 82.95% 85.00% 33.82% 92.77% all sentences
C&amp;C (normal-form) 85.22% 82.52% 83.85% 31.63% 92.40% all sentences
shift-reduce 87.55% 83.63% 85.54% 34.14% 93.11% 99.06% (C&amp;C coverage)
C&amp;C (hybrid) 85.25% 99.06% (C&amp;C coverage)
C&amp;C (normal-form) 85.22% 84.29% 84.76% 31.93% 92.83% 99.06% (C&amp;C coverage)
</table>
<tableCaption confidence="0.996312">
Table 2: Accuracies on the development test data.
</tableCaption>
<figure confidence="0.977557833333333">
60
65
70
75
80
85
90
0 5 10 15 20 25 30
precision
%
dependency length (bins of 5)
Precision comparison by dependency length
this paper
C&amp;C
50
55
60
65
70
75
80
85
90
0 5 10 15 20 25 30
recall
%
dependency length (bins of 5)
Recall comparison by dependency length
this paper
C&amp;C
</figure>
<figureCaption confidence="0.998848">
Figure 4: P &amp; R scores relative to dependency length.
</figureCaption>
<subsectionHeader confidence="0.977377">
6.2 Error comparison with C&amp;C parser
</subsectionHeader>
<bodyText confidence="0.998367254901961">
Our shift-reduce parser and the chart-based C&amp;C
parser offer two different solutions to the CCG pars-
ing problem. The comparison reported in this sec-
tion is similar to the comparison between the chart-
based MSTParser (McDonald et al., 2005) and shift-
reduce MaltParser (Nivre et al., 2006) for depen-
dency parsing. We follow McDonald and Nivre
(2007) and characterize the errors of the two parsers
by sentence and dependency length and dependency
type.
We measured precision, recall and F-score rel-
ative to different sentence lengths. Both parsers
performed better on shorter sentences, as expected.
Our shift-reduce parser performed consistently bet-
ter than C&amp;C on all sentence lengths, and there
was no significant difference in the rate of perfor-
mance degradation between the parsers as the sen-
tence length increased.
Figure 4 shows the comparison of labeled preci-
sion and recall relative to the dependency length (i.e.
the number of words between the head and depen-
dent), in bins of size 5 (e.g. the point at x=5 shows
the precision or recall for dependency lengths 1 5).
This experiment was performed using the normal-
form version of the C&amp;C parser, and the evaluation
was on the sentences for which C&amp;C gave an anal-
ysis. The number of dependencies drops when the
dependency length increases; there are 141, 180 and
124 dependencies from the gold-standard, C&amp;C out-
put and our shift-reduce parser output, respectively,
when the dependency length is between 21 and 25,
inclusive. The numbers drop to 47, 56 and 36 when
the dependency length is between 26 and 30. The
recall of our parser drops more quickly as the de-
pendency length grows beyond 15. A likely reason
is that the recovery of longer-range dependencies re-
quires more processing steps, increasing the chance
of the correct structure being thrown off the beam.
In contrast, the precision did not drop more quickly
than C&amp;C, and in fact is consistently higher than
C&amp;C across all dependency lengths, which reflects
the fact that the long range dependencies our parser
managed to recover are comparatively reliable.
Table 3 shows the comparison of labeled precision
(lp), recall (lr) and F-score (lf) for the most common
CCG dependency types. The numbers for C&amp;C are
for the hybrid model, copied from Clark and Curran
(2007). While our shift-reduce parser gave higher
precision for almost all categories, it gave higher re-
call on only half of them, but higher F-scores for all
but one dependency type.
</bodyText>
<subsectionHeader confidence="0.993961">
6.3 Final results
</subsectionHeader>
<bodyText confidence="0.9982315">
Table 4 shows the accuracies on the test data. The
numbers for the normal-form model are evaluated
by running the publicly available parser, while those
for the hybrid dependency model are from Clark
and Curran (2007). Evaluated on all sentences, the
accuracies of our parser are much higher than the
C&amp;C parser, since the C&amp;C parser failed to produce
any output for 10 sentences. When evaluating both
</bodyText>
<page confidence="0.998105">
689
</page>
<table confidence="0.992436545454545">
\x0ccategory arg lp. (o) lp. (C) lr. (o) lr. (C) lf. (o) lf. (C) freq.
N/N 1 95.77% 95.28% 95.79% 95.62% 95.78% 95.45% 7288
NP/N 1 96.70% 96.57% 96.59% 96.03% 96.65% 96.30% 4101
(NP\\NP)/NP 2 83.19% 82.17% 89.24% 88.90% 86.11% 85.40% 2379
(NP\\NP)/NP 1 82.53% 81.58% 87.99% 85.74% 85.17% 83.61% 2174
((S\\NP)\\(S\\NP))/NP 3 77.60% 71.94% 71.58% 73.32% 74.47% 72.63% 1147
((S\\NP)\\(S\\NP))/NP 2 76.30% 70.92% 70.60% 71.93% 73.34% 71.42% 1058
((S[dcl]\\NP)/NP 2 85.60% 81.57% 84.30% 86.37% 84.95% 83.90% 917
PP/NP 1 73.76% 75.06% 72.83% 70.09% 73.29% 72.49% 876
((S[dcl]\\NP)/NP 1 85.32% 81.62% 82.00% 85.55% 83.63% 83.54% 872
((S\\NP)\\(S\\NP)) 2 84.44% 86.85% 86.60% 86.73% 85.51% 86.79% 746
</table>
<tableCaption confidence="0.8883825">
Table 3: Accuracy comparison on the most common CCG dependency types. (o) our parser; (C) C&amp;C (hybrid)
lp. lr. lf. lsent. cats. evaluated
</tableCaption>
<table confidence="0.993202142857143">
shift-reduce 87.43% 83.61% 85.48% 35.19% 93.12% all sentences
C&amp;C (normal-form) 85.58% 82.85% 84.20% 32.90% 92.84% all sentences
shift-reduce 87.43% 83.71% 85.53% 35.34% 93.15% 99.58% (C&amp;C coverage)
C&amp;C (hybrid) 86.17% 84.74% 85.45% 32.92% 92.98% 99.58% (C&amp;C coverage)
C&amp;C (normal-form) 85.48% 84.60% 85.04% 33.08% 92.86% 99.58% (C&amp;C coverage)
F&amp;P (Petrov I-5)* 86.29% 85.73% 86.01% (F&amp;P C&amp;C coverage; 96.65% on dev. test)
C&amp;C hybrid* 86.46% 85.11% 85.78% (F&amp;P C&amp;C coverage; 96.65% on dev. test)
</table>
<tableCaption confidence="0.999805">
Table 4: Comparison with C&amp;C; final test. * not directly comparable.
</tableCaption>
<bodyText confidence="0.998406037037037">
parsers on the sentences for which C&amp;C produces an
analysis, our parser still gave the highest accuracies.
The shift-reduce parser gave higher precision, and
lower recall, than C&amp;C; it also gave higher sentence-
level and lexical category accuracy.
The last two rows in the table show the accuracies
of Fowler and Penn (2010) (F&amp;P), who applied the
CFG parser of Petrov and Klein (2007) to CCG, and
the corresponding accuracies for the C&amp;C parser on
the same test sentences. F&amp;P can be treated as an-
other chart-based parser; their evaluation is based
on the sentences for which both their parser and
C&amp;C produced dependencies (or more specifically
those sentences for which generate could pro-
duce dependencies), and is not directly comparable
with ours, especially considering that their test set is
smaller and potentially slightly easier.
The final comparison is parser speed. The shift-
reduce parser is linear-time (in both sentence length
and beam size), and can analyse over 10 sentences
per second on a 2GHz CPU, with a beam of 16,
which compares very well with other constituency
parsers. However, this is no faster than the chart-
based C&amp;C parser, although speed comparisons
are difficult because of implementation differences
(C&amp;C uses heavily engineered C++ with a focus on
efficiency).
</bodyText>
<sectionHeader confidence="0.999408" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999388294117647">
Sagae and Lavie (2006a) describes a shift-reduce
parser for the Penn Treebank parsing task which
uses best-first search to allow some ambiguity into
the parsing process. Differences with our approach
are that we use a beam, rather than best-first, search;
we use a global model rather than local models
chained together; and finally, our results surpass
the best published results on the CCG parsing task,
whereas Sagae and Lavie (2006a) matched the best
PTB results only by using a parser combination.
Matsuzaki et al. (2007) describes similar work
to ours but using an automatically-extracted HPSG,
rather than CCG, grammar. They also use the gen-
eralised perceptron to train a disambiguation model.
One difference is that Matsuzaki et al. (2007) use an
approximating CFG, in addition to the supertagger,
to improve the efficiency of the parser.
</bodyText>
<page confidence="0.940141">
690
</page>
<bodyText confidence="0.99988944">
\x0cNinomiya et al. (2009) (and Ninomiya et al.
(2010)) describe a greedy shift-reduce parser for
HPSG, in which a single action is chosen at each
parsing step, allowing the possibility of highly ef-
ficient parsing. Since the HPSG grammar has rela-
tively tight constraints, similar to CCG, the possibil-
ity arises that a spanning analysis cannot be found
for some sentences. Our approach to this problem
was to allow the parser to return a fragmentary anal-
ysis; Ninomiya et al. (2009) adopt a different ap-
proach based on default unification.
Finally, our work is similar to the comparison of
the chart-based MSTParser (McDonald et al., 2005)
and shift-reduce MaltParser (Nivre et al., 2006) for
dependency parsing. MSTParser can perform ex-
haustive search, given certain feature restrictions,
because the complexity of the parsing task is lower
than for constituent parsing. C&amp;C can perform ex-
haustive search because the supertagger has already
reduced the search space. We also found that ap-
proximate heuristic search for shift-reduce parsing,
utilising a rich feature space, can match the perfor-
mance of the optimal chart-based parser, as well as
similar error profiles for the two CCG parsers com-
pared to the two dependency parsers.
</bodyText>
<sectionHeader confidence="0.996604" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99304165">
This is the first work to present competitive results
for CCG using a transition-based parser, filling a gap
in the CCG parsing literature. Considered in terms
of the wider parsing problem, we have shown that
state-of-the-art parsing results can be obtained using
a global discriminative model, one of the few pa-
pers to do so without using a generative baseline as a
feature. The comparison with C&amp;C also allowed us
to compare a shift-reduce parser based on heuristic
beam search utilising a rich feature set with an opti-
mal chart-based parser whose features are restricted
by dynamic programming, with favourable results
for the shift-reduce parser.
The complementary errors made by the chart-
based and shift-reduce parsers opens the possibil-
ity of effective parser combination, following sim-
ilar work for dependency parsing.
The parser code can be downloaded at
http://www.sourceforge.net/projects/zpar,
version 0.5.
</bodyText>
<sectionHeader confidence="0.946593" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.992667">
We thank the anonymous reviewers for their sugges-
tions. Yue Zhang and Stephen Clark are supported
by the European Union Seventh Framework Pro-
gramme (FP7-ICT-2009-4) under grant agreement
no. 247762.
</bodyText>
<sectionHeader confidence="0.985889" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.970237906976744">
A. E. Ades and M. Steedman. 1982. On the order of
words. Linguistics and Philosophy, pages 517 558.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In Pro-
ceedings of COLING-04, pages 12401246, Geneva,
Switzerland.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493552.
Stephen Clark and James R. Curran. 2009. Comparing
the accuracy of CCG and Penn Treebank parsers. In
Proceedings of ACL-2009 (short papers), pages 53
56, Singapore.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL, pages 111118, Barcelona, Spain.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 18, Philadelphia, USA.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Feature-based, conditional random
field parsing. In Proceedings of the 46th Meeting of
the ACL, pages 959967, Columbus, Ohio.
Timothy A. D. Fowler and Gerald Penn. 2010. Ac-
curate context-free parsing with Combinatory Catego-
rial Grammar. In Proceedings of ACL-2010, Uppsala,
Sweden.
H. Hassan, K. Simaan, and A. Way. 2008. A syntactic
language model based on incremental CCG parsing.
In Proceedings of the Second IEEE Spoken Language
Technology Workshop, Goa, India.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355396.
Julia Hockenmaier. 2003. Data and Models for Statis-
tical Parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
</reference>
<page confidence="0.944581">
691
</page>
<reference confidence="0.99965188764045">
\x0cparsing. In Proceedings of the 2009 EMNLP Confer-
ence, pages 12221231, Singapore.
Richard Johansson and Pierre Nugues. 2007. Incre-
mental dependency parsing using online learning. In
Proceedings of the CoNLL/EMNLP Conference, pages
11341138, Prague, Czech Republic.
Takuya Matsuzaki, Yusuke Miyao, and Jun ichi Tsu-
jii. 2007. Efficient HPSG parsing with supertagging
and CFG-filtering. In Proceedings of IJCAI-07, pages
16711676, Hyderabad, India.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP/CoNLL, pages 122
131, Prague, Czech Republic.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Meeting of the
ACL, pages 9198, Michigan, Ann Arbor.
Yusuke Miyao and Junichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage HPSG pars-
ing. In Proceedings of the 43rd meeting of the ACL,
pages 8390, University of Michigan, Ann Arbor.
Takashi Ninomiya, Takuya Matsuzaki, Nobuyuki
Shimizu, and Hiroshi Nakagawa. 2009. Deterministic
shift-reduce parsing for unification-based grammars
by using default unification. In Proceedings of
EACL-09, pages 603611, Athens, Greece.
Takashi Ninomiya, Takuya Matsuzaki, Nobuyuki
Shimizu, and Hiroshi Nakagawa. 2010. Deter-
ministic shift-reduce parsing for unification-based
grammars. Journal of Natural Language Engineering,
DOI:10.1017/S1351324910000240.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of English text. In Proceedings of COLING-
04, pages 6470, Geneva, Switzerland.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of CoNLL, pages 221225,
New York, USA.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404411, Rochester, New York,
April.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Meet-
ing of the ACL, pages 271278, Philadelphia, PA.
Laura Rimell, Stephen Clark, and Mark Steedman. 2009.
Unbounded dependency recovery for parser evalua-
tion. In Proceedings of EMNLP-09, pages 813821,
Singapore.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of IWPT, pages 125132, Vancouver, Canada.
Kenji Sagae and Alon Lavie. 2006a. A best-first
probabilistic shift-reduce parser. In Proceedings of
COLING/ACL poster session, pages 691698, Sydney,
Australia, July.
Kenji Sagae and Alon Lavie. 2006b. Parser combination
by reparsing. In Proceedings of HLT/NAACL, Com-
panion Volume: Short Papers, pages 129132, New
York, USA.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, Mass.
David Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Univer-
sity of Pennsylviania.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410419, Singapore.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In Pro-
ceedings of IWPT, Nancy, France.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-based
and transition-based dependency parsing using beam-
search. In Proceedings of EMNLP-08, Hawaii, USA.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese Treebank using a global dis-
criminative model. In Proceedings of IWPT, Paris,
France, October.
Yi Zhang, Valia Kordoni, and Erin Fitzgerald. 2007. Par-
tial parse selection for robust deep processing. In Pro-
ceedings of the ACL 2007 Workshop on Deep Linguis-
tic Processing, Prague, Czech Republic.
</reference>
<page confidence="0.972029">
692
</page>
<figure confidence="0.255083">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.880760">
<note confidence="0.980798">b&apos;Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 683692, Portland, Oregon, June 19-24, 2011. c 2011 Association for Computational Linguistics</note>
<title confidence="0.977689">Shift-Reduce CCG Parsing</title>
<author confidence="0.99938">Yue Zhang</author>
<affiliation confidence="0.9998955">University of Cambridge Computer Laboratory</affiliation>
<email confidence="0.983014">yue.zhang@cl.cam.ac.uk</email>
<author confidence="0.988118">Stephen Clark</author>
<affiliation confidence="0.99985">University of Cambridge Computer Laboratory</affiliation>
<email confidence="0.992367">stephen.clark@cl.cam.ac.uk</email>
<abstract confidence="0.999270684210526">CCGs are directly compatible with binarybranching bottom-up parsing algorithms, in particular CKY and shift-reduce algorithms. While the chart-based approach has been the dominant approach for CCG, the shift-reduce method has been little explored. In this paper, we develop a shift-reduce CCG parser using a discriminative model and beam search, and compare its strengths and weaknesses with the chart-based C&amp;C parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&amp;C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A E Ades</author>
<author>M Steedman</author>
</authors>
<title>On the order of words. Linguistics and Philosophy,</title>
<date>1982</date>
<pages>517--558</pages>
<contexts>
<context position="2085" citStr="Ades and Steedman, 1982" startWordPosition="302" endWordPosition="305">07), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&amp;C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrasestructure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as shift-reduce and CKY (Ades and Steedman, 1982; Steedman, 2000). However, the parsing work by Clark and Curran (2007), and also Hockenmaier (2003) and Fowler and Penn (2010), has only considered chart-parsing. In this paper we fill a gap in the CCG literature by developing a shiftreduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). One advantage of shift-reduce parsers is that the scoring model can be defined over actions, allowing highly efficient parsing by using a greedy algorithm in which the highest scoring acti</context>
</contexts>
<marker>Ades, Steedman, 1982</marker>
<rawString>A. E. Ades and M. Steedman. 1982. On the order of words. Linguistics and Philosophy, pages 517 558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James R Curran</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Wide-coverage semantic representations from a CCG parser.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04,</booktitle>
<pages>12401246</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1511" citStr="Bos et al., 2004" startWordPosition="212" endWordPosition="215"> gives competitive accuracies compared to C&amp;C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&amp;C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrasestructure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as shift-reduce and CKY (Ades and Steedman, 1982; Steedman, 2000). However</context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>Johan Bos, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Wide-coverage semantic representations from a CCG parser. In Proceedings of COLING-04, pages 12401246, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="1465" citStr="Clark and Curran, 2007" startWordPosition="204" endWordPosition="207">e two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&amp;C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&amp;C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrasestructure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as shift-reduce and CKY (Ade</context>
<context position="3082" citStr="Clark and Curran, 2007" startWordPosition="465" endWordPosition="468"> (2003) and Nivre and Scholz (2004). One advantage of shift-reduce parsers is that the scoring model can be defined over actions, allowing highly efficient parsing by using a greedy algorithm in which the highest scoring action (or a small number of possible actions) is taken at each step. In addition, high accuracy can be maintained by using a model which utilises a rich set of features for making each local decision (Nivre et al., 2006). Following recent work applying global discriminative models to large-scale structured prediction problems (Collins and Roark, 2004; Miyao and Tsujii, 2005; Clark and Curran, 2007; Finkel et al., 2008), we build our shift-reduce parser using a global linear model, and compare it with the chartbased C&amp;C parser. Using standard development and test sets from CCGbank, our shift-reduce parser gives a labeled F-measure of 85.53%, which is competitive with the 85.45% F-measure of the C&amp;C parser on recovery of predicate-argument dependencies from CCGbank. Hence our work shows that 683 \x0ctransition-based parsing can be successfully applied to CCG, improving on earlier attempts such as Hassan et al. (2008). Detailed analysis shows that our shift-reduce parser yields a higher p</context>
<context position="5375" citStr="Clark and Curran, 2007" startWordPosition="827" endWordPosition="830">hich it can find the optimal derivation, with no pruning whatsoever at the parsing stage. In contrast, the shift-reduce parser uses a simple beam search with a relatively small beam. Perhaps surprisingly, given the ambiguity levels in an automatically-extracted grammar, and the amount of information in the CCG lexical categories which form the shift actions, the shift-reduce parser using heuristic beam search is able to outperform the chart-based parser. 2 CCG Parsing CCG, and the application of CCG to wide-coverage parsing, is described in detail elsewhere (Steedman, 2000; Hockenmaier, 2003; Clark and Curran, 2007). Here we provide only a short description. During CCG parsing, adjacent categories are combined using CCGs combinatory rules. For example, a verb phrase in English (S\\NP) can combine with 1 See e.g. Riezler et al. (2002) and Zhang et al. (2007) for chartbased parsers which can produce fragmentary analyses. an NP to its left using function application: NP S\\NP S Categories can also combine using function composition, allowing the combination of may ((S\\NP)/(S\\NP)) and like ((S\\NP)/NP) in coordination examples such as John may like but may detest Mary: (S\\NP)/(S\\NP) (S\\NP)/NP (S\\NP)/NP</context>
<context position="8465" citStr="Clark and Curran (2007)" startWordPosition="1326" endWordPosition="1329">x0cmethod, which has the potential to produce a mildlycontext sensitive grammar (given the existence of certain combinatory rules) (Weir, 1988). However, it is important to note that the advantages of CCG, in particular the tight relationship between syntax and semantic interpretation, are still maintained with the second approach, as Fowler and Penn (2010) argue. 3 The Shift-reduce CCG Parser Given an input sentence, our parser uses a stack of partial derivations, a queue of incoming words, and a series of actionsderived from the rule instances in CCGbankto build a derivation tree. Following Clark and Curran (2007), we assume that each input word has been assigned a POS-tag (from the Penn Treebank tagset) and a set of CCG lexical categories. We use the same maximum entropy POS-tagger and supertagger as the C&amp;C parser. The derivation tree can be transformed into CCG dependencies or grammatical relations by a post-processing step, which essentially runs the C&amp;C parser deterministically over the derivation, interpreting the derivation and generating the required output. The configuration of the parser, at each step of the parsing process, is shown in part (a) of Figure 1, where the stack holds the partial </context>
<context position="18735" citStr="Clark and Curran (2007)" startWordPosition="3045" endWordPosition="3048"> which was split into three subsets for training (Sections 0221), development testing (Section 00) and the final test (Section 23). Extracted from the training data, the CCG grammar used by our parser consists of 3070 binary rule instances and 191 unary rule instances. We compute F-scores over labeled CCG dependencies and also lexical category accuracy. CCG dependencies are defined in terms of lexical categories, by numbering each argument slot in a complex category. For example, the first NP in a transitive verb category is a CCG dependency relation, corresponding to the subject of the verb. Clark and Curran (2007) gives a more precise definition. We use the generate script from the C&amp;C tools3 to transform derivations into CCG dependencies. There is a mismatch between the grammar that generate uses, which is the same grammar as the C&amp;C parser, and the grammar we extract from CCGbank, which contains more rule instances. Hence generate is unable to produce dependencies for some of the derivations our shift-reduce parser produces. In order to allow generate to process all derivations from the shift-reduce parser, we repeatedly removed rules that the generate script cannot handle from our grammar, until all</context>
<context position="21284" citStr="Clark and Curran (2007)" startWordPosition="3462" endWordPosition="3466">ing the size of the beam in the parser beam search leads to higher accuracies but slower running time. In our development experiments, the accuracy improvement became small when the beam size reached 16, and so we set the size of the beam to 16 for the remainder of the experiments. 6.1 Development test accuracies Table 2 shows the labeled precision (lp), recall (lr), F-score (lf), sentence-level accuracy (lsent) and lexical category accuracy (cats) of our parser and the C&amp;C parser on the development data. We ran the C&amp;C parser using the normal-form model (we reproduced the numbers reported in Clark and Curran (2007)), and copied the results of the hybrid model from Clark and Curran (2007), since the hybrid model is not part of the public release. The accuracy of our parser is much better when evaluated on all sentences, partly because C&amp;C failed on 0.94% of the data due to the failure to produce a spanning analysis. Our shift-reduce parser does not suffer from this problem because it produces fragmentary analyses for those cases. When evaluated on only those sentences that C&amp;C could analyze, our parser gave 0.29% higher F-score. Our shift-reduce parser also gave higher accuracies on lexical category assi</context>
<context position="25166" citStr="Clark and Curran (2007)" startWordPosition="4106" endWordPosition="4109">eason is that the recovery of longer-range dependencies requires more processing steps, increasing the chance of the correct structure being thrown off the beam. In contrast, the precision did not drop more quickly than C&amp;C, and in fact is consistently higher than C&amp;C across all dependency lengths, which reflects the fact that the long range dependencies our parser managed to recover are comparatively reliable. Table 3 shows the comparison of labeled precision (lp), recall (lr) and F-score (lf) for the most common CCG dependency types. The numbers for C&amp;C are for the hybrid model, copied from Clark and Curran (2007). While our shift-reduce parser gave higher precision for almost all categories, it gave higher recall on only half of them, but higher F-scores for all but one dependency type. 6.3 Final results Table 4 shows the accuracies on the test data. The numbers for the normal-form model are evaluated by running the publicly available parser, while those for the hybrid dependency model are from Clark and Curran (2007). Evaluated on all sentences, the accuracies of our parser are much higher than the C&amp;C parser, since the C&amp;C parser failed to produce any output for 10 sentences. When evaluating both 68</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Comparing the accuracy of CCG and Penn Treebank parsers.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-2009 (short papers),</booktitle>
<pages>53--56</pages>
<contexts>
<context position="1873" citStr="Clark and Curran, 2009" startWordPosition="268" endWordPosition="271">alised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&amp;C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrasestructure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as shift-reduce and CKY (Ades and Steedman, 1982; Steedman, 2000). However, the parsing work by Clark and Curran (2007), and also Hockenmaier (2003) and Fowler and Penn (2010), has only considered chart-parsing. In this paper we fill a gap in the CCG literature by developing a shiftreduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and N</context>
</contexts>
<marker>Clark, Curran, 2009</marker>
<rawString>Stephen Clark and James R. Curran. 2009. Comparing the accuracy of CCG and Penn Treebank parsers. In Proceedings of ACL-2009 (short papers), pages 53 56, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>111118</pages>
<location>Barcelona,</location>
<contexts>
<context position="3034" citStr="Collins and Roark, 2004" startWordPosition="457" endWordPosition="460">lding on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). One advantage of shift-reduce parsers is that the scoring model can be defined over actions, allowing highly efficient parsing by using a greedy algorithm in which the highest scoring action (or a small number of possible actions) is taken at each step. In addition, high accuracy can be maintained by using a model which utilises a rich set of features for making each local decision (Nivre et al., 2006). Following recent work applying global discriminative models to large-scale structured prediction problems (Collins and Roark, 2004; Miyao and Tsujii, 2005; Clark and Curran, 2007; Finkel et al., 2008), we build our shift-reduce parser using a global linear model, and compare it with the chartbased C&amp;C parser. Using standard development and test sets from CCGbank, our shift-reduce parser gives a labeled F-measure of 85.53%, which is competitive with the 85.45% F-measure of the C&amp;C parser on recovery of predicate-argument dependencies from CCGbank. Hence our work shows that 683 \x0ctransition-based parsing can be successfully applied to CCG, improving on earlier attempts such as Hassan et al. (2008). Detailed analysis show</context>
<context position="16815" citStr="Collins and Roark (2004)" startWordPosition="2708" endWordPosition="2711"> 5 S0wcS1cQ0p, S0cS1wcQ0p, S0cS1cQ0wp, S0cS1cQ0p, S0pS1pQ0p, S0wcQ0pQ1p, S0cQ0wpQ1p, S0cQ0pQ1wp, S0cQ0pQ1p, S0pQ0pQ1p, S0wcS1cS2c, S0cS1wcS2c, S0cS1cS2wc, S0cS1cS2c, S0pS1pS2p, 6 S0cS0HcS0Lc, S0cS0HcS0Rc, S1cS1HcS1Rc, S0cS0RcQ0p, S0cS0RcQ0w, S0cS0LcS1c, S0cS0LcS1w, S0cS1cS1Rc, S0wS1cS1Rc. Table 1: Feature templates. 5 Model and Training We use a global linear model to score candidate items, trained discriminatively with the averaged perceptron (Collins, 2002). Features for a (finished or partial) candidate are extracted from each action that have been applied to build the candidate. Following Collins and Roark (2004), we apply the early update strategy to perceptron training: at any step during decoding, if neither the candidate output nor any item in the agenda is correct, decoding is stopped and the parameters are updated using the current highest scored item in the agenda or the candidate output, whichever has the higher score. Table 1 shows the feature templates used by the parser. The symbols S0, S1, S2 and S3 in the table represent the top four nodes on the stack (if existent), and Q0, Q1, Q2 and Q3 represent the front four words in the incoming queue (if existent). S0H and S1H represent the subnode</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of ACL, pages 111118, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>18</pages>
<publisher>Jenny</publisher>
<location>Philadelphia, USA.</location>
<contexts>
<context position="16654" citStr="Collins, 2002" startWordPosition="2684" endWordPosition="2685">, S1Lpc, S1Lwc, S1Rpc, S1Rwc, S1Upc, S1Uwc, 4 S0wcS1wc, S0cS1w, S0wS1c, S0cS1c, S0wcQ0wp, S0cQ0wp, S0wcQ0p, S0cQ0p, S1wcQ0wp, S1cQ0wp, S1wcQ0p, S1cQ0p, 5 S0wcS1cQ0p, S0cS1wcQ0p, S0cS1cQ0wp, S0cS1cQ0p, S0pS1pQ0p, S0wcQ0pQ1p, S0cQ0wpQ1p, S0cQ0pQ1wp, S0cQ0pQ1p, S0pQ0pQ1p, S0wcS1cS2c, S0cS1wcS2c, S0cS1cS2wc, S0cS1cS2c, S0pS1pS2p, 6 S0cS0HcS0Lc, S0cS0HcS0Rc, S1cS1HcS1Rc, S0cS0RcQ0p, S0cS0RcQ0w, S0cS0LcS1c, S0cS0LcS1w, S0cS1cS1Rc, S0wS1cS1Rc. Table 1: Feature templates. 5 Model and Training We use a global linear model to score candidate items, trained discriminatively with the averaged perceptron (Collins, 2002). Features for a (finished or partial) candidate are extracted from each action that have been applied to build the candidate. Following Collins and Roark (2004), we apply the early update strategy to perceptron training: at any step during decoding, if neither the candidate output nor any item in the agenda is correct, decoding is stopped and the parameters are updated using the current highest scored item in the agenda or the candidate output, whichever has the higher score. Table 1 shows the feature templates used by the parser. The symbols S0, S1, S2 and S3 in the table represent the top f</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP, pages 18, Philadelphia, USA. Jenny Rose Finkel, Alex Kleeman, and Christopher D.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manning</author>
</authors>
<title>Feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Meeting of the ACL,</booktitle>
<pages>959967</pages>
<location>Columbus, Ohio.</location>
<marker>Manning, 2008</marker>
<rawString>Manning. 2008. Feature-based, conditional random field parsing. In Proceedings of the 46th Meeting of the ACL, pages 959967, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy A D Fowler</author>
<author>Gerald Penn</author>
</authors>
<title>Accurate context-free parsing with Combinatory Categorial Grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL-2010,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="2212" citStr="Fowler and Penn (2010)" startWordPosition="322" endWordPosition="325">, the C&amp;C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrasestructure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as shift-reduce and CKY (Ades and Steedman, 1982; Steedman, 2000). However, the parsing work by Clark and Curran (2007), and also Hockenmaier (2003) and Fowler and Penn (2010), has only considered chart-parsing. In this paper we fill a gap in the CCG literature by developing a shiftreduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). One advantage of shift-reduce parsers is that the scoring model can be defined over actions, allowing highly efficient parsing by using a greedy algorithm in which the highest scoring action (or a small number of possible actions) is taken at each step. In addition, high accuracy can be maintained by using a model</context>
<context position="7533" citStr="Fowler and Penn (2010)" startWordPosition="1177" endWordPosition="1180">vide training data for the statistical disambiguation model. This is the method used in the C&amp;C parser.2 The second approach is to read the complete grammar from the derivations, by extracting combinatory rule instances from the local trees consisting of a parent category and one or two child categories, and applying only those instances during parsing. (These rule instances also include rules to deal with punctuation and unary type-changing rules, in addition to instances of the combinatory rule schemas.) This is the method used by Hockenmaier (2003) and is the method we adopt in this paper. Fowler and Penn (2010) demonstrate that the second extraction method results in a context-free approximation to the grammar resulting from the first 2 Although the C&amp;C default mode applies a restriction for efficiency reasons in which only rule instances seen in CCGbank can be applied, making the grammar of the second type. 684 \x0cmethod, which has the potential to produce a mildlycontext sensitive grammar (given the existence of certain combinatory rules) (Weir, 1988). However, it is important to note that the advantages of CCG, in particular the tight relationship between syntax and semantic interpretation, are </context>
<context position="27487" citStr="Fowler and Penn (2010)" startWordPosition="4470" endWordPosition="4473">e) C&amp;C (normal-form) 85.48% 84.60% 85.04% 33.08% 92.86% 99.58% (C&amp;C coverage) F&amp;P (Petrov I-5)* 86.29% 85.73% 86.01% (F&amp;P C&amp;C coverage; 96.65% on dev. test) C&amp;C hybrid* 86.46% 85.11% 85.78% (F&amp;P C&amp;C coverage; 96.65% on dev. test) Table 4: Comparison with C&amp;C; final test. * not directly comparable. parsers on the sentences for which C&amp;C produces an analysis, our parser still gave the highest accuracies. The shift-reduce parser gave higher precision, and lower recall, than C&amp;C; it also gave higher sentencelevel and lexical category accuracy. The last two rows in the table show the accuracies of Fowler and Penn (2010) (F&amp;P), who applied the CFG parser of Petrov and Klein (2007) to CCG, and the corresponding accuracies for the C&amp;C parser on the same test sentences. F&amp;P can be treated as another chart-based parser; their evaluation is based on the sentences for which both their parser and C&amp;C produced dependencies (or more specifically those sentences for which generate could produce dependencies), and is not directly comparable with ours, especially considering that their test set is smaller and potentially slightly easier. The final comparison is parser speed. The shiftreduce parser is linear-time (in both</context>
</contexts>
<marker>Fowler, Penn, 2010</marker>
<rawString>Timothy A. D. Fowler and Gerald Penn. 2010. Accurate context-free parsing with Combinatory Categorial Grammar. In Proceedings of ACL-2010, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hassan</author>
<author>K Simaan</author>
<author>A Way</author>
</authors>
<title>A syntactic language model based on incremental CCG parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second IEEE Spoken Language Technology Workshop,</booktitle>
<location>Goa, India.</location>
<contexts>
<context position="3610" citStr="Hassan et al. (2008)" startWordPosition="549" endWordPosition="553">prediction problems (Collins and Roark, 2004; Miyao and Tsujii, 2005; Clark and Curran, 2007; Finkel et al., 2008), we build our shift-reduce parser using a global linear model, and compare it with the chartbased C&amp;C parser. Using standard development and test sets from CCGbank, our shift-reduce parser gives a labeled F-measure of 85.53%, which is competitive with the 85.45% F-measure of the C&amp;C parser on recovery of predicate-argument dependencies from CCGbank. Hence our work shows that 683 \x0ctransition-based parsing can be successfully applied to CCG, improving on earlier attempts such as Hassan et al. (2008). Detailed analysis shows that our shift-reduce parser yields a higher precision, lower recall and higher F-score on most of the common CCG dependency types compared to C&amp;C. One advantage of the shift-reduce parser is that it easily handles sentences for which it is difficult to find a spanning analysis, which can happen with CCG because the lexical categories at the leaves of a derivation place strong contraints on the set of possible derivations, and the supertagger which provides the lexical categories sometimes makes mistakes. Unlike the C&amp;C parser, the shift-reduce parser naturally produc</context>
</contexts>
<marker>Hassan, Simaan, Way, 2008</marker>
<rawString>H. Hassan, K. Simaan, and A. Way. 2008. A syntactic language model based on incremental CCG parsing. In Proceedings of the Second IEEE Spoken Language Technology Workshop, Goa, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="1402" citStr="Hockenmaier and Steedman, 2007" startWordPosition="196" endWordPosition="199">s with the chart-based C&amp;C parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&amp;C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&amp;C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrasestructure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with</context>
<context position="6437" citStr="Hockenmaier and Steedman, 2007" startWordPosition="997" endWordPosition="1000"> the combination of may ((S\\NP)/(S\\NP)) and like ((S\\NP)/NP) in coordination examples such as John may like but may detest Mary: (S\\NP)/(S\\NP) (S\\NP)/NP (S\\NP)/NP In addition to binary rules, such as function application and composition, there are also unary rules which operate on a single category in order to change its type. For example, forward type-raising can change a subject NP into a complex category looking to the right for a verb phrase: NP S/(S\\NP) An example CCG derivation is given in Section 3. The resource used for building wide-coverage CCG parsers of English is CCGbank (Hockenmaier and Steedman, 2007), a version of the Penn Treebank in which each phrase-structure tree has been transformed into a normal-form CCG derivation. There are two ways to extract a grammar from this resource. One approach is to extract a lexicon, i.e. a mapping from words to sets of lexical categories, and then manually define the combinatory rule schemas, such as functional application and composition, which combine the categories together. The derivations in the treebank are then used to provide training data for the statistical disambiguation model. This is the method used in the C&amp;C parser.2 The second approach i</context>
<context position="18111" citStr="Hockenmaier and Steedman, 2007" startWordPosition="2942" endWordPosition="2945">ely. S0L represents the left subnode of S0, when the lexical head is from the right subnode. S0R and S1R represent the right subnode of S0 and S1, respectively, 687 \x0cwhen the lexical head is from the left subnode. If S0 is built by a UNARY action, S0U represents the only subnode of S0. The symbols w, p and c represent the word, the POS, and the CCG category, respectively. These rich feature templates produce a large number of features: 36 million after the first training iteration, compared to around 0.5 million in the C&amp;C parser. 6 Experiments Our experiments were performed using CCGBank (Hockenmaier and Steedman, 2007), which was split into three subsets for training (Sections 0221), development testing (Section 00) and the final test (Section 23). Extracted from the training data, the CCG grammar used by our parser consists of 3070 binary rule instances and 191 unary rule instances. We compute F-scores over labeled CCG dependencies and also lexical category accuracy. CCG dependencies are defined in terms of lexical categories, by numbering each argument slot in a complex category. For example, the first NP in a transitive verb category is a CCG dependency relation, corresponding to the subject of the verb.</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1440" citStr="Hockenmaier, 2003" startWordPosition="202" endWordPosition="203">t errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&amp;C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&amp;C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrasestructure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as </context>
<context position="5350" citStr="Hockenmaier, 2003" startWordPosition="825" endWordPosition="826">plete chart, from which it can find the optimal derivation, with no pruning whatsoever at the parsing stage. In contrast, the shift-reduce parser uses a simple beam search with a relatively small beam. Perhaps surprisingly, given the ambiguity levels in an automatically-extracted grammar, and the amount of information in the CCG lexical categories which form the shift actions, the shift-reduce parser using heuristic beam search is able to outperform the chart-based parser. 2 CCG Parsing CCG, and the application of CCG to wide-coverage parsing, is described in detail elsewhere (Steedman, 2000; Hockenmaier, 2003; Clark and Curran, 2007). Here we provide only a short description. During CCG parsing, adjacent categories are combined using CCGs combinatory rules. For example, a verb phrase in English (S\\NP) can combine with 1 See e.g. Riezler et al. (2002) and Zhang et al. (2007) for chartbased parsers which can produce fragmentary analyses. an NP to its left using function application: NP S\\NP S Categories can also combine using function composition, allowing the combination of may ((S\\NP)/(S\\NP)) and like ((S\\NP)/NP) in coordination examples such as John may like but may detest Mary: (S\\NP)/(S\\</context>
<context position="7468" citStr="Hockenmaier (2003)" startWordPosition="1166" endWordPosition="1167">ogether. The derivations in the treebank are then used to provide training data for the statistical disambiguation model. This is the method used in the C&amp;C parser.2 The second approach is to read the complete grammar from the derivations, by extracting combinatory rule instances from the local trees consisting of a parent category and one or two child categories, and applying only those instances during parsing. (These rule instances also include rules to deal with punctuation and unary type-changing rules, in addition to instances of the combinatory rule schemas.) This is the method used by Hockenmaier (2003) and is the method we adopt in this paper. Fowler and Penn (2010) demonstrate that the second extraction method results in a context-free approximation to the grammar resulting from the first 2 Although the C&amp;C default mode applies a restriction for efficiency reasons in which only rule instances seen in CCGbank can be applied, making the grammar of the second type. 684 \x0cmethod, which has the potential to produce a mildlycontext sensitive grammar (given the existence of certain combinatory rules) (Weir, 1988). However, it is important to note that the advantages of CCG, in particular the ti</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and Models for Statistical Parsing with Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<date>2009</date>
<contexts>
<context position="13071" citStr="Huang et al., 2009" startWordPosition="2117" endWordPosition="2120">ft to the parsing model, but in contrast to C&amp;C the correct lexical category could be lost at any point in the heuristic search process. Hence it is perhaps surprising that we are able to achieve a high parsing accuracy of 85.5%, given a relatively small beam size. 4 Decoding Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, Fi, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not been processed, and F is a boolean value that represents whether the candidate item has been finished. A candidate item is finished if and only if the FINISH action has been applied to it, and no more actions can be applied to a candidate item after it reaches the finished</context>
</contexts>
<marker>Huang, Jiang, Liu, 2009</marker>
<rawString>Liang Huang, Wenbin Jiang, and Qun Liu. 2009.</rawString>
</citation>
<citation valid="true">
<title>Bilingually-constrained (monolingual) shift-reduce \x0cparsing.</title>
<date></date>
<booktitle>In Proceedings of the 2009 EMNLP Conference,</booktitle>
<pages>12221231</pages>
<marker></marker>
<rawString>Bilingually-constrained (monolingual) shift-reduce \x0cparsing. In Proceedings of the 2009 EMNLP Conference, pages 12221231, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Incremental dependency parsing using online learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL/EMNLP Conference,</booktitle>
<pages>11341138</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="13004" citStr="Johansson and Nugues, 2007" startWordPosition="2105" endWordPosition="2108">o the Viterbi algorithm. For the shift-reduce parser the choice is also left to the parsing model, but in contrast to C&amp;C the correct lexical category could be lost at any point in the heuristic search process. Hence it is perhaps surprising that we are able to achieve a high parsing accuracy of 85.5%, given a relatively small beam size. 4 Decoding Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, Fi, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not been processed, and F is a boolean value that represents whether the candidate item has been finished. A candidate item is finished if and only if the FINISH action has been applied to it, and no more actio</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Incremental dependency parsing using online learning. In Proceedings of the CoNLL/EMNLP Conference, pages 11341138, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun ichi Tsujii</author>
</authors>
<title>Efficient HPSG parsing with supertagging and CFG-filtering.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI-07,</booktitle>
<pages>16711676</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="28998" citStr="Matsuzaki et al. (2007)" startWordPosition="4714" endWordPosition="4717">ntation differences (C&amp;C uses heavily engineered C++ with a focus on efficiency). 7 Related Work Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. Matsuzaki et al. (2007) describes similar work to ours but using an automatically-extracted HPSG, rather than CCG, grammar. They also use the generalised perceptron to train a disambiguation model. One difference is that Matsuzaki et al. (2007) use an approximating CFG, in addition to the supertagger, to improve the efficiency of the parser. 690 \x0cNinomiya et al. (2009) (and Ninomiya et al. (2010)) describe a greedy shift-reduce parser for HPSG, in which a single action is chosen at each parsing step, allowing the possibility of highly efficient parsing. Since the HPSG grammar has relatively tight constraints, sim</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2007</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun ichi Tsujii. 2007. Efficient HPSG parsing with supertagging and CFG-filtering. In Proceedings of IJCAI-07, pages 16711676, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP/CoNLL,</booktitle>
<pages>122--131</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="23232" citStr="McDonald and Nivre (2007)" startWordPosition="3785" endWordPosition="3788">n comparison by dependency length this paper C&amp;C 50 55 60 65 70 75 80 85 90 0 5 10 15 20 25 30 recall % dependency length (bins of 5) Recall comparison by dependency length this paper C&amp;C Figure 4: P &amp; R scores relative to dependency length. 6.2 Error comparison with C&amp;C parser Our shift-reduce parser and the chart-based C&amp;C parser offer two different solutions to the CCG parsing problem. The comparison reported in this section is similar to the comparison between the chartbased MSTParser (McDonald et al., 2005) and shiftreduce MaltParser (Nivre et al., 2006) for dependency parsing. We follow McDonald and Nivre (2007) and characterize the errors of the two parsers by sentence and dependency length and dependency type. We measured precision, recall and F-score relative to different sentence lengths. Both parsers performed better on shorter sentences, as expected. Our shift-reduce parser performed consistently better than C&amp;C on all sentence lengths, and there was no significant difference in the rate of performance degradation between the parsers as the sentence length increased. Figure 4 shows the comparison of labeled precision and recall relative to the dependency length (i.e. the number of words between</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proceedings of EMNLP/CoNLL, pages 122 131, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Meeting of the ACL,</booktitle>
<pages>9198</pages>
<location>Michigan, Ann Arbor.</location>
<contexts>
<context position="23124" citStr="McDonald et al., 2005" startWordPosition="3767" endWordPosition="3770">ent test data. 60 65 70 75 80 85 90 0 5 10 15 20 25 30 precision % dependency length (bins of 5) Precision comparison by dependency length this paper C&amp;C 50 55 60 65 70 75 80 85 90 0 5 10 15 20 25 30 recall % dependency length (bins of 5) Recall comparison by dependency length this paper C&amp;C Figure 4: P &amp; R scores relative to dependency length. 6.2 Error comparison with C&amp;C parser Our shift-reduce parser and the chart-based C&amp;C parser offer two different solutions to the CCG parsing problem. The comparison reported in this section is similar to the comparison between the chartbased MSTParser (McDonald et al., 2005) and shiftreduce MaltParser (Nivre et al., 2006) for dependency parsing. We follow McDonald and Nivre (2007) and characterize the errors of the two parsers by sentence and dependency length and dependency type. We measured precision, recall and F-score relative to different sentence lengths. Both parsers performed better on shorter sentences, as expected. Our shift-reduce parser performed consistently better than C&amp;C on all sentence lengths, and there was no significant difference in the rate of performance degradation between the parsers as the sentence length increased. Figure 4 shows the co</context>
<context position="29961" citStr="McDonald et al., 2005" startWordPosition="4871" endWordPosition="4874">al. (2009) (and Ninomiya et al. (2010)) describe a greedy shift-reduce parser for HPSG, in which a single action is chosen at each parsing step, allowing the possibility of highly efficient parsing. Since the HPSG grammar has relatively tight constraints, similar to CCG, the possibility arises that a spanning analysis cannot be found for some sentences. Our approach to this problem was to allow the parser to return a fragmentary analysis; Ninomiya et al. (2009) adopt a different approach based on default unification. Finally, our work is similar to the comparison of the chart-based MSTParser (McDonald et al., 2005) and shift-reduce MaltParser (Nivre et al., 2006) for dependency parsing. MSTParser can perform exhaustive search, given certain feature restrictions, because the complexity of the parsing task is lower than for constituent parsing. C&amp;C can perform exhaustive search because the supertagger has already reduced the search space. We also found that approximate heuristic search for shift-reduce parsing, utilising a rich feature space, can match the performance of the optimal chart-based parser, as well as similar error profiles for the two CCG parsers compared to the two dependency parsers. 8 Conc</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Meeting of the ACL, pages 9198, Michigan, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Junichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd meeting of the ACL,</booktitle>
<pages>8390</pages>
<institution>University of Michigan,</institution>
<location>Ann Arbor.</location>
<contexts>
<context position="3058" citStr="Miyao and Tsujii, 2005" startWordPosition="461" endWordPosition="464"> of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). One advantage of shift-reduce parsers is that the scoring model can be defined over actions, allowing highly efficient parsing by using a greedy algorithm in which the highest scoring action (or a small number of possible actions) is taken at each step. In addition, high accuracy can be maintained by using a model which utilises a rich set of features for making each local decision (Nivre et al., 2006). Following recent work applying global discriminative models to large-scale structured prediction problems (Collins and Roark, 2004; Miyao and Tsujii, 2005; Clark and Curran, 2007; Finkel et al., 2008), we build our shift-reduce parser using a global linear model, and compare it with the chartbased C&amp;C parser. Using standard development and test sets from CCGbank, our shift-reduce parser gives a labeled F-measure of 85.53%, which is competitive with the 85.45% F-measure of the C&amp;C parser on recovery of predicate-argument dependencies from CCGbank. Hence our work shows that 683 \x0ctransition-based parsing can be successfully applied to CCG, improving on earlier attempts such as Hassan et al. (2008). Detailed analysis shows that our shift-reduce </context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Yusuke Miyao and Junichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proceedings of the 43rd meeting of the ACL, pages 8390, University of Michigan, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Takuya Matsuzaki</author>
<author>Nobuyuki Shimizu</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Deterministic shift-reduce parsing for unification-based grammars by using default unification.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL-09,</booktitle>
<pages>603611</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="29349" citStr="Ninomiya et al. (2009)" startWordPosition="4769" endWordPosition="4772"> we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. Matsuzaki et al. (2007) describes similar work to ours but using an automatically-extracted HPSG, rather than CCG, grammar. They also use the generalised perceptron to train a disambiguation model. One difference is that Matsuzaki et al. (2007) use an approximating CFG, in addition to the supertagger, to improve the efficiency of the parser. 690 \x0cNinomiya et al. (2009) (and Ninomiya et al. (2010)) describe a greedy shift-reduce parser for HPSG, in which a single action is chosen at each parsing step, allowing the possibility of highly efficient parsing. Since the HPSG grammar has relatively tight constraints, similar to CCG, the possibility arises that a spanning analysis cannot be found for some sentences. Our approach to this problem was to allow the parser to return a fragmentary analysis; Ninomiya et al. (2009) adopt a different approach based on default unification. Finally, our work is similar to the comparison of the chart-based MSTParser (McDonald e</context>
</contexts>
<marker>Ninomiya, Matsuzaki, Shimizu, Nakagawa, 2009</marker>
<rawString>Takashi Ninomiya, Takuya Matsuzaki, Nobuyuki Shimizu, and Hiroshi Nakagawa. 2009. Deterministic shift-reduce parsing for unification-based grammars by using default unification. In Proceedings of EACL-09, pages 603611, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Takuya Matsuzaki</author>
<author>Nobuyuki Shimizu</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Deterministic shift-reduce parsing for unification-based grammars.</title>
<date>2010</date>
<journal>Journal of Natural Language Engineering,</journal>
<pages>10--1017</pages>
<contexts>
<context position="29377" citStr="Ninomiya et al. (2010)" startWordPosition="4774" endWordPosition="4777">r than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. Matsuzaki et al. (2007) describes similar work to ours but using an automatically-extracted HPSG, rather than CCG, grammar. They also use the generalised perceptron to train a disambiguation model. One difference is that Matsuzaki et al. (2007) use an approximating CFG, in addition to the supertagger, to improve the efficiency of the parser. 690 \x0cNinomiya et al. (2009) (and Ninomiya et al. (2010)) describe a greedy shift-reduce parser for HPSG, in which a single action is chosen at each parsing step, allowing the possibility of highly efficient parsing. Since the HPSG grammar has relatively tight constraints, similar to CCG, the possibility arises that a spanning analysis cannot be found for some sentences. Our approach to this problem was to allow the parser to return a fragmentary analysis; Ninomiya et al. (2009) adopt a different approach based on default unification. Finally, our work is similar to the comparison of the chart-based MSTParser (McDonald et al., 2005) and shift-reduc</context>
</contexts>
<marker>Ninomiya, Matsuzaki, Shimizu, Nakagawa, 2010</marker>
<rawString>Takashi Ninomiya, Takuya Matsuzaki, Nobuyuki Shimizu, and Hiroshi Nakagawa. 2010. Deterministic shift-reduce parsing for unification-based grammars. Journal of Natural Language Engineering, DOI:10.1017/S1351324910000240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>M Scholz</author>
</authors>
<title>Deterministic dependency parsing of English text.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING04,</booktitle>
<pages>6470</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2495" citStr="Nivre and Scholz (2004)" startWordPosition="370" endWordPosition="373">), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as shift-reduce and CKY (Ades and Steedman, 1982; Steedman, 2000). However, the parsing work by Clark and Curran (2007), and also Hockenmaier (2003) and Fowler and Penn (2010), has only considered chart-parsing. In this paper we fill a gap in the CCG literature by developing a shiftreduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). One advantage of shift-reduce parsers is that the scoring model can be defined over actions, allowing highly efficient parsing by using a greedy algorithm in which the highest scoring action (or a small number of possible actions) is taken at each step. In addition, high accuracy can be maintained by using a model which utilises a rich set of features for making each local decision (Nivre et al., 2006). Following recent work applying global discriminative models to large-scale structured prediction problems (Collins and Roark, 2004; Miyao and Tsujii, 2005; Clark and Curran, 2007; Finkel et a</context>
<context position="12824" citStr="Nivre and Scholz, 2004" startWordPosition="2079" endWordPosition="2082">r solves this problem by building the complete packed chart consistent with the lexical categories supplied by the supertagger, leaving the selection of the lexical categories to the Viterbi algorithm. For the shift-reduce parser the choice is also left to the parsing model, but in contrast to C&amp;C the correct lexical category could be lost at any point in the heuristic search process. Hence it is perhaps surprising that we are able to achieve a high parsing accuracy of 85.5%, given a relatively small beam size. 4 Decoding Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, Fi, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not been processed, and F is a</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>J. Nivre and M. Scholz. 2004. Deterministic dependency parsing of English text. In Proceedings of COLING04, pages 6470, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Gulsen Eryigit</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Labeled pseudoprojective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>221225</pages>
<location>New York, USA.</location>
<contexts>
<context position="2902" citStr="Nivre et al., 2006" startWordPosition="440" endWordPosition="443"> CCG literature by developing a shiftreduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). One advantage of shift-reduce parsers is that the scoring model can be defined over actions, allowing highly efficient parsing by using a greedy algorithm in which the highest scoring action (or a small number of possible actions) is taken at each step. In addition, high accuracy can be maintained by using a model which utilises a rich set of features for making each local decision (Nivre et al., 2006). Following recent work applying global discriminative models to large-scale structured prediction problems (Collins and Roark, 2004; Miyao and Tsujii, 2005; Clark and Curran, 2007; Finkel et al., 2008), we build our shift-reduce parser using a global linear model, and compare it with the chartbased C&amp;C parser. Using standard development and test sets from CCGbank, our shift-reduce parser gives a labeled F-measure of 85.53%, which is competitive with the 85.45% F-measure of the C&amp;C parser on recovery of predicate-argument dependencies from CCGbank. Hence our work shows that 683 \x0ctransition-</context>
<context position="4271" citStr="Nivre et al., 2006" startWordPosition="654" endWordPosition="657">educe parser yields a higher precision, lower recall and higher F-score on most of the common CCG dependency types compared to C&amp;C. One advantage of the shift-reduce parser is that it easily handles sentences for which it is difficult to find a spanning analysis, which can happen with CCG because the lexical categories at the leaves of a derivation place strong contraints on the set of possible derivations, and the supertagger which provides the lexical categories sometimes makes mistakes. Unlike the C&amp;C parser, the shift-reduce parser naturally produces fragmentary analyses when appropriate (Nivre et al., 2006), and can produce sensible local structures even when a full spanning analysis cannot be found.1 Finally, considering this work in the wider parsing context, it provides an interesting comparison between heuristic beam search using a rich set of features, and optimal dynamic programming search where the feature range is restricted. We are able to perform this comparison because the use of the CCG supertagger means that the C&amp;C parser is able to build the complete chart, from which it can find the optimal derivation, with no pruning whatsoever at the parsing stage. In contrast, the shift-reduce</context>
<context position="11824" citStr="Nivre et al., 2006" startWordPosition="1911" endWordPosition="1914"> shows the shift-reduce parsing process for the example sentence IBM bought Lotus. First the word IBM is shifted onto the stack as an NP; then bought is shifted as a transitive verb looking for its object NP on the right and subject NP on the left ((S[dcl]\\NP)/NP); and then Lotus is shifted as an NP. Then bought is combined with its object Lotus resulting in a verb phrase looking for its subject on the left (S[dcl]\\NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions a shift action for each word-lexical category pair. Given the amount of syntactic information in the lexical categories, the choice of correct category, from those supplied by the supertagger, is often a difficult one, and often a choice best left to the parsing model. The C&amp;C parser solves this problem by building the complete packed chart consistent with the lexical categories supplied by the supertagger, leaving the selection of the lexical categories to the Viterbi algorithm. For the shift-reduce </context>
<context position="23172" citStr="Nivre et al., 2006" startWordPosition="3775" endWordPosition="3778"> 30 precision % dependency length (bins of 5) Precision comparison by dependency length this paper C&amp;C 50 55 60 65 70 75 80 85 90 0 5 10 15 20 25 30 recall % dependency length (bins of 5) Recall comparison by dependency length this paper C&amp;C Figure 4: P &amp; R scores relative to dependency length. 6.2 Error comparison with C&amp;C parser Our shift-reduce parser and the chart-based C&amp;C parser offer two different solutions to the CCG parsing problem. The comparison reported in this section is similar to the comparison between the chartbased MSTParser (McDonald et al., 2005) and shiftreduce MaltParser (Nivre et al., 2006) for dependency parsing. We follow McDonald and Nivre (2007) and characterize the errors of the two parsers by sentence and dependency length and dependency type. We measured precision, recall and F-score relative to different sentence lengths. Both parsers performed better on shorter sentences, as expected. Our shift-reduce parser performed consistently better than C&amp;C on all sentence lengths, and there was no significant difference in the rate of performance degradation between the parsers as the sentence length increased. Figure 4 shows the comparison of labeled precision and recall relativ</context>
<context position="30010" citStr="Nivre et al., 2006" startWordPosition="4878" endWordPosition="4881">reedy shift-reduce parser for HPSG, in which a single action is chosen at each parsing step, allowing the possibility of highly efficient parsing. Since the HPSG grammar has relatively tight constraints, similar to CCG, the possibility arises that a spanning analysis cannot be found for some sentences. Our approach to this problem was to allow the parser to return a fragmentary analysis; Ninomiya et al. (2009) adopt a different approach based on default unification. Finally, our work is similar to the comparison of the chart-based MSTParser (McDonald et al., 2005) and shift-reduce MaltParser (Nivre et al., 2006) for dependency parsing. MSTParser can perform exhaustive search, given certain feature restrictions, because the complexity of the parsing task is lower than for constituent parsing. C&amp;C can perform exhaustive search because the supertagger has already reduced the search space. We also found that approximate heuristic search for shift-reduce parsing, utilising a rich feature space, can match the performance of the optimal chart-based parser, as well as similar error profiles for the two CCG parsers compared to the two dependency parsers. 8 Conclusion This is the first work to present competit</context>
</contexts>
<marker>Nivre, Hall, Nilsson, Eryigit, Marinov, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit, and Svetoslav Marinov. 2006. Labeled pseudoprojective dependency parsing with support vector machines. In Proceedings of CoNLL, pages 221225, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>404411</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="27548" citStr="Petrov and Klein (2007)" startWordPosition="4481" endWordPosition="4484">8% (C&amp;C coverage) F&amp;P (Petrov I-5)* 86.29% 85.73% 86.01% (F&amp;P C&amp;C coverage; 96.65% on dev. test) C&amp;C hybrid* 86.46% 85.11% 85.78% (F&amp;P C&amp;C coverage; 96.65% on dev. test) Table 4: Comparison with C&amp;C; final test. * not directly comparable. parsers on the sentences for which C&amp;C produces an analysis, our parser still gave the highest accuracies. The shift-reduce parser gave higher precision, and lower recall, than C&amp;C; it also gave higher sentencelevel and lexical category accuracy. The last two rows in the table show the accuracies of Fowler and Penn (2010) (F&amp;P), who applied the CFG parser of Petrov and Klein (2007) to CCG, and the corresponding accuracies for the C&amp;C parser on the same test sentences. F&amp;P can be treated as another chart-based parser; their evaluation is based on the sentences for which both their parser and C&amp;C produced dependencies (or more specifically those sentences for which generate could produce dependencies), and is not directly comparable with ours, especially considering that their test set is smaller and potentially slightly easier. The final comparison is parser speed. The shiftreduce parser is linear-time (in both sentence length and beam size), and can analyse over 10 sent</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLT/NAACL, pages 404411, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>271278</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5597" citStr="Riezler et al. (2002)" startWordPosition="864" endWordPosition="867"> levels in an automatically-extracted grammar, and the amount of information in the CCG lexical categories which form the shift actions, the shift-reduce parser using heuristic beam search is able to outperform the chart-based parser. 2 CCG Parsing CCG, and the application of CCG to wide-coverage parsing, is described in detail elsewhere (Steedman, 2000; Hockenmaier, 2003; Clark and Curran, 2007). Here we provide only a short description. During CCG parsing, adjacent categories are combined using CCGs combinatory rules. For example, a verb phrase in English (S\\NP) can combine with 1 See e.g. Riezler et al. (2002) and Zhang et al. (2007) for chartbased parsers which can produce fragmentary analyses. an NP to its left using function application: NP S\\NP S Categories can also combine using function composition, allowing the combination of may ((S\\NP)/(S\\NP)) and like ((S\\NP)/NP) in coordination examples such as John may like but may detest Mary: (S\\NP)/(S\\NP) (S\\NP)/NP (S\\NP)/NP In addition to binary rules, such as function application and composition, there are also unary rules which operate on a single category in order to change its type. For example, forward type-raising can change a subject </context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell III, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques. In Proceedings of the 40th Meeting of the ACL, pages 271278, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
</authors>
<title>Unbounded dependency recovery for parser evaluation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP-09,</booktitle>
<pages>813821</pages>
<contexts>
<context position="1923" citStr="Rimell et al., 2009" startWordPosition="276" endWordPosition="279">applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&amp;C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrasestructure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as shift-reduce and CKY (Ades and Steedman, 1982; Steedman, 2000). However, the parsing work by Clark and Curran (2007), and also Hockenmaier (2003) and Fowler and Penn (2010), has only considered chart-parsing. In this paper we fill a gap in the CCG literature by developing a shiftreduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). One advantage of shift-red</context>
</contexts>
<marker>Rimell, Clark, Steedman, 2009</marker>
<rawString>Laura Rimell, Stephen Clark, and Mark Steedman. 2009. Unbounded dependency recovery for parser evaluation. In Proceedings of EMNLP-09, pages 813821, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>125132</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="12799" citStr="Sagae and Lavie, 2005" startWordPosition="2075" endWordPosition="2078">ng model. The C&amp;C parser solves this problem by building the complete packed chart consistent with the lexical categories supplied by the supertagger, leaving the selection of the lexical categories to the Viterbi algorithm. For the shift-reduce parser the choice is also left to the parsing model, but in contrast to C&amp;C the correct lexical category could be lost at any point in the heuristic search process. Hence it is perhaps surprising that we are able to achieve a high parsing accuracy of 85.5%, given a relatively small beam size. 4 Decoding Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, Fi, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not b</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Kenji Sagae and Alon Lavie. 2005. A classifier-based parser with linear run-time complexity. In Proceedings of IWPT, pages 125132, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A best-first probabilistic shift-reduce parser.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL poster session,</booktitle>
<pages>691698</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="11855" citStr="Sagae and Lavie, 2006" startWordPosition="1917" endWordPosition="1920">ng process for the example sentence IBM bought Lotus. First the word IBM is shifted onto the stack as an NP; then bought is shifted as a transitive verb looking for its object NP on the right and subject NP on the left ((S[dcl]\\NP)/NP); and then Lotus is shifted as an NP. Then bought is combined with its object Lotus resulting in a verb phrase looking for its subject on the left (S[dcl]\\NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions a shift action for each word-lexical category pair. Given the amount of syntactic information in the lexical categories, the choice of correct category, from those supplied by the supertagger, is often a difficult one, and often a choice best left to the parsing model. The C&amp;C parser solves this problem by building the complete packed chart consistent with the lexical categories supplied by the supertagger, leaving the selection of the lexical categories to the Viterbi algorithm. For the shift-reduce parser the choice is also left </context>
<context position="28493" citStr="Sagae and Lavie (2006" startWordPosition="4633" endWordPosition="4636">, and is not directly comparable with ours, especially considering that their test set is smaller and potentially slightly easier. The final comparison is parser speed. The shiftreduce parser is linear-time (in both sentence length and beam size), and can analyse over 10 sentences per second on a 2GHz CPU, with a beam of 16, which compares very well with other constituency parsers. However, this is no faster than the chartbased C&amp;C parser, although speed comparisons are difficult because of implementation differences (C&amp;C uses heavily engineered C++ with a focus on efficiency). 7 Related Work Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. Matsuzaki et al. (2007) describes similar work to ours but using an automatically-extracted HPSG, rather than CCG, gra</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006a. A best-first probabilistic shift-reduce parser. In Proceedings of COLING/ACL poster session, pages 691698, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL, Companion Volume: Short Papers,</booktitle>
<pages>129132</pages>
<location>New York, USA.</location>
<contexts>
<context position="11855" citStr="Sagae and Lavie, 2006" startWordPosition="1917" endWordPosition="1920">ng process for the example sentence IBM bought Lotus. First the word IBM is shifted onto the stack as an NP; then bought is shifted as a transitive verb looking for its object NP on the right and subject NP on the left ((S[dcl]\\NP)/NP); and then Lotus is shifted as an NP. Then bought is combined with its object Lotus resulting in a verb phrase looking for its subject on the left (S[dcl]\\NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions a shift action for each word-lexical category pair. Given the amount of syntactic information in the lexical categories, the choice of correct category, from those supplied by the supertagger, is often a difficult one, and often a choice best left to the parsing model. The C&amp;C parser solves this problem by building the complete packed chart consistent with the lexical categories supplied by the supertagger, leaving the selection of the lexical categories to the Viterbi algorithm. For the shift-reduce parser the choice is also left </context>
<context position="28493" citStr="Sagae and Lavie (2006" startWordPosition="4633" endWordPosition="4636">, and is not directly comparable with ours, especially considering that their test set is smaller and potentially slightly easier. The final comparison is parser speed. The shiftreduce parser is linear-time (in both sentence length and beam size), and can analyse over 10 sentences per second on a 2GHz CPU, with a beam of 16, which compares very well with other constituency parsers. However, this is no faster than the chartbased C&amp;C parser, although speed comparisons are difficult because of implementation differences (C&amp;C uses heavily engineered C++ with a focus on efficiency). 7 Related Work Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. Matsuzaki et al. (2007) describes similar work to ours but using an automatically-extracted HPSG, rather than CCG, gra</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006b. Parser combination by reparsing. In Proceedings of HLT/NAACL, Companion Volume: Short Papers, pages 129132, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="1238" citStr="Steedman (2000)" startWordPosition="173" endWordPosition="174"> explored. In this paper, we develop a shift-reduce CCG parser using a discriminative model and beam search, and compare its strengths and weaknesses with the chart-based C&amp;C parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&amp;C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&amp;C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrasestruc</context>
<context position="5331" citStr="Steedman, 2000" startWordPosition="823" endWordPosition="824">to build the complete chart, from which it can find the optimal derivation, with no pruning whatsoever at the parsing stage. In contrast, the shift-reduce parser uses a simple beam search with a relatively small beam. Perhaps surprisingly, given the ambiguity levels in an automatically-extracted grammar, and the amount of information in the CCG lexical categories which form the shift actions, the shift-reduce parser using heuristic beam search is able to outperform the chart-based parser. 2 CCG Parsing CCG, and the application of CCG to wide-coverage parsing, is described in detail elsewhere (Steedman, 2000; Hockenmaier, 2003; Clark and Curran, 2007). Here we provide only a short description. During CCG parsing, adjacent categories are combined using CCGs combinatory rules. For example, a verb phrase in English (S\\NP) can combine with 1 See e.g. Riezler et al. (2002) and Zhang et al. (2007) for chartbased parsers which can produce fragmentary analyses. an NP to its left using function application: NP S\\NP S Categories can also combine using function composition, allowing the combination of may ((S\\NP)/(S\\NP)) and like ((S\\NP)/NP) in coordination examples such as John may like but may detest</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Weir</author>
</authors>
<title>Characterizing Mildly ContextSensitive Grammar Formalisms.</title>
<date>1988</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylviania.</institution>
<contexts>
<context position="7985" citStr="Weir, 1988" startWordPosition="1252" endWordPosition="1253">n to instances of the combinatory rule schemas.) This is the method used by Hockenmaier (2003) and is the method we adopt in this paper. Fowler and Penn (2010) demonstrate that the second extraction method results in a context-free approximation to the grammar resulting from the first 2 Although the C&amp;C default mode applies a restriction for efficiency reasons in which only rule instances seen in CCGbank can be applied, making the grammar of the second type. 684 \x0cmethod, which has the potential to produce a mildlycontext sensitive grammar (given the existence of certain combinatory rules) (Weir, 1988). However, it is important to note that the advantages of CCG, in particular the tight relationship between syntax and semantic interpretation, are still maintained with the second approach, as Fowler and Penn (2010) argue. 3 The Shift-reduce CCG Parser Given an input sentence, our parser uses a stack of partial derivations, a queue of incoming words, and a series of actionsderived from the rule instances in CCGbankto build a derivation tree. Following Clark and Curran (2007), we assume that each input word has been assigned a POS-tag (from the Penn Treebank tagset) and a set of CCG lexical ca</context>
</contexts>
<marker>Weir, 1988</marker>
<rawString>David Weir. 1988. Characterizing Mildly ContextSensitive Grammar Formalisms. Ph.D. thesis, University of Pennsylviania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
</authors>
<title>Perceptron reranking for CCG realization.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>410419</pages>
<contexts>
<context position="1562" citStr="White and Rajkumar, 2009" startWordPosition="219" endWordPosition="222">&amp;C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&amp;C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrasestructure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as shift-reduce and CKY (Ades and Steedman, 1982; Steedman, 2000). However, the parsing work by Clark and Curran (2007), and </context>
</contexts>
<marker>White, Rajkumar, 2009</marker>
<rawString>Michael White and Rajakrishnan Rajkumar. 2009. Perceptron reranking for CCG realization. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 410419, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis using support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="2467" citStr="Yamada and Matsumoto (2003)" startWordPosition="364" endWordPosition="368">re trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as shift-reduce and CKY (Ades and Steedman, 1982; Steedman, 2000). However, the parsing work by Clark and Curran (2007), and also Hockenmaier (2003) and Fowler and Penn (2010), has only considered chart-parsing. In this paper we fill a gap in the CCG literature by developing a shiftreduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). One advantage of shift-reduce parsers is that the scoring model can be defined over actions, allowing highly efficient parsing by using a greedy algorithm in which the highest scoring action (or a small number of possible actions) is taken at each step. In addition, high accuracy can be maintained by using a model which utilises a rich set of features for making each local decision (Nivre et al., 2006). Following recent work applying global discriminative models to large-scale structured prediction problems (Collins and Roark, 2004; Miyao and Tsujii, 2005; Clark a</context>
<context position="12776" citStr="Yamada and Matsumoto, 2003" startWordPosition="2071" endWordPosition="2074">hoice best left to the parsing model. The C&amp;C parser solves this problem by building the complete packed chart consistent with the lexical categories supplied by the supertagger, leaving the selection of the lexical categories to the Viterbi algorithm. For the shift-reduce parser the choice is also left to the parsing model, but in contrast to C&amp;C the correct lexical category could be lost at any point in the heuristic search process. Hence it is perhaps surprising that we are able to achieve a high parsing accuracy of 85.5%, given a relatively small beam size. 4 Decoding Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, Fi, where S represents the stack with partial derivations that have been built, Q represents the queue of incomin</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H Yamada and Y Matsumoto. 2003. Statistical dependency analysis using support vector machines. In Proceedings of IWPT, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beamsearch.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP-08,</booktitle>
<location>Hawaii, USA.</location>
<contexts>
<context position="13027" citStr="Zhang and Clark, 2008" startWordPosition="2109" endWordPosition="2112"> the shift-reduce parser the choice is also left to the parsing model, but in contrast to C&amp;C the correct lexical category could be lost at any point in the heuristic search process. Hence it is perhaps surprising that we are able to achieve a high parsing accuracy of 85.5%, given a relatively small beam size. 4 Decoding Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, Fi, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not been processed, and F is a boolean value that represents whether the candidate item has been finished. A candidate item is finished if and only if the FINISH action has been applied to it, and no more actions can be applied to a </context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beamsearch. In Proceedings of EMNLP-08, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Transition-based parsing of the Chinese Treebank using a global discriminative model.</title>
<date>2009</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<location>Paris, France,</location>
<contexts>
<context position="13050" citStr="Zhang and Clark, 2009" startWordPosition="2113" endWordPosition="2116">r the choice is also left to the parsing model, but in contrast to C&amp;C the correct lexical category could be lost at any point in the heuristic search process. Hence it is perhaps surprising that we are able to achieve a high parsing accuracy of 85.5%, given a relatively small beam size. 4 Decoding Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, Fi, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not been processed, and F is a boolean value that represents whether the candidate item has been finished. A candidate item is finished if and only if the FINISH action has been applied to it, and no more actions can be applied to a candidate item after it</context>
</contexts>
<marker>Zhang, Clark, 2009</marker>
<rawString>Yue Zhang and Stephen Clark. 2009. Transition-based parsing of the Chinese Treebank using a global discriminative model. In Proceedings of IWPT, Paris, France, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Valia Kordoni</author>
<author>Erin Fitzgerald</author>
</authors>
<title>Partial parse selection for robust deep processing.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5621" citStr="Zhang et al. (2007)" startWordPosition="869" endWordPosition="872">y-extracted grammar, and the amount of information in the CCG lexical categories which form the shift actions, the shift-reduce parser using heuristic beam search is able to outperform the chart-based parser. 2 CCG Parsing CCG, and the application of CCG to wide-coverage parsing, is described in detail elsewhere (Steedman, 2000; Hockenmaier, 2003; Clark and Curran, 2007). Here we provide only a short description. During CCG parsing, adjacent categories are combined using CCGs combinatory rules. For example, a verb phrase in English (S\\NP) can combine with 1 See e.g. Riezler et al. (2002) and Zhang et al. (2007) for chartbased parsers which can produce fragmentary analyses. an NP to its left using function application: NP S\\NP S Categories can also combine using function composition, allowing the combination of may ((S\\NP)/(S\\NP)) and like ((S\\NP)/NP) in coordination examples such as John may like but may detest Mary: (S\\NP)/(S\\NP) (S\\NP)/NP (S\\NP)/NP In addition to binary rules, such as function application and composition, there are also unary rules which operate on a single category in order to change its type. For example, forward type-raising can change a subject NP into a complex catego</context>
</contexts>
<marker>Zhang, Kordoni, Fitzgerald, 2007</marker>
<rawString>Yi Zhang, Valia Kordoni, and Erin Fitzgerald. 2007. Partial parse selection for robust deep processing. In Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>