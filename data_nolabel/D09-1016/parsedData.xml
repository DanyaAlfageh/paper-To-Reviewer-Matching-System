<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.881753">
b&apos;Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 151160,
Singapore, 6-7 August 2009. c
</bodyText>
<sectionHeader confidence="0.687107" genericHeader="abstract">
2009 ACL and AFNLP
</sectionHeader>
<title confidence="0.913254">
A Unified Model of Phrasal and Sentential Evidence
for Information Extraction
</title>
<author confidence="0.809415">
Siddharth Patwardhan and Ellen Riloff
</author>
<affiliation confidence="0.9461725">
School of Computing
University of Utah
</affiliation>
<address confidence="0.571293">
Salt Lake City, UT 84112
</address>
<email confidence="0.998235">
{sidd,riloff}@cs.utah.edu
</email>
<sectionHeader confidence="0.986497" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.989127611111111">
Information Extraction (IE) systems that
extract role fillers for events typically look
at the local context surrounding a phrase
when deciding whether to extract it. Of-
ten, however, role fillers occur in clauses
that are not directly linked to an event
word. We present a new model for event
extraction that jointly considers both the
local context around a phrase along with
the wider sentential context in a proba-
bilistic framework. Our approach uses a
sentential event recognizer and a plausible
role-filler recognizer that is conditioned on
event sentences. We evaluate our system
on two IE data sets and show that our
model performs well in comparison to ex-
isting IE systems that rely on local phrasal
context.
</bodyText>
<sectionHeader confidence="0.995259" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99557201754386">
Information Extraction (IE) systems typically use
extraction patterns (e.g., Soderland et al. (1995),
Riloff (1996), Yangarber et al. (2000), Califf
and Mooney (2003)) or classifiers (e.g., Freitag
(1998), Freitag and McCallum (2000), Chieu et al.
(2003), Bunescu and Mooney (2004)) to extract
role fillers for events. Most IE systems consider
only the immediate context surrounding a phrase
when deciding whether to extract it. For tasks such
as named entity recognition, immediate context is
usually sufficient. But for more complex tasks,
such as event extraction, a larger field of view is
often needed to understand how facts tie together.
Most IE systems are designed to identify role
fillers that appear as arguments to event verbs
or nouns, either explicitly via syntactic relations
or implicitly via proximity (e.g., John murdered
Tom or the murder of Tom by John). But many
facts are presented in clauses that do not contain
event words, requiring discourse relations or deep
structural analysis to associate the facts with event
roles. For example, consider the sentences below:
Seven people have died
. . . and 30 were injured in India after terror-
ists launched an attack on the Taj Hotel.
. . . in Mexico City and its surrounding sub-
urbs in a Swine Flu outbreak.
. . . after a tractor-trailer collided with a bus
in Arkansas.
Two bridges were destroyed
. . . in Baghdad last night in a resurgence of
bomb attacks in the capital city.
. . . and $50 million in damage was caused by
a hurricane that hit Miami on Friday.
. . . to make way for modern, safer bridges
that will be constructed early next year.
These examples illustrate a common phenomenon
in text where information is not explicitly stated
as filling an event role, but readers have no trou-
ble making this inference. The role fillers above
(seven people, two bridges) occur as arguments to
verbs that reveal state information (death, destruc-
tion) but are not event-specific (i.e., death and de-
struction can result from a wide variety of incident
types). IE systems often fail to extract these role
fillers because these systems do not recognize the
immediate context as being relevant to the specific
type of event that they are looking for.
We propose a new model for information ex-
traction that incorporates both phrasal and senten-
tial evidence in a unified framework. Our uni-
fied probabilistic model, called GLACIER, consists
of two components: a model for sentential event
recognition and a model for recognizing plausi-
ble role fillers. The Sentential Event Recognizer
offers a probabilistic assessment of whether a sen-
tence is discussing a domain-relevant event. The
</bodyText>
<page confidence="0.997696">
151
</page>
<bodyText confidence="0.981068">
\x0cPlausible Role-Filler Recognizer is then condi-
tioned to identify phrases as role fillers based upon
the assumption that the surrounding context is dis-
cussing a relevant event. This unified probabilistic
model allows the two components to jointly make
decisions based upon both the local evidence sur-
rounding each phrase and the peripheral vision
afforded by the sentential event recognizer.
This paper is organized as follows. Section
2 positions our research with respect to related
work. Section 3 presents our unified probabilistic
model for information extraction. Section 4 shows
experimental results on two IE data sets, and Sec-
tion 5 discusses directions for future work.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999594945945946">
Many event extraction systems rely heavily on the
local context around words or phrases that are can-
didates for extraction. Some systems use extrac-
tion patterns (Soderland et al., 1995; Riloff, 1996;
Yangarber et al., 2000; Califf and Mooney, 2003),
which represent the immediate contexts surround-
ing candidate extractions. Similarly, classifier-
based approaches (Freitag, 1998; Freitag and Mc-
Callum, 2000; Chieu et al., 2003; Bunescu and
Mooney, 2004) rely on features in the immedi-
ate context of the candidate extractions. Our work
seeks to incorporate additional context into IE.
Indeed, several recent approaches have shown
the need for global information to improve IE per-
formance. Maslennikov and Chua (2007) use dis-
course trees and local syntactic dependencies in
a pattern-based framework to incorporate wider
context. Finkel et al. (2005) and Ji and Grish-
man (2008) incorporate global information by en-
forcing event role or label consistency over a doc-
ument or across related documents. In contrast,
our approach simply creates a richer IE model for
individual extractions by expanding the field of
view to include the surrounding sentence.
The two components of the unified model pre-
sented in this paper are somewhat similar to our
previous work (Patwardhan and Riloff, 2007),
where we employ a relevant region identification
phase prior to pattern-based extraction. In that
work we adopted a pipeline paradigm, where a
classifier identifies relevant sentences and only
those sentences are fed to the extraction module.
Our unified probabilistic model described in this
paper does not draw a hard line between rele-
vant and irrelevant sentences, but gently balances
the influence of both local and sentential contexts
through probability estimates.
</bodyText>
<sectionHeader confidence="0.980977" genericHeader="method">
3 A Unified IE Model that Combines
</sectionHeader>
<subsectionHeader confidence="0.896846">
Phrasal and Sentential Evidence
</subsectionHeader>
<bodyText confidence="0.992036086956522">
We introduce a probabilistic model for event-
based IE that balances the influence of two kinds
of contextual information. Our goal is to create
a model that has the flexibility to make extraction
decisions based upon strong evidence from the lo-
cal context, or strong evidence from the wider con-
text coupled with a more general local context. For
example, some phrases explicitly refer to an event,
so they almost certainly warrant extraction regard-
less of the wider context (e.g., terrorists launched
an attack).1 In contrast, some phrases are poten-
tially relevant but too general to warrant extrac-
tion on their own (e.g., people died could be the
result of different incident types). If we are confi-
dent that the sentence discusses an event of inter-
est, however, then such phrases could be reliably
extracted.
Our unified model for IE (GLACIER) combines
two types of contextual information by incorpo-
rating it into a probabilistic framework. To deter-
mine whether a noun phrase instance NPi should
be extracted as a filler for an event role, GLACIER
computes the joint probability that NPi :
</bodyText>
<listItem confidence="0.863074">
(1) appears in an event sentence, and
(2) is a legitimate filler for the event role.
</listItem>
<bodyText confidence="0.999438333333333">
Thus, GLACIER is designed for noun phrase ex-
traction and, mathematically, its decisions are
based on the following joint probability:
</bodyText>
<equation confidence="0.993171">
P(EvSent(SNPi ), PlausFillr(NPi ))
</equation>
<bodyText confidence="0.996225">
where SNPi is the sentence containing noun phrase
NPi . This probability estimate is based on con-
textual features F appearing within SNPi and in
the local context of NPi . Including F in the joint
probability, and applying the product rule, we can
split our probability into two components:
</bodyText>
<equation confidence="0.999639333333333">
P(EvSent(SNPi ), PlausFillr(NPi )|F) =
P(EvSent(SNPi )|F)
P(PlausFillr(NPi )|EvSent(SNPi ), F)
</equation>
<bodyText confidence="0.9935135">
These two probability components, in the expres-
sion above, form the basis of the two modules in
</bodyText>
<page confidence="0.93725">
1
</page>
<bodyText confidence="0.99925">
There are always exceptions of course, such as hypothet-
ical statements, but they are relatively uncommon.
</bodyText>
<page confidence="0.998858">
152
</page>
<bodyText confidence="0.9967615">
\x0cour IE system the sentential event recognizer and
the plausible role-filler recognizer. In arriving at
a decision to extract a noun phrase, our unified
model for IE uses these modules to estimate the
two probabilities based on the set of contextual
features F. Note that having these two probability
components allows the system to gently balance
the influence from the sentential and phrasal con-
texts, without having to make hard decisions about
sentence relevance or phrases in isolation.
In this system, the sentential event recog-
nizer is embodied in the probability compo-
nent P(EvSent(SNPi )|F). This is essentially
the probability of a sentence describing a rel-
evant event. Similarly, the plausible role-
filler recognizer is embodied by the probabil-
ity P(PlausFillr(NPi )|EvSent(SNPi ), F). This
component, therefore, estimates the probability
that a noun phrase fills a specific event role, as-
suming that the noun phrase occurs in an event
sentence. Many different techniques could be used
to produce these probability estimates. In the rest
of this section, we present the specific models that
we used for each of these components.
</bodyText>
<subsectionHeader confidence="0.999738">
3.1 Plausible Role-Filler Recognizer
</subsectionHeader>
<bodyText confidence="0.999566347826087">
The plausible role-filler recognizer is similar to
most traditional IE systems, where the goal is to
determine whether a noun phrase can be a legiti-
mate filler for a specific type of event role based on
its local context. Pattern-based approaches match
the context surrounding a phrase using lexico-
syntactic patterns or rules. However, most of these
approaches do not produce probability estimates
for the extractions. Classifier-based approaches
use machine learning classifiers to make extrac-
tion decisions, based on features associated with
the local context. Any classifier that can generate
probability estimates, or similar confidence val-
ues, could be plugged into our model.
In our work, we use a Nave Bayes classifier as
our plausible role-filler recognizer. The probabili-
ties are computed using a generative Nave Bayes
framework, based on local contextual features sur-
rounding a noun phrase. These clues include lexi-
cal matches, semantic features, and syntactic rela-
tions, and will be described in more detail in Sec-
tion 3.3. The Nave Bayes (NB) plausible role-
filler recognizer is defined as follows:
</bodyText>
<equation confidence="0.999126571428572">
P(PlausFillr(NPi )|EvSent(SNPi ), F) =
1
Z
P(PlausFillr(NPi )|EvSent(SNPi ))
Y
fiF
P(fi|PlausFillr(NPi ), EvSent(SNPi ))
</equation>
<bodyText confidence="0.959842571428572">
where F is the set of local contextual features
and Z is the normalizing constant. The prior
P(PlausFillr(NPi )|EvSent(SNPi )) is estimated
from the fraction of role fillers in the training data.
The product term in the equation is the likelihood,
which makes the simplifying assumption that all
of the features in F are independent of one an-
other. It is important to note that these probabil-
ities are conditioned on the noun phrase NPi ap-
pearing in an event sentence.
Most IE systems need to extract several differ-
ent types of role fillers for each event. For in-
stance, to extract information about terrorist inci-
dents a system may extract the names of perpetra-
tors, victims, targets, and weapons. We create a
separate IE model for each type of event role. To
construct a unified IE model for an event role, we
must specifically create a plausible role-filler rec-
ognizer for that event role, but we can use a single
sentential event recognizer for all of the role filler
types.
</bodyText>
<subsectionHeader confidence="0.999457">
3.2 Sentential Event Recognizer
</subsectionHeader>
<bodyText confidence="0.99969025">
The task at hand for the sentential event recognizer
is to analyze features in a sentence and estimate
the probability that the sentence is discussing a
relevant event. This is very similar to the task per-
formed by text classification systems, with some
minor differences. Firstly, we are dealing with
the classification of sentences, as opposed to en-
tire documents. Secondly, we need to generate a
probability estimate of the class, and not just
a class label. Like the plausible role-filler recog-
nizer, here too we employ machine learning clas-
sifiers to estimate the desired probabilities.
</bodyText>
<subsubsectionHeader confidence="0.937121">
3.2.1 Nave Bayes Event Recognizer
</subsubsectionHeader>
<bodyText confidence="0.989510666666667">
Since Nave Bayes classifiers estimate class prob-
abilities, we employ such a classifier to create a
sentential event recognizer:
</bodyText>
<equation confidence="0.999314142857143">
P(EvSent(SNPi )|F) =
1
Z
P(EvSent(SNPi ))
Y
fiF
P(fi|EvSent(SNPi ))
</equation>
<bodyText confidence="0.981612">
where Z is the normalizing constant and F is the
set of contextual features in the sentence. The
</bodyText>
<page confidence="0.998114">
153
</page>
<bodyText confidence="0.994167714285714">
\x0cprior P(EvSentS(NPi )) is obtained from the ra-
tio of event and non-event sentences in the train-
ing data. The product term in the equation is the
likelihood, which makes the simplifying assump-
tion that the features in F are independent of one
another. The features used by the model will be
described in Section 3.3.
A known issue with Nave Bayes classifiers is
that, even though their classification accuracy is
often quite reasonable, their probability estimates
are often poor (Domingos and Pazzani, 1996;
Zadrozny and Elkan, 2001; Manning et al., 2008).
The problem is that these classifiers tend to overes-
timate the probability of the predicted class, result-
ing in a situation where most probability estimates
from the classifier tend to be either extremely close
to 0.0 or extremely close to 1.0. We observed this
problem in our classifier too, so we decided to ex-
plore an additional model to estimate probabilities
for the sentential event recognizer. This second
model, based on SVMs, is described next.
</bodyText>
<subsubsectionHeader confidence="0.953692">
3.2.2 SVM Event Recognizer
</subsubsectionHeader>
<bodyText confidence="0.98167675">
Given the all-or-nothing nature of the probability
estimates that we observed from the Nave Bayes
model, we decided to try using a Support Vector
Machine (SVM) (Vapnik, 1995; Joachims, 1998)
classifier as an alternative to Nave Bayes. One
of the issues with doing this is that SVMs are not
probabilistic classifiers. SVMs make classification
decisions using on a decision boundary defined by
support vectors identified during training. A deci-
sion function is applied to unseen test examples
to determine which side of the decision bound-
ary those examples lie. While the values obtained
from the decision function only indicate class as-
signments for the examples, we used these val-
ues to produce confidence scores for our sentential
event recognizer.
To produce a confidence score from the SVM
classifier, we take the values generated by the deci-
sion function for each test instance and normalize
them based on the minimum and maximum values
produced across all of the test instances. This nor-
malization process produces values between 0 and
1 that we use as a rough indicator of the confidence
in the SVMs classification. We observed that we
could effect a consistent recall/precision trade-off
by using these values as thresholds for classifica-
tion decisions, which suggests that this approach
worked reasonably well for our task.
</bodyText>
<subsectionHeader confidence="0.999454">
3.3 Contextual Features
</subsectionHeader>
<bodyText confidence="0.997858422222222">
We used a variety of contextual features in both
components of our system. The plausible role-
filler recognizer uses the following types of fea-
tures for each candidate noun phrase NPi : lexical
head of NPi , semantic class of NPi s lexical head,
named entity tags associated with NPi and lexico-
syntactic patterns that represent the local context
surrounding NPi . The feature set is automatically
generated from the texts. Each feature is assigned
a binary value for each instance, indicating either
the presence or absence of the feature.
The named-entity features are generated by the
freely available Stanford NER tagger (Finkel et
al., 2005). We use the pre-trained NER model
that comes with the software to identify person,
organization and location names. The syntac-
tic and semantic features are generated by the
Sundance/AutoSlog system (Riloff and Phillips,
2004). We use the Sundance shallow parser to
identify lexical heads, and use its semantic dictio-
naries to assign semantic features to words. The
AutoSlog pattern generator (Riloff, 1996) is used
to create the lexico-syntactic pattern features that
capture local context around each noun phrase.
Our training sets produce a very large number
of features, which initially bogged down our clas-
sifiers. Consequently, we reduced the size of the
feature set by discarding all features that appeared
four times or less in the training set.
Our sentential event recognizer uses the same
contextual features as the plausible role-filler rec-
ognizer, except that features are generated for
every NP in the sentence. In addition, it uses
three types of sentence-level features: sentence
length, bag of words, and verb tense, which are
also binary features. We have two binary sentence
length features indicating that the sentence is long
(greater than 35 words) or is short (shorter than 5
words). Additionally, all of the words in each sen-
tence in the training data are generated as bag of
words features for the sentential model. Finally,
we generate verb tense features from all verbs ap-
pearing in each sentence. Here too we apply a fre-
quency cutoff and eliminate all features that ap-
pear four times or less in the training data.
</bodyText>
<sectionHeader confidence="0.993671" genericHeader="method">
4 IE Evaluation
</sectionHeader>
<subsectionHeader confidence="0.979327">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.9994495">
We evaluated the performance of our IE system on
two data sets: the MUC-4 terrorism corpus (Sund-
</bodyText>
<page confidence="0.997667">
154
</page>
<bodyText confidence="0.997330795454546">
\x0cheim, 1992), and a ProMed disease outbreaks cor-
pus (Phillips and Riloff, 2007; Patwardhan and
Riloff, 2007). The MUC-4 data set is a standard
IE benchmark collection of news stories about ter-
rorist events. It contains 1700 documents divided
into 1300 development (DEV) texts, and four test
sets of 100 texts each (TST1, TST2, TST3, and
TST4). Unless otherwise stated, our experiments
adopted the same training/test split used in pre-
vious research: the 1300 DEV texts for training,
200 texts (TST1+TST2) for tuning, and 200 texts
(TST3+TST4) as the blind test set. We evaluated
our system on five MUC-4 string roles: perpetra-
tor individuals, perpetrator organizations, physi-
cal targets, victims, and weapons.
The ProMed corpus consists of 120 documents
obtained from ProMed-mail2, a freely accessible
global electronic reporting system for outbreaks
of diseases. These 120 documents are paired with
corresponding answer key templates. Unless oth-
erwise noted, all of our experiments on this data
set used 5-fold cross validation. We extracted two
types of event roles: diseases and victims3.
Unlike some other IE data sets, many of the
texts in these collections do not describe a rele-
vant event. Only about half of the MUC-4 arti-
cles describe a specific terrorist incident4, and only
about 80% of the ProMed articles describe a dis-
ease outbreak. The answer keys for the irrelevant
documents are therefore empty. IE systems are es-
pecially susceptible to false hits when they can be
given texts that contain no relevant events.
The complete IE task involves the creation of
answer key templates, one template per incident
(many documents in our data sets describe multi-
ple events). Our work focuses on accurately ex-
tracting the facts from the text and not on tem-
plate generation per se (e.g., we are not concerned
with coreference resolution or which extraction
belongs in which template). Consequently, our ex-
periments evaluate the accuracy of the extractions
individually. We used head noun scoring, where
an extraction is considered to be correct if its head
noun matches the head noun in the answer key.5
</bodyText>
<page confidence="0.46754">
2
</page>
<footnote confidence="0.383617">
http://www.promedmail.org
</footnote>
<page confidence="0.849452">
3
</page>
<bodyText confidence="0.869602">
The victims can be people, animals, or plants.
</bodyText>
<page confidence="0.888338">
4
</page>
<bodyText confidence="0.79998">
With respect to the definition of terrorist incidents in the
MUC-4 guidelines (Sundheim, 1992).
</bodyText>
<page confidence="0.909619">
5
</page>
<bodyText confidence="0.9857524">
Pronouns were discarded from both the system responses
and the answer keys since we do not perform coreference res-
olution. Duplicate extractions (e.g., the same string extracted
multiple times from the same document) were conflated be-
fore being scored, so they count as just one hit or one miss.
</bodyText>
<subsectionHeader confidence="0.983076">
4.2 Baselines
</subsectionHeader>
<bodyText confidence="0.9780255">
We generated three baselines to use as compar-
isons with our IE system. As our first baseline,
</bodyText>
<equation confidence="0.834962">
we used AutoSlog-TS (Riloff, 1996), which is a
weakly-supervised, pattern-based IE system avail-
</equation>
<bodyText confidence="0.9957285">
able as part of the Sundance/AutoSlog software
package (Riloff and Phillips, 2004). Our previous
work in event-based IE (Patwardhan and Riloff,
2007) also used a pattern-based approach that ap-
plied semantic affinity patterns to relevant regions
in text. We use this system as our second base-
line. As a third baseline, we trained a Nave Bayes
IE classifier that is analogous to the plausible role-
filler recognizer in our unified IE model, except
that this baseline system is not conditioned on the
assumption of having an event sentence. Conse-
quently, this baseline NB classifier is akin to a tra-
ditional supervised learning-based IE system that
uses only local contextual features to make extrac-
tion decisions. Formally, the baseline NB classi-
fier uses the formula:
</bodyText>
<equation confidence="0.971794333333333">
P(PlausFillr(NPi )|F) =
1
Z
P(PlausFillr(NPi ))
Y
fiF
P(fi|PlausFillr(NPi ))
where F is the set of local features,
P(PlausFillr(NPi )) is the prior probability,
</equation>
<bodyText confidence="0.9624935">
and Z is the normalizing constant. We used the
Weka (Witten and Frank, 2005) implementation
of Nave Bayes for this baseline NB system.
New Jersey, February, 26. An outbreak of Ebola has
been confirmed in Mercer County, New Jersey. Five teenage
boys appear to have contracted the deadly virus from an
unknown source. The CDC is investigating the cases and is
taking measures to prevent the spread. . .
</bodyText>
<table confidence="0.94234275">
Disease: Ebola
Victims: Five teenage boys
Location: Mercer County, New Jersey
Date: February 26
</table>
<figureCaption confidence="0.988206">
Figure 1: A Disease Outbreak Event Template
</figureCaption>
<bodyText confidence="0.985789111111111">
Both the MUC-4 and ProMed data sets have
separate answer keys rather than annotated source
documents. Figure 1 shows an example of a doc-
ument and its corresponding answer key template.
To train the baseline NB system, we identify all
instances of each answer key string in the source
document and consider every instance a positive
training example. This produces noisy training
data, however, because some instances occur in
</bodyText>
<page confidence="0.99914">
155
</page>
<table confidence="0.965914866666667">
\x0cPerpInd PerpOrg Target Victim Weapon
P R F P R F P R F P R F P R F
AutoSlog-TS .33 .49 .40 .52 .33 .41 .54 .59 .56 .49 .54 .51 .38 .44 .41
Sem Affinity .48 .39 .43 .36 .58 .45 .56 .46 .50 .46 .44 .45 .53 .46 .50
NB .50 .36 .34 .35 .35 .46 .40 .53 .49 .51 .50 .50 .50 1.00 .05 .10
NB .70 .41 .25 .31 .43 .31 .36 .58 .42 .48 .58 .37 .45 1.00 .04 .07
NB .90 .51 .17 .25 .56 .15 .24 .67 .30 .41 .75 .23 .36 1.00 .02 .04
Table 1: Baseline Results on MUC-4
Disease Victim
P R F P R F
AutoSlog-TS .33 .60 .43 .36 .49 .41
Sem Affinity .31 .49 .38 .41 .47 .44
NB .50 .20 .73 .31 .29 .56 .39
NB .70 .23 .67 .34 .37 .52 .44
NB .90 .34 .59 .43 .47 .39 .43
</table>
<tableCaption confidence="0.989863">
Table 2: Baseline Results on ProMed
</tableCaption>
<bodyText confidence="0.982560758620689">
undesirable contexts. For example, if the string
man appears in an answer key as a victim, one
instance of man may refer to the actual vic-
tim in an event sentence, while another instance
of man may occur in a non-event context (e.g.,
background information) or may refer to a com-
pletely different person.
We report three evaluation metrics in our exper-
iments: precision (P), recall (R), and F-score (F),
where recall and precision are equally weighted.
For the Nave Bayes classifier, the natural thresh-
old for distinguishing between positive and nega-
tive classes is 0.5, but we also evaluated this clas-
sifier with thresholds of 0.7 and 0.9 to see if we
could effect a recall/precision trade-off. Tables 1
and 2 present the results of our three baseline sys-
tems. The NB classifier performs comparably to
AutoSlog-TS and Semantic Affinity on most event
roles, although a threshold of 0.90 is needed to
reach comparable performance on ProMed. The
relatively low numbers across the board indicate
that these corpora are challenging, but these re-
sults suggest that our plausible role-filler recog-
nizer is competitive with other existing IE sys-
tems. In Section 4.4 we will show how our unified
IE model compares to these baselines. But before
that (in the next section) we evaluate the quality of
the second component of our IE system: the sen-
tential event recognizer.
</bodyText>
<subsectionHeader confidence="0.99935">
4.3 Sentential Event Recognizer Models
</subsectionHeader>
<bodyText confidence="0.998361772727273">
The sentential event recognizer is one of the core
contributions of this research, so in this section we
evaluate it by itself, before we employ it within the
unified framework. The purpose of the sentential
event recognizer is to determine whether a sen-
tence is discussing a domain-relevant event. For
our data sets, the classifier must decide whether a
sentence is discussing a terrorist incident (MUC-
4) or a disease outbreak (ProMed). Ideally, we
want such a classifier to operate independently
from the answer keys and the extraction task per
se. For example, a terrorism IE system could be
designed to extract only perpetrators and victims
of terrorist events, or it could be designed to ex-
tract only targets and locations. The job of the sen-
tential event recognizer remains the same: to iden-
tify sentences that discuss a terrorist event. How to
train and evaluate such a system is a difficult ques-
tion. In this section, we present two approaches
that we explored to generate the training data: (a)
using the IE answer keys, and (b) using human
judgements.
</bodyText>
<subsubsectionHeader confidence="0.863525">
4.3.1 Sentence Annotation via Answer Keys
</subsubsectionHeader>
<bodyText confidence="0.998085785714286">
We have argued that the event relevance of a sen-
tence should not be tied to a specific set of event
roles. However, the IE answer keys can be used
to identify some sentences that describe an event,
because they contain an answer string. So we can
map the answer strings back to sentences in the
source documents to automatically generate event
sentence annotations.6 These annotations will be
noisy, though, because an answer string can appear
in a non-event sentence, and some event sentences
may not contain any answer strings. The alterna-
tive, however, is sentence annotations by humans,
which (as we will discuss in Section 4.3.2) is chal-
lenging.
</bodyText>
<subsectionHeader confidence="0.3822075">
4.3.2 Sentence Annotation via Human
Judgements
</subsectionHeader>
<bodyText confidence="0.758426">
For many sentences there is a clear consensus
</bodyText>
<listItem confidence="0.862807333333333">
among people that an event is being discussed. For
example, most readers would agree that sentence
(1) below is describing a terrorist event, while sen-
</listItem>
<page confidence="0.99182">
6
</page>
<bodyText confidence="0.995543333333333">
A similar strategy was used in previous work (Patward-
han and Riloff, 2007) to generate a test set for the evaluation
of a relevant region classifier.
</bodyText>
<page confidence="0.999391">
156
</page>
<table confidence="0.99928925">
\x0cEvaluation on Answer Keys Evaluation on Human Annotations
Event Non-Event Event Non-Event
Acc Pr Rec F Pr Rec F Acc Pr Rec F Pr Rec F
MUC-4 (Terrorism)
Ans NB .80 .57 .55 .56 .86 .87 .87 .81 .46 .60 .52 .91 .85 .88
SVM .80 .68 .42 .52 .84 .93 .88 .83 .55 .44 .49 .88 .91 .90
Hum
NB .82 .64 .48 .55 .85 .92 .88 .85 .56 .57 .57 .91 .91 .91
SVM .79 .64 .41 .50 .83 .91 .87 .84 .62 .51 .56 .90 .91 .91
ProMed (Disease Outbreaks)
Ans
NB .75 .62 .61 .61 .81 .82 .82 .72 .43 .58 .50 .86 .77 .81
SVM .74 .78 .31 .44 .74 .95 .83 .76 .51 .26 .35 .80 .92 .86
Hum
NB .73 .61 .46 .52 .77 .86 .81 .79 .56 .57 .56 .87 .86 .86
SVM .70 .62 .32 .42 .73 .89 .81 .79 .62 .42 .50 .84 .90 .87
</table>
<tableCaption confidence="0.842763">
Table 3: Sentential Event Recognizers Results (5-fold Cross-Validation)
</tableCaption>
<table confidence="0.9906554">
Evaluation on Human Annotations
Event Non-Event
Acc Pr Rec F Pr Rec F
NB .83 .50 .70 .58 .94 .86 .90
SVM .89 .83 .39 .53 .89 .98 .94
</table>
<tableCaption confidence="0.928014">
Table 4: Sentential Event Recognizer Results for
</tableCaption>
<table confidence="0.464398">
MUC-4 using 1300 Documents for Training
</table>
<bodyText confidence="0.9363064">
tence (2) is not. However it is difficult to draw a
clear line. Sentence (3), for example, describes an
action taken in response to a terrorist event. Is this
a terrorist event sentence? Precisely how to define
an event sentence is not obvious.
</bodyText>
<listItem confidence="0.998807">
(1) Al Qaeda operatives launched an at-
tack on the Madrid subway system.
(2) Madrid has a population of about
3.2 million people.
(3) City officials stepped up security in
</listItem>
<bodyText confidence="0.995030608695652">
response to the attacks.
We tackled this issue by creating detailed an-
notation guidelines to define the notion of an
event sentence, and conducting a human annota-
tion study. The guidelines delineated a general
time frame for the beginning and end of an event,
and constrained the task to focus on specific inci-
dents that were reported in the IE answer key. We
gave the annotators a brief description (e.g., mur-
der in Peru) of each event that had a filled answer
key in the data set. They only labeled sentences
that discussed those particular events.
We employed two human judges, who anno-
tated 120 documents from the ProMed test set,
and 100 documents from the MUC-4 test set. We
asked both judges to label 30 of the same docu-
ments from each data set so that we could compute
inter-annotator agreement. The annotators had an
agreement of 0.72 Cohens on the ProMed data,
and 0.77 Cohens on the MUC-4 data. Given
the difficulty of this task, we were satisfied that
this task is reasonably well-defined and the anno-
tations are of good quality.
</bodyText>
<subsubsectionHeader confidence="0.626478">
4.3.3 Event Recognizer Results
</subsubsectionHeader>
<bodyText confidence="0.999118151515151">
We evaluated the two sentential event recognizer
models described in Section 3.2 in two ways:
(1) using the answer key sentence annotations for
training/testing, and (2) using the human annota-
tions for training/testing. Table 3 shows the re-
sults for all combinations of training/testing data.
Since we only have human annotations for 100
MUC-4 texts and 120 ProMed texts, we performed
5-fold cross-validation on these documents. For
our classifiers, we used the Weka (Witten and
Frank, 2005) implementation of Nave Bayes and
the SVMLight (Joachims, 1998) implementation
of the SVM. For each classifier we report overall
accuracy, and precision, recall and F-scores with
respect to both the positive and negative classes
(event vs. non-event sentences).
The rows labeled Ans show the results for mod-
els trained via answer keys, and the rows labeled
Hum show the results for the models trained with
human annotations. The left side of the table
shows the results using the answer key annotations
for evaluation, and the right side of the table shows
the results using the human annotations for evalua-
tion. One expects classifiers to perform best when
they are trained and tested on the same type of
data, and our results bear this out the classifiers
that were trained and tested on the same kind of
annotations do best. The boldfaced numbers rep-
resent the best accuracies achieved for each do-
main. As we would expect, the classifiers that are
both trained and tested with human annotations
(Hum) show the best performance, with the Nave
Bayes achieving the best accuracy of 85% on the
</bodyText>
<page confidence="0.999072">
157
</page>
<table confidence="0.999123555555555">
\x0cPerpInd PerpOrg Target Victim Weapon
P R F P R F P R F P R F P R F
AutoSlog-TS .33 .49 .40 .52 .33 .41 .54 .59 .56 .49 .54 .51 .38 .44 .41
Sem Affinity .48 .39 .43 .36 .58 .45 .56 .46 .50 .46 .44 .45 .53 .46 .50
NB (baseline) .36 .34 .35 .35 .46 .40 .53 .49 .51 .50 .50 .50 1.00 .05 .10
GLACIER
NB/NB .90 .39 .59 .47 .33 .51 .40 .39 .72 .51 .52 .54 .53 .47 .55 .51
NB/SVM .40 .51 .58 .54 .34 .45 .38 .42 .72 .53 .55 .58 .56 .57 .53 .55
NB/SVM .50 .66 .47 .55 .41 .26 .32 .50 .62 .55 .62 .36 .45 .64 .43 .52
</table>
<tableCaption confidence="0.998273">
Table 5: Unified IE Model on MUC-4
</tableCaption>
<bodyText confidence="0.997214514285714">
MUC-4 texts, and the SVM achieving the best ac-
curacy of 79% on the ProMed texts.
The recall and precision for non-event sentences
is much higher than for event sentences. This clas-
sifier is forced to draw a hard line between the
event and non-event sentences, which is a difficult
task even for people. One of the advantages of our
unified IE model, which will be described in the
next section, is that it does not require hard deci-
sions but instead uses a probabilistic estimate of
how event-ish a sentence is.
Table 3 showed that models trained on human
annotations outperform models trained on answer
key annotations. But with the MUC-4 data, we
have the luxury of 1300 training documents with
answer keys, while we only have 100 documents
with human annotations. Even though the answer
key annotations are noisier, we have 13 times as
much training data.
So we trained another sentential event recog-
nizer using the entire MUC-4 training set. These
results are shown in Table 4. Observe that using
this larger (albeit noisy) training data does not ap-
pear to affect the Nave Bayes model very much.
Compared with the model trained on 100 manu-
ally annotated documents, its accuracy decreases
by 2% from 85% to 83%. The SVM model, on
the other hand, achieves an 89% accuracy when
trained with the larger MUC-4 training data, com-
pared to 84% accuracy for the model trained from
the 100 manually labeled documents. Conse-
quently, the sentential event recognizer models
used in our unified IE framework (described in
Section 4.4) are trained with this 1300 document
training set.
</bodyText>
<subsectionHeader confidence="0.969367">
4.4 Evaluation of the Unified IE Model
</subsectionHeader>
<bodyText confidence="0.9977784">
We now evaluate the performance of our unified IE
model, GLACIER, which allows a plausible role-
filler recognizer and a sentential event recognizer
to make joint decisions about phrase extractions.
Tables 5 and 6 present the results of the unified
</bodyText>
<table confidence="0.978966111111111">
Disease Victim
P R F P R F
AutoSlog-TS .33 .60 .43 .36 .49 .41
Sem Affinity .31 .49 .38 .41 .47 .44
NB (baseline) .34 .59 .43 .47 .39 .43
GLACIER
NB/NB .90 .41 .61 .49 .38 .52 .44
NB/SVM .40 .31 .66 .42 .32 .55 .41
NB/SVM .50 .38 .54 .44 .42 .47 .44
</table>
<tableCaption confidence="0.998402">
Table 6: Unified IE Model on ProMed
</tableCaption>
<bodyText confidence="0.9977509375">
IE model on the MUC-4 and ProMed data sets.
The NB/NB systems use Nave Bayes models for
both components, while the NB/SVM systems use
a Nave Bayes model for the plausible role-filler
recognizer and an SVM for the sentential event
recognizer. As with our baseline system, we ob-
tain good results using a threshold of 0.90 for our
NB/NB model (i.e., only NPs with probability
0.90 are extracted). For our NB/SVM models, we
evaluated using the default threshold (0.50) but ob-
served that recall was sometimes low. So we also
use a threshold of 0.40, which produces superior
results. Here too, we used the Weka (Witten and
Frank, 2005) implementation of the Nave Bayes
model and the SVMLight (Joachims, 1998) imple-
mentation of the SVM.
For the MUC-4 data, our unified IE model us-
ing the SVM (0.40) outperforms all 3 baselines
on three roles (PerpInd, Victim, Weapon) and
outperforms 2 of the 3 baselines on the Target
role. When GLACIER outperforms the other sys-
tems it is often by a wide margin: the F-score
for PerpInd jumped from 0.43 for the best base-
line (Sem Affinity) to 0.54 for GLACIER, and the
F-scores for Victim and Weapon each improved
by 5% over the best baseline. These gains came
from both increased recall and increased precision,
demonstrating that GLACIER extracts some infor-
mation that was missed by the other systems and
is also less prone to false hits.
Only the PerpOrg role shows inferior per-
formance. Organizations perpetrating a terrorist
</bodyText>
<page confidence="0.987416">
158
</page>
<bodyText confidence="0.99953234375">
\x0cevent are often discussed later in a document, far
removed from the main event description. For ex-
ample, a statement that Al Qaeda is believed to
be responsible for an attack would typically ap-
pear after the event description. As a result, the
sentential event recognizer tends to generate low
probabilities for such sentences. We believe that
addressing this issue would require the use of dis-
course relations or the use of even larger context
sizes. We intend to explore these avenues of re-
search in future work.
On the ProMed data, GLACIER produces results
that are similar to the baselines for the Victim role,
but it outperforms the baselines for the Disease
role. We find that for this domain, the unified IE
model with the Nave Bayes sentential event rec-
ognizer is superior to the unified IE model with
the SVM classifier. For the Disease role, the F-
score jumped 6%, from 0.43 for the best base-
line systems (AutoSlog-TS and the NB baseline)
to 0.49 for GLACIERNB/NB. In contrast to the
MUC-4 data, this improvement was mostly due
to an increase in precision (up to 0.41), indicating
that our unified IE model was effective at elimi-
nating many false hits. For the Victim role, the
performance of the unified model is comparable
to the baselines. On this event role, the F-score
of GLACIERNB/NB (0.44) matches that of the best
baseline system (Sem Affinity, with 0.44). How-
ever, note that GLACIERNB/NB can achieve a 5%
gain in recall over this baseline, at the cost of a 3%
precision loss.
</bodyText>
<subsectionHeader confidence="0.981557">
4.5 Specific Examples
</subsectionHeader>
<bodyText confidence="0.993767058823529">
Figure 2 presents some specific examples of ex-
tractions that are failed to be extracted by the
baseline models, but are correctly identified by
GLACIER because of its use of sentential evidence.
Observe that in each of these examples, GLACIER
correctly extracts the underlined phrases, in spite
of the inconclusive evidence in the local contexts
around them. In the last sentence in Figure 2, for
example, GLACIER correctly makes the inference
that the policemen in the bus (which was traveling
on the bridge) are likely the victims of the terrorist
event. Thus, we see that our system manages to
balance the influence of the two probability com-
ponents to make extraction decisions that would
be impossible to make by relying only on the local
phrasal context. In addition, the sentential event
recognizer can also help improve precision by pre-
</bodyText>
<sectionHeader confidence="0.754409033333333" genericHeader="method">
THE MNR REPORTED ON 12 JANUARY THAT HEAVILY
ARMED MEN IN CIVILIAN CLOTHES HAD INTERCEPTED
A VEHICLE WITH OQUELI AND FLORES ENROUTE FOR
LA AURORA AIRPORT AND THAT THE TWO POLITICAL
LEADERS HAD BEEN KIDNAPPED AND WERE REPORTED
MISSING.
PerpInd: HEAVILY ARMED MEN
THE SCANT POLICE INFORMATION SAID THAT THE
DEVICES WERE APPARENTLY LEFT IN FRONT OF THE TWO
BANK BRANCHES MINUTES BEFORE THE CURFEW BEGAN
FOR THE 6TH CONSECUTIVE DAY PRECISELY TO
COUNTER THE WAVE OF TERRORISM CAUSED BY DRUG
TRAFFICKERS.
Weapon: THE DEVICES
THOSE WOUNDED INCLUDE THREE EMPLOYEES OF THE
GAS STATION WHERE THE CAR BOMB WENT OFF AND
TWO PEOPLE WHO WERE WALKING BY THE GAS STATION
AT THE MOMENT OF THE EXPLOSION.
Victim: THREE EMPLOYEES OF THE GAS STATION
Victim: TWO PEOPLE
MEMBERS OF THE BOMB SQUAD HAVE DEACTIVATED
A POWERFUL BOMB PLANTED AT THE ANDRES AVELINO
CACERES PARK, WHERE PRESIDENT ALAN GARCIA WAS
DUE TO PARTICIPATE IN THE COMMEMORATION OF THE
BATTLE OF TARAPACA.
Victim: PRESIDENT ALAN GARCIA
EPL [POPULAR LIBERATION ARMY] GUERRILLAS BLEW
UP A BRIDGE AS A PUBLIC BUS, IN WHICH SEVERAL
POLICEMEN WERE TRAVELING, WAS CROSSING IT.
Victim: SEVERAL POLICEMEN
</sectionHeader>
<figureCaption confidence="0.9238825">
Figure 2: Examples of GLACIER Extractions
venting extractions from non-event sentences.
</figureCaption>
<sectionHeader confidence="0.999028" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999901272727273">
We presented a unified model for IE that balances
the influence of sentential context with local con-
textual evidence to improve the performance of
event-based IE. Our experimental results showed
that using sentential contexts indeed produced bet-
ter results on two IE data sets. Our current model
uses supervised learning, so one direction for fu-
ture work is to adapt the model for weakly super-
vised learning. We also plan to incorporate dis-
course features and investigate even wider con-
texts to capture broader discourse effects.
</bodyText>
<sectionHeader confidence="0.978515" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9812288">
This work has been supported in part by the De-
partment of Homeland Security Grant N0014-07-
1-0152. We are grateful to Nathan Gilbert and
Adam Teichert for their help with the annotation
of event sentences.
</bodyText>
<page confidence="0.998593">
159
</page>
<reference confidence="0.998744980392157">
\x0cReferences
R. Bunescu and R. Mooney. 2004. Collective In-
formation Extraction with Relational Markov Net-
works. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 438445, Barcelona, Spain, July.
M. Califf and R. Mooney. 2003. Bottom-Up Rela-
tional Learning of Pattern Matching Rules for In-
formation Extraction. Journal of Machine Learning
Research, 4:177210, December.
H. Chieu, H. Ng, and Y. Lee. 2003. Closing the
Gap: Learning-Based Information Extraction Rival-
ing Knowledge-Engineering Methods. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 216223, Sap-
poro, Japan, July.
P. Domingos and M. Pazzani. 1996. Beyond Inde-
pendence: Conditions for the Optimality of the Sim-
ple Bayesian Classifier. In Proceedings of the Thir-
teenth International Conference on Machine Learn-
ing, pages 105112, Bari, Italy, July.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating Non-local Information into Information
Extraction Systems by Gibbs Sampling. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 363370, Ann
Arbor, MI, June.
D. Freitag and A. McCallum. 2000. Informa-
tion Extraction with HMM Structures Learned by
Stochastic Optimization. In Proceedings of the Sev-
enteenth National Conference on Artificial Intelli-
gence, pages 584589, Austin, TX, August.
D. Freitag. 1998. Toward General-Purpose Learning
for Information Extraction. In Proceedings of the
36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Confer-
ence on Computational Linguistics, pages 404408,
Montreal, Quebec, August.
H. Ji and R. Grishman. 2008. Refining Event Ex-
traction through Cross-Document Inference. In Pro-
ceedings of ACL-08: HLT, pages 254262, Colum-
bus, OH, June.
T. Joachims. 1998. Text Categorization with Sup-
port Vector Machines: Learning with Many Rele-
vant Features. In Proceedings of the Tenth European
Conference on Machine Learning, pages 137142,
April.
C. Manning, P. Raghavan, and H Schutze. 2008. Intro-
duction to Information Retrieval. Cambridge Uni-
versity Press, New York, NY.
M. Maslennikov and T. Chua. 2007. A Multi-
resolution Framework for Information Extraction
from Free Text. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 592599, Prague, Czech Republic,
June.
S. Patwardhan and E. Riloff. 2007. Effective Informa-
tion Extraction with Semantic Affinity Patterns and
Relevant Regions. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 717727, Prague, Czech Re-
public, June.
W. Phillips and E. Riloff. 2007. Exploiting Role-
Identifying Nouns and Expressions for Informa-
tion Extraction. In Proceedings of International
Conference on Recent Advances in Natural Lan-
guage Processing, pages 165172, Borovets, Bul-
garia, September.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Articial Intelli-
gence, pages 10441049, Portland, OR, August.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995. CRYSTAL: Inducing a Conceptual Dictio-
nary. In Proceedings of the Fourteenth International
Joint Conference on Artificial Intelligence, pages
13141319, Montreal, Canada, August.
B. Sundheim. 1992. Overview of the Fourth Message
Understanding Evaluation and Conference. In Pro-
ceedings of the Fourth Message Understanding Con-
ference (MUC-4), pages 321, McLean, VA, June.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer, New York, NY.
I. Witten and E. Frank. 2005. Data Mining - Practical
Machine Learning Tools and Techniques. Morgan
Kaufmann, San Francisco, CA.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the 18th International Conference on Com-
putational Linguistics, pages 940946, Saarbrucken,
Germany, August.
B. Zadrozny and C. Elkan. 2001. Obtaining Cal-
ibrated Probability Estimates from Decision Trees
and Nave Bayesian Classiers. In Proceedings of the
Eighteenth International Conference on Machine
Learning, pages 609616, Williamstown, MA, June.
</reference>
<page confidence="0.862247">
160
</page>
<figure confidence="0.327616">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.412537">
<note confidence="0.813716666666667">b&apos;Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 151160, Singapore, 6-7 August 2009. c 2009 ACL and AFNLP</note>
<title confidence="0.921795">A Unified Model of Phrasal and Sentential Evidence for Information Extraction</title>
<author confidence="0.998234">Siddharth Patwardhan</author>
<author confidence="0.998234">Ellen Riloff</author>
<affiliation confidence="0.9998895">School of Computing University of Utah</affiliation>
<address confidence="0.991226">Salt Lake City, UT 84112</address>
<email confidence="0.999703">sidd@cs.utah.edu</email>
<email confidence="0.999703">riloff@cs.utah.edu</email>
<abstract confidence="0.996624736842105">Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it. Often, however, role fillers occur in clauses that are not directly linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences R Bunescu</author>
<author>R Mooney</author>
</authors>
<title>Collective Information Extraction with Relational Markov Networks.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>438445</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1381" citStr="Bunescu and Mooney (2004)" startWordPosition="208" endWordPosition="211">entential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context. 1 Introduction Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al. (1995), Riloff (1996), Yangarber et al. (2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al. (2003), Bunescu and Mooney (2004)) to extract role fillers for events. Most IE systems consider only the immediate context surrounding a phrase when deciding whether to extract it. For tasks such as named entity recognition, immediate context is usually sufficient. But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together. Most IE systems are designed to identify role fillers that appear as arguments to event verbs or nouns, either explicitly via syntactic relations or implicitly via proximity (e.g., John murdered Tom or the murder of Tom by John). But ma</context>
<context position="4899" citStr="Bunescu and Mooney, 2004" startWordPosition="784" endWordPosition="787"> probabilistic model for information extraction. Section 4 shows experimental results on two IE data sets, and Section 5 discusses directions for future work. 2 Related Work Many event extraction systems rely heavily on the local context around words or phrases that are candidates for extraction. Some systems use extraction patterns (Soderland et al., 1995; Riloff, 1996; Yangarber et al., 2000; Califf and Mooney, 2003), which represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE. Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. In contrast, our approach simply creates a richer IE mod</context>
</contexts>
<marker>Bunescu, Mooney, 2004</marker>
<rawString>\x0cReferences R. Bunescu and R. Mooney. 2004. Collective Information Extraction with Relational Markov Networks. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 438445, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Califf</author>
<author>R Mooney</author>
</authors>
<title>Bottom-Up Relational Learning of Pattern Matching Rules for Information Extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>4</volume>
<contexts>
<context position="1266" citStr="Califf and Mooney (2003)" startWordPosition="191" endWordPosition="194">ew model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context. 1 Introduction Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al. (1995), Riloff (1996), Yangarber et al. (2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al. (2003), Bunescu and Mooney (2004)) to extract role fillers for events. Most IE systems consider only the immediate context surrounding a phrase when deciding whether to extract it. For tasks such as named entity recognition, immediate context is usually sufficient. But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together. Most IE systems are designed to identify role fillers that appear as arguments to event verbs or nouns, either explicitly</context>
<context position="4696" citStr="Califf and Mooney, 2003" startWordPosition="756" endWordPosition="759">e and the peripheral vision afforded by the sentential event recognizer. This paper is organized as follows. Section 2 positions our research with respect to related work. Section 3 presents our unified probabilistic model for information extraction. Section 4 shows experimental results on two IE data sets, and Section 5 discusses directions for future work. 2 Related Work Many event extraction systems rely heavily on the local context around words or phrases that are candidates for extraction. Some systems use extraction patterns (Soderland et al., 1995; Riloff, 1996; Yangarber et al., 2000; Califf and Mooney, 2003), which represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE. Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) a</context>
</contexts>
<marker>Califf, Mooney, 2003</marker>
<rawString>M. Califf and R. Mooney. 2003. Bottom-Up Relational Learning of Pattern Matching Rules for Information Extraction. Journal of Machine Learning Research, 4:177210, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Chieu</author>
<author>H Ng</author>
<author>Y Lee</author>
</authors>
<title>Closing the Gap: Learning-Based Information Extraction Rivaling Knowledge-Engineering Methods.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>216223</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="1354" citStr="Chieu et al. (2003)" startWordPosition="204" endWordPosition="207">long with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context. 1 Introduction Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al. (1995), Riloff (1996), Yangarber et al. (2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al. (2003), Bunescu and Mooney (2004)) to extract role fillers for events. Most IE systems consider only the immediate context surrounding a phrase when deciding whether to extract it. For tasks such as named entity recognition, immediate context is usually sufficient. But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together. Most IE systems are designed to identify role fillers that appear as arguments to event verbs or nouns, either explicitly via syntactic relations or implicitly via proximity (e.g., John murdered Tom or the mur</context>
<context position="4872" citStr="Chieu et al., 2003" startWordPosition="780" endWordPosition="783">presents our unified probabilistic model for information extraction. Section 4 shows experimental results on two IE data sets, and Section 5 discusses directions for future work. 2 Related Work Many event extraction systems rely heavily on the local context around words or phrases that are candidates for extraction. Some systems use extraction patterns (Soderland et al., 1995; Riloff, 1996; Yangarber et al., 2000; Califf and Mooney, 2003), which represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE. Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. In contrast, our approach sim</context>
</contexts>
<marker>Chieu, Ng, Lee, 2003</marker>
<rawString>H. Chieu, H. Ng, and Y. Lee. 2003. Closing the Gap: Learning-Based Information Extraction Rivaling Knowledge-Engineering Methods. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 216223, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Domingos</author>
<author>M Pazzani</author>
</authors>
<title>Beyond Independence: Conditions for the Optimality of the Simple Bayesian Classifier.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth International Conference on Machine Learning,</booktitle>
<pages>105112</pages>
<location>Bari, Italy,</location>
<contexts>
<context position="13065" citStr="Domingos and Pazzani, 1996" startWordPosition="2098" endWordPosition="2101">EvSent(SNPi )) where Z is the normalizing constant and F is the set of contextual features in the sentence. The 153 \x0cprior P(EvSentS(NPi )) is obtained from the ratio of event and non-event sentences in the training data. The product term in the equation is the likelihood, which makes the simplifying assumption that the features in F are independent of one another. The features used by the model will be described in Section 3.3. A known issue with Nave Bayes classifiers is that, even though their classification accuracy is often quite reasonable, their probability estimates are often poor (Domingos and Pazzani, 1996; Zadrozny and Elkan, 2001; Manning et al., 2008). The problem is that these classifiers tend to overestimate the probability of the predicted class, resulting in a situation where most probability estimates from the classifier tend to be either extremely close to 0.0 or extremely close to 1.0. We observed this problem in our classifier too, so we decided to explore an additional model to estimate probabilities for the sentential event recognizer. This second model, based on SVMs, is described next. 3.2.2 SVM Event Recognizer Given the all-or-nothing nature of the probability estimates that we</context>
</contexts>
<marker>Domingos, Pazzani, 1996</marker>
<rawString>P. Domingos and M. Pazzani. 1996. Beyond Independence: Conditions for the Optimality of the Simple Bayesian Classifier. In Proceedings of the Thirteenth International Conference on Machine Learning, pages 105112, Bari, Italy, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>363370</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="5294" citStr="Finkel et al. (2005)" startWordPosition="845" endWordPosition="848">liff and Mooney, 2003), which represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE. Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. In contrast, our approach simply creates a richer IE model for individual extractions by expanding the field of view to include the surrounding sentence. The two components of the unified model presented in this paper are somewhat similar to our previous work (Patwardhan and Riloff, 2007), where we employ a relevant region identification phase prior to pattern-based extraction. In that work we adopted a pipeline paradigm, where a classifier identi</context>
<context position="15597" citStr="Finkel et al., 2005" startWordPosition="2508" endWordPosition="2511">textual features in both components of our system. The plausible rolefiller recognizer uses the following types of features for each candidate noun phrase NPi : lexical head of NPi , semantic class of NPi s lexical head, named entity tags associated with NPi and lexicosyntactic patterns that represent the local context surrounding NPi . The feature set is automatically generated from the texts. Each feature is assigned a binary value for each instance, indicating either the presence or absence of the feature. The named-entity features are generated by the freely available Stanford NER tagger (Finkel et al., 2005). We use the pre-trained NER model that comes with the software to identify person, organization and location names. The syntactic and semantic features are generated by the Sundance/AutoSlog system (Riloff and Phillips, 2004). We use the Sundance shallow parser to identify lexical heads, and use its semantic dictionaries to assign semantic features to words. The AutoSlog pattern generator (Riloff, 1996) is used to create the lexico-syntactic pattern features that capture local context around each noun phrase. Our training sets produce a very large number of features, which initially bogged do</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 363370, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
<author>A McCallum</author>
</authors>
<title>Information Extraction with HMM Structures Learned by Stochastic Optimization.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence,</booktitle>
<pages>584589</pages>
<location>Austin, TX,</location>
<contexts>
<context position="1333" citStr="Freitag and McCallum (2000)" startWordPosition="200" endWordPosition="203">cal context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context. 1 Introduction Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al. (1995), Riloff (1996), Yangarber et al. (2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al. (2003), Bunescu and Mooney (2004)) to extract role fillers for events. Most IE systems consider only the immediate context surrounding a phrase when deciding whether to extract it. For tasks such as named entity recognition, immediate context is usually sufficient. But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together. Most IE systems are designed to identify role fillers that appear as arguments to event verbs or nouns, either explicitly via syntactic relations or implicitly via proximity (e.g., John mu</context>
<context position="4852" citStr="Freitag and McCallum, 2000" startWordPosition="775" endWordPosition="779"> to related work. Section 3 presents our unified probabilistic model for information extraction. Section 4 shows experimental results on two IE data sets, and Section 5 discusses directions for future work. 2 Related Work Many event extraction systems rely heavily on the local context around words or phrases that are candidates for extraction. Some systems use extraction patterns (Soderland et al., 1995; Riloff, 1996; Yangarber et al., 2000; Califf and Mooney, 2003), which represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE. Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. In contra</context>
</contexts>
<marker>Freitag, McCallum, 2000</marker>
<rawString>D. Freitag and A. McCallum. 2000. Information Extraction with HMM Structures Learned by Stochastic Optimization. In Proceedings of the Seventeenth National Conference on Artificial Intelligence, pages 584589, Austin, TX, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
</authors>
<title>Toward General-Purpose Learning for Information Extraction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>404408</pages>
<location>Montreal, Quebec,</location>
<contexts>
<context position="1304" citStr="Freitag (1998)" startWordPosition="198" endWordPosition="199">ders both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context. 1 Introduction Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al. (1995), Riloff (1996), Yangarber et al. (2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al. (2003), Bunescu and Mooney (2004)) to extract role fillers for events. Most IE systems consider only the immediate context surrounding a phrase when deciding whether to extract it. For tasks such as named entity recognition, immediate context is usually sufficient. But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together. Most IE systems are designed to identify role fillers that appear as arguments to event verbs or nouns, either explicitly via syntactic relations or implicitly</context>
<context position="4824" citStr="Freitag, 1998" startWordPosition="773" endWordPosition="774">ch with respect to related work. Section 3 presents our unified probabilistic model for information extraction. Section 4 shows experimental results on two IE data sets, and Section 5 discusses directions for future work. 2 Related Work Many event extraction systems rely heavily on the local context around words or phrases that are candidates for extraction. Some systems use extraction patterns (Soderland et al., 1995; Riloff, 1996; Yangarber et al., 2000; Califf and Mooney, 2003), which represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE. Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across </context>
</contexts>
<marker>Freitag, 1998</marker>
<rawString>D. Freitag. 1998. Toward General-Purpose Learning for Information Extraction. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 404408, Montreal, Quebec, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
</authors>
<title>Refining Event Extraction through Cross-Document Inference.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>254262</pages>
<location>Columbus, OH,</location>
<contexts>
<context position="5321" citStr="Ji and Grishman (2008)" startWordPosition="850" endWordPosition="854">hich represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE. Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. In contrast, our approach simply creates a richer IE model for individual extractions by expanding the field of view to include the surrounding sentence. The two components of the unified model presented in this paper are somewhat similar to our previous work (Patwardhan and Riloff, 2007), where we employ a relevant region identification phase prior to pattern-based extraction. In that work we adopted a pipeline paradigm, where a classifier identifies relevant sentences and</context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>H. Ji and R. Grishman. 2008. Refining Event Extraction through Cross-Document Inference. In Proceedings of ACL-08: HLT, pages 254262, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>In Proceedings of the Tenth European Conference on Machine Learning,</booktitle>
<pages>137142</pages>
<contexts>
<context position="13787" citStr="Joachims, 1998" startWordPosition="2217" endWordPosition="2218">te the probability of the predicted class, resulting in a situation where most probability estimates from the classifier tend to be either extremely close to 0.0 or extremely close to 1.0. We observed this problem in our classifier too, so we decided to explore an additional model to estimate probabilities for the sentential event recognizer. This second model, based on SVMs, is described next. 3.2.2 SVM Event Recognizer Given the all-or-nothing nature of the probability estimates that we observed from the Nave Bayes model, we decided to try using a Support Vector Machine (SVM) (Vapnik, 1995; Joachims, 1998) classifier as an alternative to Nave Bayes. One of the issues with doing this is that SVMs are not probabilistic classifiers. SVMs make classification decisions using on a decision boundary defined by support vectors identified during training. A decision function is applied to unseen test examples to determine which side of the decision boundary those examples lie. While the values obtained from the decision function only indicate class assignments for the examples, we used these values to produce confidence scores for our sentential event recognizer. To produce a confidence score from the S</context>
<context position="29206" citStr="Joachims, 1998" startWordPosition="4855" endWordPosition="4856">otations are of good quality. 4.3.3 Event Recognizer Results We evaluated the two sentential event recognizer models described in Section 3.2 in two ways: (1) using the answer key sentence annotations for training/testing, and (2) using the human annotations for training/testing. Table 3 shows the results for all combinations of training/testing data. Since we only have human annotations for 100 MUC-4 texts and 120 ProMed texts, we performed 5-fold cross-validation on these documents. For our classifiers, we used the Weka (Witten and Frank, 2005) implementation of Nave Bayes and the SVMLight (Joachims, 1998) implementation of the SVM. For each classifier we report overall accuracy, and precision, recall and F-scores with respect to both the positive and negative classes (event vs. non-event sentences). The rows labeled Ans show the results for models trained via answer keys, and the rows labeled Hum show the results for the models trained with human annotations. The left side of the table shows the results using the answer key annotations for evaluation, and the right side of the table shows the results using the human annotations for evaluation. One expects classifiers to perform best when they </context>
<context position="33635" citStr="Joachims, 1998" startWordPosition="5660" endWordPosition="5661"> both components, while the NB/SVM systems use a Nave Bayes model for the plausible role-filler recognizer and an SVM for the sentential event recognizer. As with our baseline system, we obtain good results using a threshold of 0.90 for our NB/NB model (i.e., only NPs with probability 0.90 are extracted). For our NB/SVM models, we evaluated using the default threshold (0.50) but observed that recall was sometimes low. So we also use a threshold of 0.40, which produces superior results. Here too, we used the Weka (Witten and Frank, 2005) implementation of the Nave Bayes model and the SVMLight (Joachims, 1998) implementation of the SVM. For the MUC-4 data, our unified IE model using the SVM (0.40) outperforms all 3 baselines on three roles (PerpInd, Victim, Weapon) and outperforms 2 of the 3 baselines on the Target role. When GLACIER outperforms the other systems it is often by a wide margin: the F-score for PerpInd jumped from 0.43 for the best baseline (Sem Affinity) to 0.54 for GLACIER, and the F-scores for Victim and Weapon each improved by 5% over the best baseline. These gains came from both increased recall and increased precision, demonstrating that GLACIER extracts some information that wa</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. In Proceedings of the Tenth European Conference on Machine Learning, pages 137142, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>P Raghavan</author>
<author>H Schutze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="13114" citStr="Manning et al., 2008" startWordPosition="2106" endWordPosition="2109"> F is the set of contextual features in the sentence. The 153 \x0cprior P(EvSentS(NPi )) is obtained from the ratio of event and non-event sentences in the training data. The product term in the equation is the likelihood, which makes the simplifying assumption that the features in F are independent of one another. The features used by the model will be described in Section 3.3. A known issue with Nave Bayes classifiers is that, even though their classification accuracy is often quite reasonable, their probability estimates are often poor (Domingos and Pazzani, 1996; Zadrozny and Elkan, 2001; Manning et al., 2008). The problem is that these classifiers tend to overestimate the probability of the predicted class, resulting in a situation where most probability estimates from the classifier tend to be either extremely close to 0.0 or extremely close to 1.0. We observed this problem in our classifier too, so we decided to explore an additional model to estimate probabilities for the sentential event recognizer. This second model, based on SVMs, is described next. 3.2.2 SVM Event Recognizer Given the all-or-nothing nature of the probability estimates that we observed from the Nave Bayes model, we decided t</context>
</contexts>
<marker>Manning, Raghavan, Schutze, 2008</marker>
<rawString>C. Manning, P. Raghavan, and H Schutze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maslennikov</author>
<author>T Chua</author>
</authors>
<title>A Multiresolution Framework for Information Extraction from Free Text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>592599</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5161" citStr="Maslennikov and Chua (2007)" startWordPosition="825" endWordPosition="828">s that are candidates for extraction. Some systems use extraction patterns (Soderland et al., 1995; Riloff, 1996; Yangarber et al., 2000; Califf and Mooney, 2003), which represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE. Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. In contrast, our approach simply creates a richer IE model for individual extractions by expanding the field of view to include the surrounding sentence. The two components of the unified model presented in this paper are somewhat similar to our previous work (Patwardhan and Riloff, 2007), where we employ a relevant </context>
</contexts>
<marker>Maslennikov, Chua, 2007</marker>
<rawString>M. Maslennikov and T. Chua. 2007. A Multiresolution Framework for Information Extraction from Free Text. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 592599, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>E Riloff</author>
</authors>
<title>Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>717727</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5732" citStr="Patwardhan and Riloff, 2007" startWordPosition="917" endWordPosition="920">on to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. In contrast, our approach simply creates a richer IE model for individual extractions by expanding the field of view to include the surrounding sentence. The two components of the unified model presented in this paper are somewhat similar to our previous work (Patwardhan and Riloff, 2007), where we employ a relevant region identification phase prior to pattern-based extraction. In that work we adopted a pipeline paradigm, where a classifier identifies relevant sentences and only those sentences are fed to the extraction module. Our unified probabilistic model described in this paper does not draw a hard line between relevant and irrelevant sentences, but gently balances the influence of both local and sentential contexts through probability estimates. 3 A Unified IE Model that Combines Phrasal and Sentential Evidence We introduce a probabilistic model for eventbased IE that ba</context>
<context position="17380" citStr="Patwardhan and Riloff, 2007" startWordPosition="2800" endWordPosition="2803"> than 35 words) or is short (shorter than 5 words). Additionally, all of the words in each sentence in the training data are generated as bag of words features for the sentential model. Finally, we generate verb tense features from all verbs appearing in each sentence. Here too we apply a frequency cutoff and eliminate all features that appear four times or less in the training data. 4 IE Evaluation 4.1 Data Sets We evaluated the performance of our IE system on two data sets: the MUC-4 terrorism corpus (Sund154 \x0cheim, 1992), and a ProMed disease outbreaks corpus (Phillips and Riloff, 2007; Patwardhan and Riloff, 2007). The MUC-4 data set is a standard IE benchmark collection of news stories about terrorist events. It contains 1700 documents divided into 1300 development (DEV) texts, and four test sets of 100 texts each (TST1, TST2, TST3, and TST4). Unless otherwise stated, our experiments adopted the same training/test split used in previous research: the 1300 DEV texts for training, 200 texts (TST1+TST2) for tuning, and 200 texts (TST3+TST4) as the blind test set. We evaluated our system on five MUC-4 string roles: perpetrator individuals, perpetrator organizations, physical targets, victims, and weapons.</context>
<context position="20204" citStr="Patwardhan and Riloff, 2007" startWordPosition="3254" endWordPosition="3257">ded from both the system responses and the answer keys since we do not perform coreference resolution. Duplicate extractions (e.g., the same string extracted multiple times from the same document) were conflated before being scored, so they count as just one hit or one miss. 4.2 Baselines We generated three baselines to use as comparisons with our IE system. As our first baseline, we used AutoSlog-TS (Riloff, 1996), which is a weakly-supervised, pattern-based IE system available as part of the Sundance/AutoSlog software package (Riloff and Phillips, 2004). Our previous work in event-based IE (Patwardhan and Riloff, 2007) also used a pattern-based approach that applied semantic affinity patterns to relevant regions in text. We use this system as our second baseline. As a third baseline, we trained a Nave Bayes IE classifier that is analogous to the plausible rolefiller recognizer in our unified IE model, except that this baseline system is not conditioned on the assumption of having an event sentence. Consequently, this baseline NB classifier is akin to a traditional supervised learning-based IE system that uses only local contextual features to make extraction decisions. Formally, the baseline NB classifier u</context>
<context position="26114" citStr="Patwardhan and Riloff, 2007" startWordPosition="4286" endWordPosition="4290"> sentence annotations.6 These annotations will be noisy, though, because an answer string can appear in a non-event sentence, and some event sentences may not contain any answer strings. The alternative, however, is sentence annotations by humans, which (as we will discuss in Section 4.3.2) is challenging. 4.3.2 Sentence Annotation via Human Judgements For many sentences there is a clear consensus among people that an event is being discussed. For example, most readers would agree that sentence (1) below is describing a terrorist event, while sen6 A similar strategy was used in previous work (Patwardhan and Riloff, 2007) to generate a test set for the evaluation of a relevant region classifier. 156 \x0cEvaluation on Answer Keys Evaluation on Human Annotations Event Non-Event Event Non-Event Acc Pr Rec F Pr Rec F Acc Pr Rec F Pr Rec F MUC-4 (Terrorism) Ans NB .80 .57 .55 .56 .86 .87 .87 .81 .46 .60 .52 .91 .85 .88 SVM .80 .68 .42 .52 .84 .93 .88 .83 .55 .44 .49 .88 .91 .90 Hum NB .82 .64 .48 .55 .85 .92 .88 .85 .56 .57 .57 .91 .91 .91 SVM .79 .64 .41 .50 .83 .91 .87 .84 .62 .51 .56 .90 .91 .91 ProMed (Disease Outbreaks) Ans NB .75 .62 .61 .61 .81 .82 .82 .72 .43 .58 .50 .86 .77 .81 SVM .74 .78 .31 .44 .74 .95 </context>
</contexts>
<marker>Patwardhan, Riloff, 2007</marker>
<rawString>S. Patwardhan and E. Riloff. 2007. Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 717727, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Phillips</author>
<author>E Riloff</author>
</authors>
<title>Exploiting RoleIdentifying Nouns and Expressions for Information Extraction. In</title>
<date>2007</date>
<booktitle>Proceedings of International Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>165172</pages>
<location>Borovets, Bulgaria,</location>
<contexts>
<context position="17350" citStr="Phillips and Riloff, 2007" startWordPosition="2796" endWordPosition="2799">e sentence is long (greater than 35 words) or is short (shorter than 5 words). Additionally, all of the words in each sentence in the training data are generated as bag of words features for the sentential model. Finally, we generate verb tense features from all verbs appearing in each sentence. Here too we apply a frequency cutoff and eliminate all features that appear four times or less in the training data. 4 IE Evaluation 4.1 Data Sets We evaluated the performance of our IE system on two data sets: the MUC-4 terrorism corpus (Sund154 \x0cheim, 1992), and a ProMed disease outbreaks corpus (Phillips and Riloff, 2007; Patwardhan and Riloff, 2007). The MUC-4 data set is a standard IE benchmark collection of news stories about terrorist events. It contains 1700 documents divided into 1300 development (DEV) texts, and four test sets of 100 texts each (TST1, TST2, TST3, and TST4). Unless otherwise stated, our experiments adopted the same training/test split used in previous research: the 1300 DEV texts for training, 200 texts (TST1+TST2) for tuning, and 200 texts (TST3+TST4) as the blind test set. We evaluated our system on five MUC-4 string roles: perpetrator individuals, perpetrator organizations, physical </context>
</contexts>
<marker>Phillips, Riloff, 2007</marker>
<rawString>W. Phillips and E. Riloff. 2007. Exploiting RoleIdentifying Nouns and Expressions for Information Extraction. In Proceedings of International Conference on Recent Advances in Natural Language Processing, pages 165172, Borovets, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>W Phillips</author>
</authors>
<title>An Introduction to the Sundance and AutoSlog Systems.</title>
<date>2004</date>
<tech>Technical Report UUCS-04-015,</tech>
<institution>School of Computing, University of Utah.</institution>
<contexts>
<context position="15823" citStr="Riloff and Phillips, 2004" startWordPosition="2542" endWordPosition="2545"> named entity tags associated with NPi and lexicosyntactic patterns that represent the local context surrounding NPi . The feature set is automatically generated from the texts. Each feature is assigned a binary value for each instance, indicating either the presence or absence of the feature. The named-entity features are generated by the freely available Stanford NER tagger (Finkel et al., 2005). We use the pre-trained NER model that comes with the software to identify person, organization and location names. The syntactic and semantic features are generated by the Sundance/AutoSlog system (Riloff and Phillips, 2004). We use the Sundance shallow parser to identify lexical heads, and use its semantic dictionaries to assign semantic features to words. The AutoSlog pattern generator (Riloff, 1996) is used to create the lexico-syntactic pattern features that capture local context around each noun phrase. Our training sets produce a very large number of features, which initially bogged down our classifiers. Consequently, we reduced the size of the feature set by discarding all features that appeared four times or less in the training set. Our sentential event recognizer uses the same contextual features as the</context>
<context position="20137" citStr="Riloff and Phillips, 2004" startWordPosition="3244" endWordPosition="3247"> in the MUC-4 guidelines (Sundheim, 1992). 5 Pronouns were discarded from both the system responses and the answer keys since we do not perform coreference resolution. Duplicate extractions (e.g., the same string extracted multiple times from the same document) were conflated before being scored, so they count as just one hit or one miss. 4.2 Baselines We generated three baselines to use as comparisons with our IE system. As our first baseline, we used AutoSlog-TS (Riloff, 1996), which is a weakly-supervised, pattern-based IE system available as part of the Sundance/AutoSlog software package (Riloff and Phillips, 2004). Our previous work in event-based IE (Patwardhan and Riloff, 2007) also used a pattern-based approach that applied semantic affinity patterns to relevant regions in text. We use this system as our second baseline. As a third baseline, we trained a Nave Bayes IE classifier that is analogous to the plausible rolefiller recognizer in our unified IE model, except that this baseline system is not conditioned on the assumption of having an event sentence. Consequently, this baseline NB classifier is akin to a traditional supervised learning-based IE system that uses only local contextual features t</context>
</contexts>
<marker>Riloff, Phillips, 2004</marker>
<rawString>E. Riloff and W. Phillips. 2004. An Introduction to the Sundance and AutoSlog Systems. Technical Report UUCS-04-015, School of Computing, University of Utah.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically Generating Extraction Patterns from Untagged Text.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Articial Intelligence,</booktitle>
<pages>10441049</pages>
<location>Portland, OR,</location>
<contexts>
<context position="1215" citStr="Riloff (1996)" startWordPosition="185" endWordPosition="186"> linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context. 1 Introduction Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al. (1995), Riloff (1996), Yangarber et al. (2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al. (2003), Bunescu and Mooney (2004)) to extract role fillers for events. Most IE systems consider only the immediate context surrounding a phrase when deciding whether to extract it. For tasks such as named entity recognition, immediate context is usually sufficient. But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together. Most IE systems are designed to identify role fillers that appear as a</context>
<context position="4646" citStr="Riloff, 1996" startWordPosition="750" endWordPosition="751"> local evidence surrounding each phrase and the peripheral vision afforded by the sentential event recognizer. This paper is organized as follows. Section 2 positions our research with respect to related work. Section 3 presents our unified probabilistic model for information extraction. Section 4 shows experimental results on two IE data sets, and Section 5 discusses directions for future work. 2 Related Work Many event extraction systems rely heavily on the local context around words or phrases that are candidates for extraction. Some systems use extraction patterns (Soderland et al., 1995; Riloff, 1996; Yangarber et al., 2000; Califf and Mooney, 2003), which represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE. Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to</context>
<context position="16004" citStr="Riloff, 1996" startWordPosition="2572" endWordPosition="2573">assigned a binary value for each instance, indicating either the presence or absence of the feature. The named-entity features are generated by the freely available Stanford NER tagger (Finkel et al., 2005). We use the pre-trained NER model that comes with the software to identify person, organization and location names. The syntactic and semantic features are generated by the Sundance/AutoSlog system (Riloff and Phillips, 2004). We use the Sundance shallow parser to identify lexical heads, and use its semantic dictionaries to assign semantic features to words. The AutoSlog pattern generator (Riloff, 1996) is used to create the lexico-syntactic pattern features that capture local context around each noun phrase. Our training sets produce a very large number of features, which initially bogged down our classifiers. Consequently, we reduced the size of the feature set by discarding all features that appeared four times or less in the training set. Our sentential event recognizer uses the same contextual features as the plausible role-filler recognizer, except that features are generated for every NP in the sentence. In addition, it uses three types of sentence-level features: sentence length, bag</context>
<context position="19994" citStr="Riloff, 1996" startWordPosition="3226" endWordPosition="3227">http://www.promedmail.org 3 The victims can be people, animals, or plants. 4 With respect to the definition of terrorist incidents in the MUC-4 guidelines (Sundheim, 1992). 5 Pronouns were discarded from both the system responses and the answer keys since we do not perform coreference resolution. Duplicate extractions (e.g., the same string extracted multiple times from the same document) were conflated before being scored, so they count as just one hit or one miss. 4.2 Baselines We generated three baselines to use as comparisons with our IE system. As our first baseline, we used AutoSlog-TS (Riloff, 1996), which is a weakly-supervised, pattern-based IE system available as part of the Sundance/AutoSlog software package (Riloff and Phillips, 2004). Our previous work in event-based IE (Patwardhan and Riloff, 2007) also used a pattern-based approach that applied semantic affinity patterns to relevant regions in text. We use this system as our second baseline. As a third baseline, we trained a Nave Bayes IE classifier that is analogous to the plausible rolefiller recognizer in our unified IE model, except that this baseline system is not conditioned on the assumption of having an event sentence. Co</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proceedings of the Thirteenth National Conference on Articial Intelligence, pages 10441049, Portland, OR, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soderland</author>
<author>D Fisher</author>
<author>J Aseltine</author>
<author>W Lehnert</author>
</authors>
<title>CRYSTAL: Inducing a Conceptual Dictionary.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>13141319</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="1200" citStr="Soderland et al. (1995)" startWordPosition="181" endWordPosition="184">ses that are not directly linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context. 1 Introduction Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al. (1995), Riloff (1996), Yangarber et al. (2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al. (2003), Bunescu and Mooney (2004)) to extract role fillers for events. Most IE systems consider only the immediate context surrounding a phrase when deciding whether to extract it. For tasks such as named entity recognition, immediate context is usually sufficient. But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together. Most IE systems are designed to identify role fillers t</context>
<context position="4632" citStr="Soderland et al., 1995" startWordPosition="746" endWordPosition="749">ions based upon both the local evidence surrounding each phrase and the peripheral vision afforded by the sentential event recognizer. This paper is organized as follows. Section 2 positions our research with respect to related work. Section 3 presents our unified probabilistic model for information extraction. Section 4 shows experimental results on two IE data sets, and Section 5 discusses directions for future work. 2 Related Work Many event extraction systems rely heavily on the local context around words or phrases that are candidates for extraction. Some systems use extraction patterns (Soderland et al., 1995; Riloff, 1996; Yangarber et al., 2000; Califf and Mooney, 2003), which represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE. Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-base</context>
</contexts>
<marker>Soderland, Fisher, Aseltine, Lehnert, 1995</marker>
<rawString>S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995. CRYSTAL: Inducing a Conceptual Dictionary. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, pages 13141319, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sundheim</author>
</authors>
<title>Overview of the Fourth Message Understanding Evaluation and Conference.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth Message Understanding Conference (MUC-4),</booktitle>
<pages>321</pages>
<location>McLean, VA,</location>
<contexts>
<context position="19552" citStr="Sundheim, 1992" startWordPosition="3152" endWordPosition="3153">tiple events). Our work focuses on accurately extracting the facts from the text and not on template generation per se (e.g., we are not concerned with coreference resolution or which extraction belongs in which template). Consequently, our experiments evaluate the accuracy of the extractions individually. We used head noun scoring, where an extraction is considered to be correct if its head noun matches the head noun in the answer key.5 2 http://www.promedmail.org 3 The victims can be people, animals, or plants. 4 With respect to the definition of terrorist incidents in the MUC-4 guidelines (Sundheim, 1992). 5 Pronouns were discarded from both the system responses and the answer keys since we do not perform coreference resolution. Duplicate extractions (e.g., the same string extracted multiple times from the same document) were conflated before being scored, so they count as just one hit or one miss. 4.2 Baselines We generated three baselines to use as comparisons with our IE system. As our first baseline, we used AutoSlog-TS (Riloff, 1996), which is a weakly-supervised, pattern-based IE system available as part of the Sundance/AutoSlog software package (Riloff and Phillips, 2004). Our previous </context>
</contexts>
<marker>Sundheim, 1992</marker>
<rawString>B. Sundheim. 1992. Overview of the Fourth Message Understanding Evaluation and Conference. In Proceedings of the Fourth Message Understanding Conference (MUC-4), pages 321, McLean, VA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="13770" citStr="Vapnik, 1995" startWordPosition="2215" endWordPosition="2216"> to overestimate the probability of the predicted class, resulting in a situation where most probability estimates from the classifier tend to be either extremely close to 0.0 or extremely close to 1.0. We observed this problem in our classifier too, so we decided to explore an additional model to estimate probabilities for the sentential event recognizer. This second model, based on SVMs, is described next. 3.2.2 SVM Event Recognizer Given the all-or-nothing nature of the probability estimates that we observed from the Nave Bayes model, we decided to try using a Support Vector Machine (SVM) (Vapnik, 1995; Joachims, 1998) classifier as an alternative to Nave Bayes. One of the issues with doing this is that SVMs are not probabilistic classifiers. SVMs make classification decisions using on a decision boundary defined by support vectors identified during training. A decision function is applied to unseen test examples to determine which side of the decision boundary those examples lie. While the values obtained from the decision function only indicate class assignments for the examples, we used these values to produce confidence scores for our sentential event recognizer. To produce a confidence</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining - Practical Machine Learning Tools and Techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="21058" citStr="Witten and Frank, 2005" startWordPosition="3395" endWordPosition="3398"> plausible rolefiller recognizer in our unified IE model, except that this baseline system is not conditioned on the assumption of having an event sentence. Consequently, this baseline NB classifier is akin to a traditional supervised learning-based IE system that uses only local contextual features to make extraction decisions. Formally, the baseline NB classifier uses the formula: P(PlausFillr(NPi )|F) = 1 Z P(PlausFillr(NPi )) Y fiF P(fi|PlausFillr(NPi )) where F is the set of local features, P(PlausFillr(NPi )) is the prior probability, and Z is the normalizing constant. We used the Weka (Witten and Frank, 2005) implementation of Nave Bayes for this baseline NB system. New Jersey, February, 26. An outbreak of Ebola has been confirmed in Mercer County, New Jersey. Five teenage boys appear to have contracted the deadly virus from an unknown source. The CDC is investigating the cases and is taking measures to prevent the spread. . . Disease: Ebola Victims: Five teenage boys Location: Mercer County, New Jersey Date: February 26 Figure 1: A Disease Outbreak Event Template Both the MUC-4 and ProMed data sets have separate answer keys rather than annotated source documents. Figure 1 shows an example of a do</context>
<context position="29143" citStr="Witten and Frank, 2005" startWordPosition="4844" endWordPosition="4847">we were satisfied that this task is reasonably well-defined and the annotations are of good quality. 4.3.3 Event Recognizer Results We evaluated the two sentential event recognizer models described in Section 3.2 in two ways: (1) using the answer key sentence annotations for training/testing, and (2) using the human annotations for training/testing. Table 3 shows the results for all combinations of training/testing data. Since we only have human annotations for 100 MUC-4 texts and 120 ProMed texts, we performed 5-fold cross-validation on these documents. For our classifiers, we used the Weka (Witten and Frank, 2005) implementation of Nave Bayes and the SVMLight (Joachims, 1998) implementation of the SVM. For each classifier we report overall accuracy, and precision, recall and F-scores with respect to both the positive and negative classes (event vs. non-event sentences). The rows labeled Ans show the results for models trained via answer keys, and the rows labeled Hum show the results for the models trained with human annotations. The left side of the table shows the results using the answer key annotations for evaluation, and the right side of the table shows the results using the human annotations for</context>
<context position="33562" citStr="Witten and Frank, 2005" startWordPosition="5647" endWordPosition="5650">el on the MUC-4 and ProMed data sets. The NB/NB systems use Nave Bayes models for both components, while the NB/SVM systems use a Nave Bayes model for the plausible role-filler recognizer and an SVM for the sentential event recognizer. As with our baseline system, we obtain good results using a threshold of 0.90 for our NB/NB model (i.e., only NPs with probability 0.90 are extracted). For our NB/SVM models, we evaluated using the default threshold (0.50) but observed that recall was sometimes low. So we also use a threshold of 0.40, which produces superior results. Here too, we used the Weka (Witten and Frank, 2005) implementation of the Nave Bayes model and the SVMLight (Joachims, 1998) implementation of the SVM. For the MUC-4 data, our unified IE model using the SVM (0.40) outperforms all 3 baselines on three roles (PerpInd, Victim, Weapon) and outperforms 2 of the 3 baselines on the Target role. When GLACIER outperforms the other systems it is often by a wide margin: the F-score for PerpInd jumped from 0.43 for the best baseline (Sem Affinity) to 0.54 for GLACIER, and the F-scores for Victim and Weapon each improved by 5% over the best baseline. These gains came from both increased recall and increase</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>I. Witten and E. Frank. 2005. Data Mining - Practical Machine Learning Tools and Techniques. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
<author>R Grishman</author>
<author>P Tapanainen</author>
<author>S Huttunen</author>
</authors>
<title>Automatic Acquisition of Domain Knowledge for Information Extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>940946</pages>
<location>Saarbrucken, Germany,</location>
<contexts>
<context position="1240" citStr="Yangarber et al. (2000)" startWordPosition="187" endWordPosition="190">vent word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context. 1 Introduction Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al. (1995), Riloff (1996), Yangarber et al. (2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al. (2003), Bunescu and Mooney (2004)) to extract role fillers for events. Most IE systems consider only the immediate context surrounding a phrase when deciding whether to extract it. For tasks such as named entity recognition, immediate context is usually sufficient. But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together. Most IE systems are designed to identify role fillers that appear as arguments to event verbs o</context>
<context position="4670" citStr="Yangarber et al., 2000" startWordPosition="752" endWordPosition="755">e surrounding each phrase and the peripheral vision afforded by the sentential event recognizer. This paper is organized as follows. Section 2 positions our research with respect to related work. Section 3 presents our unified probabilistic model for information extraction. Section 4 shows experimental results on two IE data sets, and Section 5 discusses directions for future work. 2 Related Work Many event extraction systems rely heavily on the local context around words or phrases that are candidates for extraction. Some systems use extraction patterns (Soderland et al., 1995; Riloff, 1996; Yangarber et al., 2000; Califf and Mooney, 2003), which represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE. Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider conte</context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen. 2000. Automatic Acquisition of Domain Knowledge for Information Extraction. In Proceedings of the 18th International Conference on Computational Linguistics, pages 940946, Saarbrucken, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zadrozny</author>
<author>C Elkan</author>
</authors>
<title>Obtaining Calibrated Probability Estimates from Decision Trees and Nave Bayesian Classiers.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>609616</pages>
<location>Williamstown, MA,</location>
<contexts>
<context position="13091" citStr="Zadrozny and Elkan, 2001" startWordPosition="2102" endWordPosition="2105">e normalizing constant and F is the set of contextual features in the sentence. The 153 \x0cprior P(EvSentS(NPi )) is obtained from the ratio of event and non-event sentences in the training data. The product term in the equation is the likelihood, which makes the simplifying assumption that the features in F are independent of one another. The features used by the model will be described in Section 3.3. A known issue with Nave Bayes classifiers is that, even though their classification accuracy is often quite reasonable, their probability estimates are often poor (Domingos and Pazzani, 1996; Zadrozny and Elkan, 2001; Manning et al., 2008). The problem is that these classifiers tend to overestimate the probability of the predicted class, resulting in a situation where most probability estimates from the classifier tend to be either extremely close to 0.0 or extremely close to 1.0. We observed this problem in our classifier too, so we decided to explore an additional model to estimate probabilities for the sentential event recognizer. This second model, based on SVMs, is described next. 3.2.2 SVM Event Recognizer Given the all-or-nothing nature of the probability estimates that we observed from the Nave Ba</context>
</contexts>
<marker>Zadrozny, Elkan, 2001</marker>
<rawString>B. Zadrozny and C. Elkan. 2001. Obtaining Calibrated Probability Estimates from Decision Trees and Nave Bayesian Classiers. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 609616, Williamstown, MA, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>