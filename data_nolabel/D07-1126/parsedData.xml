<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.350133">
b&amp;apos;Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 11491155,
Prague, June 2007. c
</bodyText>
<sectionHeader confidence="0.317264" genericHeader="abstract">
2007 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.9300355">
A Multilingual Dependency Analysis System using Online
Passive-Aggressive Learning
</title>
<author confidence="0.980261">
Le-Minh Nguyen, Akira Shimazu, and Phuong-Thai Nguyen
</author>
<affiliation confidence="0.989723">
Japan Advanced Institute of Science and Technology (JAIST)
</affiliation>
<address confidence="0.737443">
Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan
</address>
<email confidence="0.983445">
{nguyenml,shimazu,thai}@jaist.ac.jp
</email>
<author confidence="0.945901">
Xuan-Hieu Phan
</author>
<affiliation confidence="0.993822">
Tohoku University
</affiliation>
<address confidence="0.904051">
Aobayama 6-3-09, Sendai, 980-8579, Japan
</address>
<email confidence="0.986582">
hieuxuan@ecei.tohoku.ac.jp
</email>
<sectionHeader confidence="0.990488" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999632">
This paper presents an online algorithm for
dependency parsing problems. We propose
an adaptation of the passive and aggressive
online learning algorithm to the dependency
parsing domain. We evaluate the proposed
algorithms on the 2007 CONLL Shared
Task, and report errors analysis. Experimen-
tal results show that the system score is bet-
ter than the average score among the partici-
pating systems.
</bodyText>
<sectionHeader confidence="0.998147" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998278976744186">
Research on dependency parsing is mainly based
on machine learning methods, which can be called
history-based (Yamada and Matsumoto, 2003; Nivre
et al., 2006) and discriminative learning methods
(McDonald et al., 2005a; Corston-Oliver et al.,
2006). The learning methods using in discrimina-
tive parsing are Perceptron (Collins, 2002) and on-
line large-margin learning (MIRA) (Crammer and
Singer, 2003).
The difference of MIRA-based parsing in com-
parison with history-based methods is that the
MIRA-based parser were trained to maximize the
accuracy of the overall tree. The MIRA based
parsing is close to maximum-margin parsing as in
Taskar et al. (2004) and Tsochantaridis et al. (2005)
for parsing. However, unlike maximum-margin
parsing, it is not limited to parsing sentences of 15
words or less due to computation time. The perfor-
mance of MIRA based parsing achieves the state-of-
the-art performance in English data (McDonald et
al., 2005a; McDonald et al., 2006).
In this paper, we propose a new adaptation of on-
line larger-margin learning to the problem of depen-
dency parsing. Unlike the MIRA parser, our method
does not need an optimization procedure in each
learning update, but users only an update equation.
This might lead to faster training time and easier im-
plementation.
The contributions of this paper are two-fold: First,
we present a training algorithm called PA learning
for dependency parsing, which is as easy to im-
plement as Perceptron, yet competitive with large
margin methods. This algorithm has implications
for anyone interested in implementing discrimina-
tive training methods for any application. Second,
we evaluate the proposed algorithm on the multilin-
gual data task as well as the domain adaptation task
(Nivre et al., 2007).
The remaining parts of the paper are organized as
follows: Section 2 proposes our dependency pars-
ing with Passive-Aggressive learning. Section 3
discusses some experimental results and Section 4
gives conclusions and plans for future work.
</bodyText>
<sectionHeader confidence="0.766985" genericHeader="method">
2 Dependency Parsing with
Passive-Aggressive Learning
</sectionHeader>
<bodyText confidence="0.998640555555556">
This section presents the modification of Passive-
Aggressive Learning (PA) (Crammer et al., 2006)
for dependency parsing. We modify the PA algo-
rithm to deal with structured prediction, in which
our problem is to learn a discriminant function that
maps an input sentence x to a dependency tree y.
Figure 1 shows an example of dependency parsing
which depicts the relation of each word to another
word within a sentence. There are some algorithms
</bodyText>
<page confidence="0.992112">
1149
</page>
<figureCaption confidence="0.700821">
\x0cFigure 1: This is an example of dependency tree
</figureCaption>
<bodyText confidence="0.99954975">
to determine these relations of each word to another
words, for instance, the modified CKY algorithm
(Eisner, 1996) is used to define these relations for
a given sentence.
</bodyText>
<subsectionHeader confidence="0.997235">
2.1 Parsing Algorithm
</subsectionHeader>
<bodyText confidence="0.999740266666667">
Dependency-tree parsing as the search for the maxi-
mum spanning tree (MST) in a graph was proposed
by McDonald et al. (2005b). In this subsection,
we briefly describe the parsing algorithms based on
the first-order MST parsing. Due to the limitation
of participation time, we only applied the first-order
decoding parsing algorithm in CONLL-2007. How-
ever, our algorithm can be used for the second order
parsing.
Let the generic sentence be denoted by x ; the
ith word of x is denoted by wi. The generic de-
pendency tree is denoted by y. If y is a dependency
tree for sentence x, we write (i, j) y to indicate
that there is a directed edge from word xwi to word
xwj in the tree, that is, xwi is the parent of xwj .
</bodyText>
<equation confidence="0.791312">
T = {(xt, yt)}n
</equation>
<bodyText confidence="0.994641">
t=1 denotes the training data. We fol-
low the edge based factorization method of Eisner
(Eisner, 1996) and define the score of a dependency
tree as the sum of the score of all edges in the tree,
</bodyText>
<equation confidence="0.998720714285714">
s(x, y) =
X
(i,j)y
s(i, j) =
X
(i,j)y
w (i, j) (1)
</equation>
<bodyText confidence="0.966859333333333">
where (i, j) is a high-dimensional binary fea-
ture representation of the edge from xwi to xwj .
For example in Figure 1, we can present an example
</bodyText>
<equation confidence="0.9368448">
(i, j) as follows;
(i, j) =
(
1 if xwi =0 hit0 and xwj =0 ball0
0 otherwise
</equation>
<bodyText confidence="0.994032916666666">
The basic question must be answered for models
of this form: how to find the dependency tree y with
the highest score for sentence x? The two algorithms
we employed in our dependency parsing model are
the Eisner parsing (Eisner, 1996) and Chu-Lius al-
gorithm (Chu and Liu, 1965). The algorithms are
commonly used in other online-learning dependency
parsing, such as in (McDonald et al., 2005a).
In the next subsection we will address the problem
of how to estimate the weight wi associated with a
feature i in the training data using an online PA
learning algorithm.
</bodyText>
<subsectionHeader confidence="0.851859">
2.2 Online PA Learning
</subsectionHeader>
<bodyText confidence="0.999535222222222">
This section presents a modification of PA algo-
rithm for structured prediction, and its use in de-
pendency parsing. The Perceptron style for natural
language processing problems as initially proposed
by (Collins, 2002) can provide state of the art re-
sults on various domains including text chunking,
syntactic parsing, etc. The main drawback of the
Perceptron style algorithm is that it does not have a
mechanism for attaining the maximize margin of the
training data. It may be difficult to obtain high accu-
racy in dealing with hard learning data. The struc-
tured support vector machine (Tsochantaridis et al.,
2005) and the maximize margin model (Taskar et al.,
2004) can gain a maximize margin value for given
training data by solving an optimization problem (i.e
quadratic programming). It is obvious that using
such an optimization algorithm requires much com-
putational time. For dependency parsing domain,
McDonald et al (2005a) modified the MIRA learn-
ing algorithm (McDonald et al., 2005a) for struc-
tured domains in which the optimization problem
can be solved by using Hidreths algorithm (Censor
and Zenios, 1997), which is faster than the quadratic
programming technique. In contrast to the previous
method, this paper presents an online algorithm for
dependency parsing in which we can attain the max-
imize margin of the training data without using opti-
mization techniques. It is thus much faster and eas-
ier to implement. The details of PA algorithm for
dependency parsing are presented below.
Assume that we are given a set of sentences xi
and their dependency trees yi where i = 1, ..., n. Let
the feature mapping between a sentence x and a tree
y be: (x, y) = 1(x, y), 2(x, y), ..., d(x, y)
where each feature mapping j maps (x, y) to a real
value. We assume that each feature (x, y) is asso-
</bodyText>
<page confidence="0.952922">
1150
</page>
<bodyText confidence="0.98358325">
\x0cciated with a weight value. The goal of PA learning
for dependency parsing is to obtain a parameter w
that minimizes the hinge-loss function and the mar-
gin of learning data.
</bodyText>
<figure confidence="0.785443625">
Input:S = {(xi; yi), i = 1, 2, ..., n} in which
1
xi is the sentence and yi is a dependency tree
Aggressive parameter C
2
Output: the PA learning model
3
Initialize: w1 = (0, 0, ..., 0)
</figure>
<page confidence="0.845548">
4
</page>
<bodyText confidence="0.862218">
for t=1, 2... do
</bodyText>
<page confidence="0.779418">
5
</page>
<bodyText confidence="0.602185">
Receive an sentence xt
</bodyText>
<page confidence="0.817947">
6
</page>
<equation confidence="0.941138192307692">
Predict y
t = arg maxyY (wt (xt, yt))
7
Suffer loss: lt =
8
wt (xt, y
t ) wt (xt, yt) +
p
(yt, y
t )
Set:
9
PA: t = lt
||(xt,y
t )(xt,yt)||2
PA-I: t = min{C, lt
||(xt,y
t )(xt,yt)||2 }
PA-II: t = lt
||(xt,y
t )(xt,yt)||2+ 1
2C
Update:
wt+1 = wt + t((xt, yt) (xt, y
t ))
end
</equation>
<page confidence="0.980251">
10
</page>
<figureCaption confidence="0.444424">
Algorithm 1: The Passive-Aggressive algo-
</figureCaption>
<bodyText confidence="0.990691741935484">
rithm for dependency parsing.
Algorithm 1 shows the PA learning algorithm for
dependency parsing in which its three variants are
different only in the update formulas. In Algorithm
1, we employ two kinds of argmax algorithms: The
first is the decoding algorithm for projective lan-
guage data and the second one is for non-projective
language data. Algorithm 1 shows (line 8) p(y, yt)
is a real-valued loss for the tree yt relative to the
correct tree y. We define the loss of a dependency
tree as the number of words which have an incorrect
parent. Thus, the largest loss a dependency tree can
have is the length of the sentence. The similar loss
function is designed for the dependency tree with la-
beled. Algorithm 1 returns an averaged weight vec-
tor: an auxiliary weight vector v is maintained that
accumulates the values of w after each iteration, and
the returned weight vector is the average of all the
weight vectors throughout training. Averaging has
been shown to help reduce overfitting (McDonald et
al., 2005a; Collins, 2002). It is easy to see that the
main difference between the PA algorithms and the
Perceptron algorithm (PC) (Collins, 2002) as well as
the MIRA algorithm (McDonald et al., 2005a) is in
line 9. As we can see in the PC algorithm, we do
not need the value t and in the MIRA algorithm we
need an optimization algorithm to compute t. We
also have three updated formulations for obtaining
t in Line 9. In the scope of this paper, we only
focus on using the second update formulation (PA-I
method) for training dependency parsing data.
</bodyText>
<subsectionHeader confidence="0.989664">
2.3 Feature Set
</subsectionHeader>
<bodyText confidence="0.996262">
We denote p-word: word of parent node in depen-
dency tree. c-word: word of child node. p-pos: POS
of parent node. c-pos: POS of child node. p-pos+1:
POS to the right of parent in sentence. p-pos-1: POS
to the left of parent. c-pos+1: POS to the right of
child. c-pos-1: POS to the left of child. b-pos: POS
of a word in between parent and child nodes. The
</bodyText>
<figure confidence="0.946274">
p-word,p-pos
p-word
p-pos
c-word, c-pos
c-word
c-pos
</figure>
<tableCaption confidence="0.953673">
Table 1: Feature Set 1: Basic Unit-gram features
</tableCaption>
<bodyText confidence="0.814079142857143">
p-word, p-pos, c-word, c-pos
p-pos, c-word, c-pos
p-word, c-word, c-pos
p-word, p-pos, c-pos
p-word, p-pos, c-word
p-word, c-word
p-pos, c-pos
</bodyText>
<tableCaption confidence="0.861764">
Table 2: Feature Set 2: Basic bi-gram features
</tableCaption>
<bodyText confidence="0.6882422">
p-pos, b-pos, c-pos
p-pos, p-pos+1, c-pos-1, c-pos
p-pos-1, p-pos, c-pos-1, c-pos
p-pos, p-pos+1, c-pos, c-pos+1
p-pos-1, p-pos, c-pos, c-pos+1
</bodyText>
<tableCaption confidence="0.813897">
Table 3: Feature Set 3: In Between POS Features
and Surrounding Word POS Features
features used in our system are described below.
Tables 1 and 2 show our basic features. These
</tableCaption>
<page confidence="0.932537">
1151
</page>
<bodyText confidence="0.964107">
\x0cfeatures are added for entire words as well as
for the 5-gram prefix if the word is longer than
</bodyText>
<sectionHeader confidence="0.713594" genericHeader="method">
5 characters.
</sectionHeader>
<bodyText confidence="0.99911112">
In addition to these features shown in Table 1,
the morphological information for each pair of
words p-word and c-word are represented. In
addition, we also add the conjunction morpho-
logical information of p-word and c-word. We
do not use the LEMMA and CPOSTAG infor-
mation in our set features. The morphological
information are obtained from FEAT informa-
tion.
Table 3 shows our Feature set 3 which take the
form of a POS trigram: the POS of the par-
ent, of the child, and of a word in between,
for all words linearly between the parent and
the child. This feature was particularly helpful
for nouns identifying their parent (McDonald
et al., 2005a).
Table 3 also depicts these features taken the
form of a POS 4-gram: The POS of the par-
ent, child, word before/after parent and word
before/after child. The system also used back-
off features with various trigrams where one of
the local context POS tags was removed.
All features are also conjoined with the direc-
tion of attachment, as well as the distance be-
tween the two words being attached.
</bodyText>
<sectionHeader confidence="0.988571" genericHeader="evaluation">
3 Experimental Results and Discussion
</sectionHeader>
<bodyText confidence="0.998436161290322">
We test our parsing models on the CONLL-2007
(Hajic et al., 2004; Aduriz et al., 2003; Mart et
al., 2007; Chen et al., 2003; Bohmova et al., 2003;
Marcus et al., 1993; Johansson and Nugues, 2007;
Prokopidis et al., 2005; Csendes et al., 2005; Mon-
temagni et al., 2003; Oflazer et al., 2003) data set on
various languages including Arabic, Basque, Cata-
lan, Chinese, English, Italian, Hungarian, and Turk-
ish. Each word is attached by POS tags for each sen-
tence in both the training and the testing data. Table
4 shows the number of training and testing sentences
for these languages. The table shows that the sen-
tence length in Arabic data is largest and its size of
training data is smallest. These factors might be af-
fected to the accuracy of our proposed algorithm as
we will discuss later.
The training and testing were conducted on a pen-
tium IV at 4.3 GHz. The detailed information about
the data are shown in the CONLL-2007 shared task.
We applied non-projective and projective parsing
along with PA learning for the data in CONLL-2007.
Table 5 reports experimental results by using the
first order decoding method in which an MST pars-
ing algorithm (McDonald et al., 2005b) is applied
for non-projective parsing and the Eisners method
is used for projective language data. In fact, in our
method we applied non-projective parsing for the
Italian data, the Turkish data, and the Greek data.
This was because we did not have enough time to
train all training data using both projective and non-
projective parsing. This is the problem of discrimi-
native learning methods when performing on a large
set of training data. In addition, to save time in train-
ing we set the number of best trees k to 1 and the
parameter C is set to 0.05.
Table 5 shows the comparison of the proposed
method with the average, and three top systems on
the CONLL-2007. As a result, our method yields
results above the average score on the CONLL-2007
shared task (Nivre et al., 2007).
Table 5 also indicates that the Basque results ob-
tained a lower score than other data. We obtained
69.11 UA score and 58.16 LA score, respectively.
These are far from the results of the Top3 scores
(81.13 and 75.49). We checked the outputs of the
Basque data to understand the main reason for the
errors. We see that the errors in our methods are
usually mismatched with the gold data at the labels
ncmod and ncsubj. The main reason might be
that the application of projective parsing for this data
in both training and testing is not suitable. This was
because the number of sentences with at least 1 non
projective relation in the data is large (26.1).
The Arabic score is lower than the scores of other
data because of some difficulties in our method as
follows. Morphological and sentence length prob-
lems are the main factors which affect the accuracy
of parsing Arabic data. In addition, the training size
in the Arabic is also a problem for obtaining a good
result. Furthermore, since our tasks was focused on
improving the accuracy of English data, it might be
unsuitable for other languages. This is an imbalance
</bodyText>
<page confidence="0.978274">
1152
</page>
<table confidence="0.997667818181818">
\x0cLanguages Training size Tokens size tokens-per-sent % of NPR % of-sentence AL-1-NPR
Arabic 2,900 112,000 38.3 0.4 10.1
Basque 3,200 51,000 15.8 2.9 26.2
Catalan 15,000 431,000 28.8 0.1 2.9
Chinese 57,000 337,000 5.9 0.0 0.0
Czech 25,400 432,000 17.0 1.9 23.2
English 18,600 447,000 24.0 0.3 6.7
Greek 2,700 65,000 24.2 1.1 20.3
Hungarian 6,000 132,000 21.8 2.9 26.4
Italian 3,100 71,000 22.9 0.5 7.4
Turkish 5,600 65,000 11.6 0.5 33.3
</table>
<tableCaption confidence="0.99873">
Table 4: The data used in the multilingual track (Nivre et al., 2007). NPR means non-projective-relations.
</tableCaption>
<bodyText confidence="0.981813">
AL-1-NPR means at-least-least 1 non-projective relation.
problem in our method. Table 5 also shows the com-
parison of our system to the average score and the
Top3 scores. It depicts that our system is accurate
in English data, while it has low accuracy in Basque
and Arabic data.
We also evaluate our models in the domain adap-
tation tasks. This task is to adapt our model trained
on PennBank data to the test data in the Biomedical
domain. The pchemtb-closed shared task (Marcus
et al., 1993; Johansson and Nugues, 2007; Kulick
et al., 2004) is used to illustrate our models. We do
not use any additional unlabeled data in the Biomed-
ical domain. Only the training data in the PennBank
is used to train our model. Afterward, we selected
carefully a suitable parameter using the development
test set. We set the parameter C to 0.01 and se-
lect the non projective parsing for testing to obtain
the highest result in the development data after per-
forming several experiments. After that, the trained
model was used to test the data in Biomedical do-
main. The score (UA=82.04; LA=79.50) shows that
our method yields results above the average score
(UA=76.42; LA=73.03). In addition, it is officially
coming in 4th place out of 12 teams and within 1.5%
of the top systems.
The good result of performing our model in an-
other domain suggested that the PA learning seems
sensitive to noise. We hope that this problem is
solved in future work.
</bodyText>
<sectionHeader confidence="0.997547" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999812133333333">
This paper presents an online algorithm for depen-
dency parsing problem which have tested on various
language data in CONLL-2007 shared task. The per-
formance in English data is close to the Top3 score.
We also perform our algorithm on the domain adap-
tation task, in which we only focus on the training of
the source data and select a suitable parameter using
the development set. The result is very good as it
is close to the Top3 score of participating systems.
Future work will also be focused on extending our
method to a version of using semi-supervised learn-
ing that can efficiently be learnt by using labeled and
unlabeled data. We hope that the application of the
PA algorithm to other NLP problems such as seman-
tic parsing will be explored in future work.
</bodyText>
<sectionHeader confidence="0.966916" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.97955">
We would like to thank D. Yuret for his helps in
checking errors of my parsers outputs. We would
like to thank Vinh-Van Nguyen his helps during the
revision process and Mary Ann Mooradian for cor-
recting the paper.
We would like to thank to anonymous review-
ers for helpful discussions and comments on the
manuscript. Thank also to Sebastian Riedel for
checking the issues raised in the reviews.
The work on this paper was supported by a Mon-
bukagakusho 21st COE Program.
</bodyText>
<sectionHeader confidence="0.959751" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983288142857143">
A. Abeille, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201204.
</reference>
<page confidence="0.870013">
1153
</page>
<table confidence="0.997299928571429">
\x0cLanguages Unlabled Accuracy Labeled Accuracy NTeams
PA-I Average Top3 Top2 Top1 PA-I Average Top3 Top2 Top1
Arabic 73.46 78.84 84.21 85.81 86.09 68.34 74.75 83.0 75.08 76.52 20
Basque 69.11 75.15 81.13 81.93 81.13 58.16 68.06 75.49 75.73 76.92 20
Catalan 88.12 87.98 93.12 93.34 93.40 83.23 79.85 87.90 88.16 88.70 20
Chinese 84.05 81.98 87.91 88.88 88.94 79.77 76.59 83.51 83.84 84.69 21
Czech 80.91 77.56 84.19 85.16 86.28 72.54 70.12 77.98 78.60 80.19 20
English 88.01 82.67 89.87 90.13 90.63 86.73 80.95 88.41 89.01 89.61 23
Greek 77.56 77.78 81.37 82.04 84.08 70.42 70.22 74.42 74.65 76.31 20
Hungarian 78.13 76.34 82.49 83.51 83.55 68.12 71.49 78.09 79.53 80.27 21
Italian 80.40 82.45 87.68 87.77 87.91 75.06 78.06 78.09 79.53 80.27 20
Turkish 80.19 73.19 85.77 85.77 86.22 67.63 73.19 79.24 79.79 79.81 20
Multilingual-average 79.99 71.13 85.62 85.71 86.55 72.52 65.77 79.90 80.28 80.32 23
pchemtb-closed 82.04 76.42 83.08 83.38 83.42 79.50 73.03 80.22 80.40 81.06 8
</table>
<tableCaption confidence="0.986353">
Table 5: Dependency accuracy in the CONLL-2007 shared task.
</tableCaption>
<reference confidence="0.926588414285714">
A. Bohmova, J. Hajic, E. Hajicova, and B. Hladka. 2003.
The PDT: a 3-level annotation scenario. In Abeille
(Abeille, 2003), chapter 7, pages 103127.
Y. Censor and S.A. Zenios. 1997. Parallel optimization:
theory, algorithms, and applications. In Oxford Uni-
versity Press.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille
(Abeille, 2003), chapter 13, pages 231248.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. In Science Sinica.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of EMNLP.
S. Corston-Oliver, A. Aue, K. Duh, , and E. Ringger.
2006. Multilingual dependency parsing using bayes
point machines. In Proceedings of HLT/NAACL.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research, 3:951991.
K. Crammer, O. Dekel, J. Keshet, S.Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:581585.
D. Csendes, J. Csirik, T. Gyimothy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
COLING 1996, pages 340345.
J. Hajic, O. Smrz, P. Zemanek, J. Snaidauf, and E. Beska.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110117.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313330.
M. A. Mart, M. Taule, L. Marquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/mbertran/cess-ece/.
R. McDonald, K. Cramer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
pages 523530.
R. McDonald, K. Crammer, and F. Pereira. 2006. Multi-
lingual dependency parsing with a two-stage discrim-
inative parser. In Conference on Natural Language
Learning.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
</reference>
<page confidence="0.601335">
1154
</page>
<reference confidence="0.999301117647059">
\x0cR. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille (Abeille, 2003), chap-
ter 11, pages 189210.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL), pages 221225.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
K. Oflazer, B. Say, D. Zeynep Hakkani-Tur, and G. Tur.
2003. Building a Turkish treebank. In Abeille
(Abeille, 2003), chapter 15, pages 261277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149160.
B. Taskar, D. Klein, M. Collins, D. Koller, and C.D. Man-
ning. 2004. Max-margin parsing. In proceedings of
EMNLP.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2005. Support vector machine learning for interde-
pendent and structured output spaces. In proceedings
ICML 2004.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT), pages 195206.
</reference>
<page confidence="0.698605">
1155
</page>
<figure confidence="0.337007">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.423909">
<note confidence="0.818177">b&amp;apos;Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 11491155, Prague, June 2007. c 2007 Association for Computational Linguistics</note>
<title confidence="0.9904235">A Multilingual Dependency Analysis System using Online Passive-Aggressive Learning</title>
<author confidence="0.969763">Le-Minh Nguyen</author>
<author confidence="0.969763">Akira Shimazu</author>
<author confidence="0.969763">Phuong-Thai Nguyen</author>
<affiliation confidence="0.877539">Japan Advanced Institute of Science and Technology (JAIST)</affiliation>
<address confidence="0.798795">Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan</address>
<email confidence="0.950766">nguyenml@jaist.ac.jp</email>
<email confidence="0.950766">shimazu@jaist.ac.jp</email>
<email confidence="0.950766">thai@jaist.ac.jp</email>
<author confidence="0.96046">Xuan-Hieu Phan</author>
<affiliation confidence="0.999939">Tohoku University</affiliation>
<address confidence="0.976273">Aobayama 6-3-09, Sendai, 980-8579, Japan</address>
<email confidence="0.963101">hieuxuan@ecei.tohoku.ac.jp</email>
<abstract confidence="0.998140090909091">This paper presents an online algorithm for dependency parsing problems. We propose an adaptation of the passive and aggressive online learning algorithm to the dependency parsing domain. We evaluate the proposed algorithms on the 2007 CONLL Shared Task, and report errors analysis. Experimental results show that the system score is better than the average score among the participating systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Treebanks: Building and Using Parsed Corpora.</title>
<date>2003</date>
<editor>A. Abeille, editor.</editor>
<publisher>Kluwer.</publisher>
<marker>2003</marker>
<rawString>A. Abeille, editor. 2003. Treebanks: Building and Using Parsed Corpora. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Aduriz</author>
<author>M J Aranzabe</author>
<author>J M Arriola</author>
<author>A Atutxa</author>
<author>A Diaz de Ilarraza</author>
<author>A Garmendia</author>
<author>M Oronoz</author>
</authors>
<title>Construction of a Basque dependency treebank.</title>
<date>2003</date>
<booktitle>In Proc. of the 2nd Workshop on Treebanks and Linguistic Theories (TLT),</booktitle>
<pages>201204</pages>
<marker>Aduriz, Aranzabe, Arriola, Atutxa, de Ilarraza, Garmendia, Oronoz, 2003</marker>
<rawString>I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz. 2003. Construction of a Basque dependency treebank. In Proc. of the 2nd Workshop on Treebanks and Linguistic Theories (TLT), pages 201204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bohmova</author>
<author>J Hajic</author>
<author>E Hajicova</author>
<author>B Hladka</author>
</authors>
<date>2003</date>
<contexts>
<context position="11899" citStr="Bohmova et al., 2003" startWordPosition="2028" endWordPosition="2031">ying their parent (McDonald et al., 2005a). Table 3 also depicts these features taken the form of a POS 4-gram: The POS of the parent, child, word before/after parent and word before/after child. The system also used backoff features with various trigrams where one of the local context POS tags was removed. All features are also conjoined with the direction of attachment, as well as the distance between the two words being attached. 3 Experimental Results and Discussion We test our parsing models on the CONLL-2007 (Hajic et al., 2004; Aduriz et al., 2003; Mart et al., 2007; Chen et al., 2003; Bohmova et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oflazer et al., 2003) data set on various languages including Arabic, Basque, Catalan, Chinese, English, Italian, Hungarian, and Turkish. Each word is attached by POS tags for each sentence in both the training and the testing data. Table 4 shows the number of training and testing sentences for these languages. The table shows that the sentence length in Arabic data is largest and its size of training data is smallest. These factors might be affected to the accuracy of our</context>
</contexts>
<marker>Bohmova, Hajic, Hajicova, Hladka, 2003</marker>
<rawString>A. Bohmova, J. Hajic, E. Hajicova, and B. Hladka. 2003.</rawString>
</citation>
<citation valid="true">
<title>The PDT: a 3-level annotation scenario. In Abeille (Abeille,</title>
<date>2003</date>
<pages>103127</pages>
<note>chapter 7,</note>
<marker>2003</marker>
<rawString>The PDT: a 3-level annotation scenario. In Abeille (Abeille, 2003), chapter 7, pages 103127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Censor</author>
<author>S A Zenios</author>
</authors>
<title>Parallel optimization: theory, algorithms, and applications. In</title>
<date>1997</date>
<publisher>University Press.</publisher>
<location>Oxford</location>
<contexts>
<context position="6597" citStr="Censor and Zenios, 1997" startWordPosition="1070" endWordPosition="1073">in high accuracy in dealing with hard learning data. The structured support vector machine (Tsochantaridis et al., 2005) and the maximize margin model (Taskar et al., 2004) can gain a maximize margin value for given training data by solving an optimization problem (i.e quadratic programming). It is obvious that using such an optimization algorithm requires much computational time. For dependency parsing domain, McDonald et al (2005a) modified the MIRA learning algorithm (McDonald et al., 2005a) for structured domains in which the optimization problem can be solved by using Hidreths algorithm (Censor and Zenios, 1997), which is faster than the quadratic programming technique. In contrast to the previous method, this paper presents an online algorithm for dependency parsing in which we can attain the maximize margin of the training data without using optimization techniques. It is thus much faster and easier to implement. The details of PA algorithm for dependency parsing are presented below. Assume that we are given a set of sentences xi and their dependency trees yi where i = 1, ..., n. Let the feature mapping between a sentence x and a tree y be: (x, y) = 1(x, y), 2(x, y), ..., d(x, y) where each feature</context>
</contexts>
<marker>Censor, Zenios, 1997</marker>
<rawString>Y. Censor and S.A. Zenios. 1997. Parallel optimization: theory, algorithms, and applications. In Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Chen</author>
<author>C Luo</author>
<author>M Chang</author>
<author>F Chen</author>
<author>C Chen</author>
<author>C Huang</author>
<author>Z Gao</author>
</authors>
<title>Sinica treebank: Design criteria, representational issues and implementation.</title>
<date>2003</date>
<booktitle>In Abeille (Abeille,</booktitle>
<pages>231248</pages>
<contexts>
<context position="11877" citStr="Chen et al., 2003" startWordPosition="2024" endWordPosition="2027">l for nouns identifying their parent (McDonald et al., 2005a). Table 3 also depicts these features taken the form of a POS 4-gram: The POS of the parent, child, word before/after parent and word before/after child. The system also used backoff features with various trigrams where one of the local context POS tags was removed. All features are also conjoined with the direction of attachment, as well as the distance between the two words being attached. 3 Experimental Results and Discussion We test our parsing models on the CONLL-2007 (Hajic et al., 2004; Aduriz et al., 2003; Mart et al., 2007; Chen et al., 2003; Bohmova et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oflazer et al., 2003) data set on various languages including Arabic, Basque, Catalan, Chinese, English, Italian, Hungarian, and Turkish. Each word is attached by POS tags for each sentence in both the training and the testing data. Table 4 shows the number of training and testing sentences for these languages. The table shows that the sentence length in Arabic data is largest and its size of training data is smallest. These factors might be affected </context>
</contexts>
<marker>Chen, Luo, Chang, Chen, Chen, Huang, Gao, 2003</marker>
<rawString>K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang, and Z. Gao. 2003. Sinica treebank: Design criteria, representational issues and implementation. In Abeille (Abeille, 2003), chapter 13, pages 231248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y J Chu</author>
<author>T H Liu</author>
</authors>
<title>On the shortest arborescence of a directed graph.</title>
<date>1965</date>
<booktitle>In Science Sinica.</booktitle>
<contexts>
<context position="5166" citStr="Chu and Liu, 1965" startWordPosition="839" endWordPosition="842">a dependency tree as the sum of the score of all edges in the tree, s(x, y) = X (i,j)y s(i, j) = X (i,j)y w (i, j) (1) where (i, j) is a high-dimensional binary feature representation of the edge from xwi to xwj . For example in Figure 1, we can present an example (i, j) as follows; (i, j) = ( 1 if xwi =0 hit0 and xwj =0 ball0 0 otherwise The basic question must be answered for models of this form: how to find the dependency tree y with the highest score for sentence x? The two algorithms we employed in our dependency parsing model are the Eisner parsing (Eisner, 1996) and Chu-Lius algorithm (Chu and Liu, 1965). The algorithms are commonly used in other online-learning dependency parsing, such as in (McDonald et al., 2005a). In the next subsection we will address the problem of how to estimate the weight wi associated with a feature i in the training data using an online PA learning algorithm. 2.2 Online PA Learning This section presents a modification of PA algorithm for structured prediction, and its use in dependency parsing. The Perceptron style for natural language processing problems as initially proposed by (Collins, 2002) can provide state of the art results on various domains including text</context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Y.J. Chu and T.H. Liu. 1965. On the shortest arborescence of a directed graph. In Science Sinica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1282" citStr="Collins, 2002" startWordPosition="173" endWordPosition="174">ine learning algorithm to the dependency parsing domain. We evaluate the proposed algorithms on the 2007 CONLL Shared Task, and report errors analysis. Experimental results show that the system score is better than the average score among the participating systems. 1 Introduction Research on dependency parsing is mainly based on machine learning methods, which can be called history-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) and discriminative learning methods (McDonald et al., 2005a; Corston-Oliver et al., 2006). The learning methods using in discriminative parsing are Perceptron (Collins, 2002) and online large-margin learning (MIRA) (Crammer and Singer, 2003). The difference of MIRA-based parsing in comparison with history-based methods is that the MIRA-based parser were trained to maximize the accuracy of the overall tree. The MIRA based parsing is close to maximum-margin parsing as in Taskar et al. (2004) and Tsochantaridis et al. (2005) for parsing. However, unlike maximum-margin parsing, it is not limited to parsing sentences of 15 words or less due to computation time. The performance of MIRA based parsing achieves the state-ofthe-art performance in English data (McDonald et a</context>
<context position="5695" citStr="Collins, 2002" startWordPosition="926" endWordPosition="927">odel are the Eisner parsing (Eisner, 1996) and Chu-Lius algorithm (Chu and Liu, 1965). The algorithms are commonly used in other online-learning dependency parsing, such as in (McDonald et al., 2005a). In the next subsection we will address the problem of how to estimate the weight wi associated with a feature i in the training data using an online PA learning algorithm. 2.2 Online PA Learning This section presents a modification of PA algorithm for structured prediction, and its use in dependency parsing. The Perceptron style for natural language processing problems as initially proposed by (Collins, 2002) can provide state of the art results on various domains including text chunking, syntactic parsing, etc. The main drawback of the Perceptron style algorithm is that it does not have a mechanism for attaining the maximize margin of the training data. It may be difficult to obtain high accuracy in dealing with hard learning data. The structured support vector machine (Tsochantaridis et al., 2005) and the maximize margin model (Taskar et al., 2004) can gain a maximize margin value for given training data by solving an optimization problem (i.e quadratic programming). It is obvious that using suc</context>
<context position="9045" citStr="Collins, 2002" startWordPosition="1532" endWordPosition="1533"> relative to the correct tree y. We define the loss of a dependency tree as the number of words which have an incorrect parent. Thus, the largest loss a dependency tree can have is the length of the sentence. The similar loss function is designed for the dependency tree with labeled. Algorithm 1 returns an averaged weight vector: an auxiliary weight vector v is maintained that accumulates the values of w after each iteration, and the returned weight vector is the average of all the weight vectors throughout training. Averaging has been shown to help reduce overfitting (McDonald et al., 2005a; Collins, 2002). It is easy to see that the main difference between the PA algorithms and the Perceptron algorithm (PC) (Collins, 2002) as well as the MIRA algorithm (McDonald et al., 2005a) is in line 9. As we can see in the PC algorithm, we do not need the value t and in the MIRA algorithm we need an optimization algorithm to compute t. We also have three updated formulations for obtaining t in Line 9. In the scope of this paper, we only focus on using the second update formulation (PA-I method) for training dependency parsing data. 2.3 Feature Set We denote p-word: word of parent node in dependency tree. </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Corston-Oliver</author>
<author>A Aue</author>
<author>K Duh</author>
</authors>
<title>Multilingual dependency parsing using bayes point machines.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="1197" citStr="Corston-Oliver et al., 2006" startWordPosition="159" endWordPosition="162">gorithm for dependency parsing problems. We propose an adaptation of the passive and aggressive online learning algorithm to the dependency parsing domain. We evaluate the proposed algorithms on the 2007 CONLL Shared Task, and report errors analysis. Experimental results show that the system score is better than the average score among the participating systems. 1 Introduction Research on dependency parsing is mainly based on machine learning methods, which can be called history-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) and discriminative learning methods (McDonald et al., 2005a; Corston-Oliver et al., 2006). The learning methods using in discriminative parsing are Perceptron (Collins, 2002) and online large-margin learning (MIRA) (Crammer and Singer, 2003). The difference of MIRA-based parsing in comparison with history-based methods is that the MIRA-based parser were trained to maximize the accuracy of the overall tree. The MIRA based parsing is close to maximum-margin parsing as in Taskar et al. (2004) and Tsochantaridis et al. (2005) for parsing. However, unlike maximum-margin parsing, it is not limited to parsing sentences of 15 words or less due to computation time. The performance of MIRA </context>
</contexts>
<marker>Corston-Oliver, Aue, Duh, 2006</marker>
<rawString>S. Corston-Oliver, A. Aue, K. Duh, , and E. Ringger. 2006. Multilingual dependency parsing using bayes point machines. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--951991</pages>
<contexts>
<context position="1349" citStr="Crammer and Singer, 2003" startWordPosition="181" endWordPosition="184"> We evaluate the proposed algorithms on the 2007 CONLL Shared Task, and report errors analysis. Experimental results show that the system score is better than the average score among the participating systems. 1 Introduction Research on dependency parsing is mainly based on machine learning methods, which can be called history-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) and discriminative learning methods (McDonald et al., 2005a; Corston-Oliver et al., 2006). The learning methods using in discriminative parsing are Perceptron (Collins, 2002) and online large-margin learning (MIRA) (Crammer and Singer, 2003). The difference of MIRA-based parsing in comparison with history-based methods is that the MIRA-based parser were trained to maximize the accuracy of the overall tree. The MIRA based parsing is close to maximum-margin parsing as in Taskar et al. (2004) and Tsochantaridis et al. (2005) for parsing. However, unlike maximum-margin parsing, it is not limited to parsing sentences of 15 words or less due to computation time. The performance of MIRA based parsing achieves the state-ofthe-art performance in English data (McDonald et al., 2005a; McDonald et al., 2006). In this paper, we propose a new </context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--581585</pages>
<contexts>
<context position="3095" citStr="Crammer et al., 2006" startWordPosition="454" endWordPosition="457">ations for anyone interested in implementing discriminative training methods for any application. Second, we evaluate the proposed algorithm on the multilingual data task as well as the domain adaptation task (Nivre et al., 2007). The remaining parts of the paper are organized as follows: Section 2 proposes our dependency parsing with Passive-Aggressive learning. Section 3 discusses some experimental results and Section 4 gives conclusions and plans for future work. 2 Dependency Parsing with Passive-Aggressive Learning This section presents the modification of PassiveAggressive Learning (PA) (Crammer et al., 2006) for dependency parsing. We modify the PA algorithm to deal with structured prediction, in which our problem is to learn a discriminant function that maps an input sentence x to a dependency tree y. Figure 1 shows an example of dependency parsing which depicts the relation of each word to another word within a sentence. There are some algorithms 1149 \x0cFigure 1: This is an example of dependency tree to determine these relations of each word to another words, for instance, the modified CKY algorithm (Eisner, 1996) is used to define these relations for a given sentence. 2.1 Parsing Algorithm D</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S.Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:581585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Csendes</author>
<author>J Csirik</author>
<author>T Gyimothy</author>
<author>A Kocsor</author>
</authors>
<title>The Szeged Treebank.</title>
<date>2005</date>
<publisher>Springer.</publisher>
<contexts>
<context position="11995" citStr="Csendes et al., 2005" startWordPosition="2044" endWordPosition="2047">f a POS 4-gram: The POS of the parent, child, word before/after parent and word before/after child. The system also used backoff features with various trigrams where one of the local context POS tags was removed. All features are also conjoined with the direction of attachment, as well as the distance between the two words being attached. 3 Experimental Results and Discussion We test our parsing models on the CONLL-2007 (Hajic et al., 2004; Aduriz et al., 2003; Mart et al., 2007; Chen et al., 2003; Bohmova et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oflazer et al., 2003) data set on various languages including Arabic, Basque, Catalan, Chinese, English, Italian, Hungarian, and Turkish. Each word is attached by POS tags for each sentence in both the training and the testing data. Table 4 shows the number of training and testing sentences for these languages. The table shows that the sentence length in Arabic data is largest and its size of training data is smallest. These factors might be affected to the accuracy of our proposed algorithm as we will discuss later. The training and testing were conducted on a penti</context>
</contexts>
<marker>Csendes, Csirik, Gyimothy, Kocsor, 2005</marker>
<rawString>D. Csendes, J. Csirik, T. Gyimothy, and A. Kocsor. 2005. The Szeged Treebank. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>340345</pages>
<contexts>
<context position="3615" citStr="Eisner, 1996" startWordPosition="544" endWordPosition="545">s section presents the modification of PassiveAggressive Learning (PA) (Crammer et al., 2006) for dependency parsing. We modify the PA algorithm to deal with structured prediction, in which our problem is to learn a discriminant function that maps an input sentence x to a dependency tree y. Figure 1 shows an example of dependency parsing which depicts the relation of each word to another word within a sentence. There are some algorithms 1149 \x0cFigure 1: This is an example of dependency tree to determine these relations of each word to another words, for instance, the modified CKY algorithm (Eisner, 1996) is used to define these relations for a given sentence. 2.1 Parsing Algorithm Dependency-tree parsing as the search for the maximum spanning tree (MST) in a graph was proposed by McDonald et al. (2005b). In this subsection, we briefly describe the parsing algorithms based on the first-order MST parsing. Due to the limitation of participation time, we only applied the first-order decoding parsing algorithm in CONLL-2007. However, our algorithm can be used for the second order parsing. Let the generic sentence be denoted by x ; the ith word of x is denoted by wi. The generic dependency tree is </context>
<context position="5123" citStr="Eisner, 1996" startWordPosition="833" endWordPosition="834">Eisner, 1996) and define the score of a dependency tree as the sum of the score of all edges in the tree, s(x, y) = X (i,j)y s(i, j) = X (i,j)y w (i, j) (1) where (i, j) is a high-dimensional binary feature representation of the edge from xwi to xwj . For example in Figure 1, we can present an example (i, j) as follows; (i, j) = ( 1 if xwi =0 hit0 and xwj =0 ball0 0 otherwise The basic question must be answered for models of this form: how to find the dependency tree y with the highest score for sentence x? The two algorithms we employed in our dependency parsing model are the Eisner parsing (Eisner, 1996) and Chu-Lius algorithm (Chu and Liu, 1965). The algorithms are commonly used in other online-learning dependency parsing, such as in (McDonald et al., 2005a). In the next subsection we will address the problem of how to estimate the weight wi associated with a feature i in the training data using an online PA learning algorithm. 2.2 Online PA Learning This section presents a modification of PA algorithm for structured prediction, and its use in dependency parsing. The Perceptron style for natural language processing problems as initially proposed by (Collins, 2002) can provide state of the ar</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of COLING 1996, pages 340345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajic</author>
<author>O Smrz</author>
<author>P Zemanek</author>
<author>J Snaidauf</author>
<author>E Beska</author>
</authors>
<title>Prague Arabic dependency treebank: Development in data and tools.</title>
<date>2004</date>
<booktitle>In Proc. of the NEMLAR Intern. Conf. on Arabic Language Resources and Tools,</booktitle>
<pages>110117</pages>
<contexts>
<context position="11818" citStr="Hajic et al., 2004" startWordPosition="2012" endWordPosition="2015">e parent and the child. This feature was particularly helpful for nouns identifying their parent (McDonald et al., 2005a). Table 3 also depicts these features taken the form of a POS 4-gram: The POS of the parent, child, word before/after parent and word before/after child. The system also used backoff features with various trigrams where one of the local context POS tags was removed. All features are also conjoined with the direction of attachment, as well as the distance between the two words being attached. 3 Experimental Results and Discussion We test our parsing models on the CONLL-2007 (Hajic et al., 2004; Aduriz et al., 2003; Mart et al., 2007; Chen et al., 2003; Bohmova et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oflazer et al., 2003) data set on various languages including Arabic, Basque, Catalan, Chinese, English, Italian, Hungarian, and Turkish. Each word is attached by POS tags for each sentence in both the training and the testing data. Table 4 shows the number of training and testing sentences for these languages. The table shows that the sentence length in Arabic data is largest and its size of </context>
</contexts>
<marker>Hajic, Smrz, Zemanek, Snaidauf, Beska, 2004</marker>
<rawString>J. Hajic, O. Smrz, P. Zemanek, J. Snaidauf, and E. Beska. 2004. Prague Arabic dependency treebank: Development in data and tools. In Proc. of the NEMLAR Intern. Conf. on Arabic Language Resources and Tools, pages 110117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proc. of the 16th Nordic Conference on Computational Linguistics (NODALIDA).</booktitle>
<contexts>
<context position="11948" citStr="Johansson and Nugues, 2007" startWordPosition="2036" endWordPosition="2039"> Table 3 also depicts these features taken the form of a POS 4-gram: The POS of the parent, child, word before/after parent and word before/after child. The system also used backoff features with various trigrams where one of the local context POS tags was removed. All features are also conjoined with the direction of attachment, as well as the distance between the two words being attached. 3 Experimental Results and Discussion We test our parsing models on the CONLL-2007 (Hajic et al., 2004; Aduriz et al., 2003; Mart et al., 2007; Chen et al., 2003; Bohmova et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oflazer et al., 2003) data set on various languages including Arabic, Basque, Catalan, Chinese, English, Italian, Hungarian, and Turkish. Each word is attached by POS tags for each sentence in both the training and the testing data. Table 4 shows the number of training and testing sentences for these languages. The table shows that the sentence length in Arabic data is largest and its size of training data is smallest. These factors might be affected to the accuracy of our proposed algorithm as we will discuss later. The</context>
<context position="15901" citStr="Johansson and Nugues, 2007" startWordPosition="2721" endWordPosition="2724"> Table 4: The data used in the multilingual track (Nivre et al., 2007). NPR means non-projective-relations. AL-1-NPR means at-least-least 1 non-projective relation. problem in our method. Table 5 also shows the comparison of our system to the average score and the Top3 scores. It depicts that our system is accurate in English data, while it has low accuracy in Basque and Arabic data. We also evaluate our models in the domain adaptation tasks. This task is to adapt our model trained on PennBank data to the test data in the Biomedical domain. The pchemtb-closed shared task (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004) is used to illustrate our models. We do not use any additional unlabeled data in the Biomedical domain. Only the training data in the PennBank is used to train our model. Afterward, we selected carefully a suitable parameter using the development test set. We set the parameter C to 0.01 and select the non projective parsing for testing to obtain the highest result in the development data after performing several experiments. After that, the trained model was used to test the data in Biomedical domain. The score (UA=82.04; LA=79.50) shows that our method yields results ab</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>R. Johansson and P. Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proc. of the 16th Nordic Conference on Computational Linguistics (NODALIDA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kulick</author>
<author>A Bies</author>
<author>M Liberman</author>
<author>M Mandel</author>
<author>R McDonald</author>
<author>M Palmer</author>
<author>A Schein</author>
<author>L Ungar</author>
</authors>
<title>Integrated annotation for biomedical information extraction.</title>
<date>2004</date>
<booktitle>In Proc. of the Human Language Technology Conference and the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL).</booktitle>
<contexts>
<context position="15923" citStr="Kulick et al., 2004" startWordPosition="2725" endWordPosition="2728">he multilingual track (Nivre et al., 2007). NPR means non-projective-relations. AL-1-NPR means at-least-least 1 non-projective relation. problem in our method. Table 5 also shows the comparison of our system to the average score and the Top3 scores. It depicts that our system is accurate in English data, while it has low accuracy in Basque and Arabic data. We also evaluate our models in the domain adaptation tasks. This task is to adapt our model trained on PennBank data to the test data in the Biomedical domain. The pchemtb-closed shared task (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004) is used to illustrate our models. We do not use any additional unlabeled data in the Biomedical domain. Only the training data in the PennBank is used to train our model. Afterward, we selected carefully a suitable parameter using the development test set. We set the parameter C to 0.01 and select the non projective parsing for testing to obtain the highest result in the development data after performing several experiments. After that, the trained model was used to test the data in Biomedical domain. The score (UA=82.04; LA=79.50) shows that our method yields results above the average score </context>
</contexts>
<marker>Kulick, Bies, Liberman, Mandel, McDonald, Palmer, Schein, Ungar, 2004</marker>
<rawString>S. Kulick, A. Bies, M. Liberman, M. Mandel, R. McDonald, M. Palmer, A. Schein, and L. Ungar. 2004. Integrated annotation for biomedical information extraction. In Proc. of the Human Language Technology Conference and the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="11920" citStr="Marcus et al., 1993" startWordPosition="2032" endWordPosition="2035">onald et al., 2005a). Table 3 also depicts these features taken the form of a POS 4-gram: The POS of the parent, child, word before/after parent and word before/after child. The system also used backoff features with various trigrams where one of the local context POS tags was removed. All features are also conjoined with the direction of attachment, as well as the distance between the two words being attached. 3 Experimental Results and Discussion We test our parsing models on the CONLL-2007 (Hajic et al., 2004; Aduriz et al., 2003; Mart et al., 2007; Chen et al., 2003; Bohmova et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oflazer et al., 2003) data set on various languages including Arabic, Basque, Catalan, Chinese, English, Italian, Hungarian, and Turkish. Each word is attached by POS tags for each sentence in both the training and the testing data. Table 4 shows the number of training and testing sentences for these languages. The table shows that the sentence length in Arabic data is largest and its size of training data is smallest. These factors might be affected to the accuracy of our proposed algorithm a</context>
<context position="15873" citStr="Marcus et al., 1993" startWordPosition="2717" endWordPosition="2720"> 65,000 11.6 0.5 33.3 Table 4: The data used in the multilingual track (Nivre et al., 2007). NPR means non-projective-relations. AL-1-NPR means at-least-least 1 non-projective relation. problem in our method. Table 5 also shows the comparison of our system to the average score and the Top3 scores. It depicts that our system is accurate in English data, while it has low accuracy in Basque and Arabic data. We also evaluate our models in the domain adaptation tasks. This task is to adapt our model trained on PennBank data to the test data in the Biomedical domain. The pchemtb-closed shared task (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004) is used to illustrate our models. We do not use any additional unlabeled data in the Biomedical domain. Only the training data in the PennBank is used to train our model. Afterward, we selected carefully a suitable parameter using the development test set. We set the parameter C to 0.01 and select the non projective parsing for testing to obtain the highest result in the development data after performing several experiments. After that, the trained model was used to test the data in Biomedical domain. The score (UA=82.04; LA=79.50) shows that </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Mart</author>
<author>M Taule</author>
<author>L Marquez</author>
<author>M Bertran</author>
</authors>
<title>CESS-ECE: A multilingual and multilevel annotated corpus. Available for download from: http://www.lsi.upc.edu/mbertran/cess-ece/.</title>
<date>2007</date>
<contexts>
<context position="11858" citStr="Mart et al., 2007" startWordPosition="2020" endWordPosition="2023">particularly helpful for nouns identifying their parent (McDonald et al., 2005a). Table 3 also depicts these features taken the form of a POS 4-gram: The POS of the parent, child, word before/after parent and word before/after child. The system also used backoff features with various trigrams where one of the local context POS tags was removed. All features are also conjoined with the direction of attachment, as well as the distance between the two words being attached. 3 Experimental Results and Discussion We test our parsing models on the CONLL-2007 (Hajic et al., 2004; Aduriz et al., 2003; Mart et al., 2007; Chen et al., 2003; Bohmova et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oflazer et al., 2003) data set on various languages including Arabic, Basque, Catalan, Chinese, English, Italian, Hungarian, and Turkish. Each word is attached by POS tags for each sentence in both the training and the testing data. Table 4 shows the number of training and testing sentences for these languages. The table shows that the sentence length in Arabic data is largest and its size of training data is smallest. These factors</context>
</contexts>
<marker>Mart, Taule, Marquez, Bertran, 2007</marker>
<rawString>M. A. Mart, M. Taule, L. Marquez, and M. Bertran. 2007. CESS-ECE: A multilingual and multilevel annotated corpus. Available for download from: http://www.lsi.upc.edu/mbertran/cess-ece/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Cramer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1166" citStr="McDonald et al., 2005" startWordPosition="155" endWordPosition="158">er presents an online algorithm for dependency parsing problems. We propose an adaptation of the passive and aggressive online learning algorithm to the dependency parsing domain. We evaluate the proposed algorithms on the 2007 CONLL Shared Task, and report errors analysis. Experimental results show that the system score is better than the average score among the participating systems. 1 Introduction Research on dependency parsing is mainly based on machine learning methods, which can be called history-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) and discriminative learning methods (McDonald et al., 2005a; Corston-Oliver et al., 2006). The learning methods using in discriminative parsing are Perceptron (Collins, 2002) and online large-margin learning (MIRA) (Crammer and Singer, 2003). The difference of MIRA-based parsing in comparison with history-based methods is that the MIRA-based parser were trained to maximize the accuracy of the overall tree. The MIRA based parsing is close to maximum-margin parsing as in Taskar et al. (2004) and Tsochantaridis et al. (2005) for parsing. However, unlike maximum-margin parsing, it is not limited to parsing sentences of 15 words or less due to computation</context>
<context position="3816" citStr="McDonald et al. (2005" startWordPosition="577" endWordPosition="580"> problem is to learn a discriminant function that maps an input sentence x to a dependency tree y. Figure 1 shows an example of dependency parsing which depicts the relation of each word to another word within a sentence. There are some algorithms 1149 \x0cFigure 1: This is an example of dependency tree to determine these relations of each word to another words, for instance, the modified CKY algorithm (Eisner, 1996) is used to define these relations for a given sentence. 2.1 Parsing Algorithm Dependency-tree parsing as the search for the maximum spanning tree (MST) in a graph was proposed by McDonald et al. (2005b). In this subsection, we briefly describe the parsing algorithms based on the first-order MST parsing. Due to the limitation of participation time, we only applied the first-order decoding parsing algorithm in CONLL-2007. However, our algorithm can be used for the second order parsing. Let the generic sentence be denoted by x ; the ith word of x is denoted by wi. The generic dependency tree is denoted by y. If y is a dependency tree for sentence x, we write (i, j) y to indicate that there is a directed edge from word xwi to word xwj in the tree, that is, xwi is the parent of xwj . T = {(xt, </context>
<context position="5279" citStr="McDonald et al., 2005" startWordPosition="856" endWordPosition="859">, j) (1) where (i, j) is a high-dimensional binary feature representation of the edge from xwi to xwj . For example in Figure 1, we can present an example (i, j) as follows; (i, j) = ( 1 if xwi =0 hit0 and xwj =0 ball0 0 otherwise The basic question must be answered for models of this form: how to find the dependency tree y with the highest score for sentence x? The two algorithms we employed in our dependency parsing model are the Eisner parsing (Eisner, 1996) and Chu-Lius algorithm (Chu and Liu, 1965). The algorithms are commonly used in other online-learning dependency parsing, such as in (McDonald et al., 2005a). In the next subsection we will address the problem of how to estimate the weight wi associated with a feature i in the training data using an online PA learning algorithm. 2.2 Online PA Learning This section presents a modification of PA algorithm for structured prediction, and its use in dependency parsing. The Perceptron style for natural language processing problems as initially proposed by (Collins, 2002) can provide state of the art results on various domains including text chunking, syntactic parsing, etc. The main drawback of the Perceptron style algorithm is that it does not have a</context>
<context position="9028" citStr="McDonald et al., 2005" startWordPosition="1528" endWordPosition="1531">ued loss for the tree yt relative to the correct tree y. We define the loss of a dependency tree as the number of words which have an incorrect parent. Thus, the largest loss a dependency tree can have is the length of the sentence. The similar loss function is designed for the dependency tree with labeled. Algorithm 1 returns an averaged weight vector: an auxiliary weight vector v is maintained that accumulates the values of w after each iteration, and the returned weight vector is the average of all the weight vectors throughout training. Averaging has been shown to help reduce overfitting (McDonald et al., 2005a; Collins, 2002). It is easy to see that the main difference between the PA algorithms and the Perceptron algorithm (PC) (Collins, 2002) as well as the MIRA algorithm (McDonald et al., 2005a) is in line 9. As we can see in the PC algorithm, we do not need the value t and in the MIRA algorithm we need an optimization algorithm to compute t. We also have three updated formulations for obtaining t in Line 9. In the scope of this paper, we only focus on using the second update formulation (PA-I method) for training dependency parsing data. 2.3 Feature Set We denote p-word: word of parent node in </context>
<context position="11319" citStr="McDonald et al., 2005" startWordPosition="1925" endWordPosition="1928">tures shown in Table 1, the morphological information for each pair of words p-word and c-word are represented. In addition, we also add the conjunction morphological information of p-word and c-word. We do not use the LEMMA and CPOSTAG information in our set features. The morphological information are obtained from FEAT information. Table 3 shows our Feature set 3 which take the form of a POS trigram: the POS of the parent, of the child, and of a word in between, for all words linearly between the parent and the child. This feature was particularly helpful for nouns identifying their parent (McDonald et al., 2005a). Table 3 also depicts these features taken the form of a POS 4-gram: The POS of the parent, child, word before/after parent and word before/after child. The system also used backoff features with various trigrams where one of the local context POS tags was removed. All features are also conjoined with the direction of attachment, as well as the distance between the two words being attached. 3 Experimental Results and Discussion We test our parsing models on the CONLL-2007 (Hajic et al., 2004; Aduriz et al., 2003; Mart et al., 2007; Chen et al., 2003; Bohmova et al., 2003; Marcus et al., 199</context>
<context position="12928" citStr="McDonald et al., 2005" startWordPosition="2206" endWordPosition="2209">tences for these languages. The table shows that the sentence length in Arabic data is largest and its size of training data is smallest. These factors might be affected to the accuracy of our proposed algorithm as we will discuss later. The training and testing were conducted on a pentium IV at 4.3 GHz. The detailed information about the data are shown in the CONLL-2007 shared task. We applied non-projective and projective parsing along with PA learning for the data in CONLL-2007. Table 5 reports experimental results by using the first order decoding method in which an MST parsing algorithm (McDonald et al., 2005b) is applied for non-projective parsing and the Eisners method is used for projective language data. In fact, in our method we applied non-projective parsing for the Italian data, the Turkish data, and the Greek data. This was because we did not have enough time to train all training data using both projective and nonprojective parsing. This is the problem of discriminative learning methods when performing on a large set of training data. In addition, to save time in training we set the number of best trees k to 1 and the parameter C is set to 0.05. Table 5 shows the comparison of the propose</context>
</contexts>
<marker>McDonald, Cramer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Cramer, and F. Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. of the Human Language Technology Conf. and the Conf. on Empirical Methods in Natural Language Processing (HLT/EMNLP),</booktitle>
<pages>523530</pages>
<contexts>
<context position="1166" citStr="McDonald et al., 2005" startWordPosition="155" endWordPosition="158">er presents an online algorithm for dependency parsing problems. We propose an adaptation of the passive and aggressive online learning algorithm to the dependency parsing domain. We evaluate the proposed algorithms on the 2007 CONLL Shared Task, and report errors analysis. Experimental results show that the system score is better than the average score among the participating systems. 1 Introduction Research on dependency parsing is mainly based on machine learning methods, which can be called history-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) and discriminative learning methods (McDonald et al., 2005a; Corston-Oliver et al., 2006). The learning methods using in discriminative parsing are Perceptron (Collins, 2002) and online large-margin learning (MIRA) (Crammer and Singer, 2003). The difference of MIRA-based parsing in comparison with history-based methods is that the MIRA-based parser were trained to maximize the accuracy of the overall tree. The MIRA based parsing is close to maximum-margin parsing as in Taskar et al. (2004) and Tsochantaridis et al. (2005) for parsing. However, unlike maximum-margin parsing, it is not limited to parsing sentences of 15 words or less due to computation</context>
<context position="3816" citStr="McDonald et al. (2005" startWordPosition="577" endWordPosition="580"> problem is to learn a discriminant function that maps an input sentence x to a dependency tree y. Figure 1 shows an example of dependency parsing which depicts the relation of each word to another word within a sentence. There are some algorithms 1149 \x0cFigure 1: This is an example of dependency tree to determine these relations of each word to another words, for instance, the modified CKY algorithm (Eisner, 1996) is used to define these relations for a given sentence. 2.1 Parsing Algorithm Dependency-tree parsing as the search for the maximum spanning tree (MST) in a graph was proposed by McDonald et al. (2005b). In this subsection, we briefly describe the parsing algorithms based on the first-order MST parsing. Due to the limitation of participation time, we only applied the first-order decoding parsing algorithm in CONLL-2007. However, our algorithm can be used for the second order parsing. Let the generic sentence be denoted by x ; the ith word of x is denoted by wi. The generic dependency tree is denoted by y. If y is a dependency tree for sentence x, we write (i, j) y to indicate that there is a directed edge from word xwi to word xwj in the tree, that is, xwi is the parent of xwj . T = {(xt, </context>
<context position="5279" citStr="McDonald et al., 2005" startWordPosition="856" endWordPosition="859">, j) (1) where (i, j) is a high-dimensional binary feature representation of the edge from xwi to xwj . For example in Figure 1, we can present an example (i, j) as follows; (i, j) = ( 1 if xwi =0 hit0 and xwj =0 ball0 0 otherwise The basic question must be answered for models of this form: how to find the dependency tree y with the highest score for sentence x? The two algorithms we employed in our dependency parsing model are the Eisner parsing (Eisner, 1996) and Chu-Lius algorithm (Chu and Liu, 1965). The algorithms are commonly used in other online-learning dependency parsing, such as in (McDonald et al., 2005a). In the next subsection we will address the problem of how to estimate the weight wi associated with a feature i in the training data using an online PA learning algorithm. 2.2 Online PA Learning This section presents a modification of PA algorithm for structured prediction, and its use in dependency parsing. The Perceptron style for natural language processing problems as initially proposed by (Collins, 2002) can provide state of the art results on various domains including text chunking, syntactic parsing, etc. The main drawback of the Perceptron style algorithm is that it does not have a</context>
<context position="9028" citStr="McDonald et al., 2005" startWordPosition="1528" endWordPosition="1531">ued loss for the tree yt relative to the correct tree y. We define the loss of a dependency tree as the number of words which have an incorrect parent. Thus, the largest loss a dependency tree can have is the length of the sentence. The similar loss function is designed for the dependency tree with labeled. Algorithm 1 returns an averaged weight vector: an auxiliary weight vector v is maintained that accumulates the values of w after each iteration, and the returned weight vector is the average of all the weight vectors throughout training. Averaging has been shown to help reduce overfitting (McDonald et al., 2005a; Collins, 2002). It is easy to see that the main difference between the PA algorithms and the Perceptron algorithm (PC) (Collins, 2002) as well as the MIRA algorithm (McDonald et al., 2005a) is in line 9. As we can see in the PC algorithm, we do not need the value t and in the MIRA algorithm we need an optimization algorithm to compute t. We also have three updated formulations for obtaining t in Line 9. In the scope of this paper, we only focus on using the second update formulation (PA-I method) for training dependency parsing data. 2.3 Feature Set We denote p-word: word of parent node in </context>
<context position="11319" citStr="McDonald et al., 2005" startWordPosition="1925" endWordPosition="1928">tures shown in Table 1, the morphological information for each pair of words p-word and c-word are represented. In addition, we also add the conjunction morphological information of p-word and c-word. We do not use the LEMMA and CPOSTAG information in our set features. The morphological information are obtained from FEAT information. Table 3 shows our Feature set 3 which take the form of a POS trigram: the POS of the parent, of the child, and of a word in between, for all words linearly between the parent and the child. This feature was particularly helpful for nouns identifying their parent (McDonald et al., 2005a). Table 3 also depicts these features taken the form of a POS 4-gram: The POS of the parent, child, word before/after parent and word before/after child. The system also used backoff features with various trigrams where one of the local context POS tags was removed. All features are also conjoined with the direction of attachment, as well as the distance between the two words being attached. 3 Experimental Results and Discussion We test our parsing models on the CONLL-2007 (Hajic et al., 2004; Aduriz et al., 2003; Mart et al., 2007; Chen et al., 2003; Bohmova et al., 2003; Marcus et al., 199</context>
<context position="12928" citStr="McDonald et al., 2005" startWordPosition="2206" endWordPosition="2209">tences for these languages. The table shows that the sentence length in Arabic data is largest and its size of training data is smallest. These factors might be affected to the accuracy of our proposed algorithm as we will discuss later. The training and testing were conducted on a pentium IV at 4.3 GHz. The detailed information about the data are shown in the CONLL-2007 shared task. We applied non-projective and projective parsing along with PA learning for the data in CONLL-2007. Table 5 reports experimental results by using the first order decoding method in which an MST parsing algorithm (McDonald et al., 2005b) is applied for non-projective parsing and the Eisners method is used for projective language data. In fact, in our method we applied non-projective parsing for the Italian data, the Turkish data, and the Greek data. This was because we did not have enough time to train all training data using both projective and nonprojective parsing. This is the problem of discriminative learning methods when performing on a large set of training data. In addition, to save time in training we set the number of best trees k to 1 and the parameter C is set to 0.05. Table 5 shows the comparison of the propose</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proc. of the Human Language Technology Conf. and the Conf. on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 523530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Multilingual dependency parsing with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="1915" citStr="McDonald et al., 2006" startWordPosition="272" endWordPosition="275"> large-margin learning (MIRA) (Crammer and Singer, 2003). The difference of MIRA-based parsing in comparison with history-based methods is that the MIRA-based parser were trained to maximize the accuracy of the overall tree. The MIRA based parsing is close to maximum-margin parsing as in Taskar et al. (2004) and Tsochantaridis et al. (2005) for parsing. However, unlike maximum-margin parsing, it is not limited to parsing sentences of 15 words or less due to computation time. The performance of MIRA based parsing achieves the state-ofthe-art performance in English data (McDonald et al., 2005a; McDonald et al., 2006). In this paper, we propose a new adaptation of online larger-margin learning to the problem of dependency parsing. Unlike the MIRA parser, our method does not need an optimization procedure in each learning update, but users only an update equation. This might lead to faster training time and easier implementation. The contributions of this paper are two-fold: First, we present a training algorithm called PA learning for dependency parsing, which is as easy to implement as Perceptron, yet competitive with large margin methods. This algorithm has implications for anyone interested in implement</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2006</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2006. Multilingual dependency parsing with a two-stage discriminative parser. In Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delmonte</author>
</authors>
<title>Building the Italian SyntacticSemantic Treebank. In Abeille (Abeille,</title>
<date>2003</date>
<pages>189210</pages>
<marker>Delmonte, 2003</marker>
<rawString>S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and \x0cR. Delmonte. 2003. Building the Italian SyntacticSemantic Treebank. In Abeille (Abeille, 2003), chapter 11, pages 189210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>G Eryigit</author>
<author>S Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proc. of the Tenth Conf. on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>221225</pages>
<contexts>
<context position="1107" citStr="Nivre et al., 2006" startWordPosition="147" endWordPosition="150">-8579, Japan hieuxuan@ecei.tohoku.ac.jp Abstract This paper presents an online algorithm for dependency parsing problems. We propose an adaptation of the passive and aggressive online learning algorithm to the dependency parsing domain. We evaluate the proposed algorithms on the 2007 CONLL Shared Task, and report errors analysis. Experimental results show that the system score is better than the average score among the participating systems. 1 Introduction Research on dependency parsing is mainly based on machine learning methods, which can be called history-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) and discriminative learning methods (McDonald et al., 2005a; Corston-Oliver et al., 2006). The learning methods using in discriminative parsing are Perceptron (Collins, 2002) and online large-margin learning (MIRA) (Crammer and Singer, 2003). The difference of MIRA-based parsing in comparison with history-based methods is that the MIRA-based parser were trained to maximize the accuracy of the overall tree. The MIRA based parsing is close to maximum-margin parsing as in Taskar et al. (2004) and Tsochantaridis et al. (2005) for parsing. However, unlike maximum-margin parsing, it is not limited </context>
</contexts>
<marker>Nivre, Hall, Nilsson, Eryigit, Marinov, 2006</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S. Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Proc. of the Tenth Conf. on Computational Natural Language Learning (CoNLL), pages 221225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S Kubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proc.</booktitle>
<contexts>
<context position="2703" citStr="Nivre et al., 2007" startWordPosition="398" endWordPosition="401">ptimization procedure in each learning update, but users only an update equation. This might lead to faster training time and easier implementation. The contributions of this paper are two-fold: First, we present a training algorithm called PA learning for dependency parsing, which is as easy to implement as Perceptron, yet competitive with large margin methods. This algorithm has implications for anyone interested in implementing discriminative training methods for any application. Second, we evaluate the proposed algorithm on the multilingual data task as well as the domain adaptation task (Nivre et al., 2007). The remaining parts of the paper are organized as follows: Section 2 proposes our dependency parsing with Passive-Aggressive learning. Section 3 discusses some experimental results and Section 4 gives conclusions and plans for future work. 2 Dependency Parsing with Passive-Aggressive Learning This section presents the modification of PassiveAggressive Learning (PA) (Crammer et al., 2006) for dependency parsing. We modify the PA algorithm to deal with structured prediction, in which our problem is to learn a discriminant function that maps an input sentence x to a dependency tree y. Figure 1 </context>
<context position="13709" citStr="Nivre et al., 2007" startWordPosition="2345" endWordPosition="2348"> the Italian data, the Turkish data, and the Greek data. This was because we did not have enough time to train all training data using both projective and nonprojective parsing. This is the problem of discriminative learning methods when performing on a large set of training data. In addition, to save time in training we set the number of best trees k to 1 and the parameter C is set to 0.05. Table 5 shows the comparison of the proposed method with the average, and three top systems on the CONLL-2007. As a result, our method yields results above the average score on the CONLL-2007 shared task (Nivre et al., 2007). Table 5 also indicates that the Basque results obtained a lower score than other data. We obtained 69.11 UA score and 58.16 LA score, respectively. These are far from the results of the Top3 scores (81.13 and 75.49). We checked the outputs of the Basque data to understand the main reason for the errors. We see that the errors in our methods are usually mismatched with the gold data at the labels ncmod and ncsubj. The main reason might be that the application of projective parsing for this data in both training and testing is not suitable. This was because the number of sentences with at leas</context>
<context position="15345" citStr="Nivre et al., 2007" startWordPosition="2628" endWordPosition="2631"> improving the accuracy of English data, it might be unsuitable for other languages. This is an imbalance 1152 \x0cLanguages Training size Tokens size tokens-per-sent % of NPR % of-sentence AL-1-NPR Arabic 2,900 112,000 38.3 0.4 10.1 Basque 3,200 51,000 15.8 2.9 26.2 Catalan 15,000 431,000 28.8 0.1 2.9 Chinese 57,000 337,000 5.9 0.0 0.0 Czech 25,400 432,000 17.0 1.9 23.2 English 18,600 447,000 24.0 0.3 6.7 Greek 2,700 65,000 24.2 1.1 20.3 Hungarian 6,000 132,000 21.8 2.9 26.4 Italian 3,100 71,000 22.9 0.5 7.4 Turkish 5,600 65,000 11.6 0.5 33.3 Table 4: The data used in the multilingual track (Nivre et al., 2007). NPR means non-projective-relations. AL-1-NPR means at-least-least 1 non-projective relation. problem in our method. Table 5 also shows the comparison of our system to the average score and the Top3 scores. It depicts that our system is accurate in English data, while it has low accuracy in Basque and Arabic data. We also evaluate our models in the domain adaptation tasks. This task is to adapt our model trained on PennBank data to the test data in the Biomedical domain. The pchemtb-closed shared task (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004) is used to illustrate</context>
</contexts>
<marker>Nivre, Hall, Kubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proc.</rawString>
</citation>
<citation valid="true">
<title>of the CoNLL</title>
<date>2007</date>
<booktitle>Shared Task. Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL).</booktitle>
<marker>2007</marker>
<rawString>of the CoNLL 2007 Shared Task. Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
<author>B Say</author>
<author>D Zeynep Hakkani-Tur</author>
<author>G Tur</author>
</authors>
<title>Building a Turkish treebank. In Abeille (Abeille,</title>
<date>2003</date>
<pages>261277</pages>
<contexts>
<context position="12043" citStr="Oflazer et al., 2003" startWordPosition="2053" endWordPosition="2056">ord before/after parent and word before/after child. The system also used backoff features with various trigrams where one of the local context POS tags was removed. All features are also conjoined with the direction of attachment, as well as the distance between the two words being attached. 3 Experimental Results and Discussion We test our parsing models on the CONLL-2007 (Hajic et al., 2004; Aduriz et al., 2003; Mart et al., 2007; Chen et al., 2003; Bohmova et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oflazer et al., 2003) data set on various languages including Arabic, Basque, Catalan, Chinese, English, Italian, Hungarian, and Turkish. Each word is attached by POS tags for each sentence in both the training and the testing data. Table 4 shows the number of training and testing sentences for these languages. The table shows that the sentence length in Arabic data is largest and its size of training data is smallest. These factors might be affected to the accuracy of our proposed algorithm as we will discuss later. The training and testing were conducted on a pentium IV at 4.3 GHz. The detailed information about</context>
</contexts>
<marker>Oflazer, Say, Hakkani-Tur, Tur, 2003</marker>
<rawString>K. Oflazer, B. Say, D. Zeynep Hakkani-Tur, and G. Tur. 2003. Building a Turkish treebank. In Abeille (Abeille, 2003), chapter 15, pages 261277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Prokopidis</author>
<author>E Desypri</author>
<author>M Koutsombogera</author>
<author>H Papageorgiou</author>
<author>S Piperidis</author>
</authors>
<title>Theoretical and practical issues in the construction of a Greek dependency treebank.</title>
<date>2005</date>
<booktitle>In Proc. of the 4th Workshop on Treebanks and Linguistic Theories (TLT),</booktitle>
<pages>149160</pages>
<contexts>
<context position="11973" citStr="Prokopidis et al., 2005" startWordPosition="2040" endWordPosition="2043">features taken the form of a POS 4-gram: The POS of the parent, child, word before/after parent and word before/after child. The system also used backoff features with various trigrams where one of the local context POS tags was removed. All features are also conjoined with the direction of attachment, as well as the distance between the two words being attached. 3 Experimental Results and Discussion We test our parsing models on the CONLL-2007 (Hajic et al., 2004; Aduriz et al., 2003; Mart et al., 2007; Chen et al., 2003; Bohmova et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oflazer et al., 2003) data set on various languages including Arabic, Basque, Catalan, Chinese, English, Italian, Hungarian, and Turkish. Each word is attached by POS tags for each sentence in both the training and the testing data. Table 4 shows the number of training and testing sentences for these languages. The table shows that the sentence length in Arabic data is largest and its size of training data is smallest. These factors might be affected to the accuracy of our proposed algorithm as we will discuss later. The training and testing wer</context>
</contexts>
<marker>Prokopidis, Desypri, Koutsombogera, Papageorgiou, Piperidis, 2005</marker>
<rawString>P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papageorgiou, and S. Piperidis. 2005. Theoretical and practical issues in the construction of a Greek dependency treebank. In Proc. of the 4th Workshop on Treebanks and Linguistic Theories (TLT), pages 149160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C D Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In proceedings of EMNLP.</booktitle>
<contexts>
<context position="1602" citStr="Taskar et al. (2004)" startWordPosition="222" endWordPosition="225"> mainly based on machine learning methods, which can be called history-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) and discriminative learning methods (McDonald et al., 2005a; Corston-Oliver et al., 2006). The learning methods using in discriminative parsing are Perceptron (Collins, 2002) and online large-margin learning (MIRA) (Crammer and Singer, 2003). The difference of MIRA-based parsing in comparison with history-based methods is that the MIRA-based parser were trained to maximize the accuracy of the overall tree. The MIRA based parsing is close to maximum-margin parsing as in Taskar et al. (2004) and Tsochantaridis et al. (2005) for parsing. However, unlike maximum-margin parsing, it is not limited to parsing sentences of 15 words or less due to computation time. The performance of MIRA based parsing achieves the state-ofthe-art performance in English data (McDonald et al., 2005a; McDonald et al., 2006). In this paper, we propose a new adaptation of online larger-margin learning to the problem of dependency parsing. Unlike the MIRA parser, our method does not need an optimization procedure in each learning update, but users only an update equation. This might lead to faster training t</context>
<context position="6145" citStr="Taskar et al., 2004" startWordPosition="1000" endWordPosition="1003">rithm for structured prediction, and its use in dependency parsing. The Perceptron style for natural language processing problems as initially proposed by (Collins, 2002) can provide state of the art results on various domains including text chunking, syntactic parsing, etc. The main drawback of the Perceptron style algorithm is that it does not have a mechanism for attaining the maximize margin of the training data. It may be difficult to obtain high accuracy in dealing with hard learning data. The structured support vector machine (Tsochantaridis et al., 2005) and the maximize margin model (Taskar et al., 2004) can gain a maximize margin value for given training data by solving an optimization problem (i.e quadratic programming). It is obvious that using such an optimization algorithm requires much computational time. For dependency parsing domain, McDonald et al (2005a) modified the MIRA learning algorithm (McDonald et al., 2005a) for structured domains in which the optimization problem can be solved by using Hidreths algorithm (Censor and Zenios, 1997), which is faster than the quadratic programming technique. In contrast to the previous method, this paper presents an online algorithm for dependen</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>B. Taskar, D. Klein, M. Collins, D. Koller, and C.D. Manning. 2004. Max-margin parsing. In proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2005</date>
<booktitle>In proceedings ICML</booktitle>
<contexts>
<context position="1635" citStr="Tsochantaridis et al. (2005)" startWordPosition="227" endWordPosition="230">learning methods, which can be called history-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) and discriminative learning methods (McDonald et al., 2005a; Corston-Oliver et al., 2006). The learning methods using in discriminative parsing are Perceptron (Collins, 2002) and online large-margin learning (MIRA) (Crammer and Singer, 2003). The difference of MIRA-based parsing in comparison with history-based methods is that the MIRA-based parser were trained to maximize the accuracy of the overall tree. The MIRA based parsing is close to maximum-margin parsing as in Taskar et al. (2004) and Tsochantaridis et al. (2005) for parsing. However, unlike maximum-margin parsing, it is not limited to parsing sentences of 15 words or less due to computation time. The performance of MIRA based parsing achieves the state-ofthe-art performance in English data (McDonald et al., 2005a; McDonald et al., 2006). In this paper, we propose a new adaptation of online larger-margin learning to the problem of dependency parsing. Unlike the MIRA parser, our method does not need an optimization procedure in each learning update, but users only an update equation. This might lead to faster training time and easier implementation. Th</context>
<context position="6093" citStr="Tsochantaridis et al., 2005" startWordPosition="991" endWordPosition="994"> PA Learning This section presents a modification of PA algorithm for structured prediction, and its use in dependency parsing. The Perceptron style for natural language processing problems as initially proposed by (Collins, 2002) can provide state of the art results on various domains including text chunking, syntactic parsing, etc. The main drawback of the Perceptron style algorithm is that it does not have a mechanism for attaining the maximize margin of the training data. It may be difficult to obtain high accuracy in dealing with hard learning data. The structured support vector machine (Tsochantaridis et al., 2005) and the maximize margin model (Taskar et al., 2004) can gain a maximize margin value for given training data by solving an optimization problem (i.e quadratic programming). It is obvious that using such an optimization algorithm requires much computational time. For dependency parsing domain, McDonald et al (2005a) modified the MIRA learning algorithm (McDonald et al., 2005a) for structured domains in which the optimization problem can be solved by using Hidreths algorithm (Censor and Zenios, 1997), which is faster than the quadratic programming technique. In contrast to the previous method, </context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2005</marker>
<rawString>I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. 2005. Support vector machine learning for interdependent and structured output spaces. In proceedings ICML 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. 8th International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>195206</pages>
<contexts>
<context position="1086" citStr="Yamada and Matsumoto, 2003" startWordPosition="143" endWordPosition="146">Aobayama 6-3-09, Sendai, 980-8579, Japan hieuxuan@ecei.tohoku.ac.jp Abstract This paper presents an online algorithm for dependency parsing problems. We propose an adaptation of the passive and aggressive online learning algorithm to the dependency parsing domain. We evaluate the proposed algorithms on the 2007 CONLL Shared Task, and report errors analysis. Experimental results show that the system score is better than the average score among the participating systems. 1 Introduction Research on dependency parsing is mainly based on machine learning methods, which can be called history-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) and discriminative learning methods (McDonald et al., 2005a; Corston-Oliver et al., 2006). The learning methods using in discriminative parsing are Perceptron (Collins, 2002) and online large-margin learning (MIRA) (Crammer and Singer, 2003). The difference of MIRA-based parsing in comparison with history-based methods is that the MIRA-based parser were trained to maximize the accuracy of the overall tree. The MIRA based parsing is close to maximum-margin parsing as in Taskar et al. (2004) and Tsochantaridis et al. (2005) for parsing. However, unlike maximum-margin parsin</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. 8th International Workshop on Parsing Technologies (IWPT), pages 195206.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>