11 Though optimizing NER systems for F1 has been called into question CITATION, no alternative metric has achieved w,,
 algorithms (CITATION; CITATION; CITATION; Abdul-Hamid and Darwish, 2010),,
Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (CITATION; CITATION; CITATION) or to enumerate domain-specific types (CITATION; CITATION),,
Further, almost all of our article-specific categories correspond to classes in the extended NE taxonomy of CITATION, which speaks to the reasonableness of both sets of categoriesand by extension, our open-ended annotation process,,
4 Models Our starting point for statistical NER is a featurebased linear model over sequences, trained using the structured perceptron CITATION.8 In addition to lexical and morphological9 fea6 Additional details appear in the supplement,,
We will use these data as development and sible NEs CITATION,,
11 Though optimizing NER systems for F1 has been called into question CITATION, no alt,,
(2004) additionally predict within-document entity coreference for Arabic, Chinese, and English ACE text, while CITATION aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question,,
9 We obtain morphological analyses from the MADA tool (CITATION; CITATION),,
164 \x0cNEs from English articles to their Arabic counterparts CITATION, automatically clustering non-canonical types of entities into articlespecific or cross-article classes (cf,,
Even the ability to detect non-canonical types of NEs should help applications such as QA and MT (CITATION; CITATION),,
t entity coreference for Arabic, Chinese, and English ACE text, while CITATION aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question,,
CITATION and CITATION show that NER models trained on both automatically and manually annotated Wikipedia corpora perform reasonably well on news corpora,,
CITATION, which addresses English NER in noisy and topically divergent text,,
CITATION and Daume III (2007) make use of some labeled target-domain dat,,
Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (CITATION,,
, yM i, CITATIONb) define word-local cost functions that differently penalize precision errors (i.e., yi = O yi 6= O for the ith word), recall errors (yi 6= O yi = O), and entity class/position errors (other cases where yi 6= yi),,
Arabic is no exception: the publicly available NER corporaACE CITATION, ANER CITATION, and OntoNotes CITATIONall are in the news domain.2 However, 2 OntoNotes contains news-related text,,
tures known to work well for Arabic NER (CITATION; Abdul-Hamid and Darwish, 2010), we incorporate some additional features enabled by Wikipedia,,
at regularization CITATION and feature design (Daume III, 2007); we alter the loss function,,
widely used in NLP and has inspired related techniques that learn from automatically labeled data (CITATION; CITATION),,
CITATION highlight,,
These data challenge past approaches in two ways: First, Arabic is a morphologically rich language CITATION,,
Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (CITATION; CITATION; CITATION),,
Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (CITATION; CITATION; CITATION),,
Both the English and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (CITATION; CITATION),,
Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (CITATION; CITATION; CITATION) or to enumerate domain-specific types (Se,,
Ideally, an NER system would refine the traditional classes CITATION or identify new entity classes when they arise in new domains, adapting to new data,,
CITATION and CITATION show that NER models trained on both automatically and manually annotated Wikipedi,,
5.2 Self-Training Following CITATION, we applied selftraining as described in Algorithm 1, with the perceptron as the supervised learner,,
We note that CITATION similarly explored the recall vs,,
Our approach follows ACE guidelines CITATION in identifying NE boundaries and choosing POL tags,,
tures known to work well for Arabic NER (CITATION; Abdul-H,,
an effect attested in earlier research CITATION and sometimes known as semantic drift,,
Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs CITATION,,
glish and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (CITATION; CITATION),,
An alternativeand simplerapproach to controlling the precision-recall tradeoff is the CITATION strategy of tuning a single feature weight subsequent to learning (see 4.1 above),,
 for Python CITATION,,
ences, trained using the structured perceptron CITATION.8 In addition to lexical and morphological9 fea6 Additional details appear in the supplement,,
CITATION highlight the substantial divergence between entities ,,
In these respects our NER setting is closer to that of CITATION, who recognize English entities in noisy text, CITATION, which concerns information extraction in a topically distinct target domain, and CITATION, which addresses English NER in noisy and topically divergent text,,
zing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (CITATION; CITATION; CITATION) or to enumerate domain-specific types (CITATION; CITATION),,
Wikipedia, for instance CITATION,,
Research in Arabic NER has been focused on compiling and optimizing the gazetteers and fea169 \x0cture sets for standard sequential modeling algorithms (CITATION; CITATION; CITATION; Abdul-Hamid and Darwish, 2010),,
Our out-of-domain labeled NE data is drawn from the ANER CITATION and ACE-2005 CITATION newswire corpora,,
CITATION highlight the substantial dive,,
7 We downloaded a snapshot of Arabic Wikipedia (http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and metadata using the mwlib package for Python CITATION,,
This is measured by per-entity precision, recall, and F1.13 To measure statistical significance of differences between models we use Gimpel and Smiths (2010) implementation of the paired bootstrap resampler of CITATION, taking 10,000 samples for each comparison,,
11 Though optimizing NER systems for F1 has been called into question CITATION, no alternative metric has achieved widespread acceptance in the community,,
The goal of optimizing our structured NER model for recall is quite similar to the scenario explored by CITATION, as noted above,,
CITATION annotated and evaluated an Arabic NE corpus with an extended set of 18 classes (including temporal and numeric entities); this corpus has not been released publicly,,
Self-training (CITATION; CITATION; CITATION) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (CITATION; CITATION),,
s (CITATION; CITATION),,
CITATION highlight the substantial divergence between entities appearing in English Wikipedia versus traditional corpora, and the effects of this divergence on NER performance,,
CITATION annotated and evaluated an Arabic NE corpus with an extended set of 18 classes (includin,,
CITATION and Daume III (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain,,
4.1 Recall-Oriented Perceptron By augmenting the perceptrons online update with a cost function term, we can incorporate a task-dependent notion of error into the objective, as with structured SVMs (CITATION; CITATION),,
In these respects our NER setting is closer to that of CITATION, who rec,,
CITATION bootstrap the NER leaner with a subset of unlabeled instances that bridge the source and target domains,,
If c fac10 A gazetteer ought to yield further improvements in line with previous findings in NER CITATION,,
ed linear model over sequences, trained using the structured perceptron CITATION.8 In addition to lexical and morphological9 fea6 Additional details appear in the supplement,,
, we can incorporate a task-dependent notion of error into the objective, as with structured SVMs (CITATION; CITATION),,
