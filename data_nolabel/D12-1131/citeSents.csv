06) and an entropy-based regularizer for CRFs CITATION,,
For parser adaptation, self-training (CITATION,,
in adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey,,
aper, but we refer the reader to CITATION for a recent survey,,
For experiments in Bulgarian, German, Japanese, and Spanish, we use the CONLL-X data set CITATION with training data taken from the official training files,,
To fix this sparsity issue, we import additional unannotated sentences for each unknown word from the New York Times Section of the NANC corpus CITATION,,
Sentence-Level Models For dependency parsing we utilize the second-order projective MST parser CITATION1 with the gold-standard POS tags of the corpus,,
We compare the accuracy of dependency parsing with global constraints to the sentence-level dependency parser of CITATION and to a self-training baseline (CITATION; CITATION),,
Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (CITATION; CITATION), generalized expectation (CITATION; Mann and McCallum, ), and Bayesian measurements CITATION frameworks,,
ST is a self-training model based on CITATION,,
Stanford POS tagger refers to the maximum entropy trigram tagger of CITATION,,
The application of dual decomposition for inference in MRFs has been explored by CITATION, CITATION, and CITATION,,
Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifo,,
WSJ and Brown) CITATION and were s,,
sed by CITATION for POS tagger adaptation,,
s CITATION,,
recent survey see CITATION,,
Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION,,
In NLP, CITATION and CITATION applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging,,
CITATION also integrated non-local information into entity annotation algorithms using Gibbs sampling,,
Constraints similar to those we use for POS tagging were used by CITATION for POS tagger adaptation,,
Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) CITATION,,
Base is the second-order, projective dependency parser of CITATION,,
In our dual decomposition inference algorithm, we use K = 200 maximum iterations and tune the decay rate following the protocol described by CITATION,,
ave been applied to problems in IE, and RMNs to named entity recognition (NER) CITATION,,
For POS tagging we use the Stanford POS tagger CITATION2,,
For example in commonly used projective dependency parsing models CITATION, we can compute y efficiently using variants of the Viterbi algorithm,,
It is a slight variation of the proof given by CITATION,,
For domain adaptation, we show an error reduction of up to 7.7% when adapting the second-order projective MST parser CITATION from newswire to the QuestionBank domain,,
 is a self-training model based on CITATION,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup,,
 refers to the maximum entropy trigram tagger of CITATION,,
CITATION),,
ally for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup,,
Constraints similar to those we use for POS tagging were used by CITATION for POS ta,,
NER) CITATION,,
First, we follow CITATION and use lazy decoding as part of dual decomposition,,
3 We do not run self-training for POS tagging as it has been shown unuseful for this application CITATION,,
and Mcallester, 2005), manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance g,,
co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup,,
oblems in IE, and RMNs to named entity recognition (NER) CITATION,,
Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifold regularization (Bel,,
We next introduce notation for Markov random fields (MRFs) CITATION,,
 an error reduction of up to 12.8% over the same parser for five languages and an error reduction of up to 10.3% over the Stanford trigram tagger CITATION for English POS tagging,,
Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems CITATION,,
pendency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation,,
Evaluation and Baselines To measure parsing performance, we use unlabeled attachment score (UAS) given by the CONLL-X dependency parsing shared task evaluation script CITATION,,
The most similar models to our work are skip-chain CRFs CITATION, relational markov networks CITATION, and collective inference with symmetric clique potentials CITATION,,
For a recent survey see CITATION,,
ised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION,,
on dual decomposition for NLP is related to the work of CITATION who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation,,
Second, during the first iteration of the algorithm we apply max-marginal based pruning using the threshold defined by CITATION,,
Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (CITATION; CITATION), generalized expectation (CITATION; Mann a,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance gains,,
 error reduction of up to 10.3% over the Stanford trigram tagger CITATION for English POS tagging,,
 CITATION and an entropy-based regularizer for CRFs CITATION,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; Rim,,
Data for Domain Adaptation We perform domain adaptation experiments in English using the WSJ PennTreebank CITATION and the QuestionBank (QTB) (Judge et al., 2006),,
For experiments in Bulgarian, German, Japanese, and Spanish, we use the CONLL-X data set CITATION with training data,,
manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION,,
g the WSJ PennTreebank CITATION and the QuestionBank (QTB) (Judge et al., 2006),,
For parser adaptation, self-training (CITATION; CITATION), using weakly an,,
Work on dual decomposition for NLP is related to the work of CITATION who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation,,
WSJ and Brown) CITATION and were shown useful in generative and discriminative parsers (e.g,,
Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (CITATION; CITATION), generalized expectation (CITATION; Mann and McCallum, ), and Ba,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning (CITATION,,
For lightly supervised learning, we show an error reduction of up to 12.8% over the same parser for five languages and an error reduction of up to 10.3% over the Stanford trigram tagger CITATION for English POS tagging,,
