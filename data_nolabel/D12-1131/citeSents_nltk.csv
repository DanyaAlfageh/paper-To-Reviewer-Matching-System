In NLP, CITATION and CITATION applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging.,,
Work on dual decomposition for NLP is related to the work of CITATION who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation.,,
Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION.,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION,,
Work on dual decomposition for NLP is related to the work of CITATION who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation.,,
Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION.,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly an,,
pendency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation.,,
Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION.,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance g,,
Work on dual decomposition for NLP is related to the work of CITATION who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation.,,
Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION.,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; Rim,,
g the WSJ PennTreebank CITATION and the QuestionBank (QTB) (Judge et al., 2006).,,
For experiments in Bulgarian, German, Japanese, and Spanish, we use the CONLL-X data set CITATION with training data taken from the official training files.,,
Stanford POS tagger refers to the maximum entropy trigram tagger of CITATION.,,
Evaluation and Baselines To measure parsing performance, we use unlabeled attachment score (UAS) given by the CONLL-X dependency parsing shared task evaluation script CITATION.,,
We compare the accuracy of dependency parsing with global constraints to the sentence-level dependency parser of CITATION and to a self-training baseline (CITATION; CITATION).,,
The most similar models to our work are skip-chain CRFs CITATION, relational markov networks CITATION, and collective inference with symmetric clique potentials CITATION.,,
Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) CITATION.,,
CITATION also integrated non-local information into entity annotation algorithms using Gibbs sampling.,,
Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (CITATION; CITATION), generalized expectation (CITATION; Mann a,,
sed by CITATION for POS tagger adaptation.,,
WSJ and Brown) CITATION and were shown useful in generative and discriminative parsers (e.g.,,
CITATION).,,
3 We do not run self-training for POS tagging as it has been shown unuseful for this application CITATION.,,
recent survey see CITATION.,,
Constraints similar to those we use for POS tagging were used by CITATION for POS tagger adaptation.,,
WSJ and Brown) CITATION and were shown useful in generative and discriminative parsers (e.g.,,
CITATION).,,
in adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance gains.,,
For a recent survey see CITATION.,,
Constraints similar to those we use for POS tagging were used by CITATION for POS tagger adaptation.,,
The most similar models to our work are skip-chain CRFs CITATION, relational markov networks CITATION, and collective inference with symmetric clique potentials CITATION.,,
Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) CITATION.,,
CITATION also integrated non-local information into entity annotation algorithms using Gibbs sampling.,,
Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (CITATION; CITATION), generalized expectation (CITATION; Mann and McCallum, ), and Ba,,
ave been applied to problems in IE, and RMNs to named entity recognition (NER) CITATION.,,
CITATION also integrated non-local information into entity annotation algorithms using Gibbs sampling.,,
Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (CITATION; CITATION), generalized expectation (CITATION; Mann and McCallum, ), and Bayesian measurements CITATION frameworks.,,
oblems in IE, and RMNs to named entity recognition (NER) CITATION.,,
CITATION also integrated non-local information into entity annotation algorithms using Gibbs sampling.,,
Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (CITATION; CITATION), generalized expectation (CITATION; Mann and McCallum, ), and Bayesian measurements CITATION frameworks.,,
The application of dual decomposition for inference in MRFs has been explored by CITATION, CITATION, and CITATION.,,
In NLP, CITATION and CITATION applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging.,,
Work on dual decomposition for NLP is related to the work of CITATION who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation.,,
To fix this sparsity issue, we import additional unannotated sentences for each unknown word from the New York Times Section of the NANC corpus CITATION.,,
 error reduction of up to 10.3% over the Stanford trigram tagger CITATION for English POS tagging.,,
The most similar models to our work are skip-chain CRFs CITATION, relational markov networks CITATION, and collective inference with symmetric clique potentials CITATION.,,
Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) CITATION.,,
CITATION also integrated non-local information into entity annotation algorithms using Gibbs sampling.,,
ised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION.,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance gains.,,
For a recent survey see CITATION.,,
Constraints similar to those we use for POS tagging were used by CITATION for POS tagger adaptation.,,
We next introduce notation for Markov random fields (MRFs) CITATION.,,
The application of dual decomposition for inference in MRFs has been explored by CITATION, CITATION, and CITATION.,,
In NLP, CITATION and CITATION applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging.,,
Work on dual decomposition for NLP is related to the work of CITATION who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation.,,
The application of dual decomposition for inference in MRFs has been explored by CITATION, CITATION, and CITATION.,,
In NLP, CITATION and CITATION applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging.,,
Work on dual decomposition for NLP is related to the work of CITATION who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation.,,
Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifold regularization (Bel,,
The first problem factors naturally into sentence-level parsing problems and the second can be solved efficiently given our assumptions on the MRF topology G. Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems CITATION.,,
In our dual decomposition inference algorithm, we use K = 200 maximum iterations and tune the decay rate following the protocol described by CITATION.,,
Sentence-Level Models For dependency parsing we utilize the second-order projective MST parser CITATION1 with the gold-standard POS tags of the corpus.,,
For POS tagging we use the Stanford POS tagger CITATION2.,,
First, we follow CITATION and use lazy decoding as part of dual decomposition.,,
 CITATION and an entropy-based regularizer for CRFs CITATION.,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance gains.,,
For a recent survey see CITATION.,,
Constraints similar to those we use for POS tagging were used by CITATION for POS tagger adaptation.,,
Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (CITATION; CITATION), generalized expectation (CITATION; Mann and McCallum, ), and Bayesian measurements CITATION frameworks.,,
NER) CITATION.,,
CITATION also integrated non-local information into entity annotation algorithms using Gibbs sampling.,,
Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (CITATION; CITATION), generalized expectation (CITATION; Mann and McCallum, ), and Bayesian measurements CITATION frameworks.,,
Data for Domain Adaptation We perform domain adaptation experiments in English using the WSJ PennTreebank CITATION and the QuestionBank (QTB) (Judge et al., 2006).,,
For experiments in Bulgarian, German, Japanese, and Spanish, we use the CONLL-X data set CITATION with training data,,
manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION.,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance gains.,,
For a recent survey see CITATION.,,
Constraints similar to those we use for POS tagging were used by CITATION for POS tagger adaptation.,,
and Mcallester, 2005), manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION.,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance gains.,,
For a recent survey see CITATION.,,
Constraints similar to those we use for POS tagging were used by CITATION for POS tagger adaptation.,,
s CITATION.,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance gains.,,
For a recent survey see CITATION.,,
Constraints similar to those we use for POS tagging were used by CITATION for POS tagger adaptation.,,
For domain adaptation, we show an error reduction of up to 7.7% when adapting the second-order projective MST parser CITATION from newswire to the QuestionBank domain.,,
For lightly supervised learning, we show an error reduction of up to 12.8% over the same parser for five languages and an error reduction of up to 10.3% over the Stanford trigram tagger CITATION for English POS tagging.,,
For example in commonly used projective dependency parsing models CITATION, we can compute y efficiently using variants of the Viterbi algorithm.,,
In our dual decomposition inference algorithm, we use K = 200 maximum iterations and tune the decay rate following the protocol described by CITATION.,,
Sentence-Level Models For dependency parsing we utilize the second-order projective MST parser CITATION1 with the gold-standard POS tags of the corpus.,,
For POS tagging we use the Stanford POS tagger CITATION2.,,
Stanford POS tagger refers to the maximum entropy trigram tagger of CITATION.,,
Evaluation and Baselines To measure parsing performance, we use unlabeled attachment score (UAS) given by the CONLL-X dependency parsing shared task evaluation script CITATION.,,
We compare the accuracy of dependency parsing with global constraints to the sentence-level dependency parser of CITATION and to a self-training baseline (CITATION; CITATION).,,
ally for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance gains.,,
For a recent survey see CITATION.,,
Constraints similar to those we use for POS tagging were used by CITATION for POS tagger adaptation.,,
WSJ and Brown) CITATION and were s,,
Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION.,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance gains.,,
For a recent survey see CITATION.,,
Base is the second-order, projective dependency parser of CITATION.,,
ST is a self-training model based on CITATION.,,
Stanford POS tagger refers to the maximum entropy trigram tagger of CITATION.,,
06) and an entropy-based regularizer for CRFs CITATION.,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance gains.,,
For a recent survey see CITATION.,,
Constraints similar to those we use for POS tagging were used by CITATION for POS tagger adaptation.,,
The application of dual decomposition for inference in MRFs has been explored by CITATION, CITATION, and CITATION.,,
In NLP, CITATION and CITATION applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging.,,
Work on dual decomposition for NLP is related to the work of CITATION who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation.,,
Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION.,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
The application of dual decomposition for inference in MRFs has been explored by CITATION, CITATION, and CITATION.,,
In NLP, CITATION and CITATION applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging.,,
Work on dual decomposition for NLP is related to the work of CITATION who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation.,,
Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifo,,
It is a slight variation of the proof given by CITATION.,,
aper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance gains.,,
For a recent survey see CITATION.,,
Constraints similar to those we use for POS tagging were used by CITATION for POS tagger adaptation.,,
The application of dual decomposition for inference in MRFs has been explored by CITATION, CITATION, and CITATION.,,
In NLP, CITATION and CITATION applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging.,,
Work on dual decomposition for NLP is related to the work of CITATION who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation.,,
Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION.,,
Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION.,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance gains.,,
For a recent survey see CITATION.,,
Constraints similar to those we use for POS tagging were used by CITATION for POS ta,,
 refers to the maximum entropy trigram tagger of CITATION.,,
Evaluation and Baselines To measure parsing performance, we use unlabeled attachment score (UAS) given by the CONLL-X dependency parsing shared task evaluation script CITATION.,,
We compare the accuracy of dependency parsing with global constraints to the sentence-level dependency parser of CITATION and to a self-training baseline (CITATION; CITATION).,,
co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning CITATION, hierarchical bayesian models CITATION and co-training CITATION achieve substantial performance gains.,,
For a recent survey see CITATION.,,
Constraints similar to those we use for POS tagging were used by CITATION for POS tagger adaptation.,,
WSJ and Brown) CITATION and were shown useful in generative and discriminative parsers (e.g.,,
CITATION).,,
For lightly supervised learning, we show an error reduction of up to 12.8% over the same parser for five languages and an error reduction of up to 10.3% over the Stanford trigram tagger CITATION for English POS tagging.,,
The most similar models to our work are skip-chain CRFs CITATION, relational markov networks CITATION, and collective inference with symmetric clique potentials CITATION.,,
Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) CITATION.,,
CITATION also integrated non-local information into entity annotation algorithms using Gibbs sampling.,,
 an error reduction of up to 12.8% over the same parser for five languages and an error reduction of up to 10.3% over the Stanford trigram tagger CITATION for English POS tagging.,,
The most similar models to our work are skip-chain CRFs CITATION, relational markov networks CITATION, and collective inference with symmetric clique potentials CITATION.,,
Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) CITATION.,,
CITATION also integrated non-local information into entity annotation algorithms using Gibbs sampling.,,
For domain adaptation, we show an error reduction of up to 7.7% when adapting the second-order projective MST parser CITATION from newswire to the QuestionBank domain.,,
For lightly supervised learning, we show an error reduction of up to 12.8% over the same parser for five languages and an error reduction of up to 10.3% over the Stanford trigram tagger CITATION for English POS tagging.,,
The most similar models to our work are skip-chain CRFs CITATION, relational markov networks CITATION, and collective inference with symmetric clique potentials CITATION.,,
In our dual decomposition inference algorithm, we use K = 200 maximum iterations and tune the decay rate following the protocol described by CITATION.,,
Sentence-Level Models For dependency parsing we utilize the second-order projective MST parser CITATION1 with the gold-standard POS tags of the corpus.,,
For POS tagging we use the Stanford POS tagger CITATION2.,,
 is a self-training model based on CITATION.,,
Stanford POS tagger refers to the maximum entropy trigram tagger of CITATION.,,
Evaluation and Baselines To measure parsing performance, we use unlabeled attachment score (UAS) given by the CONLL-X dependency parsing shared task evaluation script CITATION.,,
We compare the accuracy of dependency parsing with global constraints to the sentence-level dependency parser of CITATION and to a self-training baseline (CITATION; CITATION).,,
The application of dual decomposition for inference in MRFs has been explored by CITATION, CITATION, and CITATION.,,
In NLP, CITATION and CITATION applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging.,,
Work on dual decomposition for NLP is related to the work of CITATION who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation.,,
on dual decomposition for NLP is related to the work of CITATION who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) CITATION that impose inference-time constraints through an ILP formulation.,,
Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles CITATION, manifold regularization CITATION, a structured version of co-training CITATION and an entropy-based regularizer for CRFs CITATION.,,
The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to CITATION for a recent survey.,,
Specifically for parsing and POS tagging, selftraining CITATION, co-training CITATION and active learning CITATION have been shown useful in the lightly supervised setup.,,
For parser adaptation, self-training (CITATION; CITATION), using weakly annotated data from the target domain (CITATION; CITATION), ensemble learning (CITATION,,
Second, during the first iteration of the algorithm we apply max-marginal based pruning using the threshold defined by CITATION.,,
