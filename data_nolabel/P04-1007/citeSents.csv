CITATION used a language model modification algorithm in the context of a reading tutor that lis,,
This work was followed up with some success by CITATION where an antiLM, estimated from weighted N-best hypotheses o,,
This technique has been shown to be very effective in a variety of NLP tasks (CITATION; CITATION),,
2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (CITATION; CITATION), and tagging or segmentation tasks (CITATION; CITATION; CITATION; CITATION),,
All of the CRF trials are significantly better than the perceptron performance, using the Matched Pair Sentence Segment test for WER included with SCTK CITATION,,
We use a limited memory variable metric method CITATION to optimize LLR,,
This section describes the perceptron algorithm, which was previously applied to language modeling in CITATION,,
We used the same training set as that used in CITATION,,
Following CITATION), we used the averaged parameters from the training algorithm in decoding heldout and test examples in our experiments,,
SVM-based approaches use an optimization criterion that is closely related to LLR see CITATION for more discussion,,
CITATION presented a method based on changing the trigram counts discriminatively, together with changing the lexicon to add new words,,
CITATION experimented with various discriminative approaches including MMIE with mixed results,,
The other two baselines are with perceptron-trained n-gram model re-weighting, and were reported in CITATION,,
Our work builds on previous work on language modeling using the perceptron algorithm, described in CITATION,,
CITATION suggested an acoustic sensitive language model whose parameters 1Note also that in addition to concerns about training time, a language model with fewer features is likely to be considerably more efficient when decoding new u,,
CITATION have shown the effectiveness of lattice-based MMIE/CMLE in challenging large scale ASR tasks such as Switchboard,,
CITATION used a language model modification algorithm in the context of a reading tutor that listens,,
There are several papers describing the use of weighted automata and transducers for speech in detail, e.g., CITATION, but for clarity and completeness this section gives a brief description of the operations which we use,,
2.1 Global linear models We follow the framework outlined in CITATION; 2004),,
CITATION suggested an acoustic sensitive language model whose parameters 1Note also that in addition to concerns about training time, a language model with fewer features is likely to be considerably more efficient when decoding new utterances,,
There is a general implementation of this method in the Tao/PETSc software libraries (CITATION; CITATION),,
Experiments in CITATION suggest that the perceptron reaches optimal performance after a small number of training iterations, for example T = 1 or T = 2,,
While we focus on n-gram models, we stress that our methods are applicable to more general language modeling features for example, syntactic features, as explored in, e.g., CITATION,,
This work was followed up with some success by CITATION where an antiLM, estimated from weighted N-best hypotheses of a baseline ASR system, was used with a negative weight in combination with the baseline LM,,
CITATION originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in CITATION),,
The GRM library, which was presented in CITATION, has a direct implementation of the function ExpCount, which simultaneously calculates the expected value of all n-grams of order less than or equal to a given n in a lattice L,,
CITATION used the generalized probabilistic descent algorithm to train relatively small language models which attempt to minimize stri,,
3.3 Representation of n-gram language models An n-gram model can be efficiently represented in a deterministic WFA, through the use of failure transitions CITATION,,
CITATION used the generalized probabilistic descent algorithm to train relatively small language models which attempt to minimize string error rate on the DARPA Communicator task,,
