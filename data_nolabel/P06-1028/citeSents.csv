Moreover, for the parameter optimization process, we can simply exploit gradient descent or quasi-Newton methods such as L-BFGS CITATION as well as ML/MAP optimization,,
With Chunking, CITATION reported the best F-score of 93.91 with the voting of several models trained by Support Vector Machine in the same experimental settings and with the same feature set,,
6 Related Work Various loss functions have been proposed for designing CRFs (CITATION; CITATION),,
, 2005) that even simple gradient-based (firstorder) optimization methods such as GPD and (approximated) second-order methods such as QuickProp CITATION and BFGS-based methods have yielded good experimental optimization results,,
Furthermore, large margin criteria have been employed to optimize the model parameters (CITATION; CITATION),,
However, MCE-F showed the better performance of 85.29 compared with CITATION of 84.04, which used the MAP training,,
12, by using the variant of the forward-backward and Viterbi algorithm described in CITATION,,
Therefore, we can find the maximum incorrect output by using the A* algorithm CITATION, if the maximum output is the correct output, and by using the Viterbi algorithm otherwise,,
1 Introduction Conditional random fields (CRFs) are a recently introduced formalism CITATION for representing a conditional model p(y|x), where both a set of inputs, x, and a set of outputs, y, display non-trivial interdependency,,
4.2 Segmentation F-score Loss for SSTs The standard evaluation measure of SSTs is the segmentation F-score CITATION: F = (2 + 1) TP 2 FN + FP + (2 + 1) TP (13) 220 \x0cHe reckons the current account deficit will narrow to only # 1.8 billion ,,
onditional probability p(y|x) itself CITATION,,
However, MCE-F showed the better performance of 85.29 compared with CITATION of 84.04, which used the MAP training of CRFs with a feature selection architecture, yielding similar results to the MAP results described here,,
In fact, to overcome this inconsistency, an SVM-based multivariate optimization method has recently been proposed CITATION,,
Although an efficient way to evaluate all possible segments has been proposed in the context of semi-Markov CRFs CITATION, we introduce a simple alternative method,,
5.2 Features As regards the basic feature set for Chunking, we followed CITATION, which is the same feature set that provided the best result in CoNLL-2000,,
(CRFs) are a recently introduced formalism CITATION for representing a conditional model p(y|x), where both a set of inputs, x, and a set of outputs, y, display non-trivial interdependency,,
We need to define a segment-wise loss, in contrast to the standard CRF loss, which is sometimes referred to as an (entire) sequential loss (CITATION; CITATION),,
This point-wise discriminant function is different from that described in (CITATION; CITATION), which is calculated based on mar,,
Our proposed framework is fundamentally derived from an approach to (smoothed) error rate minimization well 217 \x0cknown in the speech and pattern recognition community, namely the Minimum Classification Error (MCE) framework CITATION,,
econd-order methods such as QuickProp CITATION and BFGS-based methods have yielded good experimental optimization results,,
Several non-linear objective functions, such as F-score for text classification CITATION, and BLEU-score and some other evaluation measures for statistical machine translation CITATION, have been introduced with reference to th,,
The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing CITATION and information extraction CITATION,,
The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing CITATION and information extraction (McCallum and Li,,
Following the definitions of CITATION, a log-linear combination of weighted features, c(y, x; ) = exp( fc(y, x)), is used as individual potential functions, where fc represents a feature vector obtained from the corresponding clique c,,
Several non-linear objective functions, such as F-score for text classification CITATION, and BLEU-score and some other evaluation measures for statistical machine translation CITATION, have been introduced with reference to the framework of MCE criterion training,,
However, it has been shown (Le Roux and McDermott, 2005) that even simple gradient-based (firstorder) optimization methods such as GPD and (approximated) second-order methods such as QuickProp CITATION and BFGS-based methods have yielded good experimental optimization results,,
This point-wise discriminant function is different from that described in (CITATION; CITATION), which is calculated based on marginals,,
3 MCE Criterion Training for CRFs The Minimum Classification Error (MCE) framework first arose out of a broader family of approaches to pattern classifier design known as Generalized Probabilistic Descent (GPD) CITATION,,
The first approach to estimating CRF parameters is the maximum likelihood (ML) criterion over conditional probability p(y|x) itself CITATION,,
These tasks are generally treated as sequential labeling problems incorporating the IOB tagging scheme CITATION,,
Moreover, an F-score optimization method for logistic regression has also been proposed CITATION,,
The details of actual optimization procedures for linear chain CRFs, which are typical CRF applications, have already been reported CITATION,,
The maximum a posteriori (MAP) criterion over parameters, , given x and y is the natural choice for reducing over-fitting CITATION,,
5 Experiments We used the same Chunking and English NER task data used for the shared tasks of CoNLL2000 CITATION and CoNLL2003 (Sang and De Meulder, 2003), respectively,,
y p(y|x) itself CITATION,,
Moreover, the Bayes approach, which optimizes both MAP and the prior distribution of the parameters, has also been proposed CITATION,,
3 with a variant of the forwardbackward algorithm CITATION,,
