We need to define a segment-wise loss, in contrast to the standard CRF loss, which is sometimes referred to as an (entire) sequential loss (CITATION; CITATION).,,
This point-wise discriminant function is different from that described in (CITATION; CITATION), which is calculated based on marginals.,,
6 Related Work Various loss functions have been proposed for designing CRFs (CITATION; CITATION).,,
With Chunking, CITATION reported the best F-score of 93.91 with the voting of several models trained by Support Vector Machine in the same experimental settings and with the same feature set.,,
Therefore, we can find the maximum incorrect output by using the A* algorithm CITATION, if the maximum output is the correct output, and by using the Viterbi algorithm otherwise.,,
However, it has been shown (Le Roux and McDermott, 2005) that even simple gradient-based (firstorder) optimization methods such as GPD and (approximated) second-order methods such as QuickProp CITATION and BFGS-based methods have yielded good experimental optimization results.,,
Several non-linear objective functions, such as F-score for text classification CITATION, and BLEU-score and some other evaluation measures for statistical machine translation CITATION, have been introduced with reference to th,,
, 2005) that even simple gradient-based (firstorder) optimization methods such as GPD and (approximated) second-order methods such as QuickProp CITATION and BFGS-based methods have yielded good experimental optimization results.,,
Several non-linear objective functions, such as F-score for text classification CITATION, and BLEU-score and some other evaluation measures for statistical machine translation CITATION, have been introduced with reference to the framework of MCE criterion training.,,
12, by using the variant of the forward-backward and Viterbi algorithm described in CITATION.,,
Moreover, for the parameter optimization process, we can simply exploit gradient descent or quasi-Newton methods such as L-BFGS CITATION as well as ML/MAP optimization.,,
Therefore, we can find the maximum incorrect output by using the A* algorithm CITATION, if the maximum output is the correct output, and by using the Viterbi algorithm otherwise.,,
However, it has been shown (Le Roux and McDermott, 2005) that even simple gradient-based (firstorder) optimization methods such as GPD and (approximated) second-order methods such as QuickProp CITATION and BFGS-based methods have yielded good experimental optimization results.,,
In fact, to overcome this inconsistency, an SVM-based multivariate optimization method has recently been proposed CITATION.,,
Moreover, an F-score optimization method for logistic regression has also been proposed CITATION.,,
In fact, to overcome this inconsistency, an SVM-based multivariate optimization method has recently been proposed CITATION.,,
Moreover, an F-score optimization method for logistic regression has also been proposed CITATION.,,
Our proposed framework is fundamentally derived from an approach to (smoothed) error rate minimization well 217 \x0cknown in the speech and pattern recognition community, namely the Minimum Classification Error (MCE) framework CITATION.,,
We need to define a segment-wise loss, in contrast to the standard CRF loss, which is sometimes referred to as an (entire) sequential loss (CITATION; CITATION).,,
This point-wise discriminant function is different from that described in (CITATION; CITATION), which is calculated based on mar,,
6 Related Work Various loss functions have been proposed for designing CRFs (CITATION; CITATION).,,
With Chunking, CITATION reported the best F-score of 93.91 with the voting of several models trained by Support Vector Machine in the same experimental settings and with the same feature set.,,
The details of actual optimization procedures for linear chain CRFs, which are typical CRF applications, have already been reported CITATION.,,
3 MCE Criterion Training for CRFs The Minimum Classification Error (MCE) framework first arose out of a broader family of approaches to pattern classifier design known as Generalized Probabilistic Descent (GPD) CITATION.,,
5.2 Features As regards the basic feature set for Chunking, we followed CITATION, which is the same feature set that provided the best result in CoNLL-2000.,,
6 Related Work Various loss functions have been proposed for designing CRFs (CITATION; CITATION).,,
With Chunking, CITATION reported the best F-score of 93.91 with the voting of several models trained by Support Vector Machine in the same experimental settings and with the same feature set.,,
However, MCE-F showed the better performance of 85.29 compared with CITATION of 84.04, which used the MAP training,,
1 Introduction Conditional random fields (CRFs) are a recently introduced formalism CITATION for representing a conditional model p(y|x), where both a set of inputs, x, and a set of outputs, y, display non-trivial interdependency.,,
The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing CITATION and information extraction (McCallum and Li,,
12, by using the variant of the forward-backward and Viterbi algorithm described in CITATION.,,
Moreover, for the parameter optimization process, we can simply exploit gradient descent or quasi-Newton methods such as L-BFGS CITATION as well as ML/MAP optimization.,,
Therefore, we can find the maximum incorrect output by using the A* algorithm CITATION, if the maximum output is the correct output, and by using the Viterbi algorithm otherwise.,,
The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing CITATION and information extraction CITATION.,,
The first approach to estimating CRF parameters is the maximum likelihood (ML) criterion over conditional probability p(y|x) itself CITATION.,,
The maximum a posteriori (MAP) criterion over parameters, , given x and y is the natural choice for reducing over-fitting CITATION.,,
With Chunking, CITATION reported the best F-score of 93.91 with the voting of several models trained by Support Vector Machine in the same experimental settings and with the same feature set.,,
However, MCE-F showed the better performance of 85.29 compared with CITATION of 84.04, which used the MAP training of CRFs with a feature selection architecture, yielding similar results to the MAP results described here.,,
econd-order methods such as QuickProp CITATION and BFGS-based methods have yielded good experimental optimization results.,,
Several non-linear objective functions, such as F-score for text classification CITATION, and BLEU-score and some other evaluation measures for statistical machine translation CITATION, have been introduced with reference to the framework of MCE criterion training.,,
The first approach to estimating CRF parameters is the maximum likelihood (ML) criterion over conditional probability p(y|x) itself CITATION.,,
The maximum a posteriori (MAP) criterion over parameters, , given x and y is the natural choice for reducing over-fitting CITATION.,,
Moreover, the Bayes approach, which optimizes both MAP and the prior distribution of the parameters, has also been proposed CITATION.,,
Furthermore, large margin criteria have been employed to optimize the model parameters (CITATION; CITATION).,,
These tasks are generally treated as sequential labeling problems incorporating the IOB tagging scheme CITATION.,,
4.2 Segmentation F-score Loss for SSTs The standard evaluation measure of SSTs is the segmentation F-score CITATION: F = (2 + 1) TP 2 FN + FP + (2 + 1) TP (13) 220 \x0cHe reckons the current account deficit will narrow to only # 1.8 billion .,,
These tasks are generally treated as sequential labeling problems incorporating the IOB tagging scheme CITATION.,,
4.2 Segmentation F-score Loss for SSTs The standard evaluation measure of SSTs is the segmentation F-score CITATION: F = (2 + 1) TP 2 FN + FP + (2 + 1) TP (13) 220 \x0cHe reckons the current account deficit will narrow to only # 1.8 billion .,,
3 with a variant of the forwardbackward algorithm CITATION.,,
5 Experiments We used the same Chunking and English NER task data used for the shared tasks of CoNLL2000 CITATION and CoNLL2003 (Sang and De Meulder, 2003), respectively.,,
The second summation of TPl and FNl performs a summation over correct segments s. In contrast, the second summation in FPl takes all possible segments into account, but excludes the correct segments s. Although an efficient way to evaluate all possible segments has been proposed in the context of semi-Markov CRFs CITATION, we introduce a simple alternative method.,,
(CRFs) are a recently introduced formalism CITATION for representing a conditional model p(y|x), where both a set of inputs, x, and a set of outputs, y, display non-trivial interdependency.,,
The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing CITATION and information extraction CITATION.,,
The first approach to estimating CRF parameters is the maximum likelihood (ML) criterion over conditional probability p(y|x) itself CITATION.,,
Following the definitions of CITATION, a log-linear combination of weighted features, c(y, x; ) = exp( fc(y, x)), is used as individual potential functions, where fc represents a feature vector obtained from the corresponding clique c. That is, Q cC(y,x) c(y, x) = exp(F(y, x)), where F(y, x)= P c fc(y, x) is the CRFs global feature vector for x and y.,,
The details of actual optimization procedures for linear chain CRFs, which are typical CRF applications, have already been reported CITATION.,,
3 MCE Criterion Training for CRFs The Minimum Classification Error (MCE) framework first arose out of a broader family of approaches to pattern classifier design known as Generalized Probabilistic Descent (GPD) CITATION.,,
12, by using the variant of the forward-backward and Viterbi algorithm described in CITATION.,,
Moreover, for the parameter optimization process, we can simply exploit gradient descent or quasi-Newton methods such as L-BFGS CITATION as well as ML/MAP optimization.,,
Therefore, we can find the maximum incorrect output by using the A* algorithm CITATION, if the maximum output is the correct output, and by using the Viterbi algorithm otherwise,,
3 with a variant of the forwardbackward algorithm CITATION.,,
5 Experiments We used the same Chunking and English NER task data used for the shared tasks of CoNLL2000 CITATION and CoNLL2003 (Sang and De Meulder, 2003), respectively.,,
onditional probability p(y|x) itself CITATION.,,
The maximum a posteriori (MAP) criterion over parameters, , given x and y is the natural choice for reducing over-fitting CITATION.,,
Moreover, the Bayes approach, which optimizes both MAP and the prior distribution of the parameters, has also been proposed CITATION.,,
Furthermore, large margin criteria have been employed to optimize the model parameters (CITATION; CITATION).,,
y p(y|x) itself CITATION.,,
The maximum a posteriori (MAP) criterion over parameters, , given x and y is the natural choice for reducing over-fitting CITATION.,,
Moreover, the Bayes approach, which optimizes both MAP and the prior distribution of the parameters, has also been proposed CITATION.,,
Furthermore, large margin criteria have been employed to optimize the model parameters (CITATION; CITATION).,,
