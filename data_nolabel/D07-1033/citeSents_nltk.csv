Used surface features are the word (w), the downcased word (wl), the POS tag (pos), the chunk tag (chk), the prefix of the word of length n (pn), the suffix (sn), the word form features: 2d - cp (these are based on CITATION), and the gazetteer features: go for ORG, gp for PER, and gm for MISC.,,
For example, non-local features such as same phrases in a document do not have different entity classes were shown to be useful in named entity recognition (CITATION; CITATION; CITATION; CITATION).,,
Although several methods have already been proposed to incorporate non-local features (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), these present a problem that the types of non-local features are somewhat constrained.,,
For example, CITATION enabled the use of non-local features by using Gibbs sampling.,,
We implemented scaling, which is similar to that for HMMs (see such as CITATION), in the forward-backward phase of CRF training to deal with very long sequences due to sentence concatenation.8 We used Gaussian regularization CITATION for CRF training to avoid overfitting.,,
This resembles the re-ranking approach (CITATION; CITATIONb).,,
6.3 Comparison with re-ranking approach Finally, we compared our algorithm with the reranking approach (CITATION; CITATIONb), where we first generate the n-best candidates using a model with only local features (the first model) and then re-rank the candidates using a model with non-local features (the second model).,,
re-ranking 1 uses the score of the first model as a feature in addition to the non-local features as in CITATIONb).,,
With non-local features, we cannot use efficient procedures such as forward-backward procedures and the Viterbi algorithm that are required in training CRFs CITATION and perceptrons (CITATIONa).,,
Recently, several methods (CITATION; Daume III and Marcu, 2005; CITATION) have been proposed with similar motivation to ours.,,
In this paper, we follow this line of research and try to solve the problem by extending Collins perceptron algorithm (CITATIONa).,,
7 Discussion As we mentioned, there are some algorithms similar to ours (CITATION; Daume III and Marcu, 2005; CITATION; CITATION).,,
posed by CITATION is also similar to ours.,,
CITATION used n candidates of a beam search in the Collins perceptron algorithm for machine translation.,,
CITATION proposed an approximate incremental method for parsing.,,
To achieve robust training, Daume III and Marcu (2005) employed the averaged perceptron (CITATIONa) and ALMA CITATION.,,
CITATION used the averaged perceptron (CITATIONa).,,
CITATION used MIRA CITATION.,,
On the other hand, we employed the margin perceptron CITATION, extending it to sequence labeling.,,
Discriminative methods such as Conditional Random Fields (CRFs) CITATION, Semi-Markov Random Fields CITATION, and perceptrons (CITATIONa) have been popular approaches for sequence labeling because of their excellent performance, which is mainly due to their ability to incorporate many kinds of overlapping and non-independent features.,,
With non-local features, we cannot use efficient procedures such as forward-backward procedures and the Viterbi algorithm that are required in training CRFs CITATION and perceptrons (CITATIONa).,,
Recently, several methods (CITATION; Daume III and Marcu, 2005; CITATION) have been proposed with similar motivation to ours.,,
In this paper, we follow this line of research and try to solve the problem by extending Collins perceptron algorithm (CITATIONa).,,
2 Perceptron Algorithm for Sequence Labeling CITATIONa) proposed an extension of the perceptron algorithm CITATION to sequence labeling.,,
This locality constraint is also required to make the training of CRFs tractable CITATION.,,
Thus, CITATIONa) also proposed an averaged perceptron, where the final weight vector is 1 CITATIONa) also provided proof that guaranteed good learning for the non-separable case.,,
Based on the proofs in CITATIONa) and CITATION, we can prove that the algorithm converges within (2C + R2)/2 updates and that () C/(2C + R2) = (/2)(1 (R2/(2C + R2))) after training.,,
Note that if the features are all local, the secondbest candidate (generally n-best candidates) can also be found efficiently by using an A* search that uses the best scores calculated during a Viterbi search as the heuristic estimation CITATION.,,
2 (Daume III and Marcu, 2005) also presents the method using the averaged perceptron (CITATIONa) 3 For re-ranking problems, CITATION proposed a perceptron algorithm that also uses margins.,,
This resembles the re-ranking approach (CITATION; CITATIONb).,,
eptron with local and non-local features (parameters: n, Ca , Cl ) 0 until no more updates do for i 1 to L do 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; : {yn } = n-bestyl (xi, y) y = argmaxy{yn}a (xi, y) y = 2nd-besty{yn}a (xi, y) if y = y i & a (xi, y i ) a (xi, y ) Ca then = + a (xi, y i ) a (xi, y ) (A) else if a (xi, y i ) a (xi, y ) Ca then = + a (xi, y i ) a (xi, y ) (A) else (B) 8 &gt; &gt; &lt; &gt; &gt; : if y1 = yi then (y1 represents the best in {yn }) = + l (xi, y i ) l (xi, y1 ) else if l (xi, y i ) l (xi, y2 ) Cl then = + l (xi, y i ) l (xi, y2 ) proofs in CITATIONa), we can see that the essential condition for convergence is that the weights are always updated using some y (= y) that satisfies: (xi, y i ) (xi, y) 0 ( C in the case of a perceptron with a margin).,,
6.3 Comparison with re-ranking approach Finally, we compared our algorithm with the reranking approach (CITATION; CITATIONb), where we first generate the n-best candidates using a model with only local features (the first model) and then re-rank the candidates using a model with non-local features (the second model).,,
re-ranking 1 uses the score of the first model as a feature in addition to the non-local features as in CITATIONb).,,
To avoid this problem, we adopt cross-validation training as used in CITATIONb).,,
CITATION used n candidates of a beam search in the Collins perceptron algorithm for machine translation.,,
CITATION proposed an approximate incremental method for parsing.,,
To achieve robust training, Daume III and Marcu (2005) employed the averaged perceptron (CITATIONa) and ALMA CITATION.,,
CITATION used the averaged perceptron (CITATIONa).,,
CITATION used MIRA CITATION.,,
On the other hand, we employed the margin perceptron CITATION, extending it to sequence labeling.,,
With regard to the local update, (B), in Algorithm 4.2, early updates CITATION and y-good requirement in (Daume III and Marcu, 2005) resemble our local update in that they tried to avoid the situation where the correct answer cannot be output.,,
Note that if the features are all local, the secondbest candidate (generally n-best candidates) can also be found efficiently by using an A* search that uses the best scores calculated during a Viterbi search as the heuristic estimation CITATION.,,
Such methods include ALMA CITATION used in (Daume III and Marcu, 2005)2, MIRA CITATION used in CITATION, and Max-Margin Markov Networks CITATION.,,
However, to the best of our knowledge, there has been no prior work that has applied a perceptron with a margin CITATION to structured output.3 Our method described in this section is one of the easiest to implement, while guaranteeing a large margin.,,
 CITATION used n candidates of a beam search in the Collins perceptron algorithm for machine translation.,,
CITATION proposed an approximate incremental method for parsing.,,
To achieve robust training, Daume III and Marcu (2005) employed the averaged perceptron (CITATIONa) and ALMA CITATION.,,
CITATION used the averaged perceptron (CITATIONa).,,
CITATION used MIRA CITATION.,,
On the other hand, we employed the margin perceptron CITATION, extending it to sequence labeling.,,
With regard to the local update, (B), in Algorithm 4.2, early updates CITATION and y-good requirement in (Daume III and Marcu, 2005) resemble our local update in that they tried to avoid the situation where the correct answer cannot be output.,,
For example, non-local features such as same phrases in a document do not have different entity classes were shown to be useful in named entity recognition (CITATION; CITATION; CITATION; CITATION).,,
Although several methods have already been proposed to incorporate non-local features (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), these present a problem that the types of non-local features are somewhat constrained.,,
For example, CITATION enabled the use of non-local features by using Gibbs sampling.,,
We adopted IOB (IOB2) labeling CITATION, where the first word of an entity of class C is labeled B-C, the words in the entity are labeled I-C, and other words are labeled O.,,
We used non-local features based on CITATION.,,
We also implemented the majority version of these features as used in CITATION.,,
The performance of the related work (CITATION; CITATION) is listed in Table 4.,,
Method dev test CITATION CITATION baseline CRF - 85.51 + non-local features - 86.86 CITATION CITATION baseline CRF - 85.29 + non-local features - 87.24 Table 5: Summary of performance with POS/chunk tags by TagChunk.,,
The resulting performance of the proposed algorithm with non-local features is higher than that of CITATION and comparable with that of CITATION.,,
However, the achieved accuracy was not better than that of related work (CITATION; CITATION) based on CRFs.,,
Note that if the features are all local, the secondbest candidate (generally n-best candidates) can also be found efficiently by using an A* search that uses the best scores calculated during a Viterbi search as the heuristic estimation CITATION.,,
Such methods include ALMA CITATION used in (Daume III and Marcu, 2005)2, MIRA CITATION used in CITATION, and Max-Margin Markov Networks CITATION.,,
However, to the best of our knowledge, there has been no prior work that has applied a perceptron with a margin CITATION to structured output.3 Our method described in this section is one of the easiest to implement, while guaranteeing a large margin.,,
CITATION used n candidates of a beam search in the Collins perceptron algorithm for machine translation.,,
CITATION proposed an approximate incremental method for parsing.,,
To achieve robust training, Daume III and Marcu (2005) employed the averaged perceptron (CITATIONa) and ALMA CITATION.,,
CITATION used the averaged perceptron (CITATIONa).,,
CITATION used MIRA CITATION.,,
On the other hand, we employed the margin perceptron CITATION, extending it to sequence labeling.,,
With regard to the local update, (B), in Algorithm 4.2, early updates CITATION and y-good requirement in (Daume III and Marcu, 2005) resemble our local update in that they tried to avoid the situation where the correct answer cannot be output.,,
We also incorporated the idea behind Bayes point machines (BPMs) CITATION to improve the robustness of our method further.,,
3 Margin Perceptron Algorithm for Sequence Labeling We extended a perceptron with a margin CITATION to sequence labeling in this study, as CITATIONa) extended the perceptron algorithm to sequence labeling.,,
-best candidates) can also be found efficiently by using an A* search that uses the best scores calculated during a Viterbi search as the heuristic estimation CITATION.,,
Such methods include ALMA CITATION used in (Daume III and Marcu, 2005)2, MIRA CITATION used in CITATION, and Max-Margin Markov Networks CITATION.,,
However, to the best of our knowledge, there has been no prior work that has applied a perceptron with a margin CITATION to structured output.3 Our method described in this section is one of the easiest to implement, while guaranteeing a large margin.,,
CITATION proposed an approximate incremental method for parsing.,,
To achieve robust training, Daume III and Marcu (2005) employed the averaged perceptron (CITATIONa) and ALMA CITATION.,,
CITATION used the averaged perceptron (CITATIONa).,,
CITATION used MIRA CITATION.,,
On the other hand, we employed the margin perceptron CITATION, extending it to sequence labeling.,,
With regard to the local update, (B), in Algorithm 4.2, early updates CITATION and y-good requirement in (Daume III and Marcu, 2005) resemble our local update in that they tried to avoid the situation where the correct answer cannot be output.,,
For example, non-local features such as same phrases in a document do not have different entity classes were shown to be useful in named entity recognition (CITATION; CITATION; CITATION; CITATION).,,
Although several methods have already been proposed to incorporate non-local features (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), these present a problem that the types of non-local features are somewhat constrained.,,
For example, CITATION enabled the use of non-local features by using Gibbs sampling.,,
We adopted IOB (IOB2) labeling CITATION, where the first word of an entity of class C is labeled B-C, the words in the entity are labeled I-C, and other words are labeled O.,,
We used non-local features based on CITATION.,,
We also implemented the majority version of these features as used in CITATION.,,
This type of non-local feature was not used by CITATION or CITATION.,,
The performance of the related work (CITATION; CITATION) is listed in Table 4.,,
Method dev test CITATION CITATION baseline CRF - 85.51 + non-local features - 86.86 CITATION CITATION baseline CRF - 85.29 + non-local features - 87.24 Table 5: Summary of performance with POS/chunk tags by TagChunk.,,
The resulting performance of the proposed algorithm with non-local features is higher than that of CITATION and comparable with that of CITATION.,,
However, the achieved accuracy was not better than that of related work (CITATION; CITATION) based on CRFs.,,
Discriminative methods such as Conditional Random Fields (CRFs) CITATION, Semi-Markov Random Fields CITATION, and perceptrons (CITATIONa) have been popular approaches for sequence labeling because of their excellent performance, which is mainly due to their ability to incorporate many kinds of overlapping and non-independent features.,,
With non-local features, we cannot use efficient procedures such as forward-backward procedures and the Viterbi algorithm that are required in training CRFs CITATION and perceptrons (CITATIONa).,,
Recently, several methods (CITATION; Daume III and Marcu, 2005; CITATION) have been proposed with similar motivation to ours.,,
In this paper, we follow this line of research and try to solve the problem by extending Collins perceptron algorithm (CITATIONa).,,
This locality constraint is also required to make the training of CRFs tractable CITATION.,,
Thus, CITATIONa) also proposed an averaged perceptron, where the final weight vector is 1 CITATIONa) also provided proof that guaranteed good learning for the non-separable case.,,
Based on the proofs in CITATIONa) and CITATION, we can prove that the algorithm converges within (2C + R2)/2 updates and that () C/(2C + R2) = (/2)(1 (R2/(2C + R2))) after training.,,
Note that if the features are all local, the secondbest candidate (generally n-best candidates) can also be found efficiently by using an A* search that uses the best scores calculated during a Viterbi search as the heuristic estimation CITATION.,,
7 Discussion As we mentioned, there are some algorithms similar to ours (CITATION; Daume III and Marcu, 2005; CITATION; CITATION).,,
The algorithm proposed by CITATION is also similar to ours.,,
CITATION used n candidates of a beam search in the Collins perceptron algorithm for machine translation.,,
CITATION proposed an approximate incremental method for parsing.,,
To achieve robust training, Daume III and Marcu (2005) employed the averaged perceptron (CITATIONa) and ALMA CITATION.,,
CITATION used the averaged perceptron (CITATIONa).,,
CITATION used MIRA (CITATION,,
With non-local features, we cannot use efficient procedures such as forward-backward procedures and the Viterbi algorithm that are required in training CRFs CITATION and perceptrons (CITATIONa).,,
Recently, several methods (CITATION; Daume III and Marcu, 2005; CITATION) have been proposed with similar motivation to ours.,,
In this paper, we follow this line of research and try to solve the problem by extending Collins perceptron algorithm (CITATIONa).,,
7 Discussion As we mentioned, there are some algorithms similar to ours (CITATION; Daume III and Marcu, 2005; CITATION; CITATION).,,
CITATION used n candidates of a beam search in the Collins perceptron algorithm for machine translation.,,
CITATION proposed an approximate incremental method for parsing.,,
To achieve robust training, Daume III and Marcu (2005) employed the averaged perceptron (CITATIONa) and ALMA CITATION.,,
CITATION used the averaged perceptron (CITATIONa).,,
CITATION used MIRA CITATION.,,
On the other hand, we employed the margin perceptron CITATION, extending it to sequence labeling.,,
With regard to the local update, (B), in Algorithm 4.2, early updates CITATION and y-good requirement in (Daume III and Marcu, 2005) resemble our local update in that they tried to avoid the situation where the correct answer cannot be output.,,
Note that if the features are all local, the secondbest candidate (generally n-best candidates) can also be found efficiently by using an A* search that uses the best scores calculated during a Viterbi search as the heuristic estimation CITATION.,,
Such methods include ALMA CITATION used in (Daume III and Marcu, 2005)2, MIRA CITATION used in CITATION, and Max-Margin Markov Networks CITATION.,,
However, to the best of our knowledge, there has been no prior work that has applied a perceptron with a margin CITATION to structured output.3 Our method described in this section is one of the easiest to implement, while guaranteeing a large margin.,,
For example, non-local features such as same phrases in a document do not have different entity classes were shown to be useful in named entity recognition (CITATION; CITATION; CITATION; CITATION).,,
Although several methods have already been proposed to incorporate non-local features (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), these present a problem that the types of non-local features are somewhat constrained.,,
For example, CITATION enabled the use of non-local features by using Gibbs sampling.,,
CITATION divided th,,
For example, CITATION enabled the use of non-local features by using Gibbs sampling.,,
CITATION divided the model into two CRFs, where the second model uses the output of the first as a kind of non-local information.,,
Nakagawa and Matsumoto CITATION used a Bolzmann distribution to model the correlation of the POS of words having the same lexical form in a document.,,
We adopted IOB (IOB2) labeling CITATION, where the first word of an entity of class C is labeled B-C, the words in the entity are labeled I-C, and other words are labeled O.,,
We used non-local features based on CITATION.,,
We also implemented the majority version of these features as used in CITATION.,,
This type of non-local feature was not used by CITATION or CITATION.,,
The resulting performance of the proposed algorithm with non-local features is higher than that of CITATION and comparable with that of CITATION.,,
The algorithm proposed by CITATION is also similar to ours.,,
CITATION used n candidates of a beam search in the Collins perceptron algorithm for machine translation.,,
CITATION proposed an,,
We implemented scaling, which is similar to that for HMMs (see such as CITATION), in the forward-backward phase of CRF training to deal with very long sequences due to sentence concatenation.8 We used Gaussian regularization CITATION for CRF training to avoid overfitting.,,
We adopted IOB (IOB2) labeling CITATION, where the first word of an entity of class C is labeled B-C, the words in the entity are labeled I-C, and other words are labeled O.,,
We used non-local features based on CITATION.,,
We also implemented the majority version of these features as used in CITATION.,,
2 Perceptron Algorithm for Sequence Labeling CITATIONa) proposed an extension of the perceptron algorithm CITATION to sequence labeling.,,
The learning algorithm, which is illustrated in CITATIONa), proceeds as f,,
For example, non-local features such as same phrases in a document do not have different entity classes were shown to be useful in named entity recognition (CITATION; CITATION; CITATION; CITATION).,,
Although several methods have already been proposed to incorporate non-local features (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), these present a problem that the types of non-local features are somewhat constrained.,,
For example, CITATION enabled the use of non-local features by using Gibbs sampling.,,
Discriminative methods such as Conditional Random Fields (CRFs) CITATION, Semi-Markov Random Fields CITATION, and perceptrons (CITATIONa) have been popular approaches for sequence labeling because of their excellent performance, which is mainly due to their ability to incorporate many kinds of overlapping and non-independent features.,,
2 (Daume III and Marcu, 2005) also presents the method using the averaged perceptron (CITATIONa) 3 For re-ranking problems, CITATION proposed a perceptron algorithm that also uses margins.,,
Based on the proofs in CITATIONa) and CITATION, we can prove that the algorithm converges within (2C + R2)/2 updates and that () C/(2C + R2) = (/2)(1 (R2/(2C + R2))) after training.,,
Note that if the features are all local, the secondbest candidate (generally n-best candidates) can also be found efficiently by using an A* search that uses the best scores calculated during a Viterbi search as the heuristic estimation CITATION.,,
Such methods include ALMA CITATION used in (Daume III and Marcu, 2005)2, MIRA CITATION used in CITATION, and Max-Margin Markov Networks CITATION.,,
However, to the best of our knowledge, there has been no prior work that has applied a perceptron with a margin CITATION to structured output.3 Our method described in this section is one of the easiest to implement, while guaranteeing a large margin.,,
For example, non-local features such as same phrases in a document do not have different entity classes were shown to be useful in named entity recognition (CITATION; CITATION; CITATION; CITATION).,,
Although several methods have already been proposed to incorporate non-local features (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION), these present a problem that the types of non-local features are somewhat constrained.,,
For example, CITATION enabled the use of non-local features by using Gib,,
Note that if the features are all local, the secondbest candidate (generally n-best candidates) can also be found efficiently by using an A* search that uses the best scores calculated during a Viterbi search as the heuristic estimation CITATION.,,
Such methods include ALMA CITATION used in (Daume III and Marcu, 2005)2, MIRA CITATION used in CITATION, and Max-Margin Markov Networks CITATION.,,
However, to the best of our knowledge, there has been no prior work that has applied a perceptron with a margin CITATION to structured output.3 Our method described in this section is one of the easiest to implement, while guaranteeing a large margin.,,
