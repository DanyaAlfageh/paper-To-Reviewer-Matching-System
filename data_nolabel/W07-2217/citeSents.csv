Approaches to dependency parsing either generate such trees by considering all possible spanning trees CITATION, or build a single tree by means of shift-reduce parsing actions CITATION,,
 Dependency treebanks are becoming available in many languages, and several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks (CITATION; CITATION),,
The perceptron has been used in previous work on dependency parsing by CITATION, with a parser based on Eisners algorithm CITATION, and also on incremental constituent parsing (Collins & Roark, 2006),,
To regularize the model we take as the final model the average of all weight vectors posited during training CITATION,,
3 A shift-reduce parser We build upon DeSR, the shift-reduce parser described in CITATION,,
s motivated by the simplicity and performance of perceptrons, which have proved competitive on a number of tasks; e.g., in shallow parsing, where perceptrons performance is comparable to that of Conditional Random Field models CITATION,,
h trees by considering all possible spanning trees CITATION, or build a single tree by means of shift-reduce parsing actions CITATION,,
CITATION have shown that incorporating second order features relating to adjacent edge pairs impr,,
Also the MST parser of McDonald uses a variant of the perceptron algorithm CITATION,,
CITATION investigated the issue of (strict) incrementality for this type of parsers; i.e., if at any point of the analysis the processed input forms one connected structure,,
In statistical syntactic parsing a generator (e.g., a PCFG) is used to produce a number of candidate trees CITATION with associated probability scores,,
Overall the accuracy of the DeSR parser with semantic information is slightly inferior to that of the second-order MST parser CITATION (91.5% UAS),,
CITATION proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear,,
Approaches to dependency parsing either generate such trees by considering all possible spanning trees CITATION, or build a single ,,
Our base models accuracy (90.55% UAS) compares well with the accuracy of the parsers based on the polynomial kernel trained with SVM of Yamada and Matsumoto (UAS 90.3%), and CITATION (UAS 89.4%),,
s the final model the average of all weight vectors posited during training CITATION,,
CITATION showed that learning an SVM model in the dual ,,
CITATION showed that learning an SVM model in the dual space with higher-degree polynomial kernel functions improves significantly the parsers accuracy,,
Collins and Koo CITATION introduced an improved reranking model for parsing which includes a hidden layer of semantic features,,
Approaches to dependency parsing either generate such trees by considering all possible spanning trees CITATION, or build a single tree by means of shift-reduce parsing actions ,,
 multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks (CITATION; CITATION),,
MIRA CITATION) could provide further gains in accuracy, as shown with the MST parser CITATION,,
rministic dependency parsers which run in linear time have also been developed (CITATION; CITATION),,
The constituent trees were transformed into dependency trees by means of a program created by Joakim Nivre that implements the rules proposed by Yamada and Matsumoto, which in turn are based on the head rules of Collins parser CITATION5,,
CITATION) have been introduced for handling non-projective dependency trees: i.e., trees that cannot be drawn in the plane without crossing edges,,
Algorithm 2: Average multiclass perceptron input : S = (xi, yi)N ; 0 k = ~ 0, k Y for t = 1 to T do choose j Et = {r Y : hxj, t ri hxj, t yj i} if |Et |&gt; 0 then t+1 r = t r xj |Et |, r Et t+1 yj = t yj + xj output: k = 1 T P t t k, k Y 3.4 Higher-order feature spaces CITATION and CITATION have shown that higher-order feature representations and modeling can improve parsing accuracy, although at significant computational costs,,
We learn the parameters from the training data with the perceptron CITATION, in the online multiclass formulation of the algorithm CITATION with uniform negative updates,,
We notice in particular that, given the lack of nonprojective cases/rules, the parser of CITATION is almost identical to our parser, hence the difference in accuracy (+1.1%) might effectively be due to a better classifier,,
Algorithm 2: Average multiclass perceptron input : S = (xi, yi)N ; 0 k = ~ 0, k Y for t = 1 to T do choose j Et = {r Y : hxj, t ri hxj, t yj i} if |Et |&gt; 0 then t+1 r = t r xj |Et |, r Et t+1 yj = t yj + xj output: k = 1 T P t t k, k Y 3.4 Higher-order feature spaces CITATION and CITATION have shown that higher-order feature representations and modeli,,
Similar deterministic approaches to parsing have been investigated also in the context of constituent parsing (CITATION; CITATION),,
This approach has been used also for dependency parsing, generating spanning trees as candidates and computing the maximum spanning tree (MST) using discriminative learning algorithms CITATION,,
We briefly describe the tagger (see CITATION for more details), a Hidden Markov Model trained with the perceptron algorithm introduced in CITATION,,
tion extraction CITATION and machine translation CITATION,,
Yi and Palmer CITATION retrained a constituent parser in which phrases were annotated with argument information to improve SRL, however this didnt improve over the output of the basic parser,,
CITATION proposed a deterministic classifierbased parser,,
The choice is motivated by the simplicity and performance of perceptrons, which have proved competitive on a number of tasks; e.g., in shallow parsing, where perceptrons performance is comparable to that of Conditional Random Field models CITATION,,
CITATION have shown that incorporating second order features relating to adjacent edge pairs improves the accuracy of maximum spanning tree parsers (MST),,
The lemma for each token was produced using the morph function of the WordNet CITATION library6,,
 Conditional Random Field models CITATION,,
5 Parsing experiments 5.1 Data and setup We used the standard partitions of the Wall Street Journal Penn Treebank CITATION; i.e., sections 2-21 for training, section 22 for development and section 23 for evaluation,,
Rather, Yamada and Matsumoto (see also CITATION) partition the training data in different sets, on the basis of Partof-Speech, then train one dual SVM model per set,,
CITATION proposed a variant of the rules that handle non-projective relations while parsing deterministically in a single pass,,
As originally pointed out by CITATION, there are problems which require non-linear solutions that cannot be learned by such models,,
As a comparison, CITATION reports 1.5 hours for training the partitioned SVM model and 10 minutes for parsing the evaluation set on the same Penn Treebank data,,
The magnitude of the improvement is remarkable and reflects the 4.6% improvement that Yamada and Matsumoto CITATION report going from the linear SVM to the polynomial of degree two,,
Similar deterministic approaches to parsing have been investigated also in the context of constituent parsing (CITATION; Kalt, ,,
CITATION show that full parsing is effective for semantic role labeling (see also related approaches evaluated within the CoNNL 2005 shared task (Carreras et al., 2005)),,
The final average model can be computed efficiently during training without storing the individual vectors (e.g., see CITATION),,
For non-projective languages the algorithm is NP-hard and CITATION introduce an approximate algorithm to handle such cases,,
o regularize the model we take as the final model the average of all weight vectors posited during training CITATION,,
4.1 BBN Entity corpus The BBN corpus CITATION supplements the Wall Street Journal Penn Treebank with annotation of a large set of entity types,,
ve been evaluated in the CoNLL 2006 and 2007 shared tasks (CITATION; CITATION),,
Dependency trees capture grammatical structures that can be useful in several language processing tasks such as information extraction CITATION and machine translation CITATION,,
There is evidence that dependency and constituent parsing can be helpful in these and other tasks; e.g., by means of tree kernels in question classification and semantic role labeling (Zhang & Lee, 2003; CITATION),,
CITATION have shown that the degree two polynomial kernel has superior accuracy than the linear model and polynomial kernels of higher degrees,,
CITATION showed that learning an SVM model in the dual space with higher-degree polynomial kernel functions improves significantly the parsers accu,,
Dependency treebanks are becoming available in many languages, and several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks (CITATION; CITATION),,
Deterministic dependency parsers which run in linear time have also been developed (CITATION; CITATION),,
CITATION have shown that incorporating second order features relating to adjacent,,
Semantic features could be also easily included in other types of dependency parsing algorithms, e.g., MST, and in current methods for constituent parse reranking (CITATION; CITATION),,
