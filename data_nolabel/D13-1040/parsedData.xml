<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.847592">
b&apos;Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 415425,
Seattle, Washington, USA, 18-21 October 2013. c
</bodyText>
<figure confidence="0.6727825">
2013 Association for Computational Linguistics
Unsupervised Relation Extraction with General Domain Knowledge
Oier Lopez de Lacalle1,2
and Mirella Lapata1
1
Institute for Language, Cognition and Computation
</figure>
<affiliation confidence="0.747436">
School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB
</affiliation>
<page confidence="0.872934">
2
</page>
<title confidence="0.172878">
IKERBASQUE, Basque Foundation for Science, Bilbao, Spain
</title>
<email confidence="0.839387">
oier.lopezdelacalle@ehu.es, mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.983362" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993328833333333">
In this paper we present an unsupervised ap-
proach to relational information extraction.
Our model partitions tuples representing an
observed syntactic relationship between two
named entities (e.g., X was born in Y
and X is from Y) into clusters correspond-
ing to underlying semantic relation types
(e.g., BornIn, Located). Our approach incor-
porates general domain knowledge which we
encode as First Order Logic rules and auto-
matically combine with a topic model devel-
oped specifically for the relation extraction
task. Evaluation results on the ACE 2007
English Relation Detection and Categoriza-
tion (RDC) task show that our model outper-
forms competitive unsupervised approaches
by a wide margin and is able to produce clus-
ters shaped by both the data and the rules.
</bodyText>
<sectionHeader confidence="0.998301" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99962018">
Information extraction (IE) is becoming increas-
ingly useful as a form of shallow semantic analy-
sis. Learning relational facts from text is one of the
core tasks of IE and has applications in a variety
of fields including summarization, question answer-
ing, and information retrieval. Previous work (Sur-
deanu and Ciaramita, 2007; Culotta and Sorensen,
2004; Zhou et al., 2007) has traditionally relied on
extensive human involvement (e.g., hand-annotated
training instances, manual pattern extraction rules,
hand-picked seeds). Standard supervised techniques
can yield high performance when large amounts
of hand-labeled data are available for a fixed in-
ventory of relation types (e.g., Employment, Lo-
cated), however, extraction systems do not easily
generalize beyond their training domains and often
must be re-engineered for each application. Un-
supervised approaches offer a promising alternative
which could lead to significant resource savings and
more portable extraction systems.
It therefore comes as no surprise that latent topic
analysis methods have been used for a variety of
IE tasks. Yao et al. (2011), for example, propose
a series of topic models which perform relation
discovery by clustering tuples representing an ob-
served syntactic relationship between two named en-
tities (e.g., X was born in Y and X is from Y).
The clusters correspond to semantic relations whose
number or type is not known in advance. Their mod-
els depart from standard Latent Dirichlet Allocation
(Blei et al., 2003) in that a document consists of re-
lation tuples rather than individual words; moreover,
tuples have features each of which is generated in-
dependently from a hidden relation (e.g., the words
corresponding to the first and second entities, the
type and order of the named entities). Since these
features are local, they cannot capture more global
constraints pertaining to the relation extraction task.
Such constraints may take the form of restrictions
on which tuples should be clustered together or
not. For instance, different types of named entities
may be indicative of different relations (ORG-LOC
entities often express a Location relation whereas
PER-PER entities express Business or Family rela-
tions) and thus tuples bearing these entities should
not be grouped together. Another example are tuples
with identical or similar features which intuitively
should be clustered together.
In this paper, we propose an unsupervised ap-
proach to relation extraction which does not re-
</bodyText>
<page confidence="0.997519">
415
</page>
<bodyText confidence="0.97508709375">
\x0cquire any relation-specific training data and allows
to incorporate global constraints general express-
ing domain knowledge. We encode domain knowl-
edge as First Order Logic (FOL) rules and automati-
cally integrate them with a topic model to produce
clusters shaped by the data and the constraints at
hand. Specifically, we extend the Fold-all (First-
Order Logic latent Dirichlet Allocation) framework
(Andrzejewski et al., 2011) to the relation extraction
task, explain how to incorporate meaningful con-
straints, and develop a scalable inference technique.
In the presence of multiple candidate relation de-
compositions for a given corpus, domain knowledge
can steer the model towards relations which are best
aligned with user and task modeling goals. We also
argue that a general mechanism for encoding addi-
tional modeling assumptions and side information
can lessen the need for custom relation extraction
model variants. Experimental results on the ACE-
2007 Relation Detection and Categorization (RDC)
dataset show that our model outperforms competi-
tive unsupervised approaches by a wide margin and
is able to uncover meaningful relations with only
two general rule types.
Our contributions in this work are three-fold: a
new model that modifies the Fold-all framework and
extends it to the relation extraction task; a new for-
malization of the logic rules applicable to topic mod-
els defined over a rich set of features; and a proposal
for mining the logic rules automatically from a cor-
pus contrary to Andrzejewski et al. (2011) who em-
ploy manually crafted seeds.
</bodyText>
<sectionHeader confidence="0.999619" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999442725806452">
A variety of learning paradigms have been applied
to relation extraction. As mentioned earlier, super-
vised methods have been shown to perform well in
this task. The reliance on manual annotation, which
is expensive to produce and thus limited in quantity,
has provided the impetus for semi-supervised and
purely unsupervised approaches. Semi-supervised
methods use a small number of seed instances or
patterns (per relation) to launch an iterative train-
ing process (Riloff and Jones, 1999; Agichtein and
Gravano, 2000; Bunescu and Mooney, 2007; Pan-
tel and Pennacchiotti, 2006). The seeds are used
to extract a new set of patterns from a large cor-
pus, which are then used to extract more instances,
and so on. Unsupervised relation extraction meth-
ods are not limited to a predefined set of target
relations, but discover all types of relations found
in the text. The relations represent clusters over
strings of words (Banko et al., 2007; Hasegawa et
al., 2004), syntactic patterns between entities (Yao
et al., 2011; Shinyama and Sekine, 2006), or logical
expressions (Poon and Domingos, 2009). Another
learning paradigm is distant supervision which does
not require labeled data but instead access to a rela-
tional database such as Freebase (Mintz et al., 2009).
The idea is to take entities that appear in some rela-
tion in the database, find the sentences that express
the relation in an unlabeled corpus, and use them to
train a relation classifier.
Our own work adds an additional approach into
the mix. We use a topic model to infer an arbi-
trary number of relations between named entities.
Although we do not have access to relation-specific
information (either as a relational database or manu-
ally annotated data), we impose task-specific con-
straints which inject domain knowledge into the
learning algorithm. We thus alleviate known prob-
lems with the interpretability of the clusters obtained
from topic models and are able to guide our model
towards reasonable relations. Andrzejewski et al.
(2011) show how to integrate First-Order Logic with
vanilla LDA. We extend their formulation to relation
tuples rather than individual words. Our model gen-
erates a corpus of entity tuples which are in turn rep-
resented by features and uses automatically acquired
FOL rules. The idea of integrating topic modeling
with FOL builds on research in probabilistic logic
modeling such as Markov Logic Networks (Richard-
son and Domingos, 2006). Schoenmackers et al.
(2010) learn Horn clauses from web-scale text with
aim of finding answers to a users query. Our work
is complementary to theirs. We could make use of
their rules to discover more accurate relations.
The general goal of assisting the learner in re-
covering the correct clustering by supplying ad-
ditional domain knowledge is not new. Gondek and
Hofmann (2004) supply a known clustering they do
not want the learner to return, whereas Wagstaff
et al. (2001) use pairwise labels for items indicat-
ing whether they belong in the same cluster. These
methods combine domain knowledge with statistical
learning in order to improve performance with re-
</bodyText>
<page confidence="0.99894">
416
</page>
<bodyText confidence="0.996599">
\x0cspect to the true target clustering. Although, the tar-
get labels are not available in our case, we are able to
show that the inclusion of domain knowledge yields
clustering improvements.
</bodyText>
<sectionHeader confidence="0.972815" genericHeader="method">
3 Learning Setting
</sectionHeader>
<bodyText confidence="0.999049791666667">
Our relation extraction task broadly adheres to the
ACE specification guidelines. Our aim is to detect
and characterize the semantic relations between
two named entities. The input to our model is a
corpus of documents, where each document is a
bag of relation tuples which can be obtained from
the output of any dependency parser. Each tuple
represents a syntactic relationship between two
named entity (NE) mentions, and consists of three
components: the dependency path between the
two mentions, the source NE, and the target NE. A
dependency path is the concatenation of dependency
edges and nodes along a path in the dependency
tree. For example, the sentence George Bush
traveled to France on Thursday for a summit.
would yield the tuple [SOURCE:George Bush(PER),
PATH:nsubjtraveledpreptopobj,
DES:France(LOC)]. The tuple here expresses the
relation Located, however our model does not
observe any relation labels during training. The
model assigns tuples to clusters, corresponding to
an underlying relation type. Each tuple instance can
be then labeled with an identifier corresponding to
the cluster (aka relation) it has been assigned to.
</bodyText>
<sectionHeader confidence="0.995939" genericHeader="method">
4 Modeling Framework
</sectionHeader>
<bodyText confidence="0.997305714285714">
Our model builds on the work of Yao et al. (2011)
who develop a series of generative probabilistic
models for relation extraction. Specifically, we ex-
tend their relational LDA model by interfacing it
with FOL-rules. In the following, we first describe
their approach in more detail and then present our
extensions and modifications.
</bodyText>
<subsectionHeader confidence="0.972305">
4.1 Relational LDA
</subsectionHeader>
<bodyText confidence="0.987621071428571">
Relational LDA is an extension to LDA with a sim-
ilar generative story. LDA models each document
as a mixture of topics, which are in turn character-
ized as distributions over words. In relational LDA,
each document is a mixture of relations over tuples
representing syntactic relations between two named
entities. The relation tuples are in turn generated a
by set of features drawn independently from the un-
derlying relation distribution.
More technically, a multinomial distribution over
relations di
is drawn from a Dirichlet prior
( Dir()) at the document level. Relation tuples
are generated from a multinomial distribution di
</bodyText>
<equation confidence="0.746649">
(zi|di
Mult(di
</equation>
<bodyText confidence="0.99907171875">
)) and are represented with k fea-
tures. Each feature is drawn (independently) from
a multinomial distribution selected by the relation
assigned to tuple i (fik|zi, zi Mult(zi )). Rela-
tions are drawn from a Dirichlet prior ( Dir()).
In other words, each tuple in a document is assigned
a hidden relation (z = z1...zN ); each relation is
represented by a multinomial distribution over fea-
tures r (Dirichlet prior ). r is a vector with F
dimensions each corresponding to a feature. Fi-
nally, documents (j = 1...D) are associated with a
multinomial distribution j over relations (Dirichlet
prior ). j is a vector with R dimensions, one for
each relation.
Figure 1 represents relational LDA model as a an
undirected graphical model or factor graph (Bishop,
2006), ignoring for the moment the factor which
connects the d, z, f1...k and o variables. Directed
graphical models can be converted into undirected
ones by adding edges between co-parents (Koller
and Friedman, 2009). Each clique in the graph de-
fines a potential function which replaces the condi-
tional probabilities in the directed graph. Each max-
imal clique is associated with a special factor node
(the black squares) and clique members are con-
nected to that factor. The probability of any specific
configuration is calculated by multiplying the poten-
tial functions and normalizing them. We adopt the
factor graph representation as is it convenient for in-
troducing logic rules into the model. The joint prob-
ability of the model given the priors and the docu-
ments (P(p, z, , |, , d)) is equivalent to:
</bodyText>
<equation confidence="0.9981298125">
R
Y
r
p(r|)
D
Y
j
p(j|)
N
Y
i
di
(zi)
Y
kpi
zi (fk) (1)
</equation>
<bodyText confidence="0.98585125">
where di
(zi) is the zi-th element in the vector di
and zi (fk) is fk-th feature in the zi vector. Vari-
able pi is the i-th tuple containing k features. The
parameters of the latent variables (e.g., , ) are
typically estimated using an approximate inference
algorithm such as Gibbs Sampling (Griffiths and
Steyvers, 2004).
</bodyText>
<page confidence="0.99938">
417
</page>
<figureCaption confidence="0.852631666666667">
\x0cFigure 1: Relational LDA as a factor graph. Filled
circles represent observed variables, empty circles are
associated with latent variables or model hyperparame-
</figureCaption>
<bodyText confidence="0.994814451612903">
ters, and plates indicate repeating structures. The black
squares are the factor nodes and are associated with the
potential functions corresponding to conditional indepen-
dence among the variables. The model observes D doc-
uments (d) consisting of N tuples (p), each represented
by a set of features f1,f2 . . . fk. z represents the relation
type assignment to a tuple, is the relation type propor-
tion for a given document, and the relation type dis-
tribution over the features. The logic factor (indicated
with the arrow) connects the KB with the relational LDA
model. Variable o is an observed variable which contains
the side information expressed in FOL.
As shown in Figure 1, the observed variables are
represented by filled circles. In our case, our model
sees the corpus (p, d), where d is the variable rep-
resenting the document and the tuples (p) are repre-
sented by a set of features f1,f2 . . . fk in the graph.
Empty circles are associated with latent variables to
be estimated: z represents the relation type assign-
ment to the tuple, is the relation type proportion
for the given document, and is the relation type
distribution over the features.
The features representing the tuples tap onto se-
mantic information expressed by different surface
forms and are an important part of the model. We
use a subset of the features proposed in Yao et al.
(2011) which we briefly describe below:
SOURCE This feature corresponds to the first en-
tity mention of the tuple. In the sentence George
Bush traveled to France on Thursday for a summit.,
the value of this feature would be George Bush.
</bodyText>
<equation confidence="0.894734">
Value Predicate Description
zi = r Z(i, r) Latent relation type
fk = v F(k, v) feature of relation tuple
pi = i P(i, fk) tuple i contains feature fk
di = j D(i, j) observed document
</equation>
<tableCaption confidence="0.746562">
Table 1: Logical variables for Relational LDA. The vari-
able i ranges over tuples in the corpus (i = [1 . . . N]),
</tableCaption>
<bodyText confidence="0.986886833333334">
and k over features in the corpus (k = [1 . . . F]).
DEST The feature corresponds to the second entity
mention and its value would be France in the previ-
ous example.
NEPAIR The feature indicates the type and order
of two entity mentions in the tuple. This would
be PER-ORG in our example.
PATH This feature refers to the dependency
path between two entity mentions. In our
sentence, the value of the feature would be
PATH:nsubjtraveledpreptopobj.
TRIGGER Finally, trigger features are content
words occurring in the dependency path. The path
PATH:nsubjtraveledpreptopobj con-
tains only one trigger word, namely traveled. The
intuition behind this feature is that paths sharing the
same set of trigger words should be grouped in the
same cluster.
</bodyText>
<subsectionHeader confidence="0.980854">
4.2 First Order Logic and Relational LDA
</subsectionHeader>
<bodyText confidence="0.995625636363636">
We next couple relational LDA with global con-
straints, which we express using FOL rules. We
begin by representing relational LDA as a Markov
Logic Network (Richardson and Domingos, 2006).
We define a logical predicate for each model vari-
able. For example, assigned relation variable
(Z(i, r)) is true if zi = r and false otherwise. Table 1
shows the mapping of model variables onto logical
predicates. Logical rules are encoded in the form of
a weighted FOL knowledge base (KB) which is then
converted into Conjunctive Normal form:
</bodyText>
<equation confidence="0.845557">
KB = {(1, 1), ..., (L, L)} (2)
</equation>
<bodyText confidence="0.98168375">
The KB consists of L pairs, where each l rep-
resents a FOL rule and l 0 its weight. Rules
are soft preferences rather than hard constraints;
the weights represent the importance of l and are
</bodyText>
<page confidence="0.995814">
418
</page>
<bodyText confidence="0.9757934375">
\x0cset manually by the domain expert. The KB is
tied to the probabilistic model via its groundings
in the corpus. For each FOL rule l, let G(l) be
the set of groundings, each mapping the free vari-
ables in l to a specific value. For example, in the
rule i, j, p : F(i, Obama) F(j, WhiteHouse)
P(p, i) P(p, j) Z(p, r)1, G consists of all the
rules where the free variables i, j and p are instanti-
ated. At grounding time, we parse the corpus search-
ing for the tuples that satisfy the logic rules and store
the indices of the tuples that ground the rule. The
stored indices are used to set l to a specific value.
For the (Obama, White House) example above, G
consists of F propositional rules for each observed
feature, where i [1 . . . F]. For each grounding
(g G(l)) we define an indicator function:
</bodyText>
<listItem confidence="0.9014866">
1g(z, p, d, o) =
1, if g is true under
z and p, d, o
0, otherwise
(3)
</listItem>
<bodyText confidence="0.997796411764706">
where z are relation assignments to tuples, p is the
set of features in tuples, d are documents, and o
the side information encoded in FOL. Contrary to
Andrzejewski et al. (2011), we need to ground the
rules while taking into account if the feature speci-
fied in the rule is expressed by any tuple or the spe-
cific given tuple, since we are assigning relations to
tuples, and not directly to words.
Next, we define a Markov Random Field (MRF)
which combines relational LDA with the FOL
knowledge base. The MRF is defined over latent
relation tuple assignments z, relation feature multi-
nomials , and relation document multinomials
(the feature set, document, and external informa-
tion o are observed). Under this model the con-
ditional probability P(z, , |, , p, d, o, KB) is
proportional to:
</bodyText>
<equation confidence="0.983764083333333">
exp
L
X
l
X
gG(l)
l1g(z, p, d, o)
R
Y
r
p(r|)
D
Y
j
p(j|)
N
Y
i
di
(zi)
Y
kpi
zi (fk)
(4)
</equation>
<bodyText confidence="0.989031">
The first term in Equation (4) corresponds to the
logic factor in Figure 1 that groups variables d, z,
</bodyText>
<page confidence="0.765901">
1
</page>
<bodyText confidence="0.9841404">
This rule translates as every tuple containing Obama and
White House as features should be in relation cluster r.
f1, f2, . . . fk and o. The remaining terms in Equa-
tion (4) refer to relational LDA. The goal of the
model is to estimate the most likely and for the
given observed state. As z can not be marginalized
out, we proceed with MAP estimation of (z, , ),
maximizing the log of the probability as in Andrze-
jewski et al. (2011):
arg max
</bodyText>
<equation confidence="0.98926675">
z,,
L
X
l
X
gG(l)
l1g(z, p, d, o)+
R
X
r
log p(r|)+
N
X
i
log di
(zi)
Y
kpi
zi (fk)
(5)
</equation>
<bodyText confidence="0.9661114">
Once the parameters of the model are estimated
(see Section 4.3 for details), we use the proba-
bility distribution to assign a relation to a new test
tuple. We select the relation that maximizes the
probability arg maxr
</bodyText>
<equation confidence="0.974074">
Qk
i P(fi|r) where f1 . . . fk
</equation>
<bodyText confidence="0.978794">
are features representing the tuple and r the relation
index.
</bodyText>
<subsectionHeader confidence="0.984394">
4.3 Inference
</subsectionHeader>
<bodyText confidence="0.973324545454545">
Exact inference is intractable for both relational
LDA and MLN models. In order to infer the most
likely multinomial parameters and , we applied
the Alternating Optimization with Mirror Descent
algorithm introduced in Andrzejewski et al. (2011).
The algorithm alternates between optimizing the
multinomial parameters (, ), whilst holding the re-
lation assignments (z) fixed, and vice-versa. At each
iteration, the algorithm first finds the optimal (, )
for a fixed z as the MAP estimate of the Dirichlet
posterior:
</bodyText>
<equation confidence="0.9998935">
r(f) nrf + 1 (6)
j(r) njr + 1 (7)
</equation>
<bodyText confidence="0.986563666666667">
where nrf is the number of times feature f is
assigned to relation r in relation assignments z,
and njr is the number of times relation r is assigned
to document j. Next, z is optimized while keeping
and fixed. This step is divided into two parts. The
algorithm first deals with all zi which appear only in
trivial groundings, i.e., groundings whose indicator
functions 1g are not affected by the latent relation
assignment z. As zi only appears in the last term of
</bodyText>
<page confidence="0.993528">
419
</page>
<bodyText confidence="0.9673805">
\x0cEquation (5), the algorithm needs only optimize the
following term:
</bodyText>
<equation confidence="0.999382428571429">
zi = arg max
r=1...R
di
(r)
Y
kpi
zi (fk) (8)
</equation>
<bodyText confidence="0.9705135">
The second part deals with the remaining zi that ap-
pear in non-trivial groundings in the first term of
Equation (5). We follow Andrzejewski et al. (2011)
in relaxing (5) into a continuous optimization prob-
lem and refer the reader to their paper for a more
in depth treatment. Suffice it to say that once the
binary variables zir {0, 1} are relaxed to contin-
uous values zir [0, 1], it is possible to introduce
the relational LDA term in the equation and com-
pute the gradient using the Entropic Mirror Descent
</bodyText>
<figure confidence="0.71133475">
Algorithm (Beck and Teboulle, 2003):
arg max
z[0,1]|KB|
L
</figure>
<equation confidence="0.951764352941177">
X
l
X
gG(l)
l1g(z)+
X
i,r
zir log di
(r)
Y
kpi
zi (fk)
s.t zir 0 ,
X
i,r
zir = 1
(9)
</equation>
<bodyText confidence="0.99832225">
In every iteration the approximation algorithm
randomly samples a term from the objective func-
tion (Equation (9)). The sampled term can be
a particular ground rule g or the relational LDA
</bodyText>
<equation confidence="0.964048714285714">
term (
P
r zir log di
(r)
Q
kpi
zi (fk)) for some
</equation>
<bodyText confidence="0.972697428571428">
uniformly sampled index i. The sampling of the
terms is weighted according to the rule weight (l)
and the grounded value (G(l)) in the case of logic
rules, and the size of corpus in tuples (|zKB|) for re-
lational LDA. Once we choose term f and take the
gradient, we can apply the Entropic Mirror Descent
update:
</bodyText>
<equation confidence="0.9818826">
zir
zir exp(Ozir f)
P
r0 zir0 exp(Ozir0 f)
(10)
</equation>
<bodyText confidence="0.99769325">
Finally, zi is recovered by rounding to arg maxr zir.
The main advantage of this approach is that it re-
quires only a means to sample groundings g for each
rule l, and can avoid fully grounding the FOL rules.
</bodyText>
<subsectionHeader confidence="0.995199">
4.4 Logic Rules
</subsectionHeader>
<bodyText confidence="0.966447">
Our model assigns relations to tuples rather than top-
ics to words. Since our tuples are described in terms
of features our logic rules must reflect this too. For
our experiments we defined two very general types
of rules described below.
Must-link Tuple The motivation behind this rule
is that tuples which share features probably express
the same underlying relation. The rule must spec-
ify which feature has to be shared for the tuples
to be clustered together. For example, the rule be-
low states that tuples containing the dependency
path PATH:appospresidentprepofpobj
should go in the same cluster:
i, j, k : F(i, PATH:is the president of) P(j, fi)
</bodyText>
<equation confidence="0.880527">
P(k, fi) Z(j, t) Z(k, r)
</equation>
<bodyText confidence="0.9811475">
Cannot-link Tuple We also define rules prohibit-
ing tuples to be clustered together because they do
not share any features. For example, tuples with
ORG-LOC entities, probably express a Location re-
lation and should not be clustered together with
PER-PER tuples, which in all likelihood express a
different relationship (e.g., Family). The rule below
expresses this constraint:
</bodyText>
<equation confidence="0.951321">
i, j, k, l : F(i, NEPAIR:PER-PER)
F(j, NEPAIR:ORG-LOC)
P(k, fi) P(l, fj) Z(k, r) Z(l, r)
</equation>
<bodyText confidence="0.998310166666667">
The specification of the first order logic rules is
an integral part of the model. The rules express
knowledge about the task at hand, the domain in-
volved, and the way the relation extraction problem
is modeled (i.e., tuples expressed as features). So
far, we have abstractly formulated the rules without
explaining how they are specifically instantiated in
our model. We could write them down by hand after
inspecting some data or through consultation with a
domain expert. Instead, we obtain logic rules au-
tomatically from a corpus following the procedure
described in Section 5.
</bodyText>
<sectionHeader confidence="0.666375" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<table confidence="0.301154">
Data We trained our model on the New York
Times (years 20002007) corpus created by Yao et
</table>
<figureCaption confidence="0.98826325">
al. (2011). The corpus contains approximately 2M
entity tuples. The latter were extracted from
428K documents. After post-processing (tokeniza-
tion, sentence-splitting, and part-of-speech tagging),
</figureCaption>
<page confidence="0.975271">
420
</page>
<table confidence="0.9935052">
\x0cMust-link Tuple
F(i, NEPAIR:PER-PER, TRIGGER:wife) P(j, fi) P(k, fi) Z(j, t) Z(k, r)
F(i, NEPAIR:PER-LOC, TRIGGER:die) P(j, fi) P(k, fi) Z(j, t) Z(k, r)
F(i, PATH:nsubjdieprepinpobj) P(j, fi) P(k, fi) Z(j, t) Z(k, r)
F(i, SOURCE:Kobe, DEST:Lakers) P(j, fi) P(k, fi) Z(j, t) Z(k, r)
Cannot-link Tuple
F(i, NEPAIR:ORG-LOC) F(j, NEPAIR:PER-PER) P(k, fi) P(l, fj) Z(k, r) Z(l, r)
F(i, NEPAIR:LOC-LOC) F(j, TRIGGER:president) P(k, fi) P(l, fj) Z(k, r) Z(l, r)
F(i, NEPAIR:PER-LOC) F(j, TRIGER:member) P(k, fi) P(l, fj) Z(k, r) Z(l, r)
F(i, NEPAIR:PER-PER) F(j, TRIGER:sell) P(k, fi) P(l, fj) Z(k, r) Z(l, r)
</table>
<tableCaption confidence="0.997533">
Table 2: Examples of automatically extracted Must-link and Cannot-link tuple rules.
</tableCaption>
<bodyText confidence="0.952318410714286">
named entities were automatically recognized and
labeled with PER, ORG, LOC, and MISC (Finkel
et al., 2005). Dependency paths for each pair of
named entity mentions were extracted from the out-
put of the MaltParser (Nivre et al., 2004). In our
experiments, we discarded tuples with paths longer
than 10 edges (Lin and Pantel, 2001). We evalu-
ated our model on the test partition of the ACE 2007
(English) RDC dataset which is labeled with gold
standard entity mentions and their relations. There
are six general relation types and 18 subtypes. We
used 25% of the ACE training partition as a devel-
opment set for parameter tuning.
Logic Rule Extraction We automatically ex-
tracted logic rules from the New York Times
(NYT) corpus as follows. The intuition behind
Must-link rules is that tuples with common features
should cluster together. Although we do not know
which features would yield the best rules, we
naively assume that good features are frequently
co-occurring features. Using the log-likelihood
ratio (Dunning, 1993), we first discarded low
confidence feature co-occurrences (p &lt; 0.05). Two
features co-occur if they are both found within
the same sentence. We then sorted the remaining
co-occurrences by their frequency and retained the
N-best ones. We only considered unigram and
bigram features since higher-order ones tend to
be sparse. An example of a bigram feature would
be (PATH:nsubjgrowprepinpobj,
DEST:Chicago).
The main intuition behind Cannot-link rules is
that tuples without any common features should
not cluster together. So, if two features never
co-occur, they probably express different relations.
For every unigram and bigram feature in the re-
spective N-best list, we find the features it does
not co-occur with in the NYT corpus. For ex-
ample, NEPAIR:PERLOC does not co-occur with
DEST:Yankees and the bigram DEST:United Na-
tions, NEPAIR:PERORG does not co-occur with
SOURCE:Mr. Bush, NEPAIR:PERLOC. Cannot-
link rules are then based on such non-co-occurring
feature pairs.
We optimized N empirically on the development
set. We experimented with values ranging from 20
to 500. We obtained 20 Must-link rules for coarse-
grained relations and 400 rules for their subtypes.
We extracted 1,814 Cannot-link rules for general re-
lations (N = 50) and 34,522 rules for subtypes
(N = 400). The number of features involved in the
Must-link rules was 25 for coarse-grained relations
and 422 for fine-grained relations. For Cannot-link
rules, 62 features were involved in coarse-grained
relations and 422 in fine-grained relations.
Examples of the rules we extracted are shown in
</bodyText>
<tableCaption confidence="0.867444">
Table 2. The first rule in the upper half of the ta-
</tableCaption>
<bodyText confidence="0.996992083333333">
ble states that tuples must cluster together if their
source and target entities are PER and contain the
trigger word wife in their dependency path. The sec-
ond rule is similar, the source entity here is PER,
the target LOC and the trigger word is die. Ac-
cording to the third rule, tuples featuring the path
PATH:nsubjdieprepinpobj should be
in the same cluster. The fourth rule forces tuples
whose source entity is Kobe and target entity is Lak-
ers to cluster together. The second half of the table
illustrates Cannot-link tuple rules. The first rule pre-
vents tuples with ORG-LOC entities to cluster to-
</bodyText>
<page confidence="0.99292">
421
</page>
<bodyText confidence="0.998775209677419">
\x0cgether with PER-PER tuples. The second rule states
that we cannot link LOC-LOC tuples with those
whose trigger word is president, and so on.
Parameter Tuning Our framework has several
parameters that must be adjusted for an optimal clus-
tering solution. These include the hyperparame-
ters and as well as the number of clusters. In
addition, we have to assign a weight to each FOL
rule grounding. An exhaustive search on the hy-
perparameters and rule weights is not possible. We
therefore followed a step-wise approximation proce-
dure. First, we find the best and values, whilst
varying the number of clusters. Once we have the
best hyperparameters for each clustering, we set the
weights for the FOL rules. We varied the number
of relations from 5 to 50. We experimented with
values in the range of [0.05 0.5] and values in
the range of [0.05 0.5]. These values were opti-
mized separately for coarse- and fine-grained rela-
tions. Table 3 shows the optimal number of clusters
for different model variants and relation types.
The FOL weights can also make a difference in
the final output; the bigger the weight the more
times the rule will be sampled in the Mirror Descent
algorithm. We experimented with two weighting
schemes: (a) we gave a weight of 1 or 0.5 to each
rule grounding and (b) we scaled the weights so as
to make their contribution comparable to relational
LDA. We obtained best results on the development
set with the former scheme.
Baselines We compared our FOL relational LDA
model against standard LDA (Blei et al., 2003) and
relational LDA without the FOL component. In the
case of standard LDA, we estimated topics (rela-
tions) over words, and used the context of the en-
tity mentions pairs as a bag of words feature to se-
lect the most likely cluster at test time. Parameters
for LDA and relational LDA were optimized follow-
ing the same parameter tuning procedure described
above.
We also compared our model against the unsuper-
vised method introduced in Hasegawa et al. (2004).
Their key idea is to cluster pairs of co-occurring
named entities according to the similarity of their
surrounding contexts. Following their approach, we
measured context similarity using the vector space
model and the cosine metric and grouped NE pairs
into clusters using a complete linkage hierarchical
clustering algorithm. We adopted the same parame-
ter values as detailed in their paper (e.g., cosine sim-
ilarity threshold, length of context vectors). At test
time, instances were assigned to the relation cluster
most similar to them (according to the cosine mea-
sure).
Evaluation We evaluated the clusters obtained by
our model and the comparison systems using the Fs-
core measure introduced in the SemEval 2007 task
(Agirre and Soroa, 2007); it is the harmonic mean
of precision and recall defined as the number of cor-
rect members of a cluster divided by the number of
items in the cluster and the number of items in the
gold-standard class, respectively.
</bodyText>
<sectionHeader confidence="0.999916" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9874390625">
Our results are summarized in Table 3 which reports
Fscore for (Hasegawa et al., 2004), LDA, relational
LDA (RelLDA), and our model with the FOL com-
ponent. To assess the impact of the rules on the
clustering, we conducted several rule ablation stud-
ies. We thus present results with a model that in-
cludes both Must-link and Cannot-link tuple rules
(CLT+MLT), and models that include either Must-
link (MLT) or Cannot-link (CLT) rules but not both.
We show the performance of these models with the
entire feature set (see (ALL) in the table) and with a
subset consisting solely of NE pair related features
(see (NEPAIR) in the table). We report results against
coarse- and fine-grained relations (6 and 18 relation
types in ACE, respectively). The table shows the
optimal number of relation clusters (in parentheses)
per model and relation type.
We also wanted to examine the quality of the logic
rules. Recall that we learn these heuristically from
the NYT corpus. We thus trained an additional vari-
ant of our model with rules extracted from the ACE
training set (75%) which contains relation annota-
tions. The extraction procedure was similar to the
unsupervised case, save that the relation types were
known and thus informative features could be mined
more reliably. For Must-link rules, we extracted un-
igram and bigram feature frequencies for each re-
lation type and applied TF-IDF weighting in order
to discover the most discriminative ones. We cre-
ated logic rules for the 10 best feature combinations
in each relation type. Regarding Cannot-link rules,
we enumerated the features (unigrams and bigrams)
</bodyText>
<page confidence="0.998686">
422
</page>
<table confidence="0.9995648125">
\x0cModel Subtype Type
HASEGAWA 26.1 (12) 34.7 (12)
LDA 23.4 (10) 29.0 (5)
RelLDA 30.4 (40) 38.6 (5)
U-MLT (ALL) 36.6 (10) 48.0 (5)
U-CLT (ALL) 30.5 (5) 39.3 (5)
U-CLT+MLT (ALL) 29.8 (5) 42.0 (5)
U-MLT (NEPAIR) 36.5 (10) 47.2 (5)
U-CLT (NEPAIR) 28.8 (50) 40.5 (5)
U-CLT+MLT (NEPAIR) 30.9 (10) 41.5 (5)
S-MLT (ALL) 37.0 (10) 47.0 (5)
S-CLT (ALL) 31.4 (50) 40.9 (5)
S-CLT+MLT (ALL) 32.3 (10) 42.5 (5)
S-MLT (NEPAIR) 37.0 (10) 47.6 (10)
S-CLT (NEPAIR) 31.4 (10) 40.1 (5)
S-CLT+MLT (NEPAIR) 37.1 (10) 46.0 (5)
</table>
<tableCaption confidence="0.999552">
Table 3: Model performance on the ACE 2007 test set
</tableCaption>
<bodyText confidence="0.990446194805195">
using Fscore. Results are shown for six main relation
types and their subtypes (18 in total). (ALL) models con-
tain rules extracted from the entire feature set. For (NE-
PAIR) models, rules were extracted from NEPAIR-related
features only. Prefix U- denotes models that use unsu-
pervised rules; prefix S- highlights models using super-
vised rules. The optimal number of relations per model
is shown in parentheses.
that did not co-occur in any relation type and applied
TF-IDF weighting. Again, we created rules for the
10 most discriminative features. We defined rules
over the entire feature set (466 Must-link and 26,074
Cannot-link rules) and a subset containing only NE
pairs. In Table 3, prefixes S- and U- indicate model
variants with supervised and unsupervised rules, re-
spectively.
Our results show that standard LDA is not suit-
able for relation extraction. The obtained clusters
are not informative enough to induce semantic re-
lations, whereas RelLDA yields substantially bet-
ter Fscores. This is not entirely surprising, given
that RelLDA is a relation extraction specific model.
Hasegawa et al.s (2004) model lies somewhere in
the middle between LDA and RelLDA. The com-
bination of RelLDA with automatically extracted
FOL rules improves over RelLDA across the board
(see the U- models in Table 3). MLT rules deliver
the largest improvement for both coarse and fine-
grained relation types. In general, CLT models per-
form worse as well as models using both types of
rules (MLT+CLT). The inferior performance of the
rule combination may be due to the fact that MLT
and CLT rules contain conflicting information and
to a certain extent cancel each other out. The use
of many rules might also negatively impact infer-
ence, i.e., discriminative rules are sampled less and
cannot influence the model towards a better solu-
tion. Restricting the number of features and rules
to named entity pairs only incurs a negligible drop
in performance. This is good news for scaling pur-
poses, since a small number of rules can greatly
speed-up inference. Interestingly, model variants
which use supervised FOL rules (see the prefix S-
in Table 3) perform on par with unsupervised mod-
els. Again, MLT rules perform best in the super-
vised case, whereas CLT rules marginally improve
over RelLDA.
We assessed whether differences in performance
are statistically significant (p &lt; 0.05) using boot-
strap resampling (Noreen, 1989). All models across
all relation types are significantly better than LDA
and Hasegawa et al. (2004). FOL-based models per-
form significantly better than RelLDA, with the ex-
ception of all CLT models and U-CLT+MLT (ALL).
MLT models are significantly better than any other
rule-based model, except those that only use NE-
PAIR features. We also measured whether differ-
ent models agree on their topic assignments using
Cohens Kappa.2 RelLDA agrees least with MLT
models and most with CLT models (i.e., = 0.50
for U-MLT (ALL) and = 0.65 for U-CLT (ALL)).
This suggests that the CLT rules do not affect the
output of RelLDA as much as MLT ones. Examples
of relation clusters discovered by the U-MLT (ALL)
model are shown in Table 4.
A last note on parameter selection. Our experi-
ments explored the parameter space extensively in
order to examine any interactions between the in-
duced relations and the logic rules. For most model
variants inferring subtype relations, the preferred
number of clusters is 10. For coarse-grained rela-
tions, the optimal number of clusters is five. Over-
all, we found that the quality of the output is highly
correlated with the quality of the logic rules and that
a few good rules are more important than the opti-
mal number of clusters. We consider these findings
robust enough to apply across domains and datasets.
</bodyText>
<page confidence="0.893808">
2
</page>
<bodyText confidence="0.99678">
For all comparison models the number of relation clusters
was set to 10.
</bodyText>
<page confidence="0.992421">
423
</page>
<figure confidence="0.55831755">
\x0cSOURCE PATH DEST
Republican president of Senate
Senate director of Yankees
House professor at Republican
Bush chairman of Congress
Democrat spokesman for House
Mr. Bush executive of Mets
Democrats director at U. of California
Republican analyst at United Nations
Employment
SOURCE PATH DEST
Yankees defeat World Series
Mets win Olympic
United States beat World Cup
Giants play Yankees
Jets win Super Bowl
Nets lose Olympics
Knicks sign Mets
Rangers victory over Giants
Sports
</figure>
<tableCaption confidence="0.99562">
Table 4: Clusters discovered by the U-MLT (ALL) model
</tableCaption>
<bodyText confidence="0.959541666666667">
indicating employment- and sports-type relations. For the
sake of readability, we do not display the syntactic depen-
dencies between words in a path.
</bodyText>
<sectionHeader confidence="0.998276" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9982216">
In this paper we presented a new model for unsu-
pervised relation extraction which operates over tu-
ples representing a syntactic relationship between
two named entities. Our model clusters such tuples
into underlying semantic relations (e.g., Located,
Family) by incorporating general domain knowledge
which we encode as First Order Logic rules. Specif-
ically, we combine a topic model developed for the
relation extraction task with domain relevant rules,
and present an algorithm for estimating the param-
eters of this model. Evaluation results on the ACE
2007 (English) RDC task show that our model out-
performs competitive unsupervised approaches by a
wide margin and is able to produce clusters shaped
by both the data and the rules.
In the future, we would like to explore additional
types of rules such as seed rules, which would as-
sign tuples complying with the seed information
to distinct relations. Aside from devising new rule
types, an obvious next step would be to explore dif-
ferent ways of extracting the rule set based on differ-
ent criteria (e.g., the most general versus most spe-
cific rules). Also note that in the current framework
rule weights are set manually by the domain expert.
An appealing direction would be to learn these auto-
matically e.g., via a procedure that optimizes some
clustering objective. Finally, it should be interesting
to use some form of distant supervision (Mintz et al.,
2009) either as a means of obtaining useful rules or
to discard potentially noisy or uninformative rules.
</bodyText>
<sectionHeader confidence="0.9669" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.991721714285714">
We gratefully acknowledge financial support from
the Department of Education, Universities and Re-
search of the Basque Government (BFI-2011-442).
We also thank Limin Yao and Sebastian Riedel for
sharing their corpus with us and the members of the
Probabilistic Models reading group at the University
of Edinburgh for helpful feedback.
</bodyText>
<sectionHeader confidence="0.98997" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999059606060606">
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the 5th ACM International Confer-
ence on Digital Libraries, pages 8594, San Antonio,
Texas.
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
task 02: Evaluating word sense induction and discrim-
ination systems. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, pages 712,
Prague, Czech Republic.
David Andrzejewski, Xiaojin Zhu, Mark Craven, and Ben
Recht. 2011. A framework for incorporating general
domain knowledge into latent Dirichlet allocation us-
ing first-order logic. In Proceedings of the 22nd In-
ternational Joint Conference on Artificial Intelligence,
pages 11711177, Barcelona, Spain.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In Proceedings
of the 20th International Joint Conference on Artificial
Intelligence, pages 26702676, Hyderabad, India.
Amir Beck and Marc Teboulle. 2003. Mirror de-
scent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters,
31(3):167175.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:9931022.
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal su-
pervision. In Proceedings of the 45th Annual Meeting
</reference>
<page confidence="0.986299">
424
</page>
<reference confidence="0.999607836734694">
\x0cof the Association of Computational Linguistics, pages
576583, Prague, Czech Republic.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Meeting of the Association for Computational
Linguistics, Main Volume, pages 423429, Barcelona,
Spain.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):6174.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
363370, Ann Arbor, Michigan.
David Gondek and Thomas Hofmann. 2004. Non-
redundant data clustering. In IEEE International Con-
ference on Data Mining, pages 7582. IEEE Computer
Society.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(1):52285235.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics, pages 415422, Barcelona, Spain.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
Dekang Lin and Patrick Pantel. 2001. DIRT discovery
of inference rules from text. In Proceedings of the 7th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 323328, San
Francisco, California.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 10031011,
Suntec, Singapore.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of the 8th Conference on Computational Natural Lan-
guage Learning, pages 4956, Boston, Massachusetts.
Eric W. Noreen. 1989. Computer-intensive Methods for
Testing Hypotheses: An Introduction. John Wiley and
Sons Inc.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 113120, Sydney, Aus-
tralia.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 110, Suntec, Singapore.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62(12):107136.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction. In Proceedings of the
16th International Joint Conference on Artificial Intel-
ligence, pages 474479, Stockholm, Sweden.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel Weld. 2010. Learning first-order Horn clauses
from web text. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 10881098, Cambridge, MA, October. As-
sociation for Computational Linguistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference,
pages 304311, New York City, USA.
Mihai Surdeanu and Massimiliano Ciaramita. 2007. Ro-
bust information extration with perceptrons. In Pro-
ceedings of the NIST 2007 Automatic Content Extrac-
tion Workshop.
Kiri Wagstaff, Claire Cardie, C Rogers, and S Schrodl.
2001. Constrained k-means clustering with back-
ground knowledge. In International Conference on
Machine Learning, pages 577584. Morgan Kauf-
mann.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 14561466, Edinburgh, Scotland, UK.
GuoDong Zhou, Min Zhang, DongHong Ji, and QiaoM-
ing Zhu. 2007. Tree kernel-based relation extraction
with context-sensitive structured parse tree informa-
tion. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 728736, Prague, Czech Republic.
</reference>
<page confidence="0.990099">
425
</page>
<figure confidence="0.245303">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.184957">
<title>Unsupervised Relation Extraction with General Domain Knowledge</title>
<note confidence="0.730834636363636">b&apos;Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 415425, Seattle, Washington, USA, 18-21 October 2013. c 2013 Association for Computational Linguistics Unsupervised Relation Extraction with General Domain Knowledge Oier Lopez de Lacalle1,2 and Mirella Lapata1 1 Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB 2 IKERBASQUE, Basque Foundation for Science, Bilbao, Spain</note>
<email confidence="0.988989">oier.lopezdelacalle@ehu.es,mlap@inf.ed.ac.uk</email>
<abstract confidence="0.99868747368421">In this paper we present an unsupervised approach to relational information extraction. Our model partitions tuples representing an observed syntactic relationship between two named entities (e.g., X was born in Y and X is from Y) into clusters corresponding to underlying semantic relation types (e.g., BornIn, Located). Our approach incorporates general domain knowledge which we encode as First Order Logic rules and automatically combine with a topic model developed specifically for the relation extraction task. Evaluation results on the ACE 2007 English Relation Detection and Categorization (RDC) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the 5th ACM International Conference on Digital Libraries,</booktitle>
<pages>8594</pages>
<location>San Antonio, Texas.</location>
<contexts>
<context position="5928" citStr="Agichtein and Gravano, 2000" startWordPosition="900" endWordPosition="903">ically from a corpus contrary to Andrzejewski et al. (2011) who employ manually crafted seeds. 2 Related Work A variety of learning paradigms have been applied to relation extraction. As mentioned earlier, supervised methods have been shown to perform well in this task. The reliance on manual annotation, which is expensive to produce and thus limited in quantity, has provided the impetus for semi-supervised and purely unsupervised approaches. Semi-supervised methods use a small number of seed instances or patterns (per relation) to launch an iterative training process (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Bunescu and Mooney, 2007; Pantel and Pennacchiotti, 2006). The seeds are used to extract a new set of patterns from a large corpus, which are then used to extract more instances, and so on. Unsupervised relation extraction methods are not limited to a predefined set of target relations, but discover all types of relations found in the text. The relations represent clusters over strings of words (Banko et al., 2007; Hasegawa et al., 2004), syntactic patterns between entities (Yao et al., 2011; Shinyama and Sekine, 2006), or logical expressions (Poon and Domingos, 2009). Another learning parad</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of the 5th ACM International Conference on Digital Libraries, pages 8594, San Antonio, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Semeval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>712</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="30450" citStr="Agirre and Soroa, 2007" startWordPosition="5086" endWordPosition="5089">s. Following their approach, we measured context similarity using the vector space model and the cosine metric and grouped NE pairs into clusters using a complete linkage hierarchical clustering algorithm. We adopted the same parameter values as detailed in their paper (e.g., cosine similarity threshold, length of context vectors). At test time, instances were assigned to the relation cluster most similar to them (according to the cosine measure). Evaluation We evaluated the clusters obtained by our model and the comparison systems using the Fscore measure introduced in the SemEval 2007 task (Agirre and Soroa, 2007); it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively. 6 Results Our results are summarized in Table 3 which reports Fscore for (Hasegawa et al., 2004), LDA, relational LDA (RelLDA), and our model with the FOL component. To assess the impact of the rules on the clustering, we conducted several rule ablation studies. We thus present results with a model that includes both Must-link and Cannot-link tuple rules (CLT+MLT), and models that </context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination systems. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 712, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>Xiaojin Zhu</author>
<author>Mark Craven</author>
<author>Ben Recht</author>
</authors>
<title>A framework for incorporating general domain knowledge into latent Dirichlet allocation using first-order logic.</title>
<date>2011</date>
<booktitle>In Proceedings of the 22nd International Joint Conference on Artificial Intelligence,</booktitle>
<pages>11711177</pages>
<location>Barcelona,</location>
<contexts>
<context position="4258" citStr="Andrzejewski et al., 2011" startWordPosition="637" endWordPosition="640">example are tuples with identical or similar features which intuitively should be clustered together. In this paper, we propose an unsupervised approach to relation extraction which does not re415 \x0cquire any relation-specific training data and allows to incorporate global constraints general expressing domain knowledge. We encode domain knowledge as First Order Logic (FOL) rules and automatically integrate them with a topic model to produce clusters shaped by the data and the constraints at hand. Specifically, we extend the Fold-all (FirstOrder Logic latent Dirichlet Allocation) framework (Andrzejewski et al., 2011) to the relation extraction task, explain how to incorporate meaningful constraints, and develop a scalable inference technique. In the presence of multiple candidate relation decompositions for a given corpus, domain knowledge can steer the model towards relations which are best aligned with user and task modeling goals. We also argue that a general mechanism for encoding additional modeling assumptions and side information can lessen the need for custom relation extraction model variants. Experimental results on the ACE2007 Relation Detection and Categorization (RDC) dataset show that our mo</context>
<context position="7416" citStr="Andrzejewski et al. (2011)" startWordPosition="1145" endWordPosition="1148">ion in an unlabeled corpus, and use them to train a relation classifier. Our own work adds an additional approach into the mix. We use a topic model to infer an arbitrary number of relations between named entities. Although we do not have access to relation-specific information (either as a relational database or manually annotated data), we impose task-specific constraints which inject domain knowledge into the learning algorithm. We thus alleviate known problems with the interpretability of the clusters obtained from topic models and are able to guide our model towards reasonable relations. Andrzejewski et al. (2011) show how to integrate First-Order Logic with vanilla LDA. We extend their formulation to relation tuples rather than individual words. Our model generates a corpus of entity tuples which are in turn represented by features and uses automatically acquired FOL rules. The idea of integrating topic modeling with FOL builds on research in probabilistic logic modeling such as Markov Logic Networks (Richardson and Domingos, 2006). Schoenmackers et al. (2010) learn Horn clauses from web-scale text with aim of finding answers to a users query. Our work is complementary to theirs. We could make use of </context>
<context position="17467" citStr="Andrzejewski et al. (2011)" startWordPosition="2862" endWordPosition="2865">e parse the corpus searching for the tuples that satisfy the logic rules and store the indices of the tuples that ground the rule. The stored indices are used to set l to a specific value. For the (Obama, White House) example above, G consists of F propositional rules for each observed feature, where i [1 . . . F]. For each grounding (g G(l)) we define an indicator function: 1g(z, p, d, o) = 1, if g is true under z and p, d, o 0, otherwise (3) where z are relation assignments to tuples, p is the set of features in tuples, d are documents, and o the side information encoded in FOL. Contrary to Andrzejewski et al. (2011), we need to ground the rules while taking into account if the feature specified in the rule is expressed by any tuple or the specific given tuple, since we are assigning relations to tuples, and not directly to words. Next, we define a Markov Random Field (MRF) which combines relational LDA with the FOL knowledge base. The MRF is defined over latent relation tuple assignments z, relation feature multinomials , and relation document multinomials (the feature set, document, and external information o are observed). Under this model the conditional probability P(z, , |, , p, d, o, KB) is proport</context>
<context position="18705" citStr="Andrzejewski et al. (2011)" startWordPosition="3099" endWordPosition="3103">p L X l X gG(l) l1g(z, p, d, o) R Y r p(r|) D Y j p(j|) N Y i di (zi) Y kpi zi (fk) (4) The first term in Equation (4) corresponds to the logic factor in Figure 1 that groups variables d, z, 1 This rule translates as every tuple containing Obama and White House as features should be in relation cluster r. f1, f2, . . . fk and o. The remaining terms in Equation (4) refer to relational LDA. The goal of the model is to estimate the most likely and for the given observed state. As z can not be marginalized out, we proceed with MAP estimation of (z, , ), maximizing the log of the probability as in Andrzejewski et al. (2011): arg max z,, L X l X gG(l) l1g(z, p, d, o)+ R X r log p(r|)+ N X i log di (zi) Y kpi zi (fk) (5) Once the parameters of the model are estimated (see Section 4.3 for details), we use the probability distribution to assign a relation to a new test tuple. We select the relation that maximizes the probability arg maxr Qk i P(fi|r) where f1 . . . fk are features representing the tuple and r the relation index. 4.3 Inference Exact inference is intractable for both relational LDA and MLN models. In order to infer the most likely multinomial parameters and , we applied the Alternating Optimization wi</context>
<context position="20418" citStr="Andrzejewski et al. (2011)" startWordPosition="3411" endWordPosition="3414">er of times relation r is assigned to document j. Next, z is optimized while keeping and fixed. This step is divided into two parts. The algorithm first deals with all zi which appear only in trivial groundings, i.e., groundings whose indicator functions 1g are not affected by the latent relation assignment z. As zi only appears in the last term of 419 \x0cEquation (5), the algorithm needs only optimize the following term: zi = arg max r=1...R di (r) Y kpi zi (fk) (8) The second part deals with the remaining zi that appear in non-trivial groundings in the first term of Equation (5). We follow Andrzejewski et al. (2011) in relaxing (5) into a continuous optimization problem and refer the reader to their paper for a more in depth treatment. Suffice it to say that once the binary variables zir {0, 1} are relaxed to continuous values zir [0, 1], it is possible to introduce the relational LDA term in the equation and compute the gradient using the Entropic Mirror Descent Algorithm (Beck and Teboulle, 2003): arg max z[0,1]|KB| L X l X gG(l) l1g(z)+ X i,r zir log di (r) Y kpi zi (fk) s.t zir 0 , X i,r zir = 1 (9) In every iteration the approximation algorithm randomly samples a term from the objective function (Eq</context>
</contexts>
<marker>Andrzejewski, Zhu, Craven, Recht, 2011</marker>
<rawString>David Andrzejewski, Xiaojin Zhu, Mark Craven, and Ben Recht. 2011. A framework for incorporating general domain knowledge into latent Dirichlet allocation using first-order logic. In Proceedings of the 22nd International Joint Conference on Artificial Intelligence, pages 11711177, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>26702676</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="6347" citStr="Banko et al., 2007" startWordPosition="973" endWordPosition="976">rvised approaches. Semi-supervised methods use a small number of seed instances or patterns (per relation) to launch an iterative training process (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Bunescu and Mooney, 2007; Pantel and Pennacchiotti, 2006). The seeds are used to extract a new set of patterns from a large corpus, which are then used to extract more instances, and so on. Unsupervised relation extraction methods are not limited to a predefined set of target relations, but discover all types of relations found in the text. The relations represent clusters over strings of words (Banko et al., 2007; Hasegawa et al., 2004), syntactic patterns between entities (Yao et al., 2011; Shinyama and Sekine, 2006), or logical expressions (Poon and Domingos, 2009). Another learning paradigm is distant supervision which does not require labeled data but instead access to a relational database such as Freebase (Mintz et al., 2009). The idea is to take entities that appear in some relation in the database, find the sentences that express the relation in an unlabeled corpus, and use them to train a relation classifier. Our own work adds an additional approach into the mix. We use a topic model to infer</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 26702676, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Beck</author>
<author>Marc Teboulle</author>
</authors>
<title>Mirror descent and nonlinear projected subgradient methods for convex optimization.</title>
<date>2003</date>
<journal>Operations Research Letters,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="20808" citStr="Beck and Teboulle, 2003" startWordPosition="3481" endWordPosition="3484"> only optimize the following term: zi = arg max r=1...R di (r) Y kpi zi (fk) (8) The second part deals with the remaining zi that appear in non-trivial groundings in the first term of Equation (5). We follow Andrzejewski et al. (2011) in relaxing (5) into a continuous optimization problem and refer the reader to their paper for a more in depth treatment. Suffice it to say that once the binary variables zir {0, 1} are relaxed to continuous values zir [0, 1], it is possible to introduce the relational LDA term in the equation and compute the gradient using the Entropic Mirror Descent Algorithm (Beck and Teboulle, 2003): arg max z[0,1]|KB| L X l X gG(l) l1g(z)+ X i,r zir log di (r) Y kpi zi (fk) s.t zir 0 , X i,r zir = 1 (9) In every iteration the approximation algorithm randomly samples a term from the objective function (Equation (9)). The sampled term can be a particular ground rule g or the relational LDA term ( P r zir log di (r) Q kpi zi (fk)) for some uniformly sampled index i. The sampling of the terms is weighted according to the rule weight (l) and the grounded value (G(l)) in the case of logic rules, and the size of corpus in tuples (|zKB|) for relational LDA. Once we choose term f and take the gr</context>
</contexts>
<marker>Beck, Teboulle, 2003</marker>
<rawString>Amir Beck and Marc Teboulle. 2003. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="11644" citStr="Bishop, 2006" startWordPosition="1826" endWordPosition="1827"> to tuple i (fik|zi, zi Mult(zi )). Relations are drawn from a Dirichlet prior ( Dir()). In other words, each tuple in a document is assigned a hidden relation (z = z1...zN ); each relation is represented by a multinomial distribution over features r (Dirichlet prior ). r is a vector with F dimensions each corresponding to a feature. Finally, documents (j = 1...D) are associated with a multinomial distribution j over relations (Dirichlet prior ). j is a vector with R dimensions, one for each relation. Figure 1 represents relational LDA model as a an undirected graphical model or factor graph (Bishop, 2006), ignoring for the moment the factor which connects the d, z, f1...k and o variables. Directed graphical models can be converted into undirected ones by adding edges between co-parents (Koller and Friedman, 2009). Each clique in the graph defines a potential function which replaces the conditional probabilities in the directed graph. Each maximal clique is associated with a special factor node (the black squares) and clique members are connected to that factor. The probability of any specific configuration is calculated by multiplying the potential functions and normalizing them. We adopt the </context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--9931022</pages>
<contexts>
<context position="2844" citStr="Blei et al., 2003" startWordPosition="421" endWordPosition="424">alternative which could lead to significant resource savings and more portable extraction systems. It therefore comes as no surprise that latent topic analysis methods have been used for a variety of IE tasks. Yao et al. (2011), for example, propose a series of topic models which perform relation discovery by clustering tuples representing an observed syntactic relationship between two named entities (e.g., X was born in Y and X is from Y). The clusters correspond to semantic relations whose number or type is not known in advance. Their models depart from standard Latent Dirichlet Allocation (Blei et al., 2003) in that a document consists of relation tuples rather than individual words; moreover, tuples have features each of which is generated independently from a hidden relation (e.g., the words corresponding to the first and second entities, the type and order of the named entities). Since these features are local, they cannot capture more global constraints pertaining to the relation extraction task. Such constraints may take the form of restrictions on which tuples should be clustered together or not. For instance, different types of named entities may be indicative of different relations (ORG-L</context>
<context position="29248" citStr="Blei et al., 2003" startWordPosition="4890" endWordPosition="4893">lations. Table 3 shows the optimal number of clusters for different model variants and relation types. The FOL weights can also make a difference in the final output; the bigger the weight the more times the rule will be sampled in the Mirror Descent algorithm. We experimented with two weighting schemes: (a) we gave a weight of 1 or 0.5 to each rule grounding and (b) we scaled the weights so as to make their contribution comparable to relational LDA. We obtained best results on the development set with the former scheme. Baselines We compared our FOL relational LDA model against standard LDA (Blei et al., 2003) and relational LDA without the FOL component. In the case of standard LDA, we estimated topics (relations) over words, and used the context of the entity mentions pairs as a bag of words feature to select the most likely cluster at test time. Parameters for LDA and relational LDA were optimized following the same parameter tuning procedure described above. We also compared our model against the unsupervised method introduced in Hasegawa et al. (2004). Their key idea is to cluster pairs of co-occurring named entities according to the similarity of their surrounding contexts. Following their ap</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:9931022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting \x0cof the Association of Computational Linguistics,</booktitle>
<pages>576583</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5954" citStr="Bunescu and Mooney, 2007" startWordPosition="904" endWordPosition="907"> to Andrzejewski et al. (2011) who employ manually crafted seeds. 2 Related Work A variety of learning paradigms have been applied to relation extraction. As mentioned earlier, supervised methods have been shown to perform well in this task. The reliance on manual annotation, which is expensive to produce and thus limited in quantity, has provided the impetus for semi-supervised and purely unsupervised approaches. Semi-supervised methods use a small number of seed instances or patterns (per relation) to launch an iterative training process (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Bunescu and Mooney, 2007; Pantel and Pennacchiotti, 2006). The seeds are used to extract a new set of patterns from a large corpus, which are then used to extract more instances, and so on. Unsupervised relation extraction methods are not limited to a predefined set of target relations, but discover all types of relations found in the text. The relations represent clusters over strings of words (Banko et al., 2007; Hasegawa et al., 2004), syntactic patterns between entities (Yao et al., 2011; Shinyama and Sekine, 2006), or logical expressions (Poon and Domingos, 2009). Another learning paradigm is distant supervision</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of the 45th Annual Meeting \x0cof the Association of Computational Linguistics, pages 576583, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Main Volume,</booktitle>
<pages>423429</pages>
<location>Barcelona,</location>
<contexts>
<context position="1694" citStr="Culotta and Sorensen, 2004" startWordPosition="246" endWordPosition="249">on task. Evaluation results on the ACE 2007 English Relation Detection and Categorization (RDC) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules. 1 Introduction Information extraction (IE) is becoming increasingly useful as a form of shallow semantic analysis. Learning relational facts from text is one of the core tasks of IE and has applications in a variety of fields including summarization, question answering, and information retrieval. Previous work (Surdeanu and Ciaramita, 2007; Culotta and Sorensen, 2004; Zhou et al., 2007) has traditionally relied on extensive human involvement (e.g., hand-annotated training instances, manual pattern extraction rules, hand-picked seeds). Standard supervised techniques can yield high performance when large amounts of hand-labeled data are available for a fixed inventory of relation types (e.g., Employment, Located), however, extraction systems do not easily generalize beyond their training domains and often must be re-engineered for each application. Unsupervised approaches offer a promising alternative which could lead to significant resource savings and mor</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Main Volume, pages 423429, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="25487" citStr="Dunning, 1993" startWordPosition="4269" endWordPosition="4270">taset which is labeled with gold standard entity mentions and their relations. There are six general relation types and 18 subtypes. We used 25% of the ACE training partition as a development set for parameter tuning. Logic Rule Extraction We automatically extracted logic rules from the New York Times (NYT) corpus as follows. The intuition behind Must-link rules is that tuples with common features should cluster together. Although we do not know which features would yield the best rules, we naively assume that good features are frequently co-occurring features. Using the log-likelihood ratio (Dunning, 1993), we first discarded low confidence feature co-occurrences (p &lt; 0.05). Two features co-occur if they are both found within the same sentence. We then sorted the remaining co-occurrences by their frequency and retained the N-best ones. We only considered unigram and bigram features since higher-order ones tend to be sparse. An example of a bigram feature would be (PATH:nsubjgrowprepinpobj, DEST:Chicago). The main intuition behind Cannot-link rules is that tuples without any common features should not cluster together. So, if two features never co-occur, they probably express different relations</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):6174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>363370</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="24571" citStr="Finkel et al., 2005" startWordPosition="4117" endWordPosition="4120">pinpobj) P(j, fi) P(k, fi) Z(j, t) Z(k, r) F(i, SOURCE:Kobe, DEST:Lakers) P(j, fi) P(k, fi) Z(j, t) Z(k, r) Cannot-link Tuple F(i, NEPAIR:ORG-LOC) F(j, NEPAIR:PER-PER) P(k, fi) P(l, fj) Z(k, r) Z(l, r) F(i, NEPAIR:LOC-LOC) F(j, TRIGGER:president) P(k, fi) P(l, fj) Z(k, r) Z(l, r) F(i, NEPAIR:PER-LOC) F(j, TRIGER:member) P(k, fi) P(l, fj) Z(k, r) Z(l, r) F(i, NEPAIR:PER-PER) F(j, TRIGER:sell) P(k, fi) P(l, fj) Z(k, r) Z(l, r) Table 2: Examples of automatically extracted Must-link and Cannot-link tuple rules. named entities were automatically recognized and labeled with PER, ORG, LOC, and MISC (Finkel et al., 2005). Dependency paths for each pair of named entity mentions were extracted from the output of the MaltParser (Nivre et al., 2004). In our experiments, we discarded tuples with paths longer than 10 edges (Lin and Pantel, 2001). We evaluated our model on the test partition of the ACE 2007 (English) RDC dataset which is labeled with gold standard entity mentions and their relations. There are six general relation types and 18 subtypes. We used 25% of the ACE training partition as a development set for parameter tuning. Logic Rule Extraction We automatically extracted logic rules from the New York T</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 363370, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Gondek</author>
<author>Thomas Hofmann</author>
</authors>
<title>Nonredundant data clustering.</title>
<date>2004</date>
<booktitle>In IEEE International Conference on Data Mining,</booktitle>
<pages>7582</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="8222" citStr="Gondek and Hofmann (2004)" startWordPosition="1276" endWordPosition="1279"> which are in turn represented by features and uses automatically acquired FOL rules. The idea of integrating topic modeling with FOL builds on research in probabilistic logic modeling such as Markov Logic Networks (Richardson and Domingos, 2006). Schoenmackers et al. (2010) learn Horn clauses from web-scale text with aim of finding answers to a users query. Our work is complementary to theirs. We could make use of their rules to discover more accurate relations. The general goal of assisting the learner in recovering the correct clustering by supplying additional domain knowledge is not new. Gondek and Hofmann (2004) supply a known clustering they do not want the learner to return, whereas Wagstaff et al. (2001) use pairwise labels for items indicating whether they belong in the same cluster. These methods combine domain knowledge with statistical learning in order to improve performance with re416 \x0cspect to the true target clustering. Although, the target labels are not available in our case, we are able to show that the inclusion of domain knowledge yields clustering improvements. 3 Learning Setting Our relation extraction task broadly adheres to the ACE specification guidelines. Our aim is to detect</context>
</contexts>
<marker>Gondek, Hofmann, 2004</marker>
<rawString>David Gondek and Thomas Hofmann. 2004. Nonredundant data clustering. In IEEE International Conference on Data Mining, pages 7582. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<journal>PNAS,</journal>
<volume>101</volume>
<issue>1</issue>
<contexts>
<context position="12820" citStr="Griffiths and Steyvers, 2004" startWordPosition="2032" endWordPosition="2035">tential functions and normalizing them. We adopt the factor graph representation as is it convenient for introducing logic rules into the model. The joint probability of the model given the priors and the documents (P(p, z, , |, , d)) is equivalent to: R Y r p(r|) D Y j p(j|) N Y i di (zi) Y kpi zi (fk) (1) where di (zi) is the zi-th element in the vector di and zi (fk) is fk-th feature in the zi vector. Variable pi is the i-th tuple containing k features. The parameters of the latent variables (e.g., , ) are typically estimated using an approximate inference algorithm such as Gibbs Sampling (Griffiths and Steyvers, 2004). 417 \x0cFigure 1: Relational LDA as a factor graph. Filled circles represent observed variables, empty circles are associated with latent variables or model hyperparameters, and plates indicate repeating structures. The black squares are the factor nodes and are associated with the potential functions corresponding to conditional independence among the variables. The model observes D documents (d) consisting of N tuples (p), each represented by a set of features f1,f2 . . . fk. z represents the relation type assignment to a tuple, is the relation type proportion for a given document, and the</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. PNAS, 101(1):52285235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hasegawa</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>Discovering relations among named entities from large corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>415422</pages>
<location>Barcelona,</location>
<contexts>
<context position="6371" citStr="Hasegawa et al., 2004" startWordPosition="977" endWordPosition="980">emi-supervised methods use a small number of seed instances or patterns (per relation) to launch an iterative training process (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Bunescu and Mooney, 2007; Pantel and Pennacchiotti, 2006). The seeds are used to extract a new set of patterns from a large corpus, which are then used to extract more instances, and so on. Unsupervised relation extraction methods are not limited to a predefined set of target relations, but discover all types of relations found in the text. The relations represent clusters over strings of words (Banko et al., 2007; Hasegawa et al., 2004), syntactic patterns between entities (Yao et al., 2011; Shinyama and Sekine, 2006), or logical expressions (Poon and Domingos, 2009). Another learning paradigm is distant supervision which does not require labeled data but instead access to a relational database such as Freebase (Mintz et al., 2009). The idea is to take entities that appear in some relation in the database, find the sentences that express the relation in an unlabeled corpus, and use them to train a relation classifier. Our own work adds an additional approach into the mix. We use a topic model to infer an arbitrary number of </context>
<context position="29703" citStr="Hasegawa et al. (2004)" startWordPosition="4969" endWordPosition="4972">DA. We obtained best results on the development set with the former scheme. Baselines We compared our FOL relational LDA model against standard LDA (Blei et al., 2003) and relational LDA without the FOL component. In the case of standard LDA, we estimated topics (relations) over words, and used the context of the entity mentions pairs as a bag of words feature to select the most likely cluster at test time. Parameters for LDA and relational LDA were optimized following the same parameter tuning procedure described above. We also compared our model against the unsupervised method introduced in Hasegawa et al. (2004). Their key idea is to cluster pairs of co-occurring named entities according to the similarity of their surrounding contexts. Following their approach, we measured context similarity using the vector space model and the cosine metric and grouped NE pairs into clusters using a complete linkage hierarchical clustering algorithm. We adopted the same parameter values as detailed in their paper (e.g., cosine similarity threshold, length of context vectors). At test time, instances were assigned to the relation cluster most similar to them (according to the cosine measure). Evaluation We evaluated </context>
<context position="35343" citStr="Hasegawa et al. (2004)" startWordPosition="5898" endWordPosition="5901">tity pairs only incurs a negligible drop in performance. This is good news for scaling purposes, since a small number of rules can greatly speed-up inference. Interestingly, model variants which use supervised FOL rules (see the prefix Sin Table 3) perform on par with unsupervised models. Again, MLT rules perform best in the supervised case, whereas CLT rules marginally improve over RelLDA. We assessed whether differences in performance are statistically significant (p &lt; 0.05) using bootstrap resampling (Noreen, 1989). All models across all relation types are significantly better than LDA and Hasegawa et al. (2004). FOL-based models perform significantly better than RelLDA, with the exception of all CLT models and U-CLT+MLT (ALL). MLT models are significantly better than any other rule-based model, except those that only use NEPAIR features. We also measured whether different models agree on their topic assignments using Cohens Kappa.2 RelLDA agrees least with MLT models and most with CLT models (i.e., = 0.50 for U-MLT (ALL) and = 0.65 for U-CLT (ALL)). This suggests that the CLT rules do not affect the output of RelLDA as much as MLT ones. Examples of relation clusters discovered by the U-MLT (ALL) mod</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman. 2004. Discovering relations among named entities from large corpora. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 415422, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Koller</author>
<author>N Friedman</author>
</authors>
<title>Probabilistic Graphical Models: Principles and Techniques.</title>
<date>2009</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11856" citStr="Koller and Friedman, 2009" startWordPosition="1857" endWordPosition="1860">ented by a multinomial distribution over features r (Dirichlet prior ). r is a vector with F dimensions each corresponding to a feature. Finally, documents (j = 1...D) are associated with a multinomial distribution j over relations (Dirichlet prior ). j is a vector with R dimensions, one for each relation. Figure 1 represents relational LDA model as a an undirected graphical model or factor graph (Bishop, 2006), ignoring for the moment the factor which connects the d, z, f1...k and o variables. Directed graphical models can be converted into undirected ones by adding edges between co-parents (Koller and Friedman, 2009). Each clique in the graph defines a potential function which replaces the conditional probabilities in the directed graph. Each maximal clique is associated with a special factor node (the black squares) and clique members are connected to that factor. The probability of any specific configuration is calculated by multiplying the potential functions and normalizing them. We adopt the factor graph representation as is it convenient for introducing logic rules into the model. The joint probability of the model given the priors and the documents (P(p, z, , |, , d)) is equivalent to: R Y r p(r|) </context>
</contexts>
<marker>Koller, Friedman, 2009</marker>
<rawString>D. Koller and N. Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>323328</pages>
<location>San Francisco, California.</location>
<contexts>
<context position="24794" citStr="Lin and Pantel, 2001" startWordPosition="4155" endWordPosition="4158"> F(j, TRIGGER:president) P(k, fi) P(l, fj) Z(k, r) Z(l, r) F(i, NEPAIR:PER-LOC) F(j, TRIGER:member) P(k, fi) P(l, fj) Z(k, r) Z(l, r) F(i, NEPAIR:PER-PER) F(j, TRIGER:sell) P(k, fi) P(l, fj) Z(k, r) Z(l, r) Table 2: Examples of automatically extracted Must-link and Cannot-link tuple rules. named entities were automatically recognized and labeled with PER, ORG, LOC, and MISC (Finkel et al., 2005). Dependency paths for each pair of named entity mentions were extracted from the output of the MaltParser (Nivre et al., 2004). In our experiments, we discarded tuples with paths longer than 10 edges (Lin and Pantel, 2001). We evaluated our model on the test partition of the ACE 2007 (English) RDC dataset which is labeled with gold standard entity mentions and their relations. There are six general relation types and 18 subtypes. We used 25% of the ACE training partition as a development set for parameter tuning. Logic Rule Extraction We automatically extracted logic rules from the New York Times (NYT) corpus as follows. The intuition behind Must-link rules is that tuples with common features should cluster together. Although we do not know which features would yield the best rules, we naively assume that good </context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT discovery of inference rules from text. In Proceedings of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 323328, San Francisco, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>10031011</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="6672" citStr="Mintz et al., 2009" startWordPosition="1023" endWordPosition="1026"> corpus, which are then used to extract more instances, and so on. Unsupervised relation extraction methods are not limited to a predefined set of target relations, but discover all types of relations found in the text. The relations represent clusters over strings of words (Banko et al., 2007; Hasegawa et al., 2004), syntactic patterns between entities (Yao et al., 2011; Shinyama and Sekine, 2006), or logical expressions (Poon and Domingos, 2009). Another learning paradigm is distant supervision which does not require labeled data but instead access to a relational database such as Freebase (Mintz et al., 2009). The idea is to take entities that appear in some relation in the database, find the sentences that express the relation in an unlabeled corpus, and use them to train a relation classifier. Our own work adds an additional approach into the mix. We use a topic model to infer an arbitrary number of relations between named entities. Although we do not have access to relation-specific information (either as a relational database or manually annotated data), we impose task-specific constraints which inject domain knowledge into the learning algorithm. We thus alleviate known problems with the inte</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 10031011, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 8th Conference on Computational Natural Language Learning,</booktitle>
<pages>4956</pages>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="24698" citStr="Nivre et al., 2004" startWordPosition="4139" endWordPosition="4142">i, NEPAIR:ORG-LOC) F(j, NEPAIR:PER-PER) P(k, fi) P(l, fj) Z(k, r) Z(l, r) F(i, NEPAIR:LOC-LOC) F(j, TRIGGER:president) P(k, fi) P(l, fj) Z(k, r) Z(l, r) F(i, NEPAIR:PER-LOC) F(j, TRIGER:member) P(k, fi) P(l, fj) Z(k, r) Z(l, r) F(i, NEPAIR:PER-PER) F(j, TRIGER:sell) P(k, fi) P(l, fj) Z(k, r) Z(l, r) Table 2: Examples of automatically extracted Must-link and Cannot-link tuple rules. named entities were automatically recognized and labeled with PER, ORG, LOC, and MISC (Finkel et al., 2005). Dependency paths for each pair of named entity mentions were extracted from the output of the MaltParser (Nivre et al., 2004). In our experiments, we discarded tuples with paths longer than 10 edges (Lin and Pantel, 2001). We evaluated our model on the test partition of the ACE 2007 (English) RDC dataset which is labeled with gold standard entity mentions and their relations. There are six general relation types and 18 subtypes. We used 25% of the ACE training partition as a development set for parameter tuning. Logic Rule Extraction We automatically extracted logic rules from the New York Times (NYT) corpus as follows. The intuition behind Must-link rules is that tuples with common features should cluster together.</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Proceedings of the 8th Conference on Computational Natural Language Learning, pages 4956, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-intensive Methods for Testing Hypotheses: An Introduction.</title>
<date>1989</date>
<publisher>John Wiley and Sons Inc.</publisher>
<contexts>
<context position="35244" citStr="Noreen, 1989" startWordPosition="5884" endWordPosition="5885"> model towards a better solution. Restricting the number of features and rules to named entity pairs only incurs a negligible drop in performance. This is good news for scaling purposes, since a small number of rules can greatly speed-up inference. Interestingly, model variants which use supervised FOL rules (see the prefix Sin Table 3) perform on par with unsupervised models. Again, MLT rules perform best in the supervised case, whereas CLT rules marginally improve over RelLDA. We assessed whether differences in performance are statistically significant (p &lt; 0.05) using bootstrap resampling (Noreen, 1989). All models across all relation types are significantly better than LDA and Hasegawa et al. (2004). FOL-based models perform significantly better than RelLDA, with the exception of all CLT models and U-CLT+MLT (ALL). MLT models are significantly better than any other rule-based model, except those that only use NEPAIR features. We also measured whether different models agree on their topic assignments using Cohens Kappa.2 RelLDA agrees least with MLT models and most with CLT models (i.e., = 0.50 for U-MLT (ALL) and = 0.65 for U-CLT (ALL)). This suggests that the CLT rules do not affect the ou</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-intensive Methods for Testing Hypotheses: An Introduction. John Wiley and Sons Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>113120</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="5987" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="908" endWordPosition="912">011) who employ manually crafted seeds. 2 Related Work A variety of learning paradigms have been applied to relation extraction. As mentioned earlier, supervised methods have been shown to perform well in this task. The reliance on manual annotation, which is expensive to produce and thus limited in quantity, has provided the impetus for semi-supervised and purely unsupervised approaches. Semi-supervised methods use a small number of seed instances or patterns (per relation) to launch an iterative training process (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Bunescu and Mooney, 2007; Pantel and Pennacchiotti, 2006). The seeds are used to extract a new set of patterns from a large corpus, which are then used to extract more instances, and so on. Unsupervised relation extraction methods are not limited to a predefined set of target relations, but discover all types of relations found in the text. The relations represent clusters over strings of words (Banko et al., 2007; Hasegawa et al., 2004), syntactic patterns between entities (Yao et al., 2011; Shinyama and Sekine, 2006), or logical expressions (Poon and Domingos, 2009). Another learning paradigm is distant supervision which does not require labeled d</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 113120, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>110</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="6504" citStr="Poon and Domingos, 2009" startWordPosition="996" endWordPosition="999">off and Jones, 1999; Agichtein and Gravano, 2000; Bunescu and Mooney, 2007; Pantel and Pennacchiotti, 2006). The seeds are used to extract a new set of patterns from a large corpus, which are then used to extract more instances, and so on. Unsupervised relation extraction methods are not limited to a predefined set of target relations, but discover all types of relations found in the text. The relations represent clusters over strings of words (Banko et al., 2007; Hasegawa et al., 2004), syntactic patterns between entities (Yao et al., 2011; Shinyama and Sekine, 2006), or logical expressions (Poon and Domingos, 2009). Another learning paradigm is distant supervision which does not require labeled data but instead access to a relational database such as Freebase (Mintz et al., 2009). The idea is to take entities that appear in some relation in the database, find the sentences that express the relation in an unlabeled corpus, and use them to train a relation classifier. Our own work adds an additional approach into the mix. We use a topic model to infer an arbitrary number of relations between named entities. Although we do not have access to relation-specific information (either as a relational database or</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 110, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<volume>62</volume>
<issue>12</issue>
<contexts>
<context position="7843" citStr="Richardson and Domingos, 2006" startWordPosition="1212" endWordPosition="1216">g algorithm. We thus alleviate known problems with the interpretability of the clusters obtained from topic models and are able to guide our model towards reasonable relations. Andrzejewski et al. (2011) show how to integrate First-Order Logic with vanilla LDA. We extend their formulation to relation tuples rather than individual words. Our model generates a corpus of entity tuples which are in turn represented by features and uses automatically acquired FOL rules. The idea of integrating topic modeling with FOL builds on research in probabilistic logic modeling such as Markov Logic Networks (Richardson and Domingos, 2006). Schoenmackers et al. (2010) learn Horn clauses from web-scale text with aim of finding answers to a users query. Our work is complementary to theirs. We could make use of their rules to discover more accurate relations. The general goal of assisting the learner in recovering the correct clustering by supplying additional domain knowledge is not new. Gondek and Hofmann (2004) supply a known clustering they do not want the learner to return, whereas Wagstaff et al. (2001) use pairwise labels for items indicating whether they belong in the same cluster. These methods combine domain knowledge wi</context>
<context position="15849" citStr="Richardson and Domingos, 2006" startWordPosition="2553" endWordPosition="2556">en two entity mentions. In our sentence, the value of the feature would be PATH:nsubjtraveledpreptopobj. TRIGGER Finally, trigger features are content words occurring in the dependency path. The path PATH:nsubjtraveledpreptopobj contains only one trigger word, namely traveled. The intuition behind this feature is that paths sharing the same set of trigger words should be grouped in the same cluster. 4.2 First Order Logic and Relational LDA We next couple relational LDA with global constraints, which we express using FOL rules. We begin by representing relational LDA as a Markov Logic Network (Richardson and Domingos, 2006). We define a logical predicate for each model variable. For example, assigned relation variable (Z(i, r)) is true if zi = r and false otherwise. Table 1 shows the mapping of model variables onto logical predicates. Logical rules are encoded in the form of a weighted FOL knowledge base (KB) which is then converted into Conjunctive Normal form: KB = {(1, 1), ..., (L, L)} (2) The KB consists of L pairs, where each l represents a FOL rule and l 0 its weight. Rules are soft preferences rather than hard constraints; the weights represent the importance of l and are 418 \x0cset manually by the domai</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine Learning, 62(12):107136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>474479</pages>
<location>Stockholm,</location>
<contexts>
<context position="5899" citStr="Riloff and Jones, 1999" startWordPosition="896" endWordPosition="899"> the logic rules automatically from a corpus contrary to Andrzejewski et al. (2011) who employ manually crafted seeds. 2 Related Work A variety of learning paradigms have been applied to relation extraction. As mentioned earlier, supervised methods have been shown to perform well in this task. The reliance on manual annotation, which is expensive to produce and thus limited in quantity, has provided the impetus for semi-supervised and purely unsupervised approaches. Semi-supervised methods use a small number of seed instances or patterns (per relation) to launch an iterative training process (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Bunescu and Mooney, 2007; Pantel and Pennacchiotti, 2006). The seeds are used to extract a new set of patterns from a large corpus, which are then used to extract more instances, and so on. Unsupervised relation extraction methods are not limited to a predefined set of target relations, but discover all types of relations found in the text. The relations represent clusters over strings of words (Banko et al., 2007; Hasegawa et al., 2004), syntactic patterns between entities (Yao et al., 2011; Shinyama and Sekine, 2006), or logical expressions (Poon and Domingos, </context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction. In Proceedings of the 16th International Joint Conference on Artificial Intelligence, pages 474479, Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Jesse Davis</author>
<author>Oren Etzioni</author>
<author>Daniel Weld</author>
</authors>
<title>Learning first-order Horn clauses from web text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>10881098</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="7872" citStr="Schoenmackers et al. (2010)" startWordPosition="1217" endWordPosition="1220">nown problems with the interpretability of the clusters obtained from topic models and are able to guide our model towards reasonable relations. Andrzejewski et al. (2011) show how to integrate First-Order Logic with vanilla LDA. We extend their formulation to relation tuples rather than individual words. Our model generates a corpus of entity tuples which are in turn represented by features and uses automatically acquired FOL rules. The idea of integrating topic modeling with FOL builds on research in probabilistic logic modeling such as Markov Logic Networks (Richardson and Domingos, 2006). Schoenmackers et al. (2010) learn Horn clauses from web-scale text with aim of finding answers to a users query. Our work is complementary to theirs. We could make use of their rules to discover more accurate relations. The general goal of assisting the learner in recovering the correct clustering by supplying additional domain knowledge is not new. Gondek and Hofmann (2004) supply a known clustering they do not want the learner to return, whereas Wagstaff et al. (2001) use pairwise labels for items indicating whether they belong in the same cluster. These methods combine domain knowledge with statistical learning in or</context>
</contexts>
<marker>Schoenmackers, Davis, Etzioni, Weld, 2010</marker>
<rawString>Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and Daniel Weld. 2010. Learning first-order Horn clauses from web text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 10881098, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive information extraction using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>304311</pages>
<location>New York City, USA.</location>
<contexts>
<context position="6454" citStr="Shinyama and Sekine, 2006" startWordPosition="989" endWordPosition="992">lation) to launch an iterative training process (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Bunescu and Mooney, 2007; Pantel and Pennacchiotti, 2006). The seeds are used to extract a new set of patterns from a large corpus, which are then used to extract more instances, and so on. Unsupervised relation extraction methods are not limited to a predefined set of target relations, but discover all types of relations found in the text. The relations represent clusters over strings of words (Banko et al., 2007; Hasegawa et al., 2004), syntactic patterns between entities (Yao et al., 2011; Shinyama and Sekine, 2006), or logical expressions (Poon and Domingos, 2009). Another learning paradigm is distant supervision which does not require labeled data but instead access to a relational database such as Freebase (Mintz et al., 2009). The idea is to take entities that appear in some relation in the database, find the sentences that express the relation in an unlabeled corpus, and use them to train a relation classifier. Our own work adds an additional approach into the mix. We use a topic model to infer an arbitrary number of relations between named entities. Although we do not have access to relation-specif</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation discovery. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 304311, New York City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
</authors>
<title>Robust information extration with perceptrons.</title>
<date>2007</date>
<booktitle>In Proceedings of the NIST</booktitle>
<contexts>
<context position="1666" citStr="Surdeanu and Ciaramita, 2007" startWordPosition="241" endWordPosition="245">ally for the relation extraction task. Evaluation results on the ACE 2007 English Relation Detection and Categorization (RDC) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules. 1 Introduction Information extraction (IE) is becoming increasingly useful as a form of shallow semantic analysis. Learning relational facts from text is one of the core tasks of IE and has applications in a variety of fields including summarization, question answering, and information retrieval. Previous work (Surdeanu and Ciaramita, 2007; Culotta and Sorensen, 2004; Zhou et al., 2007) has traditionally relied on extensive human involvement (e.g., hand-annotated training instances, manual pattern extraction rules, hand-picked seeds). Standard supervised techniques can yield high performance when large amounts of hand-labeled data are available for a fixed inventory of relation types (e.g., Employment, Located), however, extraction systems do not easily generalize beyond their training domains and often must be re-engineered for each application. Unsupervised approaches offer a promising alternative which could lead to signific</context>
</contexts>
<marker>Surdeanu, Ciaramita, 2007</marker>
<rawString>Mihai Surdeanu and Massimiliano Ciaramita. 2007. Robust information extration with perceptrons. In Proceedings of the NIST 2007 Automatic Content Extraction Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiri Wagstaff</author>
<author>Claire Cardie</author>
<author>C Rogers</author>
<author>S Schrodl</author>
</authors>
<title>Constrained k-means clustering with background knowledge.</title>
<date>2001</date>
<booktitle>In International Conference on Machine Learning,</booktitle>
<pages>577584</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="8319" citStr="Wagstaff et al. (2001)" startWordPosition="1293" endWordPosition="1296">tegrating topic modeling with FOL builds on research in probabilistic logic modeling such as Markov Logic Networks (Richardson and Domingos, 2006). Schoenmackers et al. (2010) learn Horn clauses from web-scale text with aim of finding answers to a users query. Our work is complementary to theirs. We could make use of their rules to discover more accurate relations. The general goal of assisting the learner in recovering the correct clustering by supplying additional domain knowledge is not new. Gondek and Hofmann (2004) supply a known clustering they do not want the learner to return, whereas Wagstaff et al. (2001) use pairwise labels for items indicating whether they belong in the same cluster. These methods combine domain knowledge with statistical learning in order to improve performance with re416 \x0cspect to the true target clustering. Although, the target labels are not available in our case, we are able to show that the inclusion of domain knowledge yields clustering improvements. 3 Learning Setting Our relation extraction task broadly adheres to the ACE specification guidelines. Our aim is to detect and characterize the semantic relations between two named entities. The input to our model is a </context>
</contexts>
<marker>Wagstaff, Cardie, Rogers, Schrodl, 2001</marker>
<rawString>Kiri Wagstaff, Claire Cardie, C Rogers, and S Schrodl. 2001. Constrained k-means clustering with background knowledge. In International Conference on Machine Learning, pages 577584. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Aria Haghighi</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Structured relation discovery using generative models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>14561466</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="2453" citStr="Yao et al. (2011)" startWordPosition="356" endWordPosition="359">on rules, hand-picked seeds). Standard supervised techniques can yield high performance when large amounts of hand-labeled data are available for a fixed inventory of relation types (e.g., Employment, Located), however, extraction systems do not easily generalize beyond their training domains and often must be re-engineered for each application. Unsupervised approaches offer a promising alternative which could lead to significant resource savings and more portable extraction systems. It therefore comes as no surprise that latent topic analysis methods have been used for a variety of IE tasks. Yao et al. (2011), for example, propose a series of topic models which perform relation discovery by clustering tuples representing an observed syntactic relationship between two named entities (e.g., X was born in Y and X is from Y). The clusters correspond to semantic relations whose number or type is not known in advance. Their models depart from standard Latent Dirichlet Allocation (Blei et al., 2003) in that a document consists of relation tuples rather than individual words; moreover, tuples have features each of which is generated independently from a hidden relation (e.g., the words corresponding to th</context>
<context position="6426" citStr="Yao et al., 2011" startWordPosition="985" endWordPosition="988">r patterns (per relation) to launch an iterative training process (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Bunescu and Mooney, 2007; Pantel and Pennacchiotti, 2006). The seeds are used to extract a new set of patterns from a large corpus, which are then used to extract more instances, and so on. Unsupervised relation extraction methods are not limited to a predefined set of target relations, but discover all types of relations found in the text. The relations represent clusters over strings of words (Banko et al., 2007; Hasegawa et al., 2004), syntactic patterns between entities (Yao et al., 2011; Shinyama and Sekine, 2006), or logical expressions (Poon and Domingos, 2009). Another learning paradigm is distant supervision which does not require labeled data but instead access to a relational database such as Freebase (Mintz et al., 2009). The idea is to take entities that appear in some relation in the database, find the sentences that express the relation in an unlabeled corpus, and use them to train a relation classifier. Our own work adds an additional approach into the mix. We use a topic model to infer an arbitrary number of relations between named entities. Although we do not ha</context>
<context position="9939" citStr="Yao et al. (2011)" startWordPosition="1548" endWordPosition="1551">along a path in the dependency tree. For example, the sentence George Bush traveled to France on Thursday for a summit. would yield the tuple [SOURCE:George Bush(PER), PATH:nsubjtraveledpreptopobj, DES:France(LOC)]. The tuple here expresses the relation Located, however our model does not observe any relation labels during training. The model assigns tuples to clusters, corresponding to an underlying relation type. Each tuple instance can be then labeled with an identifier corresponding to the cluster (aka relation) it has been assigned to. 4 Modeling Framework Our model builds on the work of Yao et al. (2011) who develop a series of generative probabilistic models for relation extraction. Specifically, we extend their relational LDA model by interfacing it with FOL-rules. In the following, we first describe their approach in more detail and then present our extensions and modifications. 4.1 Relational LDA Relational LDA is an extension to LDA with a similar generative story. LDA models each document as a mixture of topics, which are in turn characterized as distributions over words. In relational LDA, each document is a mixture of relations over tuples representing syntactic relations between two </context>
<context position="14351" citStr="Yao et al. (2011)" startWordPosition="2291" endWordPosition="2294">case, our model sees the corpus (p, d), where d is the variable representing the document and the tuples (p) are represented by a set of features f1,f2 . . . fk in the graph. Empty circles are associated with latent variables to be estimated: z represents the relation type assignment to the tuple, is the relation type proportion for the given document, and is the relation type distribution over the features. The features representing the tuples tap onto semantic information expressed by different surface forms and are an important part of the model. We use a subset of the features proposed in Yao et al. (2011) which we briefly describe below: SOURCE This feature corresponds to the first entity mention of the tuple. In the sentence George Bush traveled to France on Thursday for a summit., the value of this feature would be George Bush. Value Predicate Description zi = r Z(i, r) Latent relation type fk = v F(k, v) feature of relation tuple pi = i P(i, fk) tuple i contains feature fk di = j D(i, j) observed document Table 1: Logical variables for Relational LDA. The variable i ranges over tuples in the corpus (i = [1 . . . N]), and k over features in the corpus (k = [1 . . . F]). DEST The feature corr</context>
<context position="23582" citStr="Yao et al. (2011)" startWordPosition="3971" endWordPosition="3974"> rules express knowledge about the task at hand, the domain involved, and the way the relation extraction problem is modeled (i.e., tuples expressed as features). So far, we have abstractly formulated the rules without explaining how they are specifically instantiated in our model. We could write them down by hand after inspecting some data or through consultation with a domain expert. Instead, we obtain logic rules automatically from a corpus following the procedure described in Section 5. 5 Experimental Setup Data We trained our model on the New York Times (years 20002007) corpus created by Yao et al. (2011). The corpus contains approximately 2M entity tuples. The latter were extracted from 428K documents. After post-processing (tokenization, sentence-splitting, and part-of-speech tagging), 420 \x0cMust-link Tuple F(i, NEPAIR:PER-PER, TRIGGER:wife) P(j, fi) P(k, fi) Z(j, t) Z(k, r) F(i, NEPAIR:PER-LOC, TRIGGER:die) P(j, fi) P(k, fi) Z(j, t) Z(k, r) F(i, PATH:nsubjdieprepinpobj) P(j, fi) P(k, fi) Z(j, t) Z(k, r) F(i, SOURCE:Kobe, DEST:Lakers) P(j, fi) P(k, fi) Z(j, t) Z(k, r) Cannot-link Tuple F(i, NEPAIR:ORG-LOC) F(j, NEPAIR:PER-PER) P(k, fi) P(l, fj) Z(k, r) Z(l, r) F(i, NEPAIR:LOC-LOC) F(j, TRI</context>
</contexts>
<marker>Yao, Haghighi, Riedel, McCallum, 2011</marker>
<rawString>Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew McCallum. 2011. Structured relation discovery using generative models. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 14561466, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
<author>Min Zhang</author>
<author>DongHong Ji</author>
<author>QiaoMing Zhu</author>
</authors>
<title>Tree kernel-based relation extraction with context-sensitive structured parse tree information.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>728736</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1714" citStr="Zhou et al., 2007" startWordPosition="250" endWordPosition="253">on the ACE 2007 English Relation Detection and Categorization (RDC) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules. 1 Introduction Information extraction (IE) is becoming increasingly useful as a form of shallow semantic analysis. Learning relational facts from text is one of the core tasks of IE and has applications in a variety of fields including summarization, question answering, and information retrieval. Previous work (Surdeanu and Ciaramita, 2007; Culotta and Sorensen, 2004; Zhou et al., 2007) has traditionally relied on extensive human involvement (e.g., hand-annotated training instances, manual pattern extraction rules, hand-picked seeds). Standard supervised techniques can yield high performance when large amounts of hand-labeled data are available for a fixed inventory of relation types (e.g., Employment, Located), however, extraction systems do not easily generalize beyond their training domains and often must be re-engineered for each application. Unsupervised approaches offer a promising alternative which could lead to significant resource savings and more portable extractio</context>
</contexts>
<marker>Zhou, Zhang, Ji, Zhu, 2007</marker>
<rawString>GuoDong Zhou, Min Zhang, DongHong Ji, and QiaoMing Zhu. 2007. Tree kernel-based relation extraction with context-sensitive structured parse tree information. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 728736, Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>