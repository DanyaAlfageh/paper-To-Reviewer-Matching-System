ically from a corpus contrary to CITATION who employ manually crafted seeds.,,
Semi-supervised methods use a small number of seed instances or patterns (per relation) to launch an iterative training process (CITATION; CITATION; CITATION; CITATION).,,
The relations represent clusters over strings of words (CITATION; CITATION), syntactic patterns between entities (CITATION; CITATION), or logical expressions CITATION.,,
Evaluation We evaluated the clusters obtained by our model and the comparison systems using the Fscore measure introduced in the SemEval 2007 task CITATION; it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively.,,
6 Results Our results are summarized in Table 3 which reports Fscore for CITATION, LDA, relational LDA (RelLDA), and our model with the FOL component.,,
Specifically, we extend the Fold-all (FirstOrder Logic latent Dirichlet Allocation) framework CITATION to the relation extraction task, explain how to incorporate meaningful constraints, and develop a scalable inference technique.,,
CITATION show how to integrate First-Order Logic with vanilla LDA.,,
The idea of integrating topic modeling with FOL builds on research in probabilistic logic modeling such as Markov Logic Networks CITATION.,,
CITATION learn Horn clauses from web-scale text with aim of finding answers to a users query.,,
Contrary to CITATION, we need to ground the rules while taking into account if the feature specified in the rule is expressed by any tuple or the specific given tuple, since we are assigning relations to tuples, and not directly to words.,,
As z can not be marginalized out, we proceed with MAP estimation of (z, , ), maximizing the log of the probability as in CITATION: arg max z,, L X l X gG(l) l1g(z, p, d, o)+ R X r log p(r|)+ N X i log di (zi) Y kpi zi (fk) (5) Once the parameters of the model are estimated (see Section 4.3 for details), we use the probability distribution to assign a relation to a new test tuple.,,
We follow CITATION in relaxing (5) into a continuous optimization problem and refer the reader to their paper for a more in depth treatment.,,
Suffice it to say that once the binary variables zir {0, 1} are relaxed to continuous values zir [0, 1], it is possible to introduce the relational LDA term in the equation and compute the gradient using the Entropic Mirror Descent Algorithm CITATION: arg max z[0,1]|KB| L X l X gG(l) l1g(z)+ X i,r zir log di (r) Y kpi zi (fk) s.t zir 0 , X i,r zir = 1 (9) In every iteration the approximation algorithm randomly samples a term from the objective function (Eq,,
Semi-supervised methods use a small number of seed instances or patterns (per relation) to launch an iterative training process (CITATION; CITATION; CITATION; CITATION).,,
The relations represent clusters over strings of words (CITATION; CITATION), syntactic patterns between entities (CITATION; CITATION), or logical expressions CITATION.,,
Another learning paradigm is distant supervision which does not require labeled data but instead access to a relational database such as Freebase CITATION.,,
We follow CITATION in relaxing (5) into a continuous optimization problem and refer the reader to their paper for a more in depth treatment.,,
Suffice it to say that once the binary variables zir {0, 1} are relaxed to continuous values zir [0, 1], it is possible to introduce the relational LDA term in the equation and compute the gradient using the Entropic Mirror Descent Algorithm CITATION: arg max z[0,1]|KB| L X l X gG(l) l1g(z)+ X i,r zir log di (r) Y kpi zi (fk) s.t zir 0 , X i,r zir = 1 (9) In every iteration the approximation algorithm randomly samples a term from the objective function (Equation (9)).,,
Figure 1 represents relational LDA model as a an undirected graphical model or factor graph CITATION, ignoring for the moment the factor which connects the d, z, f1...k and o variables.,,
Directed graphical models can be converted into undirected ones by adding edges between co-parents CITATION.,,
CITATION, for example, propose a series of topic models which perform relation discovery by clustering tuples representing an observed syntactic relationship between two named entities (e.g., X was born in Y and X is from Y).,,
Their models depart from standard Latent Dirichlet Allocation CITATION in that a document consists of relation tuples rather than individual words; moreover, tuples have features each of which is generated independently from a hidden relation (e.g., the words corresponding to the first and second entities, the type and order of the named entities).,,
Baselines We compared our FOL relational LDA model against standard LDA CITATION and relational LDA without the FOL component.,,
We also compared our model against the unsupervised method introduced in CITATION.,,
 to CITATION who employ manually crafted seeds.,,
Semi-supervised methods use a small number of seed instances or patterns (per relation) to launch an iterative training process (CITATION; CITATION; CITATION; CITATION).,,
The relations represent clusters over strings of words (CITATION; CITATION), syntactic patterns between entities (CITATION; CITATION), or logical expressions CITATION.,,
Previous work (CITATION; CITATION; CITATION) has traditionally relied on extensive human involvement (e.g., hand-annotated training instances, manual pattern extraction rules, hand-picked seeds).,,
Using the log-likelihood ratio CITATION, we first discarded low confidence feature co-occurrences (p < 0.05).,,
named entities were automatically recognized and labeled with PER, ORG, LOC, and MISC CITATION.,,
Dependency paths for each pair of named entity mentions were extracted from the output of the MaltParser CITATION.,,
In our experiments, we discarded tuples with paths longer than 10 edges CITATION.,,
The idea of integrating topic modeling with FOL builds on research in probabilistic logic modeling such as Markov Logic Networks CITATION.,,
CITATION learn Horn clauses from web-scale text with aim of finding answers to a users query.,,
CITATION supply a known clustering they do not want the learner to return, whereas CITATION use pairwise labels for items indicating whether they belong in the same cluster.,,
The parameters of the latent variables (e.g., , ) are typically estimated using an approximate inference algorithm such as Gibbs Sampling CITATION.,,
emi-supervised methods use a small number of seed instances or patterns (per relation) to launch an iterative training process (CITATION; CITATION; CITATION; CITATION).,,
The relations represent clusters over strings of words (CITATION; CITATION), syntactic patterns between entities (CITATION; CITATION), or logical expressions CITATION.,,
Another learning paradigm is distant supervision which does not require labeled data but instead access to a relational database such as Freebase CITATION.,,
Baselines We compared our FOL relational LDA model against standard LDA CITATION and relational LDA without the FOL component.,,
We also compared our model against the unsupervised method introduced in CITATION.,,
We assessed whether differences in performance are statistically significant (p < 0.05) using bootstrap resampling CITATION.,,
All models across all relation types are significantly better than LDA and CITATION.,,
Figure 1 represents relational LDA model as a an undirected graphical model or factor graph CITATION, ignoring for the moment the factor which connects the d, z, f1...k and o variables.,,
Directed graphical models can be converted into undirected ones by adding edges between co-parents CITATION.,,
named entities were automatically recognized and labeled with PER, ORG, LOC, and MISC CITATION.,,
Dependency paths for each pair of named entity mentions were extracted from the output of the MaltParser CITATION.,,
In our experiments, we discarded tuples with paths longer than 10 edges CITATION.,,
The relations represent clusters over strings of words (CITATION; CITATION), syntactic patterns between entities (CITATION; CITATION), or logical expressions CITATION.,,
Another learning paradigm is distant supervision which does not require labeled data but instead access to a relational database such as Freebase CITATION.,,
named entities were automatically recognized and labeled with PER, ORG, LOC, and MISC CITATION.,,
Dependency paths for each pair of named entity mentions were extracted from the output of the MaltParser CITATION.,,
In our experiments, we discarded tuples with paths longer than 10 edges CITATION.,,
We assessed whether differences in performance are statistically significant (p < 0.05) using bootstrap resampling CITATION.,,
All models across all relation types are significantly better than LDA and CITATION.,,
Semi-supervised methods use a small number of seed instances or patterns (per relation) to launch an iterative training process (CITATION; CITATION; CITATION; CITATION).,,
The relations represent clusters over strings of words (CITATION; CITATION), syntactic patterns between entities (CITATION; CITATION), or logical expressions CITATION.,,
off and Jones, 1999; CITATION; CITATION; CITATION).,,
The relations represent clusters over strings of words (CITATION; CITATION), syntactic patterns between entities (CITATION; CITATION), or logical expressions CITATION.,,
Another learning paradigm is distant supervision which does not require labeled data but instead access to a relational database such as Freebase CITATION.,,
CITATION show how to integrate First-Order Logic with vanilla LDA.,,
The idea of integrating topic modeling with FOL builds on research in probabilistic logic modeling such as Markov Logic Networks CITATION.,,
CITATION learn Horn clauses from web-scale text with aim of finding answers to a users query.,,
CITATION supply a known clustering they do not want the learner to return, whereas CITATION use pairwise labels for items indicating whether they belong in the same cluster.,,
We begin by representing relational LDA as a Markov Logic Network CITATION.,,
 the logic rules automatically from a corpus contrary to CITATION who employ manually crafted seeds.,,
Semi-supervised methods use a small number of seed instances or patterns (per relation) to launch an iterative training process (CITATION; CITATION; CITATION; CITATION).,,
The relations represent clusters over strings of words (CITATION; CITATION), syntactic patterns between entities (CITATION; CITATION), or logical expressions (Poon and Domingos,,,
CITATION show how to integrate First-Order Logic with vanilla LDA.,,
The idea of integrating topic modeling with FOL builds on research in probabilistic logic modeling such as Markov Logic Networks CITATION.,,
CITATION learn Horn clauses from web-scale text with aim of finding answers to a users query.,,
CITATION supply a known clustering they do not want the learner to return, whereas CITATION use pairwise labels for items indicating whether they belong in the same cluster.,,
lation) to launch an iterative training process (CITATION; CITATION; CITATION; CITATION).,,
The relations represent clusters over strings of words (CITATION; CITATION), syntactic patterns between entities (CITATION; CITATION), or logical expressions CITATION.,,
Another learning paradigm is distant supervision which does not require labeled data but instead access to a relational database such as Freebase CITATION.,,
Previous work (CITATION; CITATION; CITATION) has traditionally relied on extensive human involvement (e.g., hand-annotated training instances, manual pattern extraction rules, hand-picked seeds).,,
tegrating topic modeling with FOL builds on research in probabilistic logic modeling such as Markov Logic Networks CITATION.,,
CITATION learn Horn clauses from web-scale text with aim of finding answers to a users query.,,
CITATION supply a known clustering they do not want the learner to return, whereas CITATION use pairwise labels for items indicating whether they belong in the same cluster.,,
CITATION, for example, propose a series of topic models which perform relation discovery by clustering tuples representing an observed syntactic relationship between two named entities (e.g., X was born in Y and X is from Y).,,
Their models depart from standard Latent Dirichlet Allocation CITATION in that a document consists of relation tuples rather than individual words; moreover, tuples have features each of which is generated independently from a hidden relation (e.g., the words corresponding to th,,
r patterns (per relation) to launch an iterative training process (CITATION; CITATION; CITATION; CITATION).,,
The relations represent clusters over strings of words (CITATION; CITATION), syntactic patterns between entities (CITATION; CITATION), or logical expressions CITATION.,,
Another learning paradigm is distant supervision which does not require labeled data but instead access to a relational database such as Freebase CITATION.,,
4 Modeling Framework Our model builds on the work of CITATION who develop a series of generative probabilistic models for relation extraction.,,
We use a subset of the features proposed in CITATION which we briefly describe below: SOURCE This feature corresponds to the first entity mention of the tuple.,,
5 Experimental Setup Data We trained our model on the New York Times (years 20002007) corpus created by CITATION.,,
Previous work (CITATION; CITATION; CITATION) has traditionally relied on extensive human involvement (e.g., hand-annotated training instances, manual pattern extraction rules, hand-picked seeds).,,
