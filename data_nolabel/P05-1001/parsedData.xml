<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.641096">
b&amp;apos;Proceedings of the 43rd Annual Meeting of the ACL, pages 19,
Ann Arbor, June 2005. c
</bodyText>
<sectionHeader confidence="0.401459" genericHeader="abstract">
2005 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.959364">
A High-Performance Semi-Supervised Learning Method for Text Chunking
</title>
<author confidence="0.57576">
Rie Kubota Andoy Tong Zhangz
</author>
<affiliation confidence="0.404479">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.523946">
Yorktown Heights, NY 10598, U.S.A.
</address>
<email confidence="0.814267">
yrie1@us.ibm.com ztongz@us.ibm.com
</email>
<sectionHeader confidence="0.983088" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.988982318181818">
In machine learning, whether one can
build a more accurate classifier by using
unlabeled data (semi-supervised learning)
is an important issue. Although a num-
ber of semi-supervised methods have been
proposed, their effectiveness on NLP tasks
is not always clear. This paper presents
a novel semi-supervised method that em-
ploys a learning paradigm which we call
structural learning. The idea is to find
what good classifiers are like by learn-
ing from thousands of automatically gen-
erated auxiliary classification problems on
unlabeled data. By doing so, the common
predictive structure shared by the multiple
classification problems can be discovered,
which can then be used to improve perfor-
mance on the target problem. The method
produces performance higher than the pre-
vious best results on CoNLL00 syntac-
tic chunking and CoNLL03 named entity
chunking (English and German).
</bodyText>
<sectionHeader confidence="0.997771" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999515869565217">
In supervised learning applications, one can often
find a large amount of unlabeled data without diffi-
culty, while labeled data are costly to obtain. There-
fore, a natural question is whether we can use unla-
beled data to build a more accurate classifier, given
the same amount of labeled data. This problem is
often referred to as semi-supervised learning.
Although a number of semi-supervised methods
have been proposed, their effectiveness on NLP
tasks is not always clear. For example, co-training
(Blum and Mitchell, 1998) automatically bootstraps
labels, and such labels are not necessarily reliable
(Pierce and Cardie, 2001). A related idea is to
use Expectation Maximization (EM) to impute la-
bels. Although useful under some circumstances,
when a relatively large amount of labeled data is
available, the procedure often degrades performance
(e.g. Merialdo (1994)). A number of bootstrap-
ping methods have been proposed for NLP tasks
(e.g. Yarowsky (1995), Collins and Singer (1999),
Riloff and Jones (1999)). But these typically assume
a very small amount of labeled data and have not
been shown to improve state-of-the-art performance
when a large amount of labeled data is available.
Our goal has been to develop a general learning
framework for reliably using unlabeled data to im-
prove performance irrespective of the amount of la-
beled data available. It is exactly this important and
difficult problem that we tackle here.
This paper presents a novel semi-supervised
method that employs a learning framework called
structural learning (Ando and Zhang, 2004), which
seeks to discover shared predictive structures (i.e.
what good classifiers for the task are like) through
jointly learning multiple classification problems on
unlabeled data. That is, we systematically create
thousands of problems (called auxiliary problems)
relevant to the target task using unlabeled data, and
train classifiers from the automatically generated
training data. We learn the commonality (or struc-
ture) of such many classifiers relevant to the task,
and use it to improve performance on the target task.
One example of such auxiliary problems for chunk-
ing tasks is to mask a word and predict whether
it is people or not from the context, like language
modeling. Another example is to predict the pre-
</bodyText>
<page confidence="0.909312">
1
</page>
<bodyText confidence="0.997089529411764">
\x0cdiction of some classifier trained for the target task.
These auxiliary classifiers can be adequately learned
since we have very large amounts of training data
for them, which we automatically generate from a
very large amount of unlabeled data.
The contributions of this paper are two-fold. First,
we present a novel robust semi-supervised method
based on a new learning model and its application
to chunking tasks. Second, we report higher per-
formance than the previous best results on syntactic
chunking (the CoNLL00 corpus) and named entity
chunking (the CoNLL03 English and German cor-
pora). In particular, our results are obtained by us-
ing unlabeled data as the only additional resource
while many of the top systems rely on hand-crafted
resources such as large name gazetteers or even rule-
based post-processing.
</bodyText>
<sectionHeader confidence="0.794417" genericHeader="method">
2 A Model for Learning Structures
</sectionHeader>
<bodyText confidence="0.9941216">
This work uses a linear formulation of structural
learning. We first briefly review a standard linear
prediction model and then extend it for structural
learning. We sketch an optimization algorithm us-
ing SVD and compare it to related methods.
</bodyText>
<subsectionHeader confidence="0.973728">
2.1 Standard linear prediction model
</subsectionHeader>
<bodyText confidence="0.988934058823529">
In the standard formulation of supervised learning,
we seek a predictor that maps an input vector x 2 X
to the corresponding output y 2 Y. Linear predic-
tion models are based on real-valued predictors of
the form f(x) = wT
x, where w is called a weight
vector. For binary problems, the sign of the linear
prediction gives the class label. For k-way classi-
fication (with k &gt; 2), a typical method is winner
takes all, where we train one predictor per class and
choose the class with the highest output value.
A frequently used method for finding an accurate
predictor ^
f is regularized empirical risk minimiza-
tion (ERM), which minimizes an empirical loss of
the predictor (with regularization) on the n training
examples f(Xi;Yi)g:
</bodyText>
<equation confidence="0.955735555555555">
^
f = argmin
f
n
X
i=1
L(f(Xi);Yi)+r(f)
!
:
</equation>
<bodyText confidence="0.988436571428571">
L(\x01) is a loss function to quantify the difference
between the prediction f(Xi) and the true output
Yi, and r(\x01) is a regularization term to control the
model complexity. ERM-based methods for dis-
criminative learning are known to be effective for
NLP tasks such as chunking (e.g. Kudoh and Mat-
sumoto (2001), Zhang and Johnson (2003)).
</bodyText>
<subsectionHeader confidence="0.685977">
2.2 Linear model for structural learning
</subsectionHeader>
<bodyText confidence="0.987872666666667">
We present a linear prediction model for structural
learning, which extends the traditional model to
multiple problems. Specifically, we assume that
there exists a low-dimensional predictive structure
shared by multiple prediction problems. We seek to
discover this structure through joint empirical risk
minimization over the multiple problems.
Consider m problems indexed by ` 2 f1;::: ;mg,
each with n` samples (X`
</bodyText>
<equation confidence="0.998479">
i
;Y `
i
</equation>
<bodyText confidence="0.875339333333333">
) indexed by i 2
f1;::: ;n`g. In our joint linear model, a predictor
for problem ` takes the following form
</bodyText>
<equation confidence="0.999826166666667">
f`(\x02;x) = wT
`
x+vT
`
\x02x ; \x02\x02T
= I; (1)
</equation>
<bodyText confidence="0.992862571428571">
where we use I to denote the identity matrix. Ma-
trix \x02 (whose rows are orthonormal) is the common
structure parameter shared by all the problems; w`
and v` are weight vectors specific to each predic-
tion problem `. The idea of this model is to dis-
cover a common low-dimensional predictive struc-
ture (shared by the m problems) parameterized by
the projection matrix \x02. In this setting, the goal of
structural learning may also be regarded as learning
a good feature map \x02x a low-dimensional fea-
ture vector parameterized by \x02.
In joint ERM, we seek \x02(and weight vectors) that
minimizes the empirical risk summed over all the
problems:
</bodyText>
<equation confidence="0.976590764705882">
[^
\x02;f^
f`g =argmin
\x02;ff`g
m
X
`=1
n
X
i=1
L(f`(\x02;X`
i);Y `
i )
n`
+r(f`)
!
:
</equation>
<bodyText confidence="0.964742714285714">
(2)
It can be shown that using joint ERM, we can reli-
ably estimate the optimal joint parameter \x02 as long
as m is large (even when each n` is small). This is
the key reason why structural learning is effective.
A formal PAC-style analysis can be found in (Ando
and Zhang, 2004).
</bodyText>
<subsectionHeader confidence="0.993192">
2.3 Alternating structure optimization (ASO)
</subsectionHeader>
<bodyText confidence="0.966374">
The optimization problem (2) has a simple solution
using SVD when we choose square regularization:
</bodyText>
<page confidence="0.697243">
2
</page>
<equation confidence="0.8589255">
\x0cr(f`) = \x15kw`k2
2 , where the regularization parame-
</equation>
<bodyText confidence="0.738868">
ter \x15 is given. For clarity, let u` be a weight vector
for problem ` such that: u` = w` + \x02T
v` : Then,
(2) becomes the minimization of the joint empirical
risk written as:
</bodyText>
<equation confidence="0.9941849375">
m
X
`=1
n
X
i=1
L(uT
` X`
i;Y `
i )
n`
+\x15ku` \x02T
v`k2
2
!
: (3)
</equation>
<bodyText confidence="0.983347047619048">
This minimization can be approximately solved by
the following alternating optimization procedure:
\x0f Fix (\x02;fv`g), and find m predictors fu`g that
minimizes the joint empirical risk (3).
\x0f Fix m predictors fu`g, and find (\x02;fv`g) that
minimizes the joint empirical risk (3).
\x0f Iterate until a convergence criterion is met.
In the first step, we train m predictors independently.
It is the second step that couples all the problems. Its
solution is given by the SVD (singular value decom-
position) of the predictor matrix U = [u1;::: ;um:
the rows of the optimum \x02are given by the most sig-
nificant left singular vectors1 of U. Intuitively, the
optimum \x02 captures the maximal commonality of
the m predictors (each derived from u`). These m
predictors are updated using the new structure ma-
trix \x02 in the next iteration, and the process repeats.
Figure 1 summarizes the algorithm sketched
above, which we call the alternating structure op-
timization (ASO) algorithm. The formal derivation
can be found in (Ando and Zhang, 2004).
</bodyText>
<subsectionHeader confidence="0.98842">
2.4 Comparison with existing techniques
</subsectionHeader>
<bodyText confidence="0.998788">
It is important to note that this SVD-based ASO
(SVD-ASO) procedure is fundamentally different
from the usual principle component analysis (PCA),
which can be regarded as dimension reduction in the
data space X. By contrast, the dimension reduction
performed in the SVD-ASO algorithm is on the pre-
dictor space (a set of predictors). This is possible
because we observe multiple predictors from multi-
ple learning tasks. If we regard the observed predic-
tors as sample points of the predictor distribution in
</bodyText>
<page confidence="0.89114">
1
</page>
<bodyText confidence="0.9934466">
In other words, \x02 is computed so that the best low-rank
approximation of U in the least square sense is obtained by
projecting Uonto the row space of \x02; see e.g. Golub and Loan
(1996) for SVD.
Input: training data f(X`
</bodyText>
<equation confidence="0.995267">
i;Y `
i )g (` = 1;:::; m)
</equation>
<bodyText confidence="0.986975">
Parameters: dimension h and regularization param \x15
Output: matrix \x02 with h rows
</bodyText>
<equation confidence="0.545799333333333">
Initialize: u` = 0 (` = 1:::m), and arbitrary \x02
iterate
for ` = 1 to m do
</equation>
<bodyText confidence="0.907196">
With fixed \x02 and v` = \x02u`, solve for ^
</bodyText>
<equation confidence="0.981303444444444">
w`:
^
w` = argminw`
hPn`
i=1
L(wT
` X`
i+(vT
` \x02)X`
i;Y `
i )
n`
+\x15kw`k2
2
\x03
Let u` = ^
w` +\x02T v`
endfor
</equation>
<bodyText confidence="0.8252595">
Compute the SVD of U = [u1;:: :;um.
Let the rows of \x02 be the h left singular vectors of U
corresponding to the h largest singular values.
until converge
</bodyText>
<figureCaption confidence="0.995222">
Figure 1: SVD-based Alternating Structure Optimization
</figureCaption>
<sectionHeader confidence="0.493355" genericHeader="method">
(SVD-ASO) Algorithm
</sectionHeader>
<bodyText confidence="0.99765728">
the predictor space (corrupted with estimation error,
or noise), then SVD-ASO can be interpreted as find-
ing the principle components (or commonality)
of these predictors (i.e., what good predictors are
like). Consequently the method directly looks for
low-dimensional structures with the highest predic-
tive power. By contrast, the principle components of
input data in the data space (which PCA seeks) may
not necessarily have the highest predictive power.
The above argument also applies to the fea-
ture generation from unlabeled data using LSI (e.g.
Ando (2004)). Similarly, Miller et al. (2004) used
word-cluster memberships induced from an unanno-
tated corpus as features for named entity chunking.
Our work is related but more general, because we
can explore additional information from unlabeled
data using many different auxiliary problems. Since
Miller et al. (2004)s experiments used a proprietary
corpus, direct performance comparison is not pos-
sible. However, our preliminary implementation of
the word clustering approach did not provide any
improvement on our tasks. As we will see, our start-
ing performance is already high. Therefore the addi-
tional information discovered by SVD-ASO appears
crucial to achieve appreciable improvements.
</bodyText>
<sectionHeader confidence="0.983833" genericHeader="method">
3 Semi-supervised Learning Method
</sectionHeader>
<bodyText confidence="0.998690666666667">
For semi-supervised learning, the idea is to create
many auxiliary prediction problems (relevant to the
task) from unlabeled data so that we can learn the
</bodyText>
<page confidence="0.98936">
3
</page>
<bodyText confidence="0.990988466666667">
\x0cshared structure \x02 (useful for the task) using the
ASO algorithm. In particular, we want to create aux-
iliary problems with the following properties:
\x0f Automatic labeling: we need to automatically
generate various labeled data for the auxil-
iary problems from unlabeled data.
\x0f Relevancy: auxiliary problems should be re-
lated to the target problem. That is, they should
share a certain predictive structure.
The final classifier for the target task is in the form
of (1), a linear predictor for structural learning. We
fix \x02 (learned from unlabeled data through auxil-
iary problems) and optimize weight vectors wand v
on the given labeled data. We summarize this semi-
supervised learning procedure below.
</bodyText>
<listItem confidence="0.858289">
1. Create training data e
</listItem>
<equation confidence="0.8438146">
Z` = f(e
Xj ; e
Y `
j
)g for each
</equation>
<listItem confidence="0.9162852">
auxiliary problem ` from unlabeled data fe
Xj g.
2. Compute \x02 from fe
Z`g through SVD-ASO.
3. Minimize the empirical risk on the labeled data:
</listItem>
<equation confidence="0.975558727272727">
^
f = argminf
Pn
i=1
L(f (\x02;Xi);Yi)
n
+ \x15kwk2
2,
where f(\x02;x) = wT
x+vT
\x02x as in (1).
</equation>
<subsectionHeader confidence="0.994389">
3.1 Auxiliary problem creation
</subsectionHeader>
<bodyText confidence="0.999331">
The idea is to discover useful features (which do
not necessarily appear in the labeled data) from the
unlabeled data through learning auxiliary problems.
Clearly, auxiliary problems more closely related to
the target problem will be more beneficial. However,
even if some problems are less relevant, they will not
degrade performance severely since they merely re-
sult in some irrelevant features (originated from ir-
relevant \x02-components), which ERM learners can
cope with. On the other hand, potential gains from
relevant auxiliary problems can be significant. In
this sense, our method is robust.
We present two general strategies for generat-
ing useful auxiliary problems: one in a completely
unsupervised fashion, and the other in a partially-
supervised fashion.
</bodyText>
<subsubsectionHeader confidence="0.963762">
3.1.1 Unsupervised strategy
</subsubsectionHeader>
<bodyText confidence="0.99891556">
In the first strategy, we regard some observable
substructures of the input data X as auxiliary class
labels, and try to predict these labels using other
parts of the input data.
Ex 3.1 Predict words. Create auxiliary problems
by regarding the word at each position as an auxil-
iary label, which we want to predict from the context.
For instance, predict whether a word is Smith or
not from its context. This problem is relevant to,
for instance, named entity chunking since knowing
a word is Smith helps to predict whether it is part
of a name. One binary classification problem can be
created for each possible word value (e.g., IBM,
he, get, \x01\x01\x01). Hence, many auxiliary problems
can be obtained using this idea.
More generally, given a feature representation
of the input data, we may mask some features as
unobserved, and learn classifiers to predict these
masked features based on other features that are
not masked. The automatic-labeling requirement is
satisfied since the auxiliary labels are observable to
us. To create relevant problems, we should choose
to (mask and) predict features that have good cor-
relation to the target classes, such as words on text
tagging/chunking tasks.
</bodyText>
<subsubsectionHeader confidence="0.979801">
3.1.2 Partially-supervised strategy
</subsubsectionHeader>
<bodyText confidence="0.9836921">
The second strategy is motivated by co-training.
We use two (or more) distinct feature maps: \x081
and \x082. First, we train a classifier F1 for the tar-
get task, using the feature map \x081 and the labeled
data. The auxiliary tasks are to predict the behavior
of this classifier F1 (such as predicted labels) on the
unlabeled data, by using the other feature map \x082.
Note that unlike co-training, we only use the classi-
fier as a means of creating auxiliary problems that
meet the relevancy requirement, instead of using it
to bootstrap labels.
Ex 3.2 Predict the top-k choices of the classifier.
Predict the combination of k (a few) classes to which
F1 assigns the highest output (confidence) values.
For instance, predict whether F1 assigns the highest
confidence values to CLASS1 and CLASS2 in this or-
der. By setting k = 1, the auxiliary task is simply to
predict the label prediction of classifier F1. By set-
ting k &gt; 1, fine-grained distinctions (related to in-
trinsic sub-classes of target classes) can be learned.
</bodyText>
<figure confidence="0.523016285714286">
From a
-way classification problem,
!=(
k)! bi-
nary prediction problems can be created.
4
\x0c4 Algorithms Used in Experiments
</figure>
<bodyText confidence="0.995635833333333">
Using auxiliary problems introduced above, we
study the performance of our semi-supervised learn-
ing method on named entity chunking and syntac-
tic chunking. This section describes the algorithmic
aspects of the experimental framework. The task-
specific setup is described in Sections 5 and 6.
</bodyText>
<subsectionHeader confidence="0.98321">
4.1 Extension of the basic SVD-ASO algorithm
</subsectionHeader>
<bodyText confidence="0.9994567">
In our experiments, we use an extension of SVD-
ASO. In NLP applications, features have natural
grouping according to their types/origins such as
current words, parts-of-speech on the right, and
so forth. It is desirable to perform a localized op-
timization for each of such natural feature groups.
Hence, we associate each feature group with a sub-
matrix of structure matrix \x02. The optimization al-
gorithm for this extension is essentially the same as
SVD-ASO in Figure 1, but with the SVD step per-
formed separately for each group. See (Ando and
Zhang, 2004) for the precise formulation. In ad-
dition, we regularize only those components of w`
which correspond to the non-negative part of u`.
The motivation is that positive weights are usually
directly related to the target concept, while negative
ones often yield much less specific information rep-
resenting the others. The resulting extension, in
effect, only uses the positive components of U in
the SVD computation.
</bodyText>
<subsectionHeader confidence="0.880913">
4.2 Chunking algorithm, loss function, training
</subsectionHeader>
<bodyText confidence="0.993668571428572">
algorithm, and parameter settings
As is commonly done, we encode chunk informa-
tion into word tags to cast the chunking problem to
that of sequential word tagging. We perform Viterbi-
style decoding to choose the word tag sequence that
maximizes the sum of tagging confidence values.
In all settings (including baseline methods), the
loss function is a modification of the Hubers ro-
bust loss for regression: L(p;y) = max(0;1
py)2 if py \x15 1; and 4py otherwise; with square
regularization (\x15 = 10 4). One may select other
loss functions such as SVM or logistic regression.
The specific choice is not important for the purpose
of this paper. The training algorithm is stochastic
gradient descent, which is argued to perform well
for regularized convex ERM learning formulations
(Zhang, 2004).
As we will show in Section 7.3, our formulation
is relatively insensitive to the change in h (row-
dimension of the structure matrix). We fix h (for
each feature group) to 50, and use it in all settings.
The most time-consuming process is the train-
ing of m auxiliary predictors on the unlabeled data
(computing U in Figure 1). Fixing the number of
iterations to a constant, it runs in linear to m and
the number of unlabeled instances and takes hours
in our settings that use more than 20 million unla-
beled instances.
</bodyText>
<subsectionHeader confidence="0.993621">
4.3 Baseline algorithms
</subsectionHeader>
<bodyText confidence="0.993170034482759">
Supervised classifier For comparison, we train a
classifier using the same features and algorithm, but
without unlabeled data (\x02 = 0 in effect).
Co-training We test co-training since our idea of
partially-supervised auxiliary problems is motivated
by co-training. Our implementation follows the
original work (Blum and Mitchell, 1998). The two
(or more) classifiers (with distinct feature maps) are
trained with labeled data. We maintain a pool of q
unlabeled instances by random selection. The clas-
sifier proposes labels for the instances in this pool.
We choose s instances for each classifier with high
confidence while preserving the class distribution
observed in the initial labeled data, and add them
to the labeled data. The process is then repeated.
We explore q=50K, 100K, s=50,100,500,1K, and
commonly-used feature splits: current vs. context
and current+left-context vs. current+right-context.
Self-training Single-view bootstrapping is some-
times called self-training. We test the basic self-
training2, which replaces multiple classifiers in the
co-training procedure with a single classifier that
employs all the features.
co/self-training oracle performance To avoid the
issue of parameter selection for the co- and self-
training, we report their best possible oracle perfor-
mance, which is the best F-measure number among
all the co- and self-training parameter settings in-
cluding the choice of the number of iterations.
</bodyText>
<page confidence="0.965058">
2
</page>
<bodyText confidence="0.99786375">
We also tested self-training with bagging, which Ng and
Cardie (2003) used for co-reference resolution. We omit results
since it did not produce better performance than the supervised
baseline.
</bodyText>
<page confidence="0.963792">
5
</page>
<bodyText confidence="0.967961">
\x0c\x01 words, parts-of-speech (POS), character types,
4 characters at the beginning/ending in a 5-word window.
\x01 words in a 3-syntactic chunk window.
\x01 labels assigned to two words on the left.
\x01 bi-grams of the current word and the label on the left.
\x01 labels assigned to previous occurrences of the current
</bodyText>
<figureCaption confidence="0.8091685">
word.
Figure 2: Feature types for named entity chunking. POS and
</figureCaption>
<bodyText confidence="0.890891">
syntactic chunk information is provided by the organizer.
</bodyText>
<sectionHeader confidence="0.885283" genericHeader="method">
5 Named Entity Chunking Experiments
</sectionHeader>
<bodyText confidence="0.999077125">
We report named entity chunking performance on
the CoNLL03 shared-task3 corpora (English and
German). We choose this task because the original
intention of this shared task was to test the effec-
tiveness of semi-supervised learning methods. How-
ever, it turned out that none of the top performing
systems used unlabeled data. The likely reason is
that the number of labeled data is relatively large
(&gt;200K), making it hard to benefit from unlabeled
data. We show that our ASO-based semi-supervised
learning method (hereafter, ASO-semi) can produce
results appreciably better than all of the top systems,
by using unlabeled data as the only additional re-
source. In particular, we do not use any gazetteer
information, which was used in all other systems.
The CoNLL corpora are annotated with four types
of named entities: persons, organizations, locations,
and miscellaneous names (e.g., World Cup). We
use the official training/development/test splits. Our
unlabeled data sets consist of 27 million words (En-
glish) and 35 million words (German), respectively.
They were chosen from the same sources Reuters
and ECI Multilingual Text Corpus as the provided
corpora but disjoint from them.
</bodyText>
<subsectionHeader confidence="0.736661">
5.1 Features
</subsectionHeader>
<bodyText confidence="0.9993796">
Our feature representation is a slight modification of
a simpler configuration (without any gazetteer) in
(Zhang and Johnson, 2003), as shown in Figure 2.
We use POS and syntactic chunk information pro-
vided by the organizer.
</bodyText>
<subsectionHeader confidence="0.999535">
5.2 Auxiliary problems
</subsectionHeader>
<bodyText confidence="0.996821333333333">
As shown in Figure 3, we experiment with auxiliary
problems from Ex 3.1 and 3.2: Predict current (or
previous or next) words; and Predict top-2 choices
</bodyText>
<page confidence="0.721653">
3
</page>
<figure confidence="0.5510647">
http://cnts.uia.ac.be/conll2003/ner
# of aux. Auxiliary Features used for
problems labels learning aux problems
1000 previous words all but previous words
1000 current words all but current words
1000 next words all but next words
72 F1s top-2 choices \x082 (all but left context)
72 F2s top-2 choices \x081 (left context)
72 F3s top-2 choices \x084 (all but right context)
72 F4s top-2 choices \x083 (right context)
</figure>
<figureCaption confidence="0.994719">
Figure 3: Auxiliary problems used for named entity chunk-
</figureCaption>
<bodyText confidence="0.996978222222222">
ing. 3000 problems mask words and predict them from the
other features on unlabeled data. 288 problems predict classi-
fier Fis predictions on unlabeled data, where Fi is trained with
labeled data using feature map \x08i. There are 72 possible top-2
choices from 9 classes (beginning/inside of four types of name
chunks and outside).
of the classifier using feature splits left context vs.
the others and right context vs. the others. For
word-prediction problems, we only consider the in-
stances whose current words are either nouns or ad-
jectives since named entities mostly consist of these
types. Also, we leave out all but at most 1000 bi-
nary prediction problems of each type that have the
largest numbers of positive examples to ensure that
auxiliary predictors can be adequately learned with
a sufficiently large number of examples. The results
we report are obtained by using all the problems in
Figure 3 unless otherwise specified.
</bodyText>
<subsectionHeader confidence="0.979073">
5.3 Named entity chunking results
</subsectionHeader>
<bodyText confidence="0.898788333333333">
methods test diff. from supervised
data F prec. recall F
English, small (10K examples) training set
</bodyText>
<table confidence="0.984745214285714">
ASO-semi dev. 81.25 +10.02 +7.00 +8.51
co/self oracle 73.10 +0.32 +0.39 +0.36
ASO-semi test 78.42 +9.39 +10.73 +10.10
co/self oracle 69.63 +0.60 +1.95 +1.31
English, all (204K) training examples
ASO-semi dev. 93.15 +2.25 +3.00 +2.62
co/self oracle 90.64 +0.04 +0.20 +0.11
ASO-semi test 89.31 +3.20 +4.51 +3.86
co/self oracle 85.40 0.04 0.05 0.05
German, all (207K) training examples
ASO-semi dev. 74.06 +7.04 +10.19 +9.22
co/self oracle 66.47 2.59 +4.39 +1.63
ASO-semi test 75.27 +4.64 +6.59 +5.88
co/self oracle 70.45 1.26 +2.59 +1.06
</table>
<figureCaption confidence="0.998038">
Figure 4: Named entity chunking results. No gazetteer. F-
</figureCaption>
<bodyText confidence="0.9607332">
measure and performance improvements over the supervised
baseline in precision, recall, and F. For co- and self-training
(baseline), the oracle performance is shown.
Figure 4 shows results in comparison with the su-
pervised baseline in six configurations, each trained
</bodyText>
<page confidence="0.991208">
6
</page>
<bodyText confidence="0.982661708333333">
\x0cwith one of three sets of labeled training examples: a
small English set (10K examples randomly chosen),
the entire English training set (204K), and the entire
German set (207K), tested on either the development
set or test set. ASO-semi significantly improves both
precision and recall in all the six configurations, re-
sulting in improved F-measures over the supervised
baseline by +2.62% to +10.10%.
Co- and self-training, at their oracle performance,
improve recall but often degrade precision; con-
sequently, their F-measure improvements are rela-
tively low: 0.05% to +1.63%.
Comparison with top systems As shown in Fig-
ure 5, ASO-semi achieves higher performance than
the top systems on both English and German
data. Most of the top systems boost performance
by external hand-crafted resources such as: large
gazetteers4; a large amount (2 million words) of
labeled data manually annotated with finer-grained
named entities (FIJZ03); and rule-based post pro-
cessing (KSNM03). Hence, we feel that our results,
obtained by using unlabeled data as the only addi-
tional resource, are encouraging.
System Eng. Ger. Additional resources
</bodyText>
<table confidence="0.993984666666667">
ASO-semi 89.31 75.27 unlabeled data
FIJZ03 88.76 72.41 gazetteers; 2M-word labeled
data (English)
CN03 88.31 65.67 gazetteers (English); (also
very elaborated features)
KSNM03 86.31 71.90 rule-based post processing
</table>
<figureCaption confidence="0.996066">
Figure 5: Named entity chunking. F-measure on the test
</figureCaption>
<bodyText confidence="0.772016">
sets. Previous best results: FIJZ03 (Florian et al., 2003), CN03
(Chieu and Ng, 2003), KSNM03 (Klein et al., 2003).
</bodyText>
<sectionHeader confidence="0.875999" genericHeader="method">
6 Syntactic Chunking Experiments
</sectionHeader>
<bodyText confidence="0.988361">
Next, we report syntactic chunking performance on
the CoNLL00 shared-task5 corpus. The training
and test data sets consist of the Wall Street Journal
corpus (WSJ) sections 1518 (212K words) and sec-
tion 20, respectively. They are annotated with eleven
types of syntactic chunks such as noun phrases. We
</bodyText>
<page confidence="0.980846">
4
</page>
<bodyText confidence="0.9595318">
Whether or not gazetteers are useful depends on their cov-
erage. A number of top-performing systems used their own
gazetteers in addition to the organizers gazetteers and reported
significant performance improvements (e.g., FIJZ03, CN03,
and ZJ03).
</bodyText>
<page confidence="0.700855">
5
</page>
<footnote confidence="0.927473">
http://cnts.uia.ac.be/conll2000/chunking
\x01 uni- and bi-grams of words and POS in a 5-token window.
\x01 word-POS bi-grams in a 3-token window.
\x01 POS tri-grams on the left and right.
\x01 labels of the two words on the left and their bi-grams.
\x01 bi-grams of the current word and two labels on the left.
</footnote>
<figureCaption confidence="0.861319">
Figure 6: Feature types for syntactic chunking. POS informa-
tion is provided by the organizer.
prec. recall F\x0c=1
</figureCaption>
<table confidence="0.989917333333333">
supervised 93.83 93.37 93.60
ASO-semi 94.57 94.20 94.39 (+0.79)
co/self oracle 93.76 93.56 93.66 (+0.06)
</table>
<figureCaption confidence="0.995687">
Figure 7: Syntactic chunking results.
</figureCaption>
<bodyText confidence="0.9907825">
use the WSJ articles in 1991 (15 million words) from
the TREC corpus as the unlabeled data.
</bodyText>
<subsectionHeader confidence="0.971257">
6.1 Features and auxiliary problems
</subsectionHeader>
<bodyText confidence="0.999881142857143">
Our feature representation is a slight modification of
a simpler configuration (without linguistic features)
in (Zhang et al., 2002), as shown in Figure 6. We
use the POS information provided by the organizer.
The types of auxiliary problems are the same as in
the named entity experiments. For word predictions,
we exclude instances of punctuation symbols.
</bodyText>
<subsectionHeader confidence="0.999113">
6.2 Syntactic chunking results
</subsectionHeader>
<bodyText confidence="0.9993246">
As shown in Figure 7, ASO-semi improves both pre-
cision and recall over the supervised baseline. It
achieves 94:39% in F-measure, which outperforms
the supervised baseline by 0:79%. Co- and self-
training again slightly improve recall but slightly de-
grade precision at their oracle performance, which
demonstrates that it is not easy to benefit from unla-
beled data on this task.
Comparison with the previous best systems As
shown in Figure 8, ASO-semi achieves performance
higher than the previous best systems. Though the
space constraint precludes providing the detail, we
note that ASO-semi outperforms all of the previ-
ous top systems in both precision and recall. Unlike
named entity chunking, the use of external resources
on this task is rare. An exception is the use of out-
put from a grammar-based full parser as features in
ZDJ02+, which our system does not use. KM01
and CM03 boost performance by classifier combina-
tions. SP03 trains conditional random fields for NP
</bodyText>
<page confidence="0.991427">
7
</page>
<table confidence="0.996587857142857">
\x0call NP description
ASO-semi 94.39 94.70 +unlabeled data
KM01 93.91 94.39 SVM combination
CM03 93.74 94.41 perceptron in two layers
SP03 94.38 conditional random fields
ZDJ02 93.57 93.89 generalized Winnow
ZDJ02+ 94.17 94.38 +full parser output
</table>
<figureCaption confidence="0.992757">
Figure 8: Syntactic chunking F-measure. Comparison with
</figureCaption>
<bodyText confidence="0.9960684">
previous best results: KM01 (Kudoh and Matsumoto, 2001),
CM03 (Carreras and Marquez, 2003), SP03 (Sha and Pereira,
2003), ZDJ02 (Zhang et al., 2002).
(noun phrases) only. ASO-semi produces higher NP
chunking performance than the others.
</bodyText>
<sectionHeader confidence="0.983795" genericHeader="method">
7 Empirical Analysis
</sectionHeader>
<subsectionHeader confidence="0.932681">
7.1 Effectiveness of auxiliary problems
</subsectionHeader>
<bodyText confidence="0.589713">
English named entity German named entity
</bodyText>
<page confidence="0.497921">
68
</page>
<figure confidence="0.993315">
70
72
74
76
1
F-measure
(%)
85
86
87
88
89
90
dev set
F-measure
(%)
supervised
w/ &amp;quot;Predict (previous, current, or next) words&amp;quot;
w/ &amp;quot;Predict top-2 choices&amp;quot;
w/ &amp;quot;Predict words&amp;quot; + &amp;quot;Predict top-2 choices&amp;quot;
</figure>
<figureCaption confidence="0.999864">
Figure 9: Named entity F-measure produced by using individ-
</figureCaption>
<bodyText confidence="0.9438121">
ual types of auxiliary problems. Trained with the entire training
sets and tested on the test sets.
Figure 9 shows F-measure obtained by comput-
ing \x02 from individual types of auxiliary problems
on named entity chunking. Both types Predict
words and Predict top-2 choices of the classifier
are useful, producing significant performance im-
provements over the supervised baseline. The best
performance is achieved when \x02 is produced from
all of the auxiliary problems.
</bodyText>
<subsectionHeader confidence="0.98778">
7.2 Interpretation of \x02
</subsectionHeader>
<bodyText confidence="0.9772697">
To gain insights into the information obtained from
unlabeled data, we examine the \x02entries associated
with the feature current words, computed for the
English named entity task. Figure 10 shows the fea-
tures associated with the entries of \x02with the largest
values, computed from the 2000 unsupervised aux-
iliary problems: Predict previous words and Pre-
dict next words. For clarity, the figure only shows
row# Features corresponding to Interpretation
significant \x02 entries
</bodyText>
<sectionHeader confidence="0.572106" genericHeader="method">
4 Ltd, Inc, Plc, International, organizations
</sectionHeader>
<reference confidence="0.972624545454545">
Ltd., Association, Group, Inc.
7 Co, Corp, Co., Company, organizations
Authority, Corp., Services
9 PCT, N/A, Nil, Dec, BLN, no names
Avg, Year-on-year, UNCH
11 New, France, European, San, locations
North, Japan, Asian, India
15 Peter, Sir, Charles, Jose, Paul, persons
Lee, Alan, Dan, John, James
26 June, May, July, Jan, March, months
August, September, April
</reference>
<figureCaption confidence="0.917939">
Figure 10: Interpretation of \x02 computed from word-
</figureCaption>
<bodyText confidence="0.999184526315789">
prediction (unsupervised) problems for named entity chunking.
words beginning with upper-case letters (i.e., likely
to be names in English). Our method captures the
spirit of predictive word-clustering but is more gen-
eral and effective on our tasks.
It is possible to develop a general theory to show
that the auxiliary problems we use are helpful under
reasonable conditions. The intuition is as follows.
Suppose we split the features into two parts \x081 and
\x082 and predict \x081 based on \x082. Suppose features
in \x081 are correlated to the class labels (but not nec-
essarily correlated among themselves). Then, the
auxiliary prediction problems are related to the tar-
get task, and thus can reveal useful structures of \x082.
Under some conditions, it can be shown that features
in \x082 with similar predictive performance tend to
map to similar low-dimensional vectors through \x02.
This effect can be empirically observed in Figure 10
and will be formally shown elsewhere.
</bodyText>
<subsectionHeader confidence="0.835329">
7.3 Effect of the \x02 dimension
</subsectionHeader>
<figure confidence="0.910016">
85
87
89
20 40 60 80 100
dimension
F-measure
(%)
ASO-semi
supervised
</figure>
<figureCaption confidence="0.999498">
Figure 11: F-measure in relation to the row-dimension of \x02.
</figureCaption>
<bodyText confidence="0.994337285714286">
English named entity chunking, test set.
Recall that throughout the experiments, we fix the
row-dimension of \x02 (for each feature group) to 50.
Figure 11 plots F-measure in relation to the row-
dimension of \x02, which shows that the method is rel-
atively insensitive to the change of this parameter, at
least in the range which we consider.
</bodyText>
<page confidence="0.944358">
8
</page>
<sectionHeader confidence="0.945907" genericHeader="conclusions">
\x0c8 Conclusion
</sectionHeader>
<bodyText confidence="0.999390869565217">
We presented a novel semi-supervised learn-
ing method that learns the most predictive low-
dimensional feature projection from unlabeled data
using the structural learning algorithm SVD-ASO.
On CoNLL00 syntactic chunking and CoNLL03
named entity chunking (English and German), the
method exceeds the previous best systems (includ-
ing those which rely on hand-crafted resources) by
using unlabeled data as the only additional resource.
The key idea is to create auxiliary problems au-
tomatically from unlabeled data so that predictive
structures can be learned from that data. In practice,
it is desirable to create as many auxiliary problems
as possible, as long as there is some reason to be-
lieve in their relevancy to the task. This is because
the risk is relatively minor while the potential gain
from relevant problems is large. Moreover, the aux-
iliary problems used in our experiments are merely
possible examples. One advantage of our approach
is that one may design a variety of auxiliary prob-
lems to learn various aspects of the target problem
from unlabeled data. Structural learning provides a
framework for carrying out possible new ideas.
</bodyText>
<sectionHeader confidence="0.96894" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.690074">
Part of the work was supported by ARDA under the
NIMD program PNWD-SW-6059.
</bodyText>
<sectionHeader confidence="0.901912" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.981452278688524">
Rie Kubota Ando and Tong Zhang. 2004. A framework
for learning predictive structures from multiple tasks
and unlabeled data. Technical report, IBM. RC23462.
Rie Kubota Ando. 2004. Semantic lexicon construction:
Learning from unlabeled data via spectral analysis. In
Proceedings of CoNLL-2004.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In proceed-
ings of COLT-98.
Xavier Carreras and Lluis Marquez. 2003. Phrase recog-
nition by filtering and ranking with perceptrons. In
Proceedings of RANLP-2003.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach. In
Proceedings CoNLL-2003, pages 160163.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of EMNLP/VLC99.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recognition through
classifier combination. In Proceedings CoNLL-2003,
pages 168171.
Gene H. Golub and Charles F. Van Loan. 1996. Matrix
computations third edition.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings CoNLL-
2003, pages 188191.
Taku Kudoh and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL
2001.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155171.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In Proceedings of HLT-NAACL-2004.
Vincent Ng and Claire Cardie. 2003. Weakly supervised
natural language learning without redundant views. In
Proceedings of HLT-NAACL-2003.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of EMNLP-2001.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction by multi-level bootstrap-
ping. In Proceedings of AAAI-99.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings of
HLT-NAACL03.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of ACL-95.
Tong Zhang and David E. Johnson. 2003. A robust risk
minimization based named entity recognition system.
In Proceedings CoNLL-2003, pages 204207.
Tong Zhang, Fred Damerau, and David E. Johnson.
2002. Text chunking based on a generalization of Win-
now. Journal of Machine Learning Research, 2:615
637.
Tong Zhang. 2004. Solving large scale linear prediction
problems using stochastic gradient descent algorithms.
</reference>
<figure confidence="0.435601333333333">
In ICML 04, pages 919926.
9
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.520521">
<note confidence="0.887498333333333">b&amp;apos;Proceedings of the 43rd Annual Meeting of the ACL, pages 19, Ann Arbor, June 2005. c 2005 Association for Computational Linguistics</note>
<title confidence="0.992832">A High-Performance Semi-Supervised Learning Method for Text Chunking</title>
<author confidence="0.989981">Rie Kubota Andoy Tong Zhangz</author>
<affiliation confidence="0.999695">IBM T.J. Watson Research Center</affiliation>
<address confidence="0.915626">Yorktown Heights, NY 10598, U.S.A.</address>
<email confidence="0.99572">yrie1@us.ibm.comztongz@us.ibm.com</email>
<abstract confidence="0.991817913043479">In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue. Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. This paper presents a novel semi-supervised method that employs a learning paradigm which we call structural learning. The idea is to find what good classifiers are like by learning from thousands of automatically generated auxiliary classification problems on unlabeled data. By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem. The method produces performance higher than the previous best results on CoNLL00 syntactic chunking and CoNLL03 named entity chunking (English and German).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>NA PCT</author>
<author>Dec Nil</author>
<author>BLN</author>
</authors>
<title>no names Avg,</title>
<date></date>
<journal>Co, Corp, Co., Company, organizations Authority, Corp., Services</journal>
<tech>Technical report, IBM. RC23462.</tech>
<volume>7</volume>
<institution>Ltd., Association, Group, Inc.</institution>
<location>New, France, European, San, locations North, Japan, Asian, India 15 Peter, Sir, Charles, Jose, Paul, persons Lee, Alan, Dan, John, James</location>
<marker>PCT, Nil, BLN, </marker>
<rawString>Ltd., Association, Group, Inc. 7 Co, Corp, Co., Company, organizations Authority, Corp., Services 9 PCT, N/A, Nil, Dec, BLN, no names Avg, Year-on-year, UNCH 11 New, France, European, San, locations North, Japan, Asian, India 15 Peter, Sir, Charles, Jose, Paul, persons Lee, Alan, Dan, John, James 26 June, May, July, Jan, March, months August, September, April Rie Kubota Ando and Tong Zhang. 2004. A framework for learning predictive structures from multiple tasks and unlabeled data. Technical report, IBM. RC23462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
</authors>
<title>Semantic lexicon construction: Learning from unlabeled data via spectral analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004.</booktitle>
<contexts>
<context position="10836" citStr="Ando (2004)" startWordPosition="1790" endWordPosition="1791">ating Structure Optimization (SVD-ASO) Algorithm the predictor space (corrupted with estimation error, or noise), then SVD-ASO can be interpreted as finding the principle components (or commonality) of these predictors (i.e., what good predictors are like). Consequently the method directly looks for low-dimensional structures with the highest predictive power. By contrast, the principle components of input data in the data space (which PCA seeks) may not necessarily have the highest predictive power. The above argument also applies to the feature generation from unlabeled data using LSI (e.g. Ando (2004)). Similarly, Miller et al. (2004) used word-cluster memberships induced from an unannotated corpus as features for named entity chunking. Our work is related but more general, because we can explore additional information from unlabeled data using many different auxiliary problems. Since Miller et al. (2004)s experiments used a proprietary corpus, direct performance comparison is not possible. However, our preliminary implementation of the word clustering approach did not provide any improvement on our tasks. As we will see, our starting performance is already high. Therefore the additional i</context>
</contexts>
<marker>Ando, 2004</marker>
<rawString>Rie Kubota Ando. 2004. Semantic lexicon construction: Learning from unlabeled data via spectral analysis. In Proceedings of CoNLL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In proceedings of COLT-98.</booktitle>
<contexts>
<context position="1759" citStr="Blum and Mitchell, 1998" startWordPosition="265" endWordPosition="268"> CoNLL00 syntactic chunking and CoNLL03 named entity chunking (English and German). 1 Introduction In supervised learning applications, one can often find a large amount of unlabeled data without difficulty, while labeled data are costly to obtain. Therefore, a natural question is whether we can use unlabeled data to build a more accurate classifier, given the same amount of labeled data. This problem is often referred to as semi-supervised learning. Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. For example, co-training (Blum and Mitchell, 1998) automatically bootstraps labels, and such labels are not necessarily reliable (Pierce and Cardie, 2001). A related idea is to use Expectation Maximization (EM) to impute labels. Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g. Merialdo (1994)). A number of bootstrapping methods have been proposed for NLP tasks (e.g. Yarowsky (1995), Collins and Singer (1999), Riloff and Jones (1999)). But these typically assume a very small amount of labeled data and have not been shown to improve state-of-the</context>
<context position="18969" citStr="Blum and Mitchell, 1998" startWordPosition="3105" endWordPosition="3108">m auxiliary predictors on the unlabeled data (computing U in Figure 1). Fixing the number of iterations to a constant, it runs in linear to m and the number of unlabeled instances and takes hours in our settings that use more than 20 million unlabeled instances. 4.3 Baseline algorithms Supervised classifier For comparison, we train a classifier using the same features and algorithm, but without unlabeled data (\x02 = 0 in effect). Co-training We test co-training since our idea of partially-supervised auxiliary problems is motivated by co-training. Our implementation follows the original work (Blum and Mitchell, 1998). The two (or more) classifiers (with distinct feature maps) are trained with labeled data. We maintain a pool of q unlabeled instances by random selection. The classifier proposes labels for the instances in this pool. We choose s instances for each classifier with high confidence while preserving the class distribution observed in the initial labeled data, and add them to the labeled data. The process is then repeated. We explore q=50K, 100K, s=50,100,500,1K, and commonly-used feature splits: current vs. context and current+left-context vs. current+right-context. Self-training Single-view bo</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In proceedings of COLT-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis Marquez</author>
</authors>
<title>Phrase recognition by filtering and ranking with perceptrons.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP-2003.</booktitle>
<contexts>
<context position="29318" citStr="Carreras and Marquez, 2003" startWordPosition="4715" endWordPosition="4718">are. An exception is the use of output from a grammar-based full parser as features in ZDJ02+, which our system does not use. KM01 and CM03 boost performance by classifier combinations. SP03 trains conditional random fields for NP 7 \x0call NP description ASO-semi 94.39 94.70 +unlabeled data KM01 93.91 94.39 SVM combination CM03 93.74 94.41 perceptron in two layers SP03 94.38 conditional random fields ZDJ02 93.57 93.89 generalized Winnow ZDJ02+ 94.17 94.38 +full parser output Figure 8: Syntactic chunking F-measure. Comparison with previous best results: KM01 (Kudoh and Matsumoto, 2001), CM03 (Carreras and Marquez, 2003), SP03 (Sha and Pereira, 2003), ZDJ02 (Zhang et al., 2002). (noun phrases) only. ASO-semi produces higher NP chunking performance than the others. 7 Empirical Analysis 7.1 Effectiveness of auxiliary problems English named entity German named entity 68 70 72 74 76 1 F-measure (%) 85 86 87 88 89 90 dev set F-measure (%) supervised w/ &amp;quot;Predict (previous, current, or next) words&amp;quot; w/ &amp;quot;Predict top-2 choices&amp;quot; w/ &amp;quot;Predict words&amp;quot; + &amp;quot;Predict top-2 choices&amp;quot; Figure 9: Named entity F-measure produced by using individual types of auxiliary problems. Trained with the entire training sets and tested on the te</context>
</contexts>
<marker>Carreras, Marquez, 2003</marker>
<rawString>Xavier Carreras and Lluis Marquez. 2003. Phrase recognition by filtering and ranking with perceptrons. In Proceedings of RANLP-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Named entity recognition with a maximum entropy approach.</title>
<date>2003</date>
<booktitle>In Proceedings CoNLL-2003,</booktitle>
<pages>160163</pages>
<contexts>
<context position="26245" citStr="Chieu and Ng, 2003" startWordPosition="4232" endWordPosition="4235"> data manually annotated with finer-grained named entities (FIJZ03); and rule-based post processing (KSNM03). Hence, we feel that our results, obtained by using unlabeled data as the only additional resource, are encouraging. System Eng. Ger. Additional resources ASO-semi 89.31 75.27 unlabeled data FIJZ03 88.76 72.41 gazetteers; 2M-word labeled data (English) CN03 88.31 65.67 gazetteers (English); (also very elaborated features) KSNM03 86.31 71.90 rule-based post processing Figure 5: Named entity chunking. F-measure on the test sets. Previous best results: FIJZ03 (Florian et al., 2003), CN03 (Chieu and Ng, 2003), KSNM03 (Klein et al., 2003). 6 Syntactic Chunking Experiments Next, we report syntactic chunking performance on the CoNLL00 shared-task5 corpus. The training and test data sets consist of the Wall Street Journal corpus (WSJ) sections 1518 (212K words) and section 20, respectively. They are annotated with eleven types of syntactic chunks such as noun phrases. We 4 Whether or not gazetteers are useful depends on their coverage. A number of top-performing systems used their own gazetteers in addition to the organizers gazetteers and reported significant performance improvements (e.g., FIJZ03, C</context>
</contexts>
<marker>Chieu, Ng, 2003</marker>
<rawString>Hai Leong Chieu and Hwee Tou Ng. 2003. Named entity recognition with a maximum entropy approach. In Proceedings CoNLL-2003, pages 160163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC99.</booktitle>
<contexts>
<context position="2221" citStr="Collins and Singer (1999)" startWordPosition="334" endWordPosition="337">gh a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. For example, co-training (Blum and Mitchell, 1998) automatically bootstraps labels, and such labels are not necessarily reliable (Pierce and Cardie, 2001). A related idea is to use Expectation Maximization (EM) to impute labels. Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g. Merialdo (1994)). A number of bootstrapping methods have been proposed for NLP tasks (e.g. Yarowsky (1995), Collins and Singer (1999), Riloff and Jones (1999)). But these typically assume a very small amount of labeled data and have not been shown to improve state-of-the-art performance when a large amount of labeled data is available. Our goal has been to develop a general learning framework for reliably using unlabeled data to improve performance irrespective of the amount of labeled data available. It is exactly this important and difficult problem that we tackle here. This paper presents a novel semi-supervised method that employs a learning framework called structural learning (Ando and Zhang, 2004), which seeks to dis</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of EMNLP/VLC99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Tong Zhang</author>
</authors>
<title>Named entity recognition through classifier combination.</title>
<date>2003</date>
<booktitle>In Proceedings CoNLL-2003,</booktitle>
<pages>168171</pages>
<contexts>
<context position="26218" citStr="Florian et al., 2003" startWordPosition="4227" endWordPosition="4230"> (2 million words) of labeled data manually annotated with finer-grained named entities (FIJZ03); and rule-based post processing (KSNM03). Hence, we feel that our results, obtained by using unlabeled data as the only additional resource, are encouraging. System Eng. Ger. Additional resources ASO-semi 89.31 75.27 unlabeled data FIJZ03 88.76 72.41 gazetteers; 2M-word labeled data (English) CN03 88.31 65.67 gazetteers (English); (also very elaborated features) KSNM03 86.31 71.90 rule-based post processing Figure 5: Named entity chunking. F-measure on the test sets. Previous best results: FIJZ03 (Florian et al., 2003), CN03 (Chieu and Ng, 2003), KSNM03 (Klein et al., 2003). 6 Syntactic Chunking Experiments Next, we report syntactic chunking performance on the CoNLL00 shared-task5 corpus. The training and test data sets consist of the Wall Street Journal corpus (WSJ) sections 1518 (212K words) and section 20, respectively. They are annotated with eleven types of syntactic chunks such as noun phrases. We 4 Whether or not gazetteers are useful depends on their coverage. A number of top-performing systems used their own gazetteers in addition to the organizers gazetteers and reported significant performance im</context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong Zhang. 2003. Named entity recognition through classifier combination. In Proceedings CoNLL-2003, pages 168171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene H Golub</author>
<author>Charles F Van Loan</author>
</authors>
<date>1996</date>
<note>Matrix computations third edition.</note>
<marker>Golub, Van Loan, 1996</marker>
<rawString>Gene H. Golub and Charles F. Van Loan. 1996. Matrix computations third edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Joseph Smarr</author>
<author>Huy Nguyen</author>
<author>Christopher D Manning</author>
</authors>
<title>Named entity recognition with character-level models.</title>
<date>2003</date>
<booktitle>In Proceedings CoNLL2003,</booktitle>
<pages>188191</pages>
<contexts>
<context position="26274" citStr="Klein et al., 2003" startWordPosition="4237" endWordPosition="4240"> finer-grained named entities (FIJZ03); and rule-based post processing (KSNM03). Hence, we feel that our results, obtained by using unlabeled data as the only additional resource, are encouraging. System Eng. Ger. Additional resources ASO-semi 89.31 75.27 unlabeled data FIJZ03 88.76 72.41 gazetteers; 2M-word labeled data (English) CN03 88.31 65.67 gazetteers (English); (also very elaborated features) KSNM03 86.31 71.90 rule-based post processing Figure 5: Named entity chunking. F-measure on the test sets. Previous best results: FIJZ03 (Florian et al., 2003), CN03 (Chieu and Ng, 2003), KSNM03 (Klein et al., 2003). 6 Syntactic Chunking Experiments Next, we report syntactic chunking performance on the CoNLL00 shared-task5 corpus. The training and test data sets consist of the Wall Street Journal corpus (WSJ) sections 1518 (212K words) and section 20, respectively. They are annotated with eleven types of syntactic chunks such as noun phrases. We 4 Whether or not gazetteers are useful depends on their coverage. A number of top-performing systems used their own gazetteers in addition to the organizers gazetteers and reported significant performance improvements (e.g., FIJZ03, CN03, and ZJ03). 5 http://cnts</context>
</contexts>
<marker>Klein, Smarr, Nguyen, Manning, 2003</marker>
<rawString>Dan Klein, Joseph Smarr, Huy Nguyen, and Christopher D. Manning. 2003. Named entity recognition with character-level models. In Proceedings CoNLL2003, pages 188191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudoh</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="5749" citStr="Kudoh and Matsumoto (2001)" startWordPosition="910" endWordPosition="914">oose the class with the highest output value. A frequently used method for finding an accurate predictor ^ f is regularized empirical risk minimization (ERM), which minimizes an empirical loss of the predictor (with regularization) on the n training examples f(Xi;Yi)g: ^ f = argmin f n X i=1 L(f(Xi);Yi)+r(f) ! : L(\x01) is a loss function to quantify the difference between the prediction f(Xi) and the true output Yi, and r(\x01) is a regularization term to control the model complexity. ERM-based methods for discriminative learning are known to be effective for NLP tasks such as chunking (e.g. Kudoh and Matsumoto (2001), Zhang and Johnson (2003)). 2.2 Linear model for structural learning We present a linear prediction model for structural learning, which extends the traditional model to multiple problems. Specifically, we assume that there exists a low-dimensional predictive structure shared by multiple prediction problems. We seek to discover this structure through joint empirical risk minimization over the multiple problems. Consider m problems indexed by ` 2 f1;::: ;mg, each with n` samples (X` i ;Y ` i ) indexed by i 2 f1;::: ;n`g. In our joint linear model, a predictor for problem ` takes the following </context>
<context position="29283" citStr="Kudoh and Matsumoto, 2001" startWordPosition="4710" endWordPosition="4713">ternal resources on this task is rare. An exception is the use of output from a grammar-based full parser as features in ZDJ02+, which our system does not use. KM01 and CM03 boost performance by classifier combinations. SP03 trains conditional random fields for NP 7 \x0call NP description ASO-semi 94.39 94.70 +unlabeled data KM01 93.91 94.39 SVM combination CM03 93.74 94.41 perceptron in two layers SP03 94.38 conditional random fields ZDJ02 93.57 93.89 generalized Winnow ZDJ02+ 94.17 94.38 +full parser output Figure 8: Syntactic chunking F-measure. Comparison with previous best results: KM01 (Kudoh and Matsumoto, 2001), CM03 (Carreras and Marquez, 2003), SP03 (Sha and Pereira, 2003), ZDJ02 (Zhang et al., 2002). (noun phrases) only. ASO-semi produces higher NP chunking performance than the others. 7 Empirical Analysis 7.1 Effectiveness of auxiliary problems English named entity German named entity 68 70 72 74 76 1 F-measure (%) 85 86 87 88 89 90 dev set F-measure (%) supervised w/ &amp;quot;Predict (previous, current, or next) words&amp;quot; w/ &amp;quot;Predict top-2 choices&amp;quot; w/ &amp;quot;Predict words&amp;quot; + &amp;quot;Predict top-2 choices&amp;quot; Figure 9: Named entity F-measure produced by using individual types of auxiliary problems. Trained with the entire</context>
</contexts>
<marker>Kudoh, Matsumoto, 2001</marker>
<rawString>Taku Kudoh and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proceedings of NAACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="2103" citStr="Merialdo (1994)" startWordPosition="317" endWordPosition="318">given the same amount of labeled data. This problem is often referred to as semi-supervised learning. Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. For example, co-training (Blum and Mitchell, 1998) automatically bootstraps labels, and such labels are not necessarily reliable (Pierce and Cardie, 2001). A related idea is to use Expectation Maximization (EM) to impute labels. Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g. Merialdo (1994)). A number of bootstrapping methods have been proposed for NLP tasks (e.g. Yarowsky (1995), Collins and Singer (1999), Riloff and Jones (1999)). But these typically assume a very small amount of labeled data and have not been shown to improve state-of-the-art performance when a large amount of labeled data is available. Our goal has been to develop a general learning framework for reliably using unlabeled data to improve performance irrespective of the amount of labeled data available. It is exactly this important and difficult problem that we tackle here. This paper presents a novel semi-sup</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL-2004.</booktitle>
<contexts>
<context position="10870" citStr="Miller et al. (2004)" startWordPosition="1793" endWordPosition="1796">ion (SVD-ASO) Algorithm the predictor space (corrupted with estimation error, or noise), then SVD-ASO can be interpreted as finding the principle components (or commonality) of these predictors (i.e., what good predictors are like). Consequently the method directly looks for low-dimensional structures with the highest predictive power. By contrast, the principle components of input data in the data space (which PCA seeks) may not necessarily have the highest predictive power. The above argument also applies to the feature generation from unlabeled data using LSI (e.g. Ando (2004)). Similarly, Miller et al. (2004) used word-cluster memberships induced from an unannotated corpus as features for named entity chunking. Our work is related but more general, because we can explore additional information from unlabeled data using many different auxiliary problems. Since Miller et al. (2004)s experiments used a proprietary corpus, direct performance comparison is not possible. However, our preliminary implementation of the word clustering approach did not provide any improvement on our tasks. As we will see, our starting performance is already high. Therefore the additional information discovered by SVD-ASO a</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative training. In Proceedings of HLT-NAACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Weakly supervised natural language learning without redundant views.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL-2003.</booktitle>
<contexts>
<context position="20139" citStr="Ng and Cardie (2003)" startWordPosition="3280" endWordPosition="3283">urrent+right-context. Self-training Single-view bootstrapping is sometimes called self-training. We test the basic selftraining2, which replaces multiple classifiers in the co-training procedure with a single classifier that employs all the features. co/self-training oracle performance To avoid the issue of parameter selection for the co- and selftraining, we report their best possible oracle performance, which is the best F-measure number among all the co- and self-training parameter settings including the choice of the number of iterations. 2 We also tested self-training with bagging, which Ng and Cardie (2003) used for co-reference resolution. We omit results since it did not produce better performance than the supervised baseline. 5 \x0c\x01 words, parts-of-speech (POS), character types, 4 characters at the beginning/ending in a 5-word window. \x01 words in a 3-syntactic chunk window. \x01 labels assigned to two words on the left. \x01 bi-grams of the current word and the label on the left. \x01 labels assigned to previous occurrences of the current word. Figure 2: Feature types for named entity chunking. POS and syntactic chunk information is provided by the organizer. 5 Named Entity Chunking Exp</context>
</contexts>
<marker>Ng, Cardie, 2003</marker>
<rawString>Vincent Ng and Claire Cardie. 2003. Weakly supervised natural language learning without redundant views. In Proceedings of HLT-NAACL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pierce</author>
<author>Claire Cardie</author>
</authors>
<title>Limitations of co-training for natural language learning from large datasets.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP-2001.</booktitle>
<contexts>
<context position="1863" citStr="Pierce and Cardie, 2001" startWordPosition="279" endWordPosition="282">pervised learning applications, one can often find a large amount of unlabeled data without difficulty, while labeled data are costly to obtain. Therefore, a natural question is whether we can use unlabeled data to build a more accurate classifier, given the same amount of labeled data. This problem is often referred to as semi-supervised learning. Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. For example, co-training (Blum and Mitchell, 1998) automatically bootstraps labels, and such labels are not necessarily reliable (Pierce and Cardie, 2001). A related idea is to use Expectation Maximization (EM) to impute labels. Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g. Merialdo (1994)). A number of bootstrapping methods have been proposed for NLP tasks (e.g. Yarowsky (1995), Collins and Singer (1999), Riloff and Jones (1999)). But these typically assume a very small amount of labeled data and have not been shown to improve state-of-the-art performance when a large amount of labeled data is available. Our goal has been to develop a genera</context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>David Pierce and Claire Cardie. 2001. Limitations of co-training for natural language learning from large datasets. In Proceedings of EMNLP-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI-99.</booktitle>
<contexts>
<context position="2246" citStr="Riloff and Jones (1999)" startWordPosition="338" endWordPosition="341">sed methods have been proposed, their effectiveness on NLP tasks is not always clear. For example, co-training (Blum and Mitchell, 1998) automatically bootstraps labels, and such labels are not necessarily reliable (Pierce and Cardie, 2001). A related idea is to use Expectation Maximization (EM) to impute labels. Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g. Merialdo (1994)). A number of bootstrapping methods have been proposed for NLP tasks (e.g. Yarowsky (1995), Collins and Singer (1999), Riloff and Jones (1999)). But these typically assume a very small amount of labeled data and have not been shown to improve state-of-the-art performance when a large amount of labeled data is available. Our goal has been to develop a general learning framework for reliably using unlabeled data to improve performance irrespective of the amount of labeled data available. It is exactly this important and difficult problem that we tackle here. This paper presents a novel semi-supervised method that employs a learning framework called structural learning (Ando and Zhang, 2004), which seeks to discover shared predictive s</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of AAAI-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL03.</booktitle>
<contexts>
<context position="29348" citStr="Sha and Pereira, 2003" startWordPosition="4720" endWordPosition="4723">put from a grammar-based full parser as features in ZDJ02+, which our system does not use. KM01 and CM03 boost performance by classifier combinations. SP03 trains conditional random fields for NP 7 \x0call NP description ASO-semi 94.39 94.70 +unlabeled data KM01 93.91 94.39 SVM combination CM03 93.74 94.41 perceptron in two layers SP03 94.38 conditional random fields ZDJ02 93.57 93.89 generalized Winnow ZDJ02+ 94.17 94.38 +full parser output Figure 8: Syntactic chunking F-measure. Comparison with previous best results: KM01 (Kudoh and Matsumoto, 2001), CM03 (Carreras and Marquez, 2003), SP03 (Sha and Pereira, 2003), ZDJ02 (Zhang et al., 2002). (noun phrases) only. ASO-semi produces higher NP chunking performance than the others. 7 Empirical Analysis 7.1 Effectiveness of auxiliary problems English named entity German named entity 68 70 72 74 76 1 F-measure (%) 85 86 87 88 89 90 dev set F-measure (%) supervised w/ &amp;quot;Predict (previous, current, or next) words&amp;quot; w/ &amp;quot;Predict top-2 choices&amp;quot; w/ &amp;quot;Predict words&amp;quot; + &amp;quot;Predict top-2 choices&amp;quot; Figure 9: Named entity F-measure produced by using individual types of auxiliary problems. Trained with the entire training sets and tested on the test sets. Figure 9 shows F-meas</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of HLT-NAACL03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL-95.</booktitle>
<contexts>
<context position="2194" citStr="Yarowsky (1995)" startWordPosition="332" endWordPosition="333"> learning. Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. For example, co-training (Blum and Mitchell, 1998) automatically bootstraps labels, and such labels are not necessarily reliable (Pierce and Cardie, 2001). A related idea is to use Expectation Maximization (EM) to impute labels. Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g. Merialdo (1994)). A number of bootstrapping methods have been proposed for NLP tasks (e.g. Yarowsky (1995), Collins and Singer (1999), Riloff and Jones (1999)). But these typically assume a very small amount of labeled data and have not been shown to improve state-of-the-art performance when a large amount of labeled data is available. Our goal has been to develop a general learning framework for reliably using unlabeled data to improve performance irrespective of the amount of labeled data available. It is exactly this important and difficult problem that we tackle here. This paper presents a novel semi-supervised method that employs a learning framework called structural learning (Ando and Zhang</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of ACL-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
<author>David E Johnson</author>
</authors>
<title>A robust risk minimization based named entity recognition system.</title>
<date>2003</date>
<contexts>
<context position="5775" citStr="Zhang and Johnson (2003)" startWordPosition="915" endWordPosition="918">est output value. A frequently used method for finding an accurate predictor ^ f is regularized empirical risk minimization (ERM), which minimizes an empirical loss of the predictor (with regularization) on the n training examples f(Xi;Yi)g: ^ f = argmin f n X i=1 L(f(Xi);Yi)+r(f) ! : L(\x01) is a loss function to quantify the difference between the prediction f(Xi) and the true output Yi, and r(\x01) is a regularization term to control the model complexity. ERM-based methods for discriminative learning are known to be effective for NLP tasks such as chunking (e.g. Kudoh and Matsumoto (2001), Zhang and Johnson (2003)). 2.2 Linear model for structural learning We present a linear prediction model for structural learning, which extends the traditional model to multiple problems. Specifically, we assume that there exists a low-dimensional predictive structure shared by multiple prediction problems. We seek to discover this structure through joint empirical risk minimization over the multiple problems. Consider m problems indexed by ` 2 f1;::: ;mg, each with n` samples (X` i ;Y ` i ) indexed by i 2 f1;::: ;n`g. In our joint linear model, a predictor for problem ` takes the following form f`(\x02;x) = wT ` x+v</context>
<context position="22078" citStr="Zhang and Johnson, 2003" startWordPosition="3580" endWordPosition="3583">rmation, which was used in all other systems. The CoNLL corpora are annotated with four types of named entities: persons, organizations, locations, and miscellaneous names (e.g., World Cup). We use the official training/development/test splits. Our unlabeled data sets consist of 27 million words (English) and 35 million words (German), respectively. They were chosen from the same sources Reuters and ECI Multilingual Text Corpus as the provided corpora but disjoint from them. 5.1 Features Our feature representation is a slight modification of a simpler configuration (without any gazetteer) in (Zhang and Johnson, 2003), as shown in Figure 2. We use POS and syntactic chunk information provided by the organizer. 5.2 Auxiliary problems As shown in Figure 3, we experiment with auxiliary problems from Ex 3.1 and 3.2: Predict current (or previous or next) words; and Predict top-2 choices 3 http://cnts.uia.ac.be/conll2003/ner # of aux. Auxiliary Features used for problems labels learning aux problems 1000 previous words all but previous words 1000 current words all but current words 1000 next words all but next words 72 F1s top-2 choices \x082 (all but left context) 72 F2s top-2 choices \x081 (left context) 72 F3s</context>
</contexts>
<marker>Zhang, Johnson, 2003</marker>
<rawString>Tong Zhang and David E. Johnson. 2003. A robust risk minimization based named entity recognition system.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings CoNLL-2003,</booktitle>
<pages>204207</pages>
<marker></marker>
<rawString>In Proceedings CoNLL-2003, pages 204207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
<author>Fred Damerau</author>
<author>David E Johnson</author>
</authors>
<title>Text chunking based on a generalization of Winnow.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--615</pages>
<contexts>
<context position="27692" citStr="Zhang et al., 2002" startWordPosition="4458" endWordPosition="4461"> words on the left and their bi-grams. \x01 bi-grams of the current word and two labels on the left. Figure 6: Feature types for syntactic chunking. POS information is provided by the organizer. prec. recall F\x0c=1 supervised 93.83 93.37 93.60 ASO-semi 94.57 94.20 94.39 (+0.79) co/self oracle 93.76 93.56 93.66 (+0.06) Figure 7: Syntactic chunking results. use the WSJ articles in 1991 (15 million words) from the TREC corpus as the unlabeled data. 6.1 Features and auxiliary problems Our feature representation is a slight modification of a simpler configuration (without linguistic features) in (Zhang et al., 2002), as shown in Figure 6. We use the POS information provided by the organizer. The types of auxiliary problems are the same as in the named entity experiments. For word predictions, we exclude instances of punctuation symbols. 6.2 Syntactic chunking results As shown in Figure 7, ASO-semi improves both precision and recall over the supervised baseline. It achieves 94:39% in F-measure, which outperforms the supervised baseline by 0:79%. Co- and selftraining again slightly improve recall but slightly degrade precision at their oracle performance, which demonstrates that it is not easy to benefit f</context>
<context position="29376" citStr="Zhang et al., 2002" startWordPosition="4725" endWordPosition="4728">arser as features in ZDJ02+, which our system does not use. KM01 and CM03 boost performance by classifier combinations. SP03 trains conditional random fields for NP 7 \x0call NP description ASO-semi 94.39 94.70 +unlabeled data KM01 93.91 94.39 SVM combination CM03 93.74 94.41 perceptron in two layers SP03 94.38 conditional random fields ZDJ02 93.57 93.89 generalized Winnow ZDJ02+ 94.17 94.38 +full parser output Figure 8: Syntactic chunking F-measure. Comparison with previous best results: KM01 (Kudoh and Matsumoto, 2001), CM03 (Carreras and Marquez, 2003), SP03 (Sha and Pereira, 2003), ZDJ02 (Zhang et al., 2002). (noun phrases) only. ASO-semi produces higher NP chunking performance than the others. 7 Empirical Analysis 7.1 Effectiveness of auxiliary problems English named entity German named entity 68 70 72 74 76 1 F-measure (%) 85 86 87 88 89 90 dev set F-measure (%) supervised w/ &amp;quot;Predict (previous, current, or next) words&amp;quot; w/ &amp;quot;Predict top-2 choices&amp;quot; w/ &amp;quot;Predict words&amp;quot; + &amp;quot;Predict top-2 choices&amp;quot; Figure 9: Named entity F-measure produced by using individual types of auxiliary problems. Trained with the entire training sets and tested on the test sets. Figure 9 shows F-measure obtained by computing \x</context>
</contexts>
<marker>Zhang, Damerau, Johnson, 2002</marker>
<rawString>Tong Zhang, Fred Damerau, and David E. Johnson. 2002. Text chunking based on a generalization of Winnow. Journal of Machine Learning Research, 2:615 637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
</authors>
<title>Solving large scale linear prediction problems using stochastic gradient descent algorithms.</title>
<date>2004</date>
<contexts>
<context position="2801" citStr="Zhang, 2004" startWordPosition="428" endWordPosition="429">1995), Collins and Singer (1999), Riloff and Jones (1999)). But these typically assume a very small amount of labeled data and have not been shown to improve state-of-the-art performance when a large amount of labeled data is available. Our goal has been to develop a general learning framework for reliably using unlabeled data to improve performance irrespective of the amount of labeled data available. It is exactly this important and difficult problem that we tackle here. This paper presents a novel semi-supervised method that employs a learning framework called structural learning (Ando and Zhang, 2004), which seeks to discover shared predictive structures (i.e. what good classifiers for the task are like) through jointly learning multiple classification problems on unlabeled data. That is, we systematically create thousands of problems (called auxiliary problems) relevant to the target task using unlabeled data, and train classifiers from the automatically generated training data. We learn the commonality (or structure) of such many classifiers relevant to the task, and use it to improve performance on the target task. One example of such auxiliary problems for chunking tasks is to mask a w</context>
<context position="7419" citStr="Zhang, 2004" startWordPosition="1204" endWordPosition="1205"> structural learning may also be regarded as learning a good feature map \x02x a low-dimensional feature vector parameterized by \x02. In joint ERM, we seek \x02(and weight vectors) that minimizes the empirical risk summed over all the problems: [^ \x02;f^ f`g =argmin \x02;ff`g m X `=1 n X i=1 L(f`(\x02;X` i);Y ` i ) n` +r(f`) ! : (2) It can be shown that using joint ERM, we can reliably estimate the optimal joint parameter \x02 as long as m is large (even when each n` is small). This is the key reason why structural learning is effective. A formal PAC-style analysis can be found in (Ando and Zhang, 2004). 2.3 Alternating structure optimization (ASO) The optimization problem (2) has a simple solution using SVD when we choose square regularization: 2 \x0cr(f`) = \x15kw`k2 2 , where the regularization parameter \x15 is given. For clarity, let u` be a weight vector for problem ` such that: u` = w` + \x02T v` : Then, (2) becomes the minimization of the joint empirical risk written as: m X `=1 n X i=1 L(uT ` X` i;Y ` i ) n` +\x15ku` \x02T v`k2 2 ! : (3) This minimization can be approximately solved by the following alternating optimization procedure: \x0f Fix (\x02;fv`g), and find m predictors fu`g</context>
<context position="8917" citStr="Zhang, 2004" startWordPosition="1460" endWordPosition="1461">the problems. Its solution is given by the SVD (singular value decomposition) of the predictor matrix U = [u1;::: ;um: the rows of the optimum \x02are given by the most significant left singular vectors1 of U. Intuitively, the optimum \x02 captures the maximal commonality of the m predictors (each derived from u`). These m predictors are updated using the new structure matrix \x02 in the next iteration, and the process repeats. Figure 1 summarizes the algorithm sketched above, which we call the alternating structure optimization (ASO) algorithm. The formal derivation can be found in (Ando and Zhang, 2004). 2.4 Comparison with existing techniques It is important to note that this SVD-based ASO (SVD-ASO) procedure is fundamentally different from the usual principle component analysis (PCA), which can be regarded as dimension reduction in the data space X. By contrast, the dimension reduction performed in the SVD-ASO algorithm is on the predictor space (a set of predictors). This is possible because we observe multiple predictors from multiple learning tasks. If we regard the observed predictors as sample points of the predictor distribution in 1 In other words, \x02 is computed so that the best </context>
<context position="16838" citStr="Zhang, 2004" startWordPosition="2762" endWordPosition="2763"> 5 and 6. 4.1 Extension of the basic SVD-ASO algorithm In our experiments, we use an extension of SVDASO. In NLP applications, features have natural grouping according to their types/origins such as current words, parts-of-speech on the right, and so forth. It is desirable to perform a localized optimization for each of such natural feature groups. Hence, we associate each feature group with a submatrix of structure matrix \x02. The optimization algorithm for this extension is essentially the same as SVD-ASO in Figure 1, but with the SVD step performed separately for each group. See (Ando and Zhang, 2004) for the precise formulation. In addition, we regularize only those components of w` which correspond to the non-negative part of u`. The motivation is that positive weights are usually directly related to the target concept, while negative ones often yield much less specific information representing the others. The resulting extension, in effect, only uses the positive components of U in the SVD computation. 4.2 Chunking algorithm, loss function, training algorithm, and parameter settings As is commonly done, we encode chunk information into word tags to cast the chunking problem to that of s</context>
<context position="18090" citStr="Zhang, 2004" startWordPosition="2962" endWordPosition="2963">bistyle decoding to choose the word tag sequence that maximizes the sum of tagging confidence values. In all settings (including baseline methods), the loss function is a modification of the Hubers robust loss for regression: L(p;y) = max(0;1 py)2 if py \x15 1; and 4py otherwise; with square regularization (\x15 = 10 4). One may select other loss functions such as SVM or logistic regression. The specific choice is not important for the purpose of this paper. The training algorithm is stochastic gradient descent, which is argued to perform well for regularized convex ERM learning formulations (Zhang, 2004). As we will show in Section 7.3, our formulation is relatively insensitive to the change in h (rowdimension of the structure matrix). We fix h (for each feature group) to 50, and use it in all settings. The most time-consuming process is the training of m auxiliary predictors on the unlabeled data (computing U in Figure 1). Fixing the number of iterations to a constant, it runs in linear to m and the number of unlabeled instances and takes hours in our settings that use more than 20 million unlabeled instances. 4.3 Baseline algorithms Supervised classifier For comparison, we train a classifie</context>
</contexts>
<marker>Zhang, 2004</marker>
<rawString>Tong Zhang. 2004. Solving large scale linear prediction problems using stochastic gradient descent algorithms.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>