Word posterior probability p([w; , t]|X) of word w at time interval [, t] for speech signal X is calculated as follows CITATION: p([w; , t]|X) = X W W [w;,t] n p(X|W ) (p(W )) o p(X) , (1) where W is a sentence hypothesis, W [w; , t] is the set of sentence hypotheses that include w in [, t], p(X|W ) is a acoustic model score, p(W ) is a language model score, is a scaling parameter (&lt;1), and i,,
Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (CITATION; CITATION; CITATIONb; CITATION; CITATION),,
Generative NER models were used for multipass ASR and NER searches using word lattices (CITATIONb; CITATION; CITATION),,
We used an ASR engine CITATION with a speaker-independent acoustic model,,
Word posterior probability p([w; , t]|X) of word w at time interval [, t] for speech signal X is calculated as follows CITATION: p([w; , t]|X) = X W W [w;,t] n p(X|W ) (p(W )) o p(X) , (1) where W is a sentence hypothesis, W [w; , t] is the set of sentence hypotheses that include w in [, t], p(X|W ) is a ac,,
We used the training data of the Information Retrieval and Extraction Exercise (IREX) workshop CITATION as the text corpus, which consisted of 1,174 Japanese newspaper articles (10,718 sentences) and 18,200 NEs in eight categories (artifact, organization, location, person, date, time, money, and percent),,
On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (CITATION; CITATION), support vector machines (SVMs) CITATION, and conditional random fields (CRFs) CITATION,,
CITATION applied text-based NER to N-best ASR results, and merged the N-best NER results by weighted voting based on several sentence-level results such as ASR and NER scores,,
Many discriminative methods have been applied to NER, such as decision trees CITATION, ME models (CITATION; CITATION), and CRFs CITATION,,
3.3 ASR confidence scoring for using the proposed NER model ASR confidence scoring is an important technique in many ASR applications, and many methods have been proposed including using word posterior probabilities on word graphs CITATION, integrating several confidence measures using neural networks CITATION, using linear discriminant analysis CITATION, and using SVMs CITATION,,
CITATION applied a text-level ME-based NER to ASR results,,
Word posterior probability p([w; , t]|X) of word w at time interval [, t] for speech signal X is calculated as follows CITATION: p([w; , t]|X) = X W W [w;,t] n p(X|W ) (p(W )) o p(X) , (1) where W is a sentence hypothesis, W [w; , t] is the set of sentence hypotheses that include w in [, t], p(X|W ) is a acoustic model score, p(W ) is a language model score, is a scaling parameter (&lt;1), and is a language model weight,,
In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese CITATION,,
