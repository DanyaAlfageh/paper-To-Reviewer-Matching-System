For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq20) and several other statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION),,
Standard measures of accuracy are shown in table 1.3 Both the Fisher kernel and the TOP kernels show better accuracy than the baseline probabilistic 3 All our results are computed with the evalb program following the standard criteria in CITATION, and using the standard training (sections 222, 39,832 sentences, 910,196 words), validation (section 24, 1346 sentence, 31507 words), and testing (section 23, 2416 sentences, 54268 words) sets CITATION,,
The most sophisticated of these techniques (such as Support Vector Machines) are unfortunately too computationally expensive to be used on large datasets like the Penn Treebank CITATION,,
CITATION suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems,,
CITATION proposed to improve margin based methods for reranking by defining the margin not only between the top tree and all the other trees in the candidate list but between all the pairs of parses in the ordered candi,,
CITATION proposed to improve margin based methods for reranking by defining the margin not only between the top tree and all the other trees in the candidate list but between all the pairs of parses in the ordered candidate list for the given sentence,,
Instead, CITATION uses a neural network to induce a finite representation of this unbounded history, which we will denote h(d1,..., di1),,
When compared to other kernel methods, our approach performs better than those based on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for ,,
In CITATION it was pointed out that most of the arbitrary tree fragments allowed by the Tree kernel are linguistically meaningless,,
The resulting kernel is then used with the Voted Perceptron algorithm CITATION to reranking the top 20 parses from the probabilistic model,,
sed on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for parsing (CITATION; CITATION),,
The resulting kernel is then used with the Voted Perceptron algorithm CITATION to reranking the top 20 parses from the probabilistic mo,,
Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (CITATION; CITATION),,
CITATION applied an SVM based voting algorithm with the Preference kernel defined over ,,
2.1 Fisher Kernels The Fisher kernel CITATION is one of the best known kernels belonging to the class of probability model based kernels,,
For the probabilistic model, we use a state-of-the-art neural network based statistical parser CITATION,,
For this we use a statistical parser which has previously been shown to achieve state-of-the-art performance, namely that proposed in CITATION,,
articular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (CITATION; CITATION; CITATION; CITATION),,
We would expect some improvement if running it for more epochs, as has been empirically demonstrated in other domains CITATION,,
The VP algorithm was originally applied to parse reranking in CITATION with the Tree kernel,,
When compared to other kernel methods, our approach performs better than those based on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for parsing (CITATION; CITATION),,
In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (CITATION; CITATION; CITATION; CITATION),,
He defines the mapping from phrase structure trees to parse sequences using a form of left-corner parsing strategy (see CITATION for more details),,
When compared to other kernel methods, our approach performs better than those based on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for parsing (Shen et al., 200,,
Instead we use a 184 \x0cmethod which has often been shown to be virtually as good, the Voted Perceptron (VP) CITATION algorithm,,
CITATION applied an SVM based voting algorithm with the Preference kernel defined over pairs for reranking,,
First note that the parser based on the TOP efficient kernel has better accuracy than CITATION, which used the same parsing method as our baseline model, although the trained network parameters were not the same,,
This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in CITATION,,
In each case the input to the network is a sequence of tag-word pairs.2 We report results for two different vocabulary sizes, varying in the frequency with which tag-word pairs must 2 We used a publicly available tagger CITATION to provide the tags,,
5 The Experimental Results We used the Penn Treebank WSJ corpus CITATION to perform empirical experiments on the proposed parsing models,,
We expect that an improvement could be achieved by combining our approach of scaling updates by the F1 loss with the all pairs approach of CITATION,,
Use of the F1 loss function during training demonstrated better performance comparing to the 0-1 loss function when applied to a structured classification task CITATION,,
1 For example, see CITATION for a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly,,
When compared to other kernel methods, our approach performs better than those based on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for parsing (CITATION; Shen and Joshi,,
This approach is very similar to slack variable rescaling for Support Vector Machines proposed in CITATION,,
4 We measured significance with the randomized significance test of CITATION,,
ning it for more epochs, as has been empirically demonstrated in other domains CITATION,,
Standard measures of accuracy are shown in table 1.3 Both the Fisher kernel and the TOP kernels show better accuracy than the baseline probabilistic 3 All our results are computed with the evalb program following the standard criteria in CITATION, and using the standard training (sections 222, 39,832 sentences, 910,196 words), validatio,,
3.1 A History-Based Probability Model As with many other statistical parsers (CITATION; CITATION; CITATION), CITATION uses a history-based model of parsing,,
To construct a new TOP kernel for reranking, we apply an approach similar to that used for the TOP kernel CITATION, but we consider the probability P(yk|x, y1, ,,
