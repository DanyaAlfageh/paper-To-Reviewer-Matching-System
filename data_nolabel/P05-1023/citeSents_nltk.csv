For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq20) and several other statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION).,,
First note that the parser based on the TOP efficient kernel has better accuracy than CITATION, which used the same parsing method as our baseline model, although the trained network parameters were not the same.,,
When compared to other kernel methods, our approach performs better than those based on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for parsing (CITATION; CITATION).,,
3.1 A History-Based Probability Model As with many other statistical parsers (CITATION; CITATION; CITATION), CITATION uses a history-based model of parsing.,,
He defines the mapping from phrase structure trees to parse sequences using a form of left-corner parsing strategy (see CITATION for more details).,,
For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq20) and several other statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION).,,
First note that the parser based on the TOP efficient kernel has better accuracy than CITATION, which used the same parsing method as our baseline model, although the trained network parameters were not the same.,,
When compared to other kernel methods, our approach performs better than those based on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for parsing (CITATION; CITATION).,,
In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (CITATION; CITATION; CITATION; CITATION).,,
Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (CITATION; CITATION).,,
The most sophisticated of these techniques (such as Support Vector Machines) are unfortunately too computationally expensive to be used on large datasets like the Penn Treebank CITATION.,,
Instead we use a 184 \x0cmethod which has often been shown to be virtually as good, the Voted Perceptron (VP) CITATION algorithm.,,
The VP algorithm was originally applied to parse reranking in CITATION with the Tree kernel.,,
For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq20) and several other statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION).,,
First note that the parser based on the TOP efficient kernel has better accuracy than CITATION, which used the same parsing method as our baseline model, although the trained network parameters were not the same.,,
When compared to other kernel methods, our approach performs better than those based on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for,,
In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (CITATION; CITATION; CITATION; CITATION).,,
Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (CITATION; CITATION).,,
For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq20) and several other statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION).,,
First note that the parser based on the TOP efficient kernel has better accuracy than CITATION, which used the same parsing method as our baseline model, although the trained network parameters were not the same.,,
When compared to other kernel methods, our approach performs better than those based on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for parsing (Shen et al., 200,,
3.1 A History-Based Probability Model As with many other statistical parsers (CITATION; CITATION; CITATION), CITATION uses a history-based model of parsing.,,
He defines the mapping from phrase structure trees to parse sequences using a form of left-corner parsing strategy (see CITATION for more details).,,
ning it for more epochs, as has been empirically demonstrated in other domains CITATION.,,
Standard measures of accuracy are shown in table 1.3 Both the Fisher kernel and the TOP kernels show better accuracy than the baseline probabilistic 3 All our results are computed with the evalb program following the standard criteria in CITATION, and using the standard training (sections 222, 39,832 sentences, 910,196 words), validation (section 24, 1346 sentence, 31507 words), and testing (section 23, 2416 sentences, 54268 words) sets CITATION.,,
For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq20) and several other statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION).,,
First note that the parser based on the TOP efficient kernel has better accuracy than CITATION, which used the same parsing method as our baseline model, although the trained network parameters were not the same.,,
When compared to other kernel methods, our approach performs better than those based on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for parsing (CITATION; CITATION).,,
1 For example, see CITATION for a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly.,,
For the probabilistic model, we use a state-of-the-art neural network based statistical parser CITATION.,,
The resulting kernel is then used with the Voted Perceptron algorithm CITATION to reranking the top 20 parses from the probabilistic model.,,
The most sophisticated of these techniques (such as Support Vector Machines) are unfortunately too computationally expensive to be used on large datasets like the Penn Treebank CITATION.,,
Instead we use a 184 \x0cmethod which has often been shown to be virtually as good, the Voted Perceptron (VP) CITATION algorithm.,,
The VP algorithm was originally applied to parse reranking in CITATION with the Tree kernel.,,
We would expect some improvement if running it for more epochs, as has been empirically demonstrated in other domains CITATION.,,
Standard measures of accuracy are shown in table 1.3 Both the Fisher kernel and the TOP kernels show better accuracy than the baseline probabilistic 3 All our results are computed with the evalb program following the standard criteria in CITATION, and using the standard training (sections 222, 39,832 sentences, 910,196 words), validatio,,
1 For example, see CITATION for a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly.,,
For the probabilistic model, we use a state-of-the-art neural network based statistical parser CITATION.,,
The resulting kernel is then used with the Voted Perceptron algorithm CITATION to reranking the top 20 parses from the probabilistic model.,,
For this we use a statistical parser which has previously been shown to achieve state-of-the-art performance, namely that proposed in CITATION.,,
3.1 A History-Based Probability Model As with many other statistical parsers (CITATION; CITATION; CITATION), CITATION uses a history-based model of parsing.,,
He defines the mapping from phrase structure trees to parse sequences using a form of left-corner parsing strategy (see CITATION for more details).,,
Instead, CITATION uses a neural network to induce a finite representation of this unbounded history, which we will denote h(d1,..., di1).,,
This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in CITATION.,,
For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq20) and several other statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION).,,
First note that the parser based on the TOP efficient kernel has better accuracy than CITATION, which used the same parsing method as our baseline model, although the trained network parameters were not the same.,,
When compared to other kernel methods, our approach performs better than those based on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for parsing (CITATION; Shen and Joshi,,
1 For example, see CITATION for a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly.,,
For the probabilistic model, we use a state-of-the-art neural network based statistical parser CITATION.,,
The resulting kernel is then used with the Voted Perceptron algorithm CITATION to reranking the top 20 parses from the probabilistic mo,,
For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq20) and several other statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION).,,
First note that the parser based on the TOP efficient kernel has better accuracy than CITATION, which used the same parsing method as our baseline model, although the trained network parameters were not the same.,,
When compared to other kernel methods, our approach performs better than those based on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for parsing (CITATION; CITATION).,,
In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (CITATION; CITATION; CITATION; CITATION).,,
Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (CITATION; CITATION).,,
2.1 Fisher Kernels The Fisher kernel CITATION is one of the best known kernels belonging to the class of probability model based kernels.,,
The most sophisticated of these techniques (such as Support Vector Machines) are unfortunately too computationally expensive to be used on large datasets like the Penn Treebank CITATION.,,
Instead we use a 184 \x0cmethod which has often been shown to be virtually as good, the Voted Perceptron (VP) CITATION algorithm.,,
The VP algorithm was originally applied to parse reranking in CITATION with the Tree kernel.,,
5 The Experimental Results We used the Penn Treebank WSJ corpus CITATION to perform empirical experiments on the proposed parsing models.,,
In each case the input to the network is a sequence of tag-word pairs.2 We report results for two different vocabulary sizes, varying in the frequency with which tag-word pairs must 2 We used a publicly available tagger CITATION to provide the tags.,,
5 The Experimental Results We used the Penn Treebank WSJ corpus CITATION to perform empirical experiments on the proposed parsing models.,,
In each case the input to the network is a sequence of tag-word pairs.2 We report results for two different vocabulary sizes, varying in the frequency with which tag-word pairs must 2 We used a publicly available tagger CITATION to provide the tags.,,
3.1 A History-Based Probability Model As with many other statistical parsers (CITATION; CITATION; CITATION), CITATION uses a history-based model of parsing.,,
He defines the mapping from phrase structure trees to parse sequences using a form of left-corner parsing strategy (see CITATION for more details).,,
In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (CITATION; CITATION; CITATION; CITATION).,,
Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (CITATION; CITATION).,,
4 We measured significance with the randomized significance test of CITATION.,,
CITATION applied an SVM based voting algorithm with the Preference kernel defined over pairs for reranking.,,
In CITATION it was pointed out that most of the arbitrary tree fragments allowed by the Tree kernel are linguistically meaningless.,,
For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq20) and several other statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION).,,
First note that the parser based on the TOP efficient kernel has better accuracy than CITATION, which used the same parsing method as our baseline model, although the trained network parameters were not the same.,,
When compared to other kernel methods, our approach performs better than those based on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for parsing (CITATION; CITATION).,,
In CITATION it was pointed out that most of the arbitrary tree fragments allowed by the Tree kernel are linguistically meaningless.,,
CITATION proposed to improve margin based methods for reranking by defining the margin not only between the top tree and all the other trees in the candidate list but between all the pairs of parses in the ordered candidate list for the given sentence.,,
In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (CITATION; CITATION; CITATION; CITATION).,,
Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (CITATION; CITATION).,,
For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq20) and several other statistical parsers (CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION; CITATION).,,
First note that the parser based on the TOP efficient kernel has better accuracy than CITATION, which used the same parsing method as our baseline model, although the trained network parameters were not the same.,,
When compared to other kernel methods, our approach performs better than those based on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for parsing (CITATION; CITATION).,,
CITATION applied an SVM based voting algorithm with the Preference kernel defined over pairs for reranking.,,
In CITATION it was pointed out that most of the arbitrary tree fragments allowed by the Tree kernel are linguistically meaningless.,,
CITATION proposed to improve margin based methods for reranking by defining the margin not only between the top tree and all the other trees in the candidate list but between all the pairs of parses in the ordered candi,,
We expect that an improvement could be achieved by combining our approach of scaling updates by the F1 loss with the all pairs approach of CITATION.,,
Use of the F1 loss function during training demonstrated better performance comparing to the 0-1 loss function when applied to a structured classification task CITATION.,,
CITATION suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems.,,
The natural choice for the loss function would be (yj k, yj 1) = F1(yj 1) F1(yj k), where F1(yj k) denotes the F1 score value for the parse tree yj k. This approach is very similar to slack variable rescaling for Support Vector Machines proposed in CITATION.,,
We expect that an improvement could be achieved by combining our approach of scaling updates by the F1 loss with the all pairs approach of CITATION.,,
Use of the F1 loss function during training demonstrated better performance comparing to the 0-1 loss function when applied to a structured classification task CITATION.,,
CITATION suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems.,,
articular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (CITATION; CITATION; CITATION; CITATION).,,
Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (CITATION; CITATION).,,
To construct a new TOP kernel for reranking, we apply an approach similar to that used for the TOP kernel CITATION, but we consider the probability P(yk|x, y1, .,,
sed on the Tree kernel (CITATION; CITATION), and is only 0.2% worse than the best results achieved by a kernel method for parsing (CITATION; CITATION).,,
4 We measured significance with the randomized significance test of CITATION.,,
CITATION applied an SVM based voting algorithm with the Preference kernel defined over,,
