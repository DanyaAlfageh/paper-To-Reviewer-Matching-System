Applying System Combination to Base Noun Phrase Identication
Erik F. Tjong Kim Sang, Walter Daelemans, Herv
e D
ejean ,
Rob Koeling, Yuval Krymolowski, Vasin Punyakanok, Dan Roth
University of Antwerp Universit
at T
ubingen SRI Cambridge
Universiteitsplein 1 Kleine Wilhelmstrae 113 23 Millers Yard,Mill Lane
B-2610 Wilrijk, Belgium D-72074 T
ubingen, Germany Cambridge, CB2 1RQ, UK
ferikt,daelemg@uia.ua.ac.be dejean@sfs.nphil.uni-tuebingen.de koeling@cam.sri.com
Bar-Ilan University University of Illinois
Ramat Gan, 52900, Israel 1304 W. Springeld Ave.
yuvalk@macs.biu.ac.il Urbana, IL 61801, USA
fpunyakan,danrg@cs.uiuc.edu
Abstract
We use seven machine learning algorithms for
one task: identifying base noun phrases. The
results have been processed by dierent system
combination methods and all of these outper-
formed the best individual result. We have ap-
plied the seven learners with the best combina-
tor, a majority vote of the top ve systems, to a
standard data set and managed to improve the
best published result for this data set.
1 Introduction
Van Halteren et al. (1998) and Brill and Wu
(1998) show that part-of-speech tagger perfor-
mance can be improved by combining dierent
taggers. By using techniques such as majority
voting, errors made by the minority of the tag-
gers can be removed. Van Halteren et al. (1998)
report that the results of such a combined ap-
proach can improve upon the accuracy error of
the best individualsystem with as much as 19%.
The positive eect of system combination for
non-language processing tasks has been shown
in a large body of machine learning work.
In this paper we will use system combination
for identifying base noun phrases (baseNPs).
We will apply seven machine learning algo-
rithms to the same baseNP task. At two points
we will apply combination methods. We will
start with making the systems process ve out-
put representations and combine the results by
choosing the majority of the output features.
Three of the seven systems use this approach.
After this we will make an overall combination
of the results of the seven systems. There we
will evaluate several system combination meth-
ods. The best performing method will be ap-
plied to a standard data set for baseNP identi-
cation.
2 Methods and experiments
In this section we willdescribeour learningtask:
recognizing base noun phrases. After this we
will describe the data representations we used
and the machine learning algorithms that we
will apply to the task. We will conclude with
an overview of the combination methods that
we will test.
2.1 Task description
Base noun phrases (baseNPs) are noun phrases
which do not contain another noun phrase. For
example, the sentence
In [ early trading ] in [ Hong Kong ]
[ Monday ] , [ gold ] was quoted at
[ $ 366.50 ] [ an ounce ] .
contains six baseNPs (marked as phrases be-
tween square brackets). The phrase $ 366.50
an ounce is a noun phrase as well. However, it
is not a baseNP since it contains two other noun
phrases. Two baseNP data sets have been put
forward by Ramshaw and Marcus (1995). The
main data set consist of four sections of the Wall
Street Journal (WSJ) part of the Penn Tree-
bank (Marcus et al., 1993) as training mate-
rial (sections 15-18, 211727 tokens) and one sec-
tion as test material (section 20, 47377 tokens)1.
The data contains words, their part-of-speech
1
This Ramshaw and Marcus (1995) baseNP data set
is available via ftp://ftp.cis.upenn.edu/pub/chunker/
(POS) tags as computed by the Brill tagger and
their baseNP segmentation as derived from the
Treebank (with some modications).
In the baseNP identication task, perfor-
mance is measured with three rates. First,
with the percentage of detected noun phrases
that are correct (precision). Second, with the
percentage of noun phrases in the data that
were found by the classier (recall). And third,
with the F=1 rate which is equal to (2*preci-
sion*recall)/(precision+recall). The latter rate
has been used as the target for optimization.
2.2 Data representation
In our example sentence in section 2.1, noun
phrases are represented by bracket structures.
It has been shown by Mu~
noz et al. (1999)
that for baseNP recognition, the representa-
tion with brackets outperforms other data rep-
resentations. One classier can be trained to
recognize open brackets (O) and another can
handle close brackets (C). Their results can be
combined by making pairs of open and close
brackets with large probability scores. We have
used this bracket representation (O+C) as well.
However, we have not used the combination
strategy from Mu~
noz et al. (1999) but in-
stead used the strategy outlined in Tjong Kim
Sang (2000): regard only the shortest possi-
ble phrases between candidate open and close
brackets as base noun phrases.
An alternative representation for baseNPs
has been put forward by Ramshaw and Mar-
cus (1995). They have dened baseNP recog-
nition as a tagging task: words can be inside a
baseNP (I) or outside a baseNP (O). In the case
that one baseNP immediately follows another
baseNP, the rst word in the second baseNP
receives tag B. Example:
InO earlyI tradingI inO HongI KongI
MondayB ,O goldI wasO quotedO atO
$I 366.50I anB ounceI .O
This set of three tags is sucient for encod-
ing baseNP structures since these structures are
nonrecursive and nonoverlapping.
Tjong Kim Sang (2000) outlines alternative
versions of this tagging representation. First,
the B tag can be used for the rst word of ev-
ery baseNP (IOB2 representation). Second, in-
stead of the B tag an E tag can be used to
mark the last word of a baseNP immediately
before another baseNP (IOE1). And third, the
E tag can be used for every noun phrase nal
word (IOE2). He used the Ramshaw and Mar-
cus (1995) representation as well (IOB1). We
will use these four tagging representations and
the O+C representation for the system-internal
combination experiments.
2.3 Machine learning algorithms
This section contains a brief description of the
seven machine learning algorithms that we will
apply to the baseNP identication task: AL-
LiS, c5.0, IGTree, MaxEnt, MBL, MBSL and
SNoW.
ALLiS2 (Architecture for Learning Linguistic
Structures) is a learning system which uses the-
ory renement in order to learn non-recursive
NP and VP structures (D
ejean, 2000). ALLiS
generates a regular expression grammar which
describes the phrase structure (NP or VP). This
grammar is then used by the CASS parser (Ab-
ney, 1996). Following the principle of theory re-
nement, the learning task is composed of two
steps. The rst step is the generation of an
initial grammar. The generation of this gram-
mar uses the notion of default values and some
background knowledge which provides general
expectations concerning the inner structure of
NPs and VPs. This initial grammar provides
an incomplete and/or incorrect analysis of the
data. The second step is the renement of this
grammar. During this step, the validity of the
rules of the initial grammar is checked and the
rules are improved (rened) if necessary. This
renement relies on the use of two operations:
the contextualization (in which contexts such a
tag always belongs to the phrase) and lexical-
ization (use of information about the words and
not only about POS).
c5.03, a commercial version of c4.5 (Quin-
lan, 1993), performs top-down induction of de-
cision trees (tdidt). On the basis of an in-
stance base of examples, c5.0 constructs a deci-
sion tree which compresses the classication in-
formation in the instance base by exploiting dif-
ferences in relative importance of dierent fea-
tures. Instances are stored in the tree as paths
2
A demo of the NP and VP chunker is available at
http://www.sfb441.unituebingen.de/~dejean/chunker.h
tml
3
Available from http://www.rulequest.com
of connected nodes ending in leaves which con-
tain classication information. Nodes are con-
nected via arcs denoting feature values. Feature
information gain (mutual information between
features and class) is used to determine the or-
der in which features are employed as tests at all
levels of the tree (Quinlan, 1993). With the full
input representation (words and POS tags), we
were not able to run complete experiments. We
therefore experimented only with the POS tags
(with a context of two left and right). We have
used the default parameter setting with decision
trees combined with value grouping.
We have used a nearest neighbor algorithm
(ib1-ig, here listed as MBL) and a decision tree
algorithm (IGTree) from the TiMBL learning
package (Daelemans et al., 1999b). Both algo-
rithms store the training data and classify new
items by choosing the most frequent classica-
tion among training items which are closest to
this new item. Data items are represented as
sets of feature-value pairs. Each feature receives
a weight which is based on the amount of in-
formation which it provides for computing the
classication of the items in the training data.
ib1-ig uses these weights for computing the dis-
tance between a pair of data items and IGTree
uses them for deciding which feature-value de-
cisions should be made in the top nodes of the
decision tree (Daelemans et al., 1999b). We
will use their default parameters except for the
ib1-ig parameter for the number of examined
nearest neighbors (k) which we have set to 3
(Daelemans et al., 1999a). The classiers use a
left and right context of four words and part-
of-speech tags. For the four IO representations
we have used a second processing stage which
used a smaller context but which included in-
formation about the IO tags predicted by the
rst processing phase (Tjong Kim Sang, 2000).
When building a classier, one must gather
evidence for predicting the correct class of an
item from its context. The Maximum Entropy
(MaxEnt) framework is especially suited for
integrating evidence from various information
sources. Frequencies of evidence/class combi-
nations (called features) are extracted from a
sample corpus and considered to be properties
of the classication process. Attention is con-
strained to models with these properties. The
MaxEnt principle now demands that among all
the probability distributions that obey these
constraints, the most uniform is chosen. Dur-
ing training, features are assigned weights in
such a way that, given the MaxEnt principle,
the training data is matched as well as possible.
During evaluation it is tested which features are
active (i.e. a feature is active when the context
meets the requirements given by the feature).
For every class the weights of the active fea-
tures are combined and the best scoring class
is chosen (Berger et al., 1996). For the classi-
er built here the surrounding words, their POS
tags and baseNP tags predicted for the previous
words are used as evidence. A mixture of simple
features (consisting of one of the mentioned in-
formation sources) and complex features (com-
binations thereof) were used. The left context
never exceeded 3 words, the right context was
maximally 2 words. The model was calculated
using existing software (Dehaspe, 1997).
MBSL (Argamon et al., 1999) uses POS data
in order to identify baseNPs. Inference re-
lies on a memory which contains all the oc-
currences of POS sequences which appear in
the beginning, or the end, of a baseNP (in-
cluding complete phrases). These sequences
may include a few context tags, up to a pre-
specied max context. During inference, MBSL
tries to 'tile' each POS string with parts of
noun-phrases from the memory. If the string
could be fully covered by the tiles, it becomes
part of a candidate list, ambiguities between
candidates are resolved by a constraint propa-
gation algorithm. Adding a context extends the
possibilities for tiling, thereby giving more op-
portunities to better candidates. The approach
of MBSL to the problem of identifying baseNPs
is sequence-based rather than word-based, that
is, decisions are taken per POS sequence, or per
candidate, but not for a single word. In addi-
tion, the tiling process gives no preference to
any direction in the sentence. The tiles may be
of any length, up to the maximal length of a
phrase in the training data, which gives MBSL
a generalization power that compensates for the
setup of using only POS tags. The results pre-
sented here were obtained by optimizing MBSL
parameters based on 5-fold CV on the training
data.
SNoW uses the Open/Close model, described
in Mu~
noz et al. (1999). As is shown there, this
MBL MaxEnt IGTree
section 21 O C F=1 O C F=1 O C F=1
IOB1 97.81% 97.97% 91.68 97.90% 98.11% 92.43 96.62% 96.89% 87.88
IOB2 97.63% 97.96% 91.79 97.81% 98.14% 92.14 97.27% 97.30% 90.03
IOE1 97.80% 97.92% 91.54 97.88% 98.12% 92.37 95.88% 96.01% 82.80
IOE2 97.72% 97.94% 92.06 97.84% 98.12% 92.13 97.19% 97.62% 89.98
O+C 97.72% 98.04% 92.03 97.82% 98.15% 92.26 96.89% 97.49% 89.37
Majority 98.04% 98.20% 92.82 97.94% 98.24% 92.60 97.70% 97.99% 91.92
Table 1: The eects of system-internal combination by using dierent output representations. A
straight-forward majority vote of the output yields better bracket accuracies and F=1 rates than
any included individual classier. The bracket accuracies in the columns O and C show what
percentage of words was correctly classied as baseNP start, baseNP end or neither.
model produced better results than the other
paradigm evaluated there, the Inside/Outside
paradigm. The Open/Close model consists of
two SNoW predictors, one of which predicts the
beginning of baseNPs (Open predictor), and the
other predicts the end of the phrase (Close pre-
dictor). The Open predictor is learned using
SNoW (Carlson et al., 1999; Roth, 1998) as a
function of features that utilize words and POS
tags in the sentence and, given a new sentence,
will predict for each word whether it is the rst
word in the phrase or not. For each Open, the
Close predictoris learnedusingSNoW as a func-
tion of features that utilize the words in the sen-
tence, the POS tags and the open prediction. It
will predict, for each word, whether it can be
the end of the phrase, given the previously pre-
dicted Open. Each pair of predicted Open and
Close forms a candidate of a baseNP. These can-
didates may conict due to overlapping; at this
stage, a graph-based constraint satisfaction al-
gorithm that uses the condence values SNoW
associates with its predictionsis employed. This
algorithm ("the combinator") produces the list
of the nal baseNPs for each sentence. Details
of SNoW, its application in shallow parsing and
the combinator's algorithm are in Mu~
noz et al.
(1999).
2.4 Combination techniques
At two points in our noun phrase recognition
process we willuse system combination. We will
start with system-internal combination: apply
the same learning algorithm to variants of the
task and combine the results. The approach
we have chosen here is the same as in Tjong
Kim Sang (2000): generate dierent variants
of the task by using dierent representations
of the output (IOB1, IOB2, IOE1, IOE2 and
O+C). The ve outputs will converted to the
open bracket representation (O) and the close
bracket representation (C) and after this, the
most frequent of the ve analyses of each word
will chosen (majority voting, see below). We
expect the systems which use this combination
phase to perform better than their individual
members (Tjong Kim Sang, 2000).
Our seven learners willgenerate dierent clas-
sications of the training data and we need to
nd out which combination techniques are most
appropriate. For the system-external combi-
nation experiment, we have evaluated dierent
voting mechanisms, eectively the voting meth-
ods as described in Van Halteren et al. (1998).
In the rst method each classication receives
the same weight and the most frequent classi-
cation is chosen (Majority). The second method
regards as the weight of each individual clas-
sication algorithm its accuracy on some part
of the data, the tuning data (TotPrecision).
The third voting method computes the preci-
sion of each assigned tag per classier and uses
this value as a weight for the classier in those
cases that it chooses the tag (TagPrecision).
The fourth method uses both the precision of
each assigned tag and the recall of the com-
peting tags (Precision-Recall). Finally, the fth
method uses not only a weight for the current
classication but it also computes weights for
other possible classications. The other classi-
cations are determined by examining the tun-
ing data and registering the correct values for
every pair of classier results (pair-wise voting,
see Van Halteren et al. (1998) for an elaborate
explanation).
Apart from these ve voting methods we have
also processed the output streams with two clas-
siers: MBL and IGTree. This approach is
called classier stacking. Like Van Halteren et
al. (1998), we have used dierent input ver-
sions: one containing only the classier output
and another containing both classier output
and a compressed representation of the data
item under consideration. For the latter pur-
pose we have used the part-of-speech tag of the
current word.
3 Results4
We want to nd out whether system combi-
nation could improve performance of baseNP
recognition and, if this is the fact, we want to
select the best combination technique. For this
purpose we have performed an experiment with
sections 15-18 of the WSJ part of the Penn Tree-
bank as training data (211727 tokens) and sec-
tion 21 as test data (40039 tokens). Like the
data used by Ramshaw and Marcus (1995), this
data was retagged by the Brill tagger in order
to obtain realistic part-of-speech (POS) tags5.
The data was segmented into baseNP parts and
non-baseNP parts in a similar fashion as the
data used by Ramshaw and Marcus (1995). Of
the training data, only 90% was used for train-
ing. The remaining 10% was used as tuning
data for determining the weights of the combi-
nation techniques.
For three classiers (MBL, MaxEnt and
IGTree) we have used system-internal combi-
nation. These learning algorithms have pro-
cessed ve dierent representations of the out-
put (IOB1, IOB2, IOE1, IOE2 and O+C) and
the results have been combined with majority
voting. The test data results can be found in
Table 1. In all cases, the combined results were
better than that of the best included system.
The resultsof ALLiS,c5.0, MBSL andSNoW
have been converted to the O and the C repre-
4
Detailed results of our experiments are available on
http://lcg-www.uia.ac.be/~erikt/npcombi/
5
The retagging was necessary to assure that the per-
formance rates obtained here would be similar to rates
obtained for texts for which no Treebank POS tags are
available.
section 21 O C F=1
Classier
ALLiS 97.87% 98.08% 92.15
c5.0 97.05% 97.76% 89.97
IGTree 97.70% 97.99% 91.92
MaxEnt 97.94% 98.24% 92.60
MBL 98.04% 98.20% 92.82
MBSL 97.27% 97.66% 90.71
SNoW 97.78% 97.68% 91.87
Simple Voting
Majority 98.08% 98.21% 92.95
TotPrecision 98.08% 98.21% 92.95
TagPrecision 98.08% 98.21% 92.95
Precision-Recall 98.08% 98.21% 92.95
Pairwise Voting
TagPair 98.13% 98.23% 93.07
Memory-Based
Tags 98.24% 98.35% 93.39
Tags + POS 98.14% 98.33% 93.24
Decision Trees
Tags 98.24% 98.35% 93.39
Tags + POS 98.13% 98.32% 93.21
Table 2: Bracket accuracies and F=1 scores
for section WSJ 21 of the Penn Treebank with
seven individual classiers and combinations of
them. Each combination performs better than
its best individual member. The stacked classi-
ers without context information perform best.
sentation. Together with the bracket represen-
tations of the other three techniques, this gave
us a total of seven O results and seven C results.
These two data streams have been combined
with the combination techniques described in
section 2.4. After this, we built baseNPs from
the O and C results of each combination tech-
nique, like described in section 2.2. The bracket
accuracies and the F=1 scores for test data can
be found in Table 2.
All combinations improve the results of the
best individual classier. The best results were
obtained with a memory-based stacked classi-
er. This is dierent from the combination re-
sults presented in Van Halteren et al. (1998),
in which pairwise voting performed best. How-
ever, in their later work stacked classiers out-
perform voting methods as well (Van Halteren
et al., to appear).
section 20 accuracy precision recall F=1
Best-ve combination O:98.32% C:98.41% 94.18% 93.55% 93.86
Tjong Kim Sang (2000) O:98.10% C:98.29% 93.63% 92.89% 93.26
Mu~
noz et al. (1999) O:98.1% C:98.2% 92.4% 93.1% 92.8
Ramshaw and Marcus (1995) IOB1:97.37% 91.80% 92.27% 92.03
Argamon et al. (1999) - 91.6% 91.6% 91.6
Table 3: The overall performance of the majority voting combination of our best ve systems
(selected on tuning data performance) applied to the standard data set put forward by Ramshaw
and Marcus (1995) together with an overview of earlier work. The accuracy scores indicate how
often a word was classied correctly with the representation used (O, C or IOB1). The combined
system outperforms all earlier reported results for this data set.
Based on an earlier combination study
(Tjong Kim Sang, 2000) we had expected the
voting methods to do better. We suspect that
their performance is below that of the stacked
classiers because the dierence between the
best and the worst individual system is larger
than in our earlier study. We assume that the
voting methods might perform better if they
were only applied to the classiers that per-
form well on this task. In order to test this
hypothesis, we have repeated the combination
experiments with the best n classiers, where
n took values from 3 to 6 and the classiers
were ranked based on their performance on the
tuning data. The best performances were ob-
tained with ve classiers: F=1=93.44 for all
ve voting methods with the best stacked classi-
er reaching 93.24. With the top ve classiers,
the voting methods outperform the best combi-
nation with seven systems6. Adding extra clas-
sication results to a good combination system
should not make overall performance worse so
it is clear that there is some room left for im-
provement of our combination algorithms.
We conclude that the best results in this
task can be obtained with the simplest voting
method, majority voting, applied to the best
ve of our classiers. Our next task was to
apply the combination approach to a standard
data set so that we could compare our results
with other work. For this purpose we have used
6
We are unaware of a good method for determining
the signicance of F=1 dierences but we assume that
this F=1 dierence is not signicant. However, we be-
lieve that the fact that more combination methods per-
form well, shows that it easier to get a good performance
out of the best ve systems than with all seven.
the data put forward by Ramshaw and Marcus
(1995). Again, only 90% of the training data
was used for training while the remaining 10%
was reserved for ranking the classiers. The
seven learners were trained with the same pa-
rameters as in the previous experiment. Three
of the classiers (MBL, MaxEnt and IGTree)
used system-internal combination by processing
dierent output representations.
The classier output was converted to the
O and the C representation. Based on the
tuning data performance, the classiers ALLiS,
igtree, MaxEnt, MBL and SNoW were se-
lected for being combined with majority vot-
ing. After this, the resulting O and C repre-
sentations were combined to baseNPs by using
the method described in section 2.2. The re-
sults can be found in Table 3. Our combined
system obtains an F=1 score of 93.86 which
corresponds to an 8% error reduction compared
with the best published result for this data set
(93.26).
4 Concluding remarks
In this paper we have examined two methods for
combining the results of machine learning algo-
rithms for identifying base noun phrases. In the
rst method, the learnerprocessed dierent out-
put data representations and the results were
combined by majority voting. This approach
yielded better results than the best included
classier. In the second combination approach
we have combined the results of seven learning
systems (ALLiS, c5.0, IGTree, MaxEnt, MBL,
MBSL and SNoW). Here we have tested dif-
ferent combination methods. Each combination
method outperformed the best individual learn-
ing algorithm and a majority vote of the top
ve systems performed best. We have applied
this approach of system-internal and system-
external combination to a standard data set for
base noun phrase identication and the perfor-
mance of our system was better than any other
published result for this data set.
Our study shows that the combination meth-
ods that we have tested are sensitive for the in-
clusion of classier results of poor quality. This
leaves room for improvement of our results by
evaluating other combinators. Another interest-
ing approach which might lead to a better per-
formance is taking into account more context
information, for example by combining com-
plete phrases instead of independent brackets.
It would also be worthwhile to evaluate using
more elaborate methods for building baseNPs
out of open and close bracket candidates.
Acknowledgements
D
ejean, Koeling and Tjong Kim Sang are
funded by the TMR network Learning Compu-
tational Grammars7. Punyakanok and Roth are
supported by NFS grants IIS-9801638 and SBR-
9873450.
References
Steven Abney. 1996. Partial parsing via nite-
state cascades. In In Proceedings of the ESS-
LLI '96 Robust Parsing Workshop.
Shlomo Argamon, Ido Dagan, and Yuval Kry-
molowski. 1999. A memory-based approach
to learningshallow natural language patterns.
Journal of Experimental and Theoretical AI,
11(3).
Adam L. Berger, Stephen A. DellaPietra, and
Vincent J. DellaPietra. 1996. A maximum
entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1).
Eric Brill and Jun Wu. 1998. Classier com-
bination for improved lexical disambiguation.
In Proceedings of COLING-ACL '98. Associ-
ation for Computational Linguistics.
A. Carlson, C. Cumby, J. Rosen, and
D. Roth. 1999. The SNoW learning archi-
tecture. Technical Report UIUCDCS-R-99-
2101, UIUC Computer Science Department,
May.
7
http://lcg-www.uia.ac.be/
Walter Daelemans, Antal van den Bosch, and
Jakub Zavrel. 1999a. Forgetting exceptions
is harmful in language learning. Machine
Learning, 34(1).
Walter Daelemans, Jakub Zavrel, Ko van der
Sloot, and Antal van den Bosch. 1999b.
TiMBL: Tilburg Memory Based Learner, ver-
sion 2.0, Reference Guide. ILK Technical
Report 99-01. http://ilk.kub.nl/.
Luc Dehaspe. 1997. Maximum entropy model-
ing with clausal constraints. In Proceedings of
the 7th International Workshop on Inductive
Logic Programming.
Herv
e D
ejean. 2000. Theory renement and
natural language processing. In Proceedings
of the Coling2000. Association for Computa-
tional Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of english: the penn
treebank. Computational Linguistics, 19(2).
Marcia Munoz, Vasin Punyakanok, Dan Roth,
and Dav Zimak. 1999. A learning ap-
proach to shallow parsing. In Proceedings of
EMNLP-WVLC'99. Association for Compu-
tational Linguistics.
J. Ross Quinlan. 1993. c4.5: Programs for Ma-
chine Learning. Morgan Kaufmann.
Lance A. Ramshaw and Mitchell P. Marcus.
1995. Text chunking using transformation-
based learning. In Proceedings of the Third
ACL Workshop on Very Large Corpora. As-
sociation for Computational Linguistics.
D. Roth. 1998. Learning to resolve natural lan-
guage ambiguities: A unied approach. In
AAAI-98.
Erik F. Tjong Kim Sang. 2000. Noun phrase
recognition by system combination. In Pro-
ceedings of the ANLP-NAACL-2000. Seattle,
Washington, USA. Morgan Kaufman Pub-
lishers.
Hans van Halteren, Jakub Zavrel, and Wal-
ter Daelemans. 1998. Improving data driven
wordclass tagging by system combination. In
Proceedings of COLING-ACL '98. Associa-
tion for Computational Linguistics.
Hans van Halteren, Jakub Zavrel, and Walter
Daelemans. to appear. Improving accuracy
in nlp through combination of machine learn-
ing systems.
