<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<reference confidence="0.219184666666667">
b&apos;Experiments with Open-Domain Textual Question Answering
Sanda M. Harabagiu and Marius A. Pa\x18
sca and Steven J. Maiorano
</reference>
<affiliation confidence="0.9153475">
Department of Computer Science and Engineering
Southern Methodist University
</affiliation>
<address confidence="0.928633">
Dallas, TX 75275-0122
</address>
<email confidence="0.995545">
fsanda,marius,steveg@renoir.seas.smu.edu
</email>
<sectionHeader confidence="0.990748" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.987896285714286">
This paper describes the integration of several
knowledge-based natural language processing tech-
niques into a Question Answering system, capable
of mining textual answers from large collections of
texts. Surprizing quality is achieved when several
lightweight knowledge-based NLP techniques com-
plement mostly shallow, surface-based approaches.
</bodyText>
<sectionHeader confidence="0.978132" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.992513442857143">
The last decade has witnessed great advances and
interest in the area of Information Extraction (IE)
from real-world texts. Systems that participated in
the TIPSTER MUC competitions have been quite suc-
cessful at extracting information from newswire mes-
sages and \x0clling templates with information pertain-
ing to events or situations of interest. Typically, the
templates model queries regarding who did what to
whom, when and where, and eventually why.
Recently, a new trend in information processing
from texts has emerged. Textual Question Answer-
ing (Q/A) aims at identifying the answer of a ques-
tion in large collections of on-line documents. In-
stead of extracting all events of interest and their re-
lated entities, a Q/A system highlights only a short
piece of text, accounting for the answer. Moreover,
questions are expressed in natural language, are not
constrained to a speci\x0cc domain and are not limited
to the six question types sought by IE systems (i.e.
who1 did what2 to whom3, when4 and where5, and
eventually why6).
In open-domain Q/A systems, the \x0cnite-state
technology and domain knowledge that made IE sys-
tems successful are replaced by a combination of (1)
knowledge-based question processing, (2) new forms
of text indexing and (3) lightweight abduction of
queries. More generally, these systems combine cre-
atively components of the NLP basic research in-
frastructure developed in the 80s (e.g. the compu-
tational theory of Q/A reported in (Lehnert 1978)
and the theory of abductive interpretation of texts
reported in (Hobbs et al.1993)) with other shallow
techniques that make possible the open-domain pro-
cessing on real-world texts.
The idea of building open-domain Q/A systems
that perform on real-world document collections was
initiated by the eighth Text REtrieval Conference
(TREC-8), by organizing the \x0crst competition of an-
swering fact-based questions such as \\Who came up
with the name, El Nino?&amp;quot;. Resisting the tempta-
tion of merely porting and integrating existing IE
and IR technologies into Q/A systems, the develop-
ers of the TREC Q/A systems have not only shaped
new processing methods, but also inspired new re-
search in the challenging integration of surface-text-
based methods with knowledge-based text inference.
In particular, two clear knowledge processing needs
are presented: (1) capturing the semantics of open-
domain questions and (2) justifying the correctness
of answers.
In this paper, we present our experiments with
integrating knowledge-based NLP with shallow pro-
cessing techniques for these two aspects of Q/A. Our
research was motivated by the need to enhance the
precision of an implemented Q/A system and by the
requirement to prepare it for scaling to more com-
plex questions than those presented in the TREC
competition. In the remaining of the paper, we de-
scribe a Q/A architecture that allows the integra-
tion of knowledge-based NLP processing with shal-
low processing and we detail their interactions. Sec-
tion 2 presents the functionality of several knowledge
processing modules and describes the NLP tech-
niques for question and answer processing. Section
3 explains the semantic and logical interactions of
processing questions and answers whereas Section
4 highlights the inference aspects that implement
the justi\x0ccation option of a Q/A system. Section
5 presents the results and the evaluations whereas
Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.985609" genericHeader="introduction">
2 The NLP Techniques
</sectionHeader>
<bodyText confidence="0.980251833333333">
Surprising quality for open-domain textual Q/A can
be achieved when several lightweight knowledge-
based NLP techniques complement mostly shallow,
surface-based approaches. The processing imposed
by Q/A systems must be distinguished, on the one
hand, from IR techniques, that locate sets of doc-
</bodyText>
<figure confidence="0.99752705">
\x0cWorld Knowledge
Axioms
Rules
Abductive
Lexico-semantic
Patterns
Question
Taxonomies
Word Classes
Parser
Transformation
Question
Semantic
Transformation
Question
Logic Form
Expansion
Question
Question
Recognition
Class
Document
Collection
Index
Keywords 1
Paragrah
Ordering
Combine /
Rerank
Answers
Answer set 1
Answer set 2
Answer set n
Parser
Prover
Question
Expanded Question 2
Expanded Question n
Expanded Question 1
IR Search
Engine
Keywords 2
Keywords 1
Documents
Answer
Extraction
Answer(s)
Answers
True?
False?
Relax
Knowledge-Based Answer Processing
Transformation
Logic Form
Transformation
Semantic
Answer
Answer
Shallow Document Processing
Knowledge-Based Question Processing
</figure>
<figureCaption confidence="0.999785">
Figure 1: An architecture for knowledge-based Question/Answering
</figureCaption>
<bodyText confidence="0.997778234234235">
uments containing the required information, based
on keywords techniques. Q/A systems are presented
with natural language questions, far richer in seman-
tics than a set of keywords eventually structured
around some operators. Furthermore, the output
of Q/A systems is either the actual answer identi-
\x0ced in a text or small text fragments containing the
answer. This eliminates the user\&apos;s trouble of \x0cnd-
ing the required information in sometimes large sets
of retrieved documents. Open-domain Q/A systems
must also be distinguished, on the other hand, from
IE systems that model the information need through
database templates, thus less naturally than a tex-
tual answer. Moreover, open-domain IE is still di-
cult to achieve, because its linguistic pattern recog-
nition relies on domain-dependent lexico-semantic
knowledge.
To be able to satisfy the open-domain constraints,
textual Q/A systems replace the linguistic pattern
matching capabilities of IE systems with methods
that rely on the recognition of the question type and
of the expected answer type. Generally, this informa-
tion is available by accessing a classi\x0ccation based
on the question stem (i.e. what, how much, who)
and the head of the \x0crst noun phrase of the ques-
tion. Question processing also includes the identi\x0c-
cation of the question keywords. Empirical methods,
based on a set of ordered heuristics operating on the
phrasal parse of the question, extract keywords that
are passed to the search engine. The overall preci-
sion of the Q/A system depends also on the recogni-
tion of the question focus, since the answer extrac-
tion, succeeding the IR phase, is centered around
the question focus. Unfortunately, empirical meth-
ods for focus recognition are hard to develop without
the availability of richer semantic knowledge.
Special requirements are set on the document pro-
cessing component of a Q/A system. To speed-up
the answer extraction, the search engine returns only
those paragraphs from a document that contain all
queried keywords. The paragraphs are ordered to
promote the cases when the keywords not only are as
close as possible, but also preserve the syntactic de-
pendencies recognized in the question. Answers are
extracted whenever the question topic and the an-
swer type are recognized in a paragraph. Thereafter
the answers are scored based on several bag-of-words
heuristics. Throughout all this processing, the NLP
techniques are limited to (a) named entity recogni-
tion; (b) semantic classi\x0ccation of the question type,
based on information provided by an o\x0b-line ques-
tion taxonomy and semantic class information avail-
able from WordNet (Fellbaum 1998); and (c) phrasal
parsing produced by enhancing Brill\&apos;s part-of-speech
tagger with some rules for phrase formation.
However simple, this technology surpasses 75%
precision on trivia questions, as posed in the TREC-
8 competition (cf. (Moldovan et al.1999)). An im-
pressive improvement of 14% is achieved when more
knowledge-intensive NLP techniques are applied at
both question and answer processing level. Figure 1
illustrates the architecture of a system that has en-
hanced Q/A performance.
As represented in Figure 1, all three modules
of the Q/A system preserve the shallow processing
components that determine good performance. In
the Question Processing module, the Question Class
recognizer, working against a taxonomy of questions,
\x0cstill constitutes the central processing that takes
place at this stage. However, a far richer representa-
tion of the question classes is employed. To be able
to classify against the new question taxonomy each
question is \x0crst fully parsed and transformed into
a semantic representation that captures all relation-
ships between phrase heads.
The recognition of the question class is based on
the comparison of the question semantic representa-
tion with the semantic representation of the nodes
from the question taxonomy. Taxonomy nodes en-
code also the answer type, the question focus and
the semantic class of question keywords. Multiple
sets of keywords are generated based on their seman-
tic class, all pertaining to the same original ques-
tion. This feature enables the search engine to re-
trieve multiple sets of documents, pertaining to mul-
tiple sets of answers, that are extracted, combined
and ranked based on several heuristics, reported in
(Moldovan et al.1999). This process of obtaining
multiple sets of answers increases the likelihood of
\x0cnding the correct answer.
However, the big boost in the precision of the
knowledge-based Q/A system is provided by the op-
tion of enabling the justi\x0ccation of the extracted an-
swer. All extracted answers are parsed and trans-
formed in semantic representations. Thereafter,
both semantic transformations for questions and an-
swers are translated into logic forms and presented
to a simpli\x0ced theorem prover. The proof back-
chains from the question to the answer, its trace
generating a justi\x0ccation. The prover may access
a set of abduction rules that relax the justi\x0ccation
process. Whenever an answer cannot be proven, it
is discarded. This option solves multiple situations
when the correct answer is not ranked as the \x0crst
return, due to stronger surface-text-based indicators
in some other answers, which unfortunately are not
correct.
This architecture allows for simple integration of
semantic and axiomatic knowledge sources in a Q/A
system and determines ecient interaction of text-
surface-based and knowledge-based NLP techniques.
</bodyText>
<sectionHeader confidence="0.962542" genericHeader="method">
3 Interactions
</sectionHeader>
<bodyText confidence="0.919393258064516">
Three main interactions between text-surface-based
and knowledge-based NLP techniques are designed
in our Q/A architecture:
1. When multiple sets of question keywords are passed
to the search engine, increasing the chance of \x0cnd-
ing the text paragraph containing the answer.
2. When the question focus and the answer type, re-
sulting from the knowledge-based processing of the
question, are used in the extraction of the answer,
based on several empirical scores.
3. When the justi\x0ccation option of the Q/A system is
available. Instead of returning answers scored by
some empirical measures, a proof of the correctness
of the answer is produced, by accessing the logical
transformations of the question and the answer, as
well as axioms encoding world knowledge.
All these interactions depend on two factors: (1)
the transformations of the question or answer into
semantic or logical representations; and (2) the avail-
ability of knowledge resources, e.g. the question tax-
onomy and the world knowledge axioms. The avail-
ability of new, high-performace parsers that operate
on real world texts determines the transformation
into semantic and logic formulae quite simple. In ad-
dition, the acquisition of question taxonomies is alle-
viated by machine learning techniques inspired from
bootstrapping methods that learn linguistic patterns
and semantic dictionaries for IE (cf. (Rilo\x0b and
Jones, 1999)). World knowledge axioms can also be
easily derived by processing the gloss de\x0cnitions of
WordNet (Fellbaum 1998).
</bodyText>
<subsectionHeader confidence="0.9725815">
3.1 Semantic and Logic Transformations
Semantic Transformations
</subsectionHeader>
<bodyText confidence="0.9586155">
Instead of producing only a phrasal parse for the
question and answer, we make use of one of the new
statistical parsers of large real-world text coverage
(Collins, 1996). The parse trees produced by such a
parser can be easily translated into a semantic repre-
sentation that (1) comprises all the phrase heads and
(2) captures their inter-relationships by anonymous
links. Figure 2 illustrates both the parse tree and
the associated semantic representation of a TREC-8
question.
</bodyText>
<figure confidence="0.973171363636364">
Why did David Koresh ask the FBI for a word processor
WRB VBD NNP NNP VB NN
DT
IN NN
NNP
DT
NP
WHADVP NP NP
PP
SQ
VP
SBARQ
REASON
ask
processor
word
FBI
Koresh
David
Parse:
Semantic representation:
Question: Why did David Koresh ask the FBI for a word processor?
</figure>
<figureCaption confidence="0.998287">
Figure 2: Question semantic transformation
</figureCaption>
<bodyText confidence="0.997372575757576">
The actual transformation into semantic repre-
sentation of a question or an answer is obtained
as a by-product of the parse tree traversal. Ini-
tially, all leaves of the parse tree are classi\x0ced as
\x0cskipnodes or non-skipnodes. All nouns, non-auxiliary
verbs, adjectives and adverbs are categorized as
non-skipnodes. All the other leaves are skipnodes.
Bottom-up traversal of the parse tree entails the
propagation of leaf labels whenever the parent node
has more than one non-skipnode child. A rule based
on the syntactic category of the father selects one of
the children to propagate its label at the next level
in the tree. The winning node will then be consid-
ered linked to all the other former siblings that are
non-skipnodes. The propagation continues until the
parse tree root receives a label, and thus a semantic
graph is created as a by-product. Part of the la-
bel propagation, we also consider that whenever all
children of a non-terminal are skipnodes, the parent
becomes a skipnode as well.
Figure 3 represents the label propagation for the
parse tree of the question represented in Figure 2.
The labels of Koresh, ask, FBI and processor are
propagated to the next level. This entails that Ko-
resh is linked to David, ask to FBI and processor and
processor to word. As ask becomes the label of the
tree root, it is also linked to REASON, the question
type determined by the question stem: why. The
label propagation rules are identical to the rules for
mapping from trees to dependency structures used
my Michael Collins (cf. (Collins, 1996)). These rules
identify the head-child, and propagate its label up
in the tree.
</bodyText>
<figure confidence="0.991590571428572">
ask
ask
ask
Why did David Koresh the FBI for a word processor
WRB VBD NNP NNP VB NN
DT
IN NN
NNP
DT
NP
WHADVP NP NP
PP
SQ
VP
SBARQ
ask
processor
processor
FBI
Koresh
REASON
</figure>
<figureCaption confidence="0.999469">
Figure 3: Parse tree traversal
</figureCaption>
<subsectionHeader confidence="0.809112">
Logical Transformations
</subsectionHeader>
<bodyText confidence="0.95798080952381">
The logical formulae in which questions or answers
are translated are inspired by the notation proposed
in (Hobbs, 1986-1) and implemented in TACITUS
(Hobbs, 1986-2).
Based on the davidsonian treatment of action sen-
tences, in which events are treated as individuals,
every question and every answer are transformed in
a \x0crst-order predicate formula for which (1) verbs
are mapped in predicates verb(e,x,y,z,...) with the
convention that variable e represents the eventual-
ity of that action or event to take place, whereas the
other arguments (e.g. x, y, z, ...) represent the pred-
icate arguments of the verb; (2) nouns are mapped
into their lexicalized predicates; and, (3) modi\x0cers
have the same argument as the predicate they mod-
ify. For example, the question illustrated in Figure 2
has the following logical form transformation (LFT):
[REASON(x)&amp;David(y)&amp;Koresh(y)&amp;ask(e,x,y,z,p)&amp;
&amp;FBI(z)&amp;processor(p)&amp;word(p)]
The process of translating a semantic representa-
tion into a logic form has the following steps:
</bodyText>
<listItem confidence="0.915047714285714">
1. For each node in the semantic representation,
create a predicate with a distinct argument.
2.a. If a noun and an adjective predicate are linked
they should have the same argument.
2.b. The same for verbs and adverbs, pairs of nouns
or an adjective and an adverb.
3. For each verb predicate, add arguments corresponding
</listItem>
<bodyText confidence="0.994152333333333">
to each predicate to which it is directly
linked in the semantic representation.
Predicate arguments can be identi\x0ced because the
semantic representation using anonymous relations
represents uniformly adjuncts and thematic roles.
However, step 2 of the translation procedure recog-
nizes the adjuncts, making predicate arguments the
remaining connections of the verb in the semantic
representation.
</bodyText>
<subsectionHeader confidence="0.999449">
3.2 Question Taxonomy
</subsectionHeader>
<bodyText confidence="0.996779">
The question taxonomy represents each question
node as a quintuple: (1) a semantic representation
of a question; (2) the question type; (3) the answer
type; (4) the question focus and (5) the question
keywords. By using over 1500 questions provided by
Remedia, as well as other 2000 questions retrieved
from FAQFinder, we have been able to learn classi\x0c-
cation rules and build a complex question taxonomy.
</bodyText>
<figure confidence="0.996716823529412">
Date
Number
Instance
Product
Name
Name
Person
Author
Location
Name
Artwork
Name
Definition
Reason
Organization
Currency
Location
</figure>
<figureCaption confidence="0.99994">
Figure 4: A snapshot of the top Question Taxonomy
</figureCaption>
<bodyText confidence="0.9786566875">
Initially, we started with a seed hierarchy of 25
question classes, manually built, in which all the se-
mantic classes of the nodes from the semantic repre-
sentations were decided o\x0b-line, by a human expert.
300 questions were processed to create this seed hi-
erarchy. Figure 4 illustrates some of the nodes of
\x0cthe top of this hierarchy. Later, as 500 more ques-
tions were considered, we started classifying them
semi-automatically, using the following two steps:
(1) \x0crst a human would decide the semantic class of
each node in the semantic representation of the new
question; (2) then a classi\x0ccation procedure would
decide whether the question belongs to one of the
existing classes or a new class should be considered.
To be able to classify a new question in one of the ex-
isting question classes, two conditions must be satis-
</bodyText>
<listItem confidence="0.609953333333333">
\x0ced: (a) all nodes from the taxonomy question must
correspond to new question nodes with the same
semantic classes; and (b) unifyable nodes must be
</listItem>
<bodyText confidence="0.931670117647059">
linked in the same way in both representations. The
hierarchy grew to 68 question nodes.
Later, 2700 more questions were classi\x0ced fully
automatically. To decide the semantic classes of the
nodes, we used the WordNet semantic hierarchies,
by simply assigning to each semantic representation
node the same class as that of any other question
term from its WordNet hierarchy.
The semantic representation, having the same for-
mat for questions and answers, is a case frame with
anonymous relations, that allows the uni\x0ccation of
the answer to the question regardless of the case re-
lation. Figure 5 illustrates four nodes from the ques-
tion taxonomy, two for the \\currency&amp;quot; question type
and two for the \\person name&amp;quot; question type. The
Figure also represents the mappings of four TREC-
8 questions in these hierarchy nodes. The mappings
are represented by dashed arcs. In this Figure, the
nodes from the semantic representations that con-
tain a question mark are place holders for the ex-
pected answer type.
An additional set of classi\x0ccation rules is associ-
ated with this taxonomy. Initially, all rules are based
on the recognition of the question stem and of the
answer type, obtained with class information from
WordNet. However we could learn new rules when
morphological and semantic variations of the seman-
tic nodes are allowed. Moreover, along with the new
rules, we enrich the taxonomy, because often the new
questions unify only partially with the current tax-
onomy. All semantic and morphologic variations of
the semantic representation nodes are grouped into
word classes. Several of the word classes we used are
listed in Table 1.
</bodyText>
<subsectionHeader confidence="0.531532">
Word Class Words
</subsectionHeader>
<bodyText confidence="0.634788333333333">
Value words \\monetary value&amp;quot;, \\money&amp;quot;, \\price&amp;quot;
Expenditure words \\spend&amp;quot;, \\buy&amp;quot;, \\rent&amp;quot;, \\invest&amp;quot;
Creation words \\author&amp;quot;, \\designer&amp;quot;, \\invent&amp;quot;...
</bodyText>
<tableCaption confidence="0.960395">
Table 1: Examples of word classes.
</tableCaption>
<bodyText confidence="0.44468125">
The bootstrapping algorithm that learns new
classi\x0ccation rules and new classes of questions
is based on an information extraction measure:
score(rulei)=Ai \x03 log2(Ni), where Ai stands for the
</bodyText>
<figure confidence="0.995613211764706">
debts
group
Qintex
leave ?
?
timestamp
?
timestamp
?
?
of action
object timestamp
author
of action
word
expenditure
action
value
word
object
with value
timestamp
entity
object
possessing
entity type
specifier
?
Class Name:
Q32:
Q12:
?
spend
Manchester
United
1993
players
Q7:
received
Award
Will Rogers 1989
Q92:
late
1980s
released
Internet
worm
?
competition
word
award
name
word
create
creation
word
creation
title
Class Name:
Class Name:
subclasses
3
name person
semantic representation
semantic representation
Rogers Award in 1989?
Who received the Will
semantic representation
How much did Manchester
United spend on players in 1993?
semantic representation
group leave?
What debts did Qintex
worm in the late 1980s?
Who released the Internet
semantic representation
semantic representation
currency
subclasses
1
2
semantic representation
semantic representation
4
author
</figure>
<figureCaption confidence="0.998649">
Figure 5: Mapping Questions in the Taxonomy
</figureCaption>
<bodyText confidence="0.996065">
number of di\x0berent lexicon entries for the answer
type of the question, whereas Ni = Ai=Fi, where Fi
is the number of di\x0berent focus categories classi\x0ced.
The steps of the bootstrapping algorithm are:
</bodyText>
<listItem confidence="0.959333714285714">
1. Retrieve concepts morphologically/semantically related
to the semantic representations
2. Apply the classi\x0ccation rules to all questions that
contain any newly retrieved concepts.
3. New Classi\x0ccatiton Rules=fg
MUTUAL BOOTSTRAPPING LOOP
4. Score all new classi\x0ccation rules
5. best CR=the highest scoring classi\x0ccation rule
6. Add best CR to the classi\x0ccation rules
7. Add the questions classi\x0ced by best CR to the taxonomy
8. Goto step 4 three times.
9. Discard all new rules but the best scoring three.
10. Goto 4. until the Q/A performance improves.
\x0c4 The Justi\x0ccation Option
</listItem>
<bodyText confidence="0.932198727272727">
A Q/A system that provides with the option of jus-
tifying the answer has the advantage that erroneous
answers can be ruled out systematically. In our quest
of enhancing the precision of a Q/A system by incor-
porating additional knowledge, we found this option
very helpful. However, the generation of justi\x0cca-
tions for open-domain textual Q/A systems poses
some challenges. First, we needed to develop a very
ecient prover, operating on logical form transfor-
mations. Our proofs are backchaining from the ques-
tions through a mixture of axioms. We use three
forms of axioms: (1) axioms derived from the facts
stated in the textual answer; (2) axioms represent-
ing world knowledge; and (3) axioms determined by
coreference resolution in the answer text. For ex-
ample, some of the axioms employed to prove the
answer to the TREC-8 question Q6: Why did David
Koresh ask the FBI for a word processor?&amp;quot; are:
________________________________________________
SET 1
Mr(71):= null. Koresh(71):=null. word(72):=null.
processor(72):=null. sent(77 76 78 71):=null.
</bodyText>
<figure confidence="0.496855">
________________________________________________
SET 2
David(1):=Mr(1). _REASON(5):= enable(5 3 6).
________________________________________________
SET 3
FBI(1):=null.
________________________________________________
</figure>
<bodyText confidence="0.986415608695652">
The \x0crst set represents facts extracted through
LFT predicates of the textual answer: \\Over the
weekend Mr Koresh sent a request for a word proces-
sor to enable him to record his revelations&amp;quot;. The sec-
ond set represents world knowledge axioms that we
acquired semi-automatically. For instance we know
that David is a male name, thus that person can be
addressed with Mr.. Similarly, events are enabled
for some reason. The third set of axioms represent
the fact that the FBI is in the context of the text
answer. To be noted that the axioms derived from
the answer have constant arguments, represented by
convention with numbers larger than 70. All the
other arguments are variables.
Q52 Who invented the road trac cone?
Answer Smiling proudly for the cameras , Governor
(shallow Pete Wilson, US Transportation Secretary
methods) Federico Pena and Mayor Richard Riordan
removed a half - dozen plastic orange cones
from the roadway and the \x0crst cars passed
Answer David Morgan, the company\&apos;s managing
(kb-based director and inventor of the plastic
methods) cone even collects them.
</bodyText>
<tableCaption confidence="0.879158">
Table 2: Examples of improved answer correctness.
</tableCaption>
<bodyText confidence="0.967863571428571">
The justi\x0ccation of this answer is provided by the
following proof trace. The prover attempts to prove
the LFT of the question (QLF) correct by proving
from left to right each term of QLF.
-------------------------------------------------------------
-&amp;gt;Answer:Over the weekend Mr Koresh sent a request for a word
processor to enable him to record his revelations.
</bodyText>
<equation confidence="0.756936125">
-&amp;gt;QLF:David(1)^Koresh(1)^word(2)^processor(2)^FBI(4)^
ask(3 4 2 1 5)^_REASON(5)^_PER(1)^_ORG(4)
-&amp;gt;ALF:Mr(71)^Koresh(71)^word(72)^processor(72)^revelations(74)^
record(73 74 75)^enable(75 73 76)^request(76)^sent(77 76 78 71)
^weekend(78)^_PER(71)^_DATE(78)
..............................................................
--&amp;gt;Proving:David(1)^Koresh(1)^word(2)^processor(2)^FBI(4)^
ask(3 4 2 1 5)^_REASON(5)^_PER(1)^_ORG(4)
</equation>
<bodyText confidence="0.7188335">
There are 1 target axioms. Selected axiom: David(1):= Mr(1).
Unifying: 1 to 1. Ok
</bodyText>
<table confidence="0.818075954545455">
--&amp;gt; Proving: Mr(1)^Koresh(1)^word(2)^processor(2)^FBI(4)
^ask(3 4 2 1 5)^_REASON(5)^_PER(1)^_ORG(4)
There are 1 target axioms. Selected axiom: Mr(71):= null.
Unifying: 1 to 71. Ok
--&amp;gt; Proving: Koresh(71)^word(2)^processor(2)^FBI(4)^
ask(3 4 2 71 5)^_REASON(5)^_PER(71)^_ORG(4)
There are 1 target axioms. Selected axiom: Koresh(71):= null.
Unifying: 71 to 71. Ok
--&amp;gt; Proving: word(2)^processor(2)^FBI(4)^ask(3 4 2 71 5)^
_REASON(5)^_PER(71)^_ORG(4)
There are 1 target axioms. Selected axiom: word(72):= null.
Unifying: 2 to 72. Ok
--&amp;gt; Proving: processor(72)^FBI(4)^ask(3 4 72 71 5)^_REASON(5)
^_PER(71)^_ORG(4)
There are 1 target axioms. Selected axiom: processor(72):= null.
Unifying: 72 to 72. Ok
--&amp;gt; Proving: FBI(4)^ask(3 4 72 71 5)^_REASON(5)^_PER(71)^_ORG(4)
There are 1 target axioms. Selected axiom: FBI(1):= null.
Unifying: 4 to 1. Ok
--&amp;gt; Proving: ask(3 4 72 71 5)^_REASON(5)^_PER(71)^_ORG(4)
There are 2 target axioms. Selected axiom: ask(1 2 3 4 5):=
sent(1 6 7 4)^request(6).
</table>
<bodyText confidence="0.858874764705883">
Unifying: 1 to 2. 3 to 1. 5 to 5. 71 to 4. 72 to 3. Ok
--&amp;gt; Proving: sent(1 6 7 71)^request(6)^_REASON(5)^_PER(71)^_ORG(2)
There are 1 target axioms. Selected axiom: sent(77 76 78 71):= null.
Unifying: 1 to 77. 6 to 76. 7 to 78. 71 to 71. Ok
--&amp;gt; Proving: request(76)^_REASON(5)^_PER(71)^_ORG(2)
There are 1 target axioms. Selected axiom: request(76):= null.
Unifying: 76 to 76. Ok
--&amp;gt; Proving: _REASON(5)^_PER(71)^_ORG(2)
There are 1 target axioms. Selected axiom: _REASON(5):= enable(5 3 6).
Unifying: 5 to 5. Ok
--&amp;gt; Proving: enable(5 3 6)^_PER(71)^_ORG(2)
There are 1 target axioms. Selected axiom: enable(75 73 76):= null.
Unifying: 3 to 73. 5 to 75. 6 to 76. Ok
--&amp;gt; Proving: _PER(71)^_ORG(2)
There are 3 target axioms. Selected axiom: _PER(71):= null.
Unifying: 71 to 71. Ok
--&amp;gt; Proving: _ORG(2)
There are 1 target axioms. Selected axiom: _ORG(1):= FBI(1).
Unifying: 2 to 1. Ok
--&amp;gt; Proving: null  |We found:Success.
------------------------------------------------------------------
There are cases when our simple prover fails to
prove a correct answer. We have noticed that this
happens because in the answer semantic representa-
tion, some concepts that are connected in the ques-
tion semantic representation are no longer directly
linked. This is due to the fact that there are either
parser errors or there are new syntactic dependen-
cies between the two concepts. To accommodate
this situation, we allow di\x0berent constants that are
arguments of the same predicate to be uni\x0cable. The
special cases in which this relaxation of the uni\x0cca-
tion procedure is allowed constitute our abduction
rules.
</bodyText>
<sectionHeader confidence="0.738392" genericHeader="evaluation">
\x0c5 Evaluation
</sectionHeader>
<bodyText confidence="0.9832777">
Both qualitative and quantitative evaluation of the
integration of surface text-based and knowledge-
based methods for Q/A is imposed. Quantitatively,
Table 3 summarizes the scores obtained when only
shallow methods were employed, in contrast with
the results when knowledge-based methods were in-
tegrated. We have separately measured the e\x0bect
of the integration of the knowledge-based methods
at question processing and answer processing level.
We have also evaluated the precision of the sys-
tem when both integrations were implemented. The
results were the \x0crst \x0cve answers returned within
250 bytes of text, when approximatively half mil-
lion TREC documents are mined. We have used the
200 questions from TREC-8, and the correct answers
provided by NIST. The performance was measured
both with the NIST scoring method employed in the
TREC-8 and by simply assigning a score of 1 for the
question having a correct answer, regardless of its
position.
</bodyText>
<figure confidence="0.946890142857143">
Percentage of NIST score
correct answers
in top 5 returns
Text-surface-based 77.7% 64.5%
Knowledge-based 83.2% 71.5%
Question Processing
(only)
Text-surface-based 77.7% 73%
only with Answer
Justi\x0ccation
Knowledge-based 89.5% 84.75%
Question Processing
with Answer
Justi\x0ccation
</figure>
<tableCaption confidence="0.991541">
Table 3: Accuracy performance
</tableCaption>
<bodyText confidence="0.99421155">
When using the NIST scoring method to eval-
uate an individual answer, we used only six
values:(1; :5; :33; :25; :2; 0), representing the score the
answer\&apos;s question obtains. If the \x0crst answer is cor-
rect, it obtains a score of 1, if the second one is cor-
rect, it is scored with :5, if the third one is correct,
the score becomes :33, if the fourth is correct, the
score is :25 and if the \x0cfth one is correct, the score
is :2. Otherwise, it is scored with 0. No credit is
given if multiple answers are correct. Table 3 shows
that both knowledge-based methods enhanced the
precision, regardless of the scoring method.
To further evaluate the contribution of the justi-
\x0ccation option, we evaluated separately the preci-
sion of the prover for those questions for which the
surface-text-based methods of our system, when op-
erating alone, cannot \x0cnd correct answers. We had
45 TREC-8 questions for which the evaluation of the
prover was performed. Table 4 summarizes the ac-
curacy of the prover.
</bodyText>
<figure confidence="0.990039125">
Proven Proven Precision
correct incorrect
Incorrect answers 3 210 98.5%
(no knowledge)
Correct answers 127 5 96.2%
(KB-based)
Incorrect answers 4 38 90.04%
(KB-based)
</figure>
<tableCaption confidence="0.981732">
Table 4: Prover performance
</tableCaption>
<bodyText confidence="0.995862">
Qualitatively, we \x0cnd that the integration of
knowledge-based methods is very bene\x0ccial. Table 2
illustrates the correct answer obtained with these
methods, in contrast to the incorrect answer pro-
vided when only the shallow techniques are applied.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999339611111111">
We believe that the performance of a Q/A system
depends on the knowledge sources it employs. In
this paper we have presented the e\x0bect of the in-
tegration of knowledge derived from question tax-
onomies and produced by answer justi\x0ccations on
the Q/A precision. Our knowledge-based methods
are lightweight, since we do not generate precise se-
mantic representations of questions or answers, but
mere approximations determined by syntactic de-
pendencies. Furthermore, our prover operates on
very simple logical representations, in which syntac-
tic and semantic ambiguities are completely ignored.
Nevertheless, we have shown that these approxima-
tions are functional, since we implemented a prover
that justi\x0ces answers with high precision. Similarly,
our knowledge-based question processing is a mere
combination of word class information and syntactic
dependencies.
</bodyText>
<sectionHeader confidence="0.991228" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994972884615385">
Michael Collins. A New Statistical Parser Based on Bigram
Lexical Dependencies. In Proceedings of the 34st Annual
Meeting of the Association for Computational Linguistics,
ACL-96, pages 184{191, 1996.
Christiane Fellbaum (Ed). WordNet - An Electronic Lexical
Database. MIT Press, 1998.
Jerry R. Hobbs. Discourse and Inference. Unpublished
manuscript, 1986.
Jerry R. Hobbs. Overview of the TACITUS Project. In Com-
putational Linguistics, 12:(3), 1986.
Jerry Hobbs, Mark Stickel, Doug Appelt, and Paul Mar-
tin. Interpretation as abduction. Arti\x0ccial Intelligence, 63,
pages 69{142, 1993.
Wendy Lehnert. The processing of question answering.
Lawrence Erlbaum Publishers, 1978.
Dan Moldovan, Sanda Harabagiu, Marius Pa\x18
sca, Rada Mi-
halcea, Richard Goodrum, Roxana G^
\x10rju and Vasile Rus.
Lasso: a tool for sur\x0cng the answer net. In Proceedings of
TREC-8, 1999.
Ellen Rilo\x0b and Rosie Jones. Learning Dictionaries for Infor-
mation Extraction by Multi-Level Bootstrapping. In Pro-
ceedings of the 16th National Conference on Arti\x0ccial In-
telligence, AAAI-99, 1999.
\x0c&apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996261">b&apos;Experiments with Open-Domain Textual Question Answering</title>
<author confidence="0.984181">Sanda M Harabagiu</author>
<author confidence="0.984181">Marius A Pa\x sca</author>
<author confidence="0.984181">Steven J Maiorano</author>
<affiliation confidence="0.997203">Department of Computer Science and Engineering Southern Methodist University</affiliation>
<address confidence="0.985577">Dallas, TX 75275-0122</address>
<email confidence="0.999925">fsanda,marius,steveg@renoir.seas.smu.edu</email>
<abstract confidence="0.999151720930233">This paper describes the integration of several knowledge-based natural language processing techniques into a Question Answering system, capable of mining textual answers from large collections of texts. Surprizing quality is achieved when several lightweight knowledge-based NLP techniques complement mostly shallow, surface-based approaches. 1 Background The last decade has witnessed great advances and interest in the area of Information Extraction (IE) from real-world texts. Systems that participated in the TIPSTER MUC competitions have been quite successful at extracting information from newswire messages and \x0clling templates with information pertaining to events or situations of interest. Typically, the templates model queries regarding who did what to whom, when and where, and eventually why. Recently, a new trend in information processing from texts has emerged. Textual Question Answering (Q/A) aims at identifying the answer of a question in large collections of on-line documents. Instead of extracting all events of interest and their related entities, a Q/A system highlights only a short piece of text, accounting for the answer. Moreover, questions are expressed in natural language, are not constrained to a speci\x0cc domain and are not limited to the six question types sought by IE systems (i.e. who1 did what2 to whom3, when4 and where5, and eventually why6). In open-domain Q/A systems, the \x0cnite-state technology and domain knowledge that made IE systems successful are replaced by a combination of (1) knowledge-based question processing, (2) new forms of text indexing and (3) lightweight abduction of queries. More generally, these systems combine creatively components of the NLP basic research infrastructure developed in the 80s (e.g. the computational theory of Q/A reported in (Lehnert 1978) and the theory of abductive interpretation of texts reported in (Hobbs et al.1993)) with other shallow techniques that make possible the open-domain processing on real-world texts. The idea of building open-domain Q/A systems that perform on real-world document collections was initiated by the eighth Text REtrieval Conference (TREC-8), by organizing the \x0crst competition of answering fact-based questions such as \\Who came up with the name, El Nino?&amp;quot;. Resisting the temptation of merely porting and integrating existing IE and IR technologies into Q/A systems, the developers of the TREC Q/A systems have not only shaped new processing methods, but also inspired new research in the challenging integration of surface-textbased methods with knowledge-based text inference. In particular, two clear knowledge processing needs are presented: (1) capturing the semantics of opendomain questions and (2) justifying the correctness of answers. In this paper, we present our experiments with integrating knowledge-based NLP with shallow processing techniques for these two aspects of Q/A. Our research was motivated by the need to enhance the precision of an implemented Q/A system and by the requirement to prepare it for scaling to more complex questions than those presented in the TREC competition. In the remaining of the paper, we describe a Q/A architecture that allows the integration of knowledge-based NLP processing with shallow processing and we detail their interactions. Section 2 presents the functionality of several knowledge processing modules and describes the NLP techniques for question and answer processing. Section 3 explains the semantic and logical interactions of processing questions and answers whereas Section 4 highlights the inference aspects that implement the justi\x0ccation option of a Q/A system. Section 5 presents the results and the evaluations whereas Section 6 concludes the paper. 2 The NLP Techniques Surprising quality for open-domain textual Q/A can be achieved when several lightweight knowledgebased NLP techniques complement mostly shallow, surface-based approaches. The processing imposed by Q/A systems must be distinguished, on the one hand, from IR techniques, that locate sets of doc-</abstract>
<title confidence="0.923786434782609">x0cWorld Knowledge Axioms Rules Abductive Lexico-semantic Patterns Question Taxonomies Word Classes Parser Transformation Question Semantic Transformation Question Logic Form Expansion Question Question Recognition Class Document Collection</title>
<note confidence="0.5151195">Index Keywords 1</note>
<title confidence="0.830725769230769">Paragrah Ordering Combine / Rerank Answers Answer set 1 Answer set 2 Answer set n Parser Prover Question Expanded Question 2 Expanded Question n</title>
<author confidence="0.465323">Expanded Question</author>
<affiliation confidence="0.491815">IR Search</affiliation>
<note confidence="0.567022666666667">Engine Keywords 2 Keywords 1</note>
<title confidence="0.941773294117647">Documents Answer Extraction Answer(s) Answers True? False? Relax Knowledge-Based Answer Processing Transformation Logic Form Transformation Semantic Answer Answer Shallow Document Processing Knowledge-Based Question Processing</title>
<abstract confidence="0.996456263888889">Figure 1: An architecture for knowledge-based Question/Answering uments containing the required information, based on keywords techniques. Q/A systems are presented with natural language questions, far richer in semantics than a set of keywords eventually structured around some operators. Furthermore, the output of Q/A systems is either the actual answer identi- \x0ced in a text or small text fragments containing the answer. This eliminates the user\&apos;s trouble of \x0cnding the required information in sometimes large sets of retrieved documents. Open-domain Q/A systems must also be distinguished, on the other hand, from IE systems that model the information need through database templates, thus less naturally than a textual answer. Moreover, open-domain IE is still dicult to achieve, because its linguistic pattern recognition relies on domain-dependent lexico-semantic knowledge. To be able to satisfy the open-domain constraints, textual Q/A systems replace the linguistic pattern matching capabilities of IE systems with methods that rely on the recognition of the question type and of the expected answer type. Generally, this information is available by accessing a classi\x0ccation based on the question stem (i.e. what, how much, who) and the head of the \x0crst noun phrase of the question. Question processing also includes the identi\x0ccation of the question keywords. Empirical methods, based on a set of ordered heuristics operating on the phrasal parse of the question, extract keywords that are passed to the search engine. The overall precision of the Q/A system depends also on the recognition of the question focus, since the answer extraction, succeeding the IR phase, is centered around the question focus. Unfortunately, empirical methods for focus recognition are hard to develop without the availability of richer semantic knowledge. Special requirements are set on the document processing component of a Q/A system. To speed-up the answer extraction, the search engine returns only those paragraphs from a document that contain all queried keywords. The paragraphs are ordered to promote the cases when the keywords not only are as close as possible, but also preserve the syntactic dependencies recognized in the question. Answers are extracted whenever the question topic and the answer type are recognized in a paragraph. Thereafter the answers are scored based on several bag-of-words heuristics. Throughout all this processing, the NLP techniques are limited to (a) named entity recognition; (b) semantic classi\x0ccation of the question type, based on information provided by an o\x0b-line question taxonomy and semantic class information available from WordNet (Fellbaum 1998); and (c) phrasal parsing produced by enhancing Brill\&apos;s part-of-speech tagger with some rules for phrase formation. However simple, this technology surpasses 75% precision on trivia questions, as posed in the TREC- 8 competition (cf. (Moldovan et al.1999)). An impressive improvement of 14% is achieved when more knowledge-intensive NLP techniques are applied at both question and answer processing level. Figure 1 illustrates the architecture of a system that has enhanced Q/A performance. As represented in Figure 1, all three modules of the Q/A system preserve the shallow processing components that determine good performance. In the Question Processing module, the Question Class recognizer, working against a taxonomy of questions, \x0cstill constitutes the central processing that takes place at this stage. However, a far richer representation of the question classes is employed. To be able to classify against the new question taxonomy each question is \x0crst fully parsed and transformed into a semantic representation that captures all relationships between phrase heads. The recognition of the question class is based on the comparison of the question semantic representation with the semantic representation of the nodes from the question taxonomy. Taxonomy nodes encode also the answer type, the question focus and the semantic class of question keywords. Multiple sets of keywords are generated based on their semantic class, all pertaining to the same original question. This feature enables the search engine to retrieve multiple sets of documents, pertaining to multiple sets of answers, that are extracted, combined and ranked based on several heuristics, reported in (Moldovan et al.1999). This process of obtaining multiple sets of answers increases the likelihood of \x0cnding the correct answer. However, the big boost in the precision of the knowledge-based Q/A system is provided by the option of enabling the justi\x0ccation of the extracted answer. All extracted answers are parsed and transformed in semantic representations. Thereafter, both semantic transformations for questions and answers are translated into logic forms and presented to a simpli\x0ced theorem prover. The proof backchains from the question to the answer, its trace generating a justi\x0ccation. The prover may access a set of abduction rules that relax the justi\x0ccation process. Whenever an answer cannot be proven, it is discarded. This option solves multiple situations when the correct answer is not ranked as the \x0crst return, due to stronger surface-text-based indicators in some other answers, which unfortunately are not correct. This architecture allows for simple integration of semantic and axiomatic knowledge sources in a Q/A system and determines ecient interaction of textsurface-based and knowledge-based NLP techniques. 3 Interactions Three main interactions between text-surface-based and knowledge-based NLP techniques are designed in our Q/A architecture: 1. When multiple sets of question keywords are passed to the search engine, increasing the chance of \x0cnding the text paragraph containing the answer. 2. When the question focus and the answer type, resulting from the knowledge-based processing of the question, are used in the extraction of the answer, based on several empirical scores. 3. When the justi\x0ccation option of the Q/A system is available. Instead of returning answers scored by some empirical measures, a proof of the correctness of the answer is produced, by accessing the logical transformations of the question and the answer, as well as axioms encoding world knowledge. All these interactions depend on two factors: (1) the transformations of the question or answer into semantic or logical representations; and (2) the availability of knowledge resources, e.g. the question taxonomy and the world knowledge axioms. The availability of new, high-performace parsers that operate on real world texts determines the transformation into semantic and logic formulae quite simple. In addition, the acquisition of question taxonomies is alleviated by machine learning techniques inspired from bootstrapping methods that learn linguistic patterns and semantic dictionaries for IE (cf. (Rilo\x0b and Jones, 1999)). World knowledge axioms can also be easily derived by processing the gloss de\x0cnitions of WordNet (Fellbaum 1998).</abstract>
<title confidence="0.6871705">3.1 Semantic and Logic Transformations Semantic Transformations</title>
<abstract confidence="0.9874478">Instead of producing only a phrasal parse for the question and answer, we make use of one of the new statistical parsers of large real-world text coverage (Collins, 1996). The parse trees produced by such a parser can be easily translated into a semantic representation that (1) comprises all the phrase heads and (2) captures their inter-relationships by anonymous links. Figure 2 illustrates both the parse tree and the associated semantic representation of a TREC-8 question.</abstract>
<title confidence="0.89880125">Why did David Koresh ask the FBI for a word processor WRB VBD NNP NNP VB NN DT IN NN NNP DT NP WHADVP NP NP PP SQ VP SBARQ</title>
<abstract confidence="0.8892311875">REASON ask processor word FBI Koresh David Parse: Semantic representation: Question: Why did David Koresh ask the FBI for a word processor? Figure 2: Question semantic transformation The actual transformation into semantic representation of a question or an answer is obtained as a by-product of the parse tree traversal. Initially, all leaves of the parse tree are classi\x0ced as \x0cskipnodes or non-skipnodes. All nouns, non-auxiliary verbs, adjectives and adverbs are categorized as non-skipnodes. All the other leaves are skipnodes. Bottom-up traversal of the parse tree entails the propagation of leaf labels whenever the parent node has more than one non-skipnode child. A rule based on the syntactic category of the father selects one of the children to propagate its label at the next level in the tree. The winning node will then be considered linked to all the other former siblings that are non-skipnodes. The propagation continues until the parse tree root receives a label, and thus a semantic graph is created as a by-product. Part of the label propagation, we also consider that whenever all children of a non-terminal are skipnodes, the parent becomes a skipnode as well. Figure 3 represents the label propagation for the parse tree of the question represented in Figure 2. The labels of Koresh, ask, FBI and processor are propagated to the next level. This entails that Koresh is linked to David, ask to FBI and processor and processor to word. As ask becomes the label of the tree root, it is also linked to REASON, the question type determined by the question stem: why. The label propagation rules are identical to the rules for mapping from trees to dependency structures used my Michael Collins (cf. (Collins, 1996)). These rules identify the head-child, and propagate its label up in the tree. ask ask ask Why did David Koresh the FBI for a word processor</abstract>
<title confidence="0.8585908">WRB VBD NNP NNP VB NN DT IN NN NNP DT NP WHADVP NP NP PP SQ VP</title>
<abstract confidence="0.967446690909091">SBARQ ask processor processor FBI Koresh REASON Figure 3: Parse tree traversal Logical Transformations The logical formulae in which questions or answers are translated are inspired by the notation proposed in (Hobbs, 1986-1) and implemented in TACITUS (Hobbs, 1986-2). Based on the davidsonian treatment of action sentences, in which events are treated as individuals, every question and every answer are transformed in a \x0crst-order predicate formula for which (1) verbs are mapped in predicates verb(e,x,y,z,...) with the convention that variable e represents the eventuality of that action or event to take place, whereas the other arguments (e.g. x, y, z, ...) represent the predicate arguments of the verb; (2) nouns are mapped into their lexicalized predicates; and, (3) modi\x0cers have the same argument as the predicate they modify. For example, the question illustrated in Figure 2 has the following logical form transformation (LFT): [REASON(x)&amp;David(y)&amp;Koresh(y)&amp;ask(e,x,y,z,p)&amp; &amp;FBI(z)&amp;processor(p)&amp;word(p)] The process of translating a semantic representation into a logic form has the following steps: 1. For each node in the semantic representation, create a predicate with a distinct argument. 2.a. If a noun and an adjective predicate are linked they should have the same argument. 2.b. The same for verbs and adverbs, pairs of nouns or an adjective and an adverb. 3. For each verb predicate, add arguments corresponding to each predicate to which it is directly linked in the semantic representation. Predicate arguments can be identi\x0ced because the semantic representation using anonymous relations represents uniformly adjuncts and thematic roles. However, step 2 of the translation procedure recognizes the adjuncts, making predicate arguments the remaining connections of the verb in the semantic representation. 3.2 Question Taxonomy The question taxonomy represents each question node as a quintuple: (1) a semantic representation of a question; (2) the question type; (3) the answer type; (4) the question focus and (5) the question keywords. By using over 1500 questions provided by Remedia, as well as other 2000 questions retrieved from FAQFinder, we have been able to learn classi\x0ccation rules and build a complex question taxonomy.</abstract>
<note confidence="0.48771925">Date Number Instance Product</note>
<title confidence="0.903523642857143">Name Name Person Author Location Name Artwork Name Definition Reason Organization Currency Location Figure 4: A snapshot of the top Question Taxonomy</title>
<abstract confidence="0.994477577777778">Initially, we started with a seed hierarchy of 25 question classes, manually built, in which all the semantic classes of the nodes from the semantic representations were decided o\x0b-line, by a human expert. 300 questions were processed to create this seed hierarchy. Figure 4 illustrates some of the nodes of \x0cthe top of this hierarchy. Later, as 500 more questions were considered, we started classifying them semi-automatically, using the following two steps: (1) \x0crst a human would decide the semantic class of each node in the semantic representation of the new question; (2) then a classi\x0ccation procedure would decide whether the question belongs to one of the existing classes or a new class should be considered. To be able to classify a new question in one of the existing question classes, two conditions must be satis- \x0ced: (a) all nodes from the taxonomy question must correspond to new question nodes with the same semantic classes; and (b) unifyable nodes must be linked in the same way in both representations. The hierarchy grew to 68 question nodes. Later, 2700 more questions were classi\x0ced fully automatically. To decide the semantic classes of the nodes, we used the WordNet semantic hierarchies, by simply assigning to each semantic representation node the same class as that of any other question term from its WordNet hierarchy. The semantic representation, having the same format for questions and answers, is a case frame with anonymous relations, that allows the uni\x0ccation of the answer to the question regardless of the case relation. Figure 5 illustrates four nodes from the question taxonomy, two for the \\currency&amp;quot; question type and two for the \\person name&amp;quot; question type. The Figure also represents the mappings of four TREC- 8 questions in these hierarchy nodes. The mappings are represented by dashed arcs. In this Figure, the nodes from the semantic representations that contain a question mark are place holders for the expected answer type. An additional set of classi\x0ccation rules is associated with this taxonomy. Initially, all rules are based on the recognition of the question stem and of the answer type, obtained with class information from WordNet. However we could learn new rules when morphological and semantic variations of the semantic nodes are allowed. Moreover, along with the new rules, we enrich the taxonomy, because often the new questions unify only partially with the current taxonomy. All semantic and morphologic variations of the semantic representation nodes are grouped into word classes. Several of the word classes we used are listed in Table 1. Word Class Words Value words \\monetary value&amp;quot;, \\money&amp;quot;, \\price&amp;quot; Expenditure words \\spend&amp;quot;, \\buy&amp;quot;, \\rent&amp;quot;, \\invest&amp;quot; Creation words \\author&amp;quot;, \\designer&amp;quot;, \\invent&amp;quot;... Table 1: Examples of word classes. The bootstrapping algorithm that learns new classi\x0ccation rules and new classes of questions is based on an information extraction measure: score(rulei)=Ai \x03 log2(Ni), where Ai stands for the debts group Qintex leave ? ? timestamp ? timestamp ? ? of action object timestamp author of action word expenditure action value word object with value timestamp entity object possessing entity type specifier ?</abstract>
<note confidence="0.847253642857143">Class Name: Q32: Q12: ? spend Manchester United 1993 players Q7: received Award Will Rogers 1989 Q92:</note>
<abstract confidence="0.9908705625">late 1980s released Internet worm ? competition word award name word create creation word creation title</abstract>
<title confidence="0.674349">Class Name: Class Name:</title>
<abstract confidence="0.987434403846154">subclasses 3 name person semantic representation semantic representation Rogers Award in 1989? Who received the Will semantic representation How much did Manchester United spend on players in 1993? semantic representation group leave? What debts did Qintex worm in the late 1980s? Who released the Internet semantic representation semantic representation currency subclasses 1 2 semantic representation semantic representation 4 author Figure 5: Mapping Questions in the Taxonomy number of di\x0berent lexicon entries for the answer type of the question, whereas Ni = Ai=Fi, where Fi is the number of di\x0berent focus categories classi\x0ced. The steps of the bootstrapping algorithm are: 1. Retrieve concepts morphologically/semantically related to the semantic representations 2. Apply the classi\x0ccation rules to all questions that contain any newly retrieved concepts. 3. New Classi\x0ccatiton Rules=fg MUTUAL BOOTSTRAPPING LOOP 4. Score all new classi\x0ccation rules 5. best CR=the highest scoring classi\x0ccation rule 6. Add best CR to the classi\x0ccation rules 7. Add the questions classi\x0ced by best CR to the taxonomy 8. Goto step 4 three times. 9. Discard all new rules but the best scoring three. 10. Goto 4. until the Q/A performance improves. \x0c4 The Justi\x0ccation Option A Q/A system that provides with the option of justifying the answer has the advantage that erroneous answers can be ruled out systematically. In our quest of enhancing the precision of a Q/A system by incorporating additional knowledge, we found this option very helpful. However, the generation of justi\x0ccations for open-domain textual Q/A systems poses some challenges. First, we needed to develop a very ecient prover, operating on logical form transformations. Our proofs are backchaining from the questions through a mixture of axioms. We use three forms of axioms: (1) axioms derived from the facts stated in the textual answer; (2) axioms representing world knowledge; and (3) axioms determined by coreference resolution in the answer text. For example, some of the axioms employed to prove the answer to the TREC-8 question Q6: Why did David Koresh ask the FBI for a word processor?&amp;quot; are: ________________________________________________ SET 1 Mr(71):= null. Koresh(71):=null. word(72):=null. processor(72):=null. sent(77 76 78 71):=null. ________________________________________________ SET 2 David(1):=Mr(1). _REASON(5):= enable(5 3 6). ________________________________________________ SET 3 FBI(1):=null. ________________________________________________ The \x0crst set represents facts extracted through LFT predicates of the textual answer: \\Over the weekend Mr Koresh sent a request for a word processor to enable him to record his revelations&amp;quot;. The second set represents world knowledge axioms that we acquired semi-automatically. For instance we know that David is a male name, thus that person can be addressed with Mr.. Similarly, events are enabled for some reason. The third set of axioms represent the fact that the FBI is in the context of the text answer. To be noted that the axioms derived from the answer have constant arguments, represented by convention with numbers larger than 70. All the other arguments are variables. Q52 Who invented the road trac cone? Answer Smiling proudly for the cameras , Governor (shallow Pete Wilson, US Transportation Secretary methods) Federico Pena and Mayor Richard Riordan removed a half dozen plastic orange cones from the roadway and the \x0crst cars passed Answer David Morgan, the company\&apos;s managing (kb-based director and inventor of the plastic methods) cone even collects them. Table 2: Examples of improved answer correctness. The justi\x0ccation of this answer is provided by the following proof trace. The prover attempts to prove the LFT of the question (QLF) correct by proving from left to right each term of QLF. ------------------------------------------------------------- -&amp;gt;Answer:Over the weekend Mr Koresh sent a request for a word processor to enable him to record his revelations.</abstract>
<note confidence="0.934551803921569">amp;gt;QLF:David(1)^Koresh(1)^word(2)^processor(2)^FBI(4)^ ask(3 4 2 1 5)^_REASON(5)^_PER(1)^_ORG(4) -&amp;gt;ALF:Mr(71)^Koresh(71)^word(72)^processor(72)^revelations(74)^ record(73 74 75)^enable(75 73 76)^request(76)^sent(77 76 78 71) ^weekend(78)^_PER(71)^_DATE(78) .............................................................. --&amp;gt;Proving:David(1)^Koresh(1)^word(2)^processor(2)^FBI(4)^ ask(3 4 2 1 5)^_REASON(5)^_PER(1)^_ORG(4) There are 1 target axioms. Selected axiom: David(1):= Mr(1). Unifying: 1 to 1. Ok --&amp;gt; Proving: Mr(1)^Koresh(1)^word(2)^processor(2)^FBI(4) ^ask(3 4 2 1 5)^_REASON(5)^_PER(1)^_ORG(4) There are 1 target axioms. Selected axiom: Mr(71):= null. Unifying: 1 to 71. Ok --&amp;gt; Proving: Koresh(71)^word(2)^processor(2)^FBI(4)^ ask(3 4 2 71 5)^_REASON(5)^_PER(71)^_ORG(4) There are 1 target axioms. Selected axiom: Koresh(71):= null. Unifying: 71 to 71. Ok --&amp;gt; Proving: word(2)^processor(2)^FBI(4)^ask(3 4 2 71 5)^ _REASON(5)^_PER(71)^_ORG(4) There are 1 target axioms. Selected axiom: word(72):= null. Unifying: 2 to 72. Ok --&amp;gt; Proving: processor(72)^FBI(4)^ask(3 4 72 71 5)^_REASON(5) ^_PER(71)^_ORG(4) There are 1 target axioms. Selected axiom: processor(72):= null. Unifying: 72 to 72. Ok --&amp;gt; Proving: FBI(4)^ask(3 4 72 71 5)^_REASON(5)^_PER(71)^_ORG(4) There are 1 target axioms. Selected axiom: FBI(1):= null. Unifying: 4 to 1. Ok --&amp;gt; Proving: ask(3 4 72 71 5)^_REASON(5)^_PER(71)^_ORG(4) There are 2 target axioms. Selected axiom: ask(1 2 3 4 5):= sent(1 6 7 4)^request(6). Unifying: 1 to 2. 3 to 1. 5 to 5. 71 to 4. 72 to 3. Ok --&amp;gt; Proving: sent(1 6 7 71)^request(6)^_REASON(5)^_PER(71)^_ORG(2) There are 1 target axioms. Selected axiom: sent(77 76 78 71):= null. Unifying: 1 to 77. 6 to 76. 7 to 78. 71 to 71. Ok --&amp;gt; Proving: request(76)^_REASON(5)^_PER(71)^_ORG(2) There are 1 target axioms. Selected axiom: request(76):= null. Unifying: 76 to 76. Ok --&amp;gt; Proving: _REASON(5)^_PER(71)^_ORG(2) There are 1 target axioms. Selected axiom: _REASON(5):= enable(5 3 6). Unifying: 5 to 5. Ok --&amp;gt; Proving: enable(5 3 6)^_PER(71)^_ORG(2) There are 1 target axioms. Selected axiom: enable(75 73 76):= null. Unifying: 3 to 73. 5 to 75. 6 to 76. Ok --&amp;gt; Proving: _PER(71)^_ORG(2) There are 3 target axioms. Selected axiom: _PER(71):= null. Unifying: 71 to 71. Ok --&amp;gt; Proving: _ORG(2) There are 1 target axioms. Selected axiom: _ORG(1):= FBI(1). Unifying: 2 to 1. Ok</note>
<abstract confidence="0.985541163461539">amp;gt; Proving: null  |We found:Success. ------------------------------------------------------------------ There are cases when our simple prover fails to prove a correct answer. We have noticed that this happens because in the answer semantic representation, some concepts that are connected in the question semantic representation are no longer directly linked. This is due to the fact that there are either parser errors or there are new syntactic dependencies between the two concepts. To accommodate this situation, we allow di\x0berent constants that are arguments of the same predicate to be uni\x0cable. The special cases in which this relaxation of the uni\x0ccation procedure is allowed constitute our abduction rules. \x0c5 Evaluation Both qualitative and quantitative evaluation of the integration of surface text-based and knowledgebased methods for Q/A is imposed. Quantitatively, Table 3 summarizes the scores obtained when only shallow methods were employed, in contrast with the results when knowledge-based methods were integrated. We have separately measured the e\x0bect of the integration of the knowledge-based methods at question processing and answer processing level. We have also evaluated the precision of the system when both integrations were implemented. The results were the \x0crst \x0cve answers returned within 250 bytes of text, when approximatively half million TREC documents are mined. We have used the 200 questions from TREC-8, and the correct answers provided by NIST. The performance was measured both with the NIST scoring method employed in the TREC-8 and by simply assigning a score of 1 for the question having a correct answer, regardless of its position. Percentage of NIST score correct answers in top 5 returns Text-surface-based 77.7% 64.5% Knowledge-based 83.2% 71.5% Question Processing (only) Text-surface-based 77.7% 73% only with Answer Justi\x0ccation Knowledge-based 89.5% 84.75% Question Processing with Answer Justi\x0ccation Table 3: Accuracy performance When using the NIST scoring method to evaluate an individual answer, we used only six values:(1; :5; :33; :25; :2; 0), representing the score the answer\&apos;s question obtains. If the \x0crst answer is correct, it obtains a score of 1, if the second one is correct, it is scored with :5, if the third one is correct, the score becomes :33, if the fourth is correct, the score is :25 and if the \x0cfth one is correct, the score is :2. Otherwise, it is scored with 0. No credit is given if multiple answers are correct. Table 3 shows that both knowledge-based methods enhanced the precision, regardless of the scoring method. To further evaluate the contribution of the justi- \x0ccation option, we evaluated separately the precision of the prover for those questions for which the surface-text-based methods of our system, when operating alone, cannot \x0cnd correct answers. We had 45 TREC-8 questions for which the evaluation of the prover was performed. Table 4 summarizes the accuracy of the prover. Proven Proven Precision correct incorrect Incorrect answers 3 210 98.5% (no knowledge) Correct answers 127 5 96.2% (KB-based) Incorrect answers 4 38 90.04% (KB-based) Table 4: Prover performance Qualitatively, we \x0cnd that the integration of knowledge-based methods is very bene\x0ccial. Table 2 illustrates the correct answer obtained with these methods, in contrast to the incorrect answer provided when only the shallow techniques are applied. 6 Conclusions We believe that the performance of a Q/A system depends on the knowledge sources it employs. In this paper we have presented the e\x0bect of the integration of knowledge derived from question taxonomies and produced by answer justi\x0ccations on the Q/A precision. Our knowledge-based methods are lightweight, since we do not generate precise semantic representations of questions or answers, but mere approximations determined by syntactic dependencies. Furthermore, our prover operates on very simple logical representations, in which syntactic and semantic ambiguities are completely ignored. Nevertheless, we have shown that these approximations are functional, since we implemented a prover that justi\x0ces answers with high precision. Similarly, our knowledge-based question processing is a mere combination of word class information and syntactic dependencies.</abstract>
<title confidence="0.897467">References</title>
<author confidence="0.982085">A New Statistical Parser Based on Bigram</author>
<affiliation confidence="0.684633">Lexical Dependencies. In Proceedings of the 34st Annual Meeting of the Association for Computational Linguistics,</affiliation>
<note confidence="0.885762608695652">ACL-96, pages 184{191, 1996. Christiane Fellbaum (Ed). WordNet - An Electronic Lexical Database. MIT Press, 1998. Jerry R. Hobbs. Discourse and Inference. Unpublished manuscript, 1986. Jerry R. Hobbs. Overview of the TACITUS Project. In Computational Linguistics, 12:(3), 1986. Jerry Hobbs, Mark Stickel, Doug Appelt, and Paul Martin. Interpretation as abduction. Arti\x0ccial Intelligence, 63, pages 69{142, 1993. Wendy Lehnert. The processing of question answering. Lawrence Erlbaum Publishers, 1978. Dan Moldovan, Sanda Harabagiu, Marius Pa\x18 sca, Rada Mihalcea, Richard Goodrum, Roxana G^ \x10rju and Vasile Rus. Lasso: a tool for sur\x0cng the answer net. In Proceedings of TREC-8, 1999. Ellen Rilo\x0b and Rosie Jones. Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. In Proceedings of the 16th National Conference on Arti\x0ccial Intelligence, AAAI-99, 1999. \x0c&apos;</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sanda M Harabagiu</author>
<author>A Marius</author>
</authors>
<title>b&apos;Experiments with Open-Domain Textual Question Answering</title>
<date>1996</date>
<booktitle>Pa\x18 sca</booktitle>
<pages>184--191</pages>
<marker>Harabagiu, Marius, 1996</marker>
<rawString>b&apos;Experiments with Open-Domain Textual Question Answering Sanda M. Harabagiu and Marius A. Pa\x18 sca and Steven J. Maiorano Michael Collins. A New Statistical Parser Based on Bigram Lexical Dependencies. In Proceedings of the 34st Annual Meeting of the Association for Computational Linguistics, ACL-96, pages 184{191, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet - An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum (Ed). WordNet - An Electronic Lexical Database. MIT Press, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Discourse and Inference.</title>
<date>1986</date>
<note>Unpublished manuscript,</note>
<marker>Hobbs, 1986</marker>
<rawString>Jerry R. Hobbs. Discourse and Inference. Unpublished manuscript, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Overview of the TACITUS Project.</title>
<date>1986</date>
<journal>In Computational Linguistics,</journal>
<volume>12</volume>
<marker>Hobbs, 1986</marker>
<rawString>Jerry R. Hobbs. Overview of the TACITUS Project. In Computational Linguistics, 12:(3), 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
<author>Mark Stickel</author>
<author>Doug Appelt</author>
<author>Paul Martin</author>
</authors>
<title>Interpretation as abduction.</title>
<date>1993</date>
<journal>Arti\x0ccial Intelligence,</journal>
<volume>63</volume>
<pages>69--142</pages>
<marker>Hobbs, Stickel, Appelt, Martin, 1993</marker>
<rawString>Jerry Hobbs, Mark Stickel, Doug Appelt, and Paul Martin. Interpretation as abduction. Arti\x0ccial Intelligence, 63, pages 69{142, 1993. Wendy Lehnert. The processing of question answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Erlbaum Publishers</author>
</authors>
<title>Dan Moldovan, Sanda Harabagiu,</title>
<date>1978</date>
<booktitle>Marius Pa\x18 sca, Rada Mihalcea, Richard Goodrum, Roxana G^ \x10rju and Vasile Rus.</booktitle>
<marker>Publishers, 1978</marker>
<rawString>Lawrence Erlbaum Publishers, 1978. Dan Moldovan, Sanda Harabagiu, Marius Pa\x18 sca, Rada Mihalcea, Richard Goodrum, Roxana G^ \x10rju and Vasile Rus.</rawString>
</citation>
<citation valid="true">
<title>Lasso: a tool for sur\x0cng the answer net.</title>
<date>1999</date>
<booktitle>In Proceedings of TREC-8,</booktitle>
<marker>1999</marker>
<rawString>Lasso: a tool for sur\x0cng the answer net. In Proceedings of TREC-8, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Rilo\x0b</author>
<author>Rosie Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th National Conference on Arti\x0ccial Intelligence, AAAI-99,</booktitle>
<note>x0c&apos;</note>
<marker>Rilo\x0b, Jones, 1999</marker>
<rawString>Ellen Rilo\x0b and Rosie Jones. Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. In Proceedings of the 16th National Conference on Arti\x0ccial Intelligence, AAAI-99, 1999. \x0c&apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>