<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.547277">
b&amp;apos;Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 5361,
</bodyText>
<address confidence="0.496874">
Beijing, August 2010
</address>
<title confidence="0.734664">
Fast and Accurate Arc Filtering for Dependency Parsing
</title>
<author confidence="0.976131">
Shane Bergsma
</author>
<affiliation confidence="0.9934205">
Department of Computing Science
University of Alberta
</affiliation>
<email confidence="0.984593">
sbergsma@ualberta.ca
</email>
<author confidence="0.981997">
Colin Cherry
</author>
<affiliation confidence="0.84874">
Institute for Information Technology
National Research Council Canada
</affiliation>
<email confidence="0.57178">
colin.cherry@nrc-cnrc.gc.ca
</email>
<sectionHeader confidence="0.97116" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999237588235294">
We propose a series of learned arc fil-
ters to speed up graph-based dependency
parsing. A cascade of filters identify im-
plausible head-modifier pairs, with time
complexity that is first linear, and then
quadratic in the length of the sentence.
The linear filters reliably predict, in con-
text, words that are roots or leaves of de-
pendency trees, and words that are likely
to have heads on their left or right. We
use this information to quickly prune arcs
from the dependency graph. More than
78% of total arcs are pruned while retain-
ing 99.5% of the true dependencies. These
filters improve the speed of two state-of-
the-art dependency parsers, with low over-
head and negligible loss in accuracy.
</bodyText>
<sectionHeader confidence="0.998041" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998052452830189">
Dependency parsing finds direct syntactic rela-
tionships between words by connecting head-
modifier pairs into a tree structure. Depen-
dency information is useful for a wealth of nat-
ural language processing tasks, including ques-
tion answering (Wang et al., 2007), semantic pars-
ing (Poon and Domingos, 2009), and machine
translation (Galley and Manning, 2009).
We propose and test a series of arc filters for
graph-based dependency parsers, which rule out
potential head-modifier pairs before parsing be-
gins. In doing so, we hope to eliminate im-
plausible links early, saving the costs associated
with them, and speeding up parsing. In addi-
tion to the scaling benefits that come with faster
processing, we hope to enable richer features
for parsing by constraining the set of arcs that
need to be considered. This could allow ex-
tremely large feature sets (Koo et al., 2008), or the
look-up of expensive corpus-based features such
as word-pair mutual information (Wang et al.,
2006). These filters could also facilitate expen-
sive learning algorithms, such as semi-supervised
approaches (Wang et al., 2008).
We propose three levels of filtering, which are
applied in a sequence of increasing complexity:
Rules: A simple set of machine-learned rules
based only on parts-of-speech. They prune over
25% of potential arcs with almost no loss in cover-
age. Rules save on the wasted effort for assessing
implausible arcs such as DT DT.
Linear: A series of classifiers that tag words ac-
cording to their possible roles in the dependency
tree. By treating each word independently and en-
suring constant-time feature extraction, they oper-
ate in linear time. We view these as a dependency-
parsing analogue to the span-pruning proposed by
Roark and Hollingshead (2008). Our fast linear
filters prune 54.2% of potential arcs while recov-
ering 99.7% of true pairs.
Quadratic: A final stage that looks at pairs of
words to prune unlikely arcs from the dependency
tree. By employing a light-weight feature set, this
high-precision filter can enable more expensive
processing on the remaining plausible dependen-
cies.
Collectively, we show that more than 78% of
total arcs can be pruned while retaining 99.5% of
the true dependencies. We test the impact of these
filters at both train and test time, using two state-
of-the-art discriminative parsers, demonstrating
speed-ups of between 1.9 and 5.6, with little im-
pact on parsing accuracy.
</bodyText>
<page confidence="0.999374">
53
</page>
<figureCaption confidence="0.9358675">
\x0cInvestors continue to pour cash into money funds
Figure 1: An example dependency parse.
</figureCaption>
<sectionHeader confidence="0.983465" genericHeader="introduction">
2 Dependency Parsing
</sectionHeader>
<bodyText confidence="0.99872148">
A dependency tree represents the syntactic struc-
ture of a sentence as a directed graph (Figure 1),
with a node for each word, and arcs indicat-
ing head-modifier pairs (Melcuk, 1987). Though
dependencies can be extracted from many for-
malisms, there is a growing interest in predict-
ing dependency trees directly. To that end, there
are two dominant approaches: graph-based meth-
ods, characterized by arc features in an exhaus-
tive search, and transition-based methods, char-
acterized by operational features in a greedy
search (McDonald and Nivre, 2007). We focus on
graph-based parsing, as its exhaustive search has
the most to gain from our filters.
Graph-based dependency parsing finds the
highest-scoring tree according to a scoring func-
tion that decomposes under an exhaustive search
(McDonald et al., 2005). The most natural de-
composition scores individual arcs, represented as
head-modifier pairs [h, m]. This enables search
by either minimum spanning tree (West, 2001) or
by Eisners (1996) projective parser. This paper
focuses on the projective case, though our tech-
niques transfer to spanning tree parsing. With a
linear scoring function, the parser solves:
</bodyText>
<equation confidence="0.97385">
parse(s) = argmaxts
X
[h,m]t
w
f(h, m, s)
</equation>
<bodyText confidence="0.998180085714286">
The weights w are typically learned using an
online method, such as an averaged percep-
tron (Collins, 2002) or MIRA (Crammer and
Singer, 2003). 2nd-order searches, which consider
two siblings at a time, are available with no in-
crease in asymptotic complexity (McDonald and
Pereira, 2006; Carreras, 2007).
The complexity of graph-based parsing is
bounded by two processes: parsing (carrying out
the argmax) and arc scoring (calculating w
f(h, m, s)). For a sentence with n words, pro-
jective parsing takes O(n3) time, while the span-
ning tree algorithm is O(n2). Both parsers require
scores for arcs connecting each possible [h, m]
pair in s; therefore, the cost of arc scoring is also
O(n2), and may become O(n3) if the features in-
clude words in s between h and m (Galley and
Manning, 2009). Arc scoring also has a signif-
icant constant term: the number of features ex-
tracted for an [h, m] pair. Our in-house graph-
based parser collects on average 62 features for
each potential arc, a number larger than the length
of most sentences. With the cluster-based features
suggested by Koo et al. (2008), this could easily
grow by a factor of 3 or 4.
The high cost of arc scoring, coupled with
the parsing stages low grammar constant, means
that graph-based parsers spend much of their time
scoring potential arcs. Johnson (2007) reports that
when arc scores have been precomputed, the dy-
namic programming component of his 1st-order
parser can process an amazing 3,580 sentences per
second.1 Beyond reducing the number of features,
the easiest way to reduce the computational bur-
den of arc scoring is to score only plausible arcs.
</bodyText>
<sectionHeader confidence="0.999655" genericHeader="related work">
3 Related Work
</sectionHeader>
<subsectionHeader confidence="0.982091">
3.1 Vine Parsing
</subsectionHeader>
<bodyText confidence="0.999320052631579">
Filtering dependency arcs has been explored pri-
marily in the form of vine parsing (Eisner and
Smith, 2005; Dreyer et al., 2006). Vine pars-
ing establishes that, since most dependencies are
short, one can parse quickly by placing a hard
constraint on arc length. As this coarse fil-
ter quickly degrades the best achievable perfor-
mance, Eisner and Smith (2005) also consider
conditioning the constraint on the part-of-speech
(PoS) tags being linked and the direction of the
arc, resulting in a separate threshold for each
[tag(h), tag(m), dir(h, m)] triple. They sketch
an algorithm where the thresholded length for
each triple starts at the highest value seen in the
training data. Thresholds are then decreased in
a greedy fashion, with each step producing the
smallest possible reduction in reachable training
arcs. We employ this algorithm as a baseline in
our experiments. To our knowledge, vine parsing
</bodyText>
<page confidence="0.926502">
1
</page>
<bodyText confidence="0.9882622">
To calibrate this speed, consider that the publicly avail-
able 1st
-order MST parser processes 16 sentences per second
on modern hardware. This includes I/O costs in addition to
the costs of arc scoring and parsing.
</bodyText>
<page confidence="0.99579">
54
</page>
<bodyText confidence="0.9913245">
\x0chas not previously been tested with a state-of-the-
art, discriminative dependency parser.
</bodyText>
<subsectionHeader confidence="0.998624">
3.2 CFG Cell Classification
</subsectionHeader>
<bodyText confidence="0.998832785714286">
Roark and Hollingshead (2008) speed up another
exhaustive parsing algorithm, the CKY parser for
CFGs, by classifying each word in the sentence
according to whether it can open (or close) a
multi-word constituent. With a high-precision
tagger that errs on the side of permitting con-
stituents, they show a significant improvement in
speed with no reduction in accuracy.
It is difficult to port their idea directly to depen-
dency parsing without committing to a particular
search algorithm,2 and thereby sacrificing some
of the graph-based formalisms modularity. How-
ever, some of our linear filters (see Section 4.3)
were inspired by their constraints.
</bodyText>
<subsectionHeader confidence="0.978808">
3.3 Coarse-to-fine Parsing
</subsectionHeader>
<bodyText confidence="0.999157090909091">
Another common method employed to speed up
exhaustive parsers is a coarse-to-fine approach,
where a cheap, coarse model prunes the search
space for later, more expensive models (Charniak
et al., 2006; Petrov and Klein, 2007). This ap-
proach assumes a common forest or chart repre-
sentation, shared by all granularities, where one
can efficiently track the pruning decisions of the
coarse models. One could imagine applying such
a solution to dependency parsing, but the exact
implementation of the coarse pass would vary ac-
cording to the choice in search algorithm. Our fil-
ters are much more modular: they apply to both
1st-order spanning tree parsing and 2nd-order pro-
jective parsing, with no modification.
Carreras et al. (2008) use coarse-to-fine pruning
with dependency parsing, but in that case, a graph-
based dependency parser provides the coarse pass,
with the fine pass being a far-more-expensive tree-
adjoining grammar. Our filters could become a
0th pass, further increasing the efficiency of their
approach.
</bodyText>
<sectionHeader confidence="0.981376" genericHeader="method">
4 Arc Filters
</sectionHeader>
<bodyText confidence="0.9992205">
We propose arc filtering as a preprocessing step
for dependency parsing. An arc filter removes im-
</bodyText>
<page confidence="0.972438">
2
</page>
<bodyText confidence="0.974023333333333">
Johnsons (2007) split-head CFG could implement this
idea directly with little effort.
plausible head-modifier arcs from the complete
dependency graph (which initially includes all
head-modifier arcs). We use three stages of filters
that operate in sequence on progressively sparser
graphs: 1) rule-based, 2) linear: a single pass
through the n nodes in a sentence (O(n) complex-
ity), and 3) quadratic: a scoring of all remaining
arcs (O(n2)). The less intensive filters are used
first, saving time by leaving fewer arcs to be pro-
cessed by the more intensive systems.
Implementations of our rule-based, linear, and
quadratic filters are publicly available at:
http://code.google.com/p/arcfilter/
</bodyText>
<subsectionHeader confidence="0.987431">
4.1 Filter Framework
</subsectionHeader>
<bodyText confidence="0.998725857142857">
Our filters assume the input sentences have been
PoS-tagged. We also add an artificial root node
to each sentence to be the head of the trees root.
Initially, this node is a potential head for all words
in the sentence.
Each filter is a supervised classifier. For exam-
ple, the quadratic filter directly classifies whether
a proposed head-modifier pair is not a link in the
dependency tree. Training data is created from an-
notated trees. All possible arcs are extracted for
each training sentence, and those that are present
in the annotated tree are labeled as class 1, while
those not present are +1. A similar process gener-
ates training examples for the other filters. Since
our goal is to only filter very implausible arcs, we
bias the classifier to high precision, increasing the
cost for misclassifying a true arc during learning.3
Class-specific costs are command-line parame-
ters for many learning packages. One can inter-
pret the learning objective as minimizing regular-
ized, weighted loss:
</bodyText>
<equation confidence="0.968077076923077">
min
w
1
2
||w||2
+ C1
X
i:yi=1
l(w, yi, xi)
+C2
X
i:yi=1
l(w, yi, xi) (1)
</equation>
<bodyText confidence="0.9250275">
where l() is the learning methods loss function,
xi and yi are the features and label for the ith
</bodyText>
<page confidence="0.988284">
3
</page>
<bodyText confidence="0.991032">
Learning with a cost model is generally preferable to
first optimizing error rate and then thresholding the predic-
tion values to select a high-confidence subset (Joachims,
2005), but the latter approach was used successfully for cell
classification in Roark and Hollingshead (2008).
</bodyText>
<page confidence="0.903047">
55
</page>
<figure confidence="0.7060399">
\x0cnot a h , . ;  |CC PRP$ PRP EX
-RRB- -LRB-
no m EX LS POS PRP$
no m . RP
not a root , DT
no hm DT{DT,JJ,NN,NNP,NNS,.}
CDCD NN{DT,NNP}
NNP{DT,NN,NNS}
no mh {DT,IN,JJ,NN,NNP}DT
NNPIN INJJ
</figure>
<tableCaption confidence="0.992183">
Table 1: Learned rules for filtering dependency
</tableCaption>
<bodyText confidence="0.962613111111111">
arcs using PoS tags. The rules filter 25% of pos-
sible arcs while recovering 99.9% of true links.
training example, w is the learned weight vector,
and C1 and C2 are the class-specific costs. High
precision is obtained when C2 &amp;gt;&amp;gt; C1. For an
SVM, l(w, yi, xi) is the standard hinge loss.
We solve the SVM objective using LIBLIN-
EAR (Fan et al., 2008). In our experiments, each
filter is a linear SVM with the typical L1 loss and
</bodyText>
<table confidence="0.476866">
L2 regularization.4 We search for the best com-
bination of C1 and C2 using a grid search on de-
velopment data. At test time, an arc is filtered if
w x &amp;gt; 0.
</table>
<subsectionHeader confidence="0.878756">
4.2 Rule-Based Filtering
</subsectionHeader>
<bodyText confidence="0.9900235">
Our rule-based filters seek to instantly remove
those arcs that are trivially implausible on the ba-
sis of their head and modifier PoS tags. We first
extract labeled examples from gold-standard trees
for whenever a) a word is not a head, b) a word
does not have a head on the left (resp. right), and
c) a pair of words is not linked. We then trained
high-precision SVM classifiers. The only features
in x are the PoS tag(s) of the head and/or modi-
fier. The learned feature weights identify the tags
and tag-pairs to be filtered. For example, if a tag
has a positive weight in the not-a-head classifier,
all arcs having that node as head are filtered.
The classier selects a small number of high-
</bodyText>
<page confidence="0.962921">
4
</page>
<bodyText confidence="0.999476666666667">
We also tried L1-regularized filters. L1 encourages most
features to have zero weight, leading to more compact and
hence faster models. We found the L1 filters to prune fewer
arcs at a given coverage level, providing less speed-up at
parsing time. Both L1 and L2 models are available in our
publicly available implementation.
precision rules, shown in Table 1. Note that the
rules tend to use common tags with well-defined
roles. By focusing on weighted loss as opposed
to arc frequency, the classifier discovers struc-
tural zeros (Mohri and Roark, 2006), events which
could have been observed, but were not. We
consider this an improvement over the frequency-
based length thresholds employed previously in
tag-specific vine parsing.
</bodyText>
<subsectionHeader confidence="0.991237">
4.3 Linear-Time Filtering
</subsectionHeader>
<bodyText confidence="0.96962175">
In the linear filtering stage, we filter arcs on the
basis of single nodes and their contexts, passing
through the sentences in linear time. For each
node, eight separate classifiers decide whether:
</bodyText>
<listItem confidence="0.9964046">
1. It is not a head (i.e., it is a leaf of the tree).
2. Its head is on the left/right.
3. Its head is within 5 nodes on the left/right.
4. Its head is immediately on the left/right.
5. It is the root.
</listItem>
<bodyText confidence="0.996431043478261">
For each of these decisions, we again train high-
precision SVMs with C2 &amp;gt;&amp;gt; C1, and filter di-
rectly based on the classifier output.
If a word is not a head, all arcs with the given
word as head can be pruned. If a word is deemed
to have a head within a certain range on the left
or right, then all arcs that do not obey this con-
straint can be pruned. If a root is found, no other
words should link to the artificial root node. Fur-
thermore, in a projective dependency tree, no arc
will cross the root, i.e., there will be no arcs where
a head and a modifier lie on either side of the root.
We can therefore also filter arcs that violate this
constraint when parsing projectively.
Sgaard and Kuhn (2009) previously proposed
a tagger to further constrain a vine parser. Their
tags are a subset of our decisions (items 4 and 5
above), and have not yet been tested in a state-of-
the-art system.
Development experiments show that if we
could perfectly make decisions 1-5 for each word,
we could remove 91.7% of the total arcs or 95%
of negative arcs, close to the upper bound.
</bodyText>
<subsectionHeader confidence="0.682931">
Features
</subsectionHeader>
<bodyText confidence="0.9746745">
Unlike rule-based filtering, linear filtering uses
a rich set of features (Table 2). Each feature is a
</bodyText>
<page confidence="0.975851">
56
</page>
<table confidence="0.941946363636364">
\x0cPoS-tag features Other features
tagi wordi
tagi, tagi1 wordi+1
tagi, tagi+1 wordi1
tagi1, tagi+1 shapei
tagi2, tagi1 prefixi
tagi+1, tagi+2 suffixi
tagj, Left, j=i5...i1 i
tagj, Right, j=i+1...i+5 i, n
tagj, (i-j), j=i5...i1 n - i
tagj, (i-j), j=i+1...i+5
</table>
<tableCaption confidence="0.982704">
Table 2: Linear filter features for a node at po-
</tableCaption>
<bodyText confidence="0.996683740740741">
sition i in a sentence of length n. Each feature
is also conjoined (unless redundant) with wordi,
tagi, shapei, prefixi, and suffixi (both 4 letters).
The shape is the word normalized using the regu-
lar expressions [A-Z]+ A and [a-z]+ a.
binary indicator feature. To increase the speed of
applying eight classifiers, we use the same feature
vector for each of the decisions; learning gives
eight different weight vectors, one corresponding
to each decision function. Feature extraction is
constrained to be O(1) for each node, so that over-
all feature extraction and classification remain a
fast O(n) complexity. Feature extraction would
be O(n2) if, for example, we had a feature for ev-
ery tag on the left or right of a node.
Combining linear decisions
We originally optimized the C1 and C2 param-
eter separately for each linear decision function.
However, we found we could substantially im-
prove the collective performance of the linear fil-
ters by searching for the optimal combination of
the component decisions, testing different levels
of precision for each component. We selected a
few of the best settings for each decision when op-
timized separately, and then searched for the best
combination of these candidates on development
data (testing 12960 combinations in all).
</bodyText>
<subsectionHeader confidence="0.993637">
4.4 Quadratic-Time Filtering
</subsectionHeader>
<bodyText confidence="0.706066727272727">
In the quadratic filtering stage, a single classifier
decides whether each head-modifier pair should
be filtered. It is trained and applied as described
in Section 4.1.
Binary features
sign(h-m) tagshm
tagm1, tagshm tagm+1, tagshm
tagh1, tagshm tagh+1, tagshm
sign(h-m), tagh, wordm
sign(h-m), wordh, tagm
Real features values
</bodyText>
<equation confidence="0.17075175">
sign(h-m) h-m
tagh, tagm h-m
tagk, tagshm Count(tagk tagsh...m)
wordk, tagshm Count(wordk wordsh...m)
</equation>
<tableCaption confidence="0.983761">
Table 3: Quadratic filter features for a head at po-
</tableCaption>
<bodyText confidence="0.996222">
sition h and a modifier at position m in a sentence
of length n. Here tagshm = (sign(h-m), tagh,
tagm), while tagsh...m and wordsh...m are all the
tags (resp. words) between h and m, but within
5 positions of h or m.
While theoretically of the same complexity as
the parsers arc-scoring function (O(n2)), this
process can nevertheless save time by employing
a compact feature set. We view quadratic filter-
ing as a light preprocessing step, using only a por-
tion of the resources that might be used in the final
scoring function.
</bodyText>
<subsectionHeader confidence="0.840559">
Features
</subsectionHeader>
<bodyText confidence="0.99744895">
Quadratic filtering uses both binary and real-
valued features (Table 3). Real-valued features
promote a smaller feature space. For example,
one value can encode distance rather than separate
features for different distances. We also general-
ize the between-tag features used in McDonald
et al. (2005) to be the count of each tag between
the head and modifier. The count may be more in-
formative than tag presence alone, particularly for
high-precision filters. We follow Galley and Man-
ning (2009) in using only between-tags within a
fixed range of the head or modifier, so that the ex-
traction for each pair is O(1) and the overall fea-
ture extraction is O(n2).
Using only a subset of the between-tags as fea-
tures has been shown to improve speed but im-
pair parser performance (Galley and Manning,
2009). By filtering quickly first, then scoring all
remaining arcs with a cubic scoring function in the
parser, we hope to get the best of both worlds.
</bodyText>
<page confidence="0.98002">
57
</page>
<table confidence="0.3429315">
\x0c5 Filter Experiments
Data
</table>
<bodyText confidence="0.83378775">
We extract dependency structures from the
Penn Treebank using the Penn2Malt extraction
tool,5 which implements the head rules of Yamada
and Matsumoto (2003). Following convention, we
divide the Treebank into train (sections 221), de-
velopment (22) and test sets (23). The develop-
ment and test sets are re-tagged using the Stanford
tagger (Toutanova et al., 2003).
</bodyText>
<subsectionHeader confidence="0.964831">
Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.987725285714286">
To measure intrinsic filter quality, we define
Reduction as the proportion of total arcs re-
moved, and Coverage as the proportion of true
head-modifier arcs retained. Our evaluation asks,
for each filter, what Reduction can be obtained at
a given Coverage level? We also give Time: how
long it takes to apply the filters to the test set (ex-
cluding initialization).
We compute an Upper Bound for Reduction on
development data. There are 1.2 million poten-
tial dependency links in those sentences, 96.5%
of which are not present in a gold standard depen-
dency tree. Therefore, the maximum achievable
Reduction is 96.5%.
</bodyText>
<subsectionHeader confidence="0.525438">
Systems
</subsectionHeader>
<bodyText confidence="0.998609363636364">
We evaluate the following systems:
Rules: the rule-based filter (Section 4.2)
Lin.: the linear-time filters (Section 4.3)
Quad.: the quadratic filter (Section 4.4)
The latter two approaches run on the output of the
previous stage. We compare to the two vine pars-
ing approaches described in Section 3.1:
Len-Vine uses a hard limit on arc length.
Tag-Vine (later, Vine) learns a maxi-
mum length for dependency arcs for every
head/modifier tag-combination and order.
</bodyText>
<sectionHeader confidence="0.678091" genericHeader="method">
5.1 Results
</sectionHeader>
<bodyText confidence="0.969014">
We set each filters parameters by selecting
</bodyText>
<figure confidence="0.993925869565217">
a Coverage-Reduction tradeoff on development
5
http://w3.msi.vxu.se/nivre/research/Penn2Malt.
html
20
30
40
50
60
70
80
90
100
99.3 99.4 99.5 99.6 99.7 99.8 99.9
Reduction
(%)
Coverage (%)
Upper Bd
Lin-Orac.
Quad
Lin
Tag-Vine
Len-Vine
</figure>
<figureCaption confidence="0.998461">
Figure 2: Filtering performance for different fil-
</figureCaption>
<bodyText confidence="0.940683">
ters and cost parameters on development data.
Lin-Orac indicates the percentage filtered using
perfect decisions by the linear components.
</bodyText>
<table confidence="0.9721122">
Filter Coverage Reduct. Time (s)
Vine 99.62 44.0 2.9s
Rules 99.86 25.8 1.3s
Lin. 99.73 54.2 7.3s
Quad. 99.50 78.4 16.1s
</table>
<tableCaption confidence="0.998886">
Table 4: Performance (%) of filters on test data.
</tableCaption>
<bodyText confidence="0.996038428571429">
data (Figure 2). The Lin curve is obtained by vary-
ing both the C1/C2 cost parameters and the combi-
nation of components (plotting the best Reduction
at each Coverage level). We chose the linear fil-
ters with 99.8% Coverage at a 54.2% Reduction.
We apply Quad on this output, varying the cost
parameters to produce its curve. Aside from Len-
Vine, all filters remove a large number of arcs with
little drop in Coverage.
After selecting a desired trade-off for each clas-
sifier, we move to final filtering experiments on
unseen test data (Table 4). The linear filter re-
moves well over half the links but retains an as-
tounding 99.7% of correct arcs. Quad removes
78.4% of arcs at 99.5% Coverage. It thus reduces
the number of links to be scored by a dependency
parser by a factor of five.
The time for filtering the 2416 test sentences
varies from almost instantaneous for Vine and
Rules to around 16 seconds for Quad. Speed num-
bers are highly machine, design, and implemen-
</bodyText>
<page confidence="0.997127">
58
</page>
<table confidence="0.999353666666667">
\x0cDecision Precision Recall
No-Head 99.9 44.8
Right- 99.9 28.7
Left- 99.9 39.0
Right-5 99.8 31.5
Left-5 99.9 19.7
Right-1 99.7 6.2
Left-1 99.7 27.3
Root 98.6 25.5
</table>
<tableCaption confidence="0.578449">
Table 5: Linear Filters: Test-set performance (%)
on decisions for components of the combined 54.2
</tableCaption>
<table confidence="0.999530272727273">
Reduct./99.73 Coverage linear filter.
Type Coverage Reduct. Oracle
All 99.73 54.2 91.8
All\\No-Head 99.76 46.4 87.2
All\\Left- 99.74 53.2 91.4
All\\Right- 99.75 53.6 90.7
All\\Left-5 99.74 53.2 89.7
All\\Right-5 99.74 51.6 90.4
All\\Left-1 99.75 53.5 90.8
All\\Right-1 99.73 53.9 90.6
All\\Root 99.76 50.2 90.0
</table>
<tableCaption confidence="0.999676">
Table 6: Contribution of different linear filters to
</tableCaption>
<bodyText confidence="0.95192775">
test set performance (%). Oracle indicates the per-
centage filtered by perfect decisions.
tation dependent, and thus we have stressed the
asymptotic complexity of the filters. However, the
timing numbers show that arc filtering can be done
quite quickly. Section 6 confirms that these are
very reasonable costs in light of the speed-up in
overall parsing.
</bodyText>
<subsectionHeader confidence="0.99964">
5.2 Linear Filtering Analysis
</subsectionHeader>
<bodyText confidence="0.999566590909091">
It is instructive to further analyze the components
of the linear filter. Table 5 gives the performance
of each classifier on its specific decision. Preci-
sion is the proportion of positive classifications
that are correct. Recall is the proportion of pos-
itive instances that are classified positively (e.g.
the proportion of actual roots that were classified
as roots). The decisions correspond to items 1-5 in
Section 4.3. For example, Right- is the decision
that a word has no head on the right.
Most notably, the optimum Root decision has
much lower Precision than the others, but this has
little effect on its overall accuracy as a filter (Ta-
ble 6). This is perhaps because the few cases of
false positives are still likely to be main verbs or
auxiliaries, and thus still still likely to have few
links crossing them. Thus many of the filtered
links are still correct.
Table 6 provides the performance of the classi-
fier combination when each linear decision is ex-
cluded. No-Head is the most important compo-
nent in the oracle and the actual combination.
</bodyText>
<sectionHeader confidence="0.907423" genericHeader="method">
6 Parsing Experiments
</sectionHeader>
<subsectionHeader confidence="0.996582">
6.1 Set-up
</subsectionHeader>
<bodyText confidence="0.978835709677419">
In this section, we investigate the impact of our fil-
ters on graph-based dependency parsers. We train
each parser unfiltered, and then measure its speed
and accuracy once filters have been applied. We
use the same training, development and test sets
described in Section 5. We evaluate unlabeled de-
pendency parsing using head accuracy: the per-
centage of words (ignoring punctuation) that are
assigned the correct head.
The filters bypass feature extraction for each fil-
tered arc, and replace its score with an extremely
low negative value. Note that 2nd-order features
consider O(n3) [h, m1, m2] triples. These triples
are filtered if at least one component arc ([h, m1]
or [h, m2]) is filtered.
In an optimal implementation, we might also
have the parser re-use features extracted during
filtering when scoring the remaining arcs. We did
not do this. Instead, filtering was treated as a pre-
processing step, which maximizes the portability
of the filters across parsers. We test on two state-
of-the art parsers:
MST We modified the publicly-available MST
parser (McDonald et al., 2005)6 to employ our fil-
ters before carrying out feature extraction. MST
is trained with 5-best MIRA.
DepPercep We also test an in-house depen-
dency parser, which conducts projective first and
2nd-order searches using the split-head CFG de-
scribed by Johnson (2007), with a weight vec-
tor trained using an averaged perceptron (Collins,
</bodyText>
<page confidence="0.718784">
6
</page>
<footnote confidence="0.271781">
http://sourceforge.net/projects/mstparser/
</footnote>
<page confidence="0.92721">
59
</page>
<table confidence="0.996374">
\x0cDepPercep-1 DepPercep-2 MST-1 MST-2
Filter Cost Acc. Time Acc. Time Acc. Time Acc. Time
None +0 91.8 348 92.5 832 91.2 153 91.9 200
Vine +3 91.7 192 92.3 407 91.2 99 91.8 139
Rules +1 91.7 264 92.4 609 91.2 125 91.9 167
Linear +7 91.7 168 92.4 334 91.2 88 91.8 121
Quad. +16 91.7 79 92.3 125 91.2 58 91.8 80
</table>
<tableCaption confidence="0.999313">
Table 7: The effect of filtering on the speed and accuracy on 1st and 2nd-order dependency parsing.
</tableCaption>
<bodyText confidence="0.996319388888889">
2002). Its features are a mixture of those de-
scribed by McDonald et al. (2005), and those used
in the Koo et al. (2008) baseline system; we do not
use word-cluster features.
DepPercep makes some small improvements to
MSTs 1st-order feature set. We carefully de-
termined which feature types should have dis-
tance appended in addition to direction. Also, in-
spired by the reported utility of mixing PoS tags
and word-clusters (Koo et al., 2008), we created
versions of all of the Between and Surround-
ing Word features described by McDonald et al.
(2005) where we mix tags and words.7
DepPercep was developed with quadratic filters
in place, which enabled a fast development cycle
for feature engineering. As a result, it does not
implement many of the optimizations in place in
MST, and is relatively slow unfiltered.
</bodyText>
<sectionHeader confidence="0.630241" genericHeader="evaluation">
6.2 Results
</sectionHeader>
<bodyText confidence="0.9984956">
The parsing results are shown in Table 7, where
times are given in seconds, and Cost indicates the
additional cost of filtering. Note that the impact
of all filters on accuracy is negligible, with a de-
crease of at most 0.2%. In general, parsing speed-
ups mirror the amount of arc reduction measured
in our filter analysis (Section 5.1).
Accounting for filter costs, the benefits of
quadratic filtering depend on the parser. The extra
benefit of quadratic over linear is substantial for
DepPercep, but less so for 1st-order MST.
MST shows more modest speed-ups than Dep-
Percep, but MST is already among the fastest
publicly-available data-driven parsers. Under
quadratic filtering, MST-2 goes from processing
</bodyText>
<page confidence="0.980848">
7
</page>
<bodyText confidence="0.952478272727273">
This was enabled by using word features only when the
word is among the 800 most frequent in the training set.
12 sentences per second to 23 sentences.8
DepPercep-2 starts slow, but benefits greatly
from filtering. This is because, unlike MST-2,
it does not optimize feature extraction by fac-
toring its ten 2nd-order features into two triple
([h, m1, m2]) and eight sibling ([m1, m2]) fea-
tures. This suggests that filtering could have a dra-
matic effect on a parser that uses more than a few
triple features, such as Koo et al. (2008).
</bodyText>
<sectionHeader confidence="0.997228" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9997825">
We have presented a series of arc filters that speed
up graph-based dependency parsing. By treat-
ing filtering as weighted classification, we learn a
cascade of increasingly complex filters from tree-
annotated data. Linear-time filters prune 54%
of total arcs, while quadratic-time filters prune
78%. Both retain at least 99.5% of true dependen-
cies. By testing two state-of-the-art dependency
parsers, we have shown that our filters produce
substantial speed improvements in even carefully-
optimized parsers, with negligible losses in ac-
curacy. In the future we hope to leverage this
reduced search space to explore features derived
from large corpora.
</bodyText>
<sectionHeader confidence="0.967602" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.543853833333333">
Carreras, Xavier, Michael Collins, and Terry Koo.
2008. TAG, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In CoNLL.
Carreras, Xavier. 2007. Experiments with a higher-
order projective dependency parser. In EMNLP-
CoNLL.
</reference>
<page confidence="0.9876">
8
</page>
<bodyText confidence="0.9601545">
This speed accounts for 25 total seconds to apply the
rules, linear, and quadratic filters.
</bodyText>
<page confidence="0.991149">
60
</page>
<reference confidence="0.999512948717949">
\x0cCharniak, Eugene, Mark Johnson, Micha Elsner,
Joseph Austerweil, David Ellis, Isaac Haxton,
Catherine Hill, R. Shrivaths, Jeremy Moore,
Michael Pozar, and Theresa Vu. 2006. Multilevel
coarse-to-fine PCFG parsing. In HLT-NAACL.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Crammer, Koby and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
JMLR, 3:951991.
Dreyer, Markus, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking for
speed and precision. In CoNLL.
Eisner, Jason and Noah A. Smith. 2005. Parsing with
soft and hard constraints on dependency length. In
IWPT.
Eisner, Jason. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COL-
ING.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. JMLR,
9:18711874.
Galley, Michel and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine
translation. In ACL-IJCNLP.
Joachims, Thorsten. 2005. A support vector method
for multivariate performance measures. In ICML.
Johnson, Mark. 2007. Transforming projective bilex-
ical dependency grammars into efficiently-parsable
CFGs with unfold-fold. In ACL.
Koo, Terry, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL-08: HLT.
McDonald, Ryan and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In EMNLP-CoNLL.
McDonald, Ryan and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In EACL.
McDonald, Ryan, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In ACL.
Melcuk, Igor A. 1987. Dependency syntax: theory
and practice. State University of New York Press.
Mohri, Mehryar and Brian Roark. 2006. Probabilistic
context-free grammar induction based on structural
zeros. In HLT-NAACL.
Petrov, Slav and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Poon, Hoifung and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP.
Roark, Brian and Kristy Hollingshead. 2008. Classi-
fying chart cells for quadratic complexity context-
free inference. In COLING.
Sgaard, Anders and Jonas Kuhn. 2009. Using a max-
imum entropy-based tagger to improve a very fast
vine parser. In IWPT.
Toutanova, Kristina, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL.
Wang, Qin Iris, Colin Cherry, Dan Lizotte, and Dale
Schuurmans. 2006. Improved large margin depen-
dency parsing via local constraints and Laplacian
regularization. In CoNLL.
Wang, Mengqiu, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? A quasi-
synchronous grammar for QA. In EMNLP-CoNLL.
Wang, Qin Iris, Dale Schuurmans, and Dekang Lin.
2008. Semi-supervised convex training for depen-
dency parsing. In ACL-08: HLT.
West, D. 2001. Introduction to Graph Theory. Pren-
tice Hall, 2nd edition.
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In IWPT.
</reference>
<page confidence="0.973801">
61
</page>
<figure confidence="0.258591">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.454110">
<note confidence="0.818052">b&amp;apos;Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 5361, Beijing, August 2010</note>
<title confidence="0.995743">Fast and Accurate Arc Filtering for Dependency Parsing</title>
<author confidence="0.999963">Shane Bergsma</author>
<affiliation confidence="0.9998595">Department of Computing Science University of Alberta</affiliation>
<email confidence="0.986429">sbergsma@ualberta.ca</email>
<author confidence="0.894653">Colin Cherry</author>
<affiliation confidence="0.8806675">Institute for Information Technology National Research Council Canada</affiliation>
<email confidence="0.994447">colin.cherry@nrc-cnrc.gc.ca</email>
<abstract confidence="0.997943666666667">We propose a series of learned arc filters to speed up graph-based dependency parsing. A cascade of filters identify implausible head-modifier pairs, with time complexity that is first linear, and then quadratic in the length of the sentence. The linear filters reliably predict, in context, words that are roots or leaves of dependency trees, and words that are likely to have heads on their left or right. We use this information to quickly prune arcs from the dependency graph. More than 78% of total arcs are pruned while retaining 99.5% of the true dependencies. These filters improve the speed of two state-ofthe-art dependency parsers, with low overhead and negligible loss in accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="9131" citStr="Carreras et al. (2008)" startWordPosition="1460" endWordPosition="1463"> coarse model prunes the search space for later, more expensive models (Charniak et al., 2006; Petrov and Klein, 2007). This approach assumes a common forest or chart representation, shared by all granularities, where one can efficiently track the pruning decisions of the coarse models. One could imagine applying such a solution to dependency parsing, but the exact implementation of the coarse pass would vary according to the choice in search algorithm. Our filters are much more modular: they apply to both 1st-order spanning tree parsing and 2nd-order projective parsing, with no modification. Carreras et al. (2008) use coarse-to-fine pruning with dependency parsing, but in that case, a graphbased dependency parser provides the coarse pass, with the fine pass being a far-more-expensive treeadjoining grammar. Our filters could become a 0th pass, further increasing the efficiency of their approach. 4 Arc Filters We propose arc filtering as a preprocessing step for dependency parsing. An arc filter removes im2 Johnsons (2007) split-head CFG could implement this idea directly with little effort. plausible head-modifier arcs from the complete dependency graph (which initially includes all head-modifier arcs).</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Carreras, Xavier, Michael Collins, and Terry Koo. 2008. TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higherorder projective dependency parser.</title>
<date>2007</date>
<booktitle>In EMNLPCoNLL.</booktitle>
<contexts>
<context position="5131" citStr="Carreras, 2007" startWordPosition="808" endWordPosition="809"> pairs [h, m]. This enables search by either minimum spanning tree (West, 2001) or by Eisners (1996) projective parser. This paper focuses on the projective case, though our techniques transfer to spanning tree parsing. With a linear scoring function, the parser solves: parse(s) = argmaxts X [h,m]t w f(h, m, s) The weights w are typically learned using an online method, such as an averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). 2nd-order searches, which consider two siblings at a time, are available with no increase in asymptotic complexity (McDonald and Pereira, 2006; Carreras, 2007). The complexity of graph-based parsing is bounded by two processes: parsing (carrying out the argmax) and arc scoring (calculating w f(h, m, s)). For a sentence with n words, projective parsing takes O(n3) time, while the spanning tree algorithm is O(n2). Both parsers require scores for arcs connecting each possible [h, m] pair in s; therefore, the cost of arc scoring is also O(n2), and may become O(n3) if the features include words in s between h and m (Galley and Manning, 2009). Arc scoring also has a significant constant term: the number of features extracted for an [h, m] pair. Our in-hou</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Carreras, Xavier. 2007. Experiments with a higherorder projective dependency parser. In EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene \x0cCharniak</author>
<author>Mark Johnson</author>
<author>Micha Elsner</author>
<author>Joseph Austerweil</author>
<author>David Ellis</author>
<author>Isaac Haxton</author>
<author>Catherine Hill</author>
<author>R Shrivaths</author>
<author>Jeremy Moore</author>
<author>Michael Pozar</author>
<author>Theresa Vu</author>
</authors>
<title>Multilevel coarse-to-fine PCFG parsing.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<marker>\x0cCharniak, Johnson, Elsner, Austerweil, Ellis, Haxton, Hill, Shrivaths, Moore, Pozar, Vu, 2006</marker>
<rawString>\x0cCharniak, Eugene, Mark Johnson, Micha Elsner, Joseph Austerweil, David Ellis, Isaac Haxton, Catherine Hill, R. Shrivaths, Jeremy Moore, Michael Pozar, and Theresa Vu. 2006. Multilevel coarse-to-fine PCFG parsing. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="4935" citStr="Collins, 2002" startWordPosition="778" endWordPosition="779">coring tree according to a scoring function that decomposes under an exhaustive search (McDonald et al., 2005). The most natural decomposition scores individual arcs, represented as head-modifier pairs [h, m]. This enables search by either minimum spanning tree (West, 2001) or by Eisners (1996) projective parser. This paper focuses on the projective case, though our techniques transfer to spanning tree parsing. With a linear scoring function, the parser solves: parse(s) = argmaxts X [h,m]t w f(h, m, s) The weights w are typically learned using an online method, such as an averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). 2nd-order searches, which consider two siblings at a time, are available with no increase in asymptotic complexity (McDonald and Pereira, 2006; Carreras, 2007). The complexity of graph-based parsing is bounded by two processes: parsing (carrying out the argmax) and arc scoring (calculating w f(h, m, s)). For a sentence with n words, projective parsing takes O(n3) time, while the spanning tree algorithm is O(n2). Both parsers require scores for arcs connecting each possible [h, m] pair in s; therefore, the cost of arc scoring is also O(n2), and may become O(</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, Michael. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>JMLR,</journal>
<pages>3--951991</pages>
<contexts>
<context position="4970" citStr="Crammer and Singer, 2003" startWordPosition="782" endWordPosition="785"> a scoring function that decomposes under an exhaustive search (McDonald et al., 2005). The most natural decomposition scores individual arcs, represented as head-modifier pairs [h, m]. This enables search by either minimum spanning tree (West, 2001) or by Eisners (1996) projective parser. This paper focuses on the projective case, though our techniques transfer to spanning tree parsing. With a linear scoring function, the parser solves: parse(s) = argmaxts X [h,m]t w f(h, m, s) The weights w are typically learned using an online method, such as an averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). 2nd-order searches, which consider two siblings at a time, are available with no increase in asymptotic complexity (McDonald and Pereira, 2006; Carreras, 2007). The complexity of graph-based parsing is bounded by two processes: parsing (carrying out the argmax) and arc scoring (calculating w f(h, m, s)). For a sentence with n words, projective parsing takes O(n3) time, while the spanning tree algorithm is O(n2). Both parsers require scores for arcs connecting each possible [h, m] pair in s; therefore, the cost of arc scoring is also O(n2), and may become O(n3) if the features include words i</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Crammer, Koby and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. JMLR, 3:951991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Vine parsing and minimum risk reranking for speed and precision.</title>
<date>2006</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="6608" citStr="Dreyer et al., 2006" startWordPosition="1061" endWordPosition="1064">c scoring, coupled with the parsing stages low grammar constant, means that graph-based parsers spend much of their time scoring potential arcs. Johnson (2007) reports that when arc scores have been precomputed, the dynamic programming component of his 1st-order parser can process an amazing 3,580 sentences per second.1 Beyond reducing the number of features, the easiest way to reduce the computational burden of arc scoring is to score only plausible arcs. 3 Related Work 3.1 Vine Parsing Filtering dependency arcs has been explored primarily in the form of vine parsing (Eisner and Smith, 2005; Dreyer et al., 2006). Vine parsing establishes that, since most dependencies are short, one can parse quickly by placing a hard constraint on arc length. As this coarse filter quickly degrades the best achievable performance, Eisner and Smith (2005) also consider conditioning the constraint on the part-of-speech (PoS) tags being linked and the direction of the arc, resulting in a separate threshold for each [tag(h), tag(m), dir(h, m)] triple. They sketch an algorithm where the thresholded length for each triple starts at the highest value seen in the training data. Thresholds are then decreased in a greedy fashio</context>
</contexts>
<marker>Dreyer, Smith, Smith, 2006</marker>
<rawString>Dreyer, Markus, David A. Smith, and Noah A. Smith. 2006. Vine parsing and minimum risk reranking for speed and precision. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Noah A Smith</author>
</authors>
<title>Parsing with soft and hard constraints on dependency length.</title>
<date>2005</date>
<booktitle>In IWPT.</booktitle>
<contexts>
<context position="6586" citStr="Eisner and Smith, 2005" startWordPosition="1057" endWordPosition="1060">r 4. The high cost of arc scoring, coupled with the parsing stages low grammar constant, means that graph-based parsers spend much of their time scoring potential arcs. Johnson (2007) reports that when arc scores have been precomputed, the dynamic programming component of his 1st-order parser can process an amazing 3,580 sentences per second.1 Beyond reducing the number of features, the easiest way to reduce the computational burden of arc scoring is to score only plausible arcs. 3 Related Work 3.1 Vine Parsing Filtering dependency arcs has been explored primarily in the form of vine parsing (Eisner and Smith, 2005; Dreyer et al., 2006). Vine parsing establishes that, since most dependencies are short, one can parse quickly by placing a hard constraint on arc length. As this coarse filter quickly degrades the best achievable performance, Eisner and Smith (2005) also consider conditioning the constraint on the part-of-speech (PoS) tags being linked and the direction of the arc, resulting in a separate threshold for each [tag(h), tag(m), dir(h, m)] triple. They sketch an algorithm where the thresholded length for each triple starts at the highest value seen in the training data. Thresholds are then decrea</context>
</contexts>
<marker>Eisner, Smith, 2005</marker>
<rawString>Eisner, Jason and Noah A. Smith. 2005. Parsing with soft and hard constraints on dependency length. In IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In COLING.</booktitle>
<marker>Eisner, 1996</marker>
<rawString>Eisner, Jason. 1996. Three new probabilistic models for dependency parsing: An exploration. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>JMLR,</journal>
<pages>9--18711874</pages>
<contexts>
<context position="12285" citStr="Fan et al., 2008" startWordPosition="1985" endWordPosition="1988"> Roark and Hollingshead (2008). 55 \x0cnot a h , . ; |CC PRP$ PRP EX -RRB- -LRBno m EX LS POS PRP$ no m . RP not a root , DT no hm DT{DT,JJ,NN,NNP,NNS,.} CDCD NN{DT,NNP} NNP{DT,NN,NNS} no mh {DT,IN,JJ,NN,NNP}DT NNPIN INJJ Table 1: Learned rules for filtering dependency arcs using PoS tags. The rules filter 25% of possible arcs while recovering 99.9% of true links. training example, w is the learned weight vector, and C1 and C2 are the class-specific costs. High precision is obtained when C2 &amp;gt;&amp;gt; C1. For an SVM, l(w, yi, xi) is the standard hinge loss. We solve the SVM objective using LIBLINEAR (Fan et al., 2008). In our experiments, each filter is a linear SVM with the typical L1 loss and L2 regularization.4 We search for the best combination of C1 and C2 using a grid search on development data. At test time, an arc is filtered if w x &amp;gt; 0. 4.2 Rule-Based Filtering Our rule-based filters seek to instantly remove those arcs that are trivially implausible on the basis of their head and modifier PoS tags. We first extract labeled examples from gold-standard trees for whenever a) a word is not a head, b) a word does not have a head on the left (resp. right), and c) a pair of words is not linked. We then t</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. JMLR, 9:18711874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Quadratic-time dependency parsing for machine translation.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="1455" citStr="Galley and Manning, 2009" startWordPosition="217" endWordPosition="220">ckly prune arcs from the dependency graph. More than 78% of total arcs are pruned while retaining 99.5% of the true dependencies. These filters improve the speed of two state-ofthe-art dependency parsers, with low overhead and negligible loss in accuracy. 1 Introduction Dependency parsing finds direct syntactic relationships between words by connecting headmodifier pairs into a tree structure. Dependency information is useful for a wealth of natural language processing tasks, including question answering (Wang et al., 2007), semantic parsing (Poon and Domingos, 2009), and machine translation (Galley and Manning, 2009). We propose and test a series of arc filters for graph-based dependency parsers, which rule out potential head-modifier pairs before parsing begins. In doing so, we hope to eliminate implausible links early, saving the costs associated with them, and speeding up parsing. In addition to the scaling benefits that come with faster processing, we hope to enable richer features for parsing by constraining the set of arcs that need to be considered. This could allow extremely large feature sets (Koo et al., 2008), or the look-up of expensive corpus-based features such as word-pair mutual informatio</context>
<context position="5616" citStr="Galley and Manning, 2009" startWordPosition="892" endWordPosition="895"> which consider two siblings at a time, are available with no increase in asymptotic complexity (McDonald and Pereira, 2006; Carreras, 2007). The complexity of graph-based parsing is bounded by two processes: parsing (carrying out the argmax) and arc scoring (calculating w f(h, m, s)). For a sentence with n words, projective parsing takes O(n3) time, while the spanning tree algorithm is O(n2). Both parsers require scores for arcs connecting each possible [h, m] pair in s; therefore, the cost of arc scoring is also O(n2), and may become O(n3) if the features include words in s between h and m (Galley and Manning, 2009). Arc scoring also has a significant constant term: the number of features extracted for an [h, m] pair. Our in-house graphbased parser collects on average 62 features for each potential arc, a number larger than the length of most sentences. With the cluster-based features suggested by Koo et al. (2008), this could easily grow by a factor of 3 or 4. The high cost of arc scoring, coupled with the parsing stages low grammar constant, means that graph-based parsers spend much of their time scoring potential arcs. Johnson (2007) reports that when arc scores have been precomputed, the dynamic prog</context>
<context position="18702" citStr="Galley and Manning (2009)" startWordPosition="3082" endWordPosition="3086">ing as a light preprocessing step, using only a portion of the resources that might be used in the final scoring function. Features Quadratic filtering uses both binary and realvalued features (Table 3). Real-valued features promote a smaller feature space. For example, one value can encode distance rather than separate features for different distances. We also generalize the between-tag features used in McDonald et al. (2005) to be the count of each tag between the head and modifier. The count may be more informative than tag presence alone, particularly for high-precision filters. We follow Galley and Manning (2009) in using only between-tags within a fixed range of the head or modifier, so that the extraction for each pair is O(1) and the overall feature extraction is O(n2). Using only a subset of the between-tags as features has been shown to improve speed but impair parser performance (Galley and Manning, 2009). By filtering quickly first, then scoring all remaining arcs with a cubic scoring function in the parser, we hope to get the best of both worlds. 57 \x0c5 Filter Experiments Data We extract dependency structures from the Penn Treebank using the Penn2Malt extraction tool,5 which implements the h</context>
</contexts>
<marker>Galley, Manning, 2009</marker>
<rawString>Galley, Michel and Christopher D. Manning. 2009. Quadratic-time dependency parsing for machine translation. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>A support vector method for multivariate performance measures.</title>
<date>2005</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="11594" citStr="Joachims, 2005" startWordPosition="1861" endWordPosition="1862">s the classifier to high precision, increasing the cost for misclassifying a true arc during learning.3 Class-specific costs are command-line parameters for many learning packages. One can interpret the learning objective as minimizing regularized, weighted loss: min w 1 2 ||w||2 + C1 X i:yi=1 l(w, yi, xi) +C2 X i:yi=1 l(w, yi, xi) (1) where l() is the learning methods loss function, xi and yi are the features and label for the ith 3 Learning with a cost model is generally preferable to first optimizing error rate and then thresholding the prediction values to select a high-confidence subset (Joachims, 2005), but the latter approach was used successfully for cell classification in Roark and Hollingshead (2008). 55 \x0cnot a h , . ; |CC PRP$ PRP EX -RRB- -LRBno m EX LS POS PRP$ no m . RP not a root , DT no hm DT{DT,JJ,NN,NNP,NNS,.} CDCD NN{DT,NNP} NNP{DT,NN,NNS} no mh {DT,IN,JJ,NN,NNP}DT NNPIN INJJ Table 1: Learned rules for filtering dependency arcs using PoS tags. The rules filter 25% of possible arcs while recovering 99.9% of true links. training example, w is the learned weight vector, and C1 and C2 are the class-specific costs. High precision is obtained when C2 &amp;gt;&amp;gt; C1. For an SVM, l(w, yi, xi</context>
</contexts>
<marker>Joachims, 2005</marker>
<rawString>Joachims, Thorsten. 2005. A support vector method for multivariate performance measures. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Transforming projective bilexical dependency grammars into efficiently-parsable CFGs with unfold-fold.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6147" citStr="Johnson (2007)" startWordPosition="986" endWordPosition="987">O(n3) if the features include words in s between h and m (Galley and Manning, 2009). Arc scoring also has a significant constant term: the number of features extracted for an [h, m] pair. Our in-house graphbased parser collects on average 62 features for each potential arc, a number larger than the length of most sentences. With the cluster-based features suggested by Koo et al. (2008), this could easily grow by a factor of 3 or 4. The high cost of arc scoring, coupled with the parsing stages low grammar constant, means that graph-based parsers spend much of their time scoring potential arcs. Johnson (2007) reports that when arc scores have been precomputed, the dynamic programming component of his 1st-order parser can process an amazing 3,580 sentences per second.1 Beyond reducing the number of features, the easiest way to reduce the computational burden of arc scoring is to score only plausible arcs. 3 Related Work 3.1 Vine Parsing Filtering dependency arcs has been explored primarily in the form of vine parsing (Eisner and Smith, 2005; Dreyer et al., 2006). Vine parsing establishes that, since most dependencies are short, one can parse quickly by placing a hard constraint on arc length. As th</context>
<context position="25716" citStr="Johnson (2007)" startWordPosition="4235" endWordPosition="4236"> might also have the parser re-use features extracted during filtering when scoring the remaining arcs. We did not do this. Instead, filtering was treated as a preprocessing step, which maximizes the portability of the filters across parsers. We test on two stateof-the art parsers: MST We modified the publicly-available MST parser (McDonald et al., 2005)6 to employ our filters before carrying out feature extraction. MST is trained with 5-best MIRA. DepPercep We also test an in-house dependency parser, which conducts projective first and 2nd-order searches using the split-head CFG described by Johnson (2007), with a weight vector trained using an averaged perceptron (Collins, 6 http://sourceforge.net/projects/mstparser/ 59 \x0cDepPercep-1 DepPercep-2 MST-1 MST-2 Filter Cost Acc. Time Acc. Time Acc. Time Acc. Time None +0 91.8 348 92.5 832 91.2 153 91.9 200 Vine +3 91.7 192 92.3 407 91.2 99 91.8 139 Rules +1 91.7 264 92.4 609 91.2 125 91.9 167 Linear +7 91.7 168 92.4 334 91.2 88 91.8 121 Quad. +16 91.7 79 92.3 125 91.2 58 91.8 80 Table 7: The effect of filtering on the speed and accuracy on 1st and 2nd-order dependency parsing. 2002). Its features are a mixture of those described by McDonald et al</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Johnson, Mark. 2007. Transforming projective bilexical dependency grammars into efficiently-parsable CFGs with unfold-fold. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In ACL-08: HLT.</booktitle>
<contexts>
<context position="1968" citStr="Koo et al., 2008" startWordPosition="304" endWordPosition="307">., 2007), semantic parsing (Poon and Domingos, 2009), and machine translation (Galley and Manning, 2009). We propose and test a series of arc filters for graph-based dependency parsers, which rule out potential head-modifier pairs before parsing begins. In doing so, we hope to eliminate implausible links early, saving the costs associated with them, and speeding up parsing. In addition to the scaling benefits that come with faster processing, we hope to enable richer features for parsing by constraining the set of arcs that need to be considered. This could allow extremely large feature sets (Koo et al., 2008), or the look-up of expensive corpus-based features such as word-pair mutual information (Wang et al., 2006). These filters could also facilitate expensive learning algorithms, such as semi-supervised approaches (Wang et al., 2008). We propose three levels of filtering, which are applied in a sequence of increasing complexity: Rules: A simple set of machine-learned rules based only on parts-of-speech. They prune over 25% of potential arcs with almost no loss in coverage. Rules save on the wasted effort for assessing implausible arcs such as DT DT. Linear: A series of classifiers that tag words</context>
<context position="5921" citStr="Koo et al. (2008)" startWordPosition="945" endWordPosition="948">s, projective parsing takes O(n3) time, while the spanning tree algorithm is O(n2). Both parsers require scores for arcs connecting each possible [h, m] pair in s; therefore, the cost of arc scoring is also O(n2), and may become O(n3) if the features include words in s between h and m (Galley and Manning, 2009). Arc scoring also has a significant constant term: the number of features extracted for an [h, m] pair. Our in-house graphbased parser collects on average 62 features for each potential arc, a number larger than the length of most sentences. With the cluster-based features suggested by Koo et al. (2008), this could easily grow by a factor of 3 or 4. The high cost of arc scoring, coupled with the parsing stages low grammar constant, means that graph-based parsers spend much of their time scoring potential arcs. Johnson (2007) reports that when arc scores have been precomputed, the dynamic programming component of his 1st-order parser can process an amazing 3,580 sentences per second.1 Beyond reducing the number of features, the easiest way to reduce the computational burden of arc scoring is to score only plausible arcs. 3 Related Work 3.1 Vine Parsing Filtering dependency arcs has been explo</context>
<context position="26365" citStr="Koo et al. (2008)" startWordPosition="4352" endWordPosition="4355">sing an averaged perceptron (Collins, 6 http://sourceforge.net/projects/mstparser/ 59 \x0cDepPercep-1 DepPercep-2 MST-1 MST-2 Filter Cost Acc. Time Acc. Time Acc. Time Acc. Time None +0 91.8 348 92.5 832 91.2 153 91.9 200 Vine +3 91.7 192 92.3 407 91.2 99 91.8 139 Rules +1 91.7 264 92.4 609 91.2 125 91.9 167 Linear +7 91.7 168 92.4 334 91.2 88 91.8 121 Quad. +16 91.7 79 92.3 125 91.2 58 91.8 80 Table 7: The effect of filtering on the speed and accuracy on 1st and 2nd-order dependency parsing. 2002). Its features are a mixture of those described by McDonald et al. (2005), and those used in the Koo et al. (2008) baseline system; we do not use word-cluster features. DepPercep makes some small improvements to MSTs 1st-order feature set. We carefully determined which feature types should have distance appended in addition to direction. Also, inspired by the reported utility of mixing PoS tags and word-clusters (Koo et al., 2008), we created versions of all of the Between and Surrounding Word features described by McDonald et al. (2005) where we mix tags and words.7 DepPercep was developed with quadratic filters in place, which enabled a fast development cycle for feature engineering. As a result, it doe</context>
<context position="28312" citStr="Koo et al. (2008)" startWordPosition="4676" endWordPosition="4679">ilable data-driven parsers. Under quadratic filtering, MST-2 goes from processing 7 This was enabled by using word features only when the word is among the 800 most frequent in the training set. 12 sentences per second to 23 sentences.8 DepPercep-2 starts slow, but benefits greatly from filtering. This is because, unlike MST-2, it does not optimize feature extraction by factoring its ten 2nd-order features into two triple ([h, m1, m2]) and eight sibling ([m1, m2]) features. This suggests that filtering could have a dramatic effect on a parser that uses more than a few triple features, such as Koo et al. (2008). 7 Conclusion We have presented a series of arc filters that speed up graph-based dependency parsing. By treating filtering as weighted classification, we learn a cascade of increasingly complex filters from treeannotated data. Linear-time filters prune 54% of total arcs, while quadratic-time filters prune 78%. Both retain at least 99.5% of true dependencies. By testing two state-of-the-art dependency parsers, we have shown that our filters produce substantial speed improvements in even carefullyoptimized parsers, with negligible losses in accuracy. In the future we hope to leverage this redu</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Koo, Terry, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="4172" citStr="McDonald and Nivre, 2007" startWordPosition="655" endWordPosition="658"> into money funds Figure 1: An example dependency parse. 2 Dependency Parsing A dependency tree represents the syntactic structure of a sentence as a directed graph (Figure 1), with a node for each word, and arcs indicating head-modifier pairs (Melcuk, 1987). Though dependencies can be extracted from many formalisms, there is a growing interest in predicting dependency trees directly. To that end, there are two dominant approaches: graph-based methods, characterized by arc features in an exhaustive search, and transition-based methods, characterized by operational features in a greedy search (McDonald and Nivre, 2007). We focus on graph-based parsing, as its exhaustive search has the most to gain from our filters. Graph-based dependency parsing finds the highest-scoring tree according to a scoring function that decomposes under an exhaustive search (McDonald et al., 2005). The most natural decomposition scores individual arcs, represented as head-modifier pairs [h, m]. This enables search by either minimum spanning tree (West, 2001) or by Eisners (1996) projective parser. This paper focuses on the projective case, though our techniques transfer to spanning tree parsing. With a linear scoring function, the </context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>McDonald, Ryan and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="5114" citStr="McDonald and Pereira, 2006" startWordPosition="804" endWordPosition="807">represented as head-modifier pairs [h, m]. This enables search by either minimum spanning tree (West, 2001) or by Eisners (1996) projective parser. This paper focuses on the projective case, though our techniques transfer to spanning tree parsing. With a linear scoring function, the parser solves: parse(s) = argmaxts X [h,m]t w f(h, m, s) The weights w are typically learned using an online method, such as an averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). 2nd-order searches, which consider two siblings at a time, are available with no increase in asymptotic complexity (McDonald and Pereira, 2006; Carreras, 2007). The complexity of graph-based parsing is bounded by two processes: parsing (carrying out the argmax) and arc scoring (calculating w f(h, m, s)). For a sentence with n words, projective parsing takes O(n3) time, while the spanning tree algorithm is O(n2). Both parsers require scores for arcs connecting each possible [h, m] pair in s; therefore, the cost of arc scoring is also O(n2), and may become O(n3) if the features include words in s between h and m (Galley and Manning, 2009). Arc scoring also has a significant constant term: the number of features extracted for an [h, m]</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>McDonald, Ryan and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4431" citStr="McDonald et al., 2005" startWordPosition="695" endWordPosition="698">ough dependencies can be extracted from many formalisms, there is a growing interest in predicting dependency trees directly. To that end, there are two dominant approaches: graph-based methods, characterized by arc features in an exhaustive search, and transition-based methods, characterized by operational features in a greedy search (McDonald and Nivre, 2007). We focus on graph-based parsing, as its exhaustive search has the most to gain from our filters. Graph-based dependency parsing finds the highest-scoring tree according to a scoring function that decomposes under an exhaustive search (McDonald et al., 2005). The most natural decomposition scores individual arcs, represented as head-modifier pairs [h, m]. This enables search by either minimum spanning tree (West, 2001) or by Eisners (1996) projective parser. This paper focuses on the projective case, though our techniques transfer to spanning tree parsing. With a linear scoring function, the parser solves: parse(s) = argmaxts X [h,m]t w f(h, m, s) The weights w are typically learned using an online method, such as an averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). 2nd-order searches, which consider two siblings at a time, </context>
<context position="18507" citStr="McDonald et al. (2005)" startWordPosition="3049" endWordPosition="3052"> m. While theoretically of the same complexity as the parsers arc-scoring function (O(n2)), this process can nevertheless save time by employing a compact feature set. We view quadratic filtering as a light preprocessing step, using only a portion of the resources that might be used in the final scoring function. Features Quadratic filtering uses both binary and realvalued features (Table 3). Real-valued features promote a smaller feature space. For example, one value can encode distance rather than separate features for different distances. We also generalize the between-tag features used in McDonald et al. (2005) to be the count of each tag between the head and modifier. The count may be more informative than tag presence alone, particularly for high-precision filters. We follow Galley and Manning (2009) in using only between-tags within a fixed range of the head or modifier, so that the extraction for each pair is O(1) and the overall feature extraction is O(n2). Using only a subset of the between-tags as features has been shown to improve speed but impair parser performance (Galley and Manning, 2009). By filtering quickly first, then scoring all remaining arcs with a cubic scoring function in the pa</context>
<context position="25458" citStr="McDonald et al., 2005" startWordPosition="4192" endWordPosition="4195">ach filtered arc, and replace its score with an extremely low negative value. Note that 2nd-order features consider O(n3) [h, m1, m2] triples. These triples are filtered if at least one component arc ([h, m1] or [h, m2]) is filtered. In an optimal implementation, we might also have the parser re-use features extracted during filtering when scoring the remaining arcs. We did not do this. Instead, filtering was treated as a preprocessing step, which maximizes the portability of the filters across parsers. We test on two stateof-the art parsers: MST We modified the publicly-available MST parser (McDonald et al., 2005)6 to employ our filters before carrying out feature extraction. MST is trained with 5-best MIRA. DepPercep We also test an in-house dependency parser, which conducts projective first and 2nd-order searches using the split-head CFG described by Johnson (2007), with a weight vector trained using an averaged perceptron (Collins, 6 http://sourceforge.net/projects/mstparser/ 59 \x0cDepPercep-1 DepPercep-2 MST-1 MST-2 Filter Cost Acc. Time Acc. Time Acc. Time Acc. Time None +0 91.8 348 92.5 832 91.2 153 91.9 200 Vine +3 91.7 192 92.3 407 91.2 99 91.8 139 Rules +1 91.7 264 92.4 609 91.2 125 91.9 167 </context>
<context position="26794" citStr="McDonald et al. (2005)" startWordPosition="4422" endWordPosition="4425">tering on the speed and accuracy on 1st and 2nd-order dependency parsing. 2002). Its features are a mixture of those described by McDonald et al. (2005), and those used in the Koo et al. (2008) baseline system; we do not use word-cluster features. DepPercep makes some small improvements to MSTs 1st-order feature set. We carefully determined which feature types should have distance appended in addition to direction. Also, inspired by the reported utility of mixing PoS tags and word-clusters (Koo et al., 2008), we created versions of all of the Between and Surrounding Word features described by McDonald et al. (2005) where we mix tags and words.7 DepPercep was developed with quadratic filters in place, which enabled a fast development cycle for feature engineering. As a result, it does not implement many of the optimizations in place in MST, and is relatively slow unfiltered. 6.2 Results The parsing results are shown in Table 7, where times are given in seconds, and Cost indicates the additional cost of filtering. Note that the impact of all filters on accuracy is negligible, with a decrease of at most 0.2%. In general, parsing speedups mirror the amount of arc reduction measured in our filter analysis (S</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>McDonald, Ryan, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Melcuk</author>
</authors>
<title>Dependency syntax: theory and practice.</title>
<date>1987</date>
<publisher>Press.</publisher>
<institution>State University of New York</institution>
<contexts>
<context position="3805" citStr="Melcuk, 1987" startWordPosition="601" endWordPosition="602">lectively, we show that more than 78% of total arcs can be pruned while retaining 99.5% of the true dependencies. We test the impact of these filters at both train and test time, using two stateof-the-art discriminative parsers, demonstrating speed-ups of between 1.9 and 5.6, with little impact on parsing accuracy. 53 \x0cInvestors continue to pour cash into money funds Figure 1: An example dependency parse. 2 Dependency Parsing A dependency tree represents the syntactic structure of a sentence as a directed graph (Figure 1), with a node for each word, and arcs indicating head-modifier pairs (Melcuk, 1987). Though dependencies can be extracted from many formalisms, there is a growing interest in predicting dependency trees directly. To that end, there are two dominant approaches: graph-based methods, characterized by arc features in an exhaustive search, and transition-based methods, characterized by operational features in a greedy search (McDonald and Nivre, 2007). We focus on graph-based parsing, as its exhaustive search has the most to gain from our filters. Graph-based dependency parsing finds the highest-scoring tree according to a scoring function that decomposes under an exhaustive sear</context>
</contexts>
<marker>Melcuk, 1987</marker>
<rawString>Melcuk, Igor A. 1987. Dependency syntax: theory and practice. State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>Probabilistic context-free grammar induction based on structural zeros.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="13791" citStr="Mohri and Roark, 2006" startWordPosition="2254" endWordPosition="2257">e as head are filtered. The classier selects a small number of high4 We also tried L1-regularized filters. L1 encourages most features to have zero weight, leading to more compact and hence faster models. We found the L1 filters to prune fewer arcs at a given coverage level, providing less speed-up at parsing time. Both L1 and L2 models are available in our publicly available implementation. precision rules, shown in Table 1. Note that the rules tend to use common tags with well-defined roles. By focusing on weighted loss as opposed to arc frequency, the classifier discovers structural zeros (Mohri and Roark, 2006), events which could have been observed, but were not. We consider this an improvement over the frequencybased length thresholds employed previously in tag-specific vine parsing. 4.3 Linear-Time Filtering In the linear filtering stage, we filter arcs on the basis of single nodes and their contexts, passing through the sentences in linear time. For each node, eight separate classifiers decide whether: 1. It is not a head (i.e., it is a leaf of the tree). 2. Its head is on the left/right. 3. Its head is within 5 nodes on the left/right. 4. Its head is immediately on the left/right. 5. It is the </context>
</contexts>
<marker>Mohri, Roark, 2006</marker>
<rawString>Mohri, Mehryar and Brian Roark. 2006. Probabilistic context-free grammar induction based on structural zeros. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="8627" citStr="Petrov and Klein, 2007" startWordPosition="1378" endWordPosition="1381"> constituents, they show a significant improvement in speed with no reduction in accuracy. It is difficult to port their idea directly to dependency parsing without committing to a particular search algorithm,2 and thereby sacrificing some of the graph-based formalisms modularity. However, some of our linear filters (see Section 4.3) were inspired by their constraints. 3.3 Coarse-to-fine Parsing Another common method employed to speed up exhaustive parsers is a coarse-to-fine approach, where a cheap, coarse model prunes the search space for later, more expensive models (Charniak et al., 2006; Petrov and Klein, 2007). This approach assumes a common forest or chart representation, shared by all granularities, where one can efficiently track the pruning decisions of the coarse models. One could imagine applying such a solution to dependency parsing, but the exact implementation of the coarse pass would vary according to the choice in search algorithm. Our filters are much more modular: they apply to both 1st-order spanning tree parsing and 2nd-order projective parsing, with no modification. Carreras et al. (2008) use coarse-to-fine pruning with dependency parsing, but in that case, a graphbased dependency p</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Petrov, Slav and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1403" citStr="Poon and Domingos, 2009" startWordPosition="210" endWordPosition="213">their left or right. We use this information to quickly prune arcs from the dependency graph. More than 78% of total arcs are pruned while retaining 99.5% of the true dependencies. These filters improve the speed of two state-ofthe-art dependency parsers, with low overhead and negligible loss in accuracy. 1 Introduction Dependency parsing finds direct syntactic relationships between words by connecting headmodifier pairs into a tree structure. Dependency information is useful for a wealth of natural language processing tasks, including question answering (Wang et al., 2007), semantic parsing (Poon and Domingos, 2009), and machine translation (Galley and Manning, 2009). We propose and test a series of arc filters for graph-based dependency parsers, which rule out potential head-modifier pairs before parsing begins. In doing so, we hope to eliminate implausible links early, saving the costs associated with them, and speeding up parsing. In addition to the scaling benefits that come with faster processing, we hope to enable richer features for parsing by constraining the set of arcs that need to be considered. This could allow extremely large feature sets (Koo et al., 2008), or the look-up of expensive corpu</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Poon, Hoifung and Pedro Domingos. 2009. Unsupervised semantic parsing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Classifying chart cells for quadratic complexity contextfree inference.</title>
<date>2008</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="2846" citStr="Roark and Hollingshead (2008)" startWordPosition="443" endWordPosition="446"> levels of filtering, which are applied in a sequence of increasing complexity: Rules: A simple set of machine-learned rules based only on parts-of-speech. They prune over 25% of potential arcs with almost no loss in coverage. Rules save on the wasted effort for assessing implausible arcs such as DT DT. Linear: A series of classifiers that tag words according to their possible roles in the dependency tree. By treating each word independently and ensuring constant-time feature extraction, they operate in linear time. We view these as a dependencyparsing analogue to the span-pruning proposed by Roark and Hollingshead (2008). Our fast linear filters prune 54.2% of potential arcs while recovering 99.7% of true pairs. Quadratic: A final stage that looks at pairs of words to prune unlikely arcs from the dependency tree. By employing a light-weight feature set, this high-precision filter can enable more expensive processing on the remaining plausible dependencies. Collectively, we show that more than 78% of total arcs can be pruned while retaining 99.5% of the true dependencies. We test the impact of these filters at both train and test time, using two stateof-the-art discriminative parsers, demonstrating speed-ups o</context>
<context position="7756" citStr="Roark and Hollingshead (2008)" startWordPosition="1243" endWordPosition="1246">value seen in the training data. Thresholds are then decreased in a greedy fashion, with each step producing the smallest possible reduction in reachable training arcs. We employ this algorithm as a baseline in our experiments. To our knowledge, vine parsing 1 To calibrate this speed, consider that the publicly available 1st -order MST parser processes 16 sentences per second on modern hardware. This includes I/O costs in addition to the costs of arc scoring and parsing. 54 \x0chas not previously been tested with a state-of-theart, discriminative dependency parser. 3.2 CFG Cell Classification Roark and Hollingshead (2008) speed up another exhaustive parsing algorithm, the CKY parser for CFGs, by classifying each word in the sentence according to whether it can open (or close) a multi-word constituent. With a high-precision tagger that errs on the side of permitting constituents, they show a significant improvement in speed with no reduction in accuracy. It is difficult to port their idea directly to dependency parsing without committing to a particular search algorithm,2 and thereby sacrificing some of the graph-based formalisms modularity. However, some of our linear filters (see Section 4.3) were inspired by</context>
<context position="11698" citStr="Roark and Hollingshead (2008)" startWordPosition="1874" endWordPosition="1877">ng learning.3 Class-specific costs are command-line parameters for many learning packages. One can interpret the learning objective as minimizing regularized, weighted loss: min w 1 2 ||w||2 + C1 X i:yi=1 l(w, yi, xi) +C2 X i:yi=1 l(w, yi, xi) (1) where l() is the learning methods loss function, xi and yi are the features and label for the ith 3 Learning with a cost model is generally preferable to first optimizing error rate and then thresholding the prediction values to select a high-confidence subset (Joachims, 2005), but the latter approach was used successfully for cell classification in Roark and Hollingshead (2008). 55 \x0cnot a h , . ; |CC PRP$ PRP EX -RRB- -LRBno m EX LS POS PRP$ no m . RP not a root , DT no hm DT{DT,JJ,NN,NNP,NNS,.} CDCD NN{DT,NNP} NNP{DT,NN,NNS} no mh {DT,IN,JJ,NN,NNP}DT NNPIN INJJ Table 1: Learned rules for filtering dependency arcs using PoS tags. The rules filter 25% of possible arcs while recovering 99.9% of true links. training example, w is the learned weight vector, and C1 and C2 are the class-specific costs. High precision is obtained when C2 &amp;gt;&amp;gt; C1. For an SVM, l(w, yi, xi) is the standard hinge loss. We solve the SVM objective using LIBLINEAR (Fan et al., 2008). In our expe</context>
</contexts>
<marker>Roark, Hollingshead, 2008</marker>
<rawString>Roark, Brian and Kristy Hollingshead. 2008. Classifying chart cells for quadratic complexity contextfree inference. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Sgaard</author>
<author>Jonas Kuhn</author>
</authors>
<title>Using a maximum entropy-based tagger to improve a very fast vine parser.</title>
<date>2009</date>
<booktitle>In IWPT.</booktitle>
<contexts>
<context position="15096" citStr="Sgaard and Kuhn (2009)" startWordPosition="2496" endWordPosition="2499"> and filter directly based on the classifier output. If a word is not a head, all arcs with the given word as head can be pruned. If a word is deemed to have a head within a certain range on the left or right, then all arcs that do not obey this constraint can be pruned. If a root is found, no other words should link to the artificial root node. Furthermore, in a projective dependency tree, no arc will cross the root, i.e., there will be no arcs where a head and a modifier lie on either side of the root. We can therefore also filter arcs that violate this constraint when parsing projectively. Sgaard and Kuhn (2009) previously proposed a tagger to further constrain a vine parser. Their tags are a subset of our decisions (items 4 and 5 above), and have not yet been tested in a state-ofthe-art system. Development experiments show that if we could perfectly make decisions 1-5 for each word, we could remove 91.7% of the total arcs or 95% of negative arcs, close to the upper bound. Features Unlike rule-based filtering, linear filtering uses a rich set of features (Table 2). Each feature is a 56 \x0cPoS-tag features Other features tagi wordi tagi, tagi1 wordi+1 tagi, tagi+1 wordi1 tagi1, tagi+1 shapei tagi2, t</context>
</contexts>
<marker>Sgaard, Kuhn, 2009</marker>
<rawString>Sgaard, Anders and Jonas Kuhn. 2009. Using a maximum entropy-based tagger to improve a very fast vine parser. In IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="19547" citStr="Toutanova et al., 2003" startWordPosition="3226" endWordPosition="3229">en shown to improve speed but impair parser performance (Galley and Manning, 2009). By filtering quickly first, then scoring all remaining arcs with a cubic scoring function in the parser, we hope to get the best of both worlds. 57 \x0c5 Filter Experiments Data We extract dependency structures from the Penn Treebank using the Penn2Malt extraction tool,5 which implements the head rules of Yamada and Matsumoto (2003). Following convention, we divide the Treebank into train (sections 221), development (22) and test sets (23). The development and test sets are re-tagged using the Stanford tagger (Toutanova et al., 2003). Evaluation Metrics To measure intrinsic filter quality, we define Reduction as the proportion of total arcs removed, and Coverage as the proportion of true head-modifier arcs retained. Our evaluation asks, for each filter, what Reduction can be obtained at a given Coverage level? We also give Time: how long it takes to apply the filters to the test set (excluding initialization). We compute an Upper Bound for Reduction on development data. There are 1.2 million potential dependency links in those sentences, 96.5% of which are not present in a gold standard dependency tree. Therefore, the max</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Toutanova, Kristina, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Iris Wang</author>
<author>Colin Cherry</author>
<author>Dan Lizotte</author>
<author>Dale Schuurmans</author>
</authors>
<title>Improved large margin dependency parsing via local constraints and Laplacian regularization.</title>
<date>2006</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="2076" citStr="Wang et al., 2006" startWordPosition="320" endWordPosition="323">e propose and test a series of arc filters for graph-based dependency parsers, which rule out potential head-modifier pairs before parsing begins. In doing so, we hope to eliminate implausible links early, saving the costs associated with them, and speeding up parsing. In addition to the scaling benefits that come with faster processing, we hope to enable richer features for parsing by constraining the set of arcs that need to be considered. This could allow extremely large feature sets (Koo et al., 2008), or the look-up of expensive corpus-based features such as word-pair mutual information (Wang et al., 2006). These filters could also facilitate expensive learning algorithms, such as semi-supervised approaches (Wang et al., 2008). We propose three levels of filtering, which are applied in a sequence of increasing complexity: Rules: A simple set of machine-learned rules based only on parts-of-speech. They prune over 25% of potential arcs with almost no loss in coverage. Rules save on the wasted effort for assessing implausible arcs such as DT DT. Linear: A series of classifiers that tag words according to their possible roles in the dependency tree. By treating each word independently and ensuring </context>
</contexts>
<marker>Wang, Cherry, Lizotte, Schuurmans, 2006</marker>
<rawString>Wang, Qin Iris, Colin Cherry, Dan Lizotte, and Dale Schuurmans. 2006. Improved large margin dependency parsing via local constraints and Laplacian regularization. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Noah A Smith</author>
<author>Teruko Mitamura</author>
</authors>
<title>What is the Jeopardy model? A quasisynchronous grammar for QA. In EMNLP-CoNLL.</title>
<date>2007</date>
<contexts>
<context position="1359" citStr="Wang et al., 2007" startWordPosition="203" endWordPosition="206">ords that are likely to have heads on their left or right. We use this information to quickly prune arcs from the dependency graph. More than 78% of total arcs are pruned while retaining 99.5% of the true dependencies. These filters improve the speed of two state-ofthe-art dependency parsers, with low overhead and negligible loss in accuracy. 1 Introduction Dependency parsing finds direct syntactic relationships between words by connecting headmodifier pairs into a tree structure. Dependency information is useful for a wealth of natural language processing tasks, including question answering (Wang et al., 2007), semantic parsing (Poon and Domingos, 2009), and machine translation (Galley and Manning, 2009). We propose and test a series of arc filters for graph-based dependency parsers, which rule out potential head-modifier pairs before parsing begins. In doing so, we hope to eliminate implausible links early, saving the costs associated with them, and speeding up parsing. In addition to the scaling benefits that come with faster processing, we hope to enable richer features for parsing by constraining the set of arcs that need to be considered. This could allow extremely large feature sets (Koo et a</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>Wang, Mengqiu, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy model? A quasisynchronous grammar for QA. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Iris Wang</author>
<author>Dale Schuurmans</author>
<author>Dekang Lin</author>
</authors>
<title>Semi-supervised convex training for dependency parsing.</title>
<date>2008</date>
<booktitle>In ACL-08: HLT.</booktitle>
<contexts>
<context position="2199" citStr="Wang et al., 2008" startWordPosition="337" endWordPosition="340"> before parsing begins. In doing so, we hope to eliminate implausible links early, saving the costs associated with them, and speeding up parsing. In addition to the scaling benefits that come with faster processing, we hope to enable richer features for parsing by constraining the set of arcs that need to be considered. This could allow extremely large feature sets (Koo et al., 2008), or the look-up of expensive corpus-based features such as word-pair mutual information (Wang et al., 2006). These filters could also facilitate expensive learning algorithms, such as semi-supervised approaches (Wang et al., 2008). We propose three levels of filtering, which are applied in a sequence of increasing complexity: Rules: A simple set of machine-learned rules based only on parts-of-speech. They prune over 25% of potential arcs with almost no loss in coverage. Rules save on the wasted effort for assessing implausible arcs such as DT DT. Linear: A series of classifiers that tag words according to their possible roles in the dependency tree. By treating each word independently and ensuring constant-time feature extraction, they operate in linear time. We view these as a dependencyparsing analogue to the span-pr</context>
</contexts>
<marker>Wang, Schuurmans, Lin, 2008</marker>
<rawString>Wang, Qin Iris, Dale Schuurmans, and Dekang Lin. 2008. Semi-supervised convex training for dependency parsing. In ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D West</author>
</authors>
<title>Introduction to Graph Theory.</title>
<date>2001</date>
<publisher>Prentice Hall,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="4595" citStr="West, 2001" startWordPosition="721" endWordPosition="722">raph-based methods, characterized by arc features in an exhaustive search, and transition-based methods, characterized by operational features in a greedy search (McDonald and Nivre, 2007). We focus on graph-based parsing, as its exhaustive search has the most to gain from our filters. Graph-based dependency parsing finds the highest-scoring tree according to a scoring function that decomposes under an exhaustive search (McDonald et al., 2005). The most natural decomposition scores individual arcs, represented as head-modifier pairs [h, m]. This enables search by either minimum spanning tree (West, 2001) or by Eisners (1996) projective parser. This paper focuses on the projective case, though our techniques transfer to spanning tree parsing. With a linear scoring function, the parser solves: parse(s) = argmaxts X [h,m]t w f(h, m, s) The weights w are typically learned using an online method, such as an averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). 2nd-order searches, which consider two siblings at a time, are available with no increase in asymptotic complexity (McDonald and Pereira, 2006; Carreras, 2007). The complexity of graph-based parsing is bounded by two proces</context>
</contexts>
<marker>West, 2001</marker>
<rawString>West, D. 2001. Introduction to Graph Theory. Prentice Hall, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In IWPT.</booktitle>
<contexts>
<context position="19342" citStr="Yamada and Matsumoto (2003)" startWordPosition="3193" endWordPosition="3196">y between-tags within a fixed range of the head or modifier, so that the extraction for each pair is O(1) and the overall feature extraction is O(n2). Using only a subset of the between-tags as features has been shown to improve speed but impair parser performance (Galley and Manning, 2009). By filtering quickly first, then scoring all remaining arcs with a cubic scoring function in the parser, we hope to get the best of both worlds. 57 \x0c5 Filter Experiments Data We extract dependency structures from the Penn Treebank using the Penn2Malt extraction tool,5 which implements the head rules of Yamada and Matsumoto (2003). Following convention, we divide the Treebank into train (sections 221), development (22) and test sets (23). The development and test sets are re-tagged using the Stanford tagger (Toutanova et al., 2003). Evaluation Metrics To measure intrinsic filter quality, we define Reduction as the proportion of total arcs removed, and Coverage as the proportion of true head-modifier arcs retained. Our evaluation asks, for each filter, what Reduction can be obtained at a given Coverage level? We also give Time: how long it takes to apply the filters to the test set (excluding initialization). We compute</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In IWPT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>