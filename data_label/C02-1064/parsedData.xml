<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<figure confidence="0.76916725">
b&amp;apos;Text Generation from Keywords
Kiyotaka Uchimoto
Satoshi Sekine
Hitoshi Isahara
</figure>
<affiliation confidence="0.400778">
Communications Research Laboratory
</affiliation>
<address confidence="0.565884">
2-2-2, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
</address>
<email confidence="0.952826">
{uchimoto,isahara}@crl.go.jp
</email>
<affiliation confidence="0.683657">
New York University
</affiliation>
<address confidence="0.9883615">
715 Broadway, 7th floor
New York, NY 10003, USA
</address>
<email confidence="0.997483">
sekine@cs.nyu.edu
</email>
<sectionHeader confidence="0.990826" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997093058823529">
We describe a method for generating sentences
from keywords or headwords. This method
consists of two main parts, candidate-text con-
struction and evaluation. The construction part
generates text sentences in the form of depen-
dency trees by using complementary informa-
tion to replace information that is missing be-
cause of a knowledge gap and other missing
function words to generate natural text sen-
tences based on a particular monolingual cor-
pus. The evaluation part consists of a model
for generating an appropriate text when given
keywords. This model considers not only word
n-gram information, but also dependency infor-
mation between words. Furthermore, it consid-
ers both string information and morphological
information.
</bodyText>
<sectionHeader confidence="0.997285" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.961941">
Text generation is an important technique used
for applications like machine translation, sum-
marization, and human/computer dialogue. In
recent years, many corpora have become avail-
able, and have been used to generate natural
surface sentences. For example, corpora have
been used to generate sentences for language
model estimation in statistical machine trans-
lation. In such translation, given a source lan-
guage text, S, the translated text, T, in the
target language that maximizes the probabil-
ity P(T|S) is selected as the most appropri-
ate translation, Tbest, which is represented as
(Brown et al., 1990)
</bodyText>
<equation confidence="0.9981055">
Tbest = argmaxT P(T |S)
= argmaxT (P(S|T ) P(T )) . (1)
</equation>
<bodyText confidence="0.999538448275862">
In this equation, P(S|T) represents the model
used to replace words or phrases in a source lan-
guage with those in the target language. It is
called a translation model. P(T) represents a
language model that is used to reorder trans-
lated words or phrases into a natural order in
the target language. The input of the language
model is a bag of words, and the goal of the
model is basically to reorder the words. At this
point, there is an assumption that natural sen-
tences can be generated by merely reordering
the words given by a translation model. To give
such a complete set of words, however, a trans-
lation model needs a large number of bilingual
corpora. If we could automatically complement
the words needed to generate natural sentences,
we would not have to collect the large number
of bilingual corpora required by a translation
model. In this paper, we assume that the role of
the translation model is not to give a complete
set of words that can be used to generate nat-
ural sentences, but to give a set of headwords
or center words that a speaker might want to
express, and describe a model that can provide
the complementary information needed to gen-
erate natural sentences by using a target lan-
guage corpus when given a set of headwords.
If we denote a set of headwords in a target
language as K, we can express Eq. (1) as
</bodyText>
<equation confidence="0.999418">
P(T |S) = P(K|S) P(T |K). (2)
</equation>
<bodyText confidence="0.994194166666667">
P(K|S) in this equation represents a model
that gives a set of headwords in the target lan-
guage when given a source-language text sen-
tence. P(T|K) represents a model that gener-
ates text sentence T when given a set of head-
words, K. We call the model represented by
P(T|K) a text-generation model. In this paper,
we describe a text-generation model and a gen-
eration system that uses the model. Given a set
of headwords or keywords, our system outputs
the text sentence that maximizes P(T|K) as an
appropriate text sentence, Tbest:
</bodyText>
<equation confidence="0.9972815">
Tbest = argmaxT P(T |K)
= argmaxT (P(K|T ) P(T )) . (3)
</equation>
<bodyText confidence="0.991283357142857">
In this equation, we call the model represented
by P(K|T) a keyword-production model. This
equation is equal to Eq. (1) when a source-
text sentence is replaced with a set of key-
\x0cwords. Therefore, this model can be regarded
as a model that translates keywords into text
sentences. The model represented by P(T) in
Eq. (3) is a language model used in statistical
machine translation. The n-gram model is the
most popular one used as a language model.
We assume that there is one extremely proba-
ble ordered set of morphemes and dependencies
between words that produce keywords, and we
express P(K|T) as
</bodyText>
<equation confidence="0.999124">
P(K|T) P(K, M, D|T)
= P(K|M, D, T) P(D|M, T) P(M|T). (4)
</equation>
<bodyText confidence="0.999068042553191">
In this equation, M denotes an ordered set of
morphemes and D denotes an ordered set of de-
pendencies in a sentence. P(K|M, D, T) rep-
resents a keyword-production model. To es-
timate the models represented by P(D|M, T)
and P(M|T), we use a dependency model and
a morpheme model, respectively, for the depen-
dency analysis and morphological analysis.
Statistical machine translation and example-
based machine translation require numerous
high-quality bilingual corpora. Interlingual ma-
chine translation and transfer-based machine
translation require a parser with high precision.
Therefore, these approaches to translation are
not practical if we do not have enough bilingual
corpora or a good parser. This is especially so if
the source text-sentences are incomplete or have
errors like those often found in OCR and speech-
recognition output. In these cases, however, if
we translate headwords into words in the target
language and generate sentences from the trans-
lated words by using our method, we should be
able to generate natural sentences from which
we can grasp the meaning of the source-text sen-
tences.
The text-generation model represented by
P(T|K) in Eq. (2) can be applied to various
tasks besides machine translation.
Sentence-generation support system
for people with aphasia: About 300,000
people are reported to suffer from aphasia
in Japan, and 40% of them can select only
a few words to describe a picture. If candi-
date sentences can be generated from these
few words, it would help these people com-
municate with their families and friends.
Support system for second language
writing: Beginners writing in second lan-
guage usually fined it easy to produce cen-
ter words or headwords, but often have dif-
ficulty generating complete sentences. If
several possible sentences could be gener-
ated from those words, it would help begin-
ners communicate with foreigners or study
second-language writing.
These are just two examples. We believe that
there are many other possible applications.
</bodyText>
<sectionHeader confidence="0.780511" genericHeader="method">
2 Overview of the Text-Generation
</sectionHeader>
<subsectionHeader confidence="0.497785">
System
</subsectionHeader>
<bodyText confidence="0.986688">
In this section, we give an overview of our sys-
tem for generating text sentences from given
keywords. As shown in Fig. 1, this system con-
sists of three parts: generation-rule acquisition,
</bodyText>
<figureCaption confidence="0.808523">
candidate-text sentence construction, and eval-
uation.
Figure 1: Overview of the text-generation sys-
</figureCaption>
<bodyText confidence="0.643317666666667">
tem.
Given keywords, text sentences are generated
as follows.
</bodyText>
<listItem confidence="0.9021018">
1. During generation-rule acquisition, genera-
tion rules for each keyword are automati-
cally acquired.
2. Candidate-text sentences are constructed
during candidate-text construction by ap-
plying the rules acquired in the first
step. Each candidate-text sentence is rep-
resented by a graph or dependency tree.
3. Candidate-text sentences are ranked ac-
cording to their scores assigned during eval-
uation. The scores are calculated as a
probability estimated by using a keyword-
production model and a language model
that are trained with a corpus.
4. The candidate-text sentence that maxi-
</listItem>
<bodyText confidence="0.972116">
mizes the score or the candidate-text sen-
tences whose scores are over a threshold
are selected as output. The system can
also output candidate-text sentences that
are ranked within the top N sentences.
In this paper, we assume that the target lan-
guage is Japanese. We define a keyword as the
headword of a bunsetsu. A bunsetsu is a phrasal
unit that usually consists of several content and
function words. We define the headword of a
bunsetsu as the rightmost content word in the
bunsetsu, and we define a content word as a
\x0cword whose part-of-speech is a verb, adjective,
noun, demonstrative, adverb, conjunction, at-
tribute, interjection, or undefined word. We
define the other words as function words. We
define formal nouns and auxiliary verbs SURU
(do) and NARU (become) as function words,
except when there are no other content words
in the same bunsetsu. Part-of-speech categories
follow those in the Kyoto University text corpus
(Version 3.0) (Kurohashi and Nagao, 1997), a
tagged corpus of the Mainichi newspaper.
</bodyText>
<figureCaption confidence="0.988738">
Figure 2: Example of text generated from key-
</figureCaption>
<bodyText confidence="0.958216">
words.
For example, given the set of keywords
kanojo (she), ie (house), and iku (go), as
shown in Fig. 2, our system retrieves sentences
including each word, and extracts each bunsetsu
that includes each word as a headword of the
bunsetsu. If there is no tagged corpus such
as the Kyoto University text corpus, each bun-
setsu can be extracted by using a morphological-
analysis system and a dependency-analysis sys-
tem such as JUMAN (Kurohashi and Nagao,
1999) and KNP (Kurohashi, 1998). Our system
then acquires generation rules as follows.
</bodyText>
<construct confidence="0.4951964">
kanojo (she)kanojo (she) no (of)
kanojo (she)kanojo (she) ga
ie (house)ie (house) ni (to)
iku (go)iku (go)
iku (go)itta (went)
</construct>
<bodyText confidence="0.999361153846154">
The system next generates candidate bunsetsus
for each keyword and candidate-text sentences
in the form of dependency trees, such as Can-
didate 1 and Candidate 2 in Fig. 2, with
the assumption that there are dependencies be-
tween keywords. Finally, the candidate-text
sentences are ranked by their scores, calculated
by a text-generation model, and transformed
into surface sentences.
In this paper, we focus on the keyword-
production model represented by Eq. (4) and
assume that our system outputs sentences in the
form of dependency trees.
</bodyText>
<sectionHeader confidence="0.997697" genericHeader="method">
3 Candidate-Text Construction
</sectionHeader>
<bodyText confidence="0.9963962">
We automatically acquire generation rules from
a monolingual target corpus at the time of gen-
erating candidate-text sentences. Generation
rules are restricted to those that generate bun-
setsus, and the generated bunsetsus must in-
clude each input keyword as a headword in the
bunsetsu. We then generate candidate-text sen-
tences in the form of dependency trees by simply
combining the bunsetsus generated by the rules.
The simple combination of generated bunsetsus
may produce semantically or grammatically in-
appropriate candidate-text sentences, but our
goal in this work was to generate a variety of
text sentences rather than a few fixed expres-
sions with high precision 1.
</bodyText>
<subsectionHeader confidence="0.974687">
3.1 Generation-Rule Acquisition
</subsectionHeader>
<bodyText confidence="0.9988685">
Let us denote a set of keywords as KS and a
set of rules, each of which generates a bunsetsu
when given keyword k(KS), as Rk. We then
restrict rk(Rk) to those represented as
</bodyText>
<equation confidence="0.845009">
k hkm
. (5)
</equation>
<bodyText confidence="0.998765363636364">
In this rule, hk represents the head morpheme
whose word is equal to keyword k; m repre-
sents zero, one, or a series of morphemes that
are connected to hk in the same bunsetsu. Here,
we define a morpheme as consisting of a word
and its morphological information or grammat-
ical attribute, such as part-of-speech, and we
define a head morpheme as consisting of a head-
word and its grammatical attribute. By apply-
ing these rules, we generate bunsetsus from in-
put keywords.
</bodyText>
<subsectionHeader confidence="0.999932">
3.2 Construction of Dependency Trees
</subsectionHeader>
<bodyText confidence="0.998997625">
Given keywords K = k1k2 . . . kn, candidate bun-
setsus are generated by applying the generation
rules described in Section 3.1. Next, by as-
suming dependency relationships between the
bunsetsus, candidate dependency trees are con-
structed. Dependencies between the bunsetsus
are restricted in that they must have the follow-
ing characteristics of Japanese dependencies:
</bodyText>
<page confidence="0.892581">
1
</page>
<bodyText confidence="0.983501285714286">
Note that 83.33% (3,973/4,768) of the headwords in
the newspaper articles appearing on January 17, 1995
were found in those appearing from January 1st to 16th.
However, only 21.82% (2,295/10,517) of the headword
dependencies in the newspaper articles appearing on
January 17th were found in those appearing from Jan-
uary 1st to 16th.
</bodyText>
<listItem confidence="0.80754975">
\x0c(i) Dependencies are directed from left to
right.
(ii) Dependencies do not cross.
(iii) All bunsetsus except the rightmost one de-
</listItem>
<bodyText confidence="0.969159375">
pend on only one other bunsetsu.
For example, when three keywords are given
and candidate bunsetsus including each keyword
are generated as b1, b2, and b3, the candidate de-
pendency trees are (b1 (b2 b3)) and ((b1 b2) b3)
if we do not reorder keywords, but 16 trees re-
sult if we consider the order of keywords to be
arbitrary.
</bodyText>
<sectionHeader confidence="0.99931" genericHeader="method">
4 Text-Generation Model
</sectionHeader>
<bodyText confidence="0.9966762">
We next describe the model represented by Eq.
(4); that is, a keyword-production model, a
morpheme model that estimates how likely a
string is to be a morpheme, and a dependency
model. The goal of this model is to select
optimal sets of morphemes and dependencies
that can generate natural sentences. We imple-
mented these models within an maximum en-
tropy framework (Berger et al., 1996; Ristad,
1997; Ristad, 1998).
</bodyText>
<subsectionHeader confidence="0.913457">
4.1 Keyword-Production Models
</subsectionHeader>
<bodyText confidence="0.964047615384615">
This section describes five keyword-production
models which are represented by P(K|M, D, T)
in Eq. (4). In these models, we define the set of
headwords whose frequency in the corpus is over
a certain threshold as a set of keywords, KS,
and we restrict the bunsetsus to those generated
by the generation rules represented in form (5).
We assume that all keywords are independent
and that ki corresponds to word wj(1 j m)
when text is given as a series of words w1 . . . wm.
1. trigram model
We assume that ki depends only on the two
anterior words wj1 and wj2.
</bodyText>
<equation confidence="0.9996082">
P(K|M, D, T ) =
n
\x01
i=1
P(ki|wj1, wj2).(6)
</equation>
<bodyText confidence="0.892627">
2. posterior trigram model
We assume that ki depends only on the two
posterior words wj+1 and wj+2.
</bodyText>
<equation confidence="0.999482">
P(K|M, D, T ) =
n
\x01
i=1
P(ki|wj+1, wj+2).(7)
</equation>
<bodyText confidence="0.9137442">
3. dependency bigram model
We assume that ki depends only on the two
rightmost words wl and wl1 in the right-
most bunsetsu that modifies the bunsetsu
including ki (see Fig. 3).
</bodyText>
<equation confidence="0.9733198">
P(K|M, D, T ) =
n
\x01
i=1
P(ki|wl, wl1). (8)
</equation>
<figureCaption confidence="0.707777666666667">
Figure 3: Relationship between keywords and
words in bunsetsus.
4. posterior dependency bigram model
</figureCaption>
<bodyText confidence="0.99757475">
We assume that ki depends only on the
headword, ws, and the word on its right,
ws+1, in the bunsetsu that is modified by
the bunsetsu including ki (see Fig. 3).
</bodyText>
<equation confidence="0.9514436">
P(K|M, D, T ) =
n
\x01
i=1
P(ki|ws, ws+1). (9)
</equation>
<bodyText confidence="0.970327285714286">
5. dependency trigram model
We assume that ki depends only on the two
rightmost words wl and wl1 in the right-
most bunsetsu that modifies the bunsetsu,
and on the two rightmost words wh and
wh1 in the leftmost bunsetsu that modi-
fies the bunsetsu including ki (see Fig. 3).
</bodyText>
<equation confidence="0.9928784">
P(K|M, D, T) =
n
\x01
i=1
P(ki|wl, wl1, wh, wh1). (10)
</equation>
<subsectionHeader confidence="0.686869">
4.2 Morpheme Model
</subsectionHeader>
<bodyText confidence="0.995493">
Let us assume that there are l grammatical
attributes assigned to morphemes. We call a
model that estimates the likelihood that a given
string is a morpheme and has the grammatical
attribute j(1 j l) a morpheme model.
Let us also assume that morphemes in the or-
dered set of morphemes M depend on the pre-
ceding morphemes. We can then represent the
probability of M, given text T; namely, P(M|T)
in Eq. (4):
</bodyText>
<equation confidence="0.999687">
P(M|T ) =
n
\x01
i=1
P(mi|mi1
1 , T ), (11)
</equation>
<bodyText confidence="0.998276">
where mi can be one of the grammatical at-
tributes assigned to each morpheme.
</bodyText>
<subsectionHeader confidence="0.996291">
4.3 Dependency Model
</subsectionHeader>
<bodyText confidence="0.88673375">
Let us assume that dependencies di(1 i n)
in the ordered set of dependencies D are inde-
pendent. We can then represent P(D|M, T) in
Eq. (4) as
</bodyText>
<equation confidence="0.9994704">
P(D|M, T ) =
n
\x01
i=1
P(di|M, T ). (12)
</equation>
<sectionHeader confidence="0.84004" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.97090425">
To evaluate our system we made 30 sets of
keywords, with three keywords in each set, as
shown in Table 1. A human subject selected
the sets from headwords that were found ten
</bodyText>
<equation confidence="0.929971555555556">
\x0cTable 1: Input keywords and examples of sys-
tem output.
Input (Keywords) Ex. of system output
( ( ))
(( ) )
(( ) )
( ( ))
(( ) )
(( ) )
( ( ))
(( ) )
(( ) )
(( ) )
( ( ))
(( ) )
( ( ))
( ( ))
(( ) )
(( ) )
(( ) )
(( ) )
(( ) )
(( ) )
(( ) )
( ( ))
( ( ))
(( ) )
</equation>
<bodyText confidence="0.98442264">
times or more in the newspaper articles on Jan-
uary 1st in the Kyoto University text corpus
(Version 3.0) without looking at the articles.
We evaluated each model by the percentage
of outputs that were subjectively judged as ap-
propriate by one of the authors. We used two
evaluation standards.
Standard 1: If the dependency tree ranked
first is semantically and grammatically ap-
propriate, it is judged as appropriate.
Standard 2: If there is at least one depen-
dency tree that is ranked within the top
ten and is semantically and grammatically
appropriate, it is judged as appropriate.
We used headwords that were found five times
or more in the newspaper articles appearing
from January 1st to 16th in the Kyoto Univer-
sity text corpus and also found in those appear-
ing on January 1st as the set of headwords, KS.
For headwords that were not in KS, we added
their major part-of-speech categories to the set.
We trained our keyword-production models by
using 1,129 sentences (containing 10,201 head-
words) from newspaper articles appearing on
January 1st. We used a morpheme model and a
dependency model identical to those proposed
by Uchimoto et al. (Uchimoto et al., 2001; Uchi-
moto et al., 1999; Uchimoto et al., 2000b). To
train the models, we used 8,835 sentences from
newspaper articles appearing from January 1st
to 9th in 1995. Generation rules were acquired
from newspaper articles appearing from Jan-
uary 1st to 16th. The total number of sentences
was 18,435.
First, we evaluated the outputs generated
when the rightmost two keywords, such as
and , on each line of Table 1 were input.
Table 2 shows the results. KM1 through KM5
stand for the five keyword-production models
described in Section 4.1, and MM and DM stand
for the morpheme and the dependency models,
respectively. The symbol + indicates a combi-
nation of models. In the models without MM,
DM, or both, P(M|T) and P(D|M, T) were as-
sumed to be 1. We carried out additional ex-
periments with models that considered both the
anterior and posterior words, such as the com-
bination of KM1 and KM2 or KM3 and KM4.
The results were at most 16/30 by standard 1
and 24/30 by standard 1.
</bodyText>
<tableCaption confidence="0.996403">
Table 2: Results of subjective evaluation.
</tableCaption>
<table confidence="0.973405217391304">
Model Standard 1 Standard 2
KM1 (trigram) 13/30 28/30
KM1 + MM 21/30 28/30
KM1 + DM 12/30 28/30
KM1 + MM + DM 26/30 28/30
KM2 (posterior trigram) 6/30 15/30
KM2 + MM 8/30 20/30
KM2 + DM 10/30 20/30
KM2 + MM + DM 9/30 25/30
KM3 (dependency bigram) 13/30 29/30
KM3 + MM 26/30 29/30
KM3 + DM 14/30 28/30
KM3 + MM + DM 27/30 29/30
KM4 (posterior dependency bigram) 10/30 18/30
KM4 + MM 9/30 26/30
KM4 + DM 9/30 22/30
KM4 + MM + DM 13/30 27/30
KM5 (dependency trigram) 12/30 26/30
KM5 + MM 17/30 28/30
KM5 + DM 12/30 27/30
KM5 + MM + DM 26/30 28/30
The models KM1+MM+DM,
KM3+MM+DM, and KM5+MM+DM
</table>
<bodyText confidence="0.998741543859649">
achieved the best results, as shown in Ta-
ble 2. For models KM1, KM3, and KM5, the
results with MM and DM were significantly
better than those without MM and DM in
the evaluation by standard 1. We believe this
was because cases are more tightly connected
with verbs than with nouns, so models KM1,
KM3, and KM5, which learn the connection
between cases and verbs, can better rank the
candidate-text sentences that have a natural
connection between cases and verbs than other
candidates.
Next, we conducted experiments using the
30 sets of keywords shown in Table 1 as in-
puts. We used two keyword-production mod-
els: model KM3+MM+DM, which achieved
\x0cthe best results in the first experiment, and
model KM5+MM+DM, which considers the
richest information. We assumed that the in-
put keyword order was appropriate and did not
reorder the keywords. The results for both
models were the same: 19/30 in the evalu-
ation by standard 1 and 24/30 in the eval-
uation by standard 2. The right column of
Table 1 shows examples of the system out-
put. For example, for the input (syourai,
in the future), (shin-shin-tou, the New
Frontier Party), and (umareru, to
be born), the dependency tree (
[syourai wa] ( [shin-shin-tou ga]
[umareru darou])) (The New
Frontier Party will be born in the future.)
was generated. This output was automati-
cally complemented by the appropriate modal-
ity (darou, will), which agrees with
the word (syourai, in the future), as
well as by post-positional particles such as
(wa, case marker) and (ga). For
the input (gaikoku-jin, a foreigner),
(kanyuu, to join), and (zouka, to in-
crease), the dependency tree ((
[gaikokujin no] [kanyuu sya ga])
[zouka shite iru] ) (Foreigner
members are increasing in number.) was
generated. This output was complemented
not only by the modality expression
(shite iru, the progressive form) and
post-positional particles such as (no, of)
and (ga), but also by the suffix
(sya, person), and a compound noun
(kanyuu sya, member) was generated naturally.
In six cases, though, we did not obtain appro-
priate outputs because the candidate-text sen-
tences were not appropriately ranked. Improv-
ing the back-off ability of the model by using
classified words or synonyms as features should
enable us to rank sentences more appropriately.
</bodyText>
<sectionHeader confidence="0.999455" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999386577464789">
Many statistical generation methods have been
proposed. In this section, we describe the differ-
ences between our method and several previous
methods.
Japanese words are often followed by post-
positional particles, such as ga and wo,
to indicate the subject and object of a sen-
tence. There are no corresponding words in
English. Instead, English words are preceded
by articles, the and a, to distinguish def-
inite and indefinite nouns, and so on, and in
this case there are no corresponding words in
Japanese. Knight et al. proposed a way to
compensate for missing information caused by
a lack of language-dependent knowledge, or a
knowledge gap (Knight and Hatzivassiloglou,
1995; Langkilde and Knight, 1998a; Langkilde
and Knight, 1998b). They use semantic expres-
sions as input, whereas we use keywords. Also,
they construct candidate-text sentences or word
lattices by applying rules, and apply their lan-
guage model, an n-gram model, to select the
most appropriate surface text. While we can-
not use their rules to generate candidate-text
sentences when given keywords, we can apply
their language model to our system to generate
surface-text sentences from candidate-text sen-
tences in the form of dependency trees. We can
also apply the formalism proposed by Langkilde
(Langkilde, 2000) to express the candidate-text
sentences.
Bangalore and Rambow proposed a method
to generate candidate-text sentences in the form
of trees (Bangalore and Rambow, 2000). They
consider dependency information when deriving
trees by using XTAG grammar, but they as-
sume that the input contains dependency infor-
mation. Our system generates candidate-text
sentences without relying on dependency infor-
mation in the input, and our model estimates
the dependencies between keywords.
Ratnaparkhi proposed models to generate
text from semantic attributes (Ratnaparkhi,
2000). The input of these models is semantic
attributes. His models are similar to ours if the
semantic attributes are replaced with keywords.
However, his models need a training corpus in
which certain words are replaced with seman-
tic attributes. Although our model also needs
a training corpus, the corpus can be automati-
cally created by using a morphological analyzer
and a dependency analyzer, both of which are
readily available.
Humphreys et al. proposed using mod-
els developed for sentence-structure analysis to
rank candidate-text sentences (Humphreys et
al., 2001). As well as models developed for
sentence-structure analysis, we also use those
developed for morphological analysis and found
that these models contribute to the generation
of appropriate text.
Berger and Lafferty proposed a language
model for information retrieval (Berger and Laf-
ferty, 1999). Their concept is similar to that of
our model, which can be regarded as a model
that translates keywords into text, while their
model can be regarded as one that translates
query words into documents. However, the pur-
pose of their model is different: their goal is to
\x0cretrieve text that already exists while ours is to
generate new text.
</bodyText>
<sectionHeader confidence="0.99721" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.986315384615384">
We have described a method for generating sen-
tences from keywords or headwords. This
method consists of two main parts, candidate-
text construction and evaluation.
1. The construction part generates text sen-
tences in the form of dependency trees by
providing complementary information to
replace that missing due to a knowledge
gap and other missing function words, and
thus generates natural text sentences based
on a particular monolingual corpus.
2. The evaluation part consists of a model
for generating an appropriate text sentence
when given keywords. This model consid-
ers the dependency information between
words as well as word n-gram informa-
tion. Furthermore, the model considers
both string and morphological information.
If a language model, such as a word n-gram
model, is applied to the generated-text sen-
tences in the form of dependency trees, an
appropriate surface-text sentence is generated.
The word-order model proposed by Uchimoto et
al. can also generate surface text in a natural
order (Uchimoto et al., 2000a).
There are several possible directions for our
future research. In particular,
We would like to expand the generation
rules. We restricted the generation rules
automatically acquired from a corpus to
those that generate a bunsetsu. To gener-
ate a greater variety of candidate-text sen-
tences, we would like to expand the rules
that can generate a dependency tree. Ex-
pansion would lead to complementing with
content words as well as function words.
We also would like to prepare default rules
or to classify words into several classes
when no sentences including the keywords
are found in the target corpus.
Some of the N-best text sentences gener-
ated by our system are semantically and
grammatically unnatural. To remove such
sentences from among the candidate-text
sentences, we must enhance our model so
that it can consider more information, such
as classified words or those in a thesaurus.
We restricted keywords to the headwords or
rightmost content words in the bunsetsus.
We would like to expand the definition of
keywords to other content words and to
synonyms of the keywords.
</bodyText>
<sectionHeader confidence="0.978367" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.978139">
We thank the Mainichi Newspapers for permis-
sion to use their data. We also thank Kimiko
</bodyText>
<reference confidence="0.9069412">
Ohta, Hiroko Inui, Takehito Utsuro, Man-
abu Okumura, Akira Ushioda, Junichi Tsujii,
Kiyosi Yasuda, and Masahisa Ohta for their
beneficial comments during the progress of this
work.
</reference>
<sectionHeader confidence="0.588161" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997869561403509">
S. Bangalore and O. Rambow. 2000. Exploiting a Probabilis-
tic Hierarchical Model for Generation. In Proceedings of
the COLING, pages 4248.
A. Berger and J. Lafferty. 1999. Information Retrieval as
Statistical Translation. In Proceedings of the ACM SIGIR,
pages 222229.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language Pro-
cessing. Computational Linguistics, 22(1):3971.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra,
F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.
1990. A Statistical Approach to Machine Translation.
Computational Linguistics, 16(2):7985.
K. Humphreys, M. Calcagno, and D. Weise. 2001. Reusing a
Statistical Language Model for Generation. In Proceedings
of the EWNLG.
K. Knight and V. Hatzivassiloglou. 1995. Two-Level, Many-
Paths Generation. In Proceedings of the ACL, pages 252
260.
S. Kurohashi and M. Nagao. 1997. Building a Japanese
Parsed Corpus while Improving the Parsing System. In
Proceedings of the NLPRS, pages 451456.
S. Kurohashi and M. Nagao, 1999. Japanese Morphological
Analysis System JUMAN Version 3.61. Department of
Informatics, Kyoto University.
S. Kurohashi, 1998. Japanese Dependency/Case Structure
Analyzer KNP Version 2.0b6. Department of Informatics,
Kyoto University.
I. Langkilde and K. Knight. 1998a. Generation that Exploits
Corpus-Based Statistical Knowledge. In Proceedings of the
COLING-ACL, pages 704710.
I. Langkilde and K. Knight. 1998b. The Practical Value of
N-grams in Generation. In Proceedings of the INLG.
I. Langkilde. 2000. Forest-Based Statistical Sentence Gener-
ation. In Proceedings of the NAACL, pages 170177.
A. Ratnaparkhi. 2000. Trainable Methods for Surface Natu-
ral Language Generation. In Proceedings of the NAACL,
pages 194201.
E. S. Ristad. 1997. Maximum Entropy Modeling for Natural
Language. ACL/EACL Tutorial Program, Madrid.
E. S. Ristad. 1998. Maximum Entropy Modeling Toolkit,
Release 1.6 beta. http://www.mnemonic.com/software/
memt.
K. Uchimoto, S. Sekine, and H. Isahara. 1999. Japanese De-
pendency Structure Analysis Based on Maximum Entropy
Models. In Proceedings of the EACL, pages 196203.
K. Uchimoto, M. Murata, Q. Ma, S. Sekine, and H. Isahara.
2000a. Word Order Acquisition from Corpora. In Proceed-
ings of the COLING, pages 871877.
K. Uchimoto, M. Murata, S. Sekine, and H. Isahara. 2000b.
Dependency Model Using Posterior Context. In Proceed-
ings of the IWPT, pages 321322.
K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown
Word Problem: a Morphological Analysis of Japanese Us-
ing Maximum Entropy Aided by a Dictionary. In Proceed-
ings of the EMNLP, pages 9199.
\x0c&amp;apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.319051">
<title confidence="0.707823666666667">b&amp;apos;Text Generation from Keywords Kiyotaka Uchimoto Satoshi Sekine</title>
<author confidence="0.67652">Hitoshi Isahara</author>
<affiliation confidence="0.99977">Communications Research Laboratory</affiliation>
<address confidence="0.9775435">2-2-2, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan</address>
<email confidence="0.961645">uchimoto@crl.go.jp</email>
<email confidence="0.961645">isahara@crl.go.jp</email>
<affiliation confidence="0.999129">New York University</affiliation>
<address confidence="0.9985495">715 Broadway, 7th floor New York, NY 10003, USA</address>
<email confidence="0.999839">sekine@cs.nyu.edu</email>
<abstract confidence="0.996477555555555">We describe a method for generating sentences from keywords or headwords. This method consists of two main parts, candidate-text construction and evaluation. The construction part generates text sentences in the form of dependency trees by using complementary information to replace information that is missing because of a knowledge gap and other missing function words to generate natural text sentences based on a particular monolingual corpus. The evaluation part consists of a model for generating an appropriate text when given keywords. This model considers not only word n-gram information, but also dependency information between words. Furthermore, it considers both string information and morphological information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>O Rambow</author>
</authors>
<title>Exploiting a Probabilistic Hierarchical Model for Generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the COLING,</booktitle>
<pages>4248</pages>
<contexts>
<context position="21945" citStr="Bangalore and Rambow, 2000" startWordPosition="3738" endWordPosition="3741">ndidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkilde, 2000) to express the candidate-text sentences. Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees (Bangalore and Rambow, 2000). They consider dependency information when deriving trees by using XTAG grammar, but they assume that the input contains dependency information. Our system generates candidate-text sentences without relying on dependency information in the input, and our model estimates the dependencies between keywords. Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). The input of these models is semantic attributes. His models are similar to ours if the semantic attributes are replaced with keywords. However, his models need a training corpus in which certain words </context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>S. Bangalore and O. Rambow. 2000. Exploiting a Probabilistic Hierarchical Model for Generation. In Proceedings of the COLING, pages 4248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Information Retrieval as Statistical Translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACM SIGIR,</booktitle>
<pages>222229</pages>
<contexts>
<context position="23204" citStr="Berger and Lafferty, 1999" startWordPosition="3923" endWordPosition="3927">utes. Although our model also needs a training corpus, the corpus can be automatically created by using a morphological analyzer and a dependency analyzer, both of which are readily available. Humphreys et al. proposed using models developed for sentence-structure analysis to rank candidate-text sentences (Humphreys et al., 2001). As well as models developed for sentence-structure analysis, we also use those developed for morphological analysis and found that these models contribute to the generation of appropriate text. Berger and Lafferty proposed a language model for information retrieval (Berger and Lafferty, 1999). Their concept is similar to that of our model, which can be regarded as a model that translates keywords into text, while their model can be regarded as one that translates query words into documents. However, the purpose of their model is different: their goal is to \x0cretrieve text that already exists while ours is to generate new text. 7 Conclusion We have described a method for generating sentences from keywords or headwords. This method consists of two main parts, candidatetext construction and evaluation. 1. The construction part generates text sentences in the form of dependency tree</context>
</contexts>
<marker>Berger, Lafferty, 1999</marker>
<rawString>A. Berger and J. Lafferty. 1999. Information Retrieval as Statistical Translation. In Proceedings of the ACM SIGIR, pages 222229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="12553" citStr="Berger et al., 1996" startWordPosition="2054" endWordPosition="2057">eyword are generated as b1, b2, and b3, the candidate dependency trees are (b1 (b2 b3)) and ((b1 b2) b3) if we do not reorder keywords, but 16 trees result if we consider the order of keywords to be arbitrary. 4 Text-Generation Model We next describe the model represented by Eq. (4); that is, a keyword-production model, a morpheme model that estimates how likely a string is to be a morpheme, and a dependency model. The goal of this model is to select optimal sets of morphemes and dependencies that can generate natural sentences. We implemented these models within an maximum entropy framework (Berger et al., 1996; Ristad, 1997; Ristad, 1998). 4.1 Keyword-Production Models This section describes five keyword-production models which are represented by P(K|M, D, T) in Eq. (4). In these models, we define the set of headwords whose frequency in the corpus is over a certain threshold as a set of keywords, KS, and we restrict the bunsetsus to those generated by the generation rules represented in form (5). We assume that all keywords are independent and that ki corresponds to word wj(1 j m) when text is given as a series of words w1 . . . wm. 1. trigram model We assume that ki depends only on the two anterio</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):3971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J Cocke</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A Statistical Approach to Machine Translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1655" citStr="Brown et al., 1990" startWordPosition="238" endWordPosition="241">troduction Text generation is an important technique used for applications like machine translation, summarization, and human/computer dialogue. In recent years, many corpora have become available, and have been used to generate natural surface sentences. For example, corpora have been used to generate sentences for language model estimation in statistical machine translation. In such translation, given a source language text, S, the translated text, T, in the target language that maximizes the probability P(T|S) is selected as the most appropriate translation, Tbest, which is represented as (Brown et al., 1990) Tbest = argmaxT P(T |S) = argmaxT (P(S|T ) P(T )) . (1) In this equation, P(S|T) represents the model used to replace words or phrases in a source language with those in the target language. It is called a translation model. P(T) represents a language model that is used to reorder translated words or phrases into a natural order in the target language. The input of the language model is a bag of words, and the goal of the model is basically to reorder the words. At this point, there is an assumption that natural sentences can be generated by merely reordering the words given by a translation </context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. 1990. A Statistical Approach to Machine Translation. Computational Linguistics, 16(2):7985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Humphreys</author>
<author>M Calcagno</author>
<author>D Weise</author>
</authors>
<title>Reusing a Statistical Language Model for Generation.</title>
<date>2001</date>
<booktitle>In Proceedings of the EWNLG.</booktitle>
<contexts>
<context position="22909" citStr="Humphreys et al., 2001" startWordPosition="3881" endWordPosition="3884">erate text from semantic attributes (Ratnaparkhi, 2000). The input of these models is semantic attributes. His models are similar to ours if the semantic attributes are replaced with keywords. However, his models need a training corpus in which certain words are replaced with semantic attributes. Although our model also needs a training corpus, the corpus can be automatically created by using a morphological analyzer and a dependency analyzer, both of which are readily available. Humphreys et al. proposed using models developed for sentence-structure analysis to rank candidate-text sentences (Humphreys et al., 2001). As well as models developed for sentence-structure analysis, we also use those developed for morphological analysis and found that these models contribute to the generation of appropriate text. Berger and Lafferty proposed a language model for information retrieval (Berger and Lafferty, 1999). Their concept is similar to that of our model, which can be regarded as a model that translates keywords into text, while their model can be regarded as one that translates query words into documents. However, the purpose of their model is different: their goal is to \x0cretrieve text that already exis</context>
</contexts>
<marker>Humphreys, Calcagno, Weise, 2001</marker>
<rawString>K. Humphreys, M. Calcagno, and D. Weise. 2001. Reusing a Statistical Language Model for Generation. In Proceedings of the EWNLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Two-Level, ManyPaths Generation.</title>
<date>1995</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>252--260</pages>
<contexts>
<context position="21169" citStr="Knight and Hatzivassiloglou, 1995" startWordPosition="3620" endWordPosition="3623">n proposed. In this section, we describe the differences between our method and several previous methods. Japanese words are often followed by postpositional particles, such as ga and wo, to indicate the subject and object of a sentence. There are no corresponding words in English. Instead, English words are preceded by articles, the and a, to distinguish definite and indefinite nouns, and so on, and in this case there are no corresponding words in Japanese. Knight et al. proposed a way to compensate for missing information caused by a lack of language-dependent knowledge, or a knowledge gap (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998a; Langkilde and Knight, 1998b). They use semantic expressions as input, whereas we use keywords. Also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkil</context>
</contexts>
<marker>Knight, Hatzivassiloglou, 1995</marker>
<rawString>K. Knight and V. Hatzivassiloglou. 1995. Two-Level, ManyPaths Generation. In Proceedings of the ACL, pages 252 260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kurohashi</author>
<author>M Nagao</author>
</authors>
<title>Building a Japanese Parsed Corpus while Improving the Parsing System.</title>
<date>1997</date>
<booktitle>In Proceedings of the NLPRS,</booktitle>
<pages>451456</pages>
<contexts>
<context position="8265" citStr="Kurohashi and Nagao, 1997" startWordPosition="1347" endWordPosition="1350">lly consists of several content and function words. We define the headword of a bunsetsu as the rightmost content word in the bunsetsu, and we define a content word as a \x0cword whose part-of-speech is a verb, adjective, noun, demonstrative, adverb, conjunction, attribute, interjection, or undefined word. We define the other words as function words. We define formal nouns and auxiliary verbs SURU (do) and NARU (become) as function words, except when there are no other content words in the same bunsetsu. Part-of-speech categories follow those in the Kyoto University text corpus (Version 3.0) (Kurohashi and Nagao, 1997), a tagged corpus of the Mainichi newspaper. Figure 2: Example of text generated from keywords. For example, given the set of keywords kanojo (she), ie (house), and iku (go), as shown in Fig. 2, our system retrieves sentences including each word, and extracts each bunsetsu that includes each word as a headword of the bunsetsu. If there is no tagged corpus such as the Kyoto University text corpus, each bunsetsu can be extracted by using a morphologicalanalysis system and a dependency-analysis system such as JUMAN (Kurohashi and Nagao, 1999) and KNP (Kurohashi, 1998). Our system then acquires ge</context>
</contexts>
<marker>Kurohashi, Nagao, 1997</marker>
<rawString>S. Kurohashi and M. Nagao. 1997. Building a Japanese Parsed Corpus while Improving the Parsing System. In Proceedings of the NLPRS, pages 451456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kurohashi</author>
<author>M Nagao</author>
</authors>
<date>1999</date>
<booktitle>Japanese Morphological Analysis System JUMAN Version 3.61.</booktitle>
<institution>Department of Informatics, Kyoto University.</institution>
<contexts>
<context position="8810" citStr="Kurohashi and Nagao, 1999" startWordPosition="1440" endWordPosition="1443">ose in the Kyoto University text corpus (Version 3.0) (Kurohashi and Nagao, 1997), a tagged corpus of the Mainichi newspaper. Figure 2: Example of text generated from keywords. For example, given the set of keywords kanojo (she), ie (house), and iku (go), as shown in Fig. 2, our system retrieves sentences including each word, and extracts each bunsetsu that includes each word as a headword of the bunsetsu. If there is no tagged corpus such as the Kyoto University text corpus, each bunsetsu can be extracted by using a morphologicalanalysis system and a dependency-analysis system such as JUMAN (Kurohashi and Nagao, 1999) and KNP (Kurohashi, 1998). Our system then acquires generation rules as follows. kanojo (she)kanojo (she) no (of) kanojo (she)kanojo (she) ga ie (house)ie (house) ni (to) iku (go)iku (go) iku (go)itta (went) The system next generates candidate bunsetsus for each keyword and candidate-text sentences in the form of dependency trees, such as Candidate 1 and Candidate 2 in Fig. 2, with the assumption that there are dependencies between keywords. Finally, the candidate-text sentences are ranked by their scores, calculated by a text-generation model, and transformed into surface sentences. In this </context>
</contexts>
<marker>Kurohashi, Nagao, 1999</marker>
<rawString>S. Kurohashi and M. Nagao, 1999. Japanese Morphological Analysis System JUMAN Version 3.61. Department of Informatics, Kyoto University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kurohashi</author>
</authors>
<date>1998</date>
<booktitle>Japanese Dependency/Case Structure Analyzer KNP Version 2.0b6.</booktitle>
<institution>Department of Informatics, Kyoto University.</institution>
<contexts>
<context position="8836" citStr="Kurohashi, 1998" startWordPosition="1446" endWordPosition="1447">pus (Version 3.0) (Kurohashi and Nagao, 1997), a tagged corpus of the Mainichi newspaper. Figure 2: Example of text generated from keywords. For example, given the set of keywords kanojo (she), ie (house), and iku (go), as shown in Fig. 2, our system retrieves sentences including each word, and extracts each bunsetsu that includes each word as a headword of the bunsetsu. If there is no tagged corpus such as the Kyoto University text corpus, each bunsetsu can be extracted by using a morphologicalanalysis system and a dependency-analysis system such as JUMAN (Kurohashi and Nagao, 1999) and KNP (Kurohashi, 1998). Our system then acquires generation rules as follows. kanojo (she)kanojo (she) no (of) kanojo (she)kanojo (she) ga ie (house)ie (house) ni (to) iku (go)iku (go) iku (go)itta (went) The system next generates candidate bunsetsus for each keyword and candidate-text sentences in the form of dependency trees, such as Candidate 1 and Candidate 2 in Fig. 2, with the assumption that there are dependencies between keywords. Finally, the candidate-text sentences are ranked by their scores, calculated by a text-generation model, and transformed into surface sentences. In this paper, we focus on the key</context>
</contexts>
<marker>Kurohashi, 1998</marker>
<rawString>S. Kurohashi, 1998. Japanese Dependency/Case Structure Analyzer KNP Version 2.0b6. Department of Informatics, Kyoto University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that Exploits Corpus-Based Statistical Knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL,</booktitle>
<pages>704710</pages>
<contexts>
<context position="21197" citStr="Langkilde and Knight, 1998" startWordPosition="3624" endWordPosition="3627">cribe the differences between our method and several previous methods. Japanese words are often followed by postpositional particles, such as ga and wo, to indicate the subject and object of a sentence. There are no corresponding words in English. Instead, English words are preceded by articles, the and a, to distinguish definite and indefinite nouns, and so on, and in this case there are no corresponding words in Japanese. Knight et al. proposed a way to compensate for missing information caused by a lack of language-dependent knowledge, or a knowledge gap (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998a; Langkilde and Knight, 1998b). They use semantic expressions as input, whereas we use keywords. Also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkilde, 2000) to express the can</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and K. Knight. 1998a. Generation that Exploits Corpus-Based Statistical Knowledge. In Proceedings of the COLING-ACL, pages 704710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>The Practical Value of N-grams in Generation.</title>
<date>1998</date>
<booktitle>In Proceedings of the INLG.</booktitle>
<contexts>
<context position="21197" citStr="Langkilde and Knight, 1998" startWordPosition="3624" endWordPosition="3627">cribe the differences between our method and several previous methods. Japanese words are often followed by postpositional particles, such as ga and wo, to indicate the subject and object of a sentence. There are no corresponding words in English. Instead, English words are preceded by articles, the and a, to distinguish definite and indefinite nouns, and so on, and in this case there are no corresponding words in Japanese. Knight et al. proposed a way to compensate for missing information caused by a lack of language-dependent knowledge, or a knowledge gap (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998a; Langkilde and Knight, 1998b). They use semantic expressions as input, whereas we use keywords. Also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkilde, 2000) to express the can</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and K. Knight. 1998b. The Practical Value of N-grams in Generation. In Proceedings of the INLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
</authors>
<title>Forest-Based Statistical Sentence Generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the NAACL,</booktitle>
<pages>170177</pages>
<contexts>
<context position="21778" citStr="Langkilde, 2000" startWordPosition="3716" endWordPosition="3717">u, 1995; Langkilde and Knight, 1998a; Langkilde and Knight, 1998b). They use semantic expressions as input, whereas we use keywords. Also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkilde, 2000) to express the candidate-text sentences. Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees (Bangalore and Rambow, 2000). They consider dependency information when deriving trees by using XTAG grammar, but they assume that the input contains dependency information. Our system generates candidate-text sentences without relying on dependency information in the input, and our model estimates the dependencies between keywords. Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). The input of these models is semant</context>
</contexts>
<marker>Langkilde, 2000</marker>
<rawString>I. Langkilde. 2000. Forest-Based Statistical Sentence Generation. In Proceedings of the NAACL, pages 170177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Trainable Methods for Surface Natural Language Generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the NAACL,</booktitle>
<pages>194201</pages>
<contexts>
<context position="22341" citStr="Ratnaparkhi, 2000" startWordPosition="3795" endWordPosition="3796">y the formalism proposed by Langkilde (Langkilde, 2000) to express the candidate-text sentences. Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees (Bangalore and Rambow, 2000). They consider dependency information when deriving trees by using XTAG grammar, but they assume that the input contains dependency information. Our system generates candidate-text sentences without relying on dependency information in the input, and our model estimates the dependencies between keywords. Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). The input of these models is semantic attributes. His models are similar to ours if the semantic attributes are replaced with keywords. However, his models need a training corpus in which certain words are replaced with semantic attributes. Although our model also needs a training corpus, the corpus can be automatically created by using a morphological analyzer and a dependency analyzer, both of which are readily available. Humphreys et al. proposed using models developed for sentence-structure analysis to rank candidate-text sentences (Humphreys et al., 2001). As well as models developed fo</context>
</contexts>
<marker>Ratnaparkhi, 2000</marker>
<rawString>A. Ratnaparkhi. 2000. Trainable Methods for Surface Natural Language Generation. In Proceedings of the NAACL, pages 194201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S Ristad</author>
</authors>
<title>Maximum Entropy Modeling for Natural Language. ACL/EACL Tutorial Program,</title>
<date>1997</date>
<location>Madrid.</location>
<contexts>
<context position="12567" citStr="Ristad, 1997" startWordPosition="2058" endWordPosition="2059">as b1, b2, and b3, the candidate dependency trees are (b1 (b2 b3)) and ((b1 b2) b3) if we do not reorder keywords, but 16 trees result if we consider the order of keywords to be arbitrary. 4 Text-Generation Model We next describe the model represented by Eq. (4); that is, a keyword-production model, a morpheme model that estimates how likely a string is to be a morpheme, and a dependency model. The goal of this model is to select optimal sets of morphemes and dependencies that can generate natural sentences. We implemented these models within an maximum entropy framework (Berger et al., 1996; Ristad, 1997; Ristad, 1998). 4.1 Keyword-Production Models This section describes five keyword-production models which are represented by P(K|M, D, T) in Eq. (4). In these models, we define the set of headwords whose frequency in the corpus is over a certain threshold as a set of keywords, KS, and we restrict the bunsetsus to those generated by the generation rules represented in form (5). We assume that all keywords are independent and that ki corresponds to word wj(1 j m) when text is given as a series of words w1 . . . wm. 1. trigram model We assume that ki depends only on the two anterior words wj1 an</context>
</contexts>
<marker>Ristad, 1997</marker>
<rawString>E. S. Ristad. 1997. Maximum Entropy Modeling for Natural Language. ACL/EACL Tutorial Program, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S Ristad</author>
</authors>
<title>Maximum Entropy Modeling Toolkit,</title>
<date>1998</date>
<journal>Release</journal>
<volume>1</volume>
<note>beta. http://www.mnemonic.com/software/ memt.</note>
<contexts>
<context position="12582" citStr="Ristad, 1998" startWordPosition="2060" endWordPosition="2061"> b3, the candidate dependency trees are (b1 (b2 b3)) and ((b1 b2) b3) if we do not reorder keywords, but 16 trees result if we consider the order of keywords to be arbitrary. 4 Text-Generation Model We next describe the model represented by Eq. (4); that is, a keyword-production model, a morpheme model that estimates how likely a string is to be a morpheme, and a dependency model. The goal of this model is to select optimal sets of morphemes and dependencies that can generate natural sentences. We implemented these models within an maximum entropy framework (Berger et al., 1996; Ristad, 1997; Ristad, 1998). 4.1 Keyword-Production Models This section describes five keyword-production models which are represented by P(K|M, D, T) in Eq. (4). In these models, we define the set of headwords whose frequency in the corpus is over a certain threshold as a set of keywords, KS, and we restrict the bunsetsus to those generated by the generation rules represented in form (5). We assume that all keywords are independent and that ki corresponds to word wj(1 j m) when text is given as a series of words w1 . . . wm. 1. trigram model We assume that ki depends only on the two anterior words wj1 and wj2. P(K|M, D</context>
</contexts>
<marker>Ristad, 1998</marker>
<rawString>E. S. Ristad. 1998. Maximum Entropy Modeling Toolkit, Release 1.6 beta. http://www.mnemonic.com/software/ memt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Uchimoto</author>
<author>S Sekine</author>
<author>H Isahara</author>
</authors>
<title>Japanese Dependency Structure Analysis Based on Maximum Entropy Models.</title>
<date>1999</date>
<booktitle>In Proceedings of the EACL,</booktitle>
<pages>196203</pages>
<contexts>
<context position="16622" citStr="Uchimoto et al., 1999" startWordPosition="2834" endWordPosition="2838">appropriate. We used headwords that were found five times or more in the newspaper articles appearing from January 1st to 16th in the Kyoto University text corpus and also found in those appearing on January 1st as the set of headwords, KS. For headwords that were not in KS, we added their major part-of-speech categories to the set. We trained our keyword-production models by using 1,129 sentences (containing 10,201 headwords) from newspaper articles appearing on January 1st. We used a morpheme model and a dependency model identical to those proposed by Uchimoto et al. (Uchimoto et al., 2001; Uchimoto et al., 1999; Uchimoto et al., 2000b). To train the models, we used 8,835 sentences from newspaper articles appearing from January 1st to 9th in 1995. Generation rules were acquired from newspaper articles appearing from January 1st to 16th. The total number of sentences was 18,435. First, we evaluated the outputs generated when the rightmost two keywords, such as and , on each line of Table 1 were input. Table 2 shows the results. KM1 through KM5 stand for the five keyword-production models described in Section 4.1, and MM and DM stand for the morpheme and the dependency models, respectively. The symbol </context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 1999</marker>
<rawString>K. Uchimoto, S. Sekine, and H. Isahara. 1999. Japanese Dependency Structure Analysis Based on Maximum Entropy Models. In Proceedings of the EACL, pages 196203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Uchimoto</author>
<author>M Murata</author>
<author>Q Ma</author>
<author>S Sekine</author>
<author>H Isahara</author>
</authors>
<title>Word Order Acquisition from Corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the COLING,</booktitle>
<pages>871877</pages>
<contexts>
<context position="16645" citStr="Uchimoto et al., 2000" startWordPosition="2839" endWordPosition="2842">adwords that were found five times or more in the newspaper articles appearing from January 1st to 16th in the Kyoto University text corpus and also found in those appearing on January 1st as the set of headwords, KS. For headwords that were not in KS, we added their major part-of-speech categories to the set. We trained our keyword-production models by using 1,129 sentences (containing 10,201 headwords) from newspaper articles appearing on January 1st. We used a morpheme model and a dependency model identical to those proposed by Uchimoto et al. (Uchimoto et al., 2001; Uchimoto et al., 1999; Uchimoto et al., 2000b). To train the models, we used 8,835 sentences from newspaper articles appearing from January 1st to 9th in 1995. Generation rules were acquired from newspaper articles appearing from January 1st to 16th. The total number of sentences was 18,435. First, we evaluated the outputs generated when the rightmost two keywords, such as and , on each line of Table 1 were input. Table 2 shows the results. KM1 through KM5 stand for the five keyword-production models described in Section 4.1, and MM and DM stand for the morpheme and the dependency models, respectively. The symbol + indicates a combinati</context>
<context position="24590" citStr="Uchimoto et al., 2000" startWordPosition="4147" endWordPosition="4150">ased on a particular monolingual corpus. 2. The evaluation part consists of a model for generating an appropriate text sentence when given keywords. This model considers the dependency information between words as well as word n-gram information. Furthermore, the model considers both string and morphological information. If a language model, such as a word n-gram model, is applied to the generated-text sentences in the form of dependency trees, an appropriate surface-text sentence is generated. The word-order model proposed by Uchimoto et al. can also generate surface text in a natural order (Uchimoto et al., 2000a). There are several possible directions for our future research. In particular, We would like to expand the generation rules. We restricted the generation rules automatically acquired from a corpus to those that generate a bunsetsu. To generate a greater variety of candidate-text sentences, we would like to expand the rules that can generate a dependency tree. Expansion would lead to complementing with content words as well as function words. We also would like to prepare default rules or to classify words into several classes when no sentences including the keywords are found in the target </context>
</contexts>
<marker>Uchimoto, Murata, Ma, Sekine, Isahara, 2000</marker>
<rawString>K. Uchimoto, M. Murata, Q. Ma, S. Sekine, and H. Isahara. 2000a. Word Order Acquisition from Corpora. In Proceedings of the COLING, pages 871877.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Uchimoto</author>
<author>M Murata</author>
<author>S Sekine</author>
<author>H Isahara</author>
</authors>
<title>Dependency Model Using Posterior Context.</title>
<date>2000</date>
<booktitle>In Proceedings of the IWPT,</booktitle>
<pages>321322</pages>
<contexts>
<context position="16645" citStr="Uchimoto et al., 2000" startWordPosition="2839" endWordPosition="2842">adwords that were found five times or more in the newspaper articles appearing from January 1st to 16th in the Kyoto University text corpus and also found in those appearing on January 1st as the set of headwords, KS. For headwords that were not in KS, we added their major part-of-speech categories to the set. We trained our keyword-production models by using 1,129 sentences (containing 10,201 headwords) from newspaper articles appearing on January 1st. We used a morpheme model and a dependency model identical to those proposed by Uchimoto et al. (Uchimoto et al., 2001; Uchimoto et al., 1999; Uchimoto et al., 2000b). To train the models, we used 8,835 sentences from newspaper articles appearing from January 1st to 9th in 1995. Generation rules were acquired from newspaper articles appearing from January 1st to 16th. The total number of sentences was 18,435. First, we evaluated the outputs generated when the rightmost two keywords, such as and , on each line of Table 1 were input. Table 2 shows the results. KM1 through KM5 stand for the five keyword-production models described in Section 4.1, and MM and DM stand for the morpheme and the dependency models, respectively. The symbol + indicates a combinati</context>
<context position="24590" citStr="Uchimoto et al., 2000" startWordPosition="4147" endWordPosition="4150">ased on a particular monolingual corpus. 2. The evaluation part consists of a model for generating an appropriate text sentence when given keywords. This model considers the dependency information between words as well as word n-gram information. Furthermore, the model considers both string and morphological information. If a language model, such as a word n-gram model, is applied to the generated-text sentences in the form of dependency trees, an appropriate surface-text sentence is generated. The word-order model proposed by Uchimoto et al. can also generate surface text in a natural order (Uchimoto et al., 2000a). There are several possible directions for our future research. In particular, We would like to expand the generation rules. We restricted the generation rules automatically acquired from a corpus to those that generate a bunsetsu. To generate a greater variety of candidate-text sentences, we would like to expand the rules that can generate a dependency tree. Expansion would lead to complementing with content words as well as function words. We also would like to prepare default rules or to classify words into several classes when no sentences including the keywords are found in the target </context>
</contexts>
<marker>Uchimoto, Murata, Sekine, Isahara, 2000</marker>
<rawString>K. Uchimoto, M. Murata, S. Sekine, and H. Isahara. 2000b. Dependency Model Using Posterior Context. In Proceedings of the IWPT, pages 321322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Uchimoto</author>
<author>S Sekine</author>
<author>H Isahara</author>
</authors>
<title>The Unknown Word Problem: a Morphological Analysis of Japanese Using Maximum Entropy Aided by a Dictionary.</title>
<date>2001</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>9199--0</pages>
<contexts>
<context position="16599" citStr="Uchimoto et al., 2001" startWordPosition="2830" endWordPosition="2833">riate, it is judged as appropriate. We used headwords that were found five times or more in the newspaper articles appearing from January 1st to 16th in the Kyoto University text corpus and also found in those appearing on January 1st as the set of headwords, KS. For headwords that were not in KS, we added their major part-of-speech categories to the set. We trained our keyword-production models by using 1,129 sentences (containing 10,201 headwords) from newspaper articles appearing on January 1st. We used a morpheme model and a dependency model identical to those proposed by Uchimoto et al. (Uchimoto et al., 2001; Uchimoto et al., 1999; Uchimoto et al., 2000b). To train the models, we used 8,835 sentences from newspaper articles appearing from January 1st to 9th in 1995. Generation rules were acquired from newspaper articles appearing from January 1st to 16th. The total number of sentences was 18,435. First, we evaluated the outputs generated when the rightmost two keywords, such as and , on each line of Table 1 were input. Table 2 shows the results. KM1 through KM5 stand for the five keyword-production models described in Section 4.1, and MM and DM stand for the morpheme and the dependency models, re</context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 2001</marker>
<rawString>K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown Word Problem: a Morphological Analysis of Japanese Using Maximum Entropy Aided by a Dictionary. In Proceedings of the EMNLP, pages 9199. \x0c&amp;apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>