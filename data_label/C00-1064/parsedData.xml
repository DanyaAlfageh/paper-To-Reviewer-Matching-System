<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.729863">
b&quot;Structural Feature Selection For English-Korean Statistical
Machine Translation
</title>
<author confidence="0.730086">
Seonho Kim, Juntae Yoon, Mansuk Song
</author>
<email confidence="0.664609">
fpobi, jtyoon, mssongg@december.yonsei.ac.kr
</email>
<affiliation confidence="0.7647405">
Dept. of Computer Science,
Yonsei University, Seoul, Korea
</affiliation>
<sectionHeader confidence="0.975746" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993826684210526">
When aligning texts in very di\x0berent languages such
as Korean and English, structural features beyond
word or phrase give useful information. In this pa-
per, we present a method for selecting structural
features of two languages, from which we construct
a model that assigns the conditional probabilities
to corresponding tag sequences in bilingual English-
Korean corpora. For tag sequence mapping between
two langauges, we \x0crst de\x0cne a structural feature
function which represents statistical properties of
empirical distribution of a set of training samples.
Thesystem, basedonmaximumentropyconcept, se-
lectsonlyfeaturesthatproducehighincreasesinlog-
likelihood of training samples. These structurally
mapped featuresaremoreinformativeknowledgefor
statistical machine translation between English and
Korean. Also,theinformationcanhelptoreducethe
parameterspaceof statisticalalignment byeliminat-
ing syntactically unlikely alignments.
</bodyText>
<sectionHeader confidence="0.998173" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999258878787879">
Aligned texts have been used for derivation of bilin-
gual dictionaries and terminology databases which
are useful for machine translation and cross lan-
guages information retrieval. Thus, a lot of align-
ment techniques have been suggested at the sen-
tence (Gale et al., 1993), phrase (Shin et al., 1996),
noun phrase (Kupiec, 1993), word (Brown et al.,
1993; Berger et al., 1996; Melamed, 1997), collo-
cation (Smadja et al., 1996) and terminology level.
Some work has used lexical association measures
for word alignments. However, the association mea-
sures could be misled since a word in a source lan-
guage frequently co-occurs with more than one word
in a target language. In other work, iterative re-
estimation techniques have been employed. They
were usually incorporated with the EM algorithm
and dynamic programming. In that case, the prob-
abilities of alignments usually served as parameters
in a model of statistical machine translation.
In statistical machine translation, IBM 1\x185 mod-
els (Brown et al., 1993) based on the source-channel
model have been widely used and revised for many
language domains and applications. It has also
shortcoming that it needs much iteration time for
parameter estimation and high decoding complex-
ity, however.
Much work has been done to overcome the prob-
lem. Wu (1996) adopted channels that eliminate
syntactically unlikely alignments and Wang et al.
(1998) presented a model based on structures of two
languages. Tillmann et al. (1997) suggested the
dynamic programming based search to select the
best alignment and preprocessed bilingual texts to
remove word order di\x0berences. Sato et al. (1998)
and Och et al. (1998) proposed a model for learn-
ing translation rules with morphologicalinformation
and word category in order to improve statistical
translation.
Furthermore, many researches assumed one-to-
one correspondence due to the complexity and com-
putation time of statistical alignments. Although
this assumption turned out to be useful for align-
ment of close languages such as English and French,
it is not applicable to very di\x0berent languages, in
particular, Korean and English where there is rarely
close correspondence in order at the word level. For
such languages, even phrase level alignment, not to
mention word alignment, does not gives good trans-
lation due to structuraldi\x0berence. Hence, structural
features beyond word or phrase should be consid-
ered to get better translation between English and
Korean. In addition, the construction of structural
bilingual texts would be more informative for ex-
tracting linguistic knowledge.
In this paper, we suggest a method for structural
mapping of bilingual language on the basis of the
maximum entorpyand feature induction framework.
Our model based on POS tag sequence mapping has
two advantages: First, it can reduce a lot of parame-
ters in statistical machine translation by eliminating
syntactically unlikely alignments. Second, it can be
used as a preprocessorfor lexical alignmentsof bilin-
gual corpora although it can be also exploited by it-
self for alignment. In this case, it would serve as the
\x0crst step of alignment for reducing the parameter
space.
</bodyText>
<sectionHeader confidence="0.667221" genericHeader="introduction">
\x0c2 Motivation
</sectionHeader>
<bodyText confidence="0.99937625">
In order to devise parameters for statistical model-
ing of translation, we started our research from the
IBM model which has been widely used by many
researches. The IBM model is represented with the
</bodyText>
<equation confidence="0.995886727272727">
formula shown in (1)
p(f;aje) =
l
Y
i=1
n(\x1eijei)
m
Y
j=1
t(fjjeaj )d(jjaj;m;l)
(1)
</equation>
<bodyText confidence="0.998172826923077">
Here, n is the fertility probability that an English
word generates n French words, t is the alignment
probability that the English word e generates the
French word f, and d is the distortion probability
that an Englishwordin acertain positionwill gener-
ateaFrenchwordinacertainposition. Thisformula
is oneof manywaysin which p(f;aje) can be written
astheproductofaseriesofconditionalprobabilities.
In above model, the distortion probability is re-
lated with positional preference(word order). Since
Korean is a free order language, the probability is
not feasible in English-Korean translation.
Furthermore, the di\x0berence between two lan-
guages leads to the discordance between words that
the one-to-one correspondence between words gen-
erally does not keep. The model (1), however, as-
sumed that an English word can be connected with
multiple French words, but that each French word
is connected to exactly one English word including
the empty word. In conclusion, many-to-many map-
pings are not allowed in this model.
According to our experiment, many-to-many
mappings exceed 40% in English and Korean lexical
alignments. Only 25.1% of them can be explained
by wordfor word correspondences. It means that we
need a statistical model which can handle phrasal
mappings.
In the case of the phrasal mappings, a lot of pa-
rameters should be searched even if we restrict the
length of word strings. Moreover, in order to prop-
erly estimate parameters we need much larger vol-
ume of bilingual aligned text than it in word-for-
word modeling. Even though such a large corpora
exist sometimes, they do not come up with the lex-
ical alignments.
For this problem, we here consider syntactic fea-
tureswhichareimportantin determiningstructures.
A structural feature means here a mapping between
tag sequences in bilingual parallel sentences.
If we are concerned with tag sequence alignments,
it is possible to estimate statistical parameters in
a relatively small size of corpora. As a result, we
can remarkably reduce the problem space for possi-
ble lexical alignments, a sort of t probability in (1),
which improve the complexity of a statistical ma-
chine translation model.
If there are similarities between corresponding tag
sequences in two language, the structural features
would be easily computed or recognized. However,
a tag sequence in English can be often translated
into a completely di\x0berent tag sequence in Korean
as follows.
</bodyText>
<equation confidence="0.8834">
can/MD ! \x18 eul/ENTR1 su/NNDE1 &amp;apos;iss/AJMA
da/ENTE
Itmeansthatsimilaritiesoftagfeaturesbetweentwo
</equation>
<bodyText confidence="0.995466125">
languages are not kept all the time and it is neces-
sary to get the most likely tag sequence mappings
that re
ect structural correspondences between two
languages.
In this paper, the tag sequence mappings are ob-
taind by automatic feature selection based on the
maximum entropy model.
</bodyText>
<sectionHeader confidence="0.919277" genericHeader="method">
3 Problem Setting
</sectionHeader>
<bodyText confidence="0.9711925">
In this chapter, we describe how the features are
related to the training data. Let te be an English
tag sequence and tk be a Korean tag sequence. Let
TS bethesetofallpossibletagsequencemappingsin
a aligned sentence, S. We de\x0cne a feature function
(or a feature) as follows:
</bodyText>
<equation confidence="0.98967675">
f(te;tk) =
\x1a
1 pair(te;tk) 2 TS
0 otherwise
</equation>
<bodyText confidence="0.986542590909091">
It indicates co-occurrence information between
tags appeared in TS. f(te;tk) expresses the infor-
mation for predicting that te maps into tk. A fea-
turemeansasortofinformationforpredictingsome-
thing. In our model, co-occurrence information on
the same aligned sentence is used for a feature, while
context is used as a feature in most of systems using
maximum entropy. It can be less informative than
context. Hence, we considered an initial supervision
and feature selection.
Our model starts with initial seed(active) features
for mapping extracted by supervision. In the next
step, feature pool is constructed from training sam-
plesfrom\x0clteringandonlyfeatureswith alargegain
to the model are added into active feature set. The
\x0cnal outputs of our model are the set of active fea-
tures, theirgainvalues, andconditionalprobabilities
of features which maximize the model. The results
can be embedded in parameters of statistical ma-
chine translation and help to construct structural
bilingual text.
Most alignment algorithm consists of two steps:
</bodyText>
<listItem confidence="0.871707">
(1) estimate translation probabilities.
(2) use these probabilities to search for most proba-
</listItem>
<bodyText confidence="0.985677705882353">
ble alignment path.
Our study is focused on (1), especially the part of
tag string alignments.
Next, we will explain the concept of the model.
We are concerned with an optimal statistical model
which can generate the training samples. Namely,
our task is to construct a stochastic model that pro-
\x0cduces output tag sequence Tk, given a tag sequence
Te. The problem of interest is to use samples of
tagged sentences to observe the behavior of the ran-
dom process. The model p estimates the conditional
probability that the process will output te, given tk.
It is chosen out of a set of all allowed probability
distributions.
The following steps are employed for our model.
Input: a set L of POS-labeled bilingual aligned
sentences.
</bodyText>
<listItem confidence="0.969391071428571">
1. Make a set F of correspondence pairs of tag
sequences, (te;tk) from a small portion of L by
supervision.
2. Set F into a set of active features, A.
3. Maximization of parameters, \x15 of active fea-
tures by IIS(Improved Iterative Scaling) algo-
rithm.
4. Create a feature pool set P of all possible align-
ments a(te;tk) from tag sequences of samples.
5. Filter P using frequency and similarity with A.
6. Compute the approximate gains of features in
P.
7. Select new features(N) with a large gain value,
and add A.
</listItem>
<bodyText confidence="0.986913148148148">
Output: p(tkjte)where(te;tk) 2 A and their \x15i.
We began with training samples composed of
English-Korean aligned sentence pairs, (e,k). Since
they included long sentences, we broke them into
shorter ones. The length of training sentences was
limited to under 14 on the basis of English. It is
reasonable because we are interested in not lexical
alignments but tag sequence alignments. The sam-
ples were tagged using brill&amp;apos;s tagger and `Morany&amp;apos;
that we implemented as a Korean tagger. Figure 1
shows the POS tags we considered. For simplicity,
we adjusted some part of Brill&amp;apos;s tag set.
Inthesupervisionstep, 700alignedsentenceswere
used to construct the tag sequences mappings which
are referred to as an active feature set A. As Fig-
ure 2 shows, there are several ways in constructing
the correspondences. We chose the third mapping
although (1) can be more useful to explain Korean
with predicate-argument structure. Since a subject
of a English sentence is always used for a subject
form in Korean, we exlcuded a subject case from ar-
guments of a predicate. For example, `they&amp;apos; is only
used for a subject form, whereas `me&amp;apos; is used for a
object form and a dative form.
In the next step, training events, (te;tk) are con-
structed to make a feature pool from training sam-
ples. Theeventconsistsofatagstringte ofaEnglish
</bodyText>
<figureCaption confidence="0.863651">
Figure 2: Tag sequence correspondences at the
</figureCaption>
<bodyText confidence="0.971779944444445">
phrase level
POS-tagged sentence and a tag string tk of the cor-
responding Korean POS-tagged sentence and it can
be represented with indicator functions fi(te;tk).
For a given sequence, the features were drawn
fromalladjacentpossiblepairsandsomeinterrupted
pairs. Only features (tei;tki) out of the feature pool
that meet the following conditions are extracted.
\x0f #(tei;tki) \x15 3, # is count
\x0f there exist tkx, where (tei;tkx) in A and the
similarity(same tag count) of tki and tkx \x15 0:6
Table 1 shows possible features, for a given aligned
sentence , `take her out gnyeoreul baggeuro
deryeogara&amp;apos;.
Since the set of the structural features for align-
ment modeling is vast, we constructed a maximum
entropy model for p(tkjte) by the iterative model
growing method.
</bodyText>
<sectionHeader confidence="0.977864" genericHeader="method">
4 Maximum Entropy
</sectionHeader>
<bodyText confidence="0.98768748">
To explain our method, we brie
y describe the con-
cept of maximum entropy. Recently, many ap-
proachesbasedonthemaximumentropymodelhave
been applied to natural language processing (Berger
et al., 1994; Berger et al., 1996; Pietra et al., 1997).
Suppose a model p which assigns a probability to
a random variable. If we don&amp;apos;t have any knowledge,
a reasonable solution for p is the most uniform dis-
tribution. As some knowledge to estimate the model
p are added, the solution space of p are more con-
strainedandthemodelwouldbeclosetotheoptimal
probability model.
For the purpose of getting the optimal probability
model, we need to maximize the uniformity under
some constraints we have. Here, the constraints are
related with features. A feature, fi is usually repre-
sented with a binary indicator function. The impor-
tance of a feature, fi can be identi\x0ced by requiring
that the model accords with it.
As a constraint, the expected value of fi with re-
spect to the model p(fi) is supposed to be the same
as the expected value of fi with respect to empirical
distribution in training samples, ~
p(fi).
</bodyText>
<figureCaption confidence="0.319662">
\x0cFigure 1: English Tags (left) and Korean Tags (right)
</figureCaption>
<figure confidence="0.904413846153846">
English Tag Sequences Korean Tag Sequences
[VBP+IN] [take+out] [1+3] [PPCA2+PPAD+VBMA] [reul+euro+deryeoga] [2+4+5]
[VBP] [take] [1] [PN] [geunyeo] [1]
[VBP+PRP] [take+her] [1+2] [PPAD+VBMA+ENTE] [reul+euro+ deryeoga+ra] [4+5+6]
[VBP+PRP+IN] [take+her+out] [1+2+3] [NNIN2] [bagg] [3]
[PRP] [her] [2] [NNIN2+PPAD] [bagg+euro] [3+4]
[IN] [out] [3] [ENTE] [ra] [6]
[PPAD+VBMA] [euro+deryeoga] [4+5]
[PPAD+VBMA+ENTE] [euro+deryeoga+ra] [4+5+6]
[PPCA2+NNIN2+PPAD+VBMA] [reul+bagg+euro+ deryeoga] [2+3+4+5]
[PPCA2+NNIN2+PPAD+VBMA+ENTE] [reul+bagg+euro+deryeoga+ra] [2+3+4+5+6]
[PPCA2+NNIN2+PPAD+VBMA] [reul+deryeoga] [2+3+4+5]
[PPCA2+NNIN2+PPAD+VBMA+ENTE] [reul+deryeoga+ra] [2+3+4+5+6]
</figure>
<tableCaption confidence="0.974885">
Table 1: possible tag sequences
</tableCaption>
<bodyText confidence="0.999035176470588">
In sum, the maximum entropy framework \x0cnds
the model which has highest entropy(most uniform),
given constraints. It is related to the constrained
optimization. To select a model from a constrained
set C of allowed probability distributions, the model
p? 2 C with maximum entropy H(p) is chosen.
In general, for the constrained optimization prob-
lem, Lagrange multipliers of the number of features
can be used. However, it was proved that the model
with maximum entropy is equivalent to the model
that maximizes the log likelihood of the training
sampleslike(2)if wecanassumeit asanexponential
model.
In (2), the left side is Lagrangian of the condi-
tional entropy and the right side is maximum log-
likelihood. We use the right side equation of (2) to
select \x15? for the best model p?.
</bodyText>
<equation confidence="0.992526777777778">
argmax\x15i(
P
x;y ~
p(x)p(yjx)logp(yjx)+\x15i(p(fi) ~
p(fi))) (2)
=argmax\x15i
P
x;y ~
p(x;y)logp(yjx)
</equation>
<bodyText confidence="0.94292075">
Since \x15? cannot be found analytically, we use
the following improved iterative scaling algorithm to
compute \x15? of n active features in A in total sam-
ples.
</bodyText>
<listItem confidence="0.9991152">
1. Start with \x15i = 0 for all i 2 f1;2;:::;ng
2. Do for each i 2 f1;2;:::;ng :
(a) Let \x01\x15i be the solution to the log likeli-
hood
(b) Update the value of \x15i into \x15i +\x01\x15i,
</listItem>
<equation confidence="0.997807066666667">
where \x01\x15i = log
P
x;y ~
p(x;y)fi(x;y)
P
x;y ~
p(x)p\x15(yjx)fi(x;y)
p\x15(yjx) = 1
Z\x15(x)e(
P
i \x15ifi(x;y)),
Z\x15(x) = P
y e(
P
i \x15ifi(x;y))
</equation>
<footnote confidence="0.8755075">
3. Stop if not all the \x15i have converged, otherwise
go to step 2
Theexponentialmodelisrepresentedas(3). Here,
\x15i is the weight of feature fi. In our model, since
</footnote>
<figureCaption confidence="0.312099333333333">
only one feature is applied to each pair of x and y,
it can be represented as (4) and fi is the feature
related with x and y.
</figureCaption>
<equation confidence="0.977813538461538">
~
p(yjx) =
P
i e\x15ifi(x;y)
P
y e
P
i \x15ifi(x;y) (3)
~
p(yjx) = e\x15ifi(x;y)
P
y e\x15ifi(x;y) (4)
\x0c5 Feature selection
</equation>
<bodyText confidence="0.997688375">
Only a small subset of features will be employed in
a model by selecting useful features from the feature
pool P. Let pA be the optimal model constrained
by a set of active features A and A[fi be Afi. Let
pAfi be the optimal model in the space of probabil-
ity distribution C(Afi). The optimal model can be
represented as (5). Here, the optimal model means
a maximum entropy model.
</bodyText>
<equation confidence="0.995053">
p\x0b
Afi = 1
Z\x0b(x)pA(yjx)e\x0bfi(x;y)
Z\x0b(x) =
X
y
pA(yjx)e\x0bfi(x;y) (5)
</equation>
<bodyText confidence="0.9974864">
The improvement of the model regarding the ad-
dition of asinglefeaturefi can be estimatedby mea-
suring the di\x0berence of maximum log-likelihood be-
tween L(pAfi) and L(pA). We denote the gain of
feature fi by \x01(Afi) and it can be represented in
</bodyText>
<equation confidence="0.980038153846154">
(6).
\x01(Afi) \x11 max\x0bGAfi(\x0b)
GAfi(\x0b) \x11 L(pAfi) L(pA)
=
X
x
~
p(x)
X
y
pA(yjx)e\x0bfi(x;y)
+\x0b~
p(fi) (6)
</equation>
<bodyText confidence="0.994943307692308">
Note that a model pA has a set of parameters \x15
which means weights of features. The model pAfi
contains the parameters and the new parameter \x0b
with respect to the feature fi. When adding a new
feature to A, the optimal values of all parameters of
probability distribution change. To make the com-
putation of feature selection tractable, we approxi-
mate that the addition of a feature fi a\x0bects only
the single parameter \x0b, as shown in (5).
The following algorithm is used for computing the
gain of the model with respect to fi. We referred
to the studies of (Berger et al., 1996; Pietra et al.,
1997). We skip the detailed contents and proofs.
</bodyText>
<figure confidence="0.748348042553192">
1. Let
r =
\x1a
1 if ~
p(fi) \x14 pA(fi)
1 otherwise
2. Set \x0b0 = 0
3. Repeat the following until GAfi(\x0bn) has con-
verged :
Compute \x0bn+1 from \x0bn using
\x0bn+1 = \x0bn + 1
r log(1 1
r
G0
Afi(\x0bn)
G00
Afi(\x0bn))
Compute GAfi(\x0bn+1) using
GAfi(\x0b) = P
x ~
p(x)logZ\x0b(x)+\x0b~
p(fi) ,
G0
Afi(\x0b) = ~
p(fi) P
x ~
p(x)M(x) ,
G00
Afi(\x0b) = P
x ~
p(x)p\x0b
Afi((fi M(x))2jx)
set description # of disjoint total
features events
A active features 1483 4113
P feature candidates 3172 63773
N new features 97 5503
Table 2: Summery of Features Selected
where \x0b = \x0bn+1 ,
Afi = A[fi ,
M(x) \x11 p\x0b
Afi(fijx) ,
p\x0b
Afi(fijx) \x11 P
y p\x0b
Afi(yjx)fi(x;y)
4. Set \x18 \x01L(Afi) GAfi(\x0bn)
</figure>
<bodyText confidence="0.928931333333333">
This algorithm is iteratively computed using Net-
won&amp;apos;smethod. Wecanrecognizetheimportanceofa
feature with the gain value. As mentioned above, it
meanshowmuchthefeatureaccordswiththemodel.
Weviewedthefeatureastheinformationthattk and
te occur together.
</bodyText>
<sectionHeader confidence="0.993054" genericHeader="evaluation">
6 Experimental results
</sectionHeader>
<bodyText confidence="0.979795484848485">
The total samples consists of 3,000 aligned sentence
pairs of English-Korean, which were extracted from
news on the web site of `Korea Times&amp;apos; and a maga-
zine for English learning.
In the initial step, we manually constucted the
correspondences of tag sequences with 700 POS-
tagged sentence pairs. In the supervision step,
we extracted 1,483 correct tag sequence correspon-
dences as shown in Table 2, and it work as active
features. As a feature pool, 3,172 disjoint features
of tag sequence mappings were retrieved. It is very
important to make atomic features.
We maximized \x15 of active features with respect
to total samples using improved the iterative scal-
ing algorithm. Figure 3 shows \x15i of each feature
f(tBEP+JJ;tk) 2 A. There are many correspon-
dence patterns with respect to the Englsh tag string,
`BEP+JJ&amp;apos;.
Note that p(tkjte) is computed by the exponential
model of (4) and the conditional probability is the
same with empirical probability in (7). Since the
value of p(yjx) shows the maximum likelihood, it is
proved that each \x15 was converged correctly.
p(yjx) \x11 # of (x;y) occurs in sample
number of times of x
(7)
In feature selection step, we chose useful fea-
tures with the gain threshold of 0.008. Figure
4 shows some feaures with a large gain. Among
them, tag sequences mapping including `RB&amp;apos; are er-
roneous. It means that position of adverb in Ko-
rean is very complicated to handle. Also, proper
noun in English aligned common nouns in Korean
</bodyText>
<figureCaption confidence="0.9929555">
\x0cFigure 3: \x15 of active features in A
Figure 5: Best Lexical alignment
</figureCaption>
<bodyText confidence="0.979681481481481">
because of tagging errors. Note that in the case of
`PN+PPCA2+PPAD+VBMA&amp;apos;,it isnot anadjacent
string but an interrupted string. It means that a
verb in English generally map to a verb taking as
argument the accusative and adverbial postposition
in Korean.
One way of testing usefulness of our method is
to construct structured aligned bilingual sentences.
Table 3 shows lexical alignments using tag sequence
alignments drawn from our algorithm for a given
sentence, `you usually have to take regular seating
- dangsineun dachero ilbanseoke anjayaman handa&amp;apos;
and Figure 5 shows the best lexical alignment of the
sentence.
We conducted the experiment on 100 sentences
composed of words in length 14 or less and sim-
ply chose the most likely paths. As the result, the
accuray was about 71:1%. It shows that we can
partly use the tag sequence alignments for lexical
alignments. We will extend the structural mapping
model with consideration to the lexical information.
The parameters, the conditional probabilities about
stuctural mappings will be embedded in a statisti-
cal model. Table 4 shows conditional probabilities
of some features according to `DT+NN&amp;apos;. In general,
determiner is translated into NULL or adnominal
word in Korean.
</bodyText>
<sectionHeader confidence="0.994075" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.951682333333333">
When aligning English-Korean sentences, the di\x0ber-
ences of word order and word unit require structural
information. For this reason, we tried structural tag
</bodyText>
<table confidence="0.931744">
te tk p(tkjte)
DT+NN NNIN2 0.524131
DT+NN ANDE+NNIN2 0.15161
DT+NN ANNU+NNDE2 0.091036
DT+NN NNIN2+PPCA1 0.063515
DT+NN NNIN2+NNIN2 0.058322
DT+NN NNIN2+PPAU 0.05768
DT+NN ADCO 0.049622
etc etc
</table>
<tableCaption confidence="0.99593">
Table 4: Conditional Probability
</tableCaption>
<bodyText confidence="0.9857771">
string mapping using maximum entropy modeling
and feature selection concept. We devised a model
that generates a English tag string given a Korean
tag string. From initial active structural features,
useful features are extended by feature selection.
The retrieved features and parameters can be em-
bedded in statistical machine translation and reduce
the complexity of searching. We showed that they
can helpful to construct structured aligned bilingual
sentences.
</bodyText>
<sectionHeader confidence="0.986859" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9986528125">
Adam L. Berger, Peter F. Brown, Stephen A.
Della Pietra, Vincent J. Della Pietra, John R.
Gillett, JohnD. La\x0berty, Robert L. Mercer, Harry
Printz, and Lubos Ures. 1994. The Candie sys-
tem for machine translation. In Proceedings of the
ARPA Conference on Human Language Technol-
ogy, Plainsborough, New Jersey.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39-73.
Peter F. Brown, John Cocke, Stephen A. Della
Pietra, Vincent J. Della Pietra, Fredrick Jelinek,
John D. La\x0berty, Robert L. Mercer, and Paul S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79-
</reference>
<page confidence="0.942978">
85
</page>
<reference confidence="0.9129146">
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: pa-
\x0cFigure 4: Some features with a large gain
Tag alignment Conditional Lexical alignment
</reference>
<table confidence="0.704675555555556">
PRP : PN+PPAU 0.150109 you : dangsin+eun
RB : ADCO 0.142193 usually : dachero
RB : NNIN2+PPAD 0.038105 usually : ilbanseok+e
HVP+TO : ENCO3+AX+ENTE 0.982839 have+to : ayaman+handa
VBP : PPAD+VBMA 0.050224 take : e+anj
VBP : VBMA+ENCO3+AX+ENTE 0.011110 take : anjay+aman+ha+nda
VBP : PPAD+VBMA+ENCO3+AX+ENTE 0.001851 take : e+anjayaman+handa
VBP+JJ : NNIN2+PPAD+VBMA 0.057657 take+regular : ilbanseok+e+anj
JJ+NN : NNIN2 0.581791 regular+seating : ilbanseok
</table>
<tableCaption confidence="0.925879">
Table 3: Lexical alignments using tag alignments
</tableCaption>
<reference confidence="0.996077677966102">
rameter estimation. Computational Linguistics,
19(2):263-311.
Stanley F. Chen. 1993. Aligning sentences in bilin-
gualcorporausinglexicalinformation. InProceed-
ings of ACL 31, 9-16.
A. P. Dempster, N. M. Laird and D. B. Rubin.
1976. Maximum likelihood from incomplete data
via the EM algorithm. The Royal Statistics Soci-
ety, 39(B) 205-237.
William A. Gale, Kenneth W. Church. 1993. A pro-
gram for aligning sentences in bilingual corpora.
Computational Linguistics, 19:75-102.
Frederick Jelinek. 1997. Statistical Methods for
Speech Recognition MIT Press.
Marin Kay, Martin Roscheisen. 1993. Text-
translation alignment. Computational Linguis-
tics, 19:121-142.
Julian Kupiec. 1993. An algorithm for \x0cnding noun
phrase correspondences in bilingual corpora. In
Proceedings of ACL 31, 17-22.
Yuji Matsumoto, Hiroyuki Ishimoto, Takehito Ut-
suro. 1993. Structural matching of parallel texts.
In Proceedings of ACL 31, 23-30.
I. Dan Melamed. 1997. A word-to-word model of
translation equivalence. In Proceedings of ACL
35/EACL 8, 16-23.
Franz Josef Och and Hans Weber. 1998. Improv-
ing Statistical Natural Language Translation with
Categories and Rules. In Proceedings of ACL
36/COLING, 985-989.
Stephen A. Della Pietra, Vincent J. Della Pietra,
John D. La\x0berty. 1997. Inducing features of ran-
dom \x0celds. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 19(4):380-393.
Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations
for bilingual lexicons: A statistical approach.
Computational Linguistics, 22(1):1-38.
Kengo Sato 1998. Maximum Entropy Model Learn-
ing of the Translation Rules. In Proceedings of
ACL 36/COLING, 1171-1175.
Jung H. Shin, Young S. Han, and Key-Sun
Choi. 1996. Bilingual knowledge acquisition from
Korean-English parallel corpus using alignment
method. In Proceedings of COLING 96.
C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga.
1997. A DP based search using monotone align-
ments in statistical translation. In Proceedings of
ACL 35/EACL 8, 289-296.
Ye-Yi Wang and Alex Waibel. 1997. Decoding algo-
rithm in statistical machine translation. In Pro-
ceedings of ACL 35/EACL 8, 366-372.
Ye-Yi Wang and Alex Waibel. 1998. Modeling with
structures in machine translation. In Proceedings
of ACL 36/COLING
Dekai Wu 1996. A polynomial-time algorithm for
statistical machine translation. In Proceeding of
ACL 34.
\x0c&quot;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.492752">
<title confidence="0.998477">b&quot;Structural Feature Selection For English-Korean Statistical Machine Translation</title>
<author confidence="0.996352">Seonho Kim</author>
<author confidence="0.996352">Juntae Yoon</author>
<author confidence="0.996352">Mansuk Song</author>
<email confidence="0.996289">fpobi,jtyoon,mssongg@december.yonsei.ac.kr</email>
<affiliation confidence="0.7582485">Dept. of Computer Science, Yonsei University, Seoul, Korea</affiliation>
<abstract confidence="0.9980137">When aligning texts in very di\x0berent languages such as Korean and English, structural features beyond word or phrase give useful information. In this paper, we present a method for selecting structural features of two languages, from which we construct a model that assigns the conditional probabilities to corresponding tag sequences in bilingual English- Korean corpora. For tag sequence mapping between two langauges, we \x0crst de\x0cne a structural feature function which represents statistical properties of empirical distribution of a set of training samples. Thesystem, basedonmaximumentropyconcept, selectsonlyfeaturesthatproducehighincreasesinloglikelihood of training samples. These structurally mapped featuresaremoreinformativeknowledgefor statistical machine translation between English and Korean. Also,theinformationcanhelptoreducethe parameterspaceof statisticalalignment byeliminating syntactically unlikely alignments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robert L Mercer La\x0berty</author>
<author>Harry Printz</author>
<author>Lubos Ures</author>
</authors>
<title>The Candie system for machine translation.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Conference on Human Language Technology,</booktitle>
<location>Plainsborough, New Jersey.</location>
<marker>La\x0berty, Printz, Ures, 1994</marker>
<rawString>Gillett, JohnD. La\x0berty, Robert L. Mercer, Harry Printz, and Lubos Ures. 1994. The Candie system for machine translation. In Proceedings of the ARPA Conference on Human Language Technology, Plainsborough, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="1560" citStr="Berger et al., 1996" startWordPosition="196" endWordPosition="199">apped featuresaremoreinformativeknowledgefor statistical machine translation between English and Korean. Also,theinformationcanhelptoreducethe parameterspaceof statisticalalignment byeliminating syntactically unlikely alignments. 1 Introduction Aligned texts have been used for derivation of bilingual dictionaries and terminology databases which are useful for machine translation and cross languages information retrieval. Thus, a lot of alignment techniques have been suggested at the sentence (Gale et al., 1993), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), word (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), collocation (Smadja et al., 1996) and terminology level. Some work has used lexical association measures for word alignments. However, the association measures could be misled since a word in a source language frequently co-occurs with more than one word in a target language. In other work, iterative reestimation techniques have been employed. They were usually incorporated with the EM algorithm and dynamic programming. In that case, the probabilities of alignments usually served as parameters in a model of statistical machine translation. In statistical machine translation, </context>
<context position="12555" citStr="Berger et al., 1996" startWordPosition="1922" endWordPosition="1925"> count \x0f there exist tkx, where (tei;tkx) in A and the similarity(same tag count) of tki and tkx \x15 0:6 Table 1 shows possible features, for a given aligned sentence , `take her out gnyeoreul baggeuro deryeogara&amp;apos;. Since the set of the structural features for alignment modeling is vast, we constructed a maximum entropy model for p(tkjte) by the iterative model growing method. 4 Maximum Entropy To explain our method, we brie y describe the concept of maximum entropy. Recently, many approachesbasedonthemaximumentropymodelhave been applied to natural language processing (Berger et al., 1994; Berger et al., 1996; Pietra et al., 1997). Suppose a model p which assigns a probability to a random variable. If we don&amp;apos;t have any knowledge, a reasonable solution for p is the most uniform distribution. As some knowledge to estimate the model p are added, the solution space of p are more constrainedandthemodelwouldbeclosetotheoptimal probability model. For the purpose of getting the optimal probability model, we need to maximize the uniformity under some constraints we have. Here, the constraints are related with features. A feature, fi is usually represented with a binary indicator function. The importance of</context>
<context position="17439" citStr="Berger et al., 1996" startWordPosition="2705" endWordPosition="2708">)e\x0bfi(x;y) +\x0b~ p(fi) (6) Note that a model pA has a set of parameters \x15 which means weights of features. The model pAfi contains the parameters and the new parameter \x0b with respect to the feature fi. When adding a new feature to A, the optimal values of all parameters of probability distribution change. To make the computation of feature selection tractable, we approximate that the addition of a feature fi a\x0bects only the single parameter \x0b, as shown in (5). The following algorithm is used for computing the gain of the model with respect to fi. We referred to the studies of (Berger et al., 1996; Pietra et al., 1997). We skip the detailed contents and proofs. 1. Let r = \x1a 1 if ~ p(fi) \x14 pA(fi) 1 otherwise 2. Set \x0b0 = 0 3. Repeat the following until GAfi(\x0bn) has converged : Compute \x0bn+1 from \x0bn using \x0bn+1 = \x0bn + 1 r log(1 1 r G0 Afi(\x0bn) G00 Afi(\x0bn)) Compute GAfi(\x0bn+1) using GAfi(\x0b) = P x ~ p(x)logZ\x0b(x)+\x0b~ p(fi) , G0 Afi(\x0b) = ~ p(fi) P x ~ p(x)M(x) , G00 Afi(\x0b) = P x ~ p(x)p\x0b Afi((fi M(x))2jx) set description # of disjoint total features events A active features 1483 4113 P feature candidates 3172 63773 N new features 97 5503 Table 2: </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-73. Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. La\x0berty, Robert L. Mercer, and Paul S.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<marker>Roossin, 1990</marker>
<rawString>Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79-Peter F. Brown, Stephen A. Della Pietra, Vincent J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: pa\x0cFigure 4: Some features with a large gain Tag alignment Conditional Lexical alignment rameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<marker>Pietra, Mercer, 1993</marker>
<rawString>Della Pietra, Robert L. Mercer. 1993. The mathematics of statistical machine translation: pa\x0cFigure 4: Some features with a large gain Tag alignment Conditional Lexical alignment rameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Aligning sentences in bilingualcorporausinglexicalinformation.</title>
<date>1993</date>
<journal>InProceedings of ACL</journal>
<volume>31</volume>
<pages>9--16</pages>
<marker>Chen, 1993</marker>
<rawString>Stanley F. Chen. 1993. Aligning sentences in bilingualcorporausinglexicalinformation. InProceedings of ACL 31, 9-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm. The Royal Statistics Society,</title>
<date>1976</date>
<volume>39</volume>
<pages>205--237</pages>
<marker>Dempster, Laird, Rubin, 1976</marker>
<rawString>A. P. Dempster, N. M. Laird and D. B. Rubin. 1976. Maximum likelihood from incomplete data via the EM algorithm. The Royal Statistics Society, 39(B) 205-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--75</pages>
<marker>Gale, Church, 1993</marker>
<rawString>William A. Gale, Kenneth W. Church. 1993. A program for aligning sentences in bilingual corpora. Computational Linguistics, 19:75-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<marker>Jelinek, 1997</marker>
<rawString>Frederick Jelinek. 1997. Statistical Methods for Speech Recognition MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marin Kay</author>
<author>Martin Roscheisen</author>
</authors>
<title>Texttranslation alignment.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--121</pages>
<marker>Kay, Roscheisen, 1993</marker>
<rawString>Marin Kay, Martin Roscheisen. 1993. Texttranslation alignment. Computational Linguistics, 19:121-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>An algorithm for \x0cnding noun phrase correspondences in bilingual corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL 31,</booktitle>
<pages>17--22</pages>
<contexts>
<context position="1513" citStr="Kupiec, 1993" startWordPosition="189" endWordPosition="190">of training samples. These structurally mapped featuresaremoreinformativeknowledgefor statistical machine translation between English and Korean. Also,theinformationcanhelptoreducethe parameterspaceof statisticalalignment byeliminating syntactically unlikely alignments. 1 Introduction Aligned texts have been used for derivation of bilingual dictionaries and terminology databases which are useful for machine translation and cross languages information retrieval. Thus, a lot of alignment techniques have been suggested at the sentence (Gale et al., 1993), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), word (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), collocation (Smadja et al., 1996) and terminology level. Some work has used lexical association measures for word alignments. However, the association measures could be misled since a word in a source language frequently co-occurs with more than one word in a target language. In other work, iterative reestimation techniques have been employed. They were usually incorporated with the EM algorithm and dynamic programming. In that case, the probabilities of alignments usually served as parameters in a model of statistical machine tr</context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>Julian Kupiec. 1993. An algorithm for \x0cnding noun phrase correspondences in bilingual corpora. In Proceedings of ACL 31, 17-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
</authors>
<title>Hiroyuki Ishimoto, Takehito Utsuro.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL 31,</booktitle>
<pages>23--30</pages>
<marker>Matsumoto, 1993</marker>
<rawString>Yuji Matsumoto, Hiroyuki Ishimoto, Takehito Utsuro. 1993. Structural matching of parallel texts. In Proceedings of ACL 31, 23-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>A word-to-word model of translation equivalence.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL 35/EACL 8,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="1576" citStr="Melamed, 1997" startWordPosition="200" endWordPosition="201">informativeknowledgefor statistical machine translation between English and Korean. Also,theinformationcanhelptoreducethe parameterspaceof statisticalalignment byeliminating syntactically unlikely alignments. 1 Introduction Aligned texts have been used for derivation of bilingual dictionaries and terminology databases which are useful for machine translation and cross languages information retrieval. Thus, a lot of alignment techniques have been suggested at the sentence (Gale et al., 1993), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), word (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), collocation (Smadja et al., 1996) and terminology level. Some work has used lexical association measures for word alignments. However, the association measures could be misled since a word in a source language frequently co-occurs with more than one word in a target language. In other work, iterative reestimation techniques have been employed. They were usually incorporated with the EM algorithm and dynamic programming. In that case, the probabilities of alignments usually served as parameters in a model of statistical machine translation. In statistical machine translation, IBM 1\x185 model</context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>I. Dan Melamed. 1997. A word-to-word model of translation equivalence. In Proceedings of ACL 35/EACL 8, 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hans Weber</author>
</authors>
<title>Improving Statistical Natural Language Translation with Categories and Rules.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL 36/COLING,</booktitle>
<pages>985--989</pages>
<marker>Och, Weber, 1998</marker>
<rawString>Franz Josef Och and Hans Weber. 1998. Improving Statistical Natural Language Translation with Categories and Rules. In Proceedings of ACL 36/COLING, 985-989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>John D La\x0berty</author>
</authors>
<title>Inducing features of random \x0celds.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--4</pages>
<marker>Pietra, Pietra, La\x0berty, 1997</marker>
<rawString>Stephen A. Della Pietra, Vincent J. Della Pietra, John D. La\x0berty. 1997. Inducing features of random \x0celds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380-393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
<author>Kathleen R McKeown</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="1611" citStr="Smadja et al., 1996" startWordPosition="204" endWordPosition="207">stical machine translation between English and Korean. Also,theinformationcanhelptoreducethe parameterspaceof statisticalalignment byeliminating syntactically unlikely alignments. 1 Introduction Aligned texts have been used for derivation of bilingual dictionaries and terminology databases which are useful for machine translation and cross languages information retrieval. Thus, a lot of alignment techniques have been suggested at the sentence (Gale et al., 1993), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), word (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), collocation (Smadja et al., 1996) and terminology level. Some work has used lexical association measures for word alignments. However, the association measures could be misled since a word in a source language frequently co-occurs with more than one word in a target language. In other work, iterative reestimation techniques have been employed. They were usually incorporated with the EM algorithm and dynamic programming. In that case, the probabilities of alignments usually served as parameters in a model of statistical machine translation. In statistical machine translation, IBM 1\x185 models (Brown et al., 1993) based on the</context>
</contexts>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>Frank Smadja, Kathleen R. McKeown, and Vasileios Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kengo Sato</author>
</authors>
<title>Maximum Entropy Model Learning of the Translation Rules.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL 36/COLING,</booktitle>
<pages>1171--1175</pages>
<marker>Sato, 1998</marker>
<rawString>Kengo Sato 1998. Maximum Entropy Model Learning of the Translation Rules. In Proceedings of ACL 36/COLING, 1171-1175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jung H Shin</author>
<author>Young S Han</author>
<author>Key-Sun Choi</author>
</authors>
<title>Bilingual knowledge acquisition from Korean-English parallel corpus using alignment method.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING 96.</booktitle>
<contexts>
<context position="1485" citStr="Shin et al., 1996" startWordPosition="183" endWordPosition="186">ducehighincreasesinloglikelihood of training samples. These structurally mapped featuresaremoreinformativeknowledgefor statistical machine translation between English and Korean. Also,theinformationcanhelptoreducethe parameterspaceof statisticalalignment byeliminating syntactically unlikely alignments. 1 Introduction Aligned texts have been used for derivation of bilingual dictionaries and terminology databases which are useful for machine translation and cross languages information retrieval. Thus, a lot of alignment techniques have been suggested at the sentence (Gale et al., 1993), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), word (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), collocation (Smadja et al., 1996) and terminology level. Some work has used lexical association measures for word alignments. However, the association measures could be misled since a word in a source language frequently co-occurs with more than one word in a target language. In other work, iterative reestimation techniques have been employed. They were usually incorporated with the EM algorithm and dynamic programming. In that case, the probabilities of alignments usually served as parameters in a mod</context>
</contexts>
<marker>Shin, Han, Choi, 1996</marker>
<rawString>Jung H. Shin, Young S. Han, and Key-Sun Choi. 1996. Bilingual knowledge acquisition from Korean-English parallel corpus using alignment method. In Proceedings of COLING 96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>A Zubiaga</author>
</authors>
<title>A DP based search using monotone alignments in statistical translation.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL 35/EACL 8,</booktitle>
<pages>289--296</pages>
<contexts>
<context position="2659" citStr="Tillmann et al. (1997)" startWordPosition="369" endWordPosition="372">alignments usually served as parameters in a model of statistical machine translation. In statistical machine translation, IBM 1\x185 models (Brown et al., 1993) based on the source-channel model have been widely used and revised for many language domains and applications. It has also shortcoming that it needs much iteration time for parameter estimation and high decoding complexity, however. Much work has been done to overcome the problem. Wu (1996) adopted channels that eliminate syntactically unlikely alignments and Wang et al. (1998) presented a model based on structures of two languages. Tillmann et al. (1997) suggested the dynamic programming based search to select the best alignment and preprocessed bilingual texts to remove word order di\x0berences. Sato et al. (1998) and Och et al. (1998) proposed a model for learning translation rules with morphologicalinformation and word category in order to improve statistical translation. Furthermore, many researches assumed one-toone correspondence due to the complexity and computation time of statistical alignments. Although this assumption turned out to be useful for alignment of close languages such as English and French, it is not applicable to very d</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, 1997</marker>
<rawString>C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga. 1997. A DP based search using monotone alignments in statistical translation. In Proceedings of ACL 35/EACL 8, 289-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Alex Waibel</author>
</authors>
<title>Decoding algorithm in statistical machine translation.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL 35/EACL 8,</booktitle>
<pages>366--372</pages>
<marker>Wang, Waibel, 1997</marker>
<rawString>Ye-Yi Wang and Alex Waibel. 1997. Decoding algorithm in statistical machine translation. In Proceedings of ACL 35/EACL 8, 366-372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Alex Waibel</author>
</authors>
<title>Modeling with structures in machine translation.</title>
<date>1998</date>
<journal>In Proceeding of ACL</journal>
<booktitle>In Proceedings of ACL 36/COLING Dekai Wu</booktitle>
<volume>34</volume>
<pages>0</pages>
<marker>Wang, Waibel, 1998</marker>
<rawString>Ye-Yi Wang and Alex Waibel. 1998. Modeling with structures in machine translation. In Proceedings of ACL 36/COLING Dekai Wu 1996. A polynomial-time algorithm for statistical machine translation. In Proceeding of ACL 34. \x0c&quot;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>