b"Structural Feature Selection For English-Korean Statistical
Machine Translation
Seonho Kim, Juntae Yoon, Mansuk Song
fpobi, jtyoon, mssongg@december.yonsei.ac.kr
Dept. of Computer Science,
Yonsei University, Seoul, Korea
Abstract
When aligning texts in very di\x0berent languages such
as Korean and English, structural features beyond
word or phrase give useful information. In this pa-
per, we present a method for selecting structural
features of two languages, from which we construct
a model that assigns the conditional probabilities
to corresponding tag sequences in bilingual English-
Korean corpora. For tag sequence mapping between
two langauges, we \x0crst de\x0cne a structural feature
function which represents statistical properties of
empirical distribution of a set of training samples.
Thesystem, basedonmaximumentropyconcept, se-
lectsonlyfeaturesthatproducehighincreasesinlog-
likelihood of training samples. These structurally
mapped featuresaremoreinformativeknowledgefor
statistical machine translation between English and
Korean. Also,theinformationcanhelptoreducethe
parameterspaceof statisticalalignment byeliminat-
ing syntactically unlikely alignments.
1 Introduction
Aligned texts have been used for derivation of bilin-
gual dictionaries and terminology databases which
are useful for machine translation and cross lan-
guages information retrieval. Thus, a lot of align-
ment techniques have been suggested at the sen-
tence (Gale et al., 1993), phrase (Shin et al., 1996),
noun phrase (Kupiec, 1993), word (Brown et al.,
1993; Berger et al., 1996; Melamed, 1997), collo-
cation (Smadja et al., 1996) and terminology level.
Some work has used lexical association measures
for word alignments. However, the association mea-
sures could be misled since a word in a source lan-
guage frequently co-occurs with more than one word
in a target language. In other work, iterative re-
estimation techniques have been employed. They
were usually incorporated with the EM algorithm
and dynamic programming. In that case, the prob-
abilities of alignments usually served as parameters
in a model of statistical machine translation.
In statistical machine translation, IBM 1\x185 mod-
els (Brown et al., 1993) based on the source-channel
model have been widely used and revised for many
language domains and applications. It has also
shortcoming that it needs much iteration time for
parameter estimation and high decoding complex-
ity, however.
Much work has been done to overcome the prob-
lem. Wu (1996) adopted channels that eliminate
syntactically unlikely alignments and Wang et al.
(1998) presented a model based on structures of two
languages. Tillmann et al. (1997) suggested the
dynamic programming based search to select the
best alignment and preprocessed bilingual texts to
remove word order di\x0berences. Sato et al. (1998)
and Och et al. (1998) proposed a model for learn-
ing translation rules with morphologicalinformation
and word category in order to improve statistical
translation.
Furthermore, many researches assumed one-to-
one correspondence due to the complexity and com-
putation time of statistical alignments. Although
this assumption turned out to be useful for align-
ment of close languages such as English and French,
it is not applicable to very di\x0berent languages, in
particular, Korean and English where there is rarely
close correspondence in order at the word level. For
such languages, even phrase level alignment, not to
mention word alignment, does not gives good trans-
lation due to structuraldi\x0berence. Hence, structural
features beyond word or phrase should be consid-
ered to get better translation between English and
Korean. In addition, the construction of structural
bilingual texts would be more informative for ex-
tracting linguistic knowledge.
In this paper, we suggest a method for structural
mapping of bilingual language on the basis of the
maximum entorpyand feature induction framework.
Our model based on POS tag sequence mapping has
two advantages: First, it can reduce a lot of parame-
ters in statistical machine translation by eliminating
syntactically unlikely alignments. Second, it can be
used as a preprocessorfor lexical alignmentsof bilin-
gual corpora although it can be also exploited by it-
self for alignment. In this case, it would serve as the
\x0crst step of alignment for reducing the parameter
space.
\x0c2 Motivation
In order to devise parameters for statistical model-
ing of translation, we started our research from the
IBM model which has been widely used by many
researches. The IBM model is represented with the
formula shown in (1)
p(f;aje) =
l
Y
i=1
n(\x1eijei)
m
Y
j=1
t(fjjeaj )d(jjaj;m;l)
(1)
Here, n is the fertility probability that an English
word generates n French words, t is the alignment
probability that the English word e generates the
French word f, and d is the distortion probability
that an Englishwordin acertain positionwill gener-
ateaFrenchwordinacertainposition. Thisformula
is oneof manywaysin which p(f;aje) can be written
astheproductofaseriesofconditionalprobabilities.
In above model, the distortion probability is re-
lated with positional preference(word order). Since
Korean is a free order language, the probability is
not feasible in English-Korean translation.
Furthermore, the di\x0berence between two lan-
guages leads to the discordance between words that
the one-to-one correspondence between words gen-
erally does not keep. The model (1), however, as-
sumed that an English word can be connected with
multiple French words, but that each French word
is connected to exactly one English word including
the empty word. In conclusion, many-to-many map-
pings are not allowed in this model.
According to our experiment, many-to-many
mappings exceed 40% in English and Korean lexical
alignments. Only 25.1% of them can be explained
by wordfor word correspondences. It means that we
need a statistical model which can handle phrasal
mappings.
In the case of the phrasal mappings, a lot of pa-
rameters should be searched even if we restrict the
length of word strings. Moreover, in order to prop-
erly estimate parameters we need much larger vol-
ume of bilingual aligned text than it in word-for-
word modeling. Even though such a large corpora
exist sometimes, they do not come up with the lex-
ical alignments.
For this problem, we here consider syntactic fea-
tureswhichareimportantin determiningstructures.
A structural feature means here a mapping between
tag sequences in bilingual parallel sentences.
If we are concerned with tag sequence alignments,
it is possible to estimate statistical parameters in
a relatively small size of corpora. As a result, we
can remarkably reduce the problem space for possi-
ble lexical alignments, a sort of t probability in (1),
which improve the complexity of a statistical ma-
chine translation model.
If there are similarities between corresponding tag
sequences in two language, the structural features
would be easily computed or recognized. However,
a tag sequence in English can be often translated
into a completely di\x0berent tag sequence in Korean
as follows.
can/MD ! \x18 eul/ENTR1 su/NNDE1 'iss/AJMA
da/ENTE
Itmeansthatsimilaritiesoftagfeaturesbetweentwo
languages are not kept all the time and it is neces-
sary to get the most likely tag sequence mappings
that re
ect structural correspondences between two
languages.
In this paper, the tag sequence mappings are ob-
taind by automatic feature selection based on the
maximum entropy model.
3 Problem Setting
In this chapter, we describe how the features are
related to the training data. Let te be an English
tag sequence and tk be a Korean tag sequence. Let
TS bethesetofallpossibletagsequencemappingsin
a aligned sentence, S. We de\x0cne a feature function
(or a feature) as follows:
f(te;tk) =
\x1a
1 pair(te;tk) 2 TS
0 otherwise
It indicates co-occurrence information between
tags appeared in TS. f(te;tk) expresses the infor-
mation for predicting that te maps into tk. A fea-
turemeansasortofinformationforpredictingsome-
thing. In our model, co-occurrence information on
the same aligned sentence is used for a feature, while
context is used as a feature in most of systems using
maximum entropy. It can be less informative than
context. Hence, we considered an initial supervision
and feature selection.
Our model starts with initial seed(active) features
for mapping extracted by supervision. In the next
step, feature pool is constructed from training sam-
plesfrom\x0clteringandonlyfeatureswith alargegain
to the model are added into active feature set. The
\x0cnal outputs of our model are the set of active fea-
tures, theirgainvalues, andconditionalprobabilities
of features which maximize the model. The results
can be embedded in parameters of statistical ma-
chine translation and help to construct structural
bilingual text.
Most alignment algorithm consists of two steps:
(1) estimate translation probabilities.
(2) use these probabilities to search for most proba-
ble alignment path.
Our study is focused on (1), especially the part of
tag string alignments.
Next, we will explain the concept of the model.
We are concerned with an optimal statistical model
which can generate the training samples. Namely,
our task is to construct a stochastic model that pro-
\x0cduces output tag sequence Tk, given a tag sequence
Te. The problem of interest is to use samples of
tagged sentences to observe the behavior of the ran-
dom process. The model p estimates the conditional
probability that the process will output te, given tk.
It is chosen out of a set of all allowed probability
distributions.
The following steps are employed for our model.
Input: a set L of POS-labeled bilingual aligned
sentences.
1. Make a set F of correspondence pairs of tag
sequences, (te;tk) from a small portion of L by
supervision.
2. Set F into a set of active features, A.
3. Maximization of parameters, \x15 of active fea-
tures by IIS(Improved Iterative Scaling) algo-
rithm.
4. Create a feature pool set P of all possible align-
ments a(te;tk) from tag sequences of samples.
5. Filter P using frequency and similarity with A.
6. Compute the approximate gains of features in
P.
7. Select new features(N) with a large gain value,
and add A.
Output: p(tkjte)where(te;tk) 2 A and their \x15i.
We began with training samples composed of
English-Korean aligned sentence pairs, (e,k). Since
they included long sentences, we broke them into
shorter ones. The length of training sentences was
limited to under 14 on the basis of English. It is
reasonable because we are interested in not lexical
alignments but tag sequence alignments. The sam-
ples were tagged using brill's tagger and `Morany'
that we implemented as a Korean tagger. Figure 1
shows the POS tags we considered. For simplicity,
we adjusted some part of Brill's tag set.
Inthesupervisionstep, 700alignedsentenceswere
used to construct the tag sequences mappings which
are referred to as an active feature set A. As Fig-
ure 2 shows, there are several ways in constructing
the correspondences. We chose the third mapping
although (1) can be more useful to explain Korean
with predicate-argument structure. Since a subject
of a English sentence is always used for a subject
form in Korean, we exlcuded a subject case from ar-
guments of a predicate. For example, `they' is only
used for a subject form, whereas `me' is used for a
object form and a dative form.
In the next step, training events, (te;tk) are con-
structed to make a feature pool from training sam-
ples. Theeventconsistsofatagstringte ofaEnglish
Figure 2: Tag sequence correspondences at the
phrase level
POS-tagged sentence and a tag string tk of the cor-
responding Korean POS-tagged sentence and it can
be represented with indicator functions fi(te;tk).
For a given sequence, the features were drawn
fromalladjacentpossiblepairsandsomeinterrupted
pairs. Only features (tei;tki) out of the feature pool
that meet the following conditions are extracted.
\x0f #(tei;tki) \x15 3, # is count
\x0f there exist tkx, where (tei;tkx) in A and the
similarity(same tag count) of tki and tkx \x15 0:6
Table 1 shows possible features, for a given aligned
sentence , `take her out gnyeoreul baggeuro
deryeogara'.
Since the set of the structural features for align-
ment modeling is vast, we constructed a maximum
entropy model for p(tkjte) by the iterative model
growing method.
4 Maximum Entropy
To explain our method, we brie
y describe the con-
cept of maximum entropy. Recently, many ap-
proachesbasedonthemaximumentropymodelhave
been applied to natural language processing (Berger
et al., 1994; Berger et al., 1996; Pietra et al., 1997).
Suppose a model p which assigns a probability to
a random variable. If we don't have any knowledge,
a reasonable solution for p is the most uniform dis-
tribution. As some knowledge to estimate the model
p are added, the solution space of p are more con-
strainedandthemodelwouldbeclosetotheoptimal
probability model.
For the purpose of getting the optimal probability
model, we need to maximize the uniformity under
some constraints we have. Here, the constraints are
related with features. A feature, fi is usually repre-
sented with a binary indicator function. The impor-
tance of a feature, fi can be identi\x0ced by requiring
that the model accords with it.
As a constraint, the expected value of fi with re-
spect to the model p(fi) is supposed to be the same
as the expected value of fi with respect to empirical
distribution in training samples, ~
p(fi).
\x0cFigure 1: English Tags (left) and Korean Tags (right)
English Tag Sequences Korean Tag Sequences
[VBP+IN] [take+out] [1+3] [PPCA2+PPAD+VBMA] [reul+euro+deryeoga] [2+4+5]
[VBP] [take] [1] [PN] [geunyeo] [1]
[VBP+PRP] [take+her] [1+2] [PPAD+VBMA+ENTE] [reul+euro+ deryeoga+ra] [4+5+6]
[VBP+PRP+IN] [take+her+out] [1+2+3] [NNIN2] [bagg] [3]
[PRP] [her] [2] [NNIN2+PPAD] [bagg+euro] [3+4]
[IN] [out] [3] [ENTE] [ra] [6]
[PPAD+VBMA] [euro+deryeoga] [4+5]
[PPAD+VBMA+ENTE] [euro+deryeoga+ra] [4+5+6]
[PPCA2+NNIN2+PPAD+VBMA] [reul+bagg+euro+ deryeoga] [2+3+4+5]
[PPCA2+NNIN2+PPAD+VBMA+ENTE] [reul+bagg+euro+deryeoga+ra] [2+3+4+5+6]
[PPCA2+NNIN2+PPAD+VBMA] [reul+deryeoga] [2+3+4+5]
[PPCA2+NNIN2+PPAD+VBMA+ENTE] [reul+deryeoga+ra] [2+3+4+5+6]
Table 1: possible tag sequences
In sum, the maximum entropy framework \x0cnds
the model which has highest entropy(most uniform),
given constraints. It is related to the constrained
optimization. To select a model from a constrained
set C of allowed probability distributions, the model
p? 2 C with maximum entropy H(p) is chosen.
In general, for the constrained optimization prob-
lem, Lagrange multipliers of the number of features
can be used. However, it was proved that the model
with maximum entropy is equivalent to the model
that maximizes the log likelihood of the training
sampleslike(2)if wecanassumeit asanexponential
model.
In (2), the left side is Lagrangian of the condi-
tional entropy and the right side is maximum log-
likelihood. We use the right side equation of (2) to
select \x15? for the best model p?.
argmax\x15i(
P
x;y ~
p(x)p(yjx)logp(yjx)+\x15i(p(fi) ~
p(fi))) (2)
=argmax\x15i
P
x;y ~
p(x;y)logp(yjx)
Since \x15? cannot be found analytically, we use
the following improved iterative scaling algorithm to
compute \x15? of n active features in A in total sam-
ples.
1. Start with \x15i = 0 for all i 2 f1;2;:::;ng
2. Do for each i 2 f1;2;:::;ng :
(a) Let \x01\x15i be the solution to the log likeli-
hood
(b) Update the value of \x15i into \x15i +\x01\x15i,
where \x01\x15i = log
P
x;y ~
p(x;y)fi(x;y)
P
x;y ~
p(x)p\x15(yjx)fi(x;y)
p\x15(yjx) = 1
Z\x15(x)e(
P
i \x15ifi(x;y)),
Z\x15(x) = P
y e(
P
i \x15ifi(x;y))
3. Stop if not all the \x15i have converged, otherwise
go to step 2
Theexponentialmodelisrepresentedas(3). Here,
\x15i is the weight of feature fi. In our model, since
only one feature is applied to each pair of x and y,
it can be represented as (4) and fi is the feature
related with x and y.
~
p(yjx) =
P
i e\x15ifi(x;y)
P
y e
P
i \x15ifi(x;y) (3)
~
p(yjx) = e\x15ifi(x;y)
P
y e\x15ifi(x;y) (4)
\x0c5 Feature selection
Only a small subset of features will be employed in
a model by selecting useful features from the feature
pool P. Let pA be the optimal model constrained
by a set of active features A and A[fi be Afi. Let
pAfi be the optimal model in the space of probabil-
ity distribution C(Afi). The optimal model can be
represented as (5). Here, the optimal model means
a maximum entropy model.
p\x0b
Afi = 1
Z\x0b(x)pA(yjx)e\x0bfi(x;y)
Z\x0b(x) =
X
y
pA(yjx)e\x0bfi(x;y) (5)
The improvement of the model regarding the ad-
dition of asinglefeaturefi can be estimatedby mea-
suring the di\x0berence of maximum log-likelihood be-
tween L(pAfi) and L(pA). We denote the gain of
feature fi by \x01(Afi) and it can be represented in
(6).
\x01(Afi) \x11 max\x0bGAfi(\x0b)
GAfi(\x0b) \x11 L(pAfi) L(pA)
=
X
x
~
p(x)
X
y
pA(yjx)e\x0bfi(x;y)
+\x0b~
p(fi) (6)
Note that a model pA has a set of parameters \x15
which means weights of features. The model pAfi
contains the parameters and the new parameter \x0b
with respect to the feature fi. When adding a new
feature to A, the optimal values of all parameters of
probability distribution change. To make the com-
putation of feature selection tractable, we approxi-
mate that the addition of a feature fi a\x0bects only
the single parameter \x0b, as shown in (5).
The following algorithm is used for computing the
gain of the model with respect to fi. We referred
to the studies of (Berger et al., 1996; Pietra et al.,
1997). We skip the detailed contents and proofs.
1. Let
r =
\x1a
1 if ~
p(fi) \x14 pA(fi)
1 otherwise
2. Set \x0b0 = 0
3. Repeat the following until GAfi(\x0bn) has con-
verged :
Compute \x0bn+1 from \x0bn using
\x0bn+1 = \x0bn + 1
r log(1 1
r
G0
Afi(\x0bn)
G00
Afi(\x0bn))
Compute GAfi(\x0bn+1) using
GAfi(\x0b) = P
x ~
p(x)logZ\x0b(x)+\x0b~
p(fi) ,
G0
Afi(\x0b) = ~
p(fi) P
x ~
p(x)M(x) ,
G00
Afi(\x0b) = P
x ~
p(x)p\x0b
Afi((fi M(x))2jx)
set description # of disjoint total
features events
A active features 1483 4113
P feature candidates 3172 63773
N new features 97 5503
Table 2: Summery of Features Selected
where \x0b = \x0bn+1 ,
Afi = A[fi ,
M(x) \x11 p\x0b
Afi(fijx) ,
p\x0b
Afi(fijx) \x11 P
y p\x0b
Afi(yjx)fi(x;y)
4. Set \x18 \x01L(Afi) GAfi(\x0bn)
This algorithm is iteratively computed using Net-
won'smethod. Wecanrecognizetheimportanceofa
feature with the gain value. As mentioned above, it
meanshowmuchthefeatureaccordswiththemodel.
Weviewedthefeatureastheinformationthattk and
te occur together.
6 Experimental results
The total samples consists of 3,000 aligned sentence
pairs of English-Korean, which were extracted from
news on the web site of `Korea Times' and a maga-
zine for English learning.
In the initial step, we manually constucted the
correspondences of tag sequences with 700 POS-
tagged sentence pairs. In the supervision step,
we extracted 1,483 correct tag sequence correspon-
dences as shown in Table 2, and it work as active
features. As a feature pool, 3,172 disjoint features
of tag sequence mappings were retrieved. It is very
important to make atomic features.
We maximized \x15 of active features with respect
to total samples using improved the iterative scal-
ing algorithm. Figure 3 shows \x15i of each feature
f(tBEP+JJ;tk) 2 A. There are many correspon-
dence patterns with respect to the Englsh tag string,
`BEP+JJ'.
Note that p(tkjte) is computed by the exponential
model of (4) and the conditional probability is the
same with empirical probability in (7). Since the
value of p(yjx) shows the maximum likelihood, it is
proved that each \x15 was converged correctly.
p(yjx) \x11 # of (x;y) occurs in sample
number of times of x
(7)
In feature selection step, we chose useful fea-
tures with the gain threshold of 0.008. Figure
4 shows some feaures with a large gain. Among
them, tag sequences mapping including `RB' are er-
roneous. It means that position of adverb in Ko-
rean is very complicated to handle. Also, proper
noun in English aligned common nouns in Korean
\x0cFigure 3: \x15 of active features in A
Figure 5: Best Lexical alignment
because of tagging errors. Note that in the case of
`PN+PPCA2+PPAD+VBMA',it isnot anadjacent
string but an interrupted string. It means that a
verb in English generally map to a verb taking as
argument the accusative and adverbial postposition
in Korean.
One way of testing usefulness of our method is
to construct structured aligned bilingual sentences.
Table 3 shows lexical alignments using tag sequence
alignments drawn from our algorithm for a given
sentence, `you usually have to take regular seating
- dangsineun dachero ilbanseoke anjayaman handa'
and Figure 5 shows the best lexical alignment of the
sentence.
We conducted the experiment on 100 sentences
composed of words in length 14 or less and sim-
ply chose the most likely paths. As the result, the
accuray was about 71:1%. It shows that we can
partly use the tag sequence alignments for lexical
alignments. We will extend the structural mapping
model with consideration to the lexical information.
The parameters, the conditional probabilities about
stuctural mappings will be embedded in a statisti-
cal model. Table 4 shows conditional probabilities
of some features according to `DT+NN'. In general,
determiner is translated into NULL or adnominal
word in Korean.
7 Conclusion
When aligning English-Korean sentences, the di\x0ber-
ences of word order and word unit require structural
information. For this reason, we tried structural tag
te tk p(tkjte)
DT+NN NNIN2 0.524131
DT+NN ANDE+NNIN2 0.15161
DT+NN ANNU+NNDE2 0.091036
DT+NN NNIN2+PPCA1 0.063515
DT+NN NNIN2+NNIN2 0.058322
DT+NN NNIN2+PPAU 0.05768
DT+NN ADCO 0.049622
etc etc
Table 4: Conditional Probability
string mapping using maximum entropy modeling
and feature selection concept. We devised a model
that generates a English tag string given a Korean
tag string. From initial active structural features,
useful features are extended by feature selection.
The retrieved features and parameters can be em-
bedded in statistical machine translation and reduce
the complexity of searching. We showed that they
can helpful to construct structured aligned bilingual
sentences.
References
Adam L. Berger, Peter F. Brown, Stephen A.
Della Pietra, Vincent J. Della Pietra, John R.
Gillett, JohnD. La\x0berty, Robert L. Mercer, Harry
Printz, and Lubos Ures. 1994. The Candie sys-
tem for machine translation. In Proceedings of the
ARPA Conference on Human Language Technol-
ogy, Plainsborough, New Jersey.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39-73.
Peter F. Brown, John Cocke, Stephen A. Della
Pietra, Vincent J. Della Pietra, Fredrick Jelinek,
John D. La\x0berty, Robert L. Mercer, and Paul S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79-
85
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: pa-
\x0cFigure 4: Some features with a large gain
Tag alignment Conditional Lexical alignment
PRP : PN+PPAU 0.150109 you : dangsin+eun
RB : ADCO 0.142193 usually : dachero
RB : NNIN2+PPAD 0.038105 usually : ilbanseok+e
HVP+TO : ENCO3+AX+ENTE 0.982839 have+to : ayaman+handa
VBP : PPAD+VBMA 0.050224 take : e+anj
VBP : VBMA+ENCO3+AX+ENTE 0.011110 take : anjay+aman+ha+nda
VBP : PPAD+VBMA+ENCO3+AX+ENTE 0.001851 take : e+anjayaman+handa
VBP+JJ : NNIN2+PPAD+VBMA 0.057657 take+regular : ilbanseok+e+anj
JJ+NN : NNIN2 0.581791 regular+seating : ilbanseok
Table 3: Lexical alignments using tag alignments
rameter estimation. Computational Linguistics,
19(2):263-311.
Stanley F. Chen. 1993. Aligning sentences in bilin-
gualcorporausinglexicalinformation. InProceed-
ings of ACL 31, 9-16.
A. P. Dempster, N. M. Laird and D. B. Rubin.
1976. Maximum likelihood from incomplete data
via the EM algorithm. The Royal Statistics Soci-
ety, 39(B) 205-237.
William A. Gale, Kenneth W. Church. 1993. A pro-
gram for aligning sentences in bilingual corpora.
Computational Linguistics, 19:75-102.
Frederick Jelinek. 1997. Statistical Methods for
Speech Recognition MIT Press.
Marin Kay, Martin Roscheisen. 1993. Text-
translation alignment. Computational Linguis-
tics, 19:121-142.
Julian Kupiec. 1993. An algorithm for \x0cnding noun
phrase correspondences in bilingual corpora. In
Proceedings of ACL 31, 17-22.
Yuji Matsumoto, Hiroyuki Ishimoto, Takehito Ut-
suro. 1993. Structural matching of parallel texts.
In Proceedings of ACL 31, 23-30.
I. Dan Melamed. 1997. A word-to-word model of
translation equivalence. In Proceedings of ACL
35/EACL 8, 16-23.
Franz Josef Och and Hans Weber. 1998. Improv-
ing Statistical Natural Language Translation with
Categories and Rules. In Proceedings of ACL
36/COLING, 985-989.
Stephen A. Della Pietra, Vincent J. Della Pietra,
John D. La\x0berty. 1997. Inducing features of ran-
dom \x0celds. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 19(4):380-393.
Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations
for bilingual lexicons: A statistical approach.
Computational Linguistics, 22(1):1-38.
Kengo Sato 1998. Maximum Entropy Model Learn-
ing of the Translation Rules. In Proceedings of
ACL 36/COLING, 1171-1175.
Jung H. Shin, Young S. Han, and Key-Sun
Choi. 1996. Bilingual knowledge acquisition from
Korean-English parallel corpus using alignment
method. In Proceedings of COLING 96.
C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga.
1997. A DP based search using monotone align-
ments in statistical translation. In Proceedings of
ACL 35/EACL 8, 289-296.
Ye-Yi Wang and Alex Waibel. 1997. Decoding algo-
rithm in statistical machine translation. In Pro-
ceedings of ACL 35/EACL 8, 366-372.
Ye-Yi Wang and Alex Waibel. 1998. Modeling with
structures in machine translation. In Proceedings
of ACL 36/COLING
Dekai Wu 1996. A polynomial-time algorithm for
statistical machine translation. In Proceeding of
ACL 34.
\x0c"