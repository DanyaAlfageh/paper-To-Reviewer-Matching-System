Notice that some authors (CITATION; CITATION) have used the structural information, but have not used the information given by the labels on the edges as we do.,,
A sub j-verb 3This information can be produced by a functional dependency grammar (FDG), which assigns each word a specificfunction, and then structures the sentence hierarchically based on it, as we do here CITATION, but can also be gen,,
The most successful update rule is a variant of Littlestone\'s Winnow update rule CITATION, a multiplicative update rule that is tailored to the situation in which the set of input features is not known a priori, as in the infinite attribute model CITATION.,,
Machine learning based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (CITATION; CITATION; CITATION).,,
Efforts in this directions consists of (1) directly adding syntactic information, as in (CITATION; CITATION), and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used,,
ine learning based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (CITATION; CITATION; CITATION).,,
Efforts in this directions consists of (1) directly adding syntactic information, as in (CITATION; CITATION), and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sparsity), additional information compiled into a similarity measure is used CITATION.,,
Nevertheless, the efforts in this direction so far have shown very insignificant improvements, if any (CITATION; CITATION).,,
Some of the subtleties in defining the output representation are addressed in CITATION.,,
Efforts in this directions consists of (1) directly adding syntactic information, as in (CITATION; CITATION), and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sparsity), additional information compiled into a similarity measure is used CITATION.,,
Nevertheless, the efforts in this direction so far have shown very insignificant improvements, if any (CITATION; CITATION).,,
Studies have shown that both machine learning and probabilistic learning methods used in NLP make decisions using a linear decision surface over the feature space (CITATION; CITATION).,,
We chose the verb prediction task which is similar to other word prediction tasks (e.g.,CITATION) and, in particular, follows the paradigm in (CITATION; CITATION; CITATION).,,
Similarity NB SNoW 54.6% 59.1% WSJ data AP news 47.6% Table 2: Comparison of the improvement achieved using similarity methods CITATION and using the methods presented in this paper.,,
Table 2 compares our method to methods that use similarity measures (CITATION; CITATION).,,
Furthermore, we train using only 73,184 examples while CITATION train using 587, 833 examples.,,
3 The Learning Approach Our experimental investigation is done using the SNo W learning system CITATION.,,
Earlier versions of SNoW (CITATION; CITATION; CITATION; CITATION) have been applied successfully to several natural language related tasks.,,
We chose the verb prediction task which is similar to other word prediction tasks (e.g.,CITATION) and, in particular, follows the paradigm in (CITATION; CITATION; CITATION).,,
Furthermore, we train using only 73,184 examples while CITATION train using 587, 833 examples.,,
The set of candidate words is called the confusion set CITATION.,,
Background The most influential problem in motivating statistical learning application in NLP tasks is that of word selection in speech recognition CITATION.,,
g based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (CITATION; CITATION; CITATION).,,
Efforts in this directions consists of (1) directly adding syntactic information, as in (CITATION; CITATION), and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sparsity), additional information compiled into a similarity measure is used CITATION.,,
Nevertheless, the efforts in this direction so far have shown very insignificant improvements, if any (CITATION; CITATION).,,
We chose the verb prediction task which is similar to other word prediction tasks (e.g.,CITATION) and, in particular, follows the paradigm in (CITATION; CITATION; CITATION).,,
We chose the verb prediction task which is similar to other word prediction tasks (e.g.,CITATION) and, in particular, follows the paradigm in (CITATION; CITATION; CITATION).,,
Similarity NB SNoW 54.6% 59.1% WSJ data AP news 47.6% Table 2: Comparison of the improvement achieved using similarity methods CITATION and using the methods presented in this paper.,,
Table 2 compares our method to methods that use similarity measures (CITATION; CITATION).,,
Furthermore, we train using only 73,184 examples while CITATION train using 587, 833 examples.,,
The most successful update rule is a variant of Littlestone\'s Winnow update rule CITATION, a multiplicative update rule that is tailored to the situation in which the set of input features is not known a priori, as in the infinite attribute model CITATION.,,
3 The Learning Approach Our experimental investigation is done using the SNo W learning system CITATION.,,
Earlier versions of SNoW (CITATION; CITATION; CITATION; CITATION) have been applied successfully to several natural language related tasks.,,
Machine learning based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (CITATION; CITATION; CITATION).,,
Efforts in this directions consists of (1) directly adding syntactic information, as in (CITATION; CITATION), and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sparsity), additional informa,,
iers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (CITATION; CITATION; CITATION).,,
Efforts in this directions consists of (1) directly adding syntactic information, as in (CITATION; CITATION), and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sparsity), additional information compiled into a similarity measure is used CITATION.,,
Nevertheless, the efforts in this direction so far have shown very insignificant improvements, if any (CITATION; CITATION).,,
3 The Learning Approach Our experimental investigation is done using the SNo W learning system CITATION.,,
Earlier versions of SNoW (CITATION; CITATION; CITATION; CITATION) have been applied successfully to several natural language related tasks.,,
y cannot be used (due to data sparsity), additional information compiled into a similarity measure is used CITATION.,,
Nevertheless, the efforts in this direction so far have shown very insignificant improvements, if any (CITATION; CITATION).,,
Studies have shown that both machine learning and probabilistic learning methods used in NLP make decisions using a linear decision surface over the feature space (CITATION; CITATION).,,
Most efficient learning methods known today and, in particular, those used in NLP, make use of a linear decision surface over their feature space (CITATION; CITATION).,,
3 The Learning Approach Our experimental investigation is done using the SNo W learning system CITATION.,,
Earlier versions of SNoW (CITATION; CITATION; CITATION; CITATION) have been applied successfully to several natural language related tasks.,,
used (due to data sparsity), additional information compiled into a similarity measure is used CITATION.,,
Nevertheless, the efforts in this direction so far have shown very insignificant improvements, if any (CITATION; CITATION).,,
Studies have shown that both machine learning and probabilistic learning methods used in NLP make decisions using a linear decision surface over the feature space (CITATION; CITATION).,,
Most efficient learning methods known today and, in particular, those used in NLP, make use of a linear decision surface over their feature space (CITATION; CITATION).,,
3 The Learning Approach Our experimental investigation is done using the SNo W learning system CITATION.,,
Earlier versions of SNoW (CITATION; CITATION; CITATION; CITATION) have been applied successfully to several natural language related tasks.,,
We chose the verb prediction task which is similar to other word prediction tasks (e.g.,CITATION) and, in particular, follows the paradigm in (CITATION; CITATION; CITATION).,,
Furthermore, we train using only 73,184 examples while CITATION train using 587, 833 examples.,,
The set of candidate words is called the confusion set CITATION.,,
ome authors (CITATION; CITATION) have used the structural information, but have not used the information given by the labels on the edges as we do.,,
A sub j-verb 3This information can be produced by a functional dependency grammar (FDG), which assigns each word a specificfunction, and then structures the sentence hierarchically based on it, as we do here CITATION, but can also be generated by an external rule-based parser or a learned one.,,
The training and the test data were processed by the FDG parser CITATION.,,
Machine learning based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (CITATION; CITATION; CITATION).,,
Efforts in this directions consists of (1) directly adding syntactic information, as in (CITATION; CITATION), and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sp,,
Notice that some authors (CITATION; CITATION) have used the structural information, but have not used the information given by the labels on the edges as we do.,,
