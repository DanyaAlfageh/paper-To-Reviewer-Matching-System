<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000249">
<title confidence="0.845952">
b&amp;apos;Probabilistic Parsing and Psychological Plausibility
</title>
<author confidence="0.962697">
Thorsten Brants and Matthew Crocker
</author>
<affiliation confidence="0.978915">
Saarland University, Computational Linguistics
</affiliation>
<address confidence="0.834157">
D-66041 Saarbr\x7f
ucken, Germany
</address>
<email confidence="0.968329">
fbrants,crockerg@coli.uni-sb.de
</email>
<sectionHeader confidence="0.989131" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997012636363636">
Given the recent evidence for probabilistic
mechanisms in models of human ambiguity res-
olution, this paper investigates the plausibil-
ity of exploiting current wide-coverage, prob-
abilistic parsing techniques to model human
linguistic performance. In particular, we in-
vestigate the performance of standard stochas-
tic parsers when they are revised to operate
incrementally, and with reduced memory re-
sources. We present techniques for ranking
and \x0cltering analyses, together with experimen-
tal results. Our results con\x0crm that stochas-
tic parsers which adhere to these psychologi-
cally motivated constraints achieve good per-
formance. Memory can be reduced down to
1% (compared to exhausitve search) without re-
ducing recall and precision. Additionally, these
models exhibit substantially faster performance.
Finally, we argue that this general result is likely
to hold for more sophisticated, and psycholin-
guistically plausible, probabilistic parsing mod-
els.
</bodyText>
<sectionHeader confidence="0.997685" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9992532625">
Language engineering and computational psy-
cholinguistics are often viewed as distinct re-
search programmes: engineering solutions aim
at practical methods which can achieve good
performance, typically paying little attention
to linguistic or cognitive modelling. Compu-
tational psycholinguistics, on the other hand,
is often focussed on detailed modelling of hu-
man behaviour for a relatively small number
of well-studied constructions. In this paper we
suggest that, broadly, the human sentence pro-
cessing mechanism (HSPM) and current statis-
tical parsing technology can be viewed as having
similar objectives: to optimally (i.e. rapidly and
accurately) understand the text and utterances
they encounter.
Our aim is to show that large scale probabilis-
tic parsers, when subjected to basic cognitive
constraints, can still achieve high levels of pars-
ing accuracy. If successful, this will contribute
to a plausible explanation of the fact that peo-
ple, in general, are also extremely accurate and
robust. Such a result would also strengthen ex-
isting results showing that related probabilistic
mechanisms can explain speci\x0cc psycholinguis-
tic phenomena.
To investigate this issue, we construct a stan-
dard \&amp;apos;baseline\&amp;apos; stochastic parser, which mir-
rors the performance of a similar systems (e.g.
(Johnson, 1998)). We then consider an incre-
mental version of the parser, and evaluate the
e\x0bects of several probabilistic \x0cltering strate-
gies which are used to prune the parser\&amp;apos;s search
space, and thereby reduce memory load.
To assess the generality of our results for
more sophisticated probabilistic models, we also
conduct experiments using a model in which
parent-node information is encoded on the
daughters. This increase in contextual informa-
tion has been shown to improve performance
(Johnson, 1998), and the model is also shown
to be robust to the incrementality and memory
constraints investigated here.
We present the results of parsing perfor-
mance experiments, showing the accuracy of
these systems with respect to both a parsed
corpus and the baseline parser. Our experi-
ments suggest that a strictly incremental model,
in which memory resources are substantially
reduced through \x0cltering, can achieve preci-
sion and recall which equals that of \&amp;apos;unre-
stricted\&amp;apos; systems. Furthermore, implementa-
tion of these restrictions leads to substantially
faster performance. In conclusion, we argue
that such broad-coverage probabilistic parsing
\x0cmodels provide a valuable framework for ex-
plaining the human capacity to rapidly, accu-
rately, and robustly understand \\garden va-
riety&amp;quot; language. This lends further support
to psycholinguistic accounts which posit proba-
bilistic ambiguity resolution mechanisms to ex-
plain \\garden path&amp;quot; phenomena.
It is important to reiterate that our intention
here is only to investigate the performance of
probabilistic parsers under psycholinguistically
motivated constraints. We do not argue for the
psychological plausibility of SCFG parsers (or
the parent-encoded variant) per se. Our inves-
tigation of these models was motivated rather
by our desire to obtain a generalizable result
for these simple and well-understood models,
since obtaining similar results for more sophisti-
cated models (e.g. (Collins, 1996; Ratnaparkhi,
1997)) might have been attributed to special
properties of these models. Rather, the current
result should be taken as support for the poten-
tial scaleability and performance of probabilistic
psychological models such as those proposed by
(Jurafsky, 1996) and (Crocker and Brants, to
appear).
</bodyText>
<sectionHeader confidence="0.993495" genericHeader="method">
2 Psycholinguistic Motivation
</sectionHeader>
<bodyText confidence="0.999629215384615">
Theories of human sentence processing have
largely been shaped by the study of pathologies
in human language processing behaviour. Most
psycholinguistic models seek to explain the dif-
\x0cculty people have in comprehending structures
that are ambiguous or memory-intensive (see
(Crocker, 1999) for a recent overview). While
often insightful, this approach diverts attention
from the fact that people are in fact extremely
accurate and e\x0bective in understanding the
vast majority of their \\linguistic experience&amp;quot;.
This observation, combined with the mounting
psycholinguistic evidence for statistically-based
mechanisms, leads us to investigate the merit of
exploiting robust, broad coverage, probabilistic
parsing systems as models of human linguistic
performance.
The view that human language processing
can be viewed as an optimally adapted sys-
tem, within a probabilistic framework, is ad-
vanced by (Chater et al., 1998), while (Juraf-
sky, 1996) has proposed a speci\x0cc probabilis-
tic parsing model of human sentence process-
ing. In work on human lexical category dis-
ambiguation, (Crocker and Corley, to appear),
have demonstrated that a standard (incremen-
tal) HMM-based part-of-speech tagger mod-
els the \x0cnding from a range of psycholinguis-
tic experiments. In related research, (Crocker
and Brants, 1999) present evidence that an
incremental stochastic parser based on Cas-
caded Markov Models (Brants, 1999) can ac-
count for a range of experimentally observed
local ambiguity preferences. These include
NP/S complement ambiguities, reduced relative
clauses, noun-verb category ambiguities, and
\&amp;apos;that\&amp;apos;-ambiguities (where \&amp;apos;that\&amp;apos; can be either a
complementizer or a determiner) (Crocker and
Brants, to appear).
Crucially, however, there are di\x0berences be-
tween the classes of mechanisms which are psy-
chologically plausible, and those which prevail
in current language technology. We suggest that
two of the most important di\x0berences concern
incrementality, and memory resources. There is
overwhelming experimental evidence that peo-
ple construct connected (i.e. semantically in-
terpretable) analyses for each initial substring
of an utterance, as it is encountered. That is,
processing takes place incrementally, from left
to right, on a word by word basis.
Secondly, it is universally accecpted that peo-
ple can at most consider a relatively small
number of competing analyses (indeed, some
would argue that number is one, i.e. process-
ing is strictly serial). In contrast, many exist-
ing stochastic parsers are \\unrestricted&amp;quot;, in that
they are optimised for accuracy, and ignore such
psychologically motivated constraints. Thus the
appropriateness of using broad-coverage proba-
bilistic parsers to model the high level of hu-
man performance is contingent upon being able
to maintain these levels of accuracy when the
constraints of incrementality and resource limi-
tations are imposed.
</bodyText>
<sectionHeader confidence="0.7752845" genericHeader="method">
3 Incremental Stochastic
Context-Free Parsing
</sectionHeader>
<bodyText confidence="0.994706333333333">
The following assumes that the reader is fa-
miliar with stochastic context-free grammars
(SCFG) and stochastic chart-parsing tech-
niques. A good introduction can be found, e.g.,
in (Manning and Sch\x7f
utze, 1999). We use stan-
dard abbreviations for terminial nodes, non-
terminal nodes, rules and probabilities.
\x0cThis paper investigates stochastic context-
free parsing based on a grammar that is derived
from a treebank, starting with part-of-speech
tags as terminals. The grammar is derived by
collecting all rules X ! \x0b that occur in the tree-
bank and their frequencies f. The probability
of a rule is set to
</bodyText>
<equation confidence="0.999714833333333">
P(X ! \x0b) =
f(X ! \x0b)
P
\x0c
f(X ! \x0b)
(1)
</equation>
<bodyText confidence="0.999342435897436">
For a description of treebank grammars see
(Charniak, 1996). The grammar does not con-
tain \x0f-rules, otherwise there is no restriction
on the rules. In particular, we do not require
Chomsky-Normal-Form.
In addition to the rules that correspond
to structures in the corpus, we add a new
start symbol ROOT to the grammar and rules
ROOT ! X for all non-terminals X together
with probabilities derived from the root nodes
in the corpus1.
For parsing these grammars, we rely upon
a standard bottom-up chart-parsing technique
with a modi\x0ccation for incremental parsing, i.e.,
for each word, all edges are processed and possi-
bly pruned before proceeding to the next word.
The outline of the algorithm is as follows.
A chart entry E consists of a start and end po-
sition i and j, a dotted rule X ! \x0b:
, the inside
probability \x0c(Xi;j) that X generates the termi-
nal string from position i to j, and information
about the most probable inside structure. If the
dot of the dotted rule is at the rightmost posi-
tion, the corresponding edge is an inactive edge.
If the dot is at any other position, it is an active
edge. Inactive edges represent recognized hypo-
thetical constituents, while active edges repre-
sent pre\x0cxes of hypothetical constituents.
The ith terminal node ti that enters the chart
generates an inactive edge for the span (i 1;i).
Based on this, new active and inactive edges are
generated according to the standard algorithm.
Since we are interested in the most probable
parse, the chart can be minimized in the fol-
lowing way while still performing an exhaustive
search. If there is more than one edge that cov-
ers a span (i;j) having the same non-terminal
symbol on the left-hand side of the dotted rule,
</bodyText>
<page confidence="0.893423">
1
</page>
<bodyText confidence="0.9717135">
The ROOT node is used internally for parsing; it is
neither emitted nor counted for recall and precision.
only the one with the highest inside probability
is kept in the chart. The others cannot con-
tribute to the most probable parse.
For an inactive edge spanning i to j and rep-
resenting the rule X ! Y 1 :::Y k, the inside
probability \x0cI is set to
</bodyText>
<equation confidence="0.999286">
\x0cI(Xi;j) = P(X ! Y1 :::Yk)
k
Y
l=1
\x0cI(Y l
il;jl
) (2)
</equation>
<bodyText confidence="0.99482425">
where il and jl mark the start and end postition
of Y l, having i = i1 and j = jl. The inside
probability for an active edge \x0cA with the dot
after the kth symbol of the right-hand side is
</bodyText>
<equation confidence="0.944715625">
set to
\x0cA(Xi;j) =
k
Y
l=1
\x0cI(Y k
il;jl
) (3)
</equation>
<bodyText confidence="0.999230090909091">
We do not use the probability of the rule at this
point. This allows us to combine all edges with
the same span and the dot at the same position
but with di\x0berent symbols on the left-hand side.
Introducing a distinguished left-hand side only
for inactive edges signi\x0ccantly reduces the num-
ber of active edges in the chart. This goes one
step further than implicitly right-binarizing the
grammar; not only suxes of right-hand sides
are joined, but also the corresponding left-hand
sides.
</bodyText>
<sectionHeader confidence="0.996917" genericHeader="method">
4 Memory Restrictions
</sectionHeader>
<bodyText confidence="0.998681714285715">
We investigate the elimination (pruning) of
edges from the chart in our incremental pars-
ing scheme. After processing a word and before
proceeding to the next word during incremental
parsing, low ranked edges are removed. This is
equivalent to imposing memory restrictions on
the processing system.
The original algorithm keeps one edge in the
chart for each combination of span (start and
end position) and non-terminal symbol (for in-
active edges) or right-hand side pre\x0cxes of dot-
ted rules (for active edges). With pruning, we
restrict the number of edges allowed per span.
The limitation can be expressed in two ways:
</bodyText>
<listItem confidence="0.761912">
1. Variable beam. Select a threshold \x12 \x15 1.
</listItem>
<bodyText confidence="0.804992">
Edge e is removed, if its probability is pe,
the best probability for the span is p1, and
</bodyText>
<equation confidence="0.94619">
pe &amp;lt;
p1
\x12
: (4)
</equation>
<bodyText confidence="0.99868675862069">
\x0c2. Fixed beam. Select a maximum number of
edges per span m. An edge e is removed, if
its probability is not in the \x0crst m highest
probabilities for edges with the same span.
We performed experiments using both types
of beams. Fixed beams yielded consistently bet-
ter results than variable beams when plotting
chart size vs. F-score. Therefore, the following
results are reported for \x0cxed beams.
We compare and rank edges covering the
same span only, and we rank active and inactive
edges separately. This is in contrast to (Char-
niak et al., 1998) who rank all edges. They
use normalization in order to account for dif-
ferent spans since in general, edges for longer
spans involve more multiplications of probabil-
ities, yielding lower probabilities. Charniak et
al.\&amp;apos;s normalization value is calculated by a dif-
ferent probability model than the inside proba-
bilities of the edges. So, in addition to the nor-
malization for di\x0berent span lengths, they need
a normalization constant that accounts for the
di\x0berent probability models.
This investigation is based on a much simpler
ranking formula. We use what can be described
as the unigram probability of a non-terminal
node, i.e., the a priori probability of the cor-
responding non-terminal symbol(s) times the
inside probability. Thus, for an inactive edge
</bodyText>
<equation confidence="0.972079333333333">
hi;j;X ! \x0b;\x0cI(Xi;j)i, we use the probability
PRI(Xi;j) = P(X) \x01P(ti :::tj 1jX) (5)
= P(X) \x01\x0cI(Xi;j) (6)
</equation>
<bodyText confidence="0.999727636363636">
for ranking. This is the probability of the node
and its yield being present in a parse. The
higher this value, the better is this node. \x0cI is
the inside probability for inactive edges as given
in equation 2, P(X) is the a priori probability
for non-terminal X, (as estimated from the fre-
quency in the training corpus) and PRI is the
probability of the edge for the non-terminal X
spanning positions i to j that is used for rank-
ing.
For an active edge hi;j;X ! Y 1 \x01\x01\x01Y k:
</bodyText>
<equation confidence="0.927461789473684">
Y k+1 \x01\x01\x01Y m;\x0cA(Y 1
i1;j1
\x01\x01\x01Y k
ik;jk
)i (the dot is af-
ter the kth symbol of the right-hand side) we
use:
PRA(Y 1
i1;j1
:::Y k
ik;jk
) (7)
= P(Y 1 \x01\x01\x01Y k) \x01P(ti :::tj 1jY 1 \x01\x01\x01Y k)(8)
= P(Y 1 \x01\x01\x01Y k) \x01\x0cA(Y 1
i1;j1
:::Y k
ik;jk
) (9)
P(Y 1 \x01\x01\x01Y k) can be read o\x0b the corpus. It is
</equation>
<bodyText confidence="0.89903">
the a priori probability that the right-hand side
of a production has the pre\x0cx Y 1 :::Y k, which
is estimated by
</bodyText>
<equation confidence="0.979445">
f(Y 1 \x01\x01\x01Y k is pre\x0cx)
N
(10)
</equation>
<bodyText confidence="0.997929666666667">
where N is the total number of productions in
the corpus2, i = i1, j = jk and \x0cA is the inside
probability of the pre\x0cx.
</bodyText>
<sectionHeader confidence="0.997094" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.788736">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.998450222222222">
We use sections 2 { 21 of the Wall Street Jour-
nal part of the Penn Treebank (Marcus et al.,
1993) to generate a treebank grammar. Traces,
functional tags and other tag extensions that do
not mark syntactic category are removed before
training3. No other modi\x0ccations are made. For
testing, we use the 1578 sentences of length 40
or less of section 22. The input to the parser is
the sequence of part-of-speech tags.
</bodyText>
<subsectionHeader confidence="0.998056">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999703">
For evaluation, we use the parseval measures
and report labeld F-score (the harmonic mean
of labeled recall and labeled precision). Report-
ing the F-score makes our results comparable to
those of other previous experiments using the
same data sets. As a measure of the amount
of work done by the parser, we report the size
of the chart. The number of active and inac-
tive edges that enter the chart is given for the
exhaustive search, not counting those hypothet-
ical edges that are replaced or rejected because
there is an alternative edge with higher proba-
bility4. For pruned search, we give the percent-
age of edges required.
</bodyText>
<subsectionHeader confidence="0.990068">
5.3 Fixed Beam
</subsectionHeader>
<bodyText confidence="0.99822175">
For our experiments, we de\x0cne the beam by a
maximum number of edges per span. Beams
for active and inactive edges are set separately.
The beams run from 2 to 12, and we test all
</bodyText>
<page confidence="0.965146">
2
</page>
<bodyText confidence="0.998577">
Here, we use proper pre\x0cxes, i.e., all pre\x0cxes not
including the last element.
</bodyText>
<page confidence="0.972456">
3
</page>
<bodyText confidence="0.951972">
As an example, PP-TMP=3 is replaced by PP.
</bodyText>
<page confidence="0.862841">
4
</page>
<bodyText confidence="0.664853">
The size of the chart is comparable to the \
umber
of edges popped&amp;quot; as given in (Charniak et al., 1998).
\x0cResults with Original and Parent Encoding
</bodyText>
<figure confidence="0.8875084375">
1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0
71
72
73
74
75
76
77
78
79
Labeled
F-Score
% chart size
inactive: 2
active: 3
inactive: 6
</figure>
<figureCaption confidence="0.8401348">
active: 4
inactive: 8
active: 7
inactive: 4
active: 3
inactive: 8
active: 6
inactive: 12
active: 9
Figure 1: Experimental results for incremental parsing and pruning. The \x0cgure shows the percent-
</figureCaption>
<bodyText confidence="0.998236714285714">
age of edges relative to exhaustive search and the F-score achieved with this chart size. Exhaustive
search yielded 71.21% for the original encoding and 79.28% for the parent encoding. Results in the
grey areas are equivalent with a con\x0cdence degree of \x0b = 0:99.
121 combinations of these beams for active and
inactive edges. Each setting results in a partic-
ular average size of the chart and an F-score,
which are reported in the following section.
</bodyText>
<subsectionHeader confidence="0.975021">
5.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999053533333333">
The results of our 121 test runs with di\x0berent
settings for active and inactive beams are given
in \x0cgure 1. The diagram shows chart sizes vs.
labeled F-scores. It sorts chart sizes across dif-
ferent settings of the beams. If several beam
settings result in equivalent chart sizes, the di-
agram contains the one yielding the highest F-
score.
The main \x0cnding is that we can reduce the
size of the chart to between 1% and 3% of
the size required for exhaustive search without
a\x0becting the results. Only very small beams
degrade performance5. The e\x0bect occurs for
both models despite the simple ranking formula.
This signi\x0ccantly reduces memory requirements
</bodyText>
<page confidence="0.96554">
5
</page>
<bodyText confidence="0.996729375">
Given the amount of test data (26,322 non-terminal
nodes), results within a range of around 0.7% are equiv-
alent with a con\x0cdence degree of \x0b = 99%.
(given as size of the chart) and increases parsing
speed.
Exhaustive search yields an F-Score of
71.21% when using the original Penn Treebank
encoding. Only around 1% the edges are re-
quired to yield equivalent results with incremen-
tal processing and pruning after each word is
added to the chart. This result is, among other
settings, obtained by a \x0cxed beam of 2 for in-
active edges and 3 for active edges6
For the parent encoding, exhaustive search
yields an F-Score of 79.28%. Only between 2
and 3% of the edges are required to yield an
equivalent result with incremental processing
and pruning. As an example, the point at size
= 3.0% F-score = 79.1% is generated by the
beam setting of 12 for inactive and 9 for active
edges. The parent encoding yields around 8%
higher F-scores but it also imposes a higher ab-
solute and relative memory load on the process.
The higher degree of parallelism in the inactive
</bodyText>
<page confidence="0.991191">
6
</page>
<bodyText confidence="0.997103043478261">
Using variable beams, we would need 1.95% of the
chart entries to achieve an equivalent F-score.
\x0cchart stems from the parent hypothesis in each
node. In terms of pure node categories, the av-
erage number of parallel nodes at this point is
3.57.
Exhaustive search for the base encoding needs
in average 140,000 edges per sentence, for the
parent encoding 200,000 edges; equivalent re-
sults for the base encoding can be achieved with
around 1% of these edges, equivalent results for
the parent encoding need between 2 and 3%.
The lower number of edges signi\x0ccantly in-
creases parsing speed. Using exhaustive search
for the base model, the parser processes 3.0 to-
kens per second (measured on a Pentium III
500; no serious e\x0borts of optimization have gone
into the parser). With a chart size of 1%, speed
is 630 tokens/second. This is a factor of 210
without decreasing accuracy. Speed for the par-
ent model is 0.5 tokens/second (exhaustive) and
111 tokens/seconds (3.0% chart size), yielding
an improvement by factor 220.
</bodyText>
<sectionHeader confidence="0.999361" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999637916666667">
Probably mostly related to the work reported
here are (Charniak et al., 1998) and (Roark and
Johnson, 1999). Both report on signi\x0ccantly
improved parsing eciency by selecting only a
subset of edges for processing. There are three
main di\x0berences to our approach. One is that
they use a ranking for best-\x0crst search while
we immediately prune hypotheses. They need
to store a large number edges because it is not
known in advance how many of the edges will be
used until a parse is found. The second di\x0ber-
ence is that we proceed strictly incrementally
without look-ahead. (Charniak et al., 1998)
use a non-incremental procedure, (Roark and
Johnson, 1999) use a look-ahead of one word.
Thirdly, we use a much simpler ranking formula.
Additionally, (Charniak et al., 1998) and
(Roark and Johnson, 1999) do not use the
original Penntree encoding for the context-free
structures. Before training and parsing, they
change/remove some of the productions and in-
troduce new part-of-speech tags for auxiliaries.
The exact e\x0bect of these modi\x0ccations is un-
known, and it is unclear if these a\x0bect compa-
</bodyText>
<page confidence="0.980592">
7
</page>
<bodyText confidence="0.999340466666667">
For the active chart, paralellism cannot be given for
di\x0berent nodes types since active edges are introduced
for right-hand side pre\x0cxes, collapsing all possible left-
hand sides.
rability to our results.
The heavy restrictions in our method (imme-
diate pruning, no look-ahead, very simple rank-
ing formula) have consequences on the accuracy.
Using right context and sorting instead of prun-
ing yields roughly 2% higher results (compared
to our base encoding8). But our work shows
that even with these massive restrictions, the
chart size can be reduced to 1% without a de-
crease in accuracy when compared to exhaustive
search.
</bodyText>
<sectionHeader confidence="0.998611" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999865125">
A central challenge in computational psycholin-
guistics is to explain how it is that people are
so accurate and robust in processing language.
Given the substantial psycholinguistic evidence
for statistical cognitive mechanisms, our objec-
tive in this paper was to assess the plausibility
of using wide-coverage probabilistic parsers to
model human linguistic performance. In par-
ticular, we set out to investigate the e\x0bects of
imposing incremental processing and signi\x0ccant
memory limitations on such parsers.
The central \x0cnding of our experiments is that
incremental parsing with massive (97% { 99%)
pruning of the search space does not impair
the accuracy of stochastic context-free parsers.
This basic \x0cnding was robust across di\x0berent
settings of the beams and for the original Penn
Treebank encoding as well as the parent encod-
ing. We did however, observe signi\x0ccantly re-
duced memory and time requirements when us-
ing combined active/inactive edge \x0cltering. To
our knowledge, this is the \x0crst investigation on
tree-bank grammars that systematically varies
the beam for pruning.
Our aim in this paper is not to challenge
state-of-the-art parsing accuracy results. For
our experiments we used a purely context-free
stochastic parser combined with a very sim-
ple pruning scheme based on simple \\unigram&amp;quot;
probabilities, and no use of right context. We
do, however suggest that our result should ap-
ply to richer, more sophistacted probabilistic
</bodyText>
<page confidence="0.971662">
8
</page>
<bodyText confidence="0.9992628">
Comparison of results is not straight-forward since
(Roark and Johnson, 1999) report accuracies only for
those sentences for which a parse tree was generated (be-
tween 93 and 98% of the sentences), while our parser
(except for very small beams) generates parses for vir-
tually all sentences, hence we report accuracies for all
sentences.
\x0cmodels, e.g. when adding word statistics to the
model (Charniak, 1997).
We therefore conclude that wide-coverage,
probabilistic parsers do not su\x0ber impaired ac-
curacy when subject to strict cognitive memory
limitations and incremental processing. Fur-
thermore, parse times are substantially reduced.
This suggests that it may be fruitful to pursue
the use of these models within computational
psycholinguistics, where it is necessary to ex-
plain not only the relatively rare \&amp;apos;pathologies\&amp;apos; of
the human parser, but also its more frequently
observed accuracy and robustness.
</bodyText>
<sectionHeader confidence="0.986591" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998062766233766">
Thorsten Brants. 1999. Cascaded Markov mod-
els. In Proceedings of 9th Conference of
the European Chapter of the Association for
Computational Linguistics EACL-99, Bergen,
Norway.
Eugene Charniak, Sharon Goldwater, and Mark
Johnson. 1998. Edge-based best-\x0crst chart
parsing. In Proceedings of the Sixth Work-
shop on Very Large Corpora (WVLC-98),
Montreal, Kanada.
Eugene Charniak. 1996. Tree-bank grammars.
In Proceedings of the Thirteenth National
Conference on Arti\x0ccial Intelligence, pages
1031{1036, Menlo Park: AAAI Press/MIT
Press.
Eugene Charniak. 1997. Statistical pars-
ing with a context-free grammar and word
statistics. In Proceedings of the Fourteenth
National Conference on Arti\x0ccial Intelli-
gence, pages 1031{1036, Menlo Park: AAAI
Press/MIT Press.
Nicholas Chater, Matthew Crocker, and Martin
Pickering. 1998. The rational analysis of in-
quiry: The case for parsign. In Chater and
Oaksford, editors, Rational Models of Cogni-
tion. Oxford University Press.
Michael Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In
Proceedings of ACL-96, Santa Cruz, CA,
USA.
Matthew Crocker and Thorsten Brants. 1999.
Incremental probabilistic models of human
linguistic performance. In The 5th Confer-
ence on Architectures and Mechanisms for
Language Processing, Edinburgh, U.K.
Matthew Crocker and Thorsten Brants. to ap-
pear. Wide coverage probabilistic sentence
processing. Journal of Psycholinguistic Re-
search, November 2000.
Matthew Crocker and Ste\x0ban Corley. to ap-
pear. Modular architectures and statistical
mechanisms: The case from lexical category
disambiguation. In Merlo and Stevenson, ed-
itors, The Lexical Basis of Sentence Process-
ing. John Benjamins.
Matthew Crocker. 1999. Mechanisms for sen-
tence processing. In Garrod and Picker-
ing, editors, Language Processing. Psychology
Press, London, UK.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Lin-
guistics, 24(4):613{632.
Daniel Jurafsky. 1996. A probabilistic model of
lexical and syntactic access and disambigua-
tion. Cognitive Science, 20:137{194.
Christopher Manning and Hinrich Sch\x7f
utze.
1999. Foundations of Statistical Natural Lan-
guage Processing. MIT Press, Cambridge,
Massachusetts.
Mitchell Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313{330.
Adwait Ratnaparkhi. 1997. A linear observed
time statistical parser based on maximum en-
tropy models. In Proceedings of the Confer-
ence on Empirical Methods in Natural Lan-
guage Processing EMNLP-97, Providence,
RI.
Brian Roark and Mark Johnson. 1999. Ecient
probabilistic top-down and left-corner pars-
ing. In Proceedings of the 37th Annual Meet-
ing of the Association for Computation Lin-
guistics ACL-99, Maryland.
\x0c&amp;apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.552025">
<title confidence="0.999296">b&amp;apos;Probabilistic Parsing and Psychological Plausibility</title>
<author confidence="0.999939">Thorsten Brants</author>
<author confidence="0.999939">Matthew Crocker</author>
<affiliation confidence="0.999916">Saarland University, Computational Linguistics</affiliation>
<address confidence="0.865643">D-66041 Saarbr\x7f ucken, Germany</address>
<email confidence="0.999638">fbrants,crockerg@coli.uni-sb.de</email>
<abstract confidence="0.988167173913044">Given the recent evidence for probabilistic mechanisms in models of human ambiguity resolution, this paper investigates the plausibility of exploiting current wide-coverage, probabilistic parsing techniques to model human linguistic performance. In particular, we investigate the performance of standard stochastic parsers when they are revised to operate incrementally, and with reduced memory resources. We present techniques for ranking and \x0cltering analyses, together with experimental results. Our results con\x0crm that stochastic parsers which adhere to these psychologically motivated constraints achieve good performance. Memory can be reduced down to 1% (compared to exhausitve search) without reducing recall and precision. Additionally, these models exhibit substantially faster performance. Finally, we argue that this general result is likely to hold for more sophisticated, and psycholinguistically plausible, probabilistic parsing models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Cascaded Markov models.</title>
<date>1999</date>
<booktitle>In Proceedings of 9th Conference of the European Chapter of the Association for Computational Linguistics EACL-99,</booktitle>
<location>Bergen, Norway.</location>
<contexts>
<context position="6082" citStr="Brants, 1999" startWordPosition="871" endWordPosition="872">obabilistic parsing systems as models of human linguistic performance. The view that human language processing can be viewed as an optimally adapted system, within a probabilistic framework, is advanced by (Chater et al., 1998), while (Jurafsky, 1996) has proposed a speci\x0cc probabilistic parsing model of human sentence processing. In work on human lexical category disambiguation, (Crocker and Corley, to appear), have demonstrated that a standard (incremental) HMM-based part-of-speech tagger models the \x0cnding from a range of psycholinguistic experiments. In related research, (Crocker and Brants, 1999) present evidence that an incremental stochastic parser based on Cascaded Markov Models (Brants, 1999) can account for a range of experimentally observed local ambiguity preferences. These include NP/S complement ambiguities, reduced relative clauses, noun-verb category ambiguities, and \&amp;apos;that\&amp;apos;-ambiguities (where \&amp;apos;that\&amp;apos; can be either a complementizer or a determiner) (Crocker and Brants, to appear). Crucially, however, there are di\x0berences between the classes of mechanisms which are psychologically plausible, and those which prevail in current language technology. We suggest that two of </context>
</contexts>
<marker>Brants, 1999</marker>
<rawString>Thorsten Brants. 1999. Cascaded Markov models. In Proceedings of 9th Conference of the European Chapter of the Association for Computational Linguistics EACL-99, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>Edge-based best-\x0crst chart parsing.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora (WVLC-98),</booktitle>
<location>Montreal, Kanada.</location>
<contexts>
<context position="12596" citStr="Charniak et al., 1998" startWordPosition="1959" endWordPosition="1963">e, the best probability for the span is p1, and pe &amp;lt; p1 \x12 : (4) \x0c2. Fixed beam. Select a maximum number of edges per span m. An edge e is removed, if its probability is not in the \x0crst m highest probabilities for edges with the same span. We performed experiments using both types of beams. Fixed beams yielded consistently better results than variable beams when plotting chart size vs. F-score. Therefore, the following results are reported for \x0cxed beams. We compare and rank edges covering the same span only, and we rank active and inactive edges separately. This is in contrast to (Charniak et al., 1998) who rank all edges. They use normalization in order to account for different spans since in general, edges for longer spans involve more multiplications of probabilities, yielding lower probabilities. Charniak et al.\&amp;apos;s normalization value is calculated by a different probability model than the inside probabilities of the edges. So, in addition to the normalization for di\x0berent span lengths, they need a normalization constant that accounts for the di\x0berent probability models. This investigation is based on a much simpler ranking formula. We use what can be described as the unigram proba</context>
<context position="16114" citStr="Charniak et al., 1998" startWordPosition="2583" endWordPosition="2586"> counting those hypothetical edges that are replaced or rejected because there is an alternative edge with higher probability4. For pruned search, we give the percentage of edges required. 5.3 Fixed Beam For our experiments, we de\x0cne the beam by a maximum number of edges per span. Beams for active and inactive edges are set separately. The beams run from 2 to 12, and we test all 2 Here, we use proper pre\x0cxes, i.e., all pre\x0cxes not including the last element. 3 As an example, PP-TMP=3 is replaced by PP. 4 The size of the chart is comparable to the \ umber of edges popped&amp;quot; as given in (Charniak et al., 1998). \x0cResults with Original and Parent Encoding 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 71 72 73 74 75 76 77 78 79 Labeled F-Score % chart size inactive: 2 active: 3 inactive: 6 active: 4 inactive: 8 active: 7 inactive: 4 active: 3 inactive: 8 active: 6 inactive: 12 active: 9 Figure 1: Experimental results for incremental parsing and pruning. The \x0cgure shows the percentage of edges relative to exhaustive search and the F-score achieved with this chart size. Exhaustive search yielded 71.21% for the original encoding and 79.28% for the parent encoding. Results in the grey areas are equiva</context>
<context position="19832" citStr="Charniak et al., 1998" startWordPosition="3218" endWordPosition="3221">ncoding need between 2 and 3%. The lower number of edges signi\x0ccantly increases parsing speed. Using exhaustive search for the base model, the parser processes 3.0 tokens per second (measured on a Pentium III 500; no serious e\x0borts of optimization have gone into the parser). With a chart size of 1%, speed is 630 tokens/second. This is a factor of 210 without decreasing accuracy. Speed for the parent model is 0.5 tokens/second (exhaustive) and 111 tokens/seconds (3.0% chart size), yielding an improvement by factor 220. 6 Related Work Probably mostly related to the work reported here are (Charniak et al., 1998) and (Roark and Johnson, 1999). Both report on signi\x0ccantly improved parsing eciency by selecting only a subset of edges for processing. There are three main di\x0berences to our approach. One is that they use a ranking for best-\x0crst search while we immediately prune hypotheses. They need to store a large number edges because it is not known in advance how many of the edges will be used until a parse is found. The second di\x0berence is that we proceed strictly incrementally without look-ahead. (Charniak et al., 1998) use a non-incremental procedure, (Roark and Johnson, 1999) use a look-</context>
</contexts>
<marker>Charniak, Goldwater, Johnson, 1998</marker>
<rawString>Eugene Charniak, Sharon Goldwater, and Mark Johnson. 1998. Edge-based best-\x0crst chart parsing. In Proceedings of the Sixth Workshop on Very Large Corpora (WVLC-98), Montreal, Kanada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<date>1996</date>
<note>Tree-bank grammars.</note>
<contexts>
<context position="8428" citStr="Charniak, 1996" startWordPosition="1226" endWordPosition="1227">astic chart-parsing techniques. A good introduction can be found, e.g., in (Manning and Sch\x7f utze, 1999). We use standard abbreviations for terminial nodes, nonterminal nodes, rules and probabilities. \x0cThis paper investigates stochastic contextfree parsing based on a grammar that is derived from a treebank, starting with part-of-speech tags as terminals. The grammar is derived by collecting all rules X ! \x0b that occur in the treebank and their frequencies f. The probability of a rule is set to P(X ! \x0b) = f(X ! \x0b) P \x0c f(X ! \x0b) (1) For a description of treebank grammars see (Charniak, 1996). The grammar does not contain \x0f-rules, otherwise there is no restriction on the rules. In particular, we do not require Chomsky-Normal-Form. In addition to the rules that correspond to structures in the corpus, we add a new start symbol ROOT to the grammar and rules ROOT ! X for all non-terminals X together with probabilities derived from the root nodes in the corpus1. For parsing these grammars, we rely upon a standard bottom-up chart-parsing technique with a modi\x0ccation for incremental parsing, i.e., for each word, all edges are processed and possibly pruned before proceeding to the n</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>Eugene Charniak. 1996. Tree-bank grammars.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the Thirteenth National Conference on Arti\x0ccial Intelligence,</booktitle>
<pages>1031--1036</pages>
<publisher>AAAI Press/MIT Press.</publisher>
<location>Menlo Park:</location>
<marker></marker>
<rawString>In Proceedings of the Thirteenth National Conference on Arti\x0ccial Intelligence, pages 1031{1036, Menlo Park: AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Arti\x0ccial Intelligence,</booktitle>
<pages>1031--1036</pages>
<publisher>AAAI Press/MIT Press.</publisher>
<location>Menlo Park:</location>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Arti\x0ccial Intelligence, pages 1031{1036, Menlo Park: AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Chater</author>
<author>Matthew Crocker</author>
<author>Martin Pickering</author>
</authors>
<title>The rational analysis of inquiry: The case for parsign.</title>
<date>1998</date>
<booktitle>In Chater and Oaksford, editors, Rational Models of Cognition.</booktitle>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="5696" citStr="Chater et al., 1998" startWordPosition="811" endWordPosition="814">t overview). While often insightful, this approach diverts attention from the fact that people are in fact extremely accurate and e\x0bective in understanding the vast majority of their \\linguistic experience&amp;quot;. This observation, combined with the mounting psycholinguistic evidence for statistically-based mechanisms, leads us to investigate the merit of exploiting robust, broad coverage, probabilistic parsing systems as models of human linguistic performance. The view that human language processing can be viewed as an optimally adapted system, within a probabilistic framework, is advanced by (Chater et al., 1998), while (Jurafsky, 1996) has proposed a speci\x0cc probabilistic parsing model of human sentence processing. In work on human lexical category disambiguation, (Crocker and Corley, to appear), have demonstrated that a standard (incremental) HMM-based part-of-speech tagger models the \x0cnding from a range of psycholinguistic experiments. In related research, (Crocker and Brants, 1999) present evidence that an incremental stochastic parser based on Cascaded Markov Models (Brants, 1999) can account for a range of experimentally observed local ambiguity preferences. These include NP/S complement a</context>
</contexts>
<marker>Chater, Crocker, Pickering, 1998</marker>
<rawString>Nicholas Chater, Matthew Crocker, and Martin Pickering. 1998. The rational analysis of inquiry: The case for parsign. In Chater and Oaksford, editors, Rational Models of Cognition. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL-96,</booktitle>
<location>Santa Cruz, CA, USA.</location>
<contexts>
<context position="4435" citStr="Collins, 1996" startWordPosition="634" endWordPosition="635">istic accounts which posit probabilistic ambiguity resolution mechanisms to explain \\garden path&amp;quot; phenomena. It is important to reiterate that our intention here is only to investigate the performance of probabilistic parsers under psycholinguistically motivated constraints. We do not argue for the psychological plausibility of SCFG parsers (or the parent-encoded variant) per se. Our investigation of these models was motivated rather by our desire to obtain a generalizable result for these simple and well-understood models, since obtaining similar results for more sophisticated models (e.g. (Collins, 1996; Ratnaparkhi, 1997)) might have been attributed to special properties of these models. Rather, the current result should be taken as support for the potential scaleability and performance of probabilistic psychological models such as those proposed by (Jurafsky, 1996) and (Crocker and Brants, to appear). 2 Psycholinguistic Motivation Theories of human sentence processing have largely been shaped by the study of pathologies in human language processing behaviour. Most psycholinguistic models seek to explain the dif\x0cculty people have in comprehending structures that are ambiguous or memory-i</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of ACL-96, Santa Cruz, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Crocker</author>
<author>Thorsten Brants</author>
</authors>
<date>1999</date>
<contexts>
<context position="6082" citStr="Crocker and Brants, 1999" startWordPosition="869" endWordPosition="872">coverage, probabilistic parsing systems as models of human linguistic performance. The view that human language processing can be viewed as an optimally adapted system, within a probabilistic framework, is advanced by (Chater et al., 1998), while (Jurafsky, 1996) has proposed a speci\x0cc probabilistic parsing model of human sentence processing. In work on human lexical category disambiguation, (Crocker and Corley, to appear), have demonstrated that a standard (incremental) HMM-based part-of-speech tagger models the \x0cnding from a range of psycholinguistic experiments. In related research, (Crocker and Brants, 1999) present evidence that an incremental stochastic parser based on Cascaded Markov Models (Brants, 1999) can account for a range of experimentally observed local ambiguity preferences. These include NP/S complement ambiguities, reduced relative clauses, noun-verb category ambiguities, and \&amp;apos;that\&amp;apos;-ambiguities (where \&amp;apos;that\&amp;apos; can be either a complementizer or a determiner) (Crocker and Brants, to appear). Crucially, however, there are di\x0berences between the classes of mechanisms which are psychologically plausible, and those which prevail in current language technology. We suggest that two of </context>
</contexts>
<marker>Crocker, Brants, 1999</marker>
<rawString>Matthew Crocker and Thorsten Brants. 1999.</rawString>
</citation>
<citation valid="false">
<title>Incremental probabilistic models of human linguistic performance.</title>
<date>2000</date>
<journal>Journal of Psycholinguistic Research,</journal>
<booktitle>In The 5th Conference on Architectures and Mechanisms for Language Processing,</booktitle>
<editor>U.K. Matthew Crocker and Thorsten</editor>
<publisher>John Benjamins.</publisher>
<location>Edinburgh,</location>
<marker>2000</marker>
<rawString>Incremental probabilistic models of human linguistic performance. In The 5th Conference on Architectures and Mechanisms for Language Processing, Edinburgh, U.K. Matthew Crocker and Thorsten Brants. to appear. Wide coverage probabilistic sentence processing. Journal of Psycholinguistic Research, November 2000. Matthew Crocker and Ste\x0ban Corley. to appear. Modular architectures and statistical mechanisms: The case from lexical category disambiguation. In Merlo and Stevenson, editors, The Lexical Basis of Sentence Processing. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Crocker</author>
</authors>
<title>Mechanisms for sentence processing.</title>
<date>1999</date>
<booktitle>In Garrod and Pickering, editors, Language Processing.</booktitle>
<publisher>Psychology Press,</publisher>
<location>London, UK.</location>
<contexts>
<context position="5064" citStr="Crocker, 1999" startWordPosition="723" endWordPosition="724">1997)) might have been attributed to special properties of these models. Rather, the current result should be taken as support for the potential scaleability and performance of probabilistic psychological models such as those proposed by (Jurafsky, 1996) and (Crocker and Brants, to appear). 2 Psycholinguistic Motivation Theories of human sentence processing have largely been shaped by the study of pathologies in human language processing behaviour. Most psycholinguistic models seek to explain the dif\x0cculty people have in comprehending structures that are ambiguous or memory-intensive (see (Crocker, 1999) for a recent overview). While often insightful, this approach diverts attention from the fact that people are in fact extremely accurate and e\x0bective in understanding the vast majority of their \\linguistic experience&amp;quot;. This observation, combined with the mounting psycholinguistic evidence for statistically-based mechanisms, leads us to investigate the merit of exploiting robust, broad coverage, probabilistic parsing systems as models of human linguistic performance. The view that human language processing can be viewed as an optimally adapted system, within a probabilistic framework, is a</context>
</contexts>
<marker>Crocker, 1999</marker>
<rawString>Matthew Crocker. 1999. Mechanisms for sentence processing. In Garrod and Pickering, editors, Language Processing. Psychology Press, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="2490" citStr="Johnson, 1998" startWordPosition="348" endWordPosition="349"> encounter. Our aim is to show that large scale probabilistic parsers, when subjected to basic cognitive constraints, can still achieve high levels of parsing accuracy. If successful, this will contribute to a plausible explanation of the fact that people, in general, are also extremely accurate and robust. Such a result would also strengthen existing results showing that related probabilistic mechanisms can explain speci\x0cc psycholinguistic phenomena. To investigate this issue, we construct a standard \&amp;apos;baseline\&amp;apos; stochastic parser, which mirrors the performance of a similar systems (e.g. (Johnson, 1998)). We then consider an incremental version of the parser, and evaluate the e\x0bects of several probabilistic \x0cltering strategies which are used to prune the parser\&amp;apos;s search space, and thereby reduce memory load. To assess the generality of our results for more sophisticated probabilistic models, we also conduct experiments using a model in which parent-node information is encoded on the daughters. This increase in contextual information has been shown to improve performance (Johnson, 1998), and the model is also shown to be robust to the incrementality and memory constraints investigated </context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613{632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
</authors>
<title>A probabilistic model of lexical and syntactic access and disambiguation.</title>
<date>1996</date>
<journal>Cognitive Science,</journal>
<pages>20--137</pages>
<contexts>
<context position="4704" citStr="Jurafsky, 1996" startWordPosition="673" endWordPosition="674">straints. We do not argue for the psychological plausibility of SCFG parsers (or the parent-encoded variant) per se. Our investigation of these models was motivated rather by our desire to obtain a generalizable result for these simple and well-understood models, since obtaining similar results for more sophisticated models (e.g. (Collins, 1996; Ratnaparkhi, 1997)) might have been attributed to special properties of these models. Rather, the current result should be taken as support for the potential scaleability and performance of probabilistic psychological models such as those proposed by (Jurafsky, 1996) and (Crocker and Brants, to appear). 2 Psycholinguistic Motivation Theories of human sentence processing have largely been shaped by the study of pathologies in human language processing behaviour. Most psycholinguistic models seek to explain the dif\x0cculty people have in comprehending structures that are ambiguous or memory-intensive (see (Crocker, 1999) for a recent overview). While often insightful, this approach diverts attention from the fact that people are in fact extremely accurate and e\x0bective in understanding the vast majority of their \\linguistic experience&amp;quot;. This observation</context>
</contexts>
<marker>Jurafsky, 1996</marker>
<rawString>Daniel Jurafsky. 1996. A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science, 20:137{194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
</authors>
<title>and Hinrich Sch\x7f utze.</title>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Manning, 1999</marker>
<rawString>Christopher Manning and Hinrich Sch\x7f utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="14719" citStr="Marcus et al., 1993" startWordPosition="2337" endWordPosition="2340">i1;j1 :::Y k ik;jk ) (7) = P(Y 1 \x01\x01\x01Y k) \x01P(ti :::tj 1jY 1 \x01\x01\x01Y k)(8) = P(Y 1 \x01\x01\x01Y k) \x01\x0cA(Y 1 i1;j1 :::Y k ik;jk ) (9) P(Y 1 \x01\x01\x01Y k) can be read o\x0b the corpus. It is the a priori probability that the right-hand side of a production has the pre\x0cx Y 1 :::Y k, which is estimated by f(Y 1 \x01\x01\x01Y k is pre\x0cx) N (10) where N is the total number of productions in the corpus2, i = i1, j = jk and \x0cA is the inside probability of the pre\x0cx. 5 Experiments 5.1 Data We use sections 2 { 21 of the Wall Street Journal part of the Penn Treebank (Marcus et al., 1993) to generate a treebank grammar. Traces, functional tags and other tag extensions that do not mark syntactic category are removed before training3. No other modi\x0ccations are made. For testing, we use the 1578 sentences of length 40 or less of section 22. The input to the parser is the sequence of part-of-speech tags. 5.2 Evaluation For evaluation, we use the parseval measures and report labeld F-score (the harmonic mean of labeled recall and labeled precision). Reporting the F-score makes our results comparable to those of other previous experiments using the same data sets. As a measure of</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313{330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP-97,</booktitle>
<location>Providence, RI.</location>
<contexts>
<context position="4455" citStr="Ratnaparkhi, 1997" startWordPosition="636" endWordPosition="637">which posit probabilistic ambiguity resolution mechanisms to explain \\garden path&amp;quot; phenomena. It is important to reiterate that our intention here is only to investigate the performance of probabilistic parsers under psycholinguistically motivated constraints. We do not argue for the psychological plausibility of SCFG parsers (or the parent-encoded variant) per se. Our investigation of these models was motivated rather by our desire to obtain a generalizable result for these simple and well-understood models, since obtaining similar results for more sophisticated models (e.g. (Collins, 1996; Ratnaparkhi, 1997)) might have been attributed to special properties of these models. Rather, the current result should be taken as support for the potential scaleability and performance of probabilistic psychological models such as those proposed by (Jurafsky, 1996) and (Crocker and Brants, to appear). 2 Psycholinguistic Motivation Theories of human sentence processing have largely been shaped by the study of pathologies in human language processing behaviour. Most psycholinguistic models seek to explain the dif\x0cculty people have in comprehending structures that are ambiguous or memory-intensive (see (Crock</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP-97, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Mark Johnson</author>
</authors>
<title>Ecient probabilistic top-down and left-corner parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computation Linguistics ACL-99,</booktitle>
<location>Maryland. \x0c&amp;apos;</location>
<contexts>
<context position="19862" citStr="Roark and Johnson, 1999" startWordPosition="3223" endWordPosition="3226">%. The lower number of edges signi\x0ccantly increases parsing speed. Using exhaustive search for the base model, the parser processes 3.0 tokens per second (measured on a Pentium III 500; no serious e\x0borts of optimization have gone into the parser). With a chart size of 1%, speed is 630 tokens/second. This is a factor of 210 without decreasing accuracy. Speed for the parent model is 0.5 tokens/second (exhaustive) and 111 tokens/seconds (3.0% chart size), yielding an improvement by factor 220. 6 Related Work Probably mostly related to the work reported here are (Charniak et al., 1998) and (Roark and Johnson, 1999). Both report on signi\x0ccantly improved parsing eciency by selecting only a subset of edges for processing. There are three main di\x0berences to our approach. One is that they use a ranking for best-\x0crst search while we immediately prune hypotheses. They need to store a large number edges because it is not known in advance how many of the edges will be used until a parse is found. The second di\x0berence is that we proceed strictly incrementally without look-ahead. (Charniak et al., 1998) use a non-incremental procedure, (Roark and Johnson, 1999) use a look-ahead of one word. Thirdly, we</context>
<context position="23056" citStr="Roark and Johnson, 1999" startWordPosition="3720" endWordPosition="3723">ments when using combined active/inactive edge \x0cltering. To our knowledge, this is the \x0crst investigation on tree-bank grammars that systematically varies the beam for pruning. Our aim in this paper is not to challenge state-of-the-art parsing accuracy results. For our experiments we used a purely context-free stochastic parser combined with a very simple pruning scheme based on simple \\unigram&amp;quot; probabilities, and no use of right context. We do, however suggest that our result should apply to richer, more sophistacted probabilistic 8 Comparison of results is not straight-forward since (Roark and Johnson, 1999) report accuracies only for those sentences for which a parse tree was generated (between 93 and 98% of the sentences), while our parser (except for very small beams) generates parses for virtually all sentences, hence we report accuracies for all sentences. \x0cmodels, e.g. when adding word statistics to the model (Charniak, 1997). We therefore conclude that wide-coverage, probabilistic parsers do not su\x0ber impaired accuracy when subject to strict cognitive memory limitations and incremental processing. Furthermore, parse times are substantially reduced. This suggests that it may be fruitf</context>
</contexts>
<marker>Roark, Johnson, 1999</marker>
<rawString>Brian Roark and Mark Johnson. 1999. Ecient probabilistic top-down and left-corner parsing. In Proceedings of the 37th Annual Meeting of the Association for Computation Linguistics ACL-99, Maryland. \x0c&amp;apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>