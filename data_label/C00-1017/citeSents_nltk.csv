The view that human language processing can be viewed as an optimally adapted system, within a probabilistic framework, is advanced by CITATION, while CITATION has proposed a speci\x0cc probabilistic parsing model of human sentence processing.,,
In related research, (Crocker and CITATION) present evidence that an incremental stochastic parser based on Cascaded Markov Models CITATION can account for a range of experimentally observed local ambiguity preferences.,,
This is in contrast to CITATION who rank all edges.,,
4 The size of the chart is comparable to the \ umber of edges popped&quot; as given in CITATION.,,
6 Related Work Probably mostly related to the work reported here are CITATION and CITATION.,,
CITATION use a non-incremental procedure, CITATION use a look-,,
\x0b) (1) For a description of treebank grammars see CITATION.,,
The view that human language processing can be viewed as an optimally adapted system, within a probabilistic framework, is advanced by CITATION, while CITATION has proposed a speci\x0cc probabilistic parsing model of human sentence processing.,,
In related research, (Crocker and CITATION) present evidence that an incremental stochastic parser based on Cascaded Markov Models CITATION can account for a range of experimentally observed local ambiguity preferences.,,
(CITATION; CITATION)) might have been attributed to special properties of these models.,,
Rather, the current result should be taken as support for the potential scaleability and performance of probabilistic psychological models such as those proposed by CITATION and (Crocker and Brants, to appear).,,
The view that human language processing can be viewed as an optimally adapted system, within a probabilistic framework, is advanced by CITATION, while CITATION has proposed a speci\x0cc probabilistic parsing model of human sentence processing.,,
In related research, (Crocker and CITATION) present evidence that an incremental stochastic parser based on Cascaded Markov Models CITATION can account for a range of experimentally observed local ambiguity preferences.,,
Rather, the current result should be taken as support for the potential scaleability and performance of probabilistic psychological models such as those proposed by CITATION and (Crocker and Brants, to appear).,,
Most psycholinguistic models seek to explain the dif\x0cculty people have in comprehending structures that are ambiguous or memory-intensive (see CITATION for a recent overview).,,
CITATION).,,
This increase in contextual information has been shown to improve performance CITATION, and the model is also shown to be robust to the incrementality and memory constraints investigated,,
(CITATION; CITATION)) might have been attributed to special properties of these models.,,
Rather, the current result should be taken as support for the potential scaleability and performance of probabilistic psychological models such as those proposed by CITATION and (Crocker and Brants, to appear).,,
Most psycholinguistic models seek to explain the dif\x0cculty people have in comprehending structures that are ambiguous or memory-intensive (see CITATION for a recent overview).,,
5 Experiments 5.1 Data We use sections 2 { 21 of the Wall Street Journal part of the Penn Treebank CITATION to generate a treebank grammar.,,
(CITATION; CITATION)) might have been attributed to special properties of these models.,,
Rather, the current result should be taken as support for the potential scaleability and performance of probabilistic psychological models such as those proposed by CITATION and (Crocker and Brants, to appear).,,
6 Related Work Probably mostly related to the work reported here are CITATION and CITATION.,,
CITATION use a non-incremental procedure, CITATION use a look-ahead of one word.,,
We do, however suggest that our result should apply to richer, more sophistacted probabilistic 8 Comparison of results is not straight-forward since CITATION report accuracies only for those sentences for which a parse tree was generated (between 93 and 98% of the sentences), while our parser (except for very small beams) generates parses for virtually all sentences, hence we report accuracies for all sentences.,,
