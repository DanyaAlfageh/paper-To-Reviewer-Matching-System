CITATION used Alternative Structured Optimization, which is closely related to Structural Learning (cited above).,,
In the semi-supervised setting, CITATION use Structural Correspondence Learning and unlabeled data to adapt a Part-of-Speech tagger.,,
CITATION use unlabeled data (so-called background knowledge) with,,
More recently, CITATION presented an unsupervised system to learn the predominant senses of particular domains.,,
CITATION performed supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus.,,
These results are more valuable given the lack of substantial positive results on the literature on semisupervised or supervised domain adaptation for WSD (CITATION; CITATION; CITATION).,,
Compared to other settings, our semi-supervised results improve over the completely unsupervised system in CITATION, which had 43.7% and 49.9% precision for the SPORTS and FINANCES domains respectively, but lag well behind the target domain scenario, showing that there is still room for improvement in the semi-supervised setting.,,
His method improves or equals over previously explored more sophisticated methods (Daume III and Marcu, 2006; CITATION).,,
CITATION tested the supervised adaptation setting on the DSO corpus, which had examples from the Brown corpus and Wall Street Journal corpus.,,
Finally, Bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context CITATION.,,
4.2 Features from the reduced space Apart from the original space of features, we have the so called SVD features, obtained from the projection of the feature vectors into the reduced space CITATION.,,
His method improves or equals over previously explored more sophisticated methods (Daume III and Marcu, 2006; CITATION).,,
CITATION tested the supervised adaptation setting on the DSO corpus, which had examples from the Brown corpus and Wall Street Journal corpus.,,
These results are more valuable given the lack of substantial positive results on the literature on semisupervised or supervised domain adaptation for WSD (CITATION; CITATION; CITATION).,,
Compared to other settings, our semi-supervised results improve over the completely unsupervised system in CITATION, which had 43.7% and 49.9% precision for the SPORTS and FINANCES domains respectively, but lag well behind the target domain scenario, showing that there is still room for improvement in the semi-supervised setting.,,
3 Data sets The dataset we use was designed for domainrelated WSD experiments by CITATION, and is publicly available.,,
The examples come from the BNC CITATION and the SPORTS and FINANCES sections of the Reuters corpus CITATION, comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns.,,
The occurrences were hand-tagged with the senses from WordNet (WN) version 1.7.1 CITATION.,,
Singular Value Decomposition (SVD) has been shown to find correlations between terms which are helpful to overcome the scarcity of training data in WSD CITATION.,,
CITATION used SVD to reduce the space of the term-todocument matrix, and then computed the similarity between train and test instances using a mapping to the reduced space (similar to our SMA method in Section 4.2).,,
This technique is very similar to previous work on SVD (CITATION; CITATION).,,
The reduced 1 The PoS tagging was performed with the fnTBL toolkit CITATION 2 This software was kindly provided by David Yarowskys group, from Johns Hopkins University.,,
These results are in accordance with our previous experience on WSD (Agirre et al., 2005), where our OMT method got better results than SMA and those of CITATION (who also use a method similar to SMA) on the Senseval-3 lexical sample.,,
This paper shows that the OMT technique to apply SVD that we proposed in (Agirre et al., 2005) compares favorably to SMA, which has been previously used in CITATION, and that k-NN excels SVM on the features from the reduced space.,,
In the future, we plan to combine the features from the reduced space with the rest of features, either using a combination of k-NN classifiers (Agirre et al., 2005; Agirre and Lopez de Lacalle, 2007) or a complex kernel CITATION.,,
Regarding SVM we used linear kernels implemented in SVM-Light CITATION.,,
The experiments were performed on a publicly available corpus which was designed to study the effect of domain in WSD CITATION.,,
The 300 examples were drawn from the British National Corpus CITATION (BNC), the SPORTS section of the Reuters corpus (Leech, 17 \x0c1992), and the FINANCES section of Reuters in equal number.,,
More recently, CITATION presented an unsupervised system to learn the predominant senses of particular domains.,,
CITATION performed supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus.,,
3 Data sets The dataset we use was designed for domainrelated WSD experiments by CITATION, and is publicly available.,,
The examples come from the BNC CITATION and the SPORTS and FINANCES sections of the Reuters corpus CITATION, comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns.,,
The occurrences were hand-tagged with the senses from WordNet (WN) version 1.7.1 CITATION.,,
These results are more valuable given the lack of substantial positive results on the literature on semisupervised or supervised domain adaptation for WSD (CITATION; CITATION; CITATION).,,
Compared to other settings, our semi-supervised results improve over the completely unsupervised system in CITATION, which had 43.7% and 49.9% precision for the SPORTS and FINANCES domains respectively, but lag well behind the target domain scenario, showing that there is still room for improvement in the semi-supervised setting.,,
While these results are based on a lexical sample, and thus not directly generalizable to an allwords corpus, we think that they reflect the main trends for nouns, as the 41 nouns where selected among those exhibiting domain dependence CITATION.,,
The experiments were performed on a publicly available corpus which was designed to study the effect of domain in WSD CITATION.,,
The 300 examples were drawn from the British National Corpus CITATION (BNC), the SPORTS section of the Reuters corpus (Leech, 17 \x0c1992), and the FINANCES section of Reuters in equal number.,,
3 Data sets The dataset we use was designed for domainrelated WSD experiments by CITATION, and is publicly available.,,
The examples come from the BNC CITATION and the SPORTS and FINANCES sections of the Reuters corpus CITATION, comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns.,,
The occurrences were hand-tagged with the senses from WordNet (WN) version 1.7.1 CITATION.,,
These results are more valuable given the lack of substantial positive results on the literature on semisupervised or supervised domain adaptation for WSD (CITATION; CITATION; CITATION).,,
Compared to other settings, our semi-supervised results improve over the completely unsupervised system in CITATION, which had 43.7% and 49.9% precision for the SPORTS and FINANCES domains respectively, but lag well behind the target domain scenario, showing that there is still room for improvement in the semi-supervised setting.,,
This technique is very similar to previous work on SVD (CITATION; CITATION).,,
The reduced 1 The PoS tagging was performed with the fnTBL toolkit CITATION 2 This software was kindly provided by David Yarowskys group, from Johns Hopkins University.,,
Given that the WSD literature has shown that all features, including local and syntactic features, are necessary for optimal performance CITATION, we propose the following alternative to construct the matrix.,,
We have computed significance ranges for all results in this paper using bootstrap resampling CITATION.,,
Finally, Bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context CITATION.,,
4.2 Features from the reduced space Apart from the original space of features, we have the so called SVD features, obtained from the projection of the feature vectors into the reduced space CITATION.,,
The reduced 1 The PoS tagging was performed with the fnTBL toolkit CITATION 2 This software was kindly provided by David Yarowskys group, from Johns Hopkins University.,,
Given that the WSD literature has shown that all features, including local and syntactic features, are necessary for optimal performance CITATION, we propose the following alternative to construct the matrix.,,
3 Data sets The dataset we use was designed for domainrelated WSD experiments by CITATION, and is publicly available.,,
The examples come from the BNC CITATION and the SPORTS and FINANCES sections of the Reuters corpus CITATION, comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns.,,
The occurrences were hand-tagged with the senses from WordNet (WN) version 1.7.1 CITATION.,,
In the semi-supervised setting, CITATION use Structural Correspondence Learning and unlabeled data to adapt a Part-of-Speech tagger.,,
CITATION use unlabeled data (so-called background knowledge) with Latent Semantic Indexing (also based on SVD) on a Text Classification task with positive results.,,
This technique is very similar to previous work on SVD (CITATION; CITATION).,,
The reduced 1 The PoS tagging was performed with the fnTBL toolkit CITATION 2 This software was kindly provided by David Yarowskys group, from Johns Hopkins University.,,
