<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.806021">
b&amp;apos;DiaSumm: Flexible Summarization of
Spontaneous Dialogues in Unrestricted Domains
</title>
<author confidence="0.889169">
Klaus Zechner and Alex Waibel
</author>
<affiliation confidence="0.936477">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.9360625">
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
</address>
<email confidence="0.996897">
fzechner,waibelg@cs.cmu.edu
</email>
<sectionHeader confidence="0.99079" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.974344222222222">
In this paper, we present a summarization system
for spontaneous dialogues which consists of a novel
multi-stage architecture. It is speci\x0ccally aimed at
addressing issues related to the nature of the texts
being spoken vs. written and being dialogical vs.
monological. The system is embedded in a graph-
ical user interface and was developed and tested on
transcripts of recorded telephone conversations in
English and Spanish (Callhome).
</bodyText>
<sectionHeader confidence="0.997205" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999485470588235">
Summarization of written documents has recently
been a focus for much research in NLP (e.g., (Mani
and Maybury, 1997; AAAI, 1998; Mani et al., 1998;
ACL, 2000), to name some of the major events in
this \x0celd in the past few years). However, very lit-
tle attention has been given so far to the summa-
rization of spoken language, even less of conversa-
tions vs. monological texts. We believe that sum-
marization of speech will become increasingly more
important,as the amountof online audio data grows
and demand for rapid browsing, skimming, and ac-
cess of speech data increases. Another application
which particularly pertains to our interest in spo-
ken dialogue summarizationwouldbe the generation
of meeting minutes for archival purposes and/or to
update participants joining at later stages on the
progress of the conversation so far.
Summarization of dialogues within limited do-
mains has been attempted within the context of
the Verbmobil project (\\protocol generation&quot;,
(Alexandersson and Poller, 1998)) or by SRI\&amp;apos;s MIMI
summarizer (Kameyama et al., 1996). Recent work
on spoken language summarization in unrestricted
domains has focused almost exclusively on Broad-
cast News, mostly due to the spoken language track
of recent TREC evaluations (Garofolo et al., 1997;
Garofolo et al., 1999). (Waibel et al., 1998) describe
a Meeting Browser where summaries can be gener-
ated using technology established for written texts.
(Valenza et al., 1999) go one step further and incor-
porate knowledge from the speech recognizer (con-
\x0cdence scores) into their summarization system, as
well.
We argue that the nature of spoken dialogues, to-
gether with their textual representations as speech
recognizer hypotheses, requires a set of speci\x0cc ap-
proaches to make summarization feasible for this
text genre.
As a demonstrable proof of concept, we present
the multi-stage architecture of the summarization
system DiaSumm which can
exibly deal with spo-
ken dialogues in English and Spanish, without any
restrictions of domain. Since it cannot rely on any
domain speci\x0cc knowledge base, it uses shallow sta-
tistical approaches and presents (possibly modi\x0ced)
extracts from the original text as summary.
We present results of several evaluations of our
system using humantranscripts of spontaneous tele-
phone conversations inEnglish and Spanishfromthe
Callhome corpus ((LDC), 1996), in particular the
accuracy of the topic segmentation and information
condensing components (sections 6 and 7). Also, for
the purpose of a global evaluation, a user study was
performed which addressed information access time
and accuracy of retained informationcomparing dif-
ferent versions of summaries (section 10).
This paper is organized as follows: In the next sec-
tion, we provide an overview about the main issues
for summarization of spoken dialogues and indicate
the approaches we are taking in our system. We
then present the system architecture (section 3), fol-
lowedby a detailed description ofthe majorbuilding
blocks (sections 4 to 8). After a brief characteriza-
tion of the GUI (section 9) we describe a user study
for global system evaluation in section 10. We con-
clude the paper with a summaryand a brief outlook
in section 11.
</bodyText>
<sectionHeader confidence="0.906008" genericHeader="introduction">
2 Issues and Approaches: Overview
</sectionHeader>
<bodyText confidence="0.999347692307692">
In this section, we give an overview about the main
issues that any summarizationsystem for spoken di-
alogues has to address and indicate the approach we
are taking for each of these in DiaSumm.
In a general sense, when dealing with written
texts, usually there is plenty of information avail-
able which can be used for the purpose of summa-
\x0crization, such as capitalization, punctuation marks,
titles, passage headers, paragraph boundaries, or
other mark-ups. Unfortunately, however, none of
this holds for speech data which arrives as a stream
of word tokens from a recognizer, cut into \\utter-
ances&quot; by using a silence heuristic.
</bodyText>
<subsectionHeader confidence="0.992678">
2.1 Lack of clause boundaries
</subsectionHeader>
<bodyText confidence="0.9997672">
One of the most serious issues is the lack of sentence
or clause boundaries in spoken dialogues which is
particularly problematic since sentences, clauses, or
paragraphs are considered the \\minimal units&quot; in
virtually all existing summarization systems. When
humans speak, they sometimes pause during a
clause, and not always at the end of a clause, which
means that the output of a recognizer (which usu-
allyuses somesilence-heuristics to cut the segments)
frequently does not match logical sentence or clause
boundaries. Looking at \x0cve English Callhome di-
alogues with an average number of 320 utterances
each, we \x0cnd on average 30 such \\continuations&quot; of
logical clauses over automaticallydetermined acous-
tic segment boundaries. In a summary, this can
cause a reduction in coherence and readability of
the output.
We address this issue by linking adjacent turns
of the same speaker together if the silence between
them is less than a given constant (section 4).
</bodyText>
<subsectionHeader confidence="0.993141">
2.2 Distributed information
</subsectionHeader>
<bodyText confidence="0.9991691875">
Since we have multi-party conversations as opposed
to monological texts, sometimes the crucial infor-
mation is found in a question-answer-pair, i.e., it
involves more than one speaker; extracting only the
question or only the answer would be meaningless
in many cases. We found that on average about
10% of the speaker turns belong to such question-
answer pairs in \x0cve examined English Callhome
dialogues. Often, either the question or the answer
is very short and does not contain any words with
high relevance. In order not to \\lose&quot; these short
turns at a later stage, when only the most relevant
turns are extracted, we link them to the matching
question/answer ahead of time, using two di\x0berent
methods to detect questions and their answers (sec-
tion 4).
</bodyText>
<subsectionHeader confidence="0.868229333333333">
2.3 Dis
uent speech
Speech dis
</subsectionHeader>
<bodyText confidence="0.99872215">
uencies in spontaneous conversations |
such as \x0cllers, repetitions, repairs, or un\x0cnished
clauses  |can make transcripts (and summary ex-
tracts) quite hard to read and also introduce an un-
wanted bias to relevance computations (e.g., word
repetitions would cause a higher word count for the
repeated content words; words in un\x0cnished clauses
would be included in the word count.)
To alleviate this problem, we employ a clean-up
\x0clter pipeline, which eliminates \x0cller words and rep-
etitions, and segments the turns into short clauses
(section 5). We also remove incomplete clauses, typ-
ically sentence-initial repairs, at this stage of our
system. This \\cleaning-up&quot; serves two main pur-
poses: (i) it increases the readability (for the \x0cnally
extracted segments); and (ii) it makes the text more
tractable by subsequent modules.
The followingexamplecompares aturn before and
after the clean-up component:
before: I MEAN WE LOSE WE LOSE I CAN\&amp;apos;T I
</bodyText>
<sectionHeader confidence="0.693794" genericHeader="method">
CAN\&amp;apos;T DO ANYTHING ABOUT IT SO
</sectionHeader>
<bodyText confidence="0.8638105">
after: we lose / i can\&amp;apos;t do anything
about it
</bodyText>
<subsectionHeader confidence="0.989191">
2.4 Lack of topic boundaries
</subsectionHeader>
<bodyText confidence="0.9983437">
Callhome speech data is multi-topical but does
not include mark-up for paragraphs, nor any topic-
informative headers. Typically, we \x0cnd about 5{10
di\x0berent topics within a 10-minute segment of a di-
alogue, i.e., the topic changes about every 1{2 min-
utes in these conversations. To facilitate browsing
and summarization, we thus have to discover topi-
cally coherent segments automatically. This is done
using a TextTiling approach, adapted from (Hearst,
1997) (section 6).
</bodyText>
<subsectionHeader confidence="0.984545">
2.5 Speech recognizer errors
</subsectionHeader>
<bodyText confidence="0.999302481481481">
Last but not least, we face the problem of imper-
fect word accuracy of speech recognizers, particu-
larly when dealing with spontaneous speech over a
large vocabulary and over a low bandwidth channel,
such as the Callhome databases which we mainly
used for development, testing, and evaluation of our
system. Current recognizers typically exhibit word
error rates for these corpora in the order of 50%. In
DiaSumm\&amp;apos;s information condensation component,
the relevance weights of speaker turns can be ad-
justed to take into account their word con\x0cdence
scores from the speech recognizer. That way we can
reduce the likelihood of extracting passages with a
larger amount of word misrecognitions (Zechner and
Waibel, 2000). In this paper, however, the focus will
be exclusively on results of our evaluations on hu-
mangenerated transcripts. No informationfromthe
speech recognizer nor fromthe acoustic signal (other
than inter-utterance pause durations) are used. We
are aware that in particular prosodic information
may be of help for tasks such as the detection of
sentence boundaries, speech acts, or topic bound-
aries (Hirschberg and Nakatani, 1998; Shriberg et
al., 1998; Stolcke et al., 2000), but the investigation
of the integration of this additional source of infor-
mationis beyond the scope of this paper and left for
future work.
</bodyText>
<sectionHeader confidence="0.950088" genericHeader="method">
3 System Architecture
</sectionHeader>
<bodyText confidence="0.9684405">
The global system architecture of DiaSumm is a
pipeline of the following four major components:
</bodyText>
<figure confidence="0.996300692307692">
\x0cTRANS
Turn Linking
Clean-up Filter
Topic Segmentation
Information Condensation
Telegraphic Reduction
CLEAN
and TELE
input for
TRANS
CLEAN
TELE
input for
</figure>
<figureCaption confidence="0.999942">
Figure 1: System architecture
</figureCaption>
<bodyText confidence="0.9122785">
turn linking; clean-up \x0clter; topic segmentation; and
information condensation. A \x0cfth component is
added at the end for the purpose of telegraphic re-
duction, so that we can maximize the information
content in a given amount of space. The system ar-
chitecture is shown in Figure 1. It also indicates the
three major types of summaries which can be gener-
ated by DiaSumm: trans (\\transcript&quot;): not using
the linking and clean-up components; clean: us-
ing the main four components; tele (\\telegraphic&quot;
summary): additionally,using the telegraphicreduc-
tion component.
The followingsections describe the components of
DiaSumm in more detail.
</bodyText>
<sectionHeader confidence="0.979331" genericHeader="method">
4 Turn Linking
</sectionHeader>
<bodyText confidence="0.99857455">
The two main objectives of this component are: (i)
to form turns which contain a set of full (and not
partial) clauses; and (ii) to form turn-pairs in cases
where we have a question-answer pair in the dia-
logue.
Toachieve the \x0crst objective, we scan the inputfor
adjacent turns ofone speaker and linkthemtogether
if their time-stamp distance is below a pre-speci\x0ced
threshold \x12. If the threshold is too small, we don\&amp;apos;t
get most of the (logical) turn continuations across
utterance boundaries, if it is too large, we run the
risk of \\skipping&quot;over short but potentially relevant
fragments of the speaker on the other channel. We
experimented with thresholds between 0.0 and 2.0
seconds and determined a local performance maxi-
mum around \x12 = 1:0.
For the second objective, to formturn-pairs which
comprise a question-answer information exchange
between twodialogueparticipants, we need to detect
wh- and yes-no-questions in the dialogue. We tested
</bodyText>
<table confidence="0.9716331">
English Spanish
Annotated Data
turns 1603 1185
Wh-questions 42 78
yes-no-questions 43 98
questions total 85 (5.3%) 176 (14.9%)
Automatic Detection Results (F1)
SA classi\x0cer 0.24 0.22
POS rules 0.22 0.37
random baseline 0.02 0.13
</table>
<tableCaption confidence="0.998831">
Table 1: Q-A-pair distribution in the data and ex-
</tableCaption>
<bodyText confidence="0.9973114">
perimental results for automatic Q-A-detection
two approaches: (a) a HMM based speech act (SA)
classi\x0cer (Ries, 1999) and (b) a set of part-of-speech
(POS) based rules. The SA classi\x0cer was trained on
dialogues which were manuallyannotated for speech
acts, using parts of the Switchboard corpus (God-
frey et al., 1992) for English and Callhome for
Spanish. The corresponding answers for the de-
tected questions were hypothesized in the \x0crst turn
with a di\x0berent speaker, followingthe question-turn.
Table 1 shows the results of these experiments for 5
English and 5 Spanish Callhome dialogues, com-
pared to a baseline of randomlyassigning n question
speech acts, n being the number of question-turns
marked by human annotators. We report F1-scores,
</bodyText>
<equation confidence="0.696007">
where F1 = 2PR
P+R with P =precision and R=recall.
</equation>
<bodyText confidence="0.996683142857143">
We note that while the results for the SA-classi\x0cer
and the rule-based approach are very similar for En-
glish, the rule-based approach yields better results
for Spanish. The much higher random baseline for
Spanish can be explained by the higher incidence of
questions in the Spanish data (14.9% vs. 5.3% for
English).
</bodyText>
<sectionHeader confidence="0.996281" genericHeader="method">
5 Clean-up Filter
</sectionHeader>
<bodyText confidence="0.965560272727273">
The clean-up component is a sequence of modules
which serve the purposes of (a) rendering the tran-
scripts more readable, (b) simplifying the input for
subsequent components, and (c) avoiding unwanted
bias for relevance computations (see section 2). All
this has to happen without losing essential informa-
tion that could be relevant in a summary. While
other work(Heeman et al.,1996; Stolcke et al.,1998)
was concerned with building classi\x0cers that can de-
tect and possibly correct various speech dis
uencies,
</bodyText>
<footnote confidence="0.718855166666667">
our implementation is of a much simpler design. It
does not require as much manual annotated train-
ing data and uses individual components for every
major category of dis
uency.1
1While we have not yet numerically evaluated the perfor-
</footnote>
<bodyText confidence="0.964963277777778">
manceof this component,its outputis deemedverynaturalto
readby systemusers. Sincethe focusand goalsof thiscompo-
nent are somewhat di\x0berent than previous work in that area,
meaningful comparisons are hard to make.
\x0cSingle or multiple word repetitions, \x0cllers (e.g.,
\\uhm&quot;), and discourse markers without semantic
content (e.g., \\you know&quot;) are removed from the in-
put, some short forms are expanded (e.g., \\we\&amp;apos;ll&quot;
! \\we will&quot;), and frequent word sequences are
combined into a single token (e.g., \\a lot of&quot; !
\\a lot of&quot;).
Longer turns are segmented into short clauses,
which are de\x0cned as consisting of at least a sub-
ject and an in
ected verbal form. While (Stolcke
and Shriberg, 1996) use n-gram models for this task,
and (Gavald\x12
a et al., 1997) use neural networks, we
decided to use a rule-based approach (using word
and POS information), whose performance proved
to be comparable with the results in the cited pa-
pers (F1 &amp;gt; 0:85, error &lt; 0:05).2
For several of the clean-up \x0clter\&amp;apos;s components, we
make use of Brill\&amp;apos;s POS tagger (Brill, 1994). For
English, we use a modi\x0ced version of Brill\&amp;apos;s original
tagset, and the tagger was adapted andretrained for
spoken language corpora (Callhome and Switch-
board) (Zechner, 1997). For Spanish, we created
our own tag set, derived from the LDC lexicon and
fromthe CRATERproject (Le\x13
on, 1994),andtrained
the tagger on manually annotated Callhome dia-
logues. Furthermore, a POS based shallow chunk
parser (Zechner and Waibel, 1998) is used to \x0clter
out likely candidates for incomplete clauses due to
speech repair or interruption by the other speaker.
</bodyText>
<sectionHeader confidence="0.970004" genericHeader="method">
6 Topic Segmentation
</sectionHeader>
<bodyText confidence="0.972212875">
Since Callhome dialogues are alwaysmulti-topical,
segmenting them into topical units is an important
step in our summarization system. This allows us
to provide \\signature&quot; information (frequent con-
tent words) about every topic to the user as a help
for faster browsing and accessing the data. Fur-
thermore, the subsequent information condensation
component can work on smallerparts of the dialogue
and thus operate more e\x0eciently.
Following (Boguraev and Kennedy, 1997; Barzi-
lay and Elhadad, 1997) who use TextTiling (Hearst,
1997) for their summarization systems of written
text, we adapted this algorithm (its block compar-
ison version) for speech data: we choose turns to
be minimal units and compute block similarity be-
tween blocks of k turns every d turns. We use 9
English and 15 Spanish Callhome dialogues, man-
ually annotated for topic boundaries, to determine
the optimum values for a set of TextTiling param-
eters and at the same time to evaluate the accu-
racy of this algorithm. To do this, we ran an n-fold
cross-validation (\\jack-kni\x0cng&quot;) where all dialogues
but one are used to determine the best parameters
(\\train set&quot;) and the remaining dialogue is used as
</bodyText>
<table confidence="0.944421846153846">
2The comparison was done on the same data set as used
in (Gavald\x12
a et al., 1997).
English Spanish
blocksize k 25 15
sample distance d 2 2
rounds of smoothing r 2 1
smoothing width s 2 1
Table 2: Optimal TextTiling parameters for English
and Spanish Callhome dialogues
English Spanish
number of dialogues 9 15
random baseline 0.34 0.35
</table>
<tableCaption confidence="0.847982333333333">
test set avg. (\\unseen data&quot;) 0.58 0.53
train set avg. (\\seen data&quot;) 0.69 0.58
Table 3: Topic segmentation results for English and
</tableCaption>
<bodyText confidence="0.959112928571429">
Spanish Callhome dialogues (F1-scores)
a held-out data set for evaluation (\\test set&quot;). This
process is repeated n times and average results are
reported. Table 2 shows the set of parameters which
worked best for most dialogues and Table 3 shows
the evaluation results of the cross-validation exper-
iment. F1-scores improve by 18{24% absolute over
the random baseline for unseen and by 23{35% for
seen data, the performance for English being better
than for Spanish. These results, albeit achieved on
a quite di\x0berent text genre, are well in line with the
results in (Hearst, 1997) who reports an absolute im-
provement of about 20% over a random baseline for
seen data.
</bodyText>
<sectionHeader confidence="0.988894" genericHeader="method">
7 Information Condensation
</sectionHeader>
<bodyText confidence="0.948834428571429">
The informationcondensation component is the core
of our system. Its purpose is to determine weights
for terms and turns (or linked turn-pairs) and then
to rank the turns according to their relevance within
each topical segment of the dialogue.
For term-weighting, tf*idf -inspired formulae
(Salton and Buckley, 1990) are used to emphasize
words which are in the \\middle range&quot; of frequency
in the dialogue and do not appear in a stop list.3
For turn-ranking, we use a version of the \\maximal
marginal relevance&quot; (MMR) algorithm (Carbonell
and Goldstein, 1998), where emphasis is given to
turns which contain many highly weighted terms for
the current segment (\\salience&quot;) and are su\x0eciently
dissimilar to previously ranked turns (to minimize
redundancy).
For 9 English and 14 Spanish dialogues, the \\most
relevant&quot; turns were marked by human coders. We
rana series of cross-validationexperiments to(a) op-
timize the parameters of this component related to
tf*idf and MMR computation and to (b) determine
3For English, our stop list comprises 557 words, for Span-
ish, 831 words.
\x0chowwellthis informationcondensing componentcan
match the human relevance annotations.
Summarization results are computed using 11-pt-
avg precision scores for ranked turn lists where the
maximum precision of the list of retrieved turns
is averaged in the 11 evenly spaced intervals be-
tween recall=[0,0.1),[0.1,0.2), ... [1.0,1.1) (Salton
and McGill, 1983).4 Table 4 shows the results from
these experiments. Similar to other experiments in
the summarizationliterature (Mani et al., 1998), we
\x0cnd a wide performance variation across di\x0berent
texts.
</bodyText>
<sectionHeader confidence="0.9813" genericHeader="method">
8 Telegraphic Reduction
</sectionHeader>
<bodyText confidence="0.996375363636364">
The purpose of this component is to maximizeinfor-
mation in a \x0cxed amount of space. We shorten the
output of the summarizer to a \\telegraphic style&quot;;
that way, more information can be included in a
summary of k words (or n bytes). Since we only
use shallow methods for textual analysis that do
not generate a dependency structure, we cannot use
complex methods for text reduction as described,
e.g., in (Jing, 2000). Our method simply excludes
words occurring in the stop list from the summary,
except for some highlyinformativewords such as \\I&quot;
</bodyText>
<equation confidence="0.33747925">
or \
ot&quot;.
9 User Interface and System
Performance
</equation>
<bodyText confidence="0.944154071428571">
Since we want to enable interactive summarization
which allows a user to browse through a dialogue
quickly to search for information he is interested
in, we have integrated our summarization system
into a JAVA-based graphical user interface (\\Meet-
ing Browser&quot;) (Bett et al., 2000). This interface also
integrates the output of a speech recognizer (Yu et
al., 1999), and can display a wide variety of infor-
mation about a conversation, including speech acts,
dialogue games, and emotions.
For summarization, the user can determine the
size of the summary and which topical segments
he wants to have displayed. He can also focus
the summary on particular content words (\\query-
based summary&quot;) or exclude words from considera-
tion (\\dynamic stop list expansion&quot;).
Summarizing a 10 minute segment of a Call-
home dialogue with our system takes on average less
than 30 seconds on a 167 MHz 320 MB Sun Ultra1
workstation.5
4We are aware that this annotationand evaluationscheme
is far from optimal: it does neither re
ect the fact that turns
are not necessarily the best units for extraction nor that the
11-pt-avg precision score is not optimally suited for the sum-
marization task. We thus have recently developed a new
word-based method for annotation and evaluation of spon-
taneous speech (Zechner, 2000).
</bodyText>
<footnote confidence="0.979805">
5The average was computed over \x0cve English dialogues.
</footnote>
<sectionHeader confidence="0.413835" genericHeader="method">
10 Human Study
10.1 Experiment Setup
</sectionHeader>
<bodyText confidence="0.997716636363636">
In order to evaluate the system as a whole, we con-
ducted a study with humansin the loop to be able to
compare three types of summaries (trans, clean,
tele, see section 3) with the full original transcript.
We address these two main questions in this study:
(i) how fast can information be identi\x0ced using dif-
ferent types of summaries? (ii) how accurately is the
information preserved, comparing di\x0berent types of
summaries?
We did not only ask the user \
arrow&quot; questions
for a speci\x0cc piece of information  |along the lines
of the Q-A-evaluation part of the SUMMAC confer-
ence (Mani et al., 1998)  |but also very \\global&quot;,
non-speci\x0cc questions, tied to a particular (topical)
segment of the dialogue.
The experiment was conducted as follows: Sub-
jects were given24texts each, accompaniedbyeither
a generic question (\\What is the topic of the discus-
sion in this text segment?&quot;) or three speci\x0cc ques-
tions (e.g., \\Which clothes did speaker A buy?&quot;).
The texts were drawn from \x0cve topical segments
each from \x0cve English Callhome dialogues.6 They
have four di\x0berent formats: (a) full transcripts (i.e.,
the transcript of the whole segment) (full); (b)
summaryof the raw transcripts (without linkingand
clean-up) (trans); (c) cleaned-up summary (using
all four major components of our system) (clean);
and (d) telegram summary (derived from (c), using
also the telegraphic reduction component) (tele).
The texts of formats (b), (c), and (d) were gener-
ated to have the same length: 40% of (a), i.e., we
use a 60% reduction rate. All these formats can
be accompanied by either a generic or three speci\x0cc
questions, hence there are eight types of tasks for
each of the 24 texts.
We divided the subjects in eight groups such that
no subject had to perform more than one task on
the same text and we distributed the di\x0berent tasks
evenly for each group. Thus we can make unbiased
comparisons across texts and tasks.
The answer accuracy vs. a pre-de\x0cned answer key
was manually assessed on a 6 point discrete scale
between 0.0 and 1.0.
</bodyText>
<sectionHeader confidence="0.952868" genericHeader="evaluation">
10.2 Results and Discussion
</sectionHeader>
<bodyText confidence="0.982858428571429">
Of the 27 subjects taking part in this experiment,
we included 24 subjects in the evaluation; 3 sub-
jects were excluded who were extreme outliers with
respect to average answer time or score (not within
\x16 +,2stddev).
From the results in Table 5 we observe the fol-
lowing trends with respect to answer accuracy and
</bodyText>
<footnote confidence="0.890810142857143">
response time:
6One of the 25 segments was set aside for demonstration
purposes.
\x0cEnglish Spanish
number of dialogues 9 14
turns per dialogue marked as relevant by human coders 12% 25%
11-pt-avg precision (average over topical segments) 0.45 0.59
</footnote>
<tableCaption confidence="0.902096428571429">
score variation between dialogues 0.2{0.49 0.15{0.8
Table 4: Summarization results for English and Spanish Callhome
Format full trans clean tele
Time vs. Acc. Time Acc. Time Acc. Time Acc. Time Acc.
generic (q = 72) 75.2 0.814 53.9 0.739 52.6 0.617 54.4 0.622
speci\x0cc (q = 216) 109.1 0.834 82.2 0.624 88.0 0.593 91.6 0.665
Table 5: Average answer times (in sec) and accuracy scores ([0.0-1.0]) over eight di\x0berent tasks (number of
</tableCaption>
<bodyText confidence="0.984884549019608">
subjects=24; q=number of questions per task type).
summary type trans clean tele
generic / indicative 90.8 75.8 76.4
speci\x0cc / informative 74.8 71.0 79.7
Table6: Relativeanswer accuracies in%fordi\x0berent
summaries
\x0f generic questions (\\indicative summaries&quot;, the
task being to identify the topic of a text): The
two cleaned up summaries took about the same
time to process but had lower accuracy scores
than the version directly using the transcript.
\x0f speci\x0cc questions (\\informative summaries&quot;,
the task being to \x0cnd speci\x0cc informationin the
text): (1) The accuracy advantage of the raw
transcript summaries (trans) over the cleaned
up versions (clean) is only small (not statis-
tically signi\x0ccant: t=0.748)7. (2) There is a
superiority of the tele-summary to both other
kinds (tele is signi\x0ccantly more accurate than
clean for p &lt; 0:05).
Fromthis we conjecture that our methods for cus-
tomization of the summaries to spoken dialogues is
mostly relevant for informative, but not so much
for indicative summarization. We think that other
methods, such as lists of signature phrases would be
more e\x0bective to use for the latter purpose.
Table 6 shows the answer accuracy for the three
di\x0berent summary types relative to the accuracy of
the full transcript texts of the same segments (\\rela-
tive answer accuracy&quot;). We observe that the relative
accuracy reduction for all summaries is markedly
lower than the reduction of text size: all summaries
were reduced from the full transcripts by 60%,
whereas the answer accuracy only drops between 9%
(trans) and 24% (clean) for the generic questions,
7In fact, in 2 of 5 dialogues, the clean summary scores
are higher than those of the trans summaries.
and between 20% (tele) and 29% (clean) for the
speci\x0cc questions. This proves that our system is
able to retain most of the relevant information in
the summaries.
As for average answer times, we see a marked re-
duction (30%) of all summaries compared to the full
texts in the generic case; for the speci\x0cc case, the
time reduction is somewhat smaller (15%{25%).
One shortcoming of the current system is that it
operates on turns (or turn-pairs) as minimal units
for extraction. In future work, we will investigate
possibilities to reduce the minimal units of extrac-
tion to the level of clauses or sentences, without giv-
ing up the idea of linking cross-speaker information.
</bodyText>
<sectionHeader confidence="0.875667" genericHeader="conclusions">
11 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.999230952380952">
We have presented a summarization system for spo-
ken dialogues which is constructed to address key
di\x0berences of spoken vs. written language, dialogues
vs. monologues, and multi-topical vs. mono-topical
texts. The system cleans up the input for speech
dis
uencies, links turns together into coherent in-
formation units, determines topical segments, and
extracts the most relevant pieces of information in
a user-customizable way. Evaluations of major sys-
tem components and of the system as a whole were
performed. The results of a user study show that
with a summary size of 40%, between 71% and 91%
of the information of the full text is retained in the
summary, depending on the type of summary and
the types of questions being asked.
We are currently extending the system to be able
tohandle di\x0berent levels ofgranularityfor extraction
(clauses, sentences, turns). Furthermore, we plan to
investigate the integration of prosodic information
into several components of our system.
</bodyText>
<sectionHeader confidence="0.880233" genericHeader="acknowledgments">
12 Acknowledgements
</sectionHeader>
<bodyText confidence="0.828658818181818">
Wewanttothankthe annotators fortheir e\x0borts and
Klaus Ries for providing the automatic speech act
\x0ctagger. We appreciate comments and suggestions
from Alon Lavie, Marsal Gavald\x12
a, Jade Goldstein,
Thomas MacCracken, and the anonymous reviewers
on earlier drafts of this paper.
This work was funded in part by the Verbmobil
project of the Federal Republic of Germany, ATR {
Interpreting Telecommunications Research Labora-
tories of Japan, and the US Department of Defense.
</bodyText>
<sectionHeader confidence="0.971785" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9667676">
AAAI, editor. 1998. Proceedings of the AAAI-98 Spring
Symposium on Intelligent Text Summarization, Stanford,
CA.
ACL. 2000. Proceedings of the ANLP/NAACL-2000 Work-
shop on Automatic Summarization, Seattle, WA, May.
Jan Alexandersson and Peter Poller. 1998. Towards mul-
tilingual protocol generation for spontaneous speech dia-
logues. In Proceedings of the INLG-98, Niagara-on-the-
lake, Canada, August.
Regina Barzilay and Michael Elhadad. 1997. Using lexical
chains for text summarization. In ACL/EACL-97 Work-
shop on Intelligent and Scalable Text Summarization.
Michael Bett, Ralph Gross, Hua Yu, Xiaojin Zhu, Yue Pan,
Jie Yang, and Alex Waibel. 2000. Multimodal meeting
tracker. In Proceedings of the Conference on Content-
Based Multimedia Information Access, RIAO-2000, Paris,
France, April.
Branimir Boguraev and Christopher Kennedy. 1997.
Salience-based characterisation of text documents. In
ACL/EACL-97 Workshop on Intelligent and Scalable Text
Summarization.
Eric Brill. 1994. Some advancesin transformation-basedpart
of speech tagging. In Proceeedings of AAAI-94.
Jaime Carbonelland Jade Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents and
producing summaries. In Proceedings of the 21st ACM-
SIGIR International Conference on Research and Devel-
opment in Information Retrieval, Melbourne, Australia.
John S. Garofolo, Ellen M. Voorhees, Vincent M. Stanford,
and Karen Sparck Jones. 1997. TREC-6 1997 spoken doc-
ument retrieval track overview and results. In Proceed-
ings of the 1997 TREC-6 Conference, Gaithersburg, MD,
November, pages 83{91.
John S. Garofolo, Ellen M. Voorhees, Cedric G. P. Auzanne,
and Vincent M. Stanford. 1999. Spoken document re-
</reference>
<bodyText confidence="0.840600818181818">
trieval: 1998 evaluation and investigation of new metrics.
In Proceedings of the ESCA workshop: Accessing informa-
tion in spoken audio, pages 1{7. Cambridge, UK, April.
Marsal Gavald\x12
a, Klaus Zechner, and Gregory Aist. 1997.
High performancesegmentation of spontaneous speech us-
ing part of speech and trigger word information. In Pro-
ceedings of the 5th ANLP Conference, Washington DC,
pages 12{15.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
Switchboard: telephone speech corpus for research and
</bodyText>
<reference confidence="0.99713043298969">
development. In Proceedings of the ICASSP-92, volume 1,
pages 517{520.
Marti A. Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational Lin-
guistics, 23(1):33{64, March.
Peter A. Heeman, Kyung ho Loken-Kim, and James F. Allen.
1996. Combining the detection and correction of speech
repairs. In Proceedings of ICSLP-96.
Julia Hirschberg and Christine Nakatani. 1998. Acoustic
indicators of topic segmentation. In Proceedings of the
ICSLP-98, Sydney, Australia.
Hongyan Jing. 2000. Sentence reduction for automatic text
summarization. In Proceedings of ANLP-NAACL-2000,
Seattle, WA, May, pages 310{315.
Megumi Kameyama, Goh Kawai, and Isao Arima. 1996. A
real-time system for summarizing human-human sponta-
neous spoken dialogues. In Proceedings of the ICSLP-96,
pages 681{684.
Linguistic Data Consortium (LDC). 1996. CallHome and
CallFriend LVCSR databases.
Fernando S\x13
anchez Le\x13
on. 1994. Spanish tagset for the
CRATER project. http://xxx.lanl.gov/cmp-lg/9406023.
Inderjeet Mani and Mark Maybury, editors. 1997. Proceed-
ings of the ACL/EACL\&amp;apos;97 Workshop on Intelligent Scal-
able Text Summarization, Madrid, Spain.
Inderjeet Mani, David House, Gary Klein, Lynette
Hirschman, Leo Obrst, Therese Firmin, Michael
Chrzanowski, and Beth Sundheim. 1998. The TIP-
STER SUMMAC text summarization evaluation. Mitre
Technical Report MTR 98W0000138, October 1998.
Klaus Ries. 1999. HMM and neural network based speech
act detection. In Proceedings of the ICASSP-99, Phoenix,
Arizona, March.
Gerard Salton and Chris Buckley. 1990. Flexible text match-
ing for information retrieval. Technical report, Cornell
University, Department of Computer Science, TR 90-1158,
September.
Gerard Salton and Michael J. McGill. 1983. Introduction to
Modern Information Retrieval. McGraw Hill, Tokyo etc.
Elizabeth Shriberg, Rebecca Bates, Andreas Stolcke, Paul
Taylor, DanielJurafsky, Klaus Ries, Noah Coccaro, Rachel
Martin, Marie Meteer, and Carol Van Ess-Dykema. 1998.
Can prosody aid the automaticclassi\x0ccationof dialog acts
in conversational speech? Language and Speech, 41(3-
4):439{487.
Andreas Stolcke and Elizabeth Shriberg. 1996. Automatic
linguistic segmentation of conversational speech. In Pro-
ceedings of the ICSLP-96, pages 1005{1008.
Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates, Mari
Ostendorf, Dilek Hakkani, Madeleine Plauche, G\x7f
okhan
T\x7f
ur, and Yu Lu. 1998. Automatic detection of sentence
boundaries and dis
uencies based on recognized words. In
Proceedings of the ICSLP-98, Sydney, Australia, Decem-
ber, volume 5, pages 2247{2250.
Andreas Stolcke, Elizabeth Shriberg,Dilek Hakkani-T\x7f
ur, and
G\x7f
okhan T\x7f
ur. 2000. Prosody-based automatic segmenta-
tion of speech into sentences and topics. Speech Commu-
nication, 32(1-2).
Robin Valenza, Tony Robinson, Marianne Hickey, and Roger
Tucker. 1999. Summarisation of spoken audio through in-
formation extraction. In Proceedings of the ESCA work-
shop: Accessing information in spoken audio, pages 111{
116. Cambridge, UK, April.
Alex Waibel, Michael Bett, and Michael Finke. 1998. Meet-
ing browser: Tracking and summarizingmeetings. In Pro-
ceedings of the DARPA Broadcast News Workshop.
Hua Yu, Michael Finke, and Alex Waibel. 1999. Progress
in automatic meeting transcription. In Proceedings of
EUROSPEECH-99, Budapest, Hungary, September.
Klaus Zechner and Alex Waibel. 1998. Using chunk based
partial parsing of spontaneous speech in unrestricted do-
mains for reducing word error rate in speech recognition.
In Proceedings of COLING-ACL 98, Montreal, Canada.
KlausZechnerandAlex Waibel. 2000. Minimizingword error
rate in textual summaries of spoken language. In Proceed-
ings of the First Meeting of the North American Chapter of
the Association for Computational Linguistics, NAACL-
2000, Seattle, WA, April/May, pages 186{193.
Klaus Zechner. 1997. Building chunk level represen-
tations for spontaneous speech in unrestricted do-
mains: The CHUNKY system and its application to
reranking N-best lists of a speech recognizer. Mas-
ter\&amp;apos;s thesis (project report), CMU, available from:
http://www.cs.cmu.edu/~zechner/publications.html.
Klaus Zechner. 2000. A word-based annota-
tion and evaluation scheme for summariza-
tion of spontaneous speech. Available from
http://www.cs.cmu.edu/~zechner/publications.html.
\x0c&amp;apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.305351">
<title confidence="0.9987165">b&amp;apos;DiaSumm: Flexible Summarization of Spontaneous Dialogues in Unrestricted Domains</title>
<author confidence="0.999911">Klaus Zechner</author>
<author confidence="0.999911">Alex Waibel</author>
<affiliation confidence="0.995208">Language Technologies Institute Carnegie Mellon University</affiliation>
<address confidence="0.9900675">5000 Forbes Avenue Pittsburgh, PA 15213, USA</address>
<email confidence="0.999916">fzechner,waibelg@cs.cmu.edu</email>
<abstract confidence="0.9288762">In this paper, we present a summarization system for spontaneous dialogues which consists of a novel multi-stage architecture. It is speci\x0ccally aimed at addressing issues related to the nature of the texts being spoken vs. written and being dialogical vs. monological. The system is embedded in a graphical user interface and was developed and tested on transcripts of recorded telephone conversations in English and Spanish (Callhome).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>1998</date>
<booktitle>Proceedings of the AAAI-98 Spring Symposium on Intelligent Text Summarization,</booktitle>
<editor>AAAI, editor.</editor>
<location>Stanford, CA.</location>
<marker>1998</marker>
<rawString>AAAI, editor. 1998. Proceedings of the AAAI-98 Spring Symposium on Intelligent Text Summarization, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ACL</author>
</authors>
<date>2000</date>
<booktitle>Proceedings of the ANLP/NAACL-2000 Workshop on Automatic Summarization,</booktitle>
<location>Seattle, WA,</location>
<contexts>
<context position="869" citStr="ACL, 2000" startWordPosition="123" endWordPosition="124">is paper, we present a summarization system for spontaneous dialogues which consists of a novel multi-stage architecture. It is speci\x0ccally aimed at addressing issues related to the nature of the texts being spoken vs. written and being dialogical vs. monological. The system is embedded in a graphical user interface and was developed and tested on transcripts of recorded telephone conversations in English and Spanish (Callhome). 1 Introduction Summarization of written documents has recently been a focus for much research in NLP (e.g., (Mani and Maybury, 1997; AAAI, 1998; Mani et al., 1998; ACL, 2000), to name some of the major events in this \x0celd in the past few years). However, very little attention has been given so far to the summarization of spoken language, even less of conversations vs. monological texts. We believe that summarization of speech will become increasingly more important,as the amountof online audio data grows and demand for rapid browsing, skimming, and access of speech data increases. Another application which particularly pertains to our interest in spoken dialogue summarizationwouldbe the generation of meeting minutes for archival purposes and/or to update partic</context>
</contexts>
<marker>ACL, 2000</marker>
<rawString>ACL. 2000. Proceedings of the ANLP/NAACL-2000 Workshop on Automatic Summarization, Seattle, WA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Alexandersson</author>
<author>Peter Poller</author>
</authors>
<title>Towards multilingual protocol generation for spontaneous speech dialogues.</title>
<date>1998</date>
<booktitle>In Proceedings of the INLG-98,</booktitle>
<location>Niagara-on-thelake, Canada,</location>
<contexts>
<context position="1714" citStr="Alexandersson and Poller, 1998" startWordPosition="252" endWordPosition="255">xts. We believe that summarization of speech will become increasingly more important,as the amountof online audio data grows and demand for rapid browsing, skimming, and access of speech data increases. Another application which particularly pertains to our interest in spoken dialogue summarizationwouldbe the generation of meeting minutes for archival purposes and/or to update participants joining at later stages on the progress of the conversation so far. Summarization of dialogues within limited domains has been attempted within the context of the Verbmobil project (\\protocol generation&quot;, (Alexandersson and Poller, 1998)) or by SRI\&amp;apos;s MIMI summarizer (Kameyama et al., 1996). Recent work on spoken language summarization in unrestricted domains has focused almost exclusively on Broadcast News, mostly due to the spoken language track of recent TREC evaluations (Garofolo et al., 1997; Garofolo et al., 1999). (Waibel et al., 1998) describe a Meeting Browser where summaries can be generated using technology established for written texts. (Valenza et al., 1999) go one step further and incorporate knowledge from the speech recognizer (con\x0cdence scores) into their summarization system, as well. We argue that the na</context>
</contexts>
<marker>Alexandersson, Poller, 1998</marker>
<rawString>Jan Alexandersson and Peter Poller. 1998. Towards multilingual protocol generation for spontaneous speech dialogues. In Proceedings of the INLG-98, Niagara-on-thelake, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using lexical chains for text summarization.</title>
<date>1997</date>
<booktitle>In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization.</booktitle>
<contexts>
<context position="15560" citStr="Barzilay and Elhadad, 1997" startWordPosition="2414" endWordPosition="2418"> out likely candidates for incomplete clauses due to speech repair or interruption by the other speaker. 6 Topic Segmentation Since Callhome dialogues are alwaysmulti-topical, segmenting them into topical units is an important step in our summarization system. This allows us to provide \\signature&quot; information (frequent content words) about every topic to the user as a help for faster browsing and accessing the data. Furthermore, the subsequent information condensation component can work on smallerparts of the dialogue and thus operate more e\x0eciently. Following (Boguraev and Kennedy, 1997; Barzilay and Elhadad, 1997) who use TextTiling (Hearst, 1997) for their summarization systems of written text, we adapted this algorithm (its block comparison version) for speech data: we choose turns to be minimal units and compute block similarity between blocks of k turns every d turns. We use 9 English and 15 Spanish Callhome dialogues, manually annotated for topic boundaries, to determine the optimum values for a set of TextTiling parameters and at the same time to evaluate the accuracy of this algorithm. To do this, we ran an n-fold cross-validation (\\jack-kni\x0cng&quot;) where all dialogues but one are used to deter</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Regina Barzilay and Michael Elhadad. 1997. Using lexical chains for text summarization. In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bett</author>
<author>Ralph Gross</author>
<author>Hua Yu</author>
<author>Xiaojin Zhu</author>
<author>Yue Pan</author>
<author>Jie Yang</author>
<author>Alex Waibel</author>
</authors>
<title>Multimodal meeting tracker.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference on ContentBased Multimedia Information Access, RIAO-2000,</booktitle>
<location>Paris, France,</location>
<contexts>
<context position="19954" citStr="Bett et al., 2000" startWordPosition="3118" endWordPosition="3121">hods for textual analysis that do not generate a dependency structure, we cannot use complex methods for text reduction as described, e.g., in (Jing, 2000). Our method simply excludes words occurring in the stop list from the summary, except for some highlyinformativewords such as \\I&quot; or \ ot&quot;. 9 User Interface and System Performance Since we want to enable interactive summarization which allows a user to browse through a dialogue quickly to search for information he is interested in, we have integrated our summarization system into a JAVA-based graphical user interface (\\Meeting Browser&quot;) (Bett et al., 2000). This interface also integrates the output of a speech recognizer (Yu et al., 1999), and can display a wide variety of information about a conversation, including speech acts, dialogue games, and emotions. For summarization, the user can determine the size of the summary and which topical segments he wants to have displayed. He can also focus the summary on particular content words (\\querybased summary&quot;) or exclude words from consideration (\\dynamic stop list expansion&quot;). Summarizing a 10 minute segment of a Callhome dialogue with our system takes on average less than 30 seconds on a 167 MH</context>
</contexts>
<marker>Bett, Gross, Yu, Zhu, Pan, Yang, Waibel, 2000</marker>
<rawString>Michael Bett, Ralph Gross, Hua Yu, Xiaojin Zhu, Yue Pan, Jie Yang, and Alex Waibel. 2000. Multimodal meeting tracker. In Proceedings of the Conference on ContentBased Multimedia Information Access, RIAO-2000, Paris, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Branimir Boguraev</author>
<author>Christopher Kennedy</author>
</authors>
<title>Salience-based characterisation of text documents.</title>
<date>1997</date>
<booktitle>In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization.</booktitle>
<contexts>
<context position="15531" citStr="Boguraev and Kennedy, 1997" startWordPosition="2410" endWordPosition="2413">l, 1998) is used to \x0clter out likely candidates for incomplete clauses due to speech repair or interruption by the other speaker. 6 Topic Segmentation Since Callhome dialogues are alwaysmulti-topical, segmenting them into topical units is an important step in our summarization system. This allows us to provide \\signature&quot; information (frequent content words) about every topic to the user as a help for faster browsing and accessing the data. Furthermore, the subsequent information condensation component can work on smallerparts of the dialogue and thus operate more e\x0eciently. Following (Boguraev and Kennedy, 1997; Barzilay and Elhadad, 1997) who use TextTiling (Hearst, 1997) for their summarization systems of written text, we adapted this algorithm (its block comparison version) for speech data: we choose turns to be minimal units and compute block similarity between blocks of k turns every d turns. We use 9 English and 15 Spanish Callhome dialogues, manually annotated for topic boundaries, to determine the optimum values for a set of TextTiling parameters and at the same time to evaluate the accuracy of this algorithm. To do this, we ran an n-fold cross-validation (\\jack-kni\x0cng&quot;) where all dialog</context>
</contexts>
<marker>Boguraev, Kennedy, 1997</marker>
<rawString>Branimir Boguraev and Christopher Kennedy. 1997. Salience-based characterisation of text documents. In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Some advancesin transformation-basedpart of speech tagging.</title>
<date>1994</date>
<booktitle>In Proceeedings of AAAI-94.</booktitle>
<contexts>
<context position="14479" citStr="Brill, 1994" startWordPosition="2255" endWordPosition="2256">sequences are combined into a single token (e.g., \\a lot of&quot; ! \\a lot of&quot;). Longer turns are segmented into short clauses, which are de\x0cned as consisting of at least a subject and an in ected verbal form. While (Stolcke and Shriberg, 1996) use n-gram models for this task, and (Gavald\x12 a et al., 1997) use neural networks, we decided to use a rule-based approach (using word and POS information), whose performance proved to be comparable with the results in the cited papers (F1 &amp;gt; 0:85, error &lt; 0:05).2 For several of the clean-up \x0clter\&amp;apos;s components, we make use of Brill\&amp;apos;s POS tagger (Brill, 1994). For English, we use a modi\x0ced version of Brill\&amp;apos;s original tagset, and the tagger was adapted andretrained for spoken language corpora (Callhome and Switchboard) (Zechner, 1997). For Spanish, we created our own tag set, derived from the LDC lexicon and fromthe CRATERproject (Le\x13 on, 1994),andtrained the tagger on manually annotated Callhome dialogues. Furthermore, a POS based shallow chunk parser (Zechner and Waibel, 1998) is used to \x0clter out likely candidates for incomplete clauses due to speech repair or interruption by the other speaker. 6 Topic Segmentation Since Callhome dialo</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>Eric Brill. 1994. Some advancesin transformation-basedpart of speech tagging. In Proceeedings of AAAI-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonelland Jade Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st ACMSIGIR International Conference on Research and Development in Information Retrieval,</booktitle>
<location>Melbourne, Australia.</location>
<contexts>
<context position="17971" citStr="Goldstein, 1998" startWordPosition="2816" endWordPosition="2817">om baseline for seen data. 7 Information Condensation The informationcondensation component is the core of our system. Its purpose is to determine weights for terms and turns (or linked turn-pairs) and then to rank the turns according to their relevance within each topical segment of the dialogue. For term-weighting, tf*idf -inspired formulae (Salton and Buckley, 1990) are used to emphasize words which are in the \\middle range&quot; of frequency in the dialogue and do not appear in a stop list.3 For turn-ranking, we use a version of the \\maximal marginal relevance&quot; (MMR) algorithm (Carbonell and Goldstein, 1998), where emphasis is given to turns which contain many highly weighted terms for the current segment (\\salience&quot;) and are su\x0eciently dissimilar to previously ranked turns (to minimize redundancy). For 9 English and 14 Spanish dialogues, the \\most relevant&quot; turns were marked by human coders. We rana series of cross-validationexperiments to(a) optimize the parameters of this component related to tf*idf and MMR computation and to (b) determine 3For English, our stop list comprises 557 words, for Spanish, 831 words. \x0chowwellthis informationcondensing componentcan match the human relevance a</context>
</contexts>
<marker>Goldstein, 1998</marker>
<rawString>Jaime Carbonelland Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st ACMSIGIR International Conference on Research and Development in Information Retrieval, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Garofolo</author>
<author>Ellen M Voorhees</author>
<author>Vincent M Stanford</author>
<author>Karen Sparck Jones</author>
</authors>
<title>spoken document retrieval track overview and results.</title>
<date>1997</date>
<booktitle>In Proceedings of the 1997 TREC-6 Conference,</booktitle>
<pages>83--91</pages>
<location>Gaithersburg, MD,</location>
<contexts>
<context position="1978" citStr="Garofolo et al., 1997" startWordPosition="293" endWordPosition="296">en dialogue summarizationwouldbe the generation of meeting minutes for archival purposes and/or to update participants joining at later stages on the progress of the conversation so far. Summarization of dialogues within limited domains has been attempted within the context of the Verbmobil project (\\protocol generation&quot;, (Alexandersson and Poller, 1998)) or by SRI\&amp;apos;s MIMI summarizer (Kameyama et al., 1996). Recent work on spoken language summarization in unrestricted domains has focused almost exclusively on Broadcast News, mostly due to the spoken language track of recent TREC evaluations (Garofolo et al., 1997; Garofolo et al., 1999). (Waibel et al., 1998) describe a Meeting Browser where summaries can be generated using technology established for written texts. (Valenza et al., 1999) go one step further and incorporate knowledge from the speech recognizer (con\x0cdence scores) into their summarization system, as well. We argue that the nature of spoken dialogues, together with their textual representations as speech recognizer hypotheses, requires a set of speci\x0cc approaches to make summarization feasible for this text genre. As a demonstrable proof of concept, we present the multi-stage archit</context>
</contexts>
<marker>Garofolo, Voorhees, Stanford, Jones, 1997</marker>
<rawString>John S. Garofolo, Ellen M. Voorhees, Vincent M. Stanford, and Karen Sparck Jones. 1997. TREC-6 1997 spoken document retrieval track overview and results. In Proceedings of the 1997 TREC-6 Conference, Gaithersburg, MD, November, pages 83{91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Garofolo</author>
<author>Ellen M Voorhees</author>
<author>Cedric G P Auzanne</author>
<author>Vincent M Stanford</author>
</authors>
<title>Spoken document redevelopment.</title>
<date>1999</date>
<booktitle>In Proceedings of the ICASSP-92,</booktitle>
<volume>1</volume>
<pages>517--520</pages>
<contexts>
<context position="2002" citStr="Garofolo et al., 1999" startWordPosition="297" endWordPosition="300">onwouldbe the generation of meeting minutes for archival purposes and/or to update participants joining at later stages on the progress of the conversation so far. Summarization of dialogues within limited domains has been attempted within the context of the Verbmobil project (\\protocol generation&quot;, (Alexandersson and Poller, 1998)) or by SRI\&amp;apos;s MIMI summarizer (Kameyama et al., 1996). Recent work on spoken language summarization in unrestricted domains has focused almost exclusively on Broadcast News, mostly due to the spoken language track of recent TREC evaluations (Garofolo et al., 1997; Garofolo et al., 1999). (Waibel et al., 1998) describe a Meeting Browser where summaries can be generated using technology established for written texts. (Valenza et al., 1999) go one step further and incorporate knowledge from the speech recognizer (con\x0cdence scores) into their summarization system, as well. We argue that the nature of spoken dialogues, together with their textual representations as speech recognizer hypotheses, requires a set of speci\x0cc approaches to make summarization feasible for this text genre. As a demonstrable proof of concept, we present the multi-stage architecture of the summarizat</context>
</contexts>
<marker>Garofolo, Voorhees, Auzanne, Stanford, 1999</marker>
<rawString>John S. Garofolo, Ellen M. Voorhees, Cedric G. P. Auzanne, and Vincent M. Stanford. 1999. Spoken document redevelopment. In Proceedings of the ICASSP-92, volume 1, pages 517{520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="7941" citStr="Hearst, 1997" startWordPosition="1234" endWordPosition="1235">t: before: I MEAN WE LOSE WE LOSE I CAN\&amp;apos;T I CAN\&amp;apos;T DO ANYTHING ABOUT IT SO after: we lose / i can\&amp;apos;t do anything about it 2.4 Lack of topic boundaries Callhome speech data is multi-topical but does not include mark-up for paragraphs, nor any topicinformative headers. Typically, we \x0cnd about 5{10 di\x0berent topics within a 10-minute segment of a dialogue, i.e., the topic changes about every 1{2 minutes in these conversations. To facilitate browsing and summarization, we thus have to discover topically coherent segments automatically. This is done using a TextTiling approach, adapted from (Hearst, 1997) (section 6). 2.5 Speech recognizer errors Last but not least, we face the problem of imperfect word accuracy of speech recognizers, particularly when dealing with spontaneous speech over a large vocabulary and over a low bandwidth channel, such as the Callhome databases which we mainly used for development, testing, and evaluation of our system. Current recognizers typically exhibit word error rates for these corpora in the order of 50%. In DiaSumm\&amp;apos;s information condensation component, the relevance weights of speaker turns can be adjusted to take into account their word con\x0cdence scores </context>
<context position="15594" citStr="Hearst, 1997" startWordPosition="2422" endWordPosition="2423">e to speech repair or interruption by the other speaker. 6 Topic Segmentation Since Callhome dialogues are alwaysmulti-topical, segmenting them into topical units is an important step in our summarization system. This allows us to provide \\signature&quot; information (frequent content words) about every topic to the user as a help for faster browsing and accessing the data. Furthermore, the subsequent information condensation component can work on smallerparts of the dialogue and thus operate more e\x0eciently. Following (Boguraev and Kennedy, 1997; Barzilay and Elhadad, 1997) who use TextTiling (Hearst, 1997) for their summarization systems of written text, we adapted this algorithm (its block comparison version) for speech data: we choose turns to be minimal units and compute block similarity between blocks of k turns every d turns. We use 9 English and 15 Spanish Callhome dialogues, manually annotated for topic boundaries, to determine the optimum values for a set of TextTiling parameters and at the same time to evaluate the accuracy of this algorithm. To do this, we ran an n-fold cross-validation (\\jack-kni\x0cng&quot;) where all dialogues but one are used to determine the best parameters (\\train </context>
<context position="17294" citStr="Hearst, 1997" startWordPosition="2708" endWordPosition="2709">tion results for English and Spanish Callhome dialogues (F1-scores) a held-out data set for evaluation (\\test set&quot;). This process is repeated n times and average results are reported. Table 2 shows the set of parameters which worked best for most dialogues and Table 3 shows the evaluation results of the cross-validation experiment. F1-scores improve by 18{24% absolute over the random baseline for unseen and by 23{35% for seen data, the performance for English being better than for Spanish. These results, albeit achieved on a quite di\x0berent text genre, are well in line with the results in (Hearst, 1997) who reports an absolute improvement of about 20% over a random baseline for seen data. 7 Information Condensation The informationcondensation component is the core of our system. Its purpose is to determine weights for terms and turns (or linked turn-pairs) and then to rank the turns according to their relevance within each topical segment of the dialogue. For term-weighting, tf*idf -inspired formulae (Salton and Buckley, 1990) are used to emphasize words which are in the \\middle range&quot; of frequency in the dialogue and do not appear in a stop list.3 For turn-ranking, we use a version of the </context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti A. Hearst. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33{64, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
<author>Kyung ho Loken-Kim</author>
<author>James F Allen</author>
</authors>
<title>Combining the detection and correction of speech repairs.</title>
<date>1996</date>
<booktitle>In Proceedings of ICSLP-96.</booktitle>
<marker>Heeman, Loken-Kim, Allen, 1996</marker>
<rawString>Peter A. Heeman, Kyung ho Loken-Kim, and James F. Allen. 1996. Combining the detection and correction of speech repairs. In Proceedings of ICSLP-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Christine Nakatani</author>
</authors>
<title>Acoustic indicators of topic segmentation.</title>
<date>1998</date>
<booktitle>In Proceedings of the ICSLP-98,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="9128" citStr="Hirschberg and Nakatani, 1998" startWordPosition="1415" endWordPosition="1418">to account their word con\x0cdence scores from the speech recognizer. That way we can reduce the likelihood of extracting passages with a larger amount of word misrecognitions (Zechner and Waibel, 2000). In this paper, however, the focus will be exclusively on results of our evaluations on humangenerated transcripts. No informationfromthe speech recognizer nor fromthe acoustic signal (other than inter-utterance pause durations) are used. We are aware that in particular prosodic information may be of help for tasks such as the detection of sentence boundaries, speech acts, or topic boundaries (Hirschberg and Nakatani, 1998; Shriberg et al., 1998; Stolcke et al., 2000), but the investigation of the integration of this additional source of informationis beyond the scope of this paper and left for future work. 3 System Architecture The global system architecture of DiaSumm is a pipeline of the following four major components: \x0cTRANS Turn Linking Clean-up Filter Topic Segmentation Information Condensation Telegraphic Reduction CLEAN and TELE input for TRANS CLEAN TELE input for Figure 1: System architecture turn linking; clean-up \x0clter; topic segmentation; and information condensation. A \x0cfth component is </context>
</contexts>
<marker>Hirschberg, Nakatani, 1998</marker>
<rawString>Julia Hirschberg and Christine Nakatani. 1998. Acoustic indicators of topic segmentation. In Proceedings of the ICSLP-98, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP-NAACL-2000,</booktitle>
<pages>310--315</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="19491" citStr="Jing, 2000" startWordPosition="3047" endWordPosition="3048">rom these experiments. Similar to other experiments in the summarizationliterature (Mani et al., 1998), we \x0cnd a wide performance variation across di\x0berent texts. 8 Telegraphic Reduction The purpose of this component is to maximizeinformation in a \x0cxed amount of space. We shorten the output of the summarizer to a \\telegraphic style&quot;; that way, more information can be included in a summary of k words (or n bytes). Since we only use shallow methods for textual analysis that do not generate a dependency structure, we cannot use complex methods for text reduction as described, e.g., in (Jing, 2000). Our method simply excludes words occurring in the stop list from the summary, except for some highlyinformativewords such as \\I&quot; or \ ot&quot;. 9 User Interface and System Performance Since we want to enable interactive summarization which allows a user to browse through a dialogue quickly to search for information he is interested in, we have integrated our summarization system into a JAVA-based graphical user interface (\\Meeting Browser&quot;) (Bett et al., 2000). This interface also integrates the output of a speech recognizer (Yu et al., 1999), and can display a wide variety of information about</context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>Hongyan Jing. 2000. Sentence reduction for automatic text summarization. In Proceedings of ANLP-NAACL-2000, Seattle, WA, May, pages 310{315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Megumi Kameyama</author>
<author>Goh Kawai</author>
<author>Isao Arima</author>
</authors>
<title>A real-time system for summarizing human-human spontaneous spoken dialogues.</title>
<date>1996</date>
<booktitle>In Proceedings of the ICSLP-96,</booktitle>
<pages>681--684</pages>
<contexts>
<context position="1768" citStr="Kameyama et al., 1996" startWordPosition="261" endWordPosition="264">singly more important,as the amountof online audio data grows and demand for rapid browsing, skimming, and access of speech data increases. Another application which particularly pertains to our interest in spoken dialogue summarizationwouldbe the generation of meeting minutes for archival purposes and/or to update participants joining at later stages on the progress of the conversation so far. Summarization of dialogues within limited domains has been attempted within the context of the Verbmobil project (\\protocol generation&quot;, (Alexandersson and Poller, 1998)) or by SRI\&amp;apos;s MIMI summarizer (Kameyama et al., 1996). Recent work on spoken language summarization in unrestricted domains has focused almost exclusively on Broadcast News, mostly due to the spoken language track of recent TREC evaluations (Garofolo et al., 1997; Garofolo et al., 1999). (Waibel et al., 1998) describe a Meeting Browser where summaries can be generated using technology established for written texts. (Valenza et al., 1999) go one step further and incorporate knowledge from the speech recognizer (con\x0cdence scores) into their summarization system, as well. We argue that the nature of spoken dialogues, together with their textual </context>
</contexts>
<marker>Kameyama, Kawai, Arima, 1996</marker>
<rawString>Megumi Kameyama, Goh Kawai, and Isao Arima. 1996. A real-time system for summarizing human-human spontaneous spoken dialogues. In Proceedings of the ICSLP-96, pages 681{684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<title>CallHome and CallFriend LVCSR databases.</title>
<date>1996</date>
<marker>Consortium, 1996</marker>
<rawString>Linguistic Data Consortium (LDC). 1996. CallHome and CallFriend LVCSR databases.</rawString>
</citation>
<citation valid="true">
<date>1994</date>
<booktitle>Fernando S\x13 anchez Le\x13 on.</booktitle>
<note>Spanish tagset for the CRATER project. http://xxx.lanl.gov/cmp-lg/9406023.</note>
<marker>1994</marker>
<rawString>Fernando S\x13 anchez Le\x13 on. 1994. Spanish tagset for the CRATER project. http://xxx.lanl.gov/cmp-lg/9406023.</rawString>
</citation>
<citation valid="true">
<date>1997</date>
<booktitle>Proceedings of the ACL/EACL\&amp;apos;97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<editor>Inderjeet Mani and Mark Maybury, editors.</editor>
<location>Madrid, Spain.</location>
<marker>1997</marker>
<rawString>Inderjeet Mani and Mark Maybury, editors. 1997. Proceedings of the ACL/EACL\&amp;apos;97 Workshop on Intelligent Scalable Text Summarization, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>David House</author>
<author>Gary Klein</author>
<author>Lynette Hirschman</author>
<author>Leo Obrst</author>
<author>Therese Firmin</author>
<author>Michael Chrzanowski</author>
<author>Beth Sundheim</author>
</authors>
<title>The TIPSTER SUMMAC text summarization evaluation.</title>
<date>1998</date>
<tech>Mitre Technical Report MTR 98W0000138,</tech>
<contexts>
<context position="857" citStr="Mani et al., 1998" startWordPosition="119" endWordPosition="122">.edu Abstract In this paper, we present a summarization system for spontaneous dialogues which consists of a novel multi-stage architecture. It is speci\x0ccally aimed at addressing issues related to the nature of the texts being spoken vs. written and being dialogical vs. monological. The system is embedded in a graphical user interface and was developed and tested on transcripts of recorded telephone conversations in English and Spanish (Callhome). 1 Introduction Summarization of written documents has recently been a focus for much research in NLP (e.g., (Mani and Maybury, 1997; AAAI, 1998; Mani et al., 1998; ACL, 2000), to name some of the major events in this \x0celd in the past few years). However, very little attention has been given so far to the summarization of spoken language, even less of conversations vs. monological texts. We believe that summarization of speech will become increasingly more important,as the amountof online audio data grows and demand for rapid browsing, skimming, and access of speech data increases. Another application which particularly pertains to our interest in spoken dialogue summarizationwouldbe the generation of meeting minutes for archival purposes and/or to u</context>
<context position="18982" citStr="Mani et al., 1998" startWordPosition="2961" endWordPosition="2964">ated to tf*idf and MMR computation and to (b) determine 3For English, our stop list comprises 557 words, for Spanish, 831 words. \x0chowwellthis informationcondensing componentcan match the human relevance annotations. Summarization results are computed using 11-ptavg precision scores for ranked turn lists where the maximum precision of the list of retrieved turns is averaged in the 11 evenly spaced intervals between recall=[0,0.1),[0.1,0.2), ... [1.0,1.1) (Salton and McGill, 1983).4 Table 4 shows the results from these experiments. Similar to other experiments in the summarizationliterature (Mani et al., 1998), we \x0cnd a wide performance variation across di\x0berent texts. 8 Telegraphic Reduction The purpose of this component is to maximizeinformation in a \x0cxed amount of space. We shorten the output of the summarizer to a \\telegraphic style&quot;; that way, more information can be included in a summary of k words (or n bytes). Since we only use shallow methods for textual analysis that do not generate a dependency structure, we cannot use complex methods for text reduction as described, e.g., in (Jing, 2000). Our method simply excludes words occurring in the stop list from the summary, except for </context>
<context position="21671" citStr="Mani et al., 1998" startWordPosition="3403" endWordPosition="3406">eriment Setup In order to evaluate the system as a whole, we conducted a study with humansin the loop to be able to compare three types of summaries (trans, clean, tele, see section 3) with the full original transcript. We address these two main questions in this study: (i) how fast can information be identi\x0ced using different types of summaries? (ii) how accurately is the information preserved, comparing di\x0berent types of summaries? We did not only ask the user \ arrow&quot; questions for a speci\x0cc piece of information |along the lines of the Q-A-evaluation part of the SUMMAC conference (Mani et al., 1998) |but also very \\global&quot;, non-speci\x0cc questions, tied to a particular (topical) segment of the dialogue. The experiment was conducted as follows: Subjects were given24texts each, accompaniedbyeither a generic question (\\What is the topic of the discussion in this text segment?&quot;) or three speci\x0cc questions (e.g., \\Which clothes did speaker A buy?&quot;). The texts were drawn from \x0cve topical segments each from \x0cve English Callhome dialogues.6 They have four di\x0berent formats: (a) full transcripts (i.e., the transcript of the whole segment) (full); (b) summaryof the raw transcripts (</context>
</contexts>
<marker>Mani, House, Klein, Hirschman, Obrst, Firmin, Chrzanowski, Sundheim, 1998</marker>
<rawString>Inderjeet Mani, David House, Gary Klein, Lynette Hirschman, Leo Obrst, Therese Firmin, Michael Chrzanowski, and Beth Sundheim. 1998. The TIPSTER SUMMAC text summarization evaluation. Mitre Technical Report MTR 98W0000138, October 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Ries</author>
</authors>
<title>HMM and neural network based speech act detection.</title>
<date>1999</date>
<booktitle>In Proceedings of the ICASSP-99,</booktitle>
<location>Phoenix, Arizona,</location>
<contexts>
<context position="11639" citStr="Ries, 1999" startWordPosition="1803" endWordPosition="1804">2 = 1:0. For the second objective, to formturn-pairs which comprise a question-answer information exchange between twodialogueparticipants, we need to detect wh- and yes-no-questions in the dialogue. We tested English Spanish Annotated Data turns 1603 1185 Wh-questions 42 78 yes-no-questions 43 98 questions total 85 (5.3%) 176 (14.9%) Automatic Detection Results (F1) SA classi\x0cer 0.24 0.22 POS rules 0.22 0.37 random baseline 0.02 0.13 Table 1: Q-A-pair distribution in the data and experimental results for automatic Q-A-detection two approaches: (a) a HMM based speech act (SA) classi\x0cer (Ries, 1999) and (b) a set of part-of-speech (POS) based rules. The SA classi\x0cer was trained on dialogues which were manuallyannotated for speech acts, using parts of the Switchboard corpus (Godfrey et al., 1992) for English and Callhome for Spanish. The corresponding answers for the detected questions were hypothesized in the \x0crst turn with a di\x0berent speaker, followingthe question-turn. Table 1 shows the results of these experiments for 5 English and 5 Spanish Callhome dialogues, compared to a baseline of randomlyassigning n question speech acts, n being the number of question-turns marked by h</context>
</contexts>
<marker>Ries, 1999</marker>
<rawString>Klaus Ries. 1999. HMM and neural network based speech act detection. In Proceedings of the ICASSP-99, Phoenix, Arizona, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Chris Buckley</author>
</authors>
<title>Flexible text matching for information retrieval.</title>
<date>1990</date>
<tech>Technical report,</tech>
<institution>Cornell University, Department of Computer Science,</institution>
<contexts>
<context position="17726" citStr="Salton and Buckley, 1990" startWordPosition="2773" endWordPosition="2776">% for seen data, the performance for English being better than for Spanish. These results, albeit achieved on a quite di\x0berent text genre, are well in line with the results in (Hearst, 1997) who reports an absolute improvement of about 20% over a random baseline for seen data. 7 Information Condensation The informationcondensation component is the core of our system. Its purpose is to determine weights for terms and turns (or linked turn-pairs) and then to rank the turns according to their relevance within each topical segment of the dialogue. For term-weighting, tf*idf -inspired formulae (Salton and Buckley, 1990) are used to emphasize words which are in the \\middle range&quot; of frequency in the dialogue and do not appear in a stop list.3 For turn-ranking, we use a version of the \\maximal marginal relevance&quot; (MMR) algorithm (Carbonell and Goldstein, 1998), where emphasis is given to turns which contain many highly weighted terms for the current segment (\\salience&quot;) and are su\x0eciently dissimilar to previously ranked turns (to minimize redundancy). For 9 English and 14 Spanish dialogues, the \\most relevant&quot; turns were marked by human coders. We rana series of cross-validationexperiments to(a) optimiz</context>
</contexts>
<marker>Salton, Buckley, 1990</marker>
<rawString>Gerard Salton and Chris Buckley. 1990. Flexible text matching for information retrieval. Technical report, Cornell University, Department of Computer Science, TR 90-1158, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw Hill,</publisher>
<location>Tokyo etc.</location>
<contexts>
<context position="18850" citStr="Salton and McGill, 1983" startWordPosition="2942" endWordPosition="2945">vant&quot; turns were marked by human coders. We rana series of cross-validationexperiments to(a) optimize the parameters of this component related to tf*idf and MMR computation and to (b) determine 3For English, our stop list comprises 557 words, for Spanish, 831 words. \x0chowwellthis informationcondensing componentcan match the human relevance annotations. Summarization results are computed using 11-ptavg precision scores for ranked turn lists where the maximum precision of the list of retrieved turns is averaged in the 11 evenly spaced intervals between recall=[0,0.1),[0.1,0.2), ... [1.0,1.1) (Salton and McGill, 1983).4 Table 4 shows the results from these experiments. Similar to other experiments in the summarizationliterature (Mani et al., 1998), we \x0cnd a wide performance variation across di\x0berent texts. 8 Telegraphic Reduction The purpose of this component is to maximizeinformation in a \x0cxed amount of space. We shorten the output of the summarizer to a \\telegraphic style&quot;; that way, more information can be included in a summary of k words (or n bytes). Since we only use shallow methods for textual analysis that do not generate a dependency structure, we cannot use complex methods for text redu</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and Michael J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw Hill, Tokyo etc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Andreas Stolcke</author>
<author>Paul Taylor</author>
<author>Klaus Ries DanielJurafsky</author>
<author>Noah Coccaro</author>
<author>Rachel Martin</author>
<author>Marie Meteer</author>
<author>Carol Van Ess-Dykema</author>
</authors>
<title>Can prosody aid the automaticclassi\x0ccationof dialog acts in conversational speech? Language and Speech,</title>
<date>1998</date>
<pages>41--3</pages>
<marker>Shriberg, Bates, Stolcke, Taylor, DanielJurafsky, Coccaro, Martin, Meteer, Van Ess-Dykema, 1998</marker>
<rawString>Elizabeth Shriberg, Rebecca Bates, Andreas Stolcke, Paul Taylor, DanielJurafsky, Klaus Ries, Noah Coccaro, Rachel Martin, Marie Meteer, and Carol Van Ess-Dykema. 1998. Can prosody aid the automaticclassi\x0ccationof dialog acts in conversational speech? Language and Speech, 41(3-4):439{487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Automatic linguistic segmentation of conversational speech.</title>
<date>1996</date>
<booktitle>In Proceedings of the ICSLP-96,</booktitle>
<pages>1005--1008</pages>
<contexts>
<context position="14111" citStr="Stolcke and Shriberg, 1996" startWordPosition="2190" endWordPosition="2193">. Sincethe focusand goalsof thiscomponent are somewhat di\x0berent than previous work in that area, meaningful comparisons are hard to make. \x0cSingle or multiple word repetitions, \x0cllers (e.g., \\uhm&quot;), and discourse markers without semantic content (e.g., \\you know&quot;) are removed from the input, some short forms are expanded (e.g., \\we\&amp;apos;ll&quot; ! \\we will&quot;), and frequent word sequences are combined into a single token (e.g., \\a lot of&quot; ! \\a lot of&quot;). Longer turns are segmented into short clauses, which are de\x0cned as consisting of at least a subject and an in ected verbal form. While (Stolcke and Shriberg, 1996) use n-gram models for this task, and (Gavald\x12 a et al., 1997) use neural networks, we decided to use a rule-based approach (using word and POS information), whose performance proved to be comparable with the results in the cited papers (F1 &amp;gt; 0:85, error &lt; 0:05).2 For several of the clean-up \x0clter\&amp;apos;s components, we make use of Brill\&amp;apos;s POS tagger (Brill, 1994). For English, we use a modi\x0ced version of Brill\&amp;apos;s original tagset, and the tagger was adapted andretrained for spoken language corpora (Callhome and Switchboard) (Zechner, 1997). For Spanish, we created our own tag set, derived</context>
</contexts>
<marker>Stolcke, Shriberg, 1996</marker>
<rawString>Andreas Stolcke and Elizabeth Shriberg. 1996. Automatic linguistic segmentation of conversational speech. In Proceedings of the ICSLP-96, pages 1005{1008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Mari Ostendorf</author>
<author>Dilek Hakkani</author>
<author>Madeleine Plauche</author>
</authors>
<title>Automatic detection of sentence boundaries and dis uencies based on recognized words.</title>
<date>1998</date>
<booktitle>G\x7f okhan T\x7f</booktitle>
<volume>5</volume>
<pages>2247--2250</pages>
<location>Sydney, Australia,</location>
<marker>Stolcke, Shriberg, Bates, Ostendorf, Hakkani, Plauche, 1998</marker>
<rawString>Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates, Mari Ostendorf, Dilek Hakkani, Madeleine Plauche, G\x7f okhan T\x7f ur, and Yu Lu. 1998. Automatic detection of sentence boundaries and dis uencies based on recognized words. In Proceedings of the ICSLP-98, Sydney, Australia, December, volume 5, pages 2247{2250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Elizabeth Shriberg,Dilek Hakkani-T\x7f ur, and G\x7f okhan T\x7f ur.</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<pages>32--1</pages>
<marker>Stolcke, 2000</marker>
<rawString>Andreas Stolcke, Elizabeth Shriberg,Dilek Hakkani-T\x7f ur, and G\x7f okhan T\x7f ur. 2000. Prosody-based automatic segmentation of speech into sentences and topics. Speech Communication, 32(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Valenza</author>
<author>Tony Robinson</author>
<author>Marianne Hickey</author>
<author>Roger Tucker</author>
</authors>
<title>Summarisation of spoken audio through information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the ESCA workshop: Accessing information in spoken audio,</booktitle>
<pages>111--116</pages>
<location>Cambridge, UK,</location>
<contexts>
<context position="2156" citStr="Valenza et al., 1999" startWordPosition="321" endWordPosition="324"> so far. Summarization of dialogues within limited domains has been attempted within the context of the Verbmobil project (\\protocol generation&quot;, (Alexandersson and Poller, 1998)) or by SRI\&amp;apos;s MIMI summarizer (Kameyama et al., 1996). Recent work on spoken language summarization in unrestricted domains has focused almost exclusively on Broadcast News, mostly due to the spoken language track of recent TREC evaluations (Garofolo et al., 1997; Garofolo et al., 1999). (Waibel et al., 1998) describe a Meeting Browser where summaries can be generated using technology established for written texts. (Valenza et al., 1999) go one step further and incorporate knowledge from the speech recognizer (con\x0cdence scores) into their summarization system, as well. We argue that the nature of spoken dialogues, together with their textual representations as speech recognizer hypotheses, requires a set of speci\x0cc approaches to make summarization feasible for this text genre. As a demonstrable proof of concept, we present the multi-stage architecture of the summarization system DiaSumm which can exibly deal with spoken dialogues in English and Spanish, without any restrictions of domain. Since it cannot rely on any dom</context>
</contexts>
<marker>Valenza, Robinson, Hickey, Tucker, 1999</marker>
<rawString>Robin Valenza, Tony Robinson, Marianne Hickey, and Roger Tucker. 1999. Summarisation of spoken audio through information extraction. In Proceedings of the ESCA workshop: Accessing information in spoken audio, pages 111{ 116. Cambridge, UK, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Waibel</author>
<author>Michael Bett</author>
<author>Michael Finke</author>
</authors>
<title>Meeting browser: Tracking and summarizingmeetings.</title>
<date>1998</date>
<booktitle>In Proceedings of the DARPA Broadcast News Workshop.</booktitle>
<contexts>
<context position="2025" citStr="Waibel et al., 1998" startWordPosition="301" endWordPosition="304">of meeting minutes for archival purposes and/or to update participants joining at later stages on the progress of the conversation so far. Summarization of dialogues within limited domains has been attempted within the context of the Verbmobil project (\\protocol generation&quot;, (Alexandersson and Poller, 1998)) or by SRI\&amp;apos;s MIMI summarizer (Kameyama et al., 1996). Recent work on spoken language summarization in unrestricted domains has focused almost exclusively on Broadcast News, mostly due to the spoken language track of recent TREC evaluations (Garofolo et al., 1997; Garofolo et al., 1999). (Waibel et al., 1998) describe a Meeting Browser where summaries can be generated using technology established for written texts. (Valenza et al., 1999) go one step further and incorporate knowledge from the speech recognizer (con\x0cdence scores) into their summarization system, as well. We argue that the nature of spoken dialogues, together with their textual representations as speech recognizer hypotheses, requires a set of speci\x0cc approaches to make summarization feasible for this text genre. As a demonstrable proof of concept, we present the multi-stage architecture of the summarization system DiaSumm whic</context>
</contexts>
<marker>Waibel, Bett, Finke, 1998</marker>
<rawString>Alex Waibel, Michael Bett, and Michael Finke. 1998. Meeting browser: Tracking and summarizingmeetings. In Proceedings of the DARPA Broadcast News Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Yu</author>
<author>Michael Finke</author>
<author>Alex Waibel</author>
</authors>
<title>Progress in automatic meeting transcription.</title>
<date>1999</date>
<booktitle>In Proceedings of EUROSPEECH-99,</booktitle>
<location>Budapest, Hungary,</location>
<contexts>
<context position="20038" citStr="Yu et al., 1999" startWordPosition="3132" endWordPosition="3135">omplex methods for text reduction as described, e.g., in (Jing, 2000). Our method simply excludes words occurring in the stop list from the summary, except for some highlyinformativewords such as \\I&quot; or \ ot&quot;. 9 User Interface and System Performance Since we want to enable interactive summarization which allows a user to browse through a dialogue quickly to search for information he is interested in, we have integrated our summarization system into a JAVA-based graphical user interface (\\Meeting Browser&quot;) (Bett et al., 2000). This interface also integrates the output of a speech recognizer (Yu et al., 1999), and can display a wide variety of information about a conversation, including speech acts, dialogue games, and emotions. For summarization, the user can determine the size of the summary and which topical segments he wants to have displayed. He can also focus the summary on particular content words (\\querybased summary&quot;) or exclude words from consideration (\\dynamic stop list expansion&quot;). Summarizing a 10 minute segment of a Callhome dialogue with our system takes on average less than 30 seconds on a 167 MHz 320 MB Sun Ultra1 workstation.5 4We are aware that this annotationand evaluationsc</context>
</contexts>
<marker>Yu, Finke, Waibel, 1999</marker>
<rawString>Hua Yu, Michael Finke, and Alex Waibel. 1999. Progress in automatic meeting transcription. In Proceedings of EUROSPEECH-99, Budapest, Hungary, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
<author>Alex Waibel</author>
</authors>
<title>Using chunk based partial parsing of spontaneous speech in unrestricted domains for reducing word error rate in speech recognition.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL 98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="14913" citStr="Zechner and Waibel, 1998" startWordPosition="2318" endWordPosition="2321">e proved to be comparable with the results in the cited papers (F1 &amp;gt; 0:85, error &lt; 0:05).2 For several of the clean-up \x0clter\&amp;apos;s components, we make use of Brill\&amp;apos;s POS tagger (Brill, 1994). For English, we use a modi\x0ced version of Brill\&amp;apos;s original tagset, and the tagger was adapted andretrained for spoken language corpora (Callhome and Switchboard) (Zechner, 1997). For Spanish, we created our own tag set, derived from the LDC lexicon and fromthe CRATERproject (Le\x13 on, 1994),andtrained the tagger on manually annotated Callhome dialogues. Furthermore, a POS based shallow chunk parser (Zechner and Waibel, 1998) is used to \x0clter out likely candidates for incomplete clauses due to speech repair or interruption by the other speaker. 6 Topic Segmentation Since Callhome dialogues are alwaysmulti-topical, segmenting them into topical units is an important step in our summarization system. This allows us to provide \\signature&quot; information (frequent content words) about every topic to the user as a help for faster browsing and accessing the data. Furthermore, the subsequent information condensation component can work on smallerparts of the dialogue and thus operate more e\x0eciently. Following (Boguraev</context>
</contexts>
<marker>Zechner, Waibel, 1998</marker>
<rawString>Klaus Zechner and Alex Waibel. 1998. Using chunk based partial parsing of spontaneous speech in unrestricted domains for reducing word error rate in speech recognition. In Proceedings of COLING-ACL 98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>KlausZechnerandAlex Waibel</author>
</authors>
<title>Minimizingword error rate in textual summaries of spoken language.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics, NAACL2000,</booktitle>
<pages>186--193</pages>
<location>Seattle, WA, April/May,</location>
<contexts>
<context position="8701" citStr="Waibel, 2000" startWordPosition="1353" endWordPosition="1354">hen dealing with spontaneous speech over a large vocabulary and over a low bandwidth channel, such as the Callhome databases which we mainly used for development, testing, and evaluation of our system. Current recognizers typically exhibit word error rates for these corpora in the order of 50%. In DiaSumm\&amp;apos;s information condensation component, the relevance weights of speaker turns can be adjusted to take into account their word con\x0cdence scores from the speech recognizer. That way we can reduce the likelihood of extracting passages with a larger amount of word misrecognitions (Zechner and Waibel, 2000). In this paper, however, the focus will be exclusively on results of our evaluations on humangenerated transcripts. No informationfromthe speech recognizer nor fromthe acoustic signal (other than inter-utterance pause durations) are used. We are aware that in particular prosodic information may be of help for tasks such as the detection of sentence boundaries, speech acts, or topic boundaries (Hirschberg and Nakatani, 1998; Shriberg et al., 1998; Stolcke et al., 2000), but the investigation of the integration of this additional source of informationis beyond the scope of this paper and left f</context>
</contexts>
<marker>Waibel, 2000</marker>
<rawString>KlausZechnerandAlex Waibel. 2000. Minimizingword error rate in textual summaries of spoken language. In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics, NAACL2000, Seattle, WA, April/May, pages 186{193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
</authors>
<title>Building chunk level representations for spontaneous speech in unrestricted domains: The CHUNKY system and its application to reranking N-best lists of a speech recognizer. Master\&amp;apos;s thesis (project report), CMU, available from:</title>
<date>1997</date>
<note>http://www.cs.cmu.edu/~zechner/publications.html.</note>
<contexts>
<context position="14661" citStr="Zechner, 1997" startWordPosition="2282" endWordPosition="2283"> and an in ected verbal form. While (Stolcke and Shriberg, 1996) use n-gram models for this task, and (Gavald\x12 a et al., 1997) use neural networks, we decided to use a rule-based approach (using word and POS information), whose performance proved to be comparable with the results in the cited papers (F1 &amp;gt; 0:85, error &lt; 0:05).2 For several of the clean-up \x0clter\&amp;apos;s components, we make use of Brill\&amp;apos;s POS tagger (Brill, 1994). For English, we use a modi\x0ced version of Brill\&amp;apos;s original tagset, and the tagger was adapted andretrained for spoken language corpora (Callhome and Switchboard) (Zechner, 1997). For Spanish, we created our own tag set, derived from the LDC lexicon and fromthe CRATERproject (Le\x13 on, 1994),andtrained the tagger on manually annotated Callhome dialogues. Furthermore, a POS based shallow chunk parser (Zechner and Waibel, 1998) is used to \x0clter out likely candidates for incomplete clauses due to speech repair or interruption by the other speaker. 6 Topic Segmentation Since Callhome dialogues are alwaysmulti-topical, segmenting them into topical units is an important step in our summarization system. This allows us to provide \\signature&quot; information (frequent conten</context>
</contexts>
<marker>Zechner, 1997</marker>
<rawString>Klaus Zechner. 1997. Building chunk level representations for spontaneous speech in unrestricted domains: The CHUNKY system and its application to reranking N-best lists of a speech recognizer. Master\&amp;apos;s thesis (project report), CMU, available from: http://www.cs.cmu.edu/~zechner/publications.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
</authors>
<title>A word-based annotation and evaluation scheme for summarization of spontaneous speech. Available from http://www.cs.cmu.edu/~zechner/publications.html. \x0c&amp;apos;</title>
<date>2000</date>
<contexts>
<context position="20971" citStr="Zechner, 2000" startWordPosition="3287" endWordPosition="3288">mary&quot;) or exclude words from consideration (\\dynamic stop list expansion&quot;). Summarizing a 10 minute segment of a Callhome dialogue with our system takes on average less than 30 seconds on a 167 MHz 320 MB Sun Ultra1 workstation.5 4We are aware that this annotationand evaluationscheme is far from optimal: it does neither re ect the fact that turns are not necessarily the best units for extraction nor that the 11-pt-avg precision score is not optimally suited for the summarization task. We thus have recently developed a new word-based method for annotation and evaluation of spontaneous speech (Zechner, 2000). 5The average was computed over \x0cve English dialogues. 10 Human Study 10.1 Experiment Setup In order to evaluate the system as a whole, we conducted a study with humansin the loop to be able to compare three types of summaries (trans, clean, tele, see section 3) with the full original transcript. We address these two main questions in this study: (i) how fast can information be identi\x0ced using different types of summaries? (ii) how accurately is the information preserved, comparing di\x0berent types of summaries? We did not only ask the user \ arrow&quot; questions for a speci\x0cc piece of </context>
</contexts>
<marker>Zechner, 2000</marker>
<rawString>Klaus Zechner. 2000. A word-based annotation and evaluation scheme for summarization of spontaneous speech. Available from http://www.cs.cmu.edu/~zechner/publications.html. \x0c&amp;apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>