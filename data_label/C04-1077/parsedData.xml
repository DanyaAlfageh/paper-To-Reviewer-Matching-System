<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<figure confidence="0.5717082">
b&amp;apos;Corpus and Evaluation Measures for Multiple Document Summarization
with Multiple Sources
Tsutomu HIRAO
NTT Communication Science Laboratories
hirao@cslab.kecl.ntt.co.jp
Takahiro FUKUSIMA
Otemon Gakuin University
fukusima@res.otemon.ac.jp
Manabu OKUMURA
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Chikashi NOBATA
Communication Research Laboratories
nova@crl.go.jp
Hidetsugu NANBA
</figure>
<affiliation confidence="0.985561">
Hiroshima City University
</affiliation>
<email confidence="0.985853">
nanba@its.hiroshima-cu.ac.jp
</email>
<sectionHeader confidence="0.990054" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9989228">
In this paper, we introduce a large-scale test collec-
tion for multiple document summarization, the Text
Summarization Challenge 3 (TSC3) corpus. We
detail the corpus construction and evaluation mea-
sures. The significant feature of the corpus is that it
annotates not only the important sentences in a doc-
ument set, but also those among them that have the
same content. Moreover, we define new evaluation
metrics taking redundancy into account and discuss
the effectiveness of redundancy minimization.
</bodyText>
<sectionHeader confidence="0.998342" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9957202">
It has been said that we have too much informa-
tion on our hands, forcing us to read through a great
number of documents and extract relevant informa-
tion from them. With a view to coping with this situ-
ation, research on automatic text summarization has
attracted a lot of attention recently and there have
been many studies in this field. There is a particular
need to establish methods for the automatic sum-
marization of multiple documents rather than single
documents.
There have been several evaluation workshops
on text summarization. In 1998, TIPSTER SUM-
MAC (Mani et al., 2002) took place and the Doc-
ument Understanding Conference (DUC)1 has been
held annually since 2001. DUC has included multi-
ple document summarization among its tasks since
the first conference. The Text Summarization Chal-
lenge (TSC)2
has been held once in one and a half
years as part of the NTCIR (NII-NACSIS Test Col-
lection for IR Systems) project since 2001. Multiple
document summarization was included for the first
time as one of the tasks at TSC2 (in 2002) (Okumura
et al., 2003). Multiple document summarization is
now a central issue for text summarization research.
</bodyText>
<footnote confidence="0.504041">
1
http://duc.nist.gov
</footnote>
<page confidence="0.753469">
2
</page>
<bodyText confidence="0.91495575">
http://www.lr.pi.titech.ac.jp/tsc
In this paper, we detail the corpus construction
and evaluation measures used at the Text Summa-
rization Challenge 3 (TSC3 hereafter), where multi-
ple document summarization is the main issue. We
also report the results of a preliminary experiment
on simple multiple document summarization sys-
tems.
</bodyText>
<sectionHeader confidence="0.783096" genericHeader="method">
2 TSC3 Corpus
2.1 Guidelines for Corpus Construction
</sectionHeader>
<bodyText confidence="0.998679846153846">
Multiple document summarization from multiple
sources, i.e., several newspapers concerned with the
same topic but with different publishers, is more dif-
ficult than single document summarization since it
must deal with more text (in terms of numbers of
characters and sentences). Moreover, it is peculiar
to multiple document summarization that the sum-
marization system must decide how much redun-
dant information should be deleted3.
In a single document, there will be few sentences
with the same content. In contrast, in multiple doc-
uments with multiple sources, there will be many
sentences that convey the same content with differ-
ent words and phrases, or even identical sentences.
Thus, a text summarization system needs to recog-
nize such redundant sentences and reduce the redun-
dancy in the output summary.
However, we have no way of measuring the ef-
fectiveness of such redundancy in the corpora for
DUC and TSC2. Key data in TSC2 was given as
abstracts (free summaries) whose number of char-
acters was less than a fixed number and, thus, it
is difficult to use for repeated or automatic evalu-
ation, and for the extraction of important sentences.
Moreover, in DUC, where most of the key data were
abstracts whose number of words was less than a
</bodyText>
<page confidence="0.981758">
3
</page>
<bodyText confidence="0.989109488372093">
It is true that we need other important techniques such as
those for maintaining the consistency of words and phrases that
refer to the same object, and for making the results more read-
able; however, they are not included here.
\x0cfixed number, the situation was the same as TSC2.
At DUC 2002, extracts (important sentences) were
used, and this allowed us to evaluate sentence ex-
traction. However, it is not possible to measure the
effectiveness of redundant sentences reduction since
the corpus was not annotated to show sentence with
same content. In addition, this is the same even if
we use the SummBank corpus (Radev et al., 2003).
In any case, because many of the current summa-
rization systems for multiple documents are based
on sentence extraction, we believe these corpora to
be unsuitable as sets of documents for evaluation.
On this basis, in TSC3, we assumed that the pro-
cess of multiple document summarization consists
of the following three steps, and we produce a cor-
pus for the evaluation of the system at each of the
three steps4
.
Step 1 Extract important sentences from a given set
of documents
Step 2 Minimize redundant sentences from the re-
sult of Step 1
Step 3 Rewrite the result of Step 2 to reduce the
size of the summary to the specified number of
characters or less.
We have annotated not only the important sen-
tences in the document set, but also those among
them that have the same content. These are the cor-
pora for steps 1 and 2. We have prepared human-
produced free summaries (abstracts) for step 3.
In TSC3, since we have key data (a set of cor-
rect important sentences) for steps 1 and 2, we con-
ducted automatic evaluation using a scoring pro-
gram. We adopted an intrinsic evaluation by human
judges for step 3, which is currently under evalu-
ation. We provide details of the extracts prepared
for steps 1 and 2 and their evaluation measures in
the following sections. We do not report the overall
evaluation results for TSC3.
</bodyText>
<subsectionHeader confidence="0.996151">
2.2 Data Preparation for Sentence Extraction
</subsectionHeader>
<bodyText confidence="0.951509">
We begin with guidelines for annotating important
sentences (extracts). We think that there are two
kinds of extract.
</bodyText>
<listItem confidence="0.82805125">
1. A set of sentences that human annotators
judge as being important in a document set
(Fukusima and Okumura, 2001; Zechner,
1996; Paice, 1990).
</listItem>
<page confidence="0.992257">
4
</page>
<bodyText confidence="0.977712666666667">
This is based on general ideas of a summarization system
and is not intended to impose any conditions on a summariza-
tion system.
</bodyText>
<figure confidence="0.956551555555556">
Mainichi articles
Yomiuri articles
abstract
(a)
(b)
(c)
(d)
Doc. x
Doc. y
</figure>
<figureCaption confidence="0.998876">
Figure 1: An example of an abstract and its sources.
</figureCaption>
<listItem confidence="0.342992">
2. A set of sentences that are suitable as a source
</listItem>
<bodyText confidence="0.9970657">
for producing an abstract, i.e., a set of sen-
tences in the original documents that corre-
spond to the sentences in the abstracts(Kupiec
et al., 1995; Teufel and Moens, 1997; Marcu,
1999; Jing and McKeown, 1999).
When we consider how summaries are produced,
it seems more natural to identify important seg-
ments in the document set and then produce sum-
maries by combining and rephrasing such informa-
tion than to select important sentences and revise
them as summaries. Therefore, we believe that sec-
ond type of extract is superior and thus we prepared
the extracts in that way.
However, as stated in the previous section, with
multiple document summarization, there may be
more than one sentence with the same content, and
thus we may have more than one set of sentences
in the original document that corresponds to a given
sentence in the abstract; that is to say, there may be
more than one key datum for a given sentence in the
</bodyText>
<equation confidence="0.524829">
abstract5
.
</equation>
<bodyText confidence="0.9037705">
we have two sets of sentences that correspond to
sentence in the abstract.
</bodyText>
<listItem confidence="0.9501285">
(1) \x01\x03\x02 of document \x04 , or
(2) a combination of \x01\x06\x05 and \x01\x08\x07 of document \t
</listItem>
<bodyText confidence="0.997578636363636">
This means that \x01\x03\x02 alone is able to produce , and
can also be produced by combining \x01 \x05 and \x01 \x07 (Fig-
ure 1).
We marked all the sentences in the original doc-
uments that were suitable sources for producing the
sentences of the abstract, and this made it possible
for us to determine whether or not a summariza-
tion system deleted redundant sentences correctly
at Step 2. If the system outputs the sentences in
the original documents that are annotated as cor-
responding to the same sentence in the abstract, it
</bodyText>
<page confidence="0.978831">
5
</page>
<bodyText confidence="0.973958">
We use set of sentences since we often find that more
</bodyText>
<tableCaption confidence="0.58258">
than one sentence corresponds to a sentence in the abstract.
\x0cTable 1: Important Sentence Data.
</tableCaption>
<figure confidence="0.9859022">
Sentence ID of Abstract Set of Corresponding Sentences
1
\x0c\x0b\x0e
\x10\x0f
\x0c\x0b\x0e
\x12\x11\x14\x13\x15\x0b\x0c
\x16
\x17\x0f
2
\x0c\x0b\x14\x18\x14\x13\x19\x0b\x08\x1a\x14\x13\x15\x0b\x08\x1b\x1c\x0f
</figure>
<page confidence="0.334638">
3
</page>
<equation confidence="0.49090325">
\x0c\x0b\x08\x1d\x1e\x11\x08\x13\x1f\x0b\x06\x1d\x17
\x1c\x13 \x0b\x06\x1d \x18 \x0f
\x0c\x0b\x06
\x14\x13\x19\x0b \x18 \x11\x14\x13\x15\x0b \x1b \x11\x14\x0f
</equation>
<bodyText confidence="0.99313675">
has redundancy. If not, it has no redundancy. Re-
turning to the above example, if the system outputs
\x01 \x02 ,\x01\x14\x05 ,and \x01\x08\x07 , they all correspond to sentence in the
abstract, and thus it is redundant.
</bodyText>
<sectionHeader confidence="0.992009" genericHeader="method">
3 Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.864902">
We use both intrinsic and extrinsic evaluation. The
intrinsic metrics are Precision, Coverage and
Weighted Coverage. The extrinsic metric is
Pseudo Question-Answering.
</bodyText>
<subsectionHeader confidence="0.763619666666667">
3.1 Intrinsic Metrics
3.1.1 Number of Sentences System Should
Extract
</subsectionHeader>
<bodyText confidence="0.977166714285714">
Precision and Recall are generally used as evalua-
tion matrices for sentence extraction, and we used
the PR Breaking Point (Precision = Recall) for the
evaluation of extracts in TSC1 (Fukusima and Oku-
mura, 2001). This means that we evaluate systems
when the number of sentences in the correct ex-
tract is given. Moreover, in TSC3 we assume that
the number of sentences to be extracted is known
and we evaluate the system output that has the same
number of sentences.
However, it is not as easy to decide the number of
sentences to be extracted in TSC3 as in TSC1. We
assume that there are correspondences between sen-
tences in original documents and their abstract as in
</bodyText>
<tableCaption confidence="0.76531">
Table 1. An ASCII space, , is the delimiter for
</tableCaption>
<bodyText confidence="0.99422225">
the sets of corresponding sentences in the table. As
shown in the table, we often see several sets of sen-
tences that correspond to a sentence in the abstract
in multiple document summarization.
An extract here is a set of sentences needed
to produce the abstract. For instance, we can ob-
tain extracts such as \x01 \x02 ,\x01\x14\x07 ,\x01\x14! ,\x01\x14&amp;quot; ,\x01\x08\x07\x14# ,\x01\x14&amp;quot;\x08# , and
\x01 \x02$# ,\x01%\x02\x08\x02 ,\x01\x14\x07 ,\x01\x14! ,\x01\x14&amp;quot; ,\x01\x14\x05\x08# ,\x01\x14\x05 \x02 ,\x01\x14\x05\x08\x07 from Table 1 6. Often
there are several extracts and we must determine
which of these is the best. In such cases, we define
the correct extract as the set with the least number
of sentences needed to produce the abstract because
it is desirable to convey the maximum amount of
information with the least number of sentences.
Finding the minimum set of sentences to produce
the abstract amounts to solving the constraint sat-
</bodyText>
<page confidence="0.993224">
6
</page>
<bodyText confidence="0.94749425">
In fact, it is possible to produce the abstract with other sen-
tence combinations.
isfaction problem. In the example in Table 1, we
obtain the following constraints from each sentence
in the abstract:
&amp;(\&amp;apos; \x02*)+\x01\x03\x02-,/.\x08\x01\x03\x02\x1e#*0/\x01 \x02\x08\x02\x0e1 ,
&amp;(\&amp;apos; \x052)+\x01\x06\x0730/\x01\x06!304\x01\x08&amp;quot; ,
&amp;(\&amp;apos; \x072)+.\x08\x01\x06\x05\x14#504\x01\x06\x05%\x02604\x01\x08\x05\x08\x07718,4.\x14\x01\x03\x02-04\x01\x06\x07\x14#504\x01\x08&amp;quot;\x08#71
With these conditions, we now find the minimum
set that makes all the conjunctions true. We need
to find the minimum set that makes \&amp;apos; \x0290 \&amp;apos; \x05/0
\&amp;apos; \x07:)&amp;lt;;\x1f= &amp;gt;-? @ In this case, the minimum cover is
A
\x01\x03\x02\x06B \x01\x08\x07CB%\x01\x06!7B%\x01\x08&amp;quot;CB%\x01\x06\x07\x14#DB%\x01\x08&amp;quot;\x08#CE , and so the system should
extract six sentences.
In TSC3, we computed the number of sentences
that the system should extract and then evaluated the
system outputs, which must have the same number
of sentences, with the following precision and cov-
erage.
</bodyText>
<subsubsectionHeader confidence="0.640291">
3.1.2 Precision
</subsubsectionHeader>
<bodyText confidence="0.9298935">
Precision is the ratio of how many sentences in the
system output are included in the set of the corre-
sponding sentences. It is defined by the following
equation.
</bodyText>
<equation confidence="0.95736">
Precision FHG IKJ (1)
</equation>
<bodyText confidence="0.981073">
where L is the least number of sentences needed
to produce the abstract by solving the constraint
satisfaction problem and M is the number of cor-
rect sentences in the system output, i.e., the sen-
tences that are included in the set of correspond-
ing sentences. For example, the sentences listed
in Table 1 are correct. If the system output is
\x01 \x02\x1e#CB \x01 \x02\x08\x02\x0eB%\x01\x06!\x03B \x01 \x02\x08N\x0eB%\x01\x06&amp;quot;\x14#DB%\x01\x08&amp;quot; \x02 , then the Precision is as
follows:
</bodyText>
<equation confidence="0.790832">
Precision FPOQRFRSUT
Q%QWV
T (2)
</equation>
<bodyText confidence="0.492577">
for \x01 \x02XB \x01 \x02\x1e#DB%\x01\x03\x02\x14\x02\x0eB \x01\x08\x07CB%\x01\x06!\x03B \x01\x08&amp;quot;\x14# , the Precision is as fol-
lows:
</bodyText>
<equation confidence="0.567235">
Precision F
Q
Q FZY[T (3)
</equation>
<subsubsectionHeader confidence="0.34176">
3.1.3 Coverage
</subsubsectionHeader>
<bodyText confidence="0.988194142857143">
Coverage is an evaluation metric for measuring how
close the system output is to the abstract taking into
account the redundancy found in the set of sentences
in the output.
The set of sentences in the original documents
that corresponds correctly to the \\ -th sentence of
the human-produced abstract is denoted here as
</bodyText>
<figure confidence="0.891540348484849">
]_^a`
\x02\x0eB
]b^a`
\x05CB%c%c c\x08B
]_^a`d
B%c%c c\x08B
]_^a`e
. In this case, we have
\x0cf
sets of corresponding sentences. Here,
]_^a`d
indi-
cates a set of elements each of which corresponds to
the sentence number in the original documents, de-
noted as
]_^a`d
)
A7g ^\x16` d\x14`
\x02\x0eB
g ^a` d\x14`
\x057B%c c%c\x1cB
g ^\x16` d\x14` h
B c%c c E . For
instance, from Table 1,
]
\x02
`
\x05i)
g
\x02
`
\x05
`
\x02XB
g
\x02
`
\x05
`
\x05 and
g
\x02
`
\x05
`
\x023)+\x01 \x02$#CB
g
\x02
`
\x05
`
\x05j)+\x01 \x02\x08\x02 .
Then, we define the evaluation score ? .$\\\x0e1 for the
\\ -th sentence in the abstract as equation (1).
k[lnm\x06o F:prqts
u\x10vxw\x0evUy
z {}|\x7f~ \x1cz
\x06 u la\x1caw o
\x16w J (4)
where .\x081 is defined by the following equation.
lao F
Y if the system outputs
S otherwise
(5)
Function ? returns 1 (one) when any
]b^a` d
</figure>
<figureCaption confidence="0.443699">
is out-
puted completely. Otherwise it returns a partial
score according to the number of sentences
</figureCaption>
<figure confidence="0.461868125">
]_^nd
.
Given function ? and the number of sentences in
the abstract , Coverage is defined as follows:
Coverage F
u k[lnm\x06o
T (6)
If the system extracts \x01 \x02$# ,\x01 \x02\x14\x02 ,\x01\x08! ,\x01%\x02\x08N ,\x01\x14&amp;quot;\x08# ,\x01\x14&amp;quot; \x02 ,
</figure>
<table confidence="0.958224153846154">
? .\x1e\\\x0c1 is computed as follows:
k[l Y o F maxl SUJ[Y o F Y
k[l\x7fWo F maxl SUT t o F SUT %
k[l o F maxl SUJ\x10SUT % o F SUT %
and its Coverage is 0.553. If the system extracts
\x01 \x02 ,\x01%\x02$# ,\x01%\x02\x08\x02 ,\x01\x14\x07 ,\x01\x14! ,\x01\x14&amp;quot;\x08# , then the Coverage is 0.780.
ktl Y o F maxl Y[JXY o FZY
ktl\x7fxo F maxl SUT
QWV o FSUT
QWV
ktl o F maxl SUJ\x17SUT
QWV o FRSUT
QWV
</table>
<subsubsectionHeader confidence="0.939705">
3.1.4 Weighted Coverage
</subsubsectionHeader>
<bodyText confidence="0.9970005">
Now we define Weighted Coverage since each
sentence in TSC3 is ranked A, B or C, where A is
the best. This is similar to Relative Utility (Radev
et al., 2003). We only use three ranks in order to
limit the ranking cost. The definition is obtained by
modifying equation (6).
</bodyText>
<equation confidence="0.716786">
W.C. F
u\x03 lnxlnm\x06o\x10ok[lnm\x06o
l onl o l\x16 onl\x16 o la5onla5o J (7)
</equation>
<bodyText confidence="0.9863485">
where =.\x1e\\\x0c1 denotes the ranking of the \\ -th sentence
of the abstract and .$=.$\\\x0e1\x081 is its weight. .$= 1 is
the number of sentences whose ranking is = in
the abstract. Suppose the first sentence is ranked A,
the second B, and the third C in Table 1, and their
weights are given as 9.
</bodyText>
<equation confidence="0.84976">
]
1) ,.\x14a1) 7@ and
9. \&amp;apos; 16)+7@ 7.
</equation>
<bodyText confidence="0.731408666666667">
As before, if the system extracts
\x01 \x02\x1e# ,\x01 \x02\x08\x02 ,\x01 ! ,\x01 \x02\x08N ,\x01 &amp;quot;\x08# ,\x01 &amp;quot; \x02 , then the Weighted
Coverage is computed as follows:
</bodyText>
<equation confidence="0.800590571428572">
W.C. F
Y Y S7T SUT % SUT SUT %
Y Y SUT Y SUT Y
F/SUT
Q%%2
T
(8)
</equation>
<subsectionHeader confidence="0.978374">
3.2 Extrinsic Metrics
</subsectionHeader>
<subsubsectionHeader confidence="0.633024">
3.2.1 Pseudo Question-Answering
</subsubsectionHeader>
<bodyText confidence="0.982953935483871">
Sometimes question-answering (QA) by human
subjects is used for evaluation (Morris et al., 1992;
Hirao et al., 2001). That is, human subjects judge
whether predefined questions can be answered by
reading only a machine generated summary. How-
ever, the cost of this evaluation is huge. Therefore,
we employ a pseudo question-answering evaluation,
i.e., whether a summary has an answer to the ques-
tion or not. The background to this evaluation is in-
spired by TIPSTER SUMMACs QA track (Mani et
al., 2002).
For each document set, there are about five ques-
tions for a short summary and about ten questions
for long summary. Note that the questions for the
short summary are included in the questions for the
long summary. Examples of questions for the topic
Release of SONYs AIBO are as follows: How
much is AIBO?, When was AIBO sold?, and
How many AIBO are sold?.
Now, we evaluate the summary from the exact
match and edit distance for each question. Ex-
act match is a scoring function that returns one
when the summary includes the answer to the ques-
tion. Edit distance measures whether the systems
summary has strings that are similar to the answer
strings. The score 3 e based on the edit distance is
normalized with the length of the sentence and the
answer string so that the range of the score is [0,1]:
Sed F
length of the sentence edit distance
length of the answer strings
</bodyText>
<equation confidence="0.99005">
T (9)
</equation>
<bodyText confidence="0.9772545">
The score for a summary is the maximum value
of the scores for sentences in the summary. The
</bodyText>
<table confidence="0.80896775">
7 o1 n\x1714\x1614 may be computed differently. It is 1/rank (one
divided by rank) here.
\x0cTable 2: Description of TSC3 Corpus.
# of doc. sets 30
# of articles (The Mainichi) 175
# of articles (The Yomiuri) 177
Total 352
# of Sentences 3587
</table>
<bodyText confidence="0.8635226">
score is 1 if the summary has a sentence that in-
cludes the whole answer string.
It should be noted that the presence of answer
strings in the summary does not mean that a human
subject can necessarily answer the question.
</bodyText>
<sectionHeader confidence="0.924652" genericHeader="method">
4 Preliminary Experiment
</sectionHeader>
<bodyText confidence="0.999491857142857">
In order to examine whether our corpus is suitable
for summarization evaluation, our evaluation mea-
sures significant information and redundancies in
the system summaries.
Below we provide the details of the corpus, eval-
uation results and effectiveness of the minimization
of redundant sentences.
</bodyText>
<subsectionHeader confidence="0.99993">
4.1 Description of Corpus
</subsectionHeader>
<bodyText confidence="0.998897866666667">
According to the guidelines described in section
two, we constructed extracts and abstracts of thirty
sets of documents drawn from the Mainichi and
Yomiuri newspapers published between 1998 to
1999, each of which was related to a certain topic.
First, we prepared abstracts (their sizes were 5%
and 10% of the total number of the characters in
the document set), then produced extracts using the
abstracts. Table 2 shows the statistics.
One document set consists of about 10 articles
on average, and the almost same number of articles
were taken from the Mainichi newspaper and the
Yomiuri newspaper. Most of the topics are classified
into a single-event according to McKeown (2001).
The following list contains all the topics.
</bodyText>
<listItem confidence="0.637683333333333">
0310 Two-and-half-million-year old new hominid species
found in Ethiopia.
0320 Acquisition of IDC by NTT (and C&amp;W).
0340 Remarketing of game software judged legal by Tokyo
District Court.
0350 Night landing practice of carrier-based aircrafts of the
Independence.
0360 Simultaneous bombing of the US Embassies in Tanzania
and Kenya.
0370 Resignation of President Suharto.
0380 Nomination of Mr. Putin as Russian prime minister.
0400 Osama bin Laden provided shelter by Taliban regime in
Afghanistan.
0410 Transfer of Nakata to A.C. Perugia.
0420 Release of Dreamcast.
0440 Existence of Japanese otter confirmed.
0450 Kyocera Corporation makes Mita Co. Ltd. its subsidiary.
0460 Five-story pagoda at Muroji Temple damaged by ty-
phoon.
0470 Retirement of aircraft YS-11.
0480 Test observation of astronomical telescope Subaru
started.
0500 Dolly the cloned sheep.
0510 Mass of neutrinos.
0520 Human Genome Project finishes decoding of the 22nd
chromosome.
0530 Peace talks in Northern Ireland at the end of 1999.
0540 Debut of new model of bullet train (700 family).
0550 Mr. Yukio Aoshima decides not to run for gubernatorial
election.
0560 Mistakes in entrance examination of Kansai University.
0570 Space shuttle Endeavour, from its launch to return.
0580 40 million-year-old fossil of new monkey species found
by research group at Kyoto University.
0590 Dead body of George Mallory found on Mt. Everest.
0600 Release of SONYs AIBO.
</listItem>
<footnote confidence="0.6692584">
0610 e-one, look-alike of iMac.
0630 Research on Kitora tomb resumes.
0640 Tidal wave damage generated by earthquake in Papua
New Guinea.
0650 Mistaken bombing of the Chinese embassy by NATO.
</footnote>
<subsectionHeader confidence="0.98852">
4.2 Compared Extraction Methods
</subsectionHeader>
<bodyText confidence="0.999463">
We used the lead-based method, the TFc IDF-based
method (Zechner, 1996) and the sequential pattern-
based method (Hirao et al., 2003), and compared
performance of these summarization methods on
the TSC3 corpus.
</bodyText>
<subsectionHeader confidence="0.636464">
Lead-based Method
</subsectionHeader>
<bodyText confidence="0.975706555555556">
The documents in a test set were sorted in chrono-
logical and ascending order. Then, we extracted a
sentence at a time from the beginning of each docu-
ment and collected them to form a summary.
TFc IDF-based Method
The score of a sentence is the sum of the significant
scores of each content word in the sentence. We
therefore extracted sentences in descending order of
importance score. The sentence score Stfidf.\x14\x01
</bodyText>
<footnote confidence="0.385588666666667">
^
1 is
defined by the following.
</footnote>
<table confidence="0.62566725">
Stfidf
la12 o F 34\x16[A
|
loA J\x0cAjA o J (10)
where 9.\x1e;XB A3_1 is defined as follows:
lnA J\x0eAAA o F A\x06ClnA J\x0eAAA o}E\x08EoE%E
A
I IloA\x0co T (11)
</table>
<bodyText confidence="0.9887555">
;[II.$;XB%A3b1 is the frequency of word ; in the docu-
ment set, \x17I.$;X1 is the document frequency of ; , and
A4 is the total number of documents in the set. In
fact, we computed these using all the articles pub-
lished in the Mainichi and Yomiuri newspapers for
the years 1998 and 1999.
</bodyText>
<subsectionHeader confidence="0.753163">
Sequential Pattern-based Method
</subsectionHeader>
<bodyText confidence="0.950988">
The score of a sentence is the sum of the signifi-
cant scores of each sequential pattern in the sen-
tence. The patterns used for scoring were decided
</bodyText>
<tableCaption confidence="0.726474">
\x0cTable 3: Evaluation results for Precision, Cover-
age and Weighted Coverage.
</tableCaption>
<table confidence="0.9973373">
Method Length Prec. Cov. W.C.
Lead
Short .426 .212 .326
Long .539 .259 .369
TFN IDF
Short .497 .292 .397
Long .604 .325 .434
Pattern
Short .613 .305 .403
Long .665 .298 .418
</table>
<tableCaption confidence="0.971415">
Table 4: Evaluation results for Pseudo Question-
</tableCaption>
<table confidence="0.6579265625">
Answering.
Method Length Exact Edit
Lead
Short .300 .589
Long .275 .602
TFN IDF
Short .375 .643
Long .393 .659
Pattern
Short .390 .644
Long .370 .640
by using a statistical significance test such as the O
\x05
metric test and using 1,000 patterns. This is an ex-
tension of Lins method (Lin and Hovy, 2000). The
sentence score Spat.\x08\x01
^
1 is defined by the following.
Spat
la12 o F OX[A
|
lnOIo J (12)
where .$OI1 is defined as follows:
loOIo F
EE%EWlalnO J\x0cAAA o} Y oCE\x06EE%EWl
z {IOCz
O
{IO U o
U k\x10loOIo T (13)
I.\x1eOIB\x03A3_1 is the sentence frequency of pattern O in
the document set and I.\x1eOIB
]
</table>
<footnote confidence="0.66957625">
3b1 is the sentence fre-
quency of pattern O in all topics.
]
3r is the number
</footnote>
<bodyText confidence="0.7603175">
of sentences in all topics and U$?\x14.\x1eOI1 is the pattern
length.
</bodyText>
<subsectionHeader confidence="0.998672">
4.3 Evaluation Result
</subsectionHeader>
<bodyText confidence="0.9998132">
Table 3 shows the intrinsic evaluation result. All
methods have lower Coverage and Weighted Cov-
erage scores than Precision scores. This means that
the extracted sentences include redundant ones. In
particular, the difference between Precision and
Coverage is large in Pattern.
Although both Pattern and TFc IDF outper-
form Lead, the difference between them is small.
In addition, we know that Lead is a good extrac-
tion method for newspaper articles; however, this is
not true for the TSC3 corpus.
Table 4 shows the extrinsic evaluation results.
Again, both Pattern and TFc IDF outperform
Lead, but the difference between them is small.
We found a correlation between the intrinsic and ex-
</bodyText>
<tableCaption confidence="0.81025">
trinsic measures.
Table 5: Effects of clustering (Precision, Cover-
age, Weighted Coverage).
</tableCaption>
<table confidence="0.985262285714286">
Method Length Prec. Cov. W.C.
TFN IDF
Short .430 .297 .377
Long .533 .345 .455
Pattern
Short .531 .289 .390
Long .620 .338 .456
</table>
<tableCaption confidence="0.968467">
Table 6: Effects of clustering (Pseudo Question-
</tableCaption>
<table confidence="0.959862625">
Answering).
Method Length Exact Edit
TFN IDF
Short .401 .650
Long .377 .648
Pattern
Short .392 .650
Long .380 .655
</table>
<subsectionHeader confidence="0.8593155">
4.4 Effect of Redundant Sentence
Minimization
</subsectionHeader>
<bodyText confidence="0.976851972222222">
The experiment described in the previous section
shows that a group of sentences extracted in a sim-
ple way includes many redundant sentences. To
examine the effectiveness of minimizing redundant
sentences, we compare the Maximal Marginal Rele-
vance (MMR) based approach (Carbonell and Gold-
stein, 1998) with the clustering approach (Nomoto
and Matsumoto, 2001). We use cosine similarity
with a bag-of-words representation for the similar-
ity measure between sentences.
Clustering-based Approach
After computing importance scores using equations
(10) and (12), we conducted hierarchical clustering
using Wards method until we reached L (see Sec-
tion 3.1.1) clusters for the first 7L sentences. Then,
we extracted the sentence with the highest score
from each cluster.
Table 5 shows the results of the intrinsic evalu-
ation and Table 6 shows the results of the extrin-
sic evaluation. By comparison with Table 3, the
clustering-based approach resulted in TFc IDF and
Pattern scoring low in Precision, but high in Cov-
erage. When comparing Table 4 with Table 6, the
score is improved in most cases. These results im-
ply that redundancy minimization is effective for
improving the quality of summaries.
MMR-based Approach
After computing importance scores using equations
(10) and (12), we re-ranked the first DL sentences by
MMR and extracted the first L sentences.
Table 7 and 8 show the intrinsic and extrinsic
evaluation results, respectively. We can see the ef-
fectiveness of redundancy minimization by MMR.
Notably, in most cases, there is a large improvement
in both the intrinsic and extrinsic evaluation results
as compared with clustering.
</bodyText>
<tableCaption confidence="0.340712">
\x0cTable 7: Effects of MMR (Precision, Coverage,
</tableCaption>
<table confidence="0.946828375">
Weighted Coverage).
Method Length Prec. Cov. W.C.
TFN IDF
Short .469 .306 .403
Long .565 .376 .475
Pattern
Short .469 .332 .429
Long .577 .377 .500
</table>
<tableCaption confidence="0.969031">
Table 8: Effects of MMR (Pseudo Question-
</tableCaption>
<table confidence="0.91454425">
Answering).
Method Length Exact Edit
TFN IDF
Short .386 .647
Long .405 .667
Pattern
Short .417 .663
Long .390 .656
</table>
<bodyText confidence="0.986594">
These results show that redundancy minimization
has a significant effect on multiple document sum-
marization.
</bodyText>
<sectionHeader confidence="0.998892" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999875769230769">
We described the details of a corpus constructed for
TSC3 and measures for its evaluation, focusing on
sentence extraction. We think that a corpus in which
important sentences and those with the same content
are annotated for multiple documents is a new and
significant feature for summarization corpora.
It is planned to make the TSC3 corpus available
(even if the recipient is not a TSC3 participant) by
exchanging memoranda with the National Institute
of Informatics in Japan. We sincerely hope that this
corpus will be useful to researchers who are inter-
ested in text summarization and serve to facilitate
further progress in this field.
</bodyText>
<sectionHeader confidence="0.992123" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998765464788732">
J. Carbonell and J. Goldstein. 1998. The Use of
MMR, Diversity-Based Reranking for Reorder-
ing Documents and Producing Summaries. In
Proc. of the 21th ACM-SIGIR, pages 335336.
T. Fukusima and M. Okumura. 2001. Text Summa-
rization Challenge: Text Summarization Evalua-
tion in Japan. In Proc. of the NAACL 2001 Work-
shop on Automatic summarization, pages 5159.
T. Hirao, Y. Sasaki, and H. Isozaki. 2001. An
Extrinsic Evaluation for Question-Biased Text
Summarization on QA tasks. In Proc. of the
NAACL 2001 Workshop on Automatic Summa-
rization, pages 6168.
T. Hirao, J. Suzuki, H. Isozaki, and E. Maeda.
2003. Multiple Document Summarization using
Sequential Pattern Mining (in Japanese). In The
Special Interest Group Notes of IPSJ (NL-158-6),
pages 3138.
H. Jing and K. McKeown. 1999. The Decom-
position of Human-Written Summary Sentences.
Proc. of the 22nd ACM-SIGIR, pages 129136.
J. Kupiec, J Petersen, and F. Chen. 1995. A Train-
able Document Summarizer. In Proc. of the 18th
SIGIR, pages 6873.
C-Y. Lin and E. H. Hovy. 2000. The Automated
Acquisition of Topic Signatures for Text Sum-
marization. In Proc. of the 16th COLING, pages
495501.
I. Mani, G. Klein, D. House, L. Hirschman, T. Fir-
man, and B. Sundheim. 2002. SUMMAC: a text
summarization evaluation. Natural Language
Engineering, 8(1):4368.
D. Marcu. 1999. The Automatic Construction
of Large-scale Corpora for Summarization Re-
search. Proc. of the 22nd ACM-SIGIR, pages
137144.
K. McKeown, R. Barzilay, D. Evans, V. Hatzivas-
silogou, M. Y. Kan, B. Schiffman, and S. Teufel.
2001. Columbia Multi-Document Summariza-
tion: Approach and Evaluation. In Proc. of the
Document Understanding Conference 2001.
A. H. Morris, G. M. Kasper, and D.A. Adams.
1992. The Effects and Limitations of Automatic
Text Condensing on Reading Comprehension.
Information System Research, 3(1):1735.
T. Nomoto and M. Matsumoto. 2001. A New Ap-
proach to Unsupervised Text Summarization. In
Proc. of the 24th ACM-SIGIR, pages 2634.
M. Okumura, T. Fukusima, and H. Nanba. 2003.
Text Summarization Challenge 2, Text Summa-
rization Evaluation at NTCIR Workshop 3. In
Proc. of the HLT/NAACL 2003 Text Summariza-
tion Workshop, pages 4956.
C. Paice. 1990. Constructing Literature Abstracts
by Computer: Techniques and Prospects. Infor-
mation Processing and Management, 26(1):171
186.
D. R. Radev, S. Teufel, H. Saggion, W. Lam,
J. Blitzer, H. Qi, A. Celebi, D. Liu, and
E. Drabek. 2003. Evaluation challenges in large-
scale document summarization. In Proc. of the
41st ACL, pages 375382.
S. Teufel and M. Moens. 1997. Sentence Extrac-
tion as a Classification Task. In Proc. of the ACL
Workshop on Intelligent Scalable Text Summa-
rization, pages 5865.
K. Zechner. 1996. Fast Generation of Abstracts
from General Domain Text Corpora by Extract-
ing Relevant Sentences. In Proc. of the 16th
COLING, pages 986989.
\x0c&amp;apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.077855">
<title confidence="0.9998275">b&amp;apos;Corpus and Evaluation Measures for Multiple Document Summarization with Multiple Sources</title>
<author confidence="0.980429">Tsutomu HIRAO</author>
<affiliation confidence="0.983817">NTT Communication Science Laboratories</affiliation>
<email confidence="0.687266">hirao@cslab.kecl.ntt.co.jp</email>
<author confidence="0.698811">Takahiro FUKUSIMA</author>
<affiliation confidence="0.954633">Otemon Gakuin University</affiliation>
<email confidence="0.923959">fukusima@res.otemon.ac.jp</email>
<author confidence="0.951223">Manabu OKUMURA</author>
<affiliation confidence="0.999258">Tokyo Institute of Technology</affiliation>
<email confidence="0.807237">oku@pi.titech.ac.jp</email>
<author confidence="0.548889">Chikashi NOBATA</author>
<affiliation confidence="0.84036">Communication Research Laboratories</affiliation>
<email confidence="0.505496">nova@crl.go.jp</email>
<affiliation confidence="0.889244">Hidetsugu NANBA Hiroshima City University</affiliation>
<email confidence="0.892483">nanba@its.hiroshima-cu.ac.jp</email>
<abstract confidence="0.997928818181818">In this paper, we introduce a large-scale test collection for multiple document summarization, the Text Summarization Challenge 3 (TSC3) corpus. We detail the corpus construction and evaluation measures. The significant feature of the corpus is that it annotates not only the important sentences in a document set, but also those among them that have the same content. Moreover, we define new evaluation metrics taking redundancy into account and discuss the effectiveness of redundancy minimization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>J Goldstein</author>
</authors>
<title>The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries.</title>
<date>1998</date>
<booktitle>In Proc. of the 21th ACM-SIGIR,</booktitle>
<pages>335336</pages>
<contexts>
<context position="24005" citStr="Carbonell and Goldstein, 1998" startWordPosition="3940" endWordPosition="3944"> Length Prec. Cov. W.C. TFN IDF Short .430 .297 .377 Long .533 .345 .455 Pattern Short .531 .289 .390 Long .620 .338 .456 Table 6: Effects of clustering (Pseudo QuestionAnswering). Method Length Exact Edit TFN IDF Short .401 .650 Long .377 .648 Pattern Short .392 .650 Long .380 .655 4.4 Effect of Redundant Sentence Minimization The experiment described in the previous section shows that a group of sentences extracted in a simple way includes many redundant sentences. To examine the effectiveness of minimizing redundant sentences, we compare the Maximal Marginal Relevance (MMR) based approach (Carbonell and Goldstein, 1998) with the clustering approach (Nomoto and Matsumoto, 2001). We use cosine similarity with a bag-of-words representation for the similarity measure between sentences. Clustering-based Approach After computing importance scores using equations (10) and (12), we conducted hierarchical clustering using Wards method until we reached L (see Section 3.1.1) clusters for the first 7L sentences. Then, we extracted the sentence with the highest score from each cluster. Table 5 shows the results of the intrinsic evaluation and Table 6 shows the results of the extrinsic evaluation. By comparison with Table</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>J. Carbonell and J. Goldstein. 1998. The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries. In Proc. of the 21th ACM-SIGIR, pages 335336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fukusima</author>
<author>M Okumura</author>
</authors>
<title>Text Summarization Challenge: Text Summarization Evaluation in Japan.</title>
<date>2001</date>
<booktitle>In Proc. of the NAACL 2001 Workshop on Automatic summarization,</booktitle>
<pages>5159</pages>
<contexts>
<context position="6012" citStr="Fukusima and Okumura, 2001" startWordPosition="968" endWordPosition="971">eps 1 and 2, we conducted automatic evaluation using a scoring program. We adopted an intrinsic evaluation by human judges for step 3, which is currently under evaluation. We provide details of the extracts prepared for steps 1 and 2 and their evaluation measures in the following sections. We do not report the overall evaluation results for TSC3. 2.2 Data Preparation for Sentence Extraction We begin with guidelines for annotating important sentences (extracts). We think that there are two kinds of extract. 1. A set of sentences that human annotators judge as being important in a document set (Fukusima and Okumura, 2001; Zechner, 1996; Paice, 1990). 4 This is based on general ideas of a summarization system and is not intended to impose any conditions on a summarization system. Mainichi articles Yomiuri articles abstract (a) (b) (c) (d) Doc. x Doc. y Figure 1: An example of an abstract and its sources. 2. A set of sentences that are suitable as a source for producing an abstract, i.e., a set of sentences in the original documents that correspond to the sentences in the abstracts(Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing and McKeown, 1999). When we consider how summaries are produced, it </context>
<context position="9215" citStr="Fukusima and Okumura, 2001" startWordPosition="1479" endWordPosition="1483">e above example, if the system outputs \x01 \x02 ,\x01\x14\x05 ,and \x01\x08\x07 , they all correspond to sentence in the abstract, and thus it is redundant. 3 Evaluation Metrics We use both intrinsic and extrinsic evaluation. The intrinsic metrics are Precision, Coverage and Weighted Coverage. The extrinsic metric is Pseudo Question-Answering. 3.1 Intrinsic Metrics 3.1.1 Number of Sentences System Should Extract Precision and Recall are generally used as evaluation matrices for sentence extraction, and we used the PR Breaking Point (Precision = Recall) for the evaluation of extracts in TSC1 (Fukusima and Okumura, 2001). This means that we evaluate systems when the number of sentences in the correct extract is given. Moreover, in TSC3 we assume that the number of sentences to be extracted is known and we evaluate the system output that has the same number of sentences. However, it is not as easy to decide the number of sentences to be extracted in TSC3 as in TSC1. We assume that there are correspondences between sentences in original documents and their abstract as in Table 1. An ASCII space, , is the delimiter for the sets of corresponding sentences in the table. As shown in the table, we often see several </context>
</contexts>
<marker>Fukusima, Okumura, 2001</marker>
<rawString>T. Fukusima and M. Okumura. 2001. Text Summarization Challenge: Text Summarization Evaluation in Japan. In Proc. of the NAACL 2001 Workshop on Automatic summarization, pages 5159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hirao</author>
<author>Y Sasaki</author>
<author>H Isozaki</author>
</authors>
<title>An Extrinsic Evaluation for Question-Biased Text Summarization on QA tasks.</title>
<date>2001</date>
<booktitle>In Proc. of the NAACL 2001 Workshop on Automatic Summarization,</booktitle>
<pages>6168</pages>
<contexts>
<context position="15637" citStr="Hirao et al., 2001" startWordPosition="2539" endWordPosition="2542">the number of sentences whose ranking is = in the abstract. Suppose the first sentence is ranked A, the second B, and the third C in Table 1, and their weights are given as 9. ] 1) ,.\x14a1) 7@ and 9. \&amp;apos; 16)+7@ 7. As before, if the system extracts \x01 \x02\x1e# ,\x01 \x02\x08\x02 ,\x01 ! ,\x01 \x02\x08N ,\x01 &amp;quot;\x08# ,\x01 &amp;quot; \x02 , then the Weighted Coverage is computed as follows: W.C. F Y Y S7T SUT % SUT SUT % Y Y SUT Y SUT Y F/SUT Q%%2 T (8) 3.2 Extrinsic Metrics 3.2.1 Pseudo Question-Answering Sometimes question-answering (QA) by human subjects is used for evaluation (Morris et al., 1992; Hirao et al., 2001). That is, human subjects judge whether predefined questions can be answered by reading only a machine generated summary. However, the cost of this evaluation is huge. Therefore, we employ a pseudo question-answering evaluation, i.e., whether a summary has an answer to the question or not. The background to this evaluation is inspired by TIPSTER SUMMACs QA track (Mani et al., 2002). For each document set, there are about five questions for a short summary and about ten questions for long summary. Note that the questions for the short summary are included in the questions for the long summary. </context>
</contexts>
<marker>Hirao, Sasaki, Isozaki, 2001</marker>
<rawString>T. Hirao, Y. Sasaki, and H. Isozaki. 2001. An Extrinsic Evaluation for Question-Biased Text Summarization on QA tasks. In Proc. of the NAACL 2001 Workshop on Automatic Summarization, pages 6168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hirao</author>
<author>J Suzuki</author>
<author>H Isozaki</author>
<author>E Maeda</author>
</authors>
<title>Multiple Document Summarization using Sequential Pattern Mining (in Japanese).</title>
<date>2003</date>
<booktitle>In The Special Interest Group Notes of IPSJ</booktitle>
<pages>158--6</pages>
<contexts>
<context position="20324" citStr="Hirao et al., 2003" startWordPosition="3312" endWordPosition="3315">on of Kansai University. 0570 Space shuttle Endeavour, from its launch to return. 0580 40 million-year-old fossil of new monkey species found by research group at Kyoto University. 0590 Dead body of George Mallory found on Mt. Everest. 0600 Release of SONYs AIBO. 0610 e-one, look-alike of iMac. 0630 Research on Kitora tomb resumes. 0640 Tidal wave damage generated by earthquake in Papua New Guinea. 0650 Mistaken bombing of the Chinese embassy by NATO. 4.2 Compared Extraction Methods We used the lead-based method, the TFc IDF-based method (Zechner, 1996) and the sequential patternbased method (Hirao et al., 2003), and compared performance of these summarization methods on the TSC3 corpus. Lead-based Method The documents in a test set were sorted in chronological and ascending order. Then, we extracted a sentence at a time from the beginning of each document and collected them to form a summary. TFc IDF-based Method The score of a sentence is the sum of the significant scores of each content word in the sentence. We therefore extracted sentences in descending order of importance score. The sentence score Stfidf.\x14\x01 ^ 1 is defined by the following. Stfidf la12 o F 34\x16[A | loA J\x0cAjA o J (10) w</context>
</contexts>
<marker>Hirao, Suzuki, Isozaki, Maeda, 2003</marker>
<rawString>T. Hirao, J. Suzuki, H. Isozaki, and E. Maeda. 2003. Multiple Document Summarization using Sequential Pattern Mining (in Japanese). In The Special Interest Group Notes of IPSJ (NL-158-6), pages 3138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>K McKeown</author>
</authors>
<title>The Decomposition of Human-Written Summary Sentences.</title>
<date>1999</date>
<booktitle>Proc. of the 22nd ACM-SIGIR,</booktitle>
<pages>129136</pages>
<contexts>
<context position="6562" citStr="Jing and McKeown, 1999" startWordPosition="1066" endWordPosition="1069">rs judge as being important in a document set (Fukusima and Okumura, 2001; Zechner, 1996; Paice, 1990). 4 This is based on general ideas of a summarization system and is not intended to impose any conditions on a summarization system. Mainichi articles Yomiuri articles abstract (a) (b) (c) (d) Doc. x Doc. y Figure 1: An example of an abstract and its sources. 2. A set of sentences that are suitable as a source for producing an abstract, i.e., a set of sentences in the original documents that correspond to the sentences in the abstracts(Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing and McKeown, 1999). When we consider how summaries are produced, it seems more natural to identify important segments in the document set and then produce summaries by combining and rephrasing such information than to select important sentences and revise them as summaries. Therefore, we believe that second type of extract is superior and thus we prepared the extracts in that way. However, as stated in the previous section, with multiple document summarization, there may be more than one sentence with the same content, and thus we may have more than one set of sentences in the original document that corresponds</context>
</contexts>
<marker>Jing, McKeown, 1999</marker>
<rawString>H. Jing and K. McKeown. 1999. The Decomposition of Human-Written Summary Sentences. Proc. of the 22nd ACM-SIGIR, pages 129136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Petersen</author>
<author>F Chen</author>
</authors>
<title>A Trainable Document Summarizer.</title>
<date>1995</date>
<booktitle>In Proc. of the 18th SIGIR,</booktitle>
<pages>6873</pages>
<contexts>
<context position="6500" citStr="Kupiec et al., 1995" startWordPosition="1056" endWordPosition="1059">inds of extract. 1. A set of sentences that human annotators judge as being important in a document set (Fukusima and Okumura, 2001; Zechner, 1996; Paice, 1990). 4 This is based on general ideas of a summarization system and is not intended to impose any conditions on a summarization system. Mainichi articles Yomiuri articles abstract (a) (b) (c) (d) Doc. x Doc. y Figure 1: An example of an abstract and its sources. 2. A set of sentences that are suitable as a source for producing an abstract, i.e., a set of sentences in the original documents that correspond to the sentences in the abstracts(Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing and McKeown, 1999). When we consider how summaries are produced, it seems more natural to identify important segments in the document set and then produce summaries by combining and rephrasing such information than to select important sentences and revise them as summaries. Therefore, we believe that second type of extract is superior and thus we prepared the extracts in that way. However, as stated in the previous section, with multiple document summarization, there may be more than one sentence with the same content, and thus we may have more than </context>
</contexts>
<marker>Kupiec, Petersen, Chen, 1995</marker>
<rawString>J. Kupiec, J Petersen, and F. Chen. 1995. A Trainable Document Summarizer. In Proc. of the 18th SIGIR, pages 6873.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>The Automated Acquisition of Topic Signatures for Text Summarization.</title>
<date>2000</date>
<booktitle>In Proc. of the 16th COLING,</booktitle>
<pages>495501</pages>
<contexts>
<context position="22108" citStr="Lin and Hovy, 2000" startWordPosition="3626" endWordPosition="3629">g were decided \x0cTable 3: Evaluation results for Precision, Coverage and Weighted Coverage. Method Length Prec. Cov. W.C. Lead Short .426 .212 .326 Long .539 .259 .369 TFN IDF Short .497 .292 .397 Long .604 .325 .434 Pattern Short .613 .305 .403 Long .665 .298 .418 Table 4: Evaluation results for Pseudo QuestionAnswering. Method Length Exact Edit Lead Short .300 .589 Long .275 .602 TFN IDF Short .375 .643 Long .393 .659 Pattern Short .390 .644 Long .370 .640 by using a statistical significance test such as the O \x05 metric test and using 1,000 patterns. This is an extension of Lins method (Lin and Hovy, 2000). The sentence score Spat.\x08\x01 ^ 1 is defined by the following. Spat la12 o F OX[A | lnOIo J (12) where .$OI1 is defined as follows: loOIo F EE%EWlalnO J\x0cAAA o} Y oCE\x06EE%EWl z {IOCz O {IO U o U k\x10loOIo T (13) I.\x1eOIB\x03A3_1 is the sentence frequency of pattern O in the document set and I.\x1eOIB ] 3b1 is the sentence frequency of pattern O in all topics. ] 3r is the number of sentences in all topics and U$?\x14.\x1eOI1 is the pattern length. 4.3 Evaluation Result Table 3 shows the intrinsic evaluation result. All methods have lower Coverage and Weighted Coverage scores than Pre</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>C-Y. Lin and E. H. Hovy. 2000. The Automated Acquisition of Topic Signatures for Text Summarization. In Proc. of the 16th COLING, pages 495501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>G Klein</author>
<author>D House</author>
<author>L Hirschman</author>
<author>T Firman</author>
<author>B Sundheim</author>
</authors>
<title>SUMMAC: a text summarization evaluation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1549" citStr="Mani et al., 2002" startWordPosition="218" endWordPosition="221">undancy minimization. 1 Introduction It has been said that we have too much information on our hands, forcing us to read through a great number of documents and extract relevant information from them. With a view to coping with this situation, research on automatic text summarization has attracted a lot of attention recently and there have been many studies in this field. There is a particular need to establish methods for the automatic summarization of multiple documents rather than single documents. There have been several evaluation workshops on text summarization. In 1998, TIPSTER SUMMAC (Mani et al., 2002) took place and the Document Understanding Conference (DUC)1 has been held annually since 2001. DUC has included multiple document summarization among its tasks since the first conference. The Text Summarization Challenge (TSC)2 has been held once in one and a half years as part of the NTCIR (NII-NACSIS Test Collection for IR Systems) project since 2001. Multiple document summarization was included for the first time as one of the tasks at TSC2 (in 2002) (Okumura et al., 2003). Multiple document summarization is now a central issue for text summarization research. 1 http://duc.nist.gov 2 http:</context>
<context position="16021" citStr="Mani et al., 2002" startWordPosition="2603" endWordPosition="2606">W.C. F Y Y S7T SUT % SUT SUT % Y Y SUT Y SUT Y F/SUT Q%%2 T (8) 3.2 Extrinsic Metrics 3.2.1 Pseudo Question-Answering Sometimes question-answering (QA) by human subjects is used for evaluation (Morris et al., 1992; Hirao et al., 2001). That is, human subjects judge whether predefined questions can be answered by reading only a machine generated summary. However, the cost of this evaluation is huge. Therefore, we employ a pseudo question-answering evaluation, i.e., whether a summary has an answer to the question or not. The background to this evaluation is inspired by TIPSTER SUMMACs QA track (Mani et al., 2002). For each document set, there are about five questions for a short summary and about ten questions for long summary. Note that the questions for the short summary are included in the questions for the long summary. Examples of questions for the topic Release of SONYs AIBO are as follows: How much is AIBO?, When was AIBO sold?, and How many AIBO are sold?. Now, we evaluate the summary from the exact match and edit distance for each question. Exact match is a scoring function that returns one when the summary includes the answer to the question. Edit distance measures whether the systems summar</context>
</contexts>
<marker>Mani, Klein, House, Hirschman, Firman, Sundheim, 2002</marker>
<rawString>I. Mani, G. Klein, D. House, L. Hirschman, T. Firman, and B. Sundheim. 2002. SUMMAC: a text summarization evaluation. Natural Language Engineering, 8(1):4368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The Automatic Construction of Large-scale Corpora for Summarization Research.</title>
<date>1999</date>
<booktitle>Proc. of the 22nd ACM-SIGIR,</booktitle>
<pages>137144</pages>
<contexts>
<context position="6537" citStr="Marcu, 1999" startWordPosition="1064" endWordPosition="1065">uman annotators judge as being important in a document set (Fukusima and Okumura, 2001; Zechner, 1996; Paice, 1990). 4 This is based on general ideas of a summarization system and is not intended to impose any conditions on a summarization system. Mainichi articles Yomiuri articles abstract (a) (b) (c) (d) Doc. x Doc. y Figure 1: An example of an abstract and its sources. 2. A set of sentences that are suitable as a source for producing an abstract, i.e., a set of sentences in the original documents that correspond to the sentences in the abstracts(Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing and McKeown, 1999). When we consider how summaries are produced, it seems more natural to identify important segments in the document set and then produce summaries by combining and rephrasing such information than to select important sentences and revise them as summaries. Therefore, we believe that second type of extract is superior and thus we prepared the extracts in that way. However, as stated in the previous section, with multiple document summarization, there may be more than one sentence with the same content, and thus we may have more than one set of sentences in the original </context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>D. Marcu. 1999. The Automatic Construction of Large-scale Corpora for Summarization Research. Proc. of the 22nd ACM-SIGIR, pages 137144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>R Barzilay</author>
<author>D Evans</author>
<author>V Hatzivassilogou</author>
<author>M Y Kan</author>
<author>B Schiffman</author>
<author>S Teufel</author>
</authors>
<title>Columbia Multi-Document Summarization: Approach and Evaluation.</title>
<date>2001</date>
<booktitle>In Proc. of the Document Understanding Conference</booktitle>
<marker>McKeown, Barzilay, Evans, Hatzivassilogou, Kan, Schiffman, Teufel, 2001</marker>
<rawString>K. McKeown, R. Barzilay, D. Evans, V. Hatzivassilogou, M. Y. Kan, B. Schiffman, and S. Teufel. 2001. Columbia Multi-Document Summarization: Approach and Evaluation. In Proc. of the Document Understanding Conference 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A H Morris</author>
<author>G M Kasper</author>
<author>D A Adams</author>
</authors>
<title>The Effects and Limitations of Automatic Text Condensing on Reading Comprehension.</title>
<date>1992</date>
<contexts>
<context position="15616" citStr="Morris et al., 1992" startWordPosition="2535" endWordPosition="2538">its weight. .$= 1 is the number of sentences whose ranking is = in the abstract. Suppose the first sentence is ranked A, the second B, and the third C in Table 1, and their weights are given as 9. ] 1) ,.\x14a1) 7@ and 9. \&amp;apos; 16)+7@ 7. As before, if the system extracts \x01 \x02\x1e# ,\x01 \x02\x08\x02 ,\x01 ! ,\x01 \x02\x08N ,\x01 &amp;quot;\x08# ,\x01 &amp;quot; \x02 , then the Weighted Coverage is computed as follows: W.C. F Y Y S7T SUT % SUT SUT % Y Y SUT Y SUT Y F/SUT Q%%2 T (8) 3.2 Extrinsic Metrics 3.2.1 Pseudo Question-Answering Sometimes question-answering (QA) by human subjects is used for evaluation (Morris et al., 1992; Hirao et al., 2001). That is, human subjects judge whether predefined questions can be answered by reading only a machine generated summary. However, the cost of this evaluation is huge. Therefore, we employ a pseudo question-answering evaluation, i.e., whether a summary has an answer to the question or not. The background to this evaluation is inspired by TIPSTER SUMMACs QA track (Mani et al., 2002). For each document set, there are about five questions for a short summary and about ten questions for long summary. Note that the questions for the short summary are included in the questions f</context>
</contexts>
<marker>Morris, Kasper, Adams, 1992</marker>
<rawString>A. H. Morris, G. M. Kasper, and D.A. Adams. 1992. The Effects and Limitations of Automatic Text Condensing on Reading Comprehension.</rawString>
</citation>
<citation valid="false">
<journal>Information System Research,</journal>
<volume>3</volume>
<issue>1</issue>
<marker></marker>
<rawString>Information System Research, 3(1):1735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nomoto</author>
<author>M Matsumoto</author>
</authors>
<title>A New Approach to Unsupervised Text Summarization.</title>
<date>2001</date>
<booktitle>In Proc. of the 24th ACM-SIGIR,</booktitle>
<pages>2634</pages>
<contexts>
<context position="24063" citStr="Nomoto and Matsumoto, 2001" startWordPosition="3949" endWordPosition="3952">3 .345 .455 Pattern Short .531 .289 .390 Long .620 .338 .456 Table 6: Effects of clustering (Pseudo QuestionAnswering). Method Length Exact Edit TFN IDF Short .401 .650 Long .377 .648 Pattern Short .392 .650 Long .380 .655 4.4 Effect of Redundant Sentence Minimization The experiment described in the previous section shows that a group of sentences extracted in a simple way includes many redundant sentences. To examine the effectiveness of minimizing redundant sentences, we compare the Maximal Marginal Relevance (MMR) based approach (Carbonell and Goldstein, 1998) with the clustering approach (Nomoto and Matsumoto, 2001). We use cosine similarity with a bag-of-words representation for the similarity measure between sentences. Clustering-based Approach After computing importance scores using equations (10) and (12), we conducted hierarchical clustering using Wards method until we reached L (see Section 3.1.1) clusters for the first 7L sentences. Then, we extracted the sentence with the highest score from each cluster. Table 5 shows the results of the intrinsic evaluation and Table 6 shows the results of the extrinsic evaluation. By comparison with Table 3, the clustering-based approach resulted in TFc IDF and </context>
</contexts>
<marker>Nomoto, Matsumoto, 2001</marker>
<rawString>T. Nomoto and M. Matsumoto. 2001. A New Approach to Unsupervised Text Summarization. In Proc. of the 24th ACM-SIGIR, pages 2634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Okumura</author>
<author>T Fukusima</author>
<author>H Nanba</author>
</authors>
<date>2003</date>
<contexts>
<context position="2030" citStr="Okumura et al., 2003" startWordPosition="300" endWordPosition="303">ather than single documents. There have been several evaluation workshops on text summarization. In 1998, TIPSTER SUMMAC (Mani et al., 2002) took place and the Document Understanding Conference (DUC)1 has been held annually since 2001. DUC has included multiple document summarization among its tasks since the first conference. The Text Summarization Challenge (TSC)2 has been held once in one and a half years as part of the NTCIR (NII-NACSIS Test Collection for IR Systems) project since 2001. Multiple document summarization was included for the first time as one of the tasks at TSC2 (in 2002) (Okumura et al., 2003). Multiple document summarization is now a central issue for text summarization research. 1 http://duc.nist.gov 2 http://www.lr.pi.titech.ac.jp/tsc In this paper, we detail the corpus construction and evaluation measures used at the Text Summarization Challenge 3 (TSC3 hereafter), where multiple document summarization is the main issue. We also report the results of a preliminary experiment on simple multiple document summarization systems. 2 TSC3 Corpus 2.1 Guidelines for Corpus Construction Multiple document summarization from multiple sources, i.e., several newspapers concerned with the sam</context>
</contexts>
<marker>Okumura, Fukusima, Nanba, 2003</marker>
<rawString>M. Okumura, T. Fukusima, and H. Nanba. 2003.</rawString>
</citation>
<citation valid="false">
<title>Text Summarization Challenge 2, Text Summarization Evaluation at NTCIR Workshop 3.</title>
<booktitle>In Proc. of the HLT/NAACL 2003 Text Summarization Workshop,</booktitle>
<pages>4956</pages>
<marker></marker>
<rawString>Text Summarization Challenge 2, Text Summarization Evaluation at NTCIR Workshop 3. In Proc. of the HLT/NAACL 2003 Text Summarization Workshop, pages 4956.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Paice</author>
</authors>
<title>Constructing Literature Abstracts by Computer:</title>
<date>1990</date>
<booktitle>Techniques and Prospects. Information Processing and Management,</booktitle>
<volume>26</volume>
<issue>1</issue>
<pages>186</pages>
<contexts>
<context position="6041" citStr="Paice, 1990" startWordPosition="974" endWordPosition="975">ion using a scoring program. We adopted an intrinsic evaluation by human judges for step 3, which is currently under evaluation. We provide details of the extracts prepared for steps 1 and 2 and their evaluation measures in the following sections. We do not report the overall evaluation results for TSC3. 2.2 Data Preparation for Sentence Extraction We begin with guidelines for annotating important sentences (extracts). We think that there are two kinds of extract. 1. A set of sentences that human annotators judge as being important in a document set (Fukusima and Okumura, 2001; Zechner, 1996; Paice, 1990). 4 This is based on general ideas of a summarization system and is not intended to impose any conditions on a summarization system. Mainichi articles Yomiuri articles abstract (a) (b) (c) (d) Doc. x Doc. y Figure 1: An example of an abstract and its sources. 2. A set of sentences that are suitable as a source for producing an abstract, i.e., a set of sentences in the original documents that correspond to the sentences in the abstracts(Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing and McKeown, 1999). When we consider how summaries are produced, it seems more natural to identif</context>
</contexts>
<marker>Paice, 1990</marker>
<rawString>C. Paice. 1990. Constructing Literature Abstracts by Computer: Techniques and Prospects. Information Processing and Management, 26(1):171 186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>S Teufel</author>
<author>H Saggion</author>
<author>W Lam</author>
<author>J Blitzer</author>
<author>H Qi</author>
<author>A Celebi</author>
<author>D Liu</author>
<author>E Drabek</author>
</authors>
<title>Evaluation challenges in largescale document summarization.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st ACL,</booktitle>
<pages>375382</pages>
<contexts>
<context position="4414" citStr="Radev et al., 2003" startWordPosition="686" endWordPosition="689">e need other important techniques such as those for maintaining the consistency of words and phrases that refer to the same object, and for making the results more readable; however, they are not included here. \x0cfixed number, the situation was the same as TSC2. At DUC 2002, extracts (important sentences) were used, and this allowed us to evaluate sentence extraction. However, it is not possible to measure the effectiveness of redundant sentences reduction since the corpus was not annotated to show sentence with same content. In addition, this is the same even if we use the SummBank corpus (Radev et al., 2003). In any case, because many of the current summarization systems for multiple documents are based on sentence extraction, we believe these corpora to be unsuitable as sets of documents for evaluation. On this basis, in TSC3, we assumed that the process of multiple document summarization consists of the following three steps, and we produce a corpus for the evaluation of the system at each of the three steps4 . Step 1 Extract important sentences from a given set of documents Step 2 Minimize redundant sentences from the result of Step 1 Step 3 Rewrite the result of Step 2 to reduce the size of t</context>
<context position="14697" citStr="Radev et al., 2003" startWordPosition="2369" endWordPosition="2372">N ,\x01\x14&amp;quot;\x08# ,\x01\x14&amp;quot; \x02 , ? .\x1e\\\x0c1 is computed as follows: k[l Y o F maxl SUJ[Y o F Y k[l\x7fWo F maxl SUT t o F SUT % k[l o F maxl SUJ\x10SUT % o F SUT % and its Coverage is 0.553. If the system extracts \x01 \x02 ,\x01%\x02$# ,\x01%\x02\x08\x02 ,\x01\x14\x07 ,\x01\x14! ,\x01\x14&amp;quot;\x08# , then the Coverage is 0.780. ktl Y o F maxl Y[JXY o FZY ktl\x7fxo F maxl SUT QWV o FSUT QWV ktl o F maxl SUJ\x17SUT QWV o FRSUT QWV 3.1.4 Weighted Coverage Now we define Weighted Coverage since each sentence in TSC3 is ranked A, B or C, where A is the best. This is similar to Relative Utility (Radev et al., 2003). We only use three ranks in order to limit the ranking cost. The definition is obtained by modifying equation (6). W.C. F u\x03 lnxlnm\x06o\x10ok[lnm\x06o l onl o l\x16 onl\x16 o la5onla5o J (7) where =.\x1e\\\x0c1 denotes the ranking of the \\ -th sentence of the abstract and .$=.$\\\x0e1\x081 is its weight. .$= 1 is the number of sentences whose ranking is = in the abstract. Suppose the first sentence is ranked A, the second B, and the third C in Table 1, and their weights are given as 9. ] 1) ,.\x14a1) 7@ and 9. \&amp;apos; 16)+7@ 7. As before, if the system extracts \x01 \x02\x1e# ,\x01 \x02\x08\x</context>
</contexts>
<marker>Radev, Teufel, Saggion, Lam, Blitzer, Qi, Celebi, Liu, Drabek, 2003</marker>
<rawString>D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. Celebi, D. Liu, and E. Drabek. 2003. Evaluation challenges in largescale document summarization. In Proc. of the 41st ACL, pages 375382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>Sentence Extraction as a Classification Task.</title>
<date>1997</date>
<booktitle>In Proc. of the ACL Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>5865</pages>
<contexts>
<context position="6524" citStr="Teufel and Moens, 1997" startWordPosition="1060" endWordPosition="1063"> set of sentences that human annotators judge as being important in a document set (Fukusima and Okumura, 2001; Zechner, 1996; Paice, 1990). 4 This is based on general ideas of a summarization system and is not intended to impose any conditions on a summarization system. Mainichi articles Yomiuri articles abstract (a) (b) (c) (d) Doc. x Doc. y Figure 1: An example of an abstract and its sources. 2. A set of sentences that are suitable as a source for producing an abstract, i.e., a set of sentences in the original documents that correspond to the sentences in the abstracts(Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing and McKeown, 1999). When we consider how summaries are produced, it seems more natural to identify important segments in the document set and then produce summaries by combining and rephrasing such information than to select important sentences and revise them as summaries. Therefore, we believe that second type of extract is superior and thus we prepared the extracts in that way. However, as stated in the previous section, with multiple document summarization, there may be more than one sentence with the same content, and thus we may have more than one set of sentences in </context>
</contexts>
<marker>Teufel, Moens, 1997</marker>
<rawString>S. Teufel and M. Moens. 1997. Sentence Extraction as a Classification Task. In Proc. of the ACL Workshop on Intelligent Scalable Text Summarization, pages 5865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
</authors>
<title>Fast Generation of Abstracts from General Domain Text Corpora by Extracting Relevant Sentences.</title>
<date>1996</date>
<booktitle>In Proc. of the 16th COLING,</booktitle>
<pages>986989--0</pages>
<contexts>
<context position="6027" citStr="Zechner, 1996" startWordPosition="972" endWordPosition="973">tomatic evaluation using a scoring program. We adopted an intrinsic evaluation by human judges for step 3, which is currently under evaluation. We provide details of the extracts prepared for steps 1 and 2 and their evaluation measures in the following sections. We do not report the overall evaluation results for TSC3. 2.2 Data Preparation for Sentence Extraction We begin with guidelines for annotating important sentences (extracts). We think that there are two kinds of extract. 1. A set of sentences that human annotators judge as being important in a document set (Fukusima and Okumura, 2001; Zechner, 1996; Paice, 1990). 4 This is based on general ideas of a summarization system and is not intended to impose any conditions on a summarization system. Mainichi articles Yomiuri articles abstract (a) (b) (c) (d) Doc. x Doc. y Figure 1: An example of an abstract and its sources. 2. A set of sentences that are suitable as a source for producing an abstract, i.e., a set of sentences in the original documents that correspond to the sentences in the abstracts(Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing and McKeown, 1999). When we consider how summaries are produced, it seems more natu</context>
<context position="20264" citStr="Zechner, 1996" startWordPosition="3304" endWordPosition="3305">rnatorial election. 0560 Mistakes in entrance examination of Kansai University. 0570 Space shuttle Endeavour, from its launch to return. 0580 40 million-year-old fossil of new monkey species found by research group at Kyoto University. 0590 Dead body of George Mallory found on Mt. Everest. 0600 Release of SONYs AIBO. 0610 e-one, look-alike of iMac. 0630 Research on Kitora tomb resumes. 0640 Tidal wave damage generated by earthquake in Papua New Guinea. 0650 Mistaken bombing of the Chinese embassy by NATO. 4.2 Compared Extraction Methods We used the lead-based method, the TFc IDF-based method (Zechner, 1996) and the sequential patternbased method (Hirao et al., 2003), and compared performance of these summarization methods on the TSC3 corpus. Lead-based Method The documents in a test set were sorted in chronological and ascending order. Then, we extracted a sentence at a time from the beginning of each document and collected them to form a summary. TFc IDF-based Method The score of a sentence is the sum of the significant scores of each content word in the sentence. We therefore extracted sentences in descending order of importance score. The sentence score Stfidf.\x14\x01 ^ 1 is defined by the f</context>
</contexts>
<marker>Zechner, 1996</marker>
<rawString>K. Zechner. 1996. Fast Generation of Abstracts from General Domain Text Corpora by Extracting Relevant Sentences. In Proc. of the 16th COLING, pages 986989. \x0c&amp;apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>