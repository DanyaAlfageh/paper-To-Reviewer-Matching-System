<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<category confidence="0.205854">
b&apos;A non-projective dependency parser
Pasi Tapanainen and Timo J~irvinen
</category>
<affiliation confidence="0.863654">
University of Helsinki, Department of General Linguistics
</affiliation>
<bodyText confidence="0.369851333333333">
Research Unit for Multilingual Language Technology
P.O. Box 4, FIN-00014 Universityof Helsinki,Finland
{Pas i.Tapanainen, Timo. Jarvinen}@l ing. Hel sinki. f i
</bodyText>
<sectionHeader confidence="0.956339" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999247555555556">
We describe a practical parser for unre-
stricted dependencies. The parser creates
links between words and names the links
according to their syntactic functions. We
first describe the older Constraint Gram-
mar parser where many of the ideas come
from. Then we proceed to describe the cen-
tral ideas of our new parser. Finally, the
parser is evaluated.
</bodyText>
<sectionHeader confidence="0.998273" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9981715">
We are concerned with surface-syntactic parsing of
running text. Our main goal is to describe syntac-
tic analyses of sentences using dependency links that
show the he~t-modifier relations between words. In
addition, these links have labels that refer to the
syntactic function of the modifying word. A simpli-
fied example is in Figure 1, where the link between
I and see denotes that I is the modifier of see and
its syntactic function is that of subject. Similarly, a
modifies bird, and it is a determiner.
</bodyText>
<equation confidence="0.613683333333333">
see
bi
i ~ d\&apos;~b~ bird
</equation>
<figureCaption confidence="0.9976">
Figure 1: Dependencies for sentence I see a bird.
</figureCaption>
<bodyText confidence="0.9123668">
First, in this paper, we explain some central con-
cepts of the Constraint Grammar framework from
which many of the ideas are derived. Then, we give
some linguistic background to the notations we are
using, with a brief comparison to other current de-
pendency formalisms and systems. New formalism
is described briefly, and it is utilised in a small toy
grammar to illustrate how the formalism works. Fi-
nally, the real parsing system, with a grammar of
some 2 500 rules, is evaluated.
</bodyText>
<page confidence="0.997008">
64
</page>
<bodyText confidence="0.9982094">
The parser corresponds to over three man-years of
work, which does not include the lexical analyser and
the morphological disambiguator, both parts of the
existing English Constraint Grammar parser (Karls-
son et al., 1995). The parsers can be tested via
</bodyText>
<sectionHeader confidence="0.7183075" genericHeader="introduction">
WWW t .
2 Background
</sectionHeader>
<bodyText confidence="0.9959555">
Our work is partly based on the work done with
the Constraint Grammar framework that was orig-
inally proposed by Fred Karlsson (1990). A de-
tMled description of the English Constraint Gram-
mar (ENGCG) is in Karlsson et al. (1995). The basic
rule types of the Constraint Grammar (Tapanainen,
1996)2 are REMOVEand SELECTfor discarding and se-
lecting an alternative reading of a word. Rules also
have contextual tests that describe the condition ac-
cording to which they may be applied. For example,
</bodyText>
<listItem confidence="0.6028816">
the rule
REMOVE (V) IF (-1C DET);
discards a verb (V) reading if the preceding word
(-1) is unambiguously (C) a determiner (DET). More
than one such test can be appended to a rule.
</listItem>
<bodyText confidence="0.9992164">
The rule above represents a local rule: the test
checks only neighbouring words in a foreknown po-
sition before or after the target word. The test may
also refer to the positions somewhere in the sentence
without specifying the exact location. For instance,
</bodyText>
<sectionHeader confidence="0.79945" genericHeader="method">
SELECT (IMP) IF (NOT *-1 NOM-HEAD);
</sectionHeader>
<bodyText confidence="0.97697225">
means that a nominal head (NOM-HEADis a set that
contains part-of-speech tags that may represent a
nominal head) may not appear anywhere to the left
(NOT*-1).
~at http://www.ling.helsinki.fi/~tapanain/dg/
~The CG-2 notation here (Tapanainen, 1996) is dif-
ferent from the former (Karlsson et al., 1995). A con-
cise introduction to the formalism is also to be found in
Samuelsson et al. (1996) and Hurskainen (1996).
\x0cThis &quot;anywhere&quot; to the left or right may be re-
stricted by BARRIERs, which restrict the area of the
test. Basically, the barrier can be used to limit
the test only to the current clause (by using clause
boundary markers and &quot;stop words&quot;) or to a con-
stituent (by using &quot;stop categories&quot;) instead of the
whole sentence. In addition, another test may be
added relative to the unrestricted context position
using keyword LINK. For example, the following rule
discards the syntactic function3 QI-0BJ (indirect ob-
ject):
</bodyText>
<equation confidence="0.657731">
REMOVE (@I-OeJ)
IF (*-1C VFIN BARRIER SVOO
LINK NOT 0 SVOO);
</equation>
<bodyText confidence="0.984737486486487">
The rule holds if the closest finite verb to the left is
unambiguously (C) a finite verb (VFIN), and there is
no ditransitive verb or participle (subcategorisation
SV00) between the verb and the indirect object. If,
in addition, the verb does not take indirect objects,
i.e. there is no SY00 in the same verb (LINg NOT 0
SV00), the @I-0BJ reading will be discarded.
In essence, the same formalism is used in the syn-
tactic analysis in J~rvinen (1994) and Anttila (1995).
After the morphological disambiguation, all legiti-
mate surface-syntactic labels are added to the set of
morphological readings. Then, the syntactic rules
discard contextually illegitimate alternatives or se-
lect legitimate ones.
The syntactic tagset of the Constraint Grammar
provides an underspecific dependency description.
For example, labels for functional heads (such as
SUBJ, 0BJ, I-0BJ) mark the word which is a head
of a noun phrase having that function in the clause,
but the parent is not indicated. In addition, the rep-
resentation is shallow, which means that, e.g., ob-
jects of infinitives and participles receive the same
type of label as objects of finite verbs. On the other
hand, the non-finite verb forms functioning as ob-
jects receive only verbal labels.
When using the grammar formalism described
above, a considerable amount of syntactic ambigu-
ity can not be resolved reliably and is therefore left
pending in the parse. As a consequence, the output
is not optimal in many applications. For example, it
is not possible to reliably pick head-modifier pairs
from the parser output or collect arguments of verbs,
which was one of the tasks we originally were inter-
ested in.
To solve the problems, we developed a more pow-
erful rule formalism which utilises an explicit depen-
dency representation. The basic Constraint Gram-
</bodyText>
<footnote confidence="0.993997">
3The convention in the Constraint Grammar is that
the tags for syntactic functions begin with the @-sign.
</footnote>
<page confidence="0.999798">
65
</page>
<bodyText confidence="0.9876045">
mar idea of introducing the information in a piece-
meal fashion is retained, but the integration of dif-
ferent pieces of information is more efficient in the
new system.
</bodyText>
<sectionHeader confidence="0.962189" genericHeader="method">
3 Dependency grammars in a
</sectionHeader>
<bodyText confidence="0.952908">
nutshell
Our notation follows the classical model of depen-
dency theory (Heringer, 1993) introduced by Lucien
Tesni~re (1959) and later advocated by Igor Mel\&apos;~uk
(1987).
</bodyText>
<subsectionHeader confidence="0.99964">
3.1 Uniqueness and projectivity
</subsectionHeader>
<bodyText confidence="0.993948483870968">
In Tesni~re\&apos;s and Mel\&apos;Suk\&apos;s dependency notation ev-
ery element of the dependency tree has a unique
head. The verb serves as the head of a clause and
the top element of the sentence is thus the main
verb of the main clause. In some other theories, e.g.
Hudson (1991), several heads are allowed.
Projectivity (or adjacency4) was not an issue for
Tesni~re (1959, ch. 10), because he thought that the
linear order of the words does not belong to the syn-
tactic level of representation which comprises the
structural order only.
Some early formalisations, c.f. (Hays, 1964), have
brought the strict projectivity (context-free) require-
ment into the dependency framework. This kind
of restriction is present in many dependency-based
parsing systems (McCord, 1990; Sleator and Tem-
perley, 1991; Eisner, 1996).
But obviously any recognition grammar should
deal with non-projective phenomena to the extent
they occur in natural languages as, for example, in
the analysis shown in Figure 2. Our system has no
in-built restrictions concerning projectivity, though
the formalism allows us to state when crossing links
are not permitted.
We maintain that one is generally also interested
in the linear order of elements, and therefore it is
presented in the tree diagrams. But, for some pur-
poses, presenting all arguments in a canonical order
might be more adequate. This, however, is a matter
of output formatting, for which the system makes
several options available.
</bodyText>
<subsectionHeader confidence="0.999339">
3.2 Valency and categories
</subsectionHeader>
<bodyText confidence="0.9251556">
The verbs (as well as other elements) have a valency
that describes the number and type of the modifiers
they may have. In valency theory, usually, comple-
ments (obligatory) and adjuncts (optional) are dis-
tinguished.
</bodyText>
<footnote confidence="0.674131">
4D is adjacent to H provided that every word between
</footnote>
<figure confidence="0.9887962">
D and H is a subordinate of H (Hudson, 1991).
\x0cmain: &lt;ROOT&gt;
&lt;SAID&gt;
VFIN
obj:
&lt;JOAN&gt;
N SG
SG3 VFIIV\&apos;~bj:
/ ~&lt;DECIDE&gt;
&lt;WHATEVER&gt;&lt;J OHN&gt; &lt;TO&gt;
PRON WH N SG INFMARK&gt;
&lt;SUITS&gt;
SG3 VFIN
&lt;LIKES&gt; &lt;HER&gt;
PRON ACC SG3
</figure>
<figureCaption confidence="0.999368">
Figure 2: A dependency structure for the sentence: Joan said whatever John likes to decide suits her.
</figureCaption>
<bodyText confidence="0.998792692307692">
Our notation makes a difference between valency
(rule-based) and subcategorisation (lexical): the va-
lency tells which arguments are expected; the sub-
categorisation tells which combinations are legiti-
mate. The valency merely provides a possibility to
have an argument. Thus, a verb having three va-
lency slots may have e.g. subcategorisation SV00 or
SV0C. The former denotes: Subject, Verb, indirect
Object and Object, and the latter: Subject, Verb,
Object and Object Complement. The default is a
nominal type of complement, but there might also
be additional information concerning the range of
possible complements, e.g., the verb say may have
an object (SV0), which may also be realised as a
to-infinitive clause, WH-clause, that-clause or quote
structure.
The adjuncts are not usually marked in the verbs
because most of the verbs may have e.g. spatio-
temporal arguments. Instead, adverbial comple-
ments and adjuncts that are typical of particular
verbs are indicated. For instance, the verb decide
has the tag &lt;P/on&gt; which means that the preposi-
tional phrase on is typically attached to it.
The distinction between the complements and the
adjuncts is vague in the implementation; neither the
complements nor the adjuncts are obligatory.
</bodyText>
<sectionHeader confidence="0.96015" genericHeader="method">
4 Introducing the dependencies
</sectionHeader>
<bodyText confidence="0.998702">
Usually, both the dependent element and its head
are implicitly (and ambiguously) present in the Con-
straint Grammar type of rule. Here, we make this
dependency relation explicit. This is done by declar-
ing the heads and the dependents (complement or
modifier) in the context tests.
For example, the subject label (@SUB
J) is chosen
and marked as a dependent of the immediately fol-
</bodyText>
<page confidence="0.979183">
66
</page>
<bodyText confidence="0.977727333333333">
lowing auxiliary (AUXMOD)in the following rule:
SELECT (@SUBJ) IF (1C AUXMOD HEAD);
To get the full benefit of the parser, it is also use-
ful to name the valency slot in the rule. This has
two effects: (1) the valency slot is unique, i.e. no
more than one subject is linked to a finite verb5,
and (2) we can explicitly state in rules which kind
of valency slots we expect to be filled. The rule thus
is of the form:
</bodyText>
<equation confidence="0.4274105">
SELECT (@SUBJ)
IF (1C AUXMOD HEAD = subject);
</equation>
<bodyText confidence="0.998817">
The rule above works well in an unambiguous con-
text but there is still need to specify more tolerant
rules for ambiguous contexts. The rule
</bodyText>
<equation confidence="0.633276">
INDEX (@SUBJ)
IF (1C AUXMOD HEAD = subject);
</equation>
<bodyText confidence="0.9954175">
differs from the previous rule in that it leaves the
other readings of the noun intact and only adds a
(possible) subject dependency, while both the previ-
ous rules disambiguated the noun reading also.
But especially in the rule above, the contextual
test is far from being sufficient to select the subject
reading reliably. Instead, it leaves open a possibil-
ity to attach a dependency from another syntactic
function, i.e. the dependency relations remain am-
biguous. The grammar tries to be careful not to
introduce false dependencies but for an obvious rea-
son this is not always possible. If several syntac-
tic functions of a word have dependency relations,
they form a dependency forest. Therefore, when the
syntactic function is not rashly disambiguated, the
correct reading may survive even after illegitimate
</bodyText>
<footnote confidence="0.786307666666667">
5Coordination is handled via the coordinator that col-
lects coordinated subjects in one slot.
\x0clinking, as the global pruning (Section 5) later ex-
</footnote>
<bodyText confidence="0.9184025">
tracts dependency links that form consistent trees.
Links formed between syntactic labels constitute
partial trees, usually around verbal nuclei. But a
new mechanism is needed to make full use of the
structural information provided by multiple rules.
Once a link is formed between labels, it can be used
by the other rules. For example, when a head of an
object phrase (0BJ) is found and indexed to a verb,
the noun phrase to the right (if any) is probably an
object complement (PCOMPL-0). It should have the
same head as the existing object if the verb has the
proper subcategorisation tag (SV0C). The following
rule establishes a dependency relation of a verb and
its object complement, if the object already exists.
</bodyText>
<equation confidence="0.831499333333333">
INDEX (@PCOMPL-O)
IF (*-1 @OBJ BARRIER @NPHEAD
LINK 0 UP object SVOC HEAD=o-compl);
</equation>
<bodyText confidence="0.9860515">
The rule says that a dependency relation (o-omp1)
should be added but the syntactic functions should
not be disambiguated (INDEX). The object comple-
ment PCOMPL-0is linked to the verb readings hav-
ing the subcategorisation SV0C. The relation of the
object complement and its head is such that the
noun phrase to the left of the object complement is
an object (QOBJ) that has established a dependency
relation (object) to the verb.
Naturally, the dependency relations may also be
followed downwards (DOWN).But it is also possible to
declare the last item in a chMn of the links (e.g. the
verb chain would have been wanted) using the key-
words TOP and BOTTOM.
</bodyText>
<sectionHeader confidence="0.814991" genericHeader="method">
5 Ambiguity and pruning
</sectionHeader>
<bodyText confidence="0.934841">
We pursue the following strategy for linking and dis-
</bodyText>
<listItem confidence="0.877738">
ambiguation.
* In the best case, we are sure that some reading
is correct in the current context. In this case,
both disambiguation and linking can be done
at the same time (with command SELECT and
keyword HEAD).
e The most typical case is that the context gives
</listItem>
<bodyText confidence="0.9289654">
some evidence about the correct reading, but we
know that there are some rare instances when
that reading is not correct. In such a case, we
only add a link.
e Sometimes the context gives strong hints as to
what the correct reading can not be. In such
a case we can remove some readings even if
we do not know what the correct alternative
is. This is a fairly typical case in the Con-
straint Grammar framework, but relatively rare
</bodyText>
<page confidence="0.995298">
67
</page>
<bodyText confidence="0.925455567567568">
in the new dependency grammar. In practice,
these rules are most likely to cause errors, apart
from their linguistic interpretation often being
rather obscure. Moreover, there is no longer
any need to remove these readings explicitly by
rules, because the global pruning removes read-
ings which have not obtained any &quot;extra evi-
dence&quot;.
Roughly, one could say that the REMOVErules of
the Constraint Grammar are replaced by the INDEX
rules. The overall result is that the rules in the
new framework are much more careful than those
of ENGCG.
As already noted, the dependency grammar has
a big advantage over ENGCG in dealing with am-
biguity. Because the dependencies are supposed to
form a tree, we can heuristically prune readings that
are not likely to appear in such a tree. We have the
following hypotheses: (1) the dependency forest is
quite sparse and a whole parse tree can not always
be found; (2) pruning should favour large (sub)trees;
(3) unlinked readings of a word can be removed when
there is a linked reading present among the alterna-
tives; (4) unambiguous subtrees are more likely to be
correct than ambiguous ones; and (5) pruning need
not force the words to be unambiguous. Instead,
we can apply the rules iteratively, and usually some
of the rules apply when the ambiguity is reduced.
Pruning is then applied agMn, and so on. Further-
more, the pruning mechanism does not contain any
language specific statistics, but works on a topolog-
ical basis only.
Some of the most heuristic rules may be applied
only after pruning. This has two advantages: very
heuristic links would confuse the pruning mecha-
nism, and words that would not otherwise have a
head, may still get one.
</bodyText>
<sectionHeader confidence="0.993949" genericHeader="method">
6 Toy-grammar example
</sectionHeader>
<bodyText confidence="0.9748305">
In this section, we present a set of rules, and show
how those rules can parse the sentence &quot;Joan said
whatever John likes to decide suits her&quot;. The toy
grammar containing 8 rules is presented in Figure 3.
The rules are extracted from the real grammar, and
they are then simplified; some tests are omitted and
some tests are made simpler. The grammar is ap-
plied to the input sentence in Figure 4, where the
tags are almost equivalent to those used by the
English Constraint Grammar, and the final result
equals Figure 2, where only the dependencies be-
tween the words and certain tags are printed.
Some comments concerning the rules in the toy
grammar (Figure 3) are in order:
</bodyText>
<table confidence="0.640772315789474">
\x0cINDEX (@SUBJ) IF (1 @+F HEAD --- subj:);
INDEX (INF @-FMAINV) IF (-1 INFMARK) (-2 PTC1-COMPL-V + SVO HEAD -- obj:);
INDEX (@INFMARK&gt;) IF (1 (INF @-FMAINV) HEAD -- infmark:);
SELECT (PRON ACC @OBJ) IF (1C CLB) (-1 @MAINV HEAD -- obj:);
INDEX (PRON WH @OHJ)
IF (*1 @SUBJ BARRIER @NPHEAD-MAIN
LINK 0 UP subj: @+F
LINK 0 TOP v-ch: @MAINV
LINK 0 BOTTOM obj: SVO + @-FMAINV HEAD = obj:);
INDEX @MAINV
IF (*-1 WH BARRIER @MV-CLB/CC LINK -1 @MV-CLB/CC)
(*IC @+F BARRIER @SUBJ OR CLB HEAD = subj:);
PRUNING
INDEX @MAINV
IF (NOT *1 @+F BARRIER SUBJ-BARRIER)
(*-1 (PRON WH) BARRIER CLB LINK -1 VCOG + SVO + @MAINV HEAD = obj:);
INDEX @+FMAINV
IF (NOT 0 @+FAUXV) (NOT &quot;1 @+F BARRIER CLB)
(0 DOWN subj: @SUBJ LINK NOT *-1 @CS) (@0 (&lt;s&gt;) HEAD = main:);
</table>
<figureCaption confidence="0.990533">
Figure 3: A toy grammar of 8 rules
</figureCaption>
<equation confidence="0.726604222222222">
#(1)
#(2)
#(3)
#(4)
#(5)
#(6)
#(*)
#(7)
#(8)
</equation>
<listItem confidence="0.8051062">
1. A simple rule shows how the subject (QSUBJ)is
indexed to a finite verb by a link named subj.
2. The infinitivespreceded by the infinitivemarker
to can be reliably linked to the verbs with the
proper subcategorisation, i.e. the verb belongs
to both categories PTCl-COHPL-Vand SV0.
3. The infinitive marker is indexed to the infinitive
by the link named infmaxk.
4. Personal pronouns have morphological ambigu-
ity between nominative (NOM) and accusative
(ACC) readings. Here, the accusative reading
is selected and linked to the main verb imme-
diately to the left, if there is an unambiguous
clause boundary immediately to the right.
5. The WH-pronoun is a clause boundary marker,
</listItem>
<bodyText confidence="0.965761923076923">
but the only reliable means to find its head is
to follow the links. Therefore, the WH-pronoun
is not indexed before the appropriate subject is
linked to the verb chain which also has a verbal
object.
The rule states: the first noun phrase head la-
bel to the right is a subject (SUB
J), link subj
exists and is followed up to the finite verb (+F)
in a verb chain (v-ch), which is then followed
up to the main verb. Then object or comple-
ment links are followed downwards (BOTTOH),
.
</bodyText>
<page confidence="0.988706">
68
</page>
<bodyText confidence="0.99476737037037">
to the last verbal reading (here decide). If then
a verb with subcategorisation for objects is en-
countered, an object link from the WH-pronoun
is formed.
This kind of rule that starts from word A, fol-
lows links up to word B and then down to word
C, introduces a non-projective dependency link
if word B is between words A and C.
Note that the conditions TOP and BOTT0Xfollow
the chain of named link, if any, to the upper or
lower end of a chain of a multiple (zero or more)
links with the same name. Therefore TOP v-ch:
@MAINValways ends with the main verb in the
verb chain, whether this be a single finite verb
like likes or a chain like would have been liked.
The WH-clause itself may function as a subject,
object, etc. Therefore, there is a set of rules
for each function. The &quot;WH-clause as subject&quot;
rule looks for a finite verb to the right. No in-
tervening subject labels and clause boundaries
are allowed.
Rules 1-5 are applied in the first round. After
that, the pruning operation disambiguates finite
verbs, and rule 6 will apply.
Pruning will be applied once again. The sen-
tence is thus disambiguated both morphologi-
cally and morphosyntactically, and a syntactic
</bodyText>
<figure confidence="0.934788466666667">
\x0c&quot;&lt; Joan&gt;&quot;
&quot;joan&quot; N NOM SG @NH @SUBJ @OBJ @I-OBJ @PCOMPL-S @PCOMPL-O @APP @A&gt; @&lt;P @O-ADVL
&quot;&lt;said&gt;&quot;
&quot;say&quot; PCP2 @&lt;P-FMAINV @-FMAINV
&quot;say&quot; V PAST VFIN @+FMAINV
&quot;say&quot; A ABS ~PCOMPL-S @PCOMPL-O @A&gt; @APP @SUBJ @OBJ @I-OBJ @&lt;P @&lt;NOM
&quot;&lt; whatever&gt;&quot;
&quot;whatever&quot; ADV @ADVL @AD-A&gt;
&quot;whatever&quot; DET CENTRAL WH SG/PL @DN&gt;
&quot;whatever&quot; &lt;CLB&gt; PRON WH SG/PL @SUBJ ~OBJ @I-OBJ ~PCOMPL-S @PCOMPL-O @&lt;P @&lt;NOM
&quot;&lt; John&gt;&quot;
&quot;john&quot; N NOM SG ~NH @SUBJ ~OBJ ~I-OBJ @PCOMPL-S @PCOMPL-O ~APP ~A&gt; ~&lt;P ~O-ADVL
&quot;&lt;likes&gt;&quot;
&quot;like&quot; N NOM PL @NH @SUBJ @OBJ @I-OBJ @PCOMPL-S @PCOMPL-O @APP @A&gt; ~&lt;P @O-ADVL
&quot;like&quot; V PRES SG3 VFIN ~+FMAINV
&quot;&lt;to&gt;&quot;
&quot;to&quot; PREP @&lt;NOM @ADVL
&quot;to&quot; INFMARK&gt; ~INFMARK&gt;
&quot;&lt;decide&gt;&quot;
&quot;decide&quot; V SUBJUNCTIVE VFIN @+FMAINV
&quot;decide&quot; V IMP VFIN @+FMAINV
&quot;decide&quot; V INF ~-FMAINV ~&lt;P-FMAINV
&quot;decide&quot; V PRES -SG3 VFIN ~+FMAINV
&quot;&lt;suits&gt;&quot;
&quot;suit&quot; V PRES SG3 VFIN @+FMAINV
&quot;suit&quot; N NOM PL @NH @SUBJ @OBJ @I-OBJ @PCOMPL-S @PCOMPL-O @APP @A&gt; @&lt;P @O-ADVL
&quot;&lt;her&gt;&quot;
&quot;she&quot; PRON PERS FEM GEN SG3 @GN&gt;
&quot;she&quot; PRON PERS FEM ACC SG3 ~OBJ
&quot;&lt;.&gt;&quot;
</figure>
<figureCaption confidence="0.999946">
Figure 4: A sentence after morphological analysis. Each line presents a morphological and @-signs mor-
</figureCaption>
<bodyText confidence="0.9973163">
phosyntactic alternatives, e.g. whatever is ambiguous in 10 ways. The subcategorisation/valency information
is not printed here.
reading from each word belongs to a subtree of
which the root is said or suits.
7. The syntactic relationship between the verbs is
established by a rule stating that the rightmost
main verb is the (clause) object of a main verb
to the left, which Mlows such objects.
8. Finally, there is a single main verb, which is
indexed to the root (&lt;s&gt;) (in position GO).
</bodyText>
<sectionHeader confidence="0.969885" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.966827">
7.1 Efficiency
</subsectionHeader>
<bodyText confidence="0.991792555555556">
The evaluation was done using small excerpts of
data, not used in the development of the system.
All text samples were excerpted from three different
genres in the Bank of English (J~irvinen, 1994) data:
American National Public Radio (broadcast), British
Books data (literature), and The Independent (news-
paper). Figure 5 lists the samples, their sizes, and
the average and maximum sentence lengths. The
measure is in words excluding punctuation.
</bodyText>
<page confidence="0.996325">
69
</page>
<bodyText confidence="0.578056">
size avg. max. total
</bodyText>
<table confidence="0.486757">
(w) length length time
broadcast 2281 19 44 12 sec.
literature 1920 15 51 8.5 sec.
newspaper 1427 19 47 8.5 sec.
</table>
<figureCaption confidence="0.975756">
Figure 5: Benchmark used in the evaluation
</figureCaption>
<bodyText confidence="0.860883363636364">
In addition, Figure 5 shows the total processing
time required for the syntactic analysis of the sam-
ples. The syntactic analysis has been done in a nor-
mal PC with the Linux operating system. The PC
has a Pentium 90 MHz processor and 16 MB of mem-
ory. The speed roughly corresponds to 200 words in
second. The time does not include morphological
anMysis and disambiguation 6.
6The CG-2 program (Tapanainen, 1996) runs a mod-
ified disambiguation grammar of Voutilainen (1995)
about 1000 words in second.
</bodyText>
<sectionHeader confidence="0.820164" genericHeader="evaluation">
\x0cDG ENGCG
</sectionHeader>
<bodyText confidence="0.882317">
succ. arab. succ. amb.
</bodyText>
<table confidence="0.983464666666667">
broadcast 97.0 % 3.2 % 96.8 % 12.7 %
literature 97.3 % 3.3 % 95.9 % 11.3 %
newspaper 96.4 % 3.3 % 94.2 % 13.7 %
</table>
<figureCaption confidence="0.782811">
Figure 6: ENGCG syntax and morphosyntactic level
of the dependency grammar
</figureCaption>
<subsectionHeader confidence="0.99481">
7.2 Comparison to ENGCG syntax
</subsectionHeader>
<bodyText confidence="0.979403347826087">
One obvious point of reference is the ENGCG syn-
tax, which shares a level of similar representation
with an almost identical tagset to the new system.
In addition, both systems use the front parts of the
ENGCG system for processing the input. These in-
clude the tokeniser, lexical analyser and morpholog-
ical disambiguator.
Figure 6 shows the results of the comparison of the
ENGCG syntax and the morphosyntactic level of the
dependency grammar. Because both systems leave
some amount of the ambiguity pending, two figures
are given: the success rate, which is the percent-
age of correct morphosyntactic labels present in the
output, and the ambiguity rate, which is the percent-
age of words containing more than one label. The
ENGCG results compare to those reported elsewhere
(J~rvinen, 1994; Tapanainen and J/~rvinen, 1994).
The DG success rate is similar or maybe even
slightly better than in ENGCG. More importantly,
the ambiguity rate is only about a quarter of that
in the ENGCG output. The overall result should be
considered good in the sense that the output con-
tains information about the syntactic functions (see
</bodyText>
<figureCaption confidence="0.938965">
Figure 4) not only part-of-speech tags.
</figureCaption>
<subsectionHeader confidence="0.97613">
7.3 Dependencies
</subsectionHeader>
<bodyText confidence="0.9998208">
The major improvement over ENGCG is the level
of explicit dependency representation, which makes
it possible to excerpt modifiers of certain elements,
such as arguments of verbs. This section evaluates
the success of the level of dependencies.
</bodyText>
<subsubsectionHeader confidence="0.602218">
7.3.1 Unnamed dependencies
</subsubsectionHeader>
<bodyText confidence="0.995970090909091">
One of the crude measures to evaluate depen-
dencies is to count how many times the correct
head is found. The results are listed in Fig-
ure 7. Precision is [ received correct links~ and re-
received links J
call /received correct links ~ The difference between
desired links. J&quot;
precision and recall is due to the fact that the parser
does not force a head on every word. Trying out
some very heuristic methods to assign heads would
raise recall but lower precision. A similar measure
</bodyText>
<table confidence="0.900836">
precision recall
broadcast 93.4 % 88.0 %
literature 96.0 % 88.6 %
newspaper 95.3 % 87.9 %
</table>
<figureCaption confidence="0.724639">
Figure 7: Percentages of heads correctly attached
</figureCaption>
<table confidence="0.938025583333333">
broadcast precision recall N
subjects 95 % 89 % 244
objects 89 % 83 % 140
predicatives 96 % 86 % 57
literature precision recall N
subjects 98 % 92 % 195
objects 94 % 91% 118
predicatives 97 % 93 % 72
newspaper precision recall N
subjects 95 % 83 % 136
objects 94 % 88 % 103
predicatives 92 % 96 % 23
</table>
<figureCaption confidence="0.8644745">
Figure 8: Rates for main functional dependencies
is used in (Eisner, 1996) except that every word has
</figureCaption>
<bodyText confidence="0.8735735">
a head, i.e. the precision equals recall, reported as
79.2%.
</bodyText>
<subsubsectionHeader confidence="0.863192">
7.3.2 Named dependencies
</subsubsectionHeader>
<bodyText confidence="0.997918764705882">
We evaluated our parser against the selected de-
pendencies in the test samples. The samples be-
ing rather small, only the most common dependen-
cies are evaluated: subject, object and predicative.
These dependencies are usually resolved more re-
liably than, say, appositions, prepositional attach-
ments etc. The results of the test samples are listed
in Figure 8. It seems the parser leaves some amount
of the words unlinked (e.g. 10-15 % of subjects) but
what it has recognised is generally correct (precision
95-98% for subjects).
Dekang Lin (1996) has earlier used this kind of
evaluation, where precision and recall were for sub-
jects 87 % and 78 %, and for complements (includ-
ing objects) 84 % and 72 %, respectively. The results
are not strictly comparable because the syntactic de-
scription is somewhat different.
</bodyText>
<sectionHeader confidence="0.996572" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9995138">
In this paper, we have presented some main features
of our new framework for dependency syntax. The
most important result is that the new framework al-
lows us to describe non-projective dependency gram-
mars and apply them efficiently. This is a property
</bodyText>
<page confidence="0.970212">
70
</page>
<bodyText confidence="0.99927423076923">
\x0cthat will be crucial when we will apply this frame-
work to a language having free word-order.
Basically, the parsing framework combines the
Constraint Grammar framework (removing ambigu-
ous readings) with a mechanism that adds depen-
dencies between readings or tags. This means that
while the parser disambiguates it also builds up a
dependency forest that, in turn, is reduced by other
disambiguation rules and a global pruning mecha-
nism.
This setup makes it possible to operate on several
layers of information, and use and combine struc-
tural information more efficiently than in the orig-
inal Constraint Grammar framework, without any
further disadvantage in dealing with ambiguity.
First preliminary evaluations are presented. Com-
pared to the ENGCG syntactic analyser, the output
not only contains more information but it is also
more accurate and explicit. The ambiguity rate is
reduced to a quarter without any compromise in cor-
rectness. We did not have access to other systems,
and care must be taken when interpreting the re-
sults which are not strictly comparable. However,
the comparison to other current systems suggests
that our dependency parser is very promising both
theoretically and practically.
</bodyText>
<sectionHeader confidence="0.978357" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99374325">
We are using Atro Voutilainen\&apos;s (1995) improved
part-of-speech disambiguation grammar whichruns in
the CG-2parser. Voutilainen and Juha Heikkil5 created
the original ENGCG lexicon.
</bodyText>
<sectionHeader confidence="0.98584" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999657557142857">
Arto Anttila. 1995. How to recognise subjects in
English. In Karlsson et al., chapt. 9, pp. 315-358.
Dekang Lin. 1996. Evaluation of Principar with the
Susanne corpus. In John Carroll, editor, Work-
shop on Robust Parsing, pages 54-69, Prague.
Jason M. Eisner. 1996. Three new probabilistic
models for dependency parsing: An exploration.
In The 16th International Conference on Compu-
tational Linguistics, pages 340-345. Copenhagen.
David G. Hays. 1964. Dependency theory: A
formalism and some observations. Language,
40(4):511-525.
Hans Jiirgen Heringer. 1993. Dependency syntax -
basic ideas and the classical model. In Joachim
Jacobs, Arnim von Stechow, Wolfgang Sternefeld,
and Thee Venneman, editors, Syntax - An In-
ternational Handbook of Contemporary Research,
volume 1, chapter 12, pages 298-316. Walter de
Gruyter, Berlin - New York.
Richard Hudson. 1991. English Word Grammar.
Basil Blackwell, Cambridge, MA.
Arvi Hurskainen. 1996. Disambiguation of morpho-
logical analysis in Bantu languages. In The 16th
International Conference on Computational Lin-
guistics, pages 568-573. Copenhagen.
Time J~rvinen. 1994. Annotating 200 million
words: the Bank of English project. In The 15th
International Conference on Computational Lin-
guistics Proceedings, pages 565-568. Kyoto.
Fred Karlsson, Atro Voutilainen, Juha Heikkil~, and
Arto Anttila, editors. 1995. Constraint Gram-
mar: a language-independent system for parsing
unrestricted text, volume 4 of Natural Language
Processing. Mouton de Gruyter, Berlin and N.Y.
Fred Karlsson. 1990. Constraint grammar as a
framework for parsing running text. In Hans Karl-
gren, editor, Papers presented to the 13th Interna-
tional Conference on Computational Linguistics,
volume 3, pages 168-173, Helsinki, Finland.
Michael McCord. 1990. Slot grammar: A system for
simpler construction of practical natural language
grammars. In lq,Studer, editor, Natural Language
and Logic: International Scientific Symposium,
Lecture Notes in Computer Science, pages 118-
145. Springer, Berlin.
Igor A. Mel\&apos;~uk. 1987. Dependency Syntax: Theory
and Practice. State University of New York Press,
Albany.
Christer Samuelsson, Pasi Tapanainen, and Atro
Voutilainen. 1996. Inducing constraint gram-
mars. In Laurent Miclet and Colin de la Higuera,
editors, Grammatical Inference: Learning Syntax
from Sentences, volume 1147 of Lecture Notes in
Artificial Intelligence, pages 146-155, Springer.
Daniel Sleator and Davy Temperley. 1991. Parsing
English with a link grammar. Technical Report
CMU-CS-91-196, Carnegie Mellon University.
Pasi Tapanainen and Time J/irvinen. 1994. Syn-
tactic analysis of natural language using linguis-
tic rules and corpus-based patterns. In The 15th
International Conference on Computational Lin-
guistics Proceedings, pages 629-634. Kyoto.
Pasi Tapanainen. 1996. The Constraint Grammar
Parser CG-2. Number 27 in Publications of the
Department of General Linguistics, University of
Helsinki.
Lucien TesniSre. 1959. l~ldments de syntaxe stvuc-
turale, l~ditions Klincksieck, Paris.
Atro Voutilainen. 1995. Morphological disambigua-
tion. In Karlsson et al., chapter 6, pages 165-284.
</reference>
<page confidence="0.979897">
71
</page>
<figure confidence="0.253593">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.367798">
<title confidence="0.98753">b&apos;A non-projective dependency parser</title>
<author confidence="0.998842">Pasi Tapanainen</author>
<author confidence="0.998842">Timo Jirvinen</author>
<affiliation confidence="0.90071">University of Helsinki, Department of General Linguistics Research Unit for Multilingual Language Technology</affiliation>
<address confidence="0.870192">P.O. Box 4, FIN-00014 Universityof Helsinki,Finland</address>
<note confidence="0.573692">{Pas i.Tapanainen, Timo. Jarvinen}@l ing. Hel sinki. f i</note>
<abstract confidence="0.9914486">We describe a practical parser for unrestricted dependencies. The parser creates links between words and names the links according to their syntactic functions. We first describe the older Constraint Grammar parser where many of the ideas come from. Then we proceed to describe the central ideas of our new parser. Finally, the parser is evaluated.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arto Anttila</author>
</authors>
<title>How to recognise subjects in English.</title>
<date>1995</date>
<journal>In Karlsson et al., chapt. 9,</journal>
<pages>315--358</pages>
<contexts>
<context position="4426" citStr="Anttila (1995)" startWordPosition="734" endWordPosition="735">ample, the following rule discards the syntactic function3 QI-0BJ (indirect object): REMOVE (@I-OeJ) IF (*-1C VFIN BARRIER SVOO LINK NOT 0 SVOO); The rule holds if the closest finite verb to the left is unambiguously (C) a finite verb (VFIN), and there is no ditransitive verb or participle (subcategorisation SV00) between the verb and the indirect object. If, in addition, the verb does not take indirect objects, i.e. there is no SY00 in the same verb (LINg NOT 0 SV00), the @I-0BJ reading will be discarded. In essence, the same formalism is used in the syntactic analysis in J~rvinen (1994) and Anttila (1995). After the morphological disambiguation, all legitimate surface-syntactic labels are added to the set of morphological readings. Then, the syntactic rules discard contextually illegitimate alternatives or select legitimate ones. The syntactic tagset of the Constraint Grammar provides an underspecific dependency description. For example, labels for functional heads (such as SUBJ, 0BJ, I-0BJ) mark the word which is a head of a noun phrase having that function in the clause, but the parent is not indicated. In addition, the representation is shallow, which means that, e.g., objects of infinitive</context>
</contexts>
<marker>Anttila, 1995</marker>
<rawString>Arto Anttila. 1995. How to recognise subjects in English. In Karlsson et al., chapt. 9, pp. 315-358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Evaluation of Principar with the Susanne corpus. In</title>
<date>1996</date>
<booktitle>Workshop on Robust Parsing,</booktitle>
<pages>54--69</pages>
<editor>John Carroll, editor,</editor>
<location>Prague.</location>
<contexts>
<context position="25459" citStr="Lin (1996)" startWordPosition="4298" endWordPosition="4299">cision equals recall, reported as 79.2%. 7.3.2 Named dependencies We evaluated our parser against the selected dependencies in the test samples. The samples being rather small, only the most common dependencies are evaluated: subject, object and predicative. These dependencies are usually resolved more reliably than, say, appositions, prepositional attachments etc. The results of the test samples are listed in Figure 8. It seems the parser leaves some amount of the words unlinked (e.g. 10-15 % of subjects) but what it has recognised is generally correct (precision 95-98% for subjects). Dekang Lin (1996) has earlier used this kind of evaluation, where precision and recall were for subjects 87 % and 78 %, and for complements (including objects) 84 % and 72 %, respectively. The results are not strictly comparable because the syntactic description is somewhat different. 8 Conclusion In this paper, we have presented some main features of our new framework for dependency syntax. The most important result is that the new framework allows us to describe non-projective dependency grammars and apply them efficiently. This is a property 70 \x0cthat will be crucial when we will apply this framework to a</context>
</contexts>
<marker>Lin, 1996</marker>
<rawString>Dekang Lin. 1996. Evaluation of Principar with the Susanne corpus. In John Carroll, editor, Workshop on Robust Parsing, pages 54-69, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In The 16th International Conference on Computational Linguistics,</booktitle>
<pages>340--345</pages>
<location>Copenhagen.</location>
<contexts>
<context position="7069" citStr="Eisner, 1996" startWordPosition="1157" endWordPosition="1158">hus the main verb of the main clause. In some other theories, e.g. Hudson (1991), several heads are allowed. Projectivity (or adjacency4) was not an issue for Tesni~re (1959, ch. 10), because he thought that the linear order of the words does not belong to the syntactic level of representation which comprises the structural order only. Some early formalisations, c.f. (Hays, 1964), have brought the strict projectivity (context-free) requirement into the dependency framework. This kind of restriction is present in many dependency-based parsing systems (McCord, 1990; Sleator and Temperley, 1991; Eisner, 1996). But obviously any recognition grammar should deal with non-projective phenomena to the extent they occur in natural languages as, for example, in the analysis shown in Figure 2. Our system has no in-built restrictions concerning projectivity, though the formalism allows us to state when crossing links are not permitted. We maintain that one is generally also interested in the linear order of elements, and therefore it is presented in the tree diagrams. But, for some purposes, presenting all arguments in a canonical order might be more adequate. This, however, is a matter of output formatting</context>
<context position="24801" citStr="Eisner, 1996" startWordPosition="4192" endWordPosition="4193">e very heuristic methods to assign heads would raise recall but lower precision. A similar measure precision recall broadcast 93.4 % 88.0 % literature 96.0 % 88.6 % newspaper 95.3 % 87.9 % Figure 7: Percentages of heads correctly attached broadcast precision recall N subjects 95 % 89 % 244 objects 89 % 83 % 140 predicatives 96 % 86 % 57 literature precision recall N subjects 98 % 92 % 195 objects 94 % 91% 118 predicatives 97 % 93 % 72 newspaper precision recall N subjects 95 % 83 % 136 objects 94 % 88 % 103 predicatives 92 % 96 % 23 Figure 8: Rates for main functional dependencies is used in (Eisner, 1996) except that every word has a head, i.e. the precision equals recall, reported as 79.2%. 7.3.2 Named dependencies We evaluated our parser against the selected dependencies in the test samples. The samples being rather small, only the most common dependencies are evaluated: subject, object and predicative. These dependencies are usually resolved more reliably than, say, appositions, prepositional attachments etc. The results of the test samples are listed in Figure 8. It seems the parser leaves some amount of the words unlinked (e.g. 10-15 % of subjects) but what it has recognised is generally </context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In The 16th International Conference on Computational Linguistics, pages 340-345. Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Hays</author>
</authors>
<title>Dependency theory: A formalism and some observations.</title>
<date>1964</date>
<journal>Language,</journal>
<pages>40--4</pages>
<contexts>
<context position="6838" citStr="Hays, 1964" startWordPosition="1125" endWordPosition="1126">k (1987). 3.1 Uniqueness and projectivity In Tesni~re\&apos;s and Mel\&apos;Suk\&apos;s dependency notation every element of the dependency tree has a unique head. The verb serves as the head of a clause and the top element of the sentence is thus the main verb of the main clause. In some other theories, e.g. Hudson (1991), several heads are allowed. Projectivity (or adjacency4) was not an issue for Tesni~re (1959, ch. 10), because he thought that the linear order of the words does not belong to the syntactic level of representation which comprises the structural order only. Some early formalisations, c.f. (Hays, 1964), have brought the strict projectivity (context-free) requirement into the dependency framework. This kind of restriction is present in many dependency-based parsing systems (McCord, 1990; Sleator and Temperley, 1991; Eisner, 1996). But obviously any recognition grammar should deal with non-projective phenomena to the extent they occur in natural languages as, for example, in the analysis shown in Figure 2. Our system has no in-built restrictions concerning projectivity, though the formalism allows us to state when crossing links are not permitted. We maintain that one is generally also intere</context>
</contexts>
<marker>Hays, 1964</marker>
<rawString>David G. Hays. 1964. Dependency theory: A formalism and some observations. Language, 40(4):511-525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Jiirgen Heringer</author>
</authors>
<title>Dependency syntax -basic ideas and the classical model.</title>
<date>1993</date>
<booktitle>Syntax - An International Handbook of Contemporary Research,</booktitle>
<volume>1</volume>
<pages>298--316</pages>
<editor>In Joachim Jacobs, Arnim von Stechow, Wolfgang Sternefeld, and Thee Venneman, editors,</editor>
<location>Berlin - New York.</location>
<contexts>
<context position="6154" citStr="Heringer, 1993" startWordPosition="1012" endWordPosition="1013">uments of verbs, which was one of the tasks we originally were interested in. To solve the problems, we developed a more powerful rule formalism which utilises an explicit dependency representation. The basic Constraint Gram3The convention in the Constraint Grammar is that the tags for syntactic functions begin with the @-sign. 65 mar idea of introducing the information in a piecemeal fashion is retained, but the integration of different pieces of information is more efficient in the new system. 3 Dependency grammars in a nutshell Our notation follows the classical model of dependency theory (Heringer, 1993) introduced by Lucien Tesni~re (1959) and later advocated by Igor Mel\&apos;~uk (1987). 3.1 Uniqueness and projectivity In Tesni~re\&apos;s and Mel\&apos;Suk\&apos;s dependency notation every element of the dependency tree has a unique head. The verb serves as the head of a clause and the top element of the sentence is thus the main verb of the main clause. In some other theories, e.g. Hudson (1991), several heads are allowed. Projectivity (or adjacency4) was not an issue for Tesni~re (1959, ch. 10), because he thought that the linear order of the words does not belong to the syntactic level of representation whi</context>
</contexts>
<marker>Heringer, 1993</marker>
<rawString>Hans Jiirgen Heringer. 1993. Dependency syntax -basic ideas and the classical model. In Joachim Jacobs, Arnim von Stechow, Wolfgang Sternefeld, and Thee Venneman, editors, Syntax - An International Handbook of Contemporary Research, volume 1, chapter 12, pages 298-316. Walter de Gruyter, Berlin - New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hudson</author>
</authors>
<title>English Word Grammar.</title>
<date>1991</date>
<publisher>Basil Blackwell,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6536" citStr="Hudson (1991)" startWordPosition="1077" endWordPosition="1078">eal fashion is retained, but the integration of different pieces of information is more efficient in the new system. 3 Dependency grammars in a nutshell Our notation follows the classical model of dependency theory (Heringer, 1993) introduced by Lucien Tesni~re (1959) and later advocated by Igor Mel\&apos;~uk (1987). 3.1 Uniqueness and projectivity In Tesni~re\&apos;s and Mel\&apos;Suk\&apos;s dependency notation every element of the dependency tree has a unique head. The verb serves as the head of a clause and the top element of the sentence is thus the main verb of the main clause. In some other theories, e.g. Hudson (1991), several heads are allowed. Projectivity (or adjacency4) was not an issue for Tesni~re (1959, ch. 10), because he thought that the linear order of the words does not belong to the syntactic level of representation which comprises the structural order only. Some early formalisations, c.f. (Hays, 1964), have brought the strict projectivity (context-free) requirement into the dependency framework. This kind of restriction is present in many dependency-based parsing systems (McCord, 1990; Sleator and Temperley, 1991; Eisner, 1996). But obviously any recognition grammar should deal with non-projec</context>
<context position="8065" citStr="Hudson, 1991" startWordPosition="1320" endWordPosition="1321">near order of elements, and therefore it is presented in the tree diagrams. But, for some purposes, presenting all arguments in a canonical order might be more adequate. This, however, is a matter of output formatting, for which the system makes several options available. 3.2 Valency and categories The verbs (as well as other elements) have a valency that describes the number and type of the modifiers they may have. In valency theory, usually, complements (obligatory) and adjuncts (optional) are distinguished. 4D is adjacent to H provided that every word between D and H is a subordinate of H (Hudson, 1991). \x0cmain: &lt;ROOT&gt; &lt;SAID&gt; VFIN obj: &lt;JOAN&gt; N SG SG3 VFIIV\&apos;~bj: / ~&lt;DECIDE&gt; &lt;WHATEVER&gt;&lt;J OHN&gt; &lt;TO&gt; PRON WH N SG INFMARK&gt; &lt;SUITS&gt; SG3 VFIN &lt;LIKES&gt; &lt;HER&gt; PRON ACC SG3 Figure 2: A dependency structure for the sentence: Joan said whatever John likes to decide suits her. Our notation makes a difference between valency (rule-based) and subcategorisation (lexical): the valency tells which arguments are expected; the subcategorisation tells which combinations are legitimate. The valency merely provides a possibility to have an argument. Thus, a verb having three valency slots may have e.g. subcategori</context>
</contexts>
<marker>Hudson, 1991</marker>
<rawString>Richard Hudson. 1991. English Word Grammar. Basil Blackwell, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvi Hurskainen</author>
</authors>
<title>Disambiguation of morphological analysis in Bantu languages.</title>
<date>1996</date>
<booktitle>In The 16th International Conference on Computational Linguistics,</booktitle>
<pages>568--573</pages>
<location>Copenhagen.</location>
<contexts>
<context position="3378" citStr="Hurskainen (1996)" startWordPosition="555" endWordPosition="556">reknown position before or after the target word. The test may also refer to the positions somewhere in the sentence without specifying the exact location. For instance, SELECT (IMP) IF (NOT *-1 NOM-HEAD); means that a nominal head (NOM-HEADis a set that contains part-of-speech tags that may represent a nominal head) may not appear anywhere to the left (NOT*-1). ~at http://www.ling.helsinki.fi/~tapanain/dg/ ~The CG-2 notation here (Tapanainen, 1996) is different from the former (Karlsson et al., 1995). A concise introduction to the formalism is also to be found in Samuelsson et al. (1996) and Hurskainen (1996). \x0cThis &quot;anywhere&quot; to the left or right may be restricted by BARRIERs, which restrict the area of the test. Basically, the barrier can be used to limit the test only to the current clause (by using clause boundary markers and &quot;stop words&quot;) or to a constituent (by using &quot;stop categories&quot;) instead of the whole sentence. In addition, another test may be added relative to the unrestricted context position using keyword LINK. For example, the following rule discards the syntactic function3 QI-0BJ (indirect object): REMOVE (@I-OeJ) IF (*-1C VFIN BARRIER SVOO LINK NOT 0 SVOO); The rule holds if th</context>
</contexts>
<marker>Hurskainen, 1996</marker>
<rawString>Arvi Hurskainen. 1996. Disambiguation of morphological analysis in Bantu languages. In The 16th International Conference on Computational Linguistics, pages 568-573. Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Time Jrvinen</author>
</authors>
<title>Annotating 200 million words: the Bank of English project.</title>
<date>1994</date>
<booktitle>In The 15th International Conference on Computational Linguistics Proceedings,</booktitle>
<pages>565--568</pages>
<publisher>Kyoto.</publisher>
<marker>Jrvinen, 1994</marker>
<rawString>Time J~rvinen. 1994. Annotating 200 million words: the Bank of English project. In The 15th International Conference on Computational Linguistics Proceedings, pages 565-568. Kyoto.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Atro Voutilainen, Juha Heikkil~, and Arto Anttila, editors.</title>
<date>1995</date>
<booktitle>of Natural Language Processing. Mouton de Gruyter,</booktitle>
<volume>4</volume>
<pages>168--173</pages>
<editor>In Hans Karlgren, editor,</editor>
<location>Berlin</location>
<marker>Karlsson, 1995</marker>
<rawString>Fred Karlsson, Atro Voutilainen, Juha Heikkil~, and Arto Anttila, editors. 1995. Constraint Grammar: a language-independent system for parsing unrestricted text, volume 4 of Natural Language Processing. Mouton de Gruyter, Berlin and N.Y. Fred Karlsson. 1990. Constraint grammar as a framework for parsing running text. In Hans Karlgren, editor, Papers presented to the 13th International Conference on Computational Linguistics, volume 3, pages 168-173, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael McCord</author>
</authors>
<title>Slot grammar: A system for simpler construction of practical natural language grammars.</title>
<date>1990</date>
<booktitle>In lq,Studer, editor, Natural Language and Logic: International Scientific Symposium, Lecture Notes in Computer Science,</booktitle>
<pages>118--145</pages>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="7025" citStr="McCord, 1990" startWordPosition="1150" endWordPosition="1151">se and the top element of the sentence is thus the main verb of the main clause. In some other theories, e.g. Hudson (1991), several heads are allowed. Projectivity (or adjacency4) was not an issue for Tesni~re (1959, ch. 10), because he thought that the linear order of the words does not belong to the syntactic level of representation which comprises the structural order only. Some early formalisations, c.f. (Hays, 1964), have brought the strict projectivity (context-free) requirement into the dependency framework. This kind of restriction is present in many dependency-based parsing systems (McCord, 1990; Sleator and Temperley, 1991; Eisner, 1996). But obviously any recognition grammar should deal with non-projective phenomena to the extent they occur in natural languages as, for example, in the analysis shown in Figure 2. Our system has no in-built restrictions concerning projectivity, though the formalism allows us to state when crossing links are not permitted. We maintain that one is generally also interested in the linear order of elements, and therefore it is presented in the tree diagrams. But, for some purposes, presenting all arguments in a canonical order might be more adequate. Thi</context>
</contexts>
<marker>McCord, 1990</marker>
<rawString>Michael McCord. 1990. Slot grammar: A system for simpler construction of practical natural language grammars. In lq,Studer, editor, Natural Language and Logic: International Scientific Symposium, Lecture Notes in Computer Science, pages 118-145. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Mel\&apos;uk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1987</date>
<publisher>State University of New York Press,</publisher>
<location>Albany.</location>
<marker>Mel\&apos;uk, 1987</marker>
<rawString>Igor A. Mel\&apos;~uk. 1987. Dependency Syntax: Theory and Practice. State University of New York Press, Albany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Samuelsson</author>
</authors>
<title>Pasi Tapanainen, and Atro Voutilainen.</title>
<date>1996</date>
<booktitle>In Laurent Miclet and Colin de la Higuera, editors, Grammatical Inference: Learning Syntax from Sentences,</booktitle>
<volume>1147</volume>
<pages>146--155</pages>
<publisher>Springer.</publisher>
<marker>Samuelsson, 1996</marker>
<rawString>Christer Samuelsson, Pasi Tapanainen, and Atro Voutilainen. 1996. Inducing constraint grammars. In Laurent Miclet and Colin de la Higuera, editors, Grammatical Inference: Learning Syntax from Sentences, volume 1147 of Lecture Notes in Artificial Intelligence, pages 146-155, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing English with a link grammar.</title>
<date>1991</date>
<tech>Technical Report CMU-CS-91-196,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="7054" citStr="Sleator and Temperley, 1991" startWordPosition="1152" endWordPosition="1156"> element of the sentence is thus the main verb of the main clause. In some other theories, e.g. Hudson (1991), several heads are allowed. Projectivity (or adjacency4) was not an issue for Tesni~re (1959, ch. 10), because he thought that the linear order of the words does not belong to the syntactic level of representation which comprises the structural order only. Some early formalisations, c.f. (Hays, 1964), have brought the strict projectivity (context-free) requirement into the dependency framework. This kind of restriction is present in many dependency-based parsing systems (McCord, 1990; Sleator and Temperley, 1991; Eisner, 1996). But obviously any recognition grammar should deal with non-projective phenomena to the extent they occur in natural languages as, for example, in the analysis shown in Figure 2. Our system has no in-built restrictions concerning projectivity, though the formalism allows us to state when crossing links are not permitted. We maintain that one is generally also interested in the linear order of elements, and therefore it is presented in the tree diagrams. But, for some purposes, presenting all arguments in a canonical order might be more adequate. This, however, is a matter of ou</context>
</contexts>
<marker>Sleator, Temperley, 1991</marker>
<rawString>Daniel Sleator and Davy Temperley. 1991. Parsing English with a link grammar. Technical Report CMU-CS-91-196, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Time Jirvinen</author>
</authors>
<title>Syntactic analysis of natural language using linguistic rules and corpus-based patterns.</title>
<date>1994</date>
<booktitle>In The 15th International Conference on Computational Linguistics Proceedings,</booktitle>
<pages>629--634</pages>
<publisher>Kyoto.</publisher>
<marker>Tapanainen, Jirvinen, 1994</marker>
<rawString>Pasi Tapanainen and Time J/irvinen. 1994. Syntactic analysis of natural language using linguistic rules and corpus-based patterns. In The 15th International Conference on Computational Linguistics Proceedings, pages 629-634. Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
</authors>
<title>The Constraint Grammar Parser CG-2.</title>
<date>1996</date>
<journal>Number</journal>
<volume>27</volume>
<institution>Department of General Linguistics, University of Helsinki.</institution>
<note>in Publications of the</note>
<contexts>
<context position="2294" citStr="Tapanainen, 1996" startWordPosition="376" endWordPosition="377">h a grammar of some 2 500 rules, is evaluated. 64 The parser corresponds to over three man-years of work, which does not include the lexical analyser and the morphological disambiguator, both parts of the existing English Constraint Grammar parser (Karlsson et al., 1995). The parsers can be tested via WWW t . 2 Background Our work is partly based on the work done with the Constraint Grammar framework that was originally proposed by Fred Karlsson (1990). A detMled description of the English Constraint Grammar (ENGCG) is in Karlsson et al. (1995). The basic rule types of the Constraint Grammar (Tapanainen, 1996)2 are REMOVEand SELECTfor discarding and selecting an alternative reading of a word. Rules also have contextual tests that describe the condition according to which they may be applied. For example, the rule REMOVE (V) IF (-1C DET); discards a verb (V) reading if the preceding word (-1) is unambiguously (C) a determiner (DET). More than one such test can be appended to a rule. The rule above represents a local rule: the test checks only neighbouring words in a foreknown position before or after the target word. The test may also refer to the positions somewhere in the sentence without specifyi</context>
<context position="22015" citStr="Tapanainen, 1996" startWordPosition="3709" endWordPosition="3710">rds excluding punctuation. 69 size avg. max. total (w) length length time broadcast 2281 19 44 12 sec. literature 1920 15 51 8.5 sec. newspaper 1427 19 47 8.5 sec. Figure 5: Benchmark used in the evaluation In addition, Figure 5 shows the total processing time required for the syntactic analysis of the samples. The syntactic analysis has been done in a normal PC with the Linux operating system. The PC has a Pentium 90 MHz processor and 16 MB of memory. The speed roughly corresponds to 200 words in second. The time does not include morphological anMysis and disambiguation 6. 6The CG-2 program (Tapanainen, 1996) runs a modified disambiguation grammar of Voutilainen (1995) about 1000 words in second. \x0cDG ENGCG succ. arab. succ. amb. broadcast 97.0 % 3.2 % 96.8 % 12.7 % literature 97.3 % 3.3 % 95.9 % 11.3 % newspaper 96.4 % 3.3 % 94.2 % 13.7 % Figure 6: ENGCG syntax and morphosyntactic level of the dependency grammar 7.2 Comparison to ENGCG syntax One obvious point of reference is the ENGCG syntax, which shares a level of similar representation with an almost identical tagset to the new system. In addition, both systems use the front parts of the ENGCG system for processing the input. These include </context>
</contexts>
<marker>Tapanainen, 1996</marker>
<rawString>Pasi Tapanainen. 1996. The Constraint Grammar Parser CG-2. Number 27 in Publications of the Department of General Linguistics, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien TesniSre</author>
</authors>
<title>l~ldments de syntaxe stvucturale, l~ditions Klincksieck,</title>
<date>1959</date>
<location>Paris.</location>
<marker>TesniSre, 1959</marker>
<rawString>Lucien TesniSre. 1959. l~ldments de syntaxe stvucturale, l~ditions Klincksieck, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atro Voutilainen</author>
</authors>
<title>Morphological disambiguation.</title>
<date>1995</date>
<booktitle>In Karlsson et al., chapter 6,</booktitle>
<pages>165--284</pages>
<contexts>
<context position="22076" citStr="Voutilainen (1995)" startWordPosition="3718" endWordPosition="3719">h length time broadcast 2281 19 44 12 sec. literature 1920 15 51 8.5 sec. newspaper 1427 19 47 8.5 sec. Figure 5: Benchmark used in the evaluation In addition, Figure 5 shows the total processing time required for the syntactic analysis of the samples. The syntactic analysis has been done in a normal PC with the Linux operating system. The PC has a Pentium 90 MHz processor and 16 MB of memory. The speed roughly corresponds to 200 words in second. The time does not include morphological anMysis and disambiguation 6. 6The CG-2 program (Tapanainen, 1996) runs a modified disambiguation grammar of Voutilainen (1995) about 1000 words in second. \x0cDG ENGCG succ. arab. succ. amb. broadcast 97.0 % 3.2 % 96.8 % 12.7 % literature 97.3 % 3.3 % 95.9 % 11.3 % newspaper 96.4 % 3.3 % 94.2 % 13.7 % Figure 6: ENGCG syntax and morphosyntactic level of the dependency grammar 7.2 Comparison to ENGCG syntax One obvious point of reference is the ENGCG syntax, which shares a level of similar representation with an almost identical tagset to the new system. In addition, both systems use the front parts of the ENGCG system for processing the input. These include the tokeniser, lexical analyser and morphological disambiguat</context>
</contexts>
<marker>Voutilainen, 1995</marker>
<rawString>Atro Voutilainen. 1995. Morphological disambiguation. In Karlsson et al., chapter 6, pages 165-284.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>