<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.8394055">
b&amp;quot;Local context templates for Chinese constituent boundary
prediction
</bodyText>
<author confidence="0.641174">
Qiang Zhou
</author>
<affiliation confidence="0.692247">
The State Key Laboratory of Intelligent Technology and Systems
Dept. of Computer Science and technology,
Tsinghua University, Beijing 100084
</affiliation>
<email confidence="0.969144">
zhouq@s1000e.cs.tsinghua.edu.cn
</email>
<sectionHeader confidence="0.835478" genericHeader="abstract">
Abstract:
</sectionHeader>
<bodyText confidence="0.9806826">
In this paper, we proposed a shallow
syntactic knowledge description:
constituent boundary representation and its
simple and efficient prediction algorithm,
based on different local context templates
learned from the annotated corpus. An open
test on 2780 Chinese real text sentences
showed the satisfying results: 94%(92%)
precision for the words with multiple
(single) boundary tag output.
</bodyText>
<sectionHeader confidence="0.904346" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.995656076923077">
Research on syntactic parsing has been a focus
in natural language processing for a long time. As
the development of corpus linguistics, many
statistics-based parsers were proposed, such as
Magerman(1995)s statistical decision tree parser,
Collins(1996)s bigram dependency model parser,
Ratnaparkhi(1997)s maximum entropy model
parser. All of them tried to get the complete parse
trees of the input sentences, based on the
statistical data extracted from an annotated corpus.
The best parsing accuracy of these parsers was
about 87%.
Realizing the difficulties of complete parsing,
many researches turned to explore the partial
parsing techniques. Church(1988) proposed a
simple stochastic technique for recognizing the
non-recursive base noun phrases in English.
Voutilaimen(1993) designed an English noun
phrase recognition tool --- NPTool. Abney(1997)
applied both rule-based and statistics-based
approaches for parsing chunks in English. Due to
the advantages of simplicity and robustness, these
systems can be acted as good preprocessors for
the further complete parsing.
In this paper, we will introduce our partial
parsing approach for the Chinese language. We
first proposed a shallow syntactic knowledge
description: constituent boundary representation.
It simplified the complex constituent levels in
parse trees and only kept the boundary
information of every word in different
constituents. Then, we developed a simple and
efficient constituent boundary prediction
algorithm, based on different local context
templates learned from the annotated corpus. An
open test on 2780 Chinese real text sentences
showed the satisfying results: 94%(92%)
precision for the words with multiple (single)
boundary tag output.
</bodyText>
<sectionHeader confidence="0.447371" genericHeader="introduction">
2. Constituent boundary description
</sectionHeader>
<bodyText confidence="0.991021333333334">
The constituent boundary representation
comes from the simplification of the complete
parse trees of the sentences. It omits the
constituent1 levels in parse trees and only keeps
the boundary information of every word in
different constituents, i.e. it is at the left boundary,
right boundary or middle position of a
constituent.
Evidently, if the input sentence has only one
parse tree, i.e. without syntactic ambiguity, the
constituent boundary position of every word in
the sentence is clear and definite. In the sense, the
constituent boundary tag indicates the basic
syntactic structure information in the sentence.
Separating them from the constituent structure
tree and assigning them to every word in the
sentence, we can form a special syntactic unit:
word boundary block (WBB).
Definition: A word boundary block is the
combination of the word(including part-of-speech
information) and its constituent boundary tag, i.e.
</bodyText>
<equation confidence="0.617973">
wbbi
=&lt;wi
, bi
</equation>
<bodyText confidence="0.5000945">
&amp;gt;, where wi
is the ith word in the
sentence, bi
can value 0,1,2, which means wi
</bodyText>
<footnote confidence="0.5466135">
is at
1 Hereafter, constituent represents all internal or root
</footnote>
<bodyText confidence="0.9108542">
nodes in a parse tree, i.e. phrase or sentence tags. In
our system, each constituent must consist of two or
more words(leaf node in parser tree).
\x0cthe middle, left-most, or right-most position of a
constituent respectively.
In the view of syntactic description capability,
the WBBs defined above, the chunks defined by
Abney(1991) and the phrases(i.e. constituents)
defined in a parse tree have the following
realtions: WBBs &lt; chunks &lt; phrases
Here is an example:
z The input sentence (10 words):
a X oo Z a O : A
(My brother gives him a book.)
z Its parse tree representation (7 phrases):
</bodyText>
<equation confidence="0.945391444444444">
[P1 [P2 [P3 a X oo ] [P4 [P5 Z ]
a [P6 [P7 O ] : ]]] A]
z Its chunk representation (5 chunks):
[C1 a X oo ] [C2 Z ] [C3 a ]
[C4 O : ] [C5 A]
z Its constituent boundary representation
(10 WBBs): &lt;a,1&amp;gt; &lt;X,0&amp;gt; &lt;oo,2&amp;gt; &lt;
,1&amp;gt; &lt;Z,2&amp;gt; &lt;a,0&amp;gt; &lt;O,1&amp;gt; &lt; ,2&amp;gt; &lt;
:,2&amp;gt; &lt;A,2&amp;gt;
</equation>
<bodyText confidence="0.890734090909091">
The goal of the constituent boundary
prediction is to assign a suitable boundary tag for
every word in the sentence. It can provide basic
information for further syntactic parsing research.
The following lists some application examples:
z To develop a statistics-based Chinese
parser(Zhou 1997) based on the bracket
matching principle(Zhou and Huang,1997).
z To develop a Chinese maximum noun
phrase identifier(Zhou,Sun and Huang, 1999).
z The automatic inference of Chinese
</bodyText>
<listItem confidence="0.674941666666667">
probabilistic context-free grammar(PCFG)
(Zhou and Huang 1998).
3. Local context templates
</listItem>
<bodyText confidence="0.998190888888889">
The linguistic intuitions tell us that many local
contexts may be useful for constituent boundary
prediction. For example, many function words in
Chinese have their certain constituent boundary
position in the sentences, such as, most
prepositions are at the left boundaries, and the
aspectual particles (le, zhe, guo) are at the
right boundaries. Moreover, some content words
also show their preferential constituent boundary
positions in a special local context, such as most
adjectives are at the right boundary in local
context: adverb + adjective.
A tentative idea is how to use such simple
local context information(including the part-of-
speech(POS) tags and the number of Chinese
characters(CN)) to develop an efficient automatic
boundary prediction algorithm. Therefore, we
defined the following local context templates
</bodyText>
<figure confidence="0.665927909090909">
(LCTs):
1) Unigram POS template: ti
, BPFLi
2) Bigram POS templates:
z Left restriction: ti-1
ti
, BPFLi
z Right restriction:ti
ti+1
, BPFLi
3) Trigram POS template: ti-1
</figure>
<equation confidence="0.814572">
ti
ti+1
, BPFLi
4) Trigram POS+CN template: ti-1
+cni-1
ti
+cni
ti+1
+cni+1
, BPFLi
</equation>
<bodyText confidence="0.9793297">
In the above LCTs, ti
is the POS tag of the ith
word in the sentence, cni
is its character number,
and BPFLi
is the frequency distribution list of its
different BP(boundary prediction) value(0,1,2)
under the local context restrictions(LCR)(the left
and right word).
Table 1 Some examples of the local context
</bodyText>
<figure confidence="0.976116894736842">
templates
Type Token Meaning
Unigram p,
39 849 476
A preposition is prior to
at the constituent left
boundary in Chinese.
bigram
(left)
a n,
5 164 2007
A noun is prior to at the
right boundary if its
previous word is an
adjective.
bigram
(right)
a n,
4 2012 160
</figure>
<bodyText confidence="0.476229333333333">
An adjective is prior to
at the left boundary if
its next word is a noun.
</bodyText>
<equation confidence="0.48602625">
Trigram
POS
Q Q X-&apos;(\x0f
\x14 \x14\x1b \x14\x17\x1c\x19
</equation>
<bodyText confidence="0.971655090909091">
A noun is prior to at the
right boundary if its
previous word is a noun
and its next one is a
partial(De).
Table 1 shows some examples of LCTs. All
these templates can be easily acquired from the
Chinese treebanks or Chinese corpus annotated
with constituent boundary tags.
Among these templates, some special ones
have the following properties:
</bodyText>
<equation confidence="0.9879024">
a) TFi
= BPFLi
[bpi
] &amp;gt; ,
b) bpi[0,2], P(bpi
|LCRi
)=BPFLi
[bpi
] / TFi
&amp;gt;
</equation>
<bodyText confidence="0.979026375">
where the total frequency threshold and the BP
probability threshold are set to 3 and 0.95,
respectively. They are called the projected
templates (PTs) (i.e. the local context template
with a projecting BP value).
Based on the different PTs, we can design a
three-stage training procedure to overcome the
problem of data sparseness:
\x0cStage 1 : Learn the unigram and bigram
templates on the whole instances in annotated corpus.
Stage 2 : Learn the trigram POS templates on the
non-projected unigram and bigram instances (see next
section for more detailed).
Stage 3 : Learn the trigram POS+CN templates
on the non-projected trigram POS instances.
Therefore, only the useful trigram templates
can be learned.
4. Automatic prediction algorithm
After getting the LCTs, the automatic
prediction algorithm becomes very simple: 1) to
set the projecting BPs based on the projected
LCTs, 2) to select the best BPs based on the non-
projected LCTs. Some detailed information will
be discussed in the following sections.
</bodyText>
<subsectionHeader confidence="0.988909">
4.1 Set the projecting BPs
</subsectionHeader>
<bodyText confidence="0.998176222222222">
In this stage, the reference sequence to the
LCTs is : unigram bigram trigram POS
trigram POS+CN, i.e. from the rough restriction
LCTs to the tight restriction LCTs. This sequence
is same with the LCT training procedure.
The detailed algorithm is as follows:
Input: the position of the ith word in the sentence.
Background: the LCTs learned from corpus.
Output: the projecting BP of the word if found;
</bodyText>
<listItem confidence="0.9658399375">
-1 otherwise.
Procedure:
z Get the local context of the ith word.
z If its unigram template is a PT, then return
its projecting BP.
z If its left and right bigram template satisfy
the following conditions:
34 TFL + TFR = BPFLL [j] + BPFLL [j] &amp;gt;
34 P(bpj  |LCRi) = (BPFLL [j] + BPFLR [j]) /
(TFL + TFR ) &amp;gt;
then return this combined projecting BP(bpj
).
z If its trigram POS template is a PT, then
return its projecting BP.
z If its trigram POS+CN template is a PT,
then return its projecting BP.
</listItem>
<subsectionHeader confidence="0.975669">
4.2 Select the best BPs
</subsectionHeader>
<bodyText confidence="0.9952286">
In this stage, the reference sequence to the
LCTs is : trigram POS+CN trigram POS
bigram unigram. Its a backing-off model
(Katz,1987), just like the approach of Collins and
Brooks(1995) for the prepositional phrase
attachment problem in English. The detailed
algorithm is as follows:
Input: the position of the ith word in the sentence.
Background: the LCTs learned from corpus.
Output: the best BP of the word.
</bodyText>
<subsubsectionHeader confidence="0.61734">
Procedure:
</subsubsectionHeader>
<listItem confidence="0.836128058823529">
z Get the local context of the ith word.
z For the kth matched trigram POS+CN
templates, if TFk
&amp;gt; , then return SelectBestBP
(BPFLk
).
z For the mth matched left bigram and nth
matched right bigram,
34 Get the Combined BPFL = BPFLm + BPFLn
34 If TFCombined_template
&amp;gt; 0, then return
SelectBestBP(Combined BPFL).
z For the kth matched unigram templates, if
TFk
&amp;gt; 0, then return SelectBestBP(BPFLk
).
z Return 1(default is at the left boundary).
</listItem>
<bodyText confidence="0.985139142857143">
The internal function SelectBestBP() tries to
select the best BP based on the frequency
distribution list of different BP value in LCTs. It
has two output modes: 1) single-output mode:
only output the best BP with the highest
frequency in the LCT; 2) multiple-output mode:
output the BPs satisfying the conditions:
</bodyText>
<figure confidence="0.8251">
|Pbpi
-Pbest
 |&lt; , where = 0.2
5. Experimental results
</figure>
<subsectionHeader confidence="0.882882">
5.1 Training and test data
</subsectionHeader>
<bodyText confidence="0.88904">
The training data were extracted from two
different parts of annotated Chinese corpus:
</bodyText>
<listItem confidence="0.7522987">
1) The small Chinese treebank developed
in Peking University(Zhou, 1996b), which
consists of the sentences extracted from
two parts of Chinese texts: (a) test set for
Chinese-English machine translation
systems, (b) Singapore primary school
textbooks.
2) The test suite treebank being
developed in Tsinghua University(Zhou
and Sun,1999), which consists of about
10,000 representative Chinese sentences
extracted from a large-scale Chinese
balanced corpus with about 2,000,000
Chinese characters.
The test data were extracted from the articles
of Peoples Daily and manually annotated with
\x0ccorrect constituent boundary tags. It was also
divided into two parts:
1) The ordinary sentences.
2) The sentences with keywords for
</listItem>
<bodyText confidence="0.849125444444444">
conjunction structures (such as the
conjunctions or special punctuation
DunHao). They can be used to test the
performance of our prediction algorithm
on complex conjunction structures.
Table 2 shows some basic statistics of these
training and test data. Only the sentences with
more than one word were used for training and
testing.
</bodyText>
<tableCaption confidence="0.889167">
Table 2 The basic statistics of training and
test data. (ASL = Average sentence length)
</tableCaption>
<figure confidence="0.9811445">
Sent.
Num.
Word
Num.
Char.
Num.
ASL
(w/s)
</figure>
<table confidence="0.99770875">
Train1 5573 64426 89492 11.56
Train2 7774 108542 173334 13.96
Test1 2780 68986 108218 24.82
Test2 1071 32358 51169 30.21
</table>
<subsectionHeader confidence="0.861852">
5.2 The learned templates
</subsectionHeader>
<bodyText confidence="0.99847719047619">
After the three-stage learning procedure, we
got four kinds of local context templates. Table 3
shows their different distribution data, where the
section Type lists the distribution of different
kinds of LCTs and the section Token lists the
distribution of total words(i.e. tokens) covered by
the LCTs. In the column PTs and Ratio, the
slash / was used to separate the PTs with total
frequency threshold 0 and 3.
More than 66% words in the training corpus
can be covered by the unigram and bigram POS
projected templates. Then only about 1/3 tokens
will be used for training the trigram templates.
Although the type distribution of the trigram
templates shows the tendency of data sparseness
(more than 70% trigram projected templates with
total frequency less than 3), the useful trigram
templates (TF&amp;gt;3) still covers about 70% tokens
learned. Therefore, we can expect that them can
play an important role during constituent
boundary prediction in open test set.
</bodyText>
<subsectionHeader confidence="0.99856">
5.3 Prediction results
</subsectionHeader>
<bodyText confidence="0.998873666666667">
In order to evaluate the performance of the
constituent boundary prediction algorithm, the
following measures were used:
</bodyText>
<listItem confidence="0.990362">
1) The cost time(CT) of the kernal
functions(CPU: CeleronTM
366, RAM: 64M).
2) Prediction precision(PP) =
</listItem>
<bodyText confidence="0.925286518518519">
number of words with correct BPs(CortBP)
total word number (TWN)
For the words with single BP output, the
correct condition is:
Annotated BP = Predicted BP
For the words with multiple BP outputs, the
correct condition is:
Annotated BP Predicted BP set
The prediction results of the two test sets were
shown in Table 4 and Table 5, whose first
columns list the different template combinations
using in the algorithm. In the columns CortBP
and PP, the slash / was used to list the
different results of the single and multiple BP
outputs.
After analyzing the experimental results, we
found:
1) The POS information in local context is
very important for constituent boundary
prediction. After using the bigram and trigram
POS templates, the prediction accuracy was
increased by about 9% and 3% respectively.
But the character number information shows
lower boundary restriction capability. Their
application only results in a slight increase of
precision in single-output mode but a slight
decrease in multiple-output mode.
</bodyText>
<tableCaption confidence="0.837533">
Table 3 Distribution data of different learned LCTs
</tableCaption>
<table confidence="0.976831714285714">
LCTs Type Token
Total PTs(=0/3) Ratio(=0/3) Total PTs(=0/3) Ratio(=0/3)
1-gram 59 24 40.68 171705 53932 31.41
2-gram(Left) 1448 1030 / 591 71.13 / 40.81 171705 87027 / 86339 50.68 / 50.28
2-gram(Right) 1440 1008 / 567 70.00 / 39.38 171705 99443 / 98754 57.92 / 57.51
3-gram (POS) 3105 2324 / 713 74.85 / 22.96 50333 24280 / 21982 48.24 / 43.07
3-gram(P+CN) 2553 1677 / 287 65.69 / 11.24 19098 5978 / 4079 31.30 / 21.36
</table>
<bodyText confidence="0.9462574">
\x0c2) Most of the prediction errors can be
attributed to the special structures in the
sentences, such as conjunction structures (CSs)
or collocation structures. Due to the long
distance dependencies among them, its very
difficult to assign the correct boundary tags to
the words in these structures only according to
the local context templates. The lower overall
precision of the test set 2 (about 2% lower
than test set 1) also indicates the boundary
prediction difficulties of the conjunction
structures, because there are more CSs in test
set 2 than in test set 1.
3) The accuracy of the multiple output
results is about 2% better than the single
output results. But the words with multiple
boundary tags constitute only about 10% of
the total words predicted. Therefore, the
multiple-output mode shows a good trade-off
between precision and redundancy. It can be
used as the best preprocessing data for the
further syntactic parser.
4) The maximal ratio of the words set by
projected templates can reach 80%. It
guarantees the higher overall precision.
</bodyText>
<listItem confidence="0.58496525">
5) The algorithm shows high efficiency. It
can process about 6,000 words per second
(CPU: CeleronTM
366, RAM: 64M).
</listItem>
<subsectionHeader confidence="0.518178">
5.4 Compare with other work
</subsectionHeader>
<bodyText confidence="0.79748225">
Zhou(1996) proposed a constituent boundary
prediction algorithm based on hidden Marcov
model(HMM). The Viterbi algorithm was used to
find the best boundary path B:
</bodyText>
<equation confidence="0.977317">
=
=
=
n
i
i
i
i b
</equation>
<figure confidence="0.7888292">
bi
P
b
CT
P
B
P
B
T
W
P
B
1
1)
|
(
)
|
(
max
arg
)
(
)
|
,
(
max
arg
where the local POS probability P(CTi
 |bi
) was
computed by backing-off model and the bigram
parameters: f(ti-1
, ti
, bi
) and f(bi
, ti
, ti+1
).
</figure>
<bodyText confidence="0.992482833333333">
To compare its performance with our
algorithm, the trigram (POS and POS+CN)
information was added up to its backing-off
model. Table 6 and Table 7 show the prediction
results of the HMM-based algorithm, based on the
same parameters learned from training set 1 and
</bodyText>
<page confidence="0.526624">
2.
</page>
<tableCaption confidence="0.860573">
Table 6. Prediction results of the HMM-based
Table 4 Experimental results of the test set 1
templates Set the Projecting BPs Select the best BPs Total
</tableCaption>
<figure confidence="0.981932614457831">
used TWN CortBP PP(%) TWN CortBP PP(%) TWN CortBP PP(%) CT
1-gram 22408 22143 98.82 46555 32876/
24374
70.62/
73.84
68963 55019/
56517
79.78/
81.95
14/16
+2-gram 46167 45285 98.09 22796 16188/
17678
71.01/
77.55
68963 61473/
62963
89.14/
91.30
11/15
+3-gram
POS
55321 53969 97.56 13642 9946/
10986
72.90/
80.53
68963 63915/
64955
92.68/
94.19
13/11
+3-gram
P+CN
57360 55866 97.40 11603 8168/
8955
70.40/
77.18
68963 64034/
64821
92.85/
93.99
11/14
Table 5 Experimental results of the test set 2
Templates Set the Projecting BPs Select the best BPs Total
Used TWN CortBP PP(%) TWN CortBP PP(%) TWN CortBP PP(%) CT
1-gram 10016 9873 98.57 22342 15737/1
6593
70.44/
74.27
32358 25610/
26466
79.15/
81.79
6/5
+2-gram 21085 20454 97.00 11273 7856/
8607
70.44/
74.27
32358 28310/
29061
87.49/
89.81
4/4
+3-gram
POS
24974 24079 96.42 7384 5225/
5777
70.76/
78.24
32358 29304/
29856
90.56/
92.27
3/6
+3-gram
P+CN
25839 24866 96.23 6519 4525/
4958
69.40/
76.05
32358 29390/
29824
90.83/
92.17
</figure>
<page confidence="0.890321">
8/6
</page>
<table confidence="0.9811234">
\x0calgorithm(test set 1)
TWN CortBP PP(%) Ctime
2-gram 68963 60908 88.32 144
+ 3g-POS 68963 63397 91.93 138
+3g-P+CN 68963 63649 92.29 139
</table>
<tableCaption confidence="0.973404">
Table 7. Prediction results of the HMM-based
</tableCaption>
<table confidence="0.957984">
algorithm(test set 2)
TWN CortBP PP(%) Ctime
+2-gram 32358 27792 85.89 68
+3g-POS 32358 28918 89.37 70
+3g-P+CN 32358 29030 89.72 68
</table>
<bodyText confidence="0.934266153846154">
The performance of the LCT-based algorithm
surpassed the HMM-based algorithm in
accuracy(about 1%) and efficiency (about 10
times).
Another similar work is Sun(1999). The
difference lies in the definition of the constituent
boundary tags: he defined them between word
pair: wi
bi
wi+1
, not for the word. By using the
HMM and Viterbi model, his algorithm showed
the similar performance with Zhou(1996) (using
</bodyText>
<table confidence="0.6255368">
bigram POS parameters):
z Training data : 3051 sentences extracted
from Peoples Daily.
z Test data: 1000 sentences.
z Best precision:86.3%
</table>
<sectionHeader confidence="0.97349" genericHeader="conclusions">
6. Conclusions
</sectionHeader>
<bodyText confidence="0.998988516129032">
The paper proposed a constituent boundary
prediction algorithm based on local context
templates. Its characteristics can be summarized
as follows:
z The simple definition of the local context
templates made the training procedure very
easy.
z The three-stage training procedure
guarantees that only the useful trigram
templates can be learned. Thus, the data
sparseness problem was partially overcome.
z The high coverage of different types of
projected templates assures a higher overall
prediction accuracy.
z The multiple output mode provides the
possibility to describe different boundary
ambiguities.
z The algorithm runs very fast, surpasses
the HMM-based algorithm in accuracy and
efficiency.
There are a few possible improvement which
may raise performance further. Firstly, some
lexical-based templates, such as prepositions as
left restriction, may improve performance further
this needs to be investigated. The introduction
of the automatic identifiers for some special
structures, such as conjunction structures or
collocation structures, may reduce the prediction
errors due to the long distance dependency
problem. Finally, more training data is almost
certain to improve results.
</bodyText>
<sectionHeader confidence="0.96872" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.560828333333333">
The research was supported by National
Natural Science Foundation of China (NSFC)
(Grant No. 69903007).
</reference>
<sectionHeader confidence="0.673524" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997372371428571">
Abney S. (1991). Parsing by Chunks, In Robert
Berwick, Steven Abney and Carol Tenny (eds.)
Principle-Based Parsing, Kluwer Academic
Publishers.
Abney S. (1997). Part-of-speech Tagging and Partial
Parsing, In Young S. Bloothooft G. (eds.) Corpus-
based methods in language and speech processings,
118-136.
Collins M. and Brooks J. (1995) Prepositional Phrase
Attachment through a Backing-Off Model, In
David Yarowsky &amp; Ken Church(eds.) Proceedings
of the third workshop on very large corpora, MIT.
27-38.
Church K. (1988). A Stochastic Parts Program and
Noun Phrase Parser for Unrestricted Text. In:
Proceedings of Second Conference on Applied
Natural Language Processing, Austin, Texas, 136-
143.
Collins M. J. (1996). A New Statistical Parser Based
on Bigram Lexical Dependencies. In Proc. of ACL-
34, 184-191.
Katz S. (1987). Estimation of Probabilities from
sparse data for the language model component of a
speech recogniser. IEEE Transactions on ASSP,
Vol .35, No. 3.
Magerman. D. M. (1995). Statistical Decision-Tree
Models for Parsing, In Proc. of ACL-95, 276-303.
Ratnaparkhi A.(1997). A linear observed time
statistical parser based on maximum entropy
models. In Claire Cardie and Ralph
Weischedel(eds.), Second Conference on Empirical
Methods in Natural Language Processing(EMNLP-
2), Somerset, New Jersey, ACL.
Sun H. L., Lu Q. and Yu S. W.(1999). Two-level
shallow parser for unrestricted Chinese text, In
\x0cChangning Huang and Zhendong Dong (eds.)
Proceedings of Computational linguistics, Beijing:
Tsinghua University press, 280-286.
Voutilamen A. (1993). NPTool, a detector of English
Noun Phrases. In: Ken Church (ed.) Proceedings of
the Workshop on Very Large Corpora: Academic
and Industrial Perspectives. Columbus, Ohio, USA,
48-57.
Zhou Q. (1996a). A Model for Automatic Prediction
of Chinese Phrase Boundary Location,
Zhou Q. (1996b). Phrase Bracketing and Annotating
on Chinese Language Corpus . Ph.D. Dissertation,
Peking University.
Zhou Q. (1997) A Statistics-Based Chinese Parser,
In Proc. of the Fifth Workshop on Very Large
Corpora, 4-15.
Zhou Q. and Huang C.N. (1997) A Chinese syntactic
parser based on bracket matching principle,
Communication of COLIPS, 7(2), #97008.
Zhou Q. and Huang C.N. (1998). An Inference
Approach for Chinese Probabilistic Context-Free
Grammar, Chinese Journal of Computers, 21(5),
385-392.
Zhou Q. and Sun M.S. (1999). Build a Chinese
Treebank as the test suite for Chinese parser, In
Key-Sun Choi &amp; Young-Soog Chae(eds.)
Proceedings of the workshop MAL99, Beijing. 32-
37.
Zhou Q., Sun M.S. and Huang C.N.(1999)
Automatically Identify Chinese Maximal Noun
Phrases, Technical Report 99001, State Key Lab. of
Intelligent Technology and Systems, Dept. of
Computer Science and Technology, Tsinghua
University.
\x0c&amp;quot;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.455455">
<title confidence="0.9277265">b&amp;quot;Local context templates for Chinese constituent boundary prediction</title>
<author confidence="0.979898">Qiang Zhou</author>
<affiliation confidence="0.9831635">The State Key Laboratory of Intelligent Technology and Systems Dept. of Computer Science and technology,</affiliation>
<address confidence="0.80524">Tsinghua University, Beijing 100084</address>
<email confidence="0.634614">zhouq@s1000e.cs.tsinghua.edu.cn</email>
<abstract confidence="0.996756727272727">In this paper, we proposed a shallow syntactic knowledge description: constituent boundary representation and its simple and efficient prediction algorithm, based on different local context templates learned from the annotated corpus. An open test on 2780 Chinese real text sentences showed the satisfying results: 94%(92%) precision for the words with multiple (single) boundary tag output.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by Chunks,</title>
<date>1991</date>
<editor>In Robert Berwick, Steven Abney and Carol Tenny (eds.) Principle-Based Parsing,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Abney, 1991</marker>
<rawString>Abney S. (1991). Parsing by Chunks, In Robert Berwick, Steven Abney and Carol Tenny (eds.) Principle-Based Parsing, Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Part-of-speech Tagging and Partial Parsing, In</title>
<date>1997</date>
<booktitle>Corpusbased methods in language and speech processings,</booktitle>
<pages>118--136</pages>
<editor>Young S. Bloothooft G. (eds.)</editor>
<marker>Abney, 1997</marker>
<rawString>Abney S. (1997). Part-of-speech Tagging and Partial Parsing, In Young S. Bloothooft G. (eds.) Corpusbased methods in language and speech processings, 118-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>J Brooks</author>
</authors>
<title>Prepositional Phrase Attachment through a Backing-Off Model,</title>
<date>1995</date>
<booktitle>In David Yarowsky &amp; Ken Church(eds.) Proceedings of the third workshop on very large corpora, MIT.</booktitle>
<pages>27--38</pages>
<marker>Collins, Brooks, 1995</marker>
<rawString>Collins M. and Brooks J. (1995) Prepositional Phrase Attachment through a Backing-Off Model, In David Yarowsky &amp; Ken Church(eds.) Proceedings of the third workshop on very large corpora, MIT. 27-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. In:</title>
<date>1988</date>
<booktitle>Proceedings of Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<location>Austin, Texas,</location>
<marker>Church, 1988</marker>
<rawString>Church K. (1988). A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. In: Proceedings of Second Conference on Applied Natural Language Processing, Austin, Texas, 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
</authors>
<title>A New Statistical Parser Based on Bigram Lexical Dependencies.</title>
<date>1996</date>
<booktitle>In Proc. of ACL34,</booktitle>
<pages>184--191</pages>
<marker>Collins, 1996</marker>
<rawString>Collins M. J. (1996). A New Statistical Parser Based on Bigram Lexical Dependencies. In Proc. of ACL34, 184-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Katz</author>
</authors>
<title>Estimation of Probabilities from sparse data for the language model component of a speech recogniser.</title>
<date>1987</date>
<journal>IEEE Transactions on ASSP, Vol</journal>
<volume>35</volume>
<marker>Katz, 1987</marker>
<rawString>Katz S. (1987). Estimation of Probabilities from sparse data for the language model component of a speech recogniser. IEEE Transactions on ASSP, Vol .35, No. 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M</author>
</authors>
<title>Statistical Decision-Tree Models for Parsing,</title>
<date>1995</date>
<booktitle>In Proc. of ACL-95,</booktitle>
<pages>276--303</pages>
<marker>M, 1995</marker>
<rawString>Magerman. D. M. (1995). Statistical Decision-Tree Models for Parsing, In Proc. of ACL-95, 276-303.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<booktitle>In Claire Cardie and Ralph Weischedel(eds.), Second Conference on Empirical Methods in Natural Language Processing(EMNLP2),</booktitle>
<location>Somerset, New Jersey, ACL.</location>
<marker>Ratnaparkhi, </marker>
<rawString>Ratnaparkhi A.(1997). A linear observed time statistical parser based on maximum entropy models. In Claire Cardie and Ralph Weischedel(eds.), Second Conference on Empirical Methods in Natural Language Processing(EMNLP2), Somerset, New Jersey, ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H L Sun</author>
<author>Q Lu</author>
<author>S W Yu</author>
</authors>
<title>Two-level shallow parser for unrestricted Chinese text,</title>
<booktitle>In \x0cChangning Huang and Zhendong Dong (eds.) Proceedings of Computational linguistics,</booktitle>
<pages>280--286</pages>
<publisher>Tsinghua University press,</publisher>
<location>Beijing:</location>
<marker>Sun, Lu, Yu, </marker>
<rawString>Sun H. L., Lu Q. and Yu S. W.(1999). Two-level shallow parser for unrestricted Chinese text, In \x0cChangning Huang and Zhendong Dong (eds.) Proceedings of Computational linguistics, Beijing: Tsinghua University press, 280-286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Voutilamen</author>
</authors>
<title>NPTool, a detector of English Noun Phrases.</title>
<date>1993</date>
<booktitle>Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives.</booktitle>
<pages>48--57</pages>
<editor>In: Ken Church (ed.)</editor>
<location>Columbus, Ohio, USA,</location>
<marker>Voutilamen, 1993</marker>
<rawString>Voutilamen A. (1993). NPTool, a detector of English Noun Phrases. In: Ken Church (ed.) Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives. Columbus, Ohio, USA, 48-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zhou</author>
</authors>
<title>A Model for Automatic Prediction of Chinese Phrase Boundary Location, Zhou Q.</title>
<date>1996</date>
<booktitle>Phrase Bracketing and Annotating on Chinese Language Corpus . Ph.D. Dissertation,</booktitle>
<institution>Peking University.</institution>
<contexts>
<context position="10516" citStr="Zhou, 1996" startWordPosition="1693" endWordPosition="1694">P(BPFLk ). z Return 1(default is at the left boundary). The internal function SelectBestBP() tries to select the best BP based on the frequency distribution list of different BP value in LCTs. It has two output modes: 1) single-output mode: only output the best BP with the highest frequency in the LCT; 2) multiple-output mode: output the BPs satisfying the conditions: |Pbpi -Pbest |&lt; , where = 0.2 5. Experimental results 5.1 Training and test data The training data were extracted from two different parts of annotated Chinese corpus: 1) The small Chinese treebank developed in Peking University(Zhou, 1996b), which consists of the sentences extracted from two parts of Chinese texts: (a) test set for Chinese-English machine translation systems, (b) Singapore primary school textbooks. 2) The test suite treebank being developed in Tsinghua University(Zhou and Sun,1999), which consists of about 10,000 representative Chinese sentences extracted from a large-scale Chinese balanced corpus with about 2,000,000 Chinese characters. The test data were extracted from the articles of Peoples Daily and manually annotated with \x0ccorrect constituent boundary tags. It was also divided into two parts: 1) The o</context>
</contexts>
<marker>Zhou, 1996</marker>
<rawString>Zhou Q. (1996a). A Model for Automatic Prediction of Chinese Phrase Boundary Location, Zhou Q. (1996b). Phrase Bracketing and Annotating on Chinese Language Corpus . Ph.D. Dissertation, Peking University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zhou</author>
</authors>
<title>A Statistics-Based Chinese Parser,</title>
<date>1997</date>
<booktitle>In Proc. of the Fifth Workshop on Very Large Corpora,</booktitle>
<pages>4--15</pages>
<contexts>
<context position="4662" citStr="Zhou 1997" startWordPosition="712" endWordPosition="713">s him a book.) z Its parse tree representation (7 phrases): [P1 [P2 [P3 a X oo ] [P4 [P5 Z ] a [P6 [P7 O ] : ]]] A] z Its chunk representation (5 chunks): [C1 a X oo ] [C2 Z ] [C3 a ] [C4 O : ] [C5 A] z Its constituent boundary representation (10 WBBs): &lt;a,1&amp;gt; &lt;X,0&amp;gt; &lt;oo,2&amp;gt; &lt; ,1&amp;gt; &lt;Z,2&amp;gt; &lt;a,0&amp;gt; &lt;O,1&amp;gt; &lt; ,2&amp;gt; &lt; :,2&amp;gt; &lt;A,2&amp;gt; The goal of the constituent boundary prediction is to assign a suitable boundary tag for every word in the sentence. It can provide basic information for further syntactic parsing research. The following lists some application examples: z To develop a statistics-based Chinese parser(Zhou 1997) based on the bracket matching principle(Zhou and Huang,1997). z To develop a Chinese maximum noun phrase identifier(Zhou,Sun and Huang, 1999). z The automatic inference of Chinese probabilistic context-free grammar(PCFG) (Zhou and Huang 1998). 3. Local context templates The linguistic intuitions tell us that many local contexts may be useful for constituent boundary prediction. For example, many function words in Chinese have their certain constituent boundary position in the sentences, such as, most prepositions are at the left boundaries, and the aspectual particles (le, zhe, guo) are at th</context>
</contexts>
<marker>Zhou, 1997</marker>
<rawString>Zhou Q. (1997) A Statistics-Based Chinese Parser, In Proc. of the Fifth Workshop on Very Large Corpora, 4-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zhou</author>
<author>C N Huang</author>
</authors>
<title>A Chinese syntactic parser based on bracket matching principle,</title>
<date>1997</date>
<journal>Communication of COLIPS,</journal>
<volume>7</volume>
<issue>2</issue>
<pages>97008</pages>
<marker>Zhou, Huang, 1997</marker>
<rawString>Zhou Q. and Huang C.N. (1997) A Chinese syntactic parser based on bracket matching principle, Communication of COLIPS, 7(2), #97008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zhou</author>
<author>C N Huang</author>
</authors>
<title>An Inference Approach for Chinese Probabilistic Context-Free Grammar,</title>
<date>1998</date>
<journal>Chinese Journal of Computers,</journal>
<volume>21</volume>
<issue>5</issue>
<pages>385--392</pages>
<contexts>
<context position="4905" citStr="Zhou and Huang 1998" startWordPosition="743" endWordPosition="746">sentation (10 WBBs): &lt;a,1&amp;gt; &lt;X,0&amp;gt; &lt;oo,2&amp;gt; &lt; ,1&amp;gt; &lt;Z,2&amp;gt; &lt;a,0&amp;gt; &lt;O,1&amp;gt; &lt; ,2&amp;gt; &lt; :,2&amp;gt; &lt;A,2&amp;gt; The goal of the constituent boundary prediction is to assign a suitable boundary tag for every word in the sentence. It can provide basic information for further syntactic parsing research. The following lists some application examples: z To develop a statistics-based Chinese parser(Zhou 1997) based on the bracket matching principle(Zhou and Huang,1997). z To develop a Chinese maximum noun phrase identifier(Zhou,Sun and Huang, 1999). z The automatic inference of Chinese probabilistic context-free grammar(PCFG) (Zhou and Huang 1998). 3. Local context templates The linguistic intuitions tell us that many local contexts may be useful for constituent boundary prediction. For example, many function words in Chinese have their certain constituent boundary position in the sentences, such as, most prepositions are at the left boundaries, and the aspectual particles (le, zhe, guo) are at the right boundaries. Moreover, some content words also show their preferential constituent boundary positions in a special local context, such as most adjectives are at the right boundary in local context: adverb + adjective. A tentative idea i</context>
</contexts>
<marker>Zhou, Huang, 1998</marker>
<rawString>Zhou Q. and Huang C.N. (1998). An Inference Approach for Chinese Probabilistic Context-Free Grammar, Chinese Journal of Computers, 21(5), 385-392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zhou</author>
<author>M S Sun</author>
</authors>
<title>Build a Chinese Treebank as the test suite for Chinese parser,</title>
<date>1999</date>
<booktitle>In Key-Sun Choi &amp; Young-Soog Chae(eds.) Proceedings of the workshop MAL99, Beijing.</booktitle>
<pages>32--37</pages>
<marker>Zhou, Sun, 1999</marker>
<rawString>Zhou Q. and Sun M.S. (1999). Build a Chinese Treebank as the test suite for Chinese parser, In Key-Sun Choi &amp; Young-Soog Chae(eds.) Proceedings of the workshop MAL99, Beijing. 32-37.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Q Zhou</author>
<author>M S Sun</author>
</authors>
<title>and Huang C.N.(1999) Automatically Identify Chinese Maximal Noun Phrases,</title>
<tech>Technical Report 99001,</tech>
<institution>State Key Lab. of Intelligent Technology and Systems, Dept. of Computer Science and Technology, Tsinghua University.</institution>
<note>x0c&amp;quot;</note>
<marker>Zhou, Sun, </marker>
<rawString>Zhou Q., Sun M.S. and Huang C.N.(1999) Automatically Identify Chinese Maximal Noun Phrases, Technical Report 99001, State Key Lab. of Intelligent Technology and Systems, Dept. of Computer Science and Technology, Tsinghua University. \x0c&amp;quot;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>