b'Committee-based Decision Making
in Probabilistic Partial Parsing
INUI Takashi3 and INUI Kentaro3y
3 Department of Arti\x0ccial Intelligence, Kyushu Institute of Technology
y PRESTO, Japan Science and Technology Corporation
ft inui,inuig@pluto.ai.kyutech.ac.jp
Abstract
This paper explores two directions for the
next step beyond the state of the art of statis-
tical parsing: probabilistic partial parsing and
committee-based decision making. Probabilis-
tic partial parsing is a probabilistic extension of
the existing notion of partial parsing, which en-
ables \x0cne-grained arbitrary choice on the trade-
o\x0b between accuracy and coverage. Committee-
based decision making is to combine the out-
puts from di\x0berent systems to make a better
decision. While various committee-based tech-
niques for NLP have recently been investigated,
they would need to be further extended so as
to be applicable to probabilistic partial pars-
ing. Aiming at this coupling, this paper gives
a general framework to committee-based deci-
sion making, which consists of a set of weight-
ing functions and a combination function, and
discusses how it can be coupled with probabilis-
tic partial parsing. Our experiments have so far
been producing promising results.
1 Introduction
There have been a number of attempts to use
statistical techniques to improve parsing perfor-
mance. While this goal has been achieved to a
certain degree given the increasing availability
of large tree banks, the remaining room for the
improvement appears to be getting saturated
as long as only statistical techniques are taken
into account. This paper explores two directions
for the next step beyond the state of the art of
statistical parsing: probabilistic partial parsing
and committee-based decision making.
Probabilistic partial parsing is a probabilistic
extension of the existing notion of partial pars-
ing ( e.g. (Jensen et al., 1993)) where a parser
selects as its output only a part of the parse tree
that are probabilistically highly reliable. This
decision-making scheme enables a \x0cne-grained
arbitrary choice on the trade-o\x0b between ac-
curacy and coverage. Such trade-o\x0b is impor-
tant since there are various applications that re-
quire reasonably high accuracy even sacri\x0ccing
coverage. A typical example is the paraphras-
ing task embedded in summarization, sentence
simpli\x0ccation (e.g. (Carroll et al., 1998)), etc.
Enabling such trade-o\x0b choice will make state-
of-the-art parsers of wider application. Partial
parsing has also been proven useful for boot-
strapping learning.
One may suspect that the realization of par-
tial parsing is a trivial matter in probabilistic
parsing just because a probabilistic parser in-
herently has the notion of \\reliability" and thus
has the trade-o\x0b between accuracy and cover-
age. However, there has so far been surpris-
ingly little research focusing on this matter and
almost no work that evaluates statistical parsers
according to their coverage-accuracy (or recall-
precision) curves. Taking the signi\x0ccance of
partial parsing into account, therefore in this
paper, we evaluate parsing performance accord-
ing to coverage-accuracy curves.
Committee-based decision making is to com-
bine the outputs from several di\x0berent systems
(e.g. parsers) to make a better decision. Re-
cently, there have been various attempts to ap-
ply committee-based techniques to NLP tasks
such as POS tagging (Halteren et al., 1998;
Brill et al., 1998), parsing (Henderson and
Brill, 1999), word sense disambiguation (Peder-
sen, 2000), machine translation (Frederking and
Nirenburg, 1994), and speech recognition (Fis-
cus, 1997). Those works empirically demon-
strated that combining di\x0berent systems often
achieved signi\x0ccant improvements over the pre-
vious best system.
In order to couple those committee-based
\x0cschemes with probabilistic partial parsing, how-
ever, one would still need to make a further ex-
tension. Aiming at this coupling, in this paper,
we consider a general framework of committee-
based decision making that consists of a set
of weighting functions and a combination func-
tion, and discuss how that framework enables
the coupling with probabilistic partial parsing.
To demonstrate how it works, we report the re-
sults of our parsing experiments on a Japanese
tree bank.
2 Probabilistic partial parsing
2.1 Dependency probability
In this paper, we consider the task of decid-
ing the dependency structure of a Japanese in-
put sentence. Note that, while we restrict our
discussion to analysis of Japanese sentences in
this paper, what we present below should also
be straightforwardly applicable to more wide-
ranged tasks such as English dependency anal-
ysis just like the problem setting considered by
Collins (1996).
Given an input sentence s as a sequence of
Bunsetsu-phrases (BPs)1, b1 b2 ::: bn, our task
is to identify their inter-BP dependency struc-
ture R = fr(bi;bj)ji = 1;:::;ng, where r(bi;bj)
denotes that bi depends on (or modi\x0ces) bj.
Let us consider a dependency probability (DP),
P(r(bi;bj)js), a probability that r(bi;bj) holds
in a given sentence s: 8i:
P
j P(r(bi;bj)js) = 1.
2.2 Estimation of DPs
Some of the state-of-the-art probabilistic lan-
guage models such as the bottomup models
P(Rjs) proposed by Collins (1996) and Fujio
et al. (1998) directly estimate DPs for a given
input, whereas other models such as PCFG-
based topdown generation models P(R;s) do
not (Charniak, 1997; Collins, 1997; Shirai et al.,
1998). If the latter type of models were totally
excluded from any committee, our committee-
based framework would not work well in prac-
tice. Fortunately, however, even for such a
model, one can still estimate DPs in the follow-
ing way if the model provides the n-best depen-
1
A bunsetsu phrase (BP) is a chunk of words con-
sisting of a content word (noun, verb, adjective, etc.)
accompanied by some functional word(s) (particle, aux-
iliary, etc.). A Japanese sentence can be analyzed as a
sequence of BPs, which constitutes an inter-BP depen-
dency structure
dency structure candidates coupled with prob-
abilistic scores.
Let Ri be the i-th best dependency structure
(i = 1;:::;n) of a given input s according to a
given model, and let RH be a set of Ri. Then,
P(r(bi;bj)js) can be estimated by the following
approximation equation:
P(r(bi;bj)js) \x19
Pr
RH
PRH
(1)
where PRH is the probability mass of R 2 RH,
and Pr
RH is the probability mass of R 2 RH
that supports r(bi;bj). The approximation er-
ror \x0f is given by \x0f \x14
PR0PRH
PR , where PR is the
probability mass of all the dependency struc-
ture candidates for s (see (Poole, 1993) for the
proof). This means that the approximation er-
ror is negligible if PRH is su\x0eciently close to
PR, which holds for a reasonably small number
n in most cases in practical statistical parsing.
2.3 Coverage-accuracy curves
We then consider the task of selecting depen-
dency relations whose estimated probability is
higher than a certain threshold \x1b (0 < \x1b \x14 1).
When \x1b is set to be higher (closer to 1.0), the
accuracy is expected to become higher, while
the coverage is expected to become lower, and
vice versa. Here, coverage C and accuracy A
are de\x0cned as follows:
C =
# of the decided relations
# of all the relations in the test set
(2)
A =
# of the correctly decided relations
# of the decided relations
(3)
Moving the threshold \x1b from 1.0 down to-
ward 0.0, one can obtain a coverage-accuracy
curve (C-A curve). In probabilistic partial pars-
ing, we evaluate the performance of a model
according to its C-A curve. A few examples
are shown in Figure 1, which were obtained
in our experiment (see Section 4). Obviously,
Figure 1 shows that model A outperformed the
other two. To summarize a C-A curve, we use
the 11-point average of accuracy (11-point ac-
curacy, hereafter), where the eleven points are
C = 0:5; 0:55;:::; 1:0. The accuracy of total
parsing corresponds to the accuracy of the point
in a C-A curve where C = 1:0. We call it total
accuracy to distinguish it from 11-point accu-
racy. Note that two models with equal achieve-
\x0caccuracy
A
D
C
A
D
C
0.8
0.85
0.9
0.95
1
0 0.2 0.4 0.6 0.8 1
0.8
0.85
0.9
0.95
1
0 0.2 0.4 0.6 0.8 1
0.8
0.85
0.9
0.95
1
0 0.2 0.4 0.6 0.8 1
coverage
Figure 1: C-A curves
ments in total accuracy may be di\x0berent in 11-
point accuracy. In fact, we found such cases in
our experiments reported below. Plotting C-A
curves enable us to make a more \x0cne-grained
performance evaluation of a model.
3 Committee-based probabilis-
tic partial parsing
We consider a general scheme of committee-
based probabilistic partial parsing as illustrated
in Figure 2. Here we assume that each commit-
tee member Mk (k = 1;:::;m) provides a DP
matrix PMk(r(bi;bj)js) (bi; bj 2 s) for each in-
put s. Those matrices are called input matrices,
and are given to the committee as its input.
A committee consists of a set of weighting
functions and a combination function. The role
assigned to weighting functions is to standardize
input matrices. The weighting function associ-
ated with model Mk transforms an input ma-
trix given by Mk to a weight matrix WMk. The
majority function then combines all the given
weight matrices to produce an output matrix O,
which represents the \x0cnal decision of the com-
mittee. One can consider various options for
both functions.
3.1 Weighting functions
We have so far considered the following three
options.
Simple The simplest option is to do nothing:
wMk
ij = PMk(r(bi;bj)js) (4)
where wMk
ij is the (i;j) element of WMk.
Normal A bare DP may not be a precise es-
timation of the actual accuracy. One can see
this by plotting probability-accuracy curves (P-
A curves) as shown in Figure 3. Figure 3 shows
that model A tends to overestimate DPs, while
Mk
M1
Mm
PMk
PM1
PMm
WM1
WMk
WMm
O
input
matrices
weight matrices
output
matrix
committee- based decision making
CF
WFMk
WFM1
WFMm
WF: Weighting Function
CF: Combination Function
models
Mk
M1
Mm
PMk
PM1
PMm
WM1
WMk
WMm
O
input
matrices
weight matrices
output
matrix
committee- based decision making
CF
WFMk
WFM1
WFMm
WF: Weighting Function
CF: Combination Function
models
Figure 2: Committee-based probabilistic partial
parsing
D
C
A
D
C
A
0.5
0.6
0.7
0.8
0.9
1
0.5 0.6 0.7 0.8 0.9 1
0.5
0.6
0.7
0.8
0.9
1
0.5 0.6 0.7 0.8 0.9 1
dependency probability
0.5
0.6
0.7
0.8
0.9
1
0.5 0.6 0.7 0.8 0.9 1
0.5
0.6
0.7
0.8
0.9
1
0.5 0.6 0.7 0.8 0.9 1
0.5
0.6
0.7
0.8
0.9
1
0.5 0.6 0.7 0.8 0.9 1
dependency probability
accuracy
Figure 3: P-A curves
model C tends to underestimate DPs. This
means that if A and C give di\x0berent answers
with the same DP, C\'s answer is more likely
to be correct. Thus, it is not necessarily a
good strategy to simply use given bare DPs in
weighted majority. To avoid this problem, we
consider the following weighting function:
wMk
ij = \x0bMk
i AMk(PMk(r(bi;bj)js)) (5)
where AMk(p) is the function that returns the
expected accuracy of Mk\'s vote with its depen-
dency probability p, and \x0bMk
i is a normalization
factor. Such a function can be trained by plot-
ting a P-A curve for training data. Note that
training data should be shared by all the com-
mittee members. In practice, for training a P-A
curve, some smoothing technique should be ap-
plied to avoid over\x0ctting.
Class The standardization process in the
above option Normal can also be seen as an
e\x0bort for reducing the averaged cross entropy
of the model on test data. Since P-A curves
tend to defer not only between di\x0berent mod-
els but also between di\x0berent problem classes,
if one incorporates some problem classi\x0ccation
into (5), the averaged cross entropy is expected
\x0cto be reduced further:
wMk
ij = \x0cMk
i AMkCbi(PMk(r(bi;bj)js)) (6)
where AMkCbi(p) is the P-A curve of model Mk
only for the problems of class Cbi in training
data, and \x0cMk
i is a normalization factor. For
problem classi\x0ccation, syntactic/lexical features
of bi may be useful.
3.2 Combining functions
For combination functions, we have so far con-
sidered only simple weighted voting, which av-
erages the given weight matrices:
oMk
ij =
1
m
m
X
k=1
wMk
ij (7)
where oMk
ij is the (i;j) element of O.
Note that the committee-based partial pars-
ing framework presented here can be seen as
a generalization of the previously proposed
voting-based techniques in the following re-
spects:
(a) A committee accepts probabilistically para-
meterized votes as its input.
(d) A committee accepts multiple voting (i.e. it
allow a committee member to vote not only
to the best-scored candidate but also to all
other potential candidates).
(c) A committee provides a means for standard-
izing original votes.
(b) A committee outputs a probabilistic distri-
bution representing a \x0cnal decision, which
constitutes a C-A curve.
For example, none of simple voting techniques
for word class tagging proposed by van Hal-
teren et al. (1998) does not accepts multiple
voting. Henderson and Brill (1999) examined
constituent voting and naive Bayes classi\x0cca-
tion for parsing, obtaining positive results for
each. Simple constituent voting, however, does
not accept parametric votes. While Naive Bayes
seems to partly accept parametric multiple vot-
ing, it does not consider either standardization
or coverage/accuracy trade-o\x0b.
4 Experiments
4.1 Settings
We conducted experiments using the follow-
ing \x0cve statistical parsers:
Table 1: The total / 11-point accuracy achieved
by each individual model
total 11-point
A 0.8974 0.9607
B 0.8551 0.9281
C 0.8586 0.9291
D 0.8470 0.9266
E 0.7885 0.8567
\x0f KANA (Ehara, 1998): a bottom-up model
based on maximum entropy estimation.
Since dependency score matrices given by
KANA have no probabilistic semantics, we
normalized them for each row using a cer-
tain function manually tuned for this parser.
\x0f CHAGAKE (Fujio et al., 1998): an exten-
sion of the bottom-up model proposed by
Collins (Collins, 1996).
\x0f Kanayama\'s parser (Kanayama et al., 1999):
a bottom-up model coupled with an HPSG.
\x0f Shirai\'s parser (Shirai et al., 1998): a top-
down model incorporating lexical collocation
statistics. Equation (1) was used for estimat-
ing DPs.
\x0f Peach Pie Parser (Uchimoto et al., 1999):
a bottom-up model based on maximum en-
tropy estimation.
Note that these models were developed fully
independently of each other, and have signi\x0c-
cantly di\x0berent characters (for a comparison of
their performance, see Table 1). In what fol-
lows, these models are referred to anonymously.
For the source of the training/test set, we
used the Kyoto corpus (ver.2.0) (Kurohashi et
al., 1997), which is a collection of Japanese
newspaper articles annotated in terms of word
boundaries, POS tags, BP boundaries, and
inter-BP dependency relations. The corpus
originally contained 19,956 sentences. To make
the training/test sets, we \x0crst removed all the
sentences that were rejected by any of the above
\x0cve parsers (3,146 sentences). For the remain-
ing 16,810 sentences, we next checked the con-
sistency of the BP boundaries given by the
parsers since they had slightly di\x0berent crite-
ria for BP segmentation from each other. In
this process, we tried to recover as many in-
consistent boundaries as possible. For example,
we found there were quite a few cases where
a parser recognized a certain word sequence as
a single BP, whereas some other parser recog-
nized the same sequence as two BPs. In such
\x0c0.95
0.955
0.96
0.965
0.97
A
B
A
C
A
D
A
E
A
B
C
A
C
D
A
C
E
A
D
E
A
B
C
D
A
B
C
D
E
Simple Normal Class
A : 0.9607
Figure 4: 11-point accuracy: A included
0.92
0.93
0.94
0.95
0.96
B
C
B
D
B
E
B
C
D
B
D
E
B
C
D
E
C
D
C
E
C
D
E
Simple Normal Class
B : 0.9281
C : 0.9291
Figure 5: 11-point accuracy: B/C included
a case, we regarded that sequence as a single
BP under a certain condition. As a result, we
obtained 13,990 sentences that can be accepted
by all the parsers with all the BP boundaries
consistent 2. We used this set for training and
evaluation.
For closed tests, we used 11,192 sentences
(66,536 BPs3) for both training and tests.
For open tests, we conducted \x0cve-fold cross-
validation on the whole sentence set.
2
In the BP concatenation process described here,
quite a few trivial dependency relations between neigh-
boring BPs were removed from the test set. This made
our test set slightly more di\x0ecult than what it should
have been.
3
This is the total number of BPs excluding the right-
most two BPs for each sentence. Since, in Japanese, a
BP always depends on a BP following it, the right-most
BP of a sentence does not depend on any other BP, and
the second right-most BP always depends on the right-
most BP. Therefore, they were not seen as subjects of
evaluation.
0.955
0.96
0.965
0.97
0.975
A
B
F
A
C
F
A
D
F
A
E
F
A
B
C
F
A
C
D
F
A
C
E
F
A
D
E
F
A
B
C
D
F
A
B
C
D
E
F
Simple Normal Class
A : 0.9607
A : 0.9607
A : 0.9607
Figure 6: 11-point accuracy: +KNP
For the classi\x0ccation of problems, we man-
ually established the following twelve classes,
each of which is de\x0cned in terms of a certain
morphological pattern of depending BPs:
1. nominal BP with a case marker \\wa (topic)"
2. nominal BP with a case marker \
o (POS)"
3. nominal BP with a case marker \\ga (NOM)"
4. nominal BP with a case marker \\o (ACC)"
5. nominal BP with a case marker \
i (DAT)"
6. nominal BP with a case marker \\de (LOC/: : :)"
7. nominal BP (residue)
8. adnominal verbal BP
9. verbal BP (residue)
10. adverb
11. adjective
12. residue
4.2 Results and discussion
Table 1 shows the total/11-point accuracy of
each individual model. The performance of each
model widely ranged from 0.96 down to 0.86
in 11-point accuracy. Remember that A is the
optimal model, and there are two second-best
models, B and C, which are closely comparable.
In what follows, we use these achievements as
the baseline for evaluating the error reduction
achieved by organizing a committee.
The performance of various committees is
shown in Figure 4 and 5. Our primary inter-
est here is whether the weighting functions pre-
sented above e\x0bectively contribute to error re-
duction. According to those two \x0cgures, al-
though the contribution of the function Nor-
mal were nor very visible, the function Class
consistently improved the accuracy. These re-
sults can be a good evidence for the important
role of weighting functions in combining parsers.
\x0c0.92
0.94
0.96
A
B
A
C
A
D
A
E
A
B
C
A
C
D
A
C
E
A
D
E
A
B
C
D
A
B
C
D
E
multiple voting
single voting
Figure 7: Single voting vs. Multiple voting
While we manually built the problem classi\x0cca-
tion in our experiment, automatic classi\x0ccation
techniques will also be obviously worth consid-
ering.
We then conducted another experiment to ex-
amine the e\x0bects of multiple voting. One can
straightforwardly simulate a single-voting com-
mittee by replacing wij in equation (7) with w0
ij
given by:
w0
ij =
\x1a
wij (if j = arg maxk wik)
0 (otherwise)
(8)
The results are shown in Figure 7, which
compares the original multi-voting committees
and the simulated single-voting committees.
Clearly, in our settings, multiple voting signif-
icantly outperformed single voting particularly
when the size of a committee is small.
The next issues are whether a committee al-
ways outperform its individual members, and if
not, what should be considered in organizing a
committee. Figure 4 and 5 show that commit-
tees not including the optimal model A achieved
extensive improvements, whereas the merit of
organizing committees including A is not very
visible. This can be partly attributed to the
fact that the competence of the individual mem-
bers widely diversed, and A signi\x0ccantly outper-
forms the other models.
Given the good error reduction achieved
by committees containing comparable members
such as BC, BD and BCD, however, it should be
reasonable to expect that a committee includ-
ing A would achieve a signi\x0ccant improvement if
another nearly optimal model was also incorpo-
0.5
0.6
0.7
0.8
0.9
1
0.5 0.6 0.7 0.8 0.9 1
CF
DF
dependency probability
accuracy
Figure 8: P-A curves: +KNP
rated. To empirically prove this assumption, we
conducted another experiment, where we add
another parser KNP (Kurohashi et al., 1994) to
each committee that appears in Figure 4. KNP
is much closer to model A in total accuracy
than the other models (0.8725 in total accu-
racy). However, it does not provide DP ma-
trices since it is designed in a rule-based fash-
ion | the current version of KNP provides only
the best-preferred parse tree for each input sen-
tence without any scoring annotation. We thus
let KNP to simply vote its total accuracy. The
results are shown in Figure 6. This time all the
committees achieved signi\x0ccant improvements,
with the maximum error reduction rate up to
31%.
As suggested by the results of this experiment
with KNP, our scheme allows a rule-based non-
parametric parser to play in a committee pre-
serving its ability to output parametric DP ma-
trices. To push the argument further, suppose
a plausible situation where we have an optimal
but non-parametric rule-based parser and sev-
eral suboptimal statistical parsers. In such a
case, our committee-based scheme may be able
to organize a committee that can provide DP
matrices while preserving the original total ac-
curacy of the rule-based parser. To see this, we
conducted another small experiment, where we
combined KNP with each of C and D, both of
which are less competent than KNP. The result-
ing committees successfully provided reasonable
P-A curves as shown in Figure 8, while even
further improving the original total accuracy of
KNP (0.8725 to 0.8868 for CF and 0.8860 for
DF). Furthermore, the committees also gained
the 11-point accuracy over C and D (0.9291 to
\x0c0.9600 for CF and 0.9266 to 0.9561 for DF).
These results suggest that our committee-based
scheme does work even if the most competent
member of a committee is rule-based and thus
non-parametric.
5 Conclusion
This paper presented a general committee-
based framework that can be coupled with prob-
abilistic partial parsing. In this framework, a
committee accepts parametric multiple votes,
and then standardizes them, and \x0cnally pro-
vides a probabilistic distribution. We presented
a general method for producing probabilistic
multiple votes (i.e. DP matrices), which al-
lows most of the existing probabilistic models
for parsing to join a committee. Our experi-
ments revealed that (a) if more than two compa-
rably competent models are available, it is likely
to be worthwhile to combine them, (b) both
multiple voting and vote standardization e\x0bec-
tively work in committee-based partial parsing,
(c) our scheme also allows a non-parametric
rule-based parser to make a good contribution.
While our experiments have so far been produc-
ing promising results, there seems to be much
room left for investigation and improvement.
Acknowledgments
We would like to express our special thanks to
all the creators of the parsers used here for en-
abling all of this research by providing us their
systems. We would also like to thank the re-
viewers for their suggestive comments.
References
Brill, E. and J. Wu. Classi\x0cer Combination for Im-
proved Lexical Disambiguation. In Proc. of the
17th COLING, pp.191{195, 1998.
Carroll, J. ,G. Minnen, Y. Canning, S. Devlin and
J. Tait. Practical Simpli\x0ccation of English News-
paper Text to Assist Aphasic Readers. In Proc. of
AAAI-98 Workshop on Integrating Arti\x0ccial In-
telligence and Assistive Technology,1998.
Charniak, E. Statistical parsing with a context-
free grammar and word statistics. In Proc. of the
AAAI, pp.598{603, 1997.
Collins, M. J. A new statistical parser based on bi-
gram lexical dependencies. In Proc. of the 34th
ACL, pp.184{191, 1996.
Collins, M. J. Three generative, lexicalised models
for statistical parsing. In Proc. of the 35th ACL,
pp.16{23, 1997.
Ehara, T. Estimating the consistency of Japanese
dependency relations based on the maximam en-
tropy modeling. In Proc. of the 4th Annual Meet-
ing of The Association of Natural Language Pro-
cessing, pp.382{385, 1998. (In Japanese)
Fiscus, J. G. A post-processing system to yield re-
duced word error rates: Recognizer output voting
error reduction (ROVER). In EuroSpeech, 1997.
Frederking, R. and S. Nirenburg. Three heads are
better than one. In Proc. of the 4th ANLP, 1994.
Fujio, M. and Y. Matsumoto. Japanese dependency
structure analysis based on lexicalized statistics.
In Proc. of the 3rd EMNLP, pp.87{96, 1998.
Henderson, J. C. and E. Brill. Exploiting Diver-
sity in Natural Language Processing: Combining
Parsers. In Proc. of the 1999 Joint SIGDAT Con-
ference on EMNLP and VLC, pp.187{194.
Jensen, K., G. E. Heidorn, and S. D. Richardson,
editors. natural language processing: The PLNLP
Approach. Kluwer Academic Publishers, 1993.
Kanayama, H., K. Torisawa, Y. Mitsuisi, and
J. Tsujii. Statistical Dependency Analysis with
an HPSG-based Japanese Grammar. In Proc. of
the NLPRS, pp.138{143, 1999.
Kurohashi, S. and M. Nagao. Building a Japanese
parsed corpus while improving the parsing system.
In Proc. of NLPRS, pp.151{156, 1997.
Kurohashi, S. and M. Nagao. KN Parser : Japanese
Dependency/Case Structure Analyzer. In Proc. of
The International Workshop on Sharable Natural
Language Resources, pp.48-55, 1994.
Poole, D. Average-case analysis of a search algo-
rithm for estimating prior and posterior probabil-
ities in Bayesian networks with extreme probabil-
ities. the 13th IJCAI, pp.606{612, 1993.
Pedersen, T. A Simple Approach to Building En-
sembles of Naive Bayesian Classi\x0cers for Word
Sense Disambiguation In Proc. of the NAACL,
pp.63-69, 2000.
Shirai, K., K. Inui, T. Tokunaga and H. Tanaka
An empirical evaluation on statistical parsing
of Japanese sentences using a lexical association
statistics. the 3rd EMNLP, pp.80-87, 1998.
Uchimoto, K., S. Sekine, and H. Isahara. Japanese
dependency structure analysis based on maxi-
mum entopy models. In Proc. of the 13th EACL,
pp.196-203, 1999.
van Halteren, H., J. Zavrel, and W. Daelemans. Im-
proving data driven wordclass tagging by system
combination. In Proc. of the 17th COLING, 1998.
\x0c'